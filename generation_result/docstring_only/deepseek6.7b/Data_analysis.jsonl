{"instance_id": "58936b2a621b", "generation_result": [{"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n", "CodeBleu_score": 83.87646776544821}]}
{"instance_id": "83b142d9d251", "generation_result": [{"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=1 - support_ratio,\n        stratify=label,\n        random_state=42,\n    )\n    return x_train, x_test, y_train, y_test", "CodeBleu_score": 65.30740775035834}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=1 - support_ratio,\n        stratify=label,\n        random_state=42,\n    )\n    return x_train, x_test, y_train, y_test", "CodeBleu_score": 65.30740775035834}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 65.28958649903562}]}
{"instance_id": "5fd02b540f61", "generation_result": [{"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    X, y = make_regression(n_samples=100, n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 38.988730693271556}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    X, y = make_regression(n_samples=1000, n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, X_test, y_train, y_test", "CodeBleu_score": 38.79514250108158}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    X, y = make_regression(n_samples=1000, n_features=3, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, y_train, X_test, y_test", "CodeBleu_score": 38.92483101817302}]}
{"instance_id": "ad958cb31a57", "generation_result": [{"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    return GMMParameters(\n        label=ndarrays[0],\n        means=ndarrays[1],\n        weights=ndarrays[2],\n        covariances=ndarrays[3],\n        num_samples=ndarrays[4],\n    )\n\n", "CodeBleu_score": 79.03672928896962}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    return GMMParameters(\n        label=ndarrays[0],\n        means=ndarrays[1],\n        weights=ndarrays[2],\n        covariances=ndarrays[3],\n        num_samples=ndarrays[4],\n    )\n\n", "CodeBleu_score": 79.03672928896962}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    label = ndarrays[0]\n    means = ndarrays[1]\n    weights = ndarrays[2]\n    covariances = ndarrays[3]\n    num_samples = ndarrays[4]\n    return GMMParameters(label, means, weights, covariances, num_samples)\n", "CodeBleu_score": 40.08951228295815}]}
{"instance_id": "061d5a5b9d56", "generation_result": [{"code": "def datafiles_fusion(data_paths):\n    \n    # Load the first data file\n    X, y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    y = np.array(y)\n    \n    # Concatenate the remaining data files\n    for path in data_paths[1:]:\n        X_new, y_new = load_svmlight_file(path)\n        X_new = X_new.toarray()\n        y_new = np.array(y_new)\n        \n        # Check if the number of features matches\n        if X_new.shape[1] != X.shape[1]:\n            raise ValueError(\"Number of features in data files do not match\")\n        \n        # Concatenate the new data\n        X = np.concatenate((X, X_new), axis=0)\n        y = np.concatenate((y, y_new), axis=0)\n    \n    return X, y", "CodeBleu_score": 35.31758451474275}, {"code": "def datafiles_fusion(data_paths):\n    \n    X, y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for path in data_paths[1:]:\n        X_new, y_new = load_svmlight_file(path)\n        X_new = X_new.toarray()\n        assert X_new.shape[1] == X.shape[1], \"Number of features in {} does not match number of features in {}\".format(path, data_paths[0])\n        X = np.concatenate((X, X_new), axis=0)\n        y = np.concatenate((y, y_new), axis=0)\n    return X, y\n", "CodeBleu_score": 32.857800169983726}, {"code": "def datafiles_fusion(data_paths):\n    \n    # Load the first data file\n    X, y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    \n    # Concatenate the features and labels from each subsequent data file\n    for path in data_paths[1:]:\n        X_new, y_new = load_svmlight_file(path)\n        X_new = X_new.toarray()\n        assert X_new.shape[1] == X.shape[1], \"Number of features does not match\"\n        X = np.concatenate((X, X_new), axis=0)\n        y = np.concatenate((y, y_new), axis=0)\n    \n    return X, y\n", "CodeBleu_score": 33.14792500465338}]}
{"instance_id": "977d3bd5d5a3", "generation_result": [{"code": "def eval_learning(y_test, y_pred)\n    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n    return accuracy, recall, precision, f1\n", "CodeBleu_score": 54.6058043693212}, {"code": "def eval_learning(y_test, y_pred)\n    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 54.6058043693212}, {"code": "def eval_learning(y_test, y_pred)\n    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n    return accuracy, recall, precision, f1\n", "CodeBleu_score": 54.6058043693212}]}
{"instance_id": "f4c838f1f092", "generation_result": [{"code": "def train(model, train_loader, num_epochs=1)\n    criterion = torch.nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    for epoch in range(num_epochs):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n", "CodeBleu_score": 47.1430657025005}, {"code": "def train(model, train_loader, num_epochs=1)\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            if (i+1) % 100 == 0:\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n", "CodeBleu_score": 51.311382198983615}, {"code": "def train(model, train_loader, num_epochs=1)\n    criterion = torch.nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for i, (images, labels) in enumerate(train_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            if (i + 1) % 100 == 0:\n                print(\n                    f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n                )\n\n", "CodeBleu_score": 47.77329969192687}]}
{"instance_id": "35068c779c02", "generation_result": [{"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    assert cfg.dataset.type == \"LIBSVM\"\n    dataset_path = cfg.dataset.path\n    dataset_name = cfg.dataset.name\n\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.toarray().astype(np.float32)\n    labels = labels.astype(np.int64)\n\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Features shape: {data.shape}\")\n\n    if dataset_name == \"MUSHROOMS\":\n        labels = np.where(labels == 1, 0, 1)\n    else:\n        raise RuntimeError(f\"Unknown dataset: {dataset_name}\")\n\n    dataset = TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n    return dataset\n", "CodeBleu_score": 33.0365532101997}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    if cfg.dataset.type != 'LIBSVM':\n        raise RuntimeError(f'Expected dataset type to be LIBSVM, but got {cfg.dataset.type}')\n    path = cfg.dataset.path\n    name = cfg.dataset.name\n    if name == 'MUSHROOMS':\n        data, labels = load_svmlight_file(path)\n        labels = labels.reshape(-1, 1)\n        labels = np.where(labels == 1, 0, 1)\n        print('Labels:', np.unique(labels, return_counts=True))\n        print('Features shape:', data.shape)\n        data = data.toarray().astype(np.float32)\n        dataset = TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n        return dataset\n    else:\n        raise RuntimeError(f'Dataset {name} is not supported')", "CodeBleu_score": 42.58799717378977}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    if cfg.dataset.type != \"LIBSVM\":\n        raise RuntimeError(\n            f\"Dataset type {cfg.dataset.type} is not supported for LIBSVM datasets\"\n        )\n    if cfg.dataset.name == \"MUSHROOMS\":\n        data, labels = load_svmlight_file(\n            f\"{cfg.dataset.path}/{cfg.dataset.name}/{cfg.dataset.name}.scale\"\n        )\n        labels = np.array(labels, dtype=np.float32)\n        labels[labels == 1] = 0\n        labels[labels != 0] = 1\n        print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n        print(f\"Shape of features: {data.shape}\")\n        return TensorDataset(torch.from_numpy(data.toarray().astype(np.float32)), torch.from_numpy(labels))\n    else:\n        raise RuntimeError(\n            f\"Dataset {cfg.dataset.name} is not supported for LIBSVM datasets\"\n        )", "CodeBleu_score": 38.09523665792501}]}
{"instance_id": "d602a96e571e", "generation_result": [{"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, 'fit_transform'):\n        if weight is not None:\n            return transformer.fit_transform(X, y, **fit_params) * weight, transformer\n        else:\n            return transformer.fit_transform(X, y, **fit_params), transformer\n    else:\n        transformer = transformer.fit(X, y, **fit_params)\n        if weight is not None:\n            return transformer.transform(X) * weight, transformer\n        else:\n            return transformer.transform(X), transformer\n", "CodeBleu_score": 39.22353388769431}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, \"fit_transform\"):\n        if y is None:\n            res = transformer.fit_transform(X, **fit_params)\n        else:\n            res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        if y is not None:\n            transformer.fit(X, y, **fit_params)\n        else:\n            transformer.fit(X, **fit_params)\n        res = transformer.transform(X)\n    if weight is not None:\n        res *= weight\n    return res, transformer\n", "CodeBleu_score": 53.776693373391026}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, 'fit_transform'):\n        Xt = transformer.fit_transform(X, y, **fit_params)\n    else:\n        transformer.fit(X, y, **fit_params)\n        Xt = transformer.transform(X)\n    if weight is not None:\n        Xt = Xt * weight\n    return Xt, transformer\n", "CodeBleu_score": 59.359532206453835}]}
{"instance_id": "fcf88a773fd8", "generation_result": [{"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    if copy:\n        X = X.copy()\n    if not isinstance(X, np.ndarray):\n        raise TypeError(\"X must be a numpy array\")\n    if not isinstance(transform, str):\n        raise TypeError(\"transform must be a string\")\n    if transform not in [\"log\", \"sqrt\", \"exp\"]:\n        raise ValueError(\"transform must be 'log', 'sqrt', or 'exp'\")\n    if selected == \"all\":\n        if transform == \"log\":\n            X = np.log(X)\n        elif transform == \"sqrt\":\n            X = np.sqrt(X)\n        elif transform == \"exp\":\n            X = np.exp(X)\n        return X\n    elif selected == \"none\":\n        return X\n    else:\n        if not isinstance(selected, list):\n            raise TypeError(\"selected must be a list\")\n        if not all(isinstance(x, int) for x in selected):\n            raise TypeError(\"selected must be a list of integers\")\n        if not all(0 <= x < X.shape[1] for x in selected):\n            raise ValueError(\"selected must be a list of integers between 0 and the number of features\")\n        if not isinstance(X, sparse.csc_matrix):\n            raise TypeError(\"X must be a sparse csc matrix\")\n        if transform == \"log\":\n            X_selected = np.log(X[:, selected])\n            X[:, selected] = X_selected\n        elif transform == \"sqrt\":\n            X_selected = np.sqrt(X[:, selected])\n            X[:, selected] = X_selected\n        elif transform == \"exp\":\n            X_selected = np.exp(X[:, selected])\n            X[:, selected] = X_selected\n        return X\n", "CodeBleu_score": 29.537803191695694}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    # Check that X is a valid array\n    if not isinstance(X, (np.ndarray, csc_matrix)):\n        raise ValueError(\"X must be a numpy array or sparse csc matrix\")\n    # If all features are to be transformed, apply the transformation to the entire array\n    if selected == \"all\":\n        return transform(X)\n    # If no features are selected, return X unchanged\n    if selected is None or len(selected) == 0:\n        return X\n    # Identify the selected and non-selected features\n    selected_features = np.array(selected)\n    non_selected_features = np.setdiff1d(np.arange(X.shape[1]), selected_features)\n    # Apply the transformation to the selected features\n    transformed_selected_features = transform(X[:, selected_features])\n    # Combine the transformed and non-transformed features\n    X_transformed = np.hstack((X[:, non_selected_features], transformed_selected_features))\n    # Return the combined result\n    return X_transformed\n", "CodeBleu_score": 36.61367356327572}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    if copy:\n        X = X.copy()\n    if selected == \"all\":\n        return transform(X)\n    else:\n        selected = np.asarray(selected, dtype=int)\n        if selected.ndim != 1:\n            raise ValueError(\"selected must be a 1D array\")\n        if selected.size == 0:\n            return X\n        n_features = X.shape[1]\n        selected = np.sort(selected)\n        if np.any(selected < 0) or np.any(selected >= n_features):\n            raise ValueError(\"selected features are not valid for the input array\")\n        if np.any(np.diff(selected) < 0):\n            raise ValueError(\"selected features must be sorted\")\n        if np.any(np.diff(selected) == 0):\n            raise ValueError(\"selected features must be unique\")\n        non_selected = np.setdiff1d(np.arange(n_features), selected)\n        X_selected = X[:, selected]\n        X_non_selected = X[:, non_selected]\n        X_selected = transform(X_selected)\n        X[:, selected] = X_selected\n        if X_non_selected.size > 0:\n            X[:, non_selected] = X_non_selected\n        return X\n", "CodeBleu_score": 41.550668923582045}]}
{"instance_id": "c9dfb00047d1", "generation_result": [{"code": "def _transform_new(self, X)\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    if not X.dtype == np.dtype(np.object):\n        X = X.astype(np.object)\n    n_samples, n_features = X.shape\n    X_int = np.zeros((n_samples, n_features), dtype=np.int32)\n    valid_mask = np.ones((n_samples, n_features), dtype=bool)\n    n_values = np.zeros(n_features, dtype=np.int32)\n    feature_indices = np.zeros(n_features, dtype=np.int32)\n    for j in range(n_features):\n        X_int[:, j] = self._label_encoder[j].transform(X[:, j])\n        if self._label_encoder[j].indices is not None:\n            n_values[j] = len(self._label_encoder[j].indices)\n            feature_indices[j] = self._label_encoder[j].indices[0]\n        else:\n            valid_mask[:, j] = False\n    if self.sparse:\n        return csr_matrix((X_int.ravel(), feature_indices, np.arange(n_samples + 1) * n_features), shape=(n_samples, np.sum(n_values)))\n    else:\n        return X_int", "CodeBleu_score": 34.15704393864915}, {"code": "def _transform_new(self, X)\n    X = self._check_array(X)\n    n_samples, n_features = X.shape\n    n_values = np.unique(X, axis=0, return_counts=False)\n    n_values = n_values.shape[0]\n    indptr = np.zeros(n_features + 1, dtype=np.intc)\n    indices = np.empty(n_samples * n_features, dtype=np.intc)\n    data = np.ones(n_samples * n_features, dtype=np.bool_)\n    for j in range(n_features):\n        feature_values = X[:, j]\n        feature_indices = np.arange(j * n_samples, (j + 1) * n_samples)\n        feature_indices = feature_indices[~np.isnan(feature_values)]\n        feature_values = feature_values[~np.isnan(feature_values)]\n        feature_values = self._label_encoder.transform(feature_values)\n        feature_values = feature_values.astype(np.intc)\n        indptr[j + 1] = indptr[j] + len(feature_values)\n        indices[indptr[j]:indptr[j + 1]] = feature_indices\n        data[indptr[j]:indptr[j + 1]] = feature_values\n    return csr_matrix((data, indices, indptr), shape=(n_samples, n_values))\n\n", "CodeBleu_score": 29.8909943602961}, {"code": "def _transform_new(self, X)\n    X = self._check_input_array(X)\n    n_samples, n_features = X.shape\n    mask = _check_feature_lengths(X, self.categories_)\n    n_values = [len(categories) for categories in self.categories_]\n    feature_indices = _make_feature_indices(n_values)\n    data = _encode(X, self.categories_, mask, feature_indices)\n    indptr = _make_indptr(n_samples, mask)\n    indices = _make_indices(X, mask, feature_indices)\n    return csr_matrix((data, indices, indptr), shape=(n_samples, n_features))\n", "CodeBleu_score": 16.851628008052344}]}
{"instance_id": "fcde391a8972", "generation_result": [{"code": "def convert_to_sklearn(self)\n    from scipy.stats import uniform, loguniform\n    if self.log:\n        return loguniform(self.lower_bound, self.upper_bound)\n    else:\n        return uniform(self.lower_bound, self.upper_bound)", "CodeBleu_score": 33.674985747608375}, {"code": "def convert_to_sklearn(self)\n    import numpy as np\n    from scipy.stats import loguniform, uniform\n    if self.log:\n        return loguniform(self.lower, self.upper)\n    else:\n        return uniform(self.lower, self.upper)\n", "CodeBleu_score": 44.768554116020134}, {"code": "def convert_to_sklearn(self)\n    if self.log:\n        from scipy.stats import uniform\n        sampler = uniform(loc=self.lower, scale=self.upper - self.lower)\n    else:\n        from scipy.stats import uniform\n        sampler = uniform(loc=self.lower, scale=self.upper - self.lower)\n    return sampler", "CodeBleu_score": 49.98322635936251}]}
{"instance_id": "5d5413adcca6", "generation_result": [{"code": "def _render(self, state: AnalysisState) -> None\n    additional_args = self.additional_args\n    figure_args = self.figure_args\n    features = self.features\n\n    if self.headers:\n        self._render_header(state)\n\n    fig, ax = plt.subplots(**figure_args)\n    kwargs = {**self.predefined_kwargs, **additional_args}\n\n    if kwargs.get(\"two_way\"):\n        data = state.data\n        data = data[~data.isna().any(axis=1)]\n\n    display = PartialDependenceDisplay.from_estimator(\n        state.model,\n        data,\n        features,\n        ax=ax,\n        **kwargs,\n    )\n\n    fig.tight_layout()\n    plt.show()", "CodeBleu_score": 44.48212913639967}, {"code": "def _render(self, state: AnalysisState) -> None\n    additional_args = self.get_additional_args(state)\n    figure_args = self.get_figure_args(state)\n    features = self.get_features(state)\n\n    self.render_header(state)\n\n    fig, ax = plt.subplots(**figure_args)\n    kwargs = self._prepare_kwargs(additional_args)\n\n    if additional_args.get(\"two_way_interaction\"):\n        data = state.data[state.data[features[0]].notnull() & state.data[features[1]].notnull()]\n    else:\n        data = state.data\n\n    PartialDependenceDisplay.from_estimator(\n        state.model,\n        data,\n        features,\n        ax=ax,\n        **kwargs,\n    )\n\n    fig.tight_layout()\n    plt.show()", "CodeBleu_score": 37.36874459711848}, {"code": "def _render(self, state: AnalysisState) -> None\n    additional_arguments = self._get_additional_arguments(state)\n    figure_arguments = self._get_figure_arguments(state)\n    features = self._get_features(state)\n    self._render_header(state)\n    fig, axes = plt.subplots(**figure_arguments)\n    kwargs = {**additional_arguments, **self.predefined_keyword_arguments}\n    if self.two_way_interaction:\n        data = self._filter_data(state)\n    else:\n        data = state.data\n    display = PartialDependenceDisplay.from_estimator(\n        state.model, data, features, ax=axes, **kwargs\n    )\n    fig.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 34.971108584732406}]}
{"instance_id": "db8aac29180b", "generation_result": [{"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if not self.feature_generator:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.features_:\n        X = X.copy()\n        X[self.feature_generator.features_] = self.feature_generator.transform(X[self.feature_generator.features_])\n    X = X.fillna(0).astype('float32')\n    return X", "CodeBleu_score": 49.73033755155808}, {"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_generator is None:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.features:\n        X = X.copy()\n        X[self.feature_generator.features] = self.feature_generator.transform(X[self.feature_generator.features])\n    X = X.fillna(0).astype(np.float32)\n    return X", "CodeBleu_score": 51.43110202819406}, {"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_generator is None:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.has_features():\n        X = X.copy()\n        X[self.feature_generator.features] = self.feature_generator.transform(X[self.feature_generator.features])\n    X = X.fillna(0).astype(np.float32)\n    return X", "CodeBleu_score": 51.19494149059194}]}
{"instance_id": "d4eb45e6d013", "generation_result": [{"code": "def _estimate_memory_usage(self, X, **kwargs)\n    # Retrieve model parameters\n    n_estimators = self.get_params()[\"n_estimators\"]\n    max_depth = self.get_params()[\"max_depth\"]\n    min_samples_split = self.get_params()[\"min_samples_split\"]\n    min_samples_leaf = self.get_params()[\"min_samples_leaf\"]\n    max_features = self.get_params()[\"max_features\"]\n    max_leaf_nodes = self.get_params()[\"max_leaf_nodes\"]\n    min_impurity_decrease = self.get_params()[\"min_impurity_decrease\"]\n    bootstrap = self.get_params()[\"bootstrap\"]\n    oob_score = self.get_params()[\"oob_score\"]\n    n_jobs = self.get_params()[\"n_jobs\"]\n\n    # Determine the final number of estimators\n    if self.search_space is not None:\n        final_n_estimators = len(self.search_space)\n    else:\n        final_n_estimators = n_estimators\n    if final_n_estimators < 40:\n        final_n_estimators = 40\n\n    # Calculate the number of trees per estimator\n    n_trees_per_estimator = self.get_params()[\"n_estimators\"] // final_n_estimators\n    if n_trees_per_estimator < 1:\n        n_trees_per_estimator = 1\n\n    # Estimate the bytes used per estimator based on the size of X\n    n_features = X.shape[1]\n    n_samples = X.shape[0]\n    bytes_per_estimator = (\n        n_features * 4\n        + n_samples * 4\n        + n_samples * 4\n        + n_samples * 4\n        + n_samples\n        + n_samples\n        + n_samples\n        + n_samples\n        + n_samples\n        + n_samples\n        + n_samples\n        + n_samples\n        + n_samples\n        +", "CodeBleu_score": 27.260798906967587}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n    n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"auto\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n    if n_estimators == \"search\":\n        n_estimators = self._get_param(\"n_estimators\")\n", "CodeBleu_score": 15.104300366460436}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n    model_params = self.get_params()\n    final_n_estimators = self._get_final_n_estimators(X, **kwargs)\n    n_trees_per_estimator = self.get_params()['n_estimators'] // final_n_estimators\n    bytes_per_estimator = self._get_bytes_per_estimator(X)\n    expected_min_memory_usage = (bytes_per_estimator * final_n_estimators) / 1e6\n    return expected_min_memory_usage", "CodeBleu_score": 19.63114864099008}]}
{"instance_id": "6923cb007f4c", "generation_result": [{"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n    ngram_counter = Counter(ngram_freq)\n    top_ngrams = ngram_counter.most_common(vocab_size)\n    top_ngrams_sorted = sorted([ngram for ngram, _ in top_ngrams])\n    new_vocab = {ngram: index for index, ngram in enumerate(top_ngrams_sorted)}\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer\n", "CodeBleu_score": 35.521732470246384}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n    # Create a Counter object from the n-gram frequency data\n    ngram_counter = Counter(ngram_freq)\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    top_ngrams = ngram_counter.most_common(vocab_size)\n    # Sort the names of the top n n-grams\n    top_ngrams_names = sorted([ngram for ngram, freq in top_ngrams])\n    # Create a new vocabulary dictionary mapping each n-gram to a unique index\n    new_vocab = {ngram: index for index, ngram in enumerate(top_ngrams_names)}\n    # Update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n    return vectorizer", "CodeBleu_score": 35.86340142960695}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n    # Create a Counter object from the n-gram frequency data\n    ngram_counter = Counter(ngram_freq)\n\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    top_ngrams = ngram_counter.most_common(vocab_size)\n\n    # Sort the top n n-grams by name\n    top_ngrams = sorted(top_ngrams, key=lambda x: x[0])\n\n    # Create a new vocabulary dictionary mapping each n-gram to a unique index\n    new_vocab = {ngram: index for index, (ngram, _) in enumerate(top_ngrams)}\n\n    # Update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n\n    return vectorizer\n\n", "CodeBleu_score": 33.00631993750176}]}
{"instance_id": "0e837b2d8088", "generation_result": [{"code": " def _select_continuous(self, df, features)\n    feature_types = {}\n    skewed_threshold = self.parameters.get('skewed_threshold', 0.75)\n\n    for feature in features:\n        skewness = df[feature].skew()\n        if abs(skewness) > skewed_threshold:\n            feature_types[feature] = 'skewed'\n        else:\n            feature_types[feature] = 'continuous'\n\n    return feature_types\n\n", "CodeBleu_score": 41.28857753368694}, {"code": " def _select_continuous(self, df, features)\n    feature_types = {}\n    skew_threshold = self.params[\"skew_threshold\"]\n    for feature in features:\n        if df[feature].nunique() > 20:\n            feature_types[feature] = \"skewed\"\n        else:\n            feature_types[feature] = \"continuous\"\n    return feature_types", "CodeBleu_score": 36.320266368359974}, {"code": " def _select_continuous(self, df, features)\n    feature_types = {}\n    skewness_threshold = self.params[\"skewness_threshold\"]\n    for feature in features:\n        skewness = df[feature].skew()\n        if abs(skewness) > skewness_threshold:\n            feature_types[feature] = \"skewed\"\n        else:\n            feature_types[feature] = \"continuous\"\n    return feature_types\n\n def _select_categorical(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either continuous or skewed based on their skewness.\\nIt initializes a dictionary to store these feature types and retrieves the skewness threshold from the parameters.\\nFor each feature, it calculates the skewness and compares it to the threshold.\\nFeatures with skewness exceeding the threshold are classified as \"skewed\", while others are classified as \"continuous\".\\nThe function returns a dictionary with lists of continuous and skewed features.\"\"\"\n    feature_types = {}\n    skewness_threshold = self.params[\"skewness_threshold\"]\n    for feature in features:\n        skewness = df[feature].skew()\n        if abs(skewness) > skewness_threshold:\n            feature_types[feature] = \"skewed\"\n        else:\n            feature_types[feature] = \"continuous\"\n    return feature_types\n\n def _select_categorical(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either continuous or skewed based on their skewness.\\nIt initializes a dictionary to store these feature types and retrieves the skewness threshold from the parameters.\\nFor each feature, it calculates the skewness and compares it to the threshold.\\nFeatures with skewness exceeding the threshold are classified as \"skewed\", while others are classified as \"continuous\".\\nThe function returns a dictionary with lists of continuous and skewed features.\"\"\"\n    feature_types = {}\n    skewness_threshold = self.params[\"skewness_threshold\"]\n    for feature in features:\n        skewness = df[feature].skew", "CodeBleu_score": 36.548804073741856}]}
{"instance_id": "bb569c10f975", "generation_result": [{"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    if weights == \"distance\":\n        dist[dist == 0] = np.inf\n        weights = 1.0 / dist\n        weights /= np.sum(weights, axis=1)[:, np.newaxis]\n        weights = np.nan_to_num(weights, copy=False)\n        return weights\n    if callable(weights):\n        return weights(dist)\n    raise ValueError(\n        \"Unknown weights parameter: {}\".format(weights)\n    )\n\n", "CodeBleu_score": 43.306000440560815}, {"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        weights = 1.0 / dist\n        weights[weights == np.inf] = 0\n        weights = np.diag(weights)\n        return weights\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\n            \"Unrecognized weights parameter. Should be 'uniform', 'distance', or a callable function.\"\n        )\n\n", "CodeBleu_score": 49.11158345182795}, {"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        weights = 1.0 / dist\n        weights[weights == np.inf] = 0.0\n        weights = np.diag(weights)\n        return weights\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"Unrecognized weights parameter\")\n", "CodeBleu_score": 41.9407352198453}]}
{"instance_id": "12aaa9c78402", "generation_result": [{"code": "def predict_loo(self)\n    # Compute distances and indices of the k-nearest neighbors\n    distances, indices = self._kneighbors(self._X)\n\n    # Compute weights for the neighbors\n    weights = self._get_weights(distances)\n\n    # Reshape the target variable if it is one-dimensional\n    if self._y.ndim == 1:\n        self._y = self._y.reshape(-1, 1)\n\n    # Calculate predictions\n    if self.weights == 'uniform':\n        # Unweighted prediction\n        predictions = np.mean(self._y[indices], axis=1)\n    else:\n        # Weighted prediction\n        predictions = np.sum(self._y[indices] * weights, axis=1) / np.sum(weights, axis=1)\n\n    # Flatten the predictions if the original target variable was one-dimensional\n    if self._y.ndim == 1:\n        predictions = predictions.flatten()\n\n    return predictions", "CodeBleu_score": 29.138082079917137}, {"code": "def predict_loo(self)\n    # Compute distances and indices of k nearest neighbors\n    distances, indices = self.kneighbors(self.X)\n\n    # Get weights for neighbors\n    weights = self._get_weights(distances)\n\n    # Reshape y if necessary\n    if len(self.y.shape) == 1:\n        self.y = self.y.reshape(-1, 1)\n\n    # Compute predictions\n    if self.weights == 'uniform':\n        predictions = np.mean(self.y[indices], axis=1)\n    elif self.weights == 'distance':\n        predictions = np.sum(self.y[indices] * weights, axis=1) / np.sum(weights, axis=1)\n\n    # Flatten predictions if necessary\n    if len(predictions.shape) > 1:\n        predictions = predictions.flatten()\n\n    return predictions", "CodeBleu_score": 29.71651506758608}, {"code": "def predict_loo(self)\n    n_samples = self._X.shape[0]\n    n_neighbors = self._n_neighbors\n    distances, indices = self._tree.query(self._X, k=n_neighbors, return_distance=True)\n    weights = self._get_weights(distances)\n    predictions = np.zeros(n_samples)\n    for i in range(n_samples):\n        neighbor_targets = self._y[indices[i]]\n        if self._y.ndim == 1:\n            neighbor_targets = neighbor_targets.reshape(-1, 1)\n        if self._weights == 'uniform':\n            predictions[i] = np.mean(neighbor_targets, axis=0)\n        elif self._weights == 'distance':\n            weighted_targets = neighbor_targets * weights[i]\n            predictions[i] = np.sum(weighted_targets, axis=0) / np.sum(weights[i])\n        else:\n            raise ValueError(\"Invalid weights parameter\")\n    if self._y.ndim == 1:\n        predictions = predictions.flatten()\n    return predictions", "CodeBleu_score": 32.903032357260464}]}
{"instance_id": "b316687d9e9b", "generation_result": [{"code": "def isotonic(input_data, quantile_list)\n    import numpy as np\n    from sklearn.isotonic import IsotonicRegression\n    # Reshape quantile_list to a one-dimensional array\n    quantile_list = np.array(quantile_list).reshape(-1)\n    # Get the batch size from the first dimension of input_data\n    batch_size = input_data.shape[0]\n    # Initialize an empty array to store the transformed data\n    transformed_data = np.zeros_like(input_data)\n    # Iterate over each row of the input data\n    for i in range(batch_size):\n        # Fit and transform the data using IsotonicRegression with the given quantile list\n        transformed_data[i] = IsotonicRegression(quantile_list).fit_transform(input_data[i])\n    # Return the transformed data\n    return transformed_data", "CodeBleu_score": 54.62547886608523}, {"code": "def isotonic(input_data, quantile_list)\n    quantile_list = np.reshape(quantile_list, -1)\n    batch_size = input_data.shape[0]\n    output_data = np.zeros_like(input_data)\n    for i in range(batch_size):\n        iso = IsotonicRegression(quantile_list)\n        iso.fit(input_data[i], input_data[i])\n        output_data[i] = iso.transform(input_data[i])\n    return output_data", "CodeBleu_score": 39.66143074250854}, {"code": "def isotonic(input_data, quantile_list)\n    import numpy as np\n    from sklearn.isotonic import IsotonicRegression\n    quantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    output_data = np.zeros_like(input_data)\n    for i in range(batch_size):\n        ir = IsotonicRegression(out_of_bounds='clip')\n        ir.fit(quantile_list, input_data[i])\n        output_data[i] = ir.transform(input_data[i])\n    return output_data", "CodeBleu_score": 54.28069224510202}]}
{"instance_id": "17566e088b4e", "generation_result": [{"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, vectorizer.transform(text_data)\n\n", "CodeBleu_score": 40.6826747095547}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    vectorized_data = vectorizer.transform(text_data)\n    return vectorizer, vectorized_data\n\n", "CodeBleu_score": 45.25213029105295}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.stop_words_ = None\n    vectorizer.fit(text_data)\n    return vectorizer, vectorizer.transform(text_data)\n\n", "CodeBleu_score": 33.51652817065228}]}
{"instance_id": "be23c0afc2d6", "generation_result": [{"code": "def preprocess(df, test_size, shuffle)\n    df = df[df.columns[~df.isin(['--']).any()]]\n    df = df.dropna(subset=['days_to_death', 'year_of_death'])\n    df = df.drop(df.columns[df.nunique() <= 1], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('id')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('shortcut')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('death')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('date')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('year')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('month')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('day')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('age')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('day_of_week')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('hour')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('minute')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('second')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('millisecond')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('time')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('latitude')], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('longitude')], axis=1)\n", "CodeBleu_score": 18.033689972714313}, {"code": "def preprocess(df, test_size, shuffle)\n    # Remove rows with missing entries marked by \"--\"\n    df = df[df.isin(['--']).sum(axis=1) == 0]\n\n    # Drop columns with unique values less than or equal to one\n    df = df.drop(df.columns[df.nunique() <= 1], axis=1)\n\n    # Drop columns containing \"id\" in their name\n    df = df.drop(df.columns[df.columns.str.contains('id', case=False)], axis=1)\n\n    # Drop shortcut columns like \"days_to_death\" and \"year_of_death\"\n    df = df.drop(df.columns[df.columns.str.contains('days_to_death', case=False)], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains('year_of_death', case=False)], axis=1)\n\n    # Split the dataframe into training and testing sets\n    train, test = train_test_split(df, test_size=test_size, shuffle=shuffle)\n\n    return train, test\n\n", "CodeBleu_score": 28.41044594579083}, {"code": "def preprocess(df, test_size, shuffle)\n    # Remove rows with missing entries\n    df = df[df.apply(lambda x: x.str.contains('--').sum() == 0, axis=1)]\n\n    # Drop columns with unique values less than or equal to one\n    unique_values = df.nunique()\n    columns_to_drop = unique_values[unique_values <= 1].index\n    df = df.drop(columns_to_drop, axis=1)\n\n    # Drop columns with \"id\" in their name\n    columns_to_drop = [col for col in df.columns if 'id' in col]\n    df = df.drop(columns_to_drop, axis=1)\n\n    # Drop shortcut columns\n    shortcut_columns = ['days_to_death', 'year_of_death']\n    df = df.drop(shortcut_columns, axis=1)\n\n    # Split into training and testing sets\n    X = df.drop('death_event', axis=1)\n    y = df['death_event']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n\n    return X_train, X_test, y_train, y_test", "CodeBleu_score": 26.895935484503557}]}
{"instance_id": "6a82997a8c59", "generation_result": [{"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n    from sklearn.utils import shuffle\n    from sklearn.metrics import accuracy_score\n    import numpy as np\n    import pandas as pd\n    from sklearn.preprocessing import OneHotEncoder\n    if dataset == \"iris\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"breast_cancer\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"digits\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"wine\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"diabetes\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"boston\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"california_housing\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"linnerud\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"wine_quality_red\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"wine_quality_white\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"titanic\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"telco_customer_churn\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"credit_card_default\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"mushroom\":\n        X, y = load_iris(return_X_y=True)\n    elif dataset == \"adult\":\n        X, y =", "CodeBleu_score": 9.986043849886114}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n    import numpy as np\n    if dataset == \"iris\":\n        data = load_iris()\n    else:\n        raise ValueError(\"Invalid dataset specified.\")\n    X = data.data\n    y = data.target\n    if sparse:\n        X = X.tocsc()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    hp_space = classifier.get_hyperparameter_search_space()\n    default_config = hp_space.get_default_configuration()\n    clf = classifier(**default_config, random_state=0)\n    clf.fit(X_train, y_train)\n    for i in range(2):\n        clf.fit(X_train, y_train, refit=True)\n    for i in range(2):\n        clf.fit(X_train, y_train, refit=True, n_iter=2)\n    for i in range(2):\n        clf.fit(X_train, y_train, refit=True, n_iter=4)\n    for i in range(2):\n        clf.fit(X_train, y_train, refit=True, n_iter=8)\n    y_pred = clf.predict(X_test)\n    return y_pred, y_test, clf\n", "CodeBleu_score": 35.22261602301139}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n    import numpy as np\n    import pandas as pd\n    import time\n    import os\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    np.random.seed(0)\n    if dataset == \"iris\":\n        data = load_iris()\n        X = data.data\n        y = data.target\n    elif dataset == \"diabetes\":\n        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\", header=None)\n        X = data.iloc[:, :-1]\n        y = data.iloc[:, -1]\n    elif dataset == \"digits\":\n        from sklearn.datasets import load_digits\n        data = load_digits()\n        X = data.data\n        y = data.target\n    elif dataset == \"cancer\":\n        from sklearn.datasets import load_breast_cancer\n        data = load_breast_cancer()\n        X = data.data\n        y = data.target\n    elif dataset == \"wine\":\n        from sklearn.datasets import load_wine\n        data = load_wine()\n        X = data.data\n        y = data.target\n    elif dataset == \"boston\":\n        from sklearn.datasets import load_boston\n        data = load_boston()\n        X = data.data\n        y = data.target\n    elif dataset == \"linnerud\":\n        from sklearn.datasets import load_linnerud\n        data = load_linnerud()\n        X = data.data\n        y = data.target\n    elif dataset == \"diabetes_multivariate\":\n        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00291/diabetes_multivariate_compressed.csv.gz\", compression=\"gzip\")\n       ", "CodeBleu_score": 19.887574390705197}]}
{"instance_id": "d0d8450c4c17", "generation_result": [{"code": "def _load_models(self)\n    if self.ensemble_class is not None:\n        self._load_ensemble()\n    else:\n        self._load_best_individual()\n    if self.models:\n        self._load_model_identifiers()\n        self._load_models_from_identifiers()\n        if self.resampling_strategy == \"cv\":\n            self._load_cross_validated_models()\n        else:\n            self._cross_validated_models = None\n", "CodeBleu_score": 16.218908842655143}, {"code": "def _load_models(self)\n    self._load_ensemble()\n    if self._ensemble is not None:\n        self._model_ids = self._ensemble.model_ids\n        self._models = self._backend.load_models(self._model_ids)\n        if self._resampling_strategy == 'cv':\n            self._cross_validated_models = self._backend.load_models(self._ensemble.cross_validated_model_ids)\n        else:\n            self._cross_validated_models = None\n    else:\n        if self._evaluator_output_conditions and self._resampling_strategy == 'cv':\n            self._model_ids = [self._backend.load_best_individual_model_id()]\n            self._models = self._backend.load_models(self._model_ids)\n            self._cross_validated_models = None\n        else:\n            self._model_ids = []\n            self._models = []\n            self._cross_validated_models = []\n", "CodeBleu_score": 36.949326413045405}, {"code": "def _load_models(self)\n    if self.ensemble_class:\n        self.ensemble = self.backend.load_ensemble(\n            self.ensemble_class, self.seed)\n        self.model_identifiers = self.ensemble.model_identifiers\n        self.models = [\n            self.backend.load_model(model_identifier)\n            for model_identifier in self.model_identifiers\n        ]\n        self.cross_validated_models = None\n    else:\n        if (self.evaluator.output_conditions_met and\n                self.resampling_strategy):\n            self.ensemble = None\n            self.model_identifiers = [\n                self.evaluator.get_best_model_identifier()\n            ]\n            self.models = [\n                self.backend.load_model(self.model_identifiers[0])\n            ]\n            self.cross_validated_models = None\n        else:\n            self.ensemble = None\n            self.model_identifiers = []\n            self.models = []\n            self.cross_validated_models = []", "CodeBleu_score": 38.37847152171361}]}
{"instance_id": "ef71b1cddb50", "generation_result": [{"code": "def load_task(task_id)\n    task = openml.tasks.get_task(task_id)\n    X, y, _, _ = task.get_X_and_y(target=task.default_target_attribute)\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n    categorical_features = task.get_dataset().get_features_by_type(\"categorical\")\n    numerical_features = task.get_dataset().get_features_by_type(\"numeric\")\n    feature_types = []\n    for i in range(X.shape[1]):\n        if i in categorical_features:\n            feature_types.append(\"categorical\")\n        else:\n            feature_types.append(\"numerical\")\n    y_train = np.unique(y_train, return_inverse=True)[1]\n    y_test = np.unique(y_test, return_inverse=True)[1]\n    return X_train, X_test, y_train, y_test, feature_types\n\n", "CodeBleu_score": 31.740948604258314}, {"code": "def load_task(task_id)\n    import openml\n    task = openml.tasks.get_task(task_id)\n    X, y, _, _ = task.get_X_and_y(target=task.default_target_attribute)\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n    feature_types = task.get_dataset().get_features_by_type()\n    categorical_feature_indices = [i for i, feature_type in enumerate(feature_types) if feature_type == 'categorical']\n    numerical_feature_indices = [i for i, feature_type in enumerate(feature_types) if feature_type == 'numerical']\n    X_train_categorical, X_train_numerical = X_train[:, categorical_feature_indices], X_train[:, numerical_feature_indices]\n    X_test_categorical, X_test_numerical = X_test[:, categorical_feature_indices], X_test[:, numerical_feature_indices]\n    y_train = y_train.astype(int)\n    y_test = y_test.astype(int)\n    label_mapping = {label: i for i, label in enumerate(np.unique(y_train))}\n    y_train = np.array([label_mapping[label] for label in y_train])\n    y_test = np.array([label_mapping[label] for label in y_test])\n    return X_train_categorical, X_train_numerical, y_train, X_test_categorical, X_test_numerical, y_test, categorical_feature_indices, numerical_feature_indices\n", "CodeBleu_score": 34.789673242507966}, {"code": "def load_task(task_id)\n    import openml\n    task = openml.tasks.get_task(task_id)\n    X, y, _, _ = task.get_X_and_y(target=task.default_target_attribute)\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train, X_test = X[train_indices], X[test_indices]\n    y_train, y_test = y[train_indices], y[test_indices]\n    feature_types = task.get_dataset().get_features_as_dataframe()['feature_type']\n    is_categorical = [feature_type == 'categorical' for feature_type in feature_types]\n    label_encoder = LabelEncoder()\n    y_train = label_encoder.fit_transform(y_train)\n    y_test = label_encoder.transform(y_test)\n    return X_train, y_train, X_test, y_test, is_categorical\n", "CodeBleu_score": 24.840482778718574}]}
{"instance_id": "90bd6edbcc9e", "generation_result": [{"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if sp.issparse(y):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y\n", "CodeBleu_score": 51.99553313693461}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if isinstance(y, csr_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, csc_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, coo_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, bsr_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, bcsc_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, bcrs_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, bdia_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, dia_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, dia_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, dia_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, dia_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, dia_matrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    elif isinstance(y, dia_matrix):\n        y = y.toarray()\n       ", "CodeBleu_score": 39.26315911145984}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if isinstance(y, sp.spmatrix):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n    return y", "CodeBleu_score": 55.56702222115098}]}
{"instance_id": "5728e3d4f914", "generation_result": [{"code": "def _calculate_weights(self) -> None\n    # Count the occurrences of each ensemble member\n    counts = np.bincount(self.ensemble_indices)\n\n    # Initialize the weights array with zeros\n    weights = np.zeros(self.ensemble_size)\n\n    # Calculate the weight for each ensemble member\n    for i, count in enumerate(counts):\n        weight = count / self.ensemble_size\n        weights[i] = weight\n\n    # Normalize the weights if their sum is less than 1\n    if np.sum(weights) < 1:\n        weights /= np.sum(weights)\n\n    # Store the calculated weights\n    self.weights_ = weights", "CodeBleu_score": 35.41768200828855}, {"code": "def _calculate_weights(self) -> None\n    # Count the occurrences of each ensemble member index\n    counts = Counter(self.ensemble_member_indices)\n\n    # Initialize a weights array with zeros\n    weights = np.zeros(self.n_models)\n\n    # Calculate the weight for each ensemble member\n    for index, count in counts.items():\n        weight = count / self.n_models\n        weights[index] = weight\n\n    # Normalize the weights if their sum is less than 1\n    if sum(weights) < 1:\n        weights = weights / sum(weights)\n\n    # Store the calculated weights\n    self.weights_ = weights\n\n", "CodeBleu_score": 32.18569538489869}, {"code": "def _calculate_weights(self) -> None\n    # Count the occurrences of each ensemble member index\n    counts = np.bincount(self.ensemble_indices)\n    # Initialize a weights array with zeros, having a length equal to the number of input models\n    weights = np.zeros(self.n_models)\n    # For each ensemble member, calculate the weight as the frequency divided by the ensemble size\n    for i, count in enumerate(counts):\n        if count > 0:\n            weights[i] = count / self.n_models\n    # Normalize the weights if the sum is less than 1\n    if np.sum(weights) < 1:\n        weights /= np.sum(weights)\n    # Store the calculated weights in the instance variable self.weights_\n    self.weights_ = weights\n\n", "CodeBleu_score": 31.726107359751232}]}
{"instance_id": "4f9c4799fd8f", "generation_result": [{"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    # Call the superclass's predict_proba method to obtain the predicted probabilities\n    proba = super().predict_proba(X, batch_size, n_jobs)\n    # If the target type is not \"multilabel-indicator\", assert that the sum of probabilities for each instance is 1\n    if self.target_type_ != \"multilabel-indicator\":\n        assert np.allclose(np.sum(proba, axis=1), 1)\n    # Assert that all probability values lie between 0 and 1\n    assert np.all(proba >= 0) and np.all(proba <= 1)\n    # Return the predicted probabilities\n    return proba", "CodeBleu_score": 28.27262305185459}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    y_pred = super(MultiLabelClassifier, self).predict_proba(X, batch_size, n_jobs)\n    if self.target_type != \"multilabel-indicator\":\n        assert np.allclose(y_pred.sum(axis=1), 1)\n    assert np.all(y_pred >= 0) and np.all(y_pred <= 1)\n    return y_pred", "CodeBleu_score": 27.556051665018984}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    y_pred = super(MultiOutputClassifier, self).predict_proba(X, batch_size, n_jobs)\n    if self.target_type_ == \"multilabel-indicator\":\n        return y_pred\n    else:\n        assert np.allclose(np.sum(y_pred, axis=1), np.ones(len(y_pred)))\n        assert np.all(y_pred >= 0) and np.all(y_pred <= 1)\n        return y_pred", "CodeBleu_score": 31.60244593175997}]}
{"instance_id": "a9a13150aebe", "generation_result": [{"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    Y = np.dot(Ybin, np.arange(Ybin.shape[1]))\n    return Y\n", "CodeBleu_score": 36.324248170074}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    if Ybin.ndim == 1:\n        Ynum = Ybin\n    else:\n        Ynum = np.dot(Ybin, np.arange(Ybin.shape[1]))\n    return Ynum", "CodeBleu_score": 45.870465134800334}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    if Ybin.ndim == 1:\n        return Ybin\n    else:\n        return Ybin.dot(np.arange(Ybin.shape[1]))", "CodeBleu_score": 27.13397993791789}]}
{"instance_id": "e97d38e62f81", "generation_result": [{"code": "def fit_transformer(self, X, y, fit_params=None)\n    if fit_params is None:\n        fit_params = {}\n    if self.config.balancing_strategy == \"weighting\":\n        balancing = Balancing(\n            self.config.classifier_choice,\n            self.config.feature_preprocessor_choice,\n            self.config.feature_preprocessor_params,\n            self.config.classifier_params,\n            self.config.balancing_params,\n        )\n        weights = balancing.get_weights(X, y)\n        fit_params = {**fit_params, **weights}\n    self.config.update_init_params(fit_params)\n    self.hyperparameters = self.config.get_hyperparameters()\n    return super().fit_transformer(X, y, fit_params)\n\n", "CodeBleu_score": 48.3576380167736}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n    fit_params = fit_params or {}\n    if self.config.balancing_strategy == \"weighting\":\n        balancing = Balancing(self.config.classifier, self.config.feature_preprocessor)\n        weights = balancing.get_weights(X, y)\n        fit_params.update(weights)\n    self.init_params.update(fit_params)\n    self.hyperparams = self.config.update_hyperparams(self.init_params)\n    return super().fit_transformer(X, y, fit_params)\n", "CodeBleu_score": 33.857973269646955}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n    if fit_params is None:\n        fit_params = {}\n    if self.config.balancing_strategy == \"weighting\":\n        balancing = Balancing(self.config)\n        weights = balancing.get_weights(X, y)\n        fit_params[\"weights\"] = weights\n    self.config.update_init_params(self.init_params)\n    self.config.update_hyperparameters(self.hyperparameters)\n    fit_params = {**self.config.get_fit_params(), **fit_params}\n    return super().fit_transformer(X, y, fit_params)", "CodeBleu_score": 40.84672809921529}]}
{"instance_id": "b1aaad80a9c4", "generation_result": [{"code": "def test_glm_identity_link_func()\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.linalg import Vectors\n    from pyspark.sql.functions import col\n    from pyspark.sql.types import DoubleType\n    from pyspark.sql.session import SparkSession\n    import numpy as np\n    import pyspark.sql.functions as F\n    spark = SparkSession.builder.appName(\"GLMTest\").getOrCreate()\n    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n    data = [(1, 1.0, 1.0), (2, 2.0, 2.0), (3, 3.0, 3.0), (4, 4.0, 4.0), (5, 5.0, 5.0)]\n    df = spark.createDataFrame(data, [\"id\", \"x\", \"y\"])\n    glm = GeneralizedLinearRegression(family=\"tweedie\", link=\"identity\", maxIter=10, regParam=0.0)\n    glm_model = glm.fit(df.select(\"x\", \"y\"))\n    assembler = VectorAssembler(inputCols=[\"x\"], outputCol=\"features\")\n    assembled_df = assembler.transform(df)\n    assembled_df = assembled_df.withColumn(\"prediction\", glm_model.transform(assembled_df).select(\"prediction\"))\n    assembled_df = assembled_df.withColumn(\"prediction\", F.col(\"prediction\").cast(DoubleType()))\n    assembled_df = assembled_df.withColumn(\"expected\", F.col(\"x\"))\n    assembled_df = assembled_df.withColumn(\"error\", F.col(\"prediction\") - F.col(\"expected\"))\n    assembled_df = assembled_df.withColumn(\"squared_error\", F.col(\"error\") * F.col(\"error\"))\n    mse = assembled_df.agg(F.mean(\"squared_error\")).collect()[0][0]\n    assert np.", "CodeBleu_score": 32.89237143899573}, {"code": "def test_glm_identity_link_func()\n    # Import necessary libraries\n    import numpy as np\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    from pyspark.sql.types import DoubleType\n    from pyspark.sql.functions import col, udf\n    from pyspark.sql.session import SparkSession\n    from pyspark.ml.linalg import Vectors\n    from pyspark.ml.stat import Correlation\n    from pyspark.ml.evaluation import RegressionEvaluator\n    from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n    from pyspark.ml.evaluation import RegressionEvaluator\n    from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n    from pyspark.ml.tuning import CrossValidatorModel\n    from pyspark.ml.tuning import ParamGridBuilder\n    from pyspark.ml.tuning import CrossValidator\n    from pyspark.ml.tuning import CrossValidatorModel\n    from pyspark.ml.tuning import ParamGridBuilder\n    from pyspark.ml.tuning import CrossValidator\n    from pyspark.ml.tuning import CrossValidatorModel\n    from pyspark.ml.tuning import ParamGridBuilder\n    from pyspark.ml.tuning import CrossValidator\n    from pyspark.ml.tuning import CrossValidatorModel\n    from pyspark.ml.tuning import ParamGridBuilder\n    from pyspark.ml.tuning import CrossValidator\n    from pyspark.ml.tuning import CrossValidatorModel\n    from pyspark.ml.tuning import ParamGridBuilder\n    from pyspark.ml.tuning import CrossValidator\n    from pyspark.ml.tuning import CrossValidatorModel\n    from pyspark.ml.tuning import ParamGridBuilder\n    from pyspark.ml.tuning import CrossValidator\n    from pyspark.ml.tuning import CrossValidatorModel\n    from pyspark.ml.tuning import ParamGridBuilder\n    from pyspark.ml.tuning import CrossValidator\n    from pyspark.ml", "CodeBleu_score": 5.747723170832826}, {"code": "def test_glm_identity_link_func()\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.sql.functions import col\n    from pyspark.sql.types import DoubleType\n    from pyspark.sql.functions import udf\n    from pyspark.sql.types import ArrayType\n    from pyspark.sql.types import IntegerType\n    from pyspark.sql.types import StringType\n    from pyspark.sql.types import StructField\n    from pyspark.sql.types import StructType\n    from pyspark.sql.functions import lit\n    from pyspark.sql.functions import rand\n    from pyspark.sql.functions import when\n    from pyspark.sql.functions import count\n    from pyspark.sql.functions import sum\n    from pyspark.sql.functions import col\n    from pyspark.sql.functions import mean\n    from pyspark.sql.functions import stddev\n    from pyspark.sql.functions import max\n    from pyspark.sql.functions import min\n    from pyspark.sql.functions import corr\n    from pyspark.sql.functions import covar_pop\n    from pyspark.sql.functions import covar_samp\n    from pyspark.sql.functions import rand\n    from pyspark.sql.functions import when\n    from pyspark.sql.functions import count\n    from pyspark.sql.functions import sum\n    from pyspark.sql.functions import col\n    from pyspark.sql.functions import mean\n    from pyspark.sql.functions import stddev\n    from pyspark.sql.functions import max\n    from pyspark.sql.functions import min\n    from pyspark.sql.functions import corr\n    from pyspark.sql.functions import covar_pop\n    from pyspark.sql.functions import covar_samp\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    from pyspark.sql.functions import udf\n    from pyspark.sql.types", "CodeBleu_score": 8.181154789099912}]}
{"instance_id": "bcab63e592d0", "generation_result": [{"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    elif isinstance(agg, str):\n        agg_name = agg\n        agg_func = getattr(pd.core.groupby.generic.DataFrameGroupBy, agg)\n    else:\n        agg_name = agg.__name__\n        agg_func = agg\n    new_col_name = f\"{col.name}_{agg_name}\"\n    return col.groupby(level=0).apply(agg_func).rename(new_col_name)\n", "CodeBleu_score": 44.391252189025984}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    elif isinstance(agg, str):\n        agg_name = agg\n        agg_func = getattr(pd.core.groupby.generic.DataFrameGroupBy, agg)\n    else:\n        agg_name = agg.__name__\n        agg_func = agg\n\n    col_name = f\"{col.name}_{agg_name}\"\n\n    return col.groupby(level=0).apply(agg_func).rename(col_name)\n\n", "CodeBleu_score": 44.70840319000651}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    elif callable(agg):\n        agg_name = agg.__name__\n        agg_func = agg\n    else:\n        agg_name = agg\n        agg_func = getattr(pd.core.groupby.generic.DataFrameGroupBy, agg)\n\n    new_col_name = f\"{col.name}_{agg_name}\"\n    return col.groupby(self.group_cols).apply(agg_func).rename(new_col_name)", "CodeBleu_score": 47.639288097640694}]}
{"instance_id": "114d623dae7b", "generation_result": [{"code": "def _corrfunc(self, x, y, **kws)\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Set default values for keyword arguments\n    kws.setdefault('ax', plt.gca())\n    kws.setdefault('data', self.data)\n    kws.setdefault('hue_order', self.hue_order)\n\n    # Handle old and new keyword arguments for axes\n    if 'ax' in kws:\n        ax = kws['ax']\n    else:\n        ax = kws['axes']\n\n    # Compute correlation coefficient and adjust font size based on number of hue groups\n    corr = np.corrcoef(x, y)[0, 1]\n    font_size = max(5, 10 - 3 * len(self.hue_order))\n\n    # Retrieve appropriate axis and annotate with correlation coefficient\n    if 'ax' in kws:\n        ax.annotate(f'{corr:.2f}', xy=(0.5, 0.5), xycoords='axes fraction',\n                    ha='center', va='center', fontsize=font_size)\n    else:\n        ax.annotate(f'{corr:.2f}', xy=(0.5, 0.5), xycoords='axes fraction',\n                    ha='center', va='center', fontsize=font_size)\n\n    # Set the title of the plot to the correlation coefficient\n    ax.set_title(f'{corr:.2f}')\n\n    return ax\n\n", "CodeBleu_score": 27.048240782772755}, {"code": "def _corrfunc(self, x, y, **kws)\n    if self.hue is not None:\n        x = x.groupby(self.hue)\n        y = y.groupby(self.hue)\n    else:\n        x = x.copy()\n        y = y.copy()\n    x = x.dropna()\n    y = y.dropna()\n    if self.hue is not None:\n        x = x.reset_index(level=self.hue, drop=True)\n        y = y.reset_index(level=self.hue, drop=True)\n    corr = x.corrwith(y)\n    if self.hue is not None:\n        corr = corr.reset_index(level=self.hue, drop=True)\n    if self.ax is None:\n        self.ax = kws.pop(\"ax\", None)\n    if self.ax is None:\n        self.ax = plt.gca()\n    if self.hue is not None:\n        n_hue = len(self.hue_names)\n        if n_hue > 1:\n            fontsize = 12\n        else:\n            fontsize = 14\n    else:\n        fontsize = 14\n    if self.hue is not None:\n        for i, hue_name in enumerate(self.hue_names):\n            self.ax.annotate(f\"{corr[i]:.2f}\", xy=(0.5, 0.5), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=fontsize)\n    else:\n        self.ax.annotate(f\"{corr:.2f}\", xy=(0.5, 0.5), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=fontsize)\n    return self", "CodeBleu_score": 35.03750446324802}, {"code": "def _corrfunc(self, x, y, **kws)\n    import seaborn as sns\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    # Compute the correlation coefficient\n    corr = np.corrcoef(x, y)[0, 1]\n\n    # Determine the font size based on the number of hue groups and the correlation value\n    font_size = 12\n    if \"hue_groups\" in kws:\n        font_size -= len(kws[\"hue_groups\"]) * 2\n        if corr > 0.5:\n            font_size -= 2\n        elif corr < -0.5:\n            font_size += 2\n\n    # Get the appropriate axis\n    if \"ax\" in kws:\n        ax = kws[\"ax\"]\n    else:\n        ax = plt.gca()\n\n    # Annotate the axis with the correlation coefficient\n    ax.annotate(\n        f\"{corr:.2f}\",\n        xy=(0.5, 0.5),\n        xycoords=ax.transAxes,\n        ha=\"center\",\n        va=\"center\",\n        fontsize=font_size,\n    )\n\n", "CodeBleu_score": 24.697641760711107}]}
{"instance_id": "4c1ade308953", "generation_result": [{"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return src\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        from decimal import Decimal\n        return Decimal(str(src)).quantize(Decimal('1e-{}'.format(rounddigit)))\n    elif method == 'format':\n        return float(f'{src:.{rounddigit}g}')\n    else:\n        raise ValueError(f'Invalid method: {method}')\n\n", "CodeBleu_score": 51.82651603442221}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return src\n    if method == 'decimal':\n        return round(src, rounddigit)\n    if method == 'sig':\n        from decimal import Decimal\n        return Decimal(str(src)).quantize(Decimal(f'1e-{rounddigit}'))\n    if method == 'format':\n        return float(f'{src:.{rounddigit}f}')\n\n", "CodeBleu_score": 52.26559479582584}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return src\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        from decimal import Decimal\n        return Decimal(str(src)).quantize(Decimal('1e-{}'.format(rounddigit)))\n    elif method == 'format':\n        return float('{:.{}g}'.format(src, rounddigit))\n    else:\n        raise ValueError('Unknown method: {}'.format(method))\n\n", "CodeBleu_score": 51.49212814370079}]}
{"instance_id": "a9923e678320", "generation_result": [{"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return srcdict\n    else:\n        if method == 'decimal':\n            return {k: _round_digits(v, rounddigit) if isinstance(v, float) else v for k, v in srcdict.items()}\n        elif method == 'sig':\n            return {k: _round_sig_digits(v, rounddigit) if isinstance(v, float) else v for k, v in srcdict.items()}\n        elif method == 'format':\n            return {k: _round_format_digits(v, rounddigit) if isinstance(v, float) else v for k, v in srcdict.items()}\n        else:\n            raise ValueError(f\"Invalid rounding method: {method}\")\n", "CodeBleu_score": 30.485518694111164}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return srcdict\n    if method == 'decimal':\n        return {k: _round_digits(v, rounddigit) if isinstance(v, float) else v for k, v in srcdict.items()}\n    if method == 'sig':\n        return {k: _round_sigdigits(v, rounddigit) if isinstance(v, float) else v for k, v in srcdict.items()}\n    if method == 'format':\n        return {k: _round_format(v, rounddigit) if isinstance(v, float) else v for k, v in srcdict.items()}\n    raise ValueError(f'Invalid method: {method}')\n\n", "CodeBleu_score": 31.481580748669895}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return srcdict\n    if method == 'decimal':\n        return {k: _round_digits(v, rounddigit) for k, v in srcdict.items()}\n    elif method == 'sig':\n        return {k: _round_sig_digits(v, rounddigit) for k, v in srcdict.items()}\n    elif method == 'format':\n        return {k: _round_format_digits(v, rounddigit) for k, v in srcdict.items()}\n    else:\n        raise ValueError(f'Invalid rounding method: {method}')\n\n", "CodeBleu_score": 29.96327877538342}]}
{"instance_id": "b6106e079dc0", "generation_result": [{"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    import pandas as pd\n    import seaborn as sns\n    df = pd.DataFrame({x_name: x, y_name: y})\n    if hue_data is not None:\n        df['hue'] = hue_data\n    sns.scatterplot(data=df, x=x_name, y=y_name, hue='hue', ax=ax, **scatter_kws)\n    if legend_kws.get('title') is None:\n        legend_kws['title'] = hue_name\n    ax.legend(**legend_kws)\n", "CodeBleu_score": 37.61963860851907}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    import pandas as pd\n    import seaborn as sns\n    if hue_data is not None:\n        data = pd.DataFrame({x_name: x.flatten(), y_name: y.flatten(), hue_name: hue_data.flatten()})\n    else:\n        data = pd.DataFrame({x_name: x.flatten(), y_name: y.flatten()})\n    sns.scatterplot(data=data, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n    if legend_kws.get(\"title\") is None:\n        legend_kws[\"title\"] = hue_name\n    ax.legend(**legend_kws)", "CodeBleu_score": 45.625129003811566}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    import pandas as pd\n    import seaborn as sns\n    if hue_data is not None:\n        df = pd.DataFrame({x_name: x.flatten(), y_name: y.flatten(), hue_name: hue_data.flatten()})\n        sns.scatterplot(x=x_name, y=y_name, hue=hue_name, data=df, ax=ax, **scatter_kws)\n        if legend_kws.get('title') is None:\n            legend_kws['title'] = hue_name\n    else:\n        df = pd.DataFrame({x_name: x.flatten(), y_name: y.flatten()})\n        sns.scatterplot(x=x_name, y=y_name, data=df, ax=ax, **scatter_kws)\n    return ax", "CodeBleu_score": 45.46359862350949}]}
{"instance_id": "257e0aa5dd4d", "generation_result": [{"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    # Retrieve the main time series data\n    data = self.get_data()\n    # Check for any exogenous variables\n    exog = self.get_exog()\n    # Suppress warnings related to user actions and convergence\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        # Fit the time series model using the endogenous and exogenous data, along with any additional fitting arguments\n        self.model = self.model.fit(data, exog, **fit_args)\n    return self", "CodeBleu_score": 42.92721885909531}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        self.model = self.model.fit(\n            self.endog, exog=self.exog, **fit_args\n        )\n    return self\n", "CodeBleu_score": 23.29351085623543}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    return self._fit(**fit_args)\n\n", "CodeBleu_score": 3.6097862346295333}]}
{"instance_id": "6446de2223e8", "generation_result": [{"code": "def create_model(self, **kwargs)\n    if self.model_type == \"holt_winters\":\n        transformed_data = self.transformed_data\n        model = ExponentialSmoothing(transformed_data, seasonal=\"add\", seasonal_periods=self.seasonal_periods)\n    elif self.model_type == \"auto_arima\":\n        try:\n            from pmdarima import auto_arima\n        except ImportError:\n            logger.error(\"pmdarima is not installed. Please install it to use auto_arima model.\")\n            sys.exit(1)\n        model = auto_arima(self.transformed_data, exogenous=self.exogenous_data, **kwargs)\n    elif self.model_type == \"sarimax\":\n        transformed_data = self.transformed_data\n        if self.exogenous_data is not None:\n            model = SARIMAX(transformed_data, exog=self.exogenous_data, order=self.order, seasonal_order=self.seasonal_order)\n        else:\n            model = SARIMAX(transformed_data, order=self.order, seasonal_order=self.seasonal_order)\n    else:\n        raise ValueError(\"Invalid model type. Please use one of the following: 'holt_winters', 'auto_arima', 'sarimax'\")\n    return model", "CodeBleu_score": 39.68351548702443}, {"code": "def create_model(self, **kwargs)\n    model_type = kwargs.get('model_type', 'auto_arima')\n    if model_type == 'holt_winters':\n        transformed_data = self.transformed_data\n        model = ExponentialSmoothing(transformed_data)\n    elif model_type == 'auto_arima':\n        try:\n            from pmdarima import auto_arima\n        except ImportError:\n            logger.error('pmdarima is not installed. Please install pmdarima to use auto_arima model.')\n            sys.exit(1)\n        exogenous_vars = kwargs.get('exogenous_vars', None)\n        if exogenous_vars:\n            model = auto_arima(self.transformed_data, exogenous=exogenous_vars)\n        else:\n            model = auto_arima(self.transformed_data)\n    elif model_type == 'sarimax':\n        transformed_data = self.transformed_data\n        exogenous_vars = kwargs.get('exogenous_vars', None)\n        if exogenous_vars:\n            model = SARIMAX(transformed_data, exogenous_vars=exogenous_vars)\n        else:\n            model = SARIMAX(transformed_data)\n    else:\n        raise ValueError('Invalid model type.')\n    return model", "CodeBleu_score": 37.20534582203262}, {"code": "def create_model(self, **kwargs)\n    model_type = kwargs.get('model_type', 'holt_winters')\n    if model_type == 'holt_winters':\n        transformed_data = self.get_transformed_data()\n        model = ExponentialSmoothing(transformed_data, seasonal_periods=self.seasonal_periods, seasonal='add', trend='add', damped=False)\n    elif model_type == 'auto_arima':\n        try:\n            from pmdarima import auto_arima\n        except ImportError:\n            self.logger.error('pmdarima not installed. Please install pmdarima to use auto_arima model.')\n            sys.exit(1)\n        transformed_data = self.get_transformed_data()\n        exogenous_vars = kwargs.get('exogenous_vars', None)\n        if exogenous_vars:\n            model = auto_arima(transformed_data, exogenous=exogenous_vars, seasonal=True, m=self.seasonal_periods, trace=True, error_action='ignore', suppress_warnings=True)\n        else:\n            model = auto_arima(transformed_data, seasonal=True, m=self.seasonal_periods, trace=True, error_action='ignore', suppress_warnings=True)\n    elif model_type == 'sarimax':\n        transformed_data = self.get_transformed_data()\n        exogenous_vars = kwargs.get('exogenous_vars', None)\n        if exogenous_vars:\n            model = SARIMAX(transformed_data, exog=exogenous_vars, order=(1, 0, 0), seasonal_order=(1, 0, 0, self.seasonal_periods))\n        else:\n            model = SARIMAX(transformed_data, order=(1, 0, 0), seasonal_order=(1, 0, 0, self.seasonal_periods))\n    else:\n        raise ValueError('Invalid model type. Please choose from: holt_winters", "CodeBleu_score": 38.48097474280549}]}
{"instance_id": "b2f67820e27f", "generation_result": [{"code": "def _to_chx(array)\n    import cupy\n    import numpy\n    import chainerx\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    if isinstance(array, cupy.ndarray) and cupy:\n        return _from_cupy(array)\n    return array\n", "CodeBleu_score": 69.99220211698479}, {"code": "def _to_chx(array)\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    else:\n        return array", "CodeBleu_score": 89.59878239425522}, {"code": "def _to_chx(array)\n    if isinstance(array, np.ndarray):\n        return _from_numpy(array)\n    elif cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    else:\n        return array", "CodeBleu_score": 85.75562864977404}]}
{"instance_id": "b96f7d9bcc95", "generation_result": [{"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    if len(ys) == 0:\n        return ys\n    if len(ys) == 1:\n        return ys\n    if len(ys) == 2:\n        if ys[0].ndim == x.ndim:\n            return ys\n        else:\n            x_shape = x.shape\n            y_shape = ys[0].shape\n            y_shape = list(y_shape)\n            y_shape[axis] = x_shape[axis] - y_shape[axis]\n            y_shape = tuple(y_shape)\n            ys[0] = ys[0].reshape(y_shape)\n            return ys\n    if len(ys) > 2:\n        if ys[0].ndim == x.ndim:\n            return ys\n        else:\n            x_shape = x.shape\n            y_shape = ys[0].shape\n            y_shape = list(y_shape)\n            y_shape[axis] = x_shape[axis] - y_shape[axis]\n            y_shape = tuple(y_shape)\n            ys[0] = ys[0].reshape(y_shape)\n            return ys\n\n", "CodeBleu_score": 28.608019003210895}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    if len(ys) == 0:\n        return ys\n    if len(ys) == 1:\n        return ys\n    if len(ys) != indices_or_sections + 1:\n        raise ValueError(\"indices_or_sections must have length \"\n                         \"len(ys) - 1\")\n    if isinstance(indices_or_sections, int):\n        indices_or_sections = [indices_or_sections]\n    if not isinstance(indices_or_sections, (list, tuple)):\n        raise ValueError(\"indices_or_sections must be a list or tuple\")\n    if any(not isinstance(i, int) for i in indices_or_sections):\n        raise ValueError(\"indices_or_sections must be a list of integers\")\n    if any(i < 0 for i in indices_or_sections):\n        raise ValueError(\"indices_or_sections must be non-negative\")\n    if any(i >= x.shape[axis] for i in indices_or_sections):\n        raise ValueError(\"indices_or_sections must be less than the size of \"\n                         \"the axis being split\")\n    if sum(indices_or_sections) != x.shape[axis]:\n        raise ValueError(\"indices_or_sections must sum to the size of the \"\n                         \"axis being split\")\n    if axis < 0:\n        axis += x.ndim\n    if axis >= x.ndim:\n        raise ValueError(\"axis must be less than the number of dimensions \"\n                         \"of x\")\n    if len(ys) == 1:\n        return ys\n    if len(ys) != indices_or_sections + 1:\n        raise ValueError(\"indices_or_sections must have length \"\n                         \"len(ys) - 1\")\n    if isinstance(indices_or_sections, int):\n        indices_or_sections = [indices_or_sections]\n    if not isinstance(indices_or_sections, (list, tuple)):\n        raise ValueError(\"indices_or_sections must be a list or tuple\")\n    if any(not isinstance(i, int) for i in indices_or", "CodeBleu_score": 22.407980492740087}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    if len(ys) > 1:\n        if not all(\n            [\n                len(y.shape) == len(x.shape)\n                for y in ys\n                if y is not None\n            ]\n        ):\n            split_sizes = np.array_split(\n                np.empty(x.shape), indices_or_sections, axis\n            )\n            for i, y in enumerate(ys):\n                if y is not None:\n                    expected_shape = list(x.shape)\n                    expected_shape[axis] = split_sizes[i].size\n                    ys[i] = y.reshape(expected_shape)\n    return ys\n\n", "CodeBleu_score": 35.54197827926119}]}
{"instance_id": "cf39fb123842", "generation_result": [{"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    if device < 0:\n        return [chainer.dataset.to_device(device, x) for x in batch]\n    if isinstance(batch[0], np.ndarray):\n        xp = np\n    elif isinstance(batch[0], chainer.cuda.cupy.ndarray):\n        xp = chainer.cuda.cupy\n    else:\n        raise ValueError('Unsupported type: {}'.format(type(batch[0])))\n    x = xp.concatenate([x for x in batch], axis=0)\n    sections = [x.shape[0] for x in batch]\n    return xp.split(xp.asarray(xp.asarray(x, device=device)), sections, axis=0)", "CodeBleu_score": 47.16691944508107}, {"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    elif device < 0:\n        return [to_device(x, device) for x in batch]\n    else:\n        xp = batch[0].__class__\n        x = xp.concatenate([xp.asarray(x) for x in batch], axis=0)\n        sections = [x.shape[0] for x in batch]\n        x = xp.asarray(x, device=device)\n        return xp.split(x, sections, axis=0)\n", "CodeBleu_score": 45.51856409530869}, {"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    if device < 0:\n        return [to_device(device, b) for b in batch]\n    xp = chainer.cuda.get_array_module(batch[0])\n    x = xp.concatenate([b[0] for b in batch])\n    t = xp.concatenate([b[1] for b in batch])\n    if device >= 0:\n        x = chainer.cuda.to_gpu(x, device=device)\n        t = chainer.cuda.to_gpu(t, device=device)\n    return [(x[i:i + len(b[0])], t[i:i + len(b[1])]) for i, b in enumerate(batch)]", "CodeBleu_score": 45.740920154642936}]}
{"instance_id": "845c744d897c", "generation_result": [{"code": "def make_image(trainer)\n    import torch\n    import numpy as np\n    from PIL import Image\n    import os\n    import matplotlib.pyplot as plt\n\n    # Set a random seed for reproducibility\n    torch.manual_seed(0)\n\n    # Determine the number of images to generate\n    num_images = 16\n\n    # Create a latent variable z\n    z = torch.randn(num_images, trainer.z_dim, 1, 1)\n\n    # Generate images using the generator model\n    with torch.no_grad():\n        trainer.generator.eval()\n        generated_images = trainer.generator(z)\n        generated_images = generated_images.detach().cpu().numpy()\n\n    # Scale the generated images to [0, 255] range and convert to uint8 type\n    generated_images = (generated_images + 1) / 2 * 255\n    generated_images = np.clip(generated_images, 0, 255).astype(np.uint8)\n\n    # Reshape and transpose the generated images to form a grid of images\n    generated_images = generated_images.reshape(num_images, trainer.image_size, trainer.image_size)\n    generated_images = generated_images.transpose(0, 2, 1, 3)\n    generated_images = generated_images.reshape(trainer.image_size, num_images * trainer.image_size)\n\n    # Create a directory for saving the preview image if it doesn't exist\n    preview_dir = os.path.join(trainer.checkpoint_dir, 'preview')\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n\n    # Save the generated image as a PNG file\n    preview_file = os.path.join(preview_dir, f'preview_{trainer.iteration}.png')\n    plt.imsave(preview_file, generated_images, cmap='gray')\n\n    # Print a message indicating that the image was saved\n    print(f", "CodeBleu_score": 35.551217328016826}, {"code": "def make_image(trainer)\n    trainer.model_g.eval()\n    with torch.no_grad():\n        torch.manual_seed(0)\n        if trainer.config.num_preview_imgs == 0:\n            return\n        z = torch.randn(trainer.config.num_preview_imgs, trainer.config.latent_dim, 1, 1, device=trainer.device)\n        imgs = trainer.model_g(z)\n        imgs = imgs.cpu().numpy()\n        imgs = (imgs + 1) / 2 * 255\n        imgs = imgs.astype(np.uint8)\n        imgs = imgs.reshape(-1, 3, trainer.config.image_size, trainer.config.image_size)\n        imgs = imgs.transpose(0, 2, 3, 1)\n        preview_dir = os.path.join(trainer.config.output_dir, 'preview')\n        if not os.path.exists(preview_dir):\n            os.makedirs(preview_dir)\n        for i, img in enumerate(imgs):\n            img = Image.fromarray(img)\n            img.save(os.path.join(preview_dir, f'{i}.png'))\n        trainer.model_g.train()", "CodeBleu_score": 37.31616073532402}, {"code": "def make_image(trainer)\n    trainer.model.train(False)\n    trainer.model.eval()\n    seed = random.randint(0, 2**32 - 1)\n    torch.manual_seed(seed)\n    num_images = 10\n    z = torch.randn(num_images, trainer.model.latent_dim, 1, 1, device=trainer.device)\n    with torch.no_grad():\n        generated_images = trainer.model.generator(z)\n    generated_images = generated_images.cpu().numpy()\n    generated_images = (generated_images + 1) / 2\n    generated_images = (generated_images * 255).astype(np.uint8)\n    generated_images = np.transpose(generated_images, (0, 2, 3, 1))\n    generated_images = generated_images.reshape((num_images, trainer.model.image_size, trainer.model.image_size))\n    preview_dir = os.path.join(trainer.output_dir, \"preview\")\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    preview_file = os.path.join(preview_dir, \"preview_{}.png\".format(trainer.iteration))\n    imageio.imwrite(preview_file, generated_images)\n", "CodeBleu_score": 38.38949645049424}]}
{"instance_id": "fc2162d43aee", "generation_result": [{"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    _typechars = \"?bBhHiIlLqQefd\"\n    return _typechars.find(_np.dtype(from_dtype).char) <= _typechars.find(_np.dtype(to_dtype).char)", "CodeBleu_score": 41.20024473995073}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    _typechars = \"?biufFcCmMhHlLqQ\"\n    from_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    from_char = _typechars.find(from_dtype.char)\n    to_char = _typechars.find(to_dtype.char)\n    return from_char >= 0 and to_char >= 0 and from_char <= to_char\n", "CodeBleu_score": 45.36261079477051}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    from_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    _typechars = \"?biufFcCDOQgGhHmM\"\n    from_pos = _typechars.find(from_dtype.char)\n    to_pos = _typechars.find(to_dtype.char)\n    return from_pos <= to_pos\n", "CodeBleu_score": 46.00124086146131}]}
{"instance_id": "5b8e2d5e6e12", "generation_result": [{"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n    if g is None:\n        ax = plt.gca()\n    else:\n        ax = g.ax\n    if len(labels) != len(colors):\n        raise ValueError(\"Length of labels and colors must be the same\")\n    handles = [patches.Patch(color=colors[i], label=labels[i]) for i in range(len(labels))]\n    return ax.legend(handles=handles, **legend_kwargs)\n", "CodeBleu_score": 53.09067200206433}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n    import numpy as np\n    ax = g.ax if hasattr(g, 'ax') else g\n    if ax is None:\n        ax = plt.gca()\n    if len(labels) != len(colors):\n        raise ValueError('Length of labels and colors must be the same.')\n    handles = [patches.Patch(color=colors[i], label=labels[i]) for i in range(len(labels))]\n    return ax.legend(handles=handles, **legend_kwargs)\n", "CodeBleu_score": 50.090821976404584}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as mpatches\n    if g is None:\n        g = plt.gca()\n    elif isinstance(g, plt.Axes):\n        pass\n    else:\n        raise ValueError(\"g must be a matplotlib Axes object or None\")\n    if len(labels) != len(colors):\n        raise ValueError(\"Length of labels must match length of colors\")\n    handles = [mpatches.Patch(color=colors[i], label=labels[i]) for i in range(len(labels))]\n    return g.legend(handles=handles, **legend_kwargs)", "CodeBleu_score": 53.03129203820396}]}
{"instance_id": "494e7f719eb6", "generation_result": [{"code": "def pcoords_time(X, y, fast=True)\n    import matplotlib.pyplot as plt\n    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n    from sklearn.datasets import make_classification\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.decomposition import PCA\n    from sklearn.manifold import TSNE\n    from sklearn.pipeline import Pipeline\n    from sklearn.svm import SVC\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.metrics import classification_report\n    from sklearn.datasets import make_classification\n    from sklearn.model_selection import train_test_split\n    from skallel.visualizers import ParallelCoordinates\n    import time\n\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Create a ParallelCoordinates visualizer with the specified speed setting\n    pcoords = ParallelCoordinates(ax, speed=fast)\n\n    # Record the start time\n    start_time = time.time()\n\n    # Fit and transform the data using the visualizer\n    pcoords.fit_transform(X, y)\n\n    # Calculate the elapsed time\n    elapsed_time = time.time() - start_time\n\n    # Clear and close the current plot\n    plt.clf()\n    plt.close(fig)\n\n    return elapsed_time\n\n", "CodeBleu_score": 41.31060363826995}, {"code": "def pcoords_time(X, y, fast=True)\n    import time\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from yellowbrick.visualizer import ParallelCoordinates\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    plt.figure()\n    visualizer = ParallelCoordinates(labels=['sepal length', 'sepal width', 'petal length', 'petal width'],\n                                     classes=['setosa', 'versicolor', 'virginica'],\n                                     support_points=True,\n                                     show_points=True,\n                                     speed=fast)\n    start_time = time.time()\n    visualizer.fit(X_train, y_train)\n    visualizer.transform(X_test)\n    elapsed_time = time.time() - start_time\n    plt.close()\n    return elapsed_time\n\n", "CodeBleu_score": 39.264150295834014}, {"code": "def pcoords_time(X, y, fast=True)\n    import matplotlib.pyplot as plt\n    from time import time\n    from sklearn.datasets import make_classification\n    from parcoords import ParallelCoordinates\n\n    if fast:\n        speed = 'fast'\n    else:\n        speed = 'slow'\n\n    fig, ax = plt.subplots(figsize=(12, 8))\n    pcoords = ParallelCoordinates(ax, speed=speed)\n\n    start = time()\n    pcoords.fit(X, y)\n    pcoords.transform(X)\n    elapsed_time = time() - start\n\n    plt.close(fig)\n\n    return elapsed_time\n\n", "CodeBleu_score": 37.757049246703986}]}
{"instance_id": "db4143bc029f", "generation_result": [{"code": "def draw_instances(self, X, y, **kwargs):\n    # Retrieve alpha value from object attributes or default to 0.25\n    alpha = self.alpha if hasattr(self, 'alpha') else 0.25\n\n    # Initialize axis object\n    ax = plt.gca()\n\n    # Iterate over instances\n    for i, (x, yi) in enumerate(zip(X, y)):\n        # Extract feature vector and label\n        xi = x.reshape(self.n_features_, 1)\n        yi = yi.reshape(1, 1)\n\n        # Determine color based on label\n        if yi == 0:\n            color = 'red'\n        elif yi == 1:\n            color = 'blue'\n        else:\n            color = 'green'\n\n        # Plot feature vector against predefined increment sequence\n        ax.plot(xi, np.arange(i, i + xi.shape[0]), color=color, alpha=alpha)\n\n    # Return axis object\n    return ax", "CodeBleu_score": 25.98409125369065}, {"code": "def draw_instances(self, X, y, **kwargs):\n    # Retrieve the alpha transparency value from the object's attributes or defaults to 0.25\n    alpha = self.alpha if hasattr(self, 'alpha') else 0.25\n\n    # Initialize the axis object\n    ax = kwargs.pop('ax', None)\n\n    # If no axis object is provided, create a new one\n    if ax is None:\n        ax = plt.gca()\n\n    # Iterate over each instance and plot it on the axis\n    for i, (x, yi) in enumerate(zip(X, y)):\n        color = self.colors[yi]\n        ax.plot(x, yi, 'o', alpha=alpha, color=color)\n\n    # Return the axis object with the plotted instances\n    return ax", "CodeBleu_score": 33.34723603880696}, {"code": "def draw_instances(self, X, y, **kwargs):\n    alpha = self.alpha if hasattr(self, 'alpha') else 0.25\n    colors = ['b', 'r', 'g', 'm', 'c', 'y', 'k', 'w']\n    markers = ['o', 's', '^', 'v', 'p', '*', '+', 'x']\n    for label, color, marker in zip(np.unique(y), colors, markers):\n        idx = y == label\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, marker=marker, alpha=alpha, label=label)\n    return plt.gca()", "CodeBleu_score": 30.675288562028957}]}
{"instance_id": "676433312ffe", "generation_result": [{"code": "def draw_classes(self, X, y, **kwargs):\n    alpha = kwargs.pop('alpha', self.alpha)\n    X = np.c_[np.ones(X.shape[0]), X]\n    increments = [None] * (X.shape[0] + 1)\n    classes = np.unique(y)\n    for c in classes:\n        color = kwargs.pop('color', self.colors[c])\n        idx = y == c\n        self.ax.plot(X[idx, 1], X[idx, 2], color=color, alpha=alpha)\n        self.ax.plot(X[idx, 1], X[idx, 2], 'o', color=color, alpha=alpha)\n    return self.ax", "CodeBleu_score": 32.38564650917788}, {"code": "def draw_classes(self, X, y, **kwargs):\n    alpha = kwargs.get('alpha', self.alpha)\n    X = np.c_[np.ones(X.shape[0]), X]\n    increments = [None] * (X.shape[0] + 1)\n    classes = np.unique(y)\n    for c in classes:\n        color = self.colors[c]\n        indices = np.where(y == c)[0]\n        instances = X[indices]\n        self.axis.plot(instances[:, 0], instances[:, 1], color=color, alpha=alpha)\n        self.axis.plot(instances[:, 0], instances[:, 1], 'o', color=color, alpha=alpha)\n    return self.axis\n", "CodeBleu_score": 35.317063940006356}, {"code": "def draw_classes(self, X, y, **kwargs):\n    alpha = kwargs.get('alpha', self.alpha)\n    X = np.c_[np.ones(X.shape[0]), X]\n    increments = [None] * (X.shape[0] + 1)\n    for i, label in enumerate(np.unique(y)):\n        color = self.colors[i % len(self.colors)]\n        indices = y == label\n        self.ax.plot(X[indices, 0], X[indices, 1], color=color, alpha=alpha)\n        self.ax.plot(X[indices, 0], X[indices, 1], color=color, alpha=alpha)\n    return self.ax", "CodeBleu_score": 31.44428791696313}]}
{"instance_id": "9d297b12a673", "generation_result": [{"code": "def feature_analysis(fname=\"feature_analysis.png\")\n    # Import necessary libraries\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.backends.backend_pdf import PdfPages\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.decomposition import PCA\n    from yellowbrick.features import RadViz\n    from yellowbrick.features import Rank2D\n    from yellowbrick.features import ParallelCoordinates\n\n    # Load the occupancy data\n    occupancy_data = pd.read_csv('occupancy_data.csv')\n\n    # Create a standard scaler object\n    scaler = StandardScaler()\n\n    # Fit the scaler to the occupancy data and transform the data\n    scaled_occupancy_data = scaler.fit_transform(occupancy_data)\n\n    # Create a PCA object with 2 components\n    pca = PCA(n_components=2)\n\n    # Fit the PCA object to the scaled occupancy data and transform the data\n    pca_occupancy_data = pca.fit_transform(scaled_occupancy_data)\n\n    # Load the concrete data\n    concrete_data = pd.read_csv('concrete_data.csv')\n\n    # Create a standard scaler object\n    scaler = StandardScaler()\n\n    # Fit the scaler to the concrete data and transform the data\n    scaled_concrete_data = scaler.fit_transform(concrete_data)\n\n    # Create a PCA object with 2 components\n    pca = PCA(n_components=2)\n\n    # Fit the PCA object to the scaled concrete data and transform the data\n    pca_concrete_data = pca.fit_transform(scaled_concrete_data)\n\n    # Set up a side-by-side axes grid with a specified figure size\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Perform RadViz analysis on the occupancy data and fit the visualization\n    occupancy_radviz = RadViz(ax", "CodeBleu_score": 29.093227856599015}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n    import pandas as pd\n    from matplotlib import pyplot as plt\n    from pandas.plotting import radviz\n    from pandas.plotting import parallel_coordinates\n    from pandas.plotting import andrews_curves\n    from pandas.plotting import scatter_matrix\n    from pandas.plotting import parallel_coordinates\n    from pandas.plotting import radviz\n    from pandas.plotting import andrews_curves\n    from pandas.plotting import scatter_matrix\n    from pandas.plotting import parallel_coordinates\n    from pandas.plotting import radviz\n    from pandas.plotting import andrews_curves\n    from pandas.plotting import scatter_matrix\n    from pandas.plotting import parallel_coordinates\n    from pandas.plotting import radviz\n    from pandas.plotting import andrews_curves\n    from pandas.plotting import scatter_matrix\n    from pandas.plotting import parallel_coordinates\n    from pandas.plotting import radviz\n    from pandas.plotting import andrews_curves\n    from pandas.plotting import scatter_matrix\n    from pandas.plotting import parallel_coordinates\n    from pandas.plotting import radviz\n    from pandas.plotting import andrews_curves\n    from pandas.plotting import scatter_matrix\n    from pandas.plotting import parallel_coordinates\n    from pandas.plotting import radviz\n    from pandas.plotting import andrews_curves\n    from pandas.plotting import scatter_matrix\n    from pandas.plotting import parallel_coordinates\n    from pandas.plotting import radviz\n    from pandas.plotting import andrews_curves\n    from pandas.plotting import scatter_matrix\n    from pandas.plotting import parallel_coordinates\n    from pandas.plotting import radviz\n    from pandas.plotting import andrews_curves\n    from pandas.plotting import scatter_matrix\n    from pandas.plotting import parallel_coordinates\n   ", "CodeBleu_score": 5.358765379801976}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n    # Import necessary libraries\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from pandas.plotting import radviz\n    from yellowbrick.features import Rank2D\n\n    # Load the occupancy data\n    occupancy = pd.read_csv(\"occupancy.csv\")\n\n    # Load the concrete data\n    concrete = pd.read_csv(\"concrete.csv\")\n\n    # Set up the figure and axes grid\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Perform RadViz analysis on the occupancy data\n    radviz(occupancy, \"Occupancy\", ax=ax1)\n    ax1.set_title(\"RadViz Analysis of Occupancy Data\")\n\n    # Perform Rank2D analysis on the concrete data\n    visualizer = Rank2D(algorithm=\"pearson\", ax=ax2)\n    visualizer.fit(concrete)\n    visualizer.transform(concrete)\n    visualizer.finalize()\n    ax2.set_title(\"Rank2D Analysis of Concrete Data\")\n\n    # Save the figure to a file\n    plt.savefig(fname)\n\n", "CodeBleu_score": 29.319175922662495}]}
{"instance_id": "fd1b4b9cd7ef", "generation_result": [{"code": "def regression(fname=\"regression.png\")\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    from sklearn.linear_model import LassoCV, RidgeCV\n    from sklearn.model_selection import train_test_split\n\n    # Load the concrete dataset\n    concrete = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/concrete.csv\")\n\n    # Split the dataset into training and testing sets\n    X = concrete.drop(\"CompressiveStrength\", axis=1)\n    y = concrete[\"CompressiveStrength\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Define a range of alpha values for regularization\n    alphas = np.logspace(-4, 4, 100)\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n\n    # Plot the prediction error using LassoCV with the specified alphas\n    lasso = LassoCV(alphas=alphas, cv=5)\n    lasso.fit(X_train, y_train)\n    ax1.plot(alphas, np.abs(lasso.mse_path_ - lasso.mse_path_.mean(axis=-1)[:, np.newaxis]), 'k', label='LassoCV')\n    ax1.axhline(y=lasso.alpha_, linestyle='--', color='k', label='alpha: CV estimate')\n    ax1.axhline(y=lasso.alpha_ + 1, linestyle='--', color='k')\n    ax1.axhline(y=lasso.alpha_ - 1, linestyle='--', color='k')\n    ax1.set_xscale('log')\n    ax1.set_xlabel('alpha')\n    ax1.set_ylabel('Mean squ", "CodeBleu_score": 30.97728009638294}, {"code": "def regression(fname=\"regression.png\")\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.linear_model import LassoCV, RidgeCV\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error\n\n    # Load the concrete dataset\n    concrete = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/concrete.csv\")\n\n    # Split the dataset into training and testing sets\n    X = concrete.drop(\"CompressiveStrength\", axis=1)\n    y = concrete[\"CompressiveStrength\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Define the range of alpha values for regularization\n    alphas = np.logspace(-4, 4, 100)\n\n    # Fit and score the LassoCV model\n    lasso = LassoCV(alphas=alphas, cv=5)\n    lasso.fit(X_train, y_train)\n    lasso_pred = lasso.predict(X_test)\n    lasso_mse = mean_squared_error(y_test, lasso_pred)\n    lasso_rmse = np.sqrt(lasso_mse)\n\n    # Fit and score the RidgeCV model\n    ridge = RidgeCV(alphas=alphas, cv=5)\n    ridge.fit(X_train, y_train)\n    ridge_pred = ridge.predict(X_test)\n    ridge_mse = mean_squared_error(y_test, ridge_pred)\n    ridge_rmse = np.sqrt(ridge_mse)\n\n    # Create a figure with two subplots\n    fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n\n    # Plot the prediction error using LassoCV with the specified alphas\n    ax[0].plot(", "CodeBleu_score": 32.2920590101728}, {"code": "def regression(fname=\"regression.png\")\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    from sklearn.linear_model import LassoCV, RidgeCV\n    from sklearn.metrics import mean_squared_error\n    from sklearn.model_selection import train_test_split\n\n    # Load the concrete dataset\n    concrete = pd.read_csv(\"concrete.csv\")\n\n    # Split the dataset into training and testing sets\n    X = concrete.drop(\"strength\", axis=1)\n    y = concrete[\"strength\"]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Define the range of alpha values for regularization\n    alphas = np.logspace(-4, 4, 100)\n\n    # Fit and score the LassoCV model on the training and testing data\n    lasso = LassoCV(alphas=alphas, cv=5, max_iter=10000)\n    lasso.fit(X_train, y_train)\n    lasso_train_error = mean_squared_error(y_train, lasso.predict(X_train))\n    lasso_test_error = mean_squared_error(y_test, lasso.predict(X_test))\n\n    # Fit and score the RidgeCV model on the training and testing data\n    ridge = RidgeCV(alphas=alphas, cv=5)\n    ridge.fit(X_train, y_train)\n    ridge_train_error = mean_squared_error(y_train, ridge.predict(X_train))\n    ridge_test_error = mean_squared_error(y_test, ridge.predict(X_test))\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n\n    # Plot the prediction error using LassoCV with the specified alphas\n    ax1.plot", "CodeBleu_score": 31.06216397453334}]}
{"instance_id": "627126918f88", "generation_result": [{"code": "def classification(fname=\"classification.png\")\n    import matplotlib.pyplot as plt\n    from sklearn.datasets import load_svmlight_files\n    from sklearn.model_selection import train_test_split\n    from sklearn.naive_bayes import MultinomialNB\n    from yellowbrick.classifier import ClassificationReport\n    from yellowbrick.classifier import DiscriminationThreshold\n\n    # Load the classification dataset\n    X_train, X_test, y_train, y_test = load_svmlight_files(\n        (\"spam/spam_train.txt\", \"spam/spam_test.txt\")\n    )\n\n    # Create a new figure and axes\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 6))\n\n    # Load the classification dataset\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_train, y_train, test_size=0.2, random_state=42\n    )\n\n    # Create a Multinomial Naive Bayes model and fit it to the training data\n    model = MultinomialNB()\n    model.fit(X_train, y_train)\n\n    # Create a ClassificationReport visualizer and fit it to the model and data\n    visualizer = ClassificationReport(model, classes=[\"ham\", \"spam\"])\n    visualizer.fit(X_train, y_train)\n\n    # Draw the ClassificationReport on the left axis\n    visualizer.show(ax=axes[0])\n\n    # Load the entire spam dataset\n    X_train, y_train = load_svmlight_files((\"spam/spam_all.txt\",))\n\n    # Create a LogisticRegression model and fit it to the training data\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n\n    # Create a DiscriminationThreshold visualizer and fit it to the model and data\n    visualizer = DiscriminationThreshold(model)\n    visualizer.fit(X_train, y", "CodeBleu_score": 36.25496304852318}, {"code": "def classification(fname=\"classification.png\")\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn.feature_extraction.text import CountVectorizer\n    from sklearn.naive_bayes import MultinomialNB\n    from yellowbrick.classifier import ClassificationReport\n    from yellowbrick.classifier import DiscriminationThreshold\n    from yellowbrick.style import set_palette\n    set_palette('bright')\n\n    # Load the spam dataset\n    spam = pd.read_csv(\"spam.csv\")\n\n    # Create the train and test data\n    X_train, X_test, y_train, y_test = train_test_split(\n        spam.text, spam.label, test_size=0.2, random_state=42)\n\n    # Create the CountVectorizer character n-grams\n    vec = CountVectorizer(ngram_range=(1, 3))\n    X_train = vec.fit_transform(X_train)\n    X_test = vec.transform(X_test)\n\n    # Create the classifier and visualizer\n    model = MultinomialNB()\n    visualizer = ClassificationReport(\n        model, classes=[\"ham\", \"spam\"], support=True)\n\n    # Fit and score the visualizer\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n\n    # Create the visualizer for the DiscriminationThreshold\n    visualizer = DiscriminationThreshold(\n        model, size=(800, 400), threshold=0.5)\n\n    # Fit and score the visualizer\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n\n    # Create the figure and grid\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Plot the classification report\n    visualizer.show(ax=ax1)", "CodeBleu_score": 32.60152947646753}, {"code": "def classification(fname=\"classification.png\")\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.datasets import fetch_openml\n    from sklearn.model_selection import train_test_split\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.linear_model import LogisticRegression\n    from yellowbrick.classifier import ClassificationReport, DiscriminationThreshold\n\n    # Create figure and axes\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Load spam dataset\n    data = fetch_openml('spam', version=1, as_frame=True)\n    X, y = data.data, data.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Create MultinomialNB classifier and fit to training data\n    nb = MultinomialNB()\n    nb.fit(X_train, y_train)\n\n    # Create ClassificationReport visualizer and fit to training data\n    viz1 = ClassificationReport(nb, ax=axes[0])\n    viz1.fit(X_train, y_train)\n    viz1.score(X_test, y_test)\n\n    # Create LogisticRegression model and fit to entire dataset\n    lr = LogisticRegression(max_iter=1000)\n    lr.fit(X, y)\n\n    # Create DiscriminationThreshold visualizer and fit to entire dataset\n    viz2 = DiscriminationThreshold(lr, ax=axes[1])\n    viz2.fit(X, y)\n\n    # Finalize visualizations and adjust layout\n    viz1.finalize()\n    viz2.finalize()\n    fig.tight_layout()\n\n    # Save figure to file\n    plt.savefig(fname)\n\n    # Display figure\n    plt.show()\n\n", "CodeBleu_score": 38.194193366766044}]}
{"instance_id": "318d3a674694", "generation_result": [{"code": "def get_color_cycle()\n    from matplotlib.pyplot import gca\n    from matplotlib.colors import to_rgba\n    from matplotlib import __version__\n    if __version__ >= '1.5.0':\n        try:\n            return [to_rgba(color) for color in gca().get_prop_cycle().by_key()['color']]\n        except KeyError:\n            return [to_rgba(color) for color in gca().get_prop_cycle().by_key()['color']]\n    else:\n        return [to_rgba(color) for color in gca().get_color_cycle()]", "CodeBleu_score": 33.82169533541045}, {"code": "def get_color_cycle()\n    try:\n        from matplotlib.axes._axes import _base\n        if hasattr(_base, \"get_color_cycle\"):\n            return _base.get_color_cycle()\n    except:\n        pass\n    try:\n        from matplotlib.axes._axes import _AxesBase\n        if hasattr(_AxesBase, \"get_color_cycle\"):\n            return _AxesBase.get_color_cycle()\n    except:\n        pass\n    try:\n        from matplotlib.axes._base import _AxesBase\n        if hasattr(_AxesBase, \"get_color_cycle\"):\n            return _AxesBase.get_color_cycle()\n    except:\n        pass\n    try:\n        from matplotlib.axes._base import _BaseAxes\n        if hasattr(_BaseAxes, \"get_color_cycle\"):\n            return _BaseAxes.get_color_cycle()\n    except:\n        pass\n    try:\n        from matplotlib.axes._base import _BasePseudoAxes\n        if hasattr(_BasePseudoAxes, \"get_color_cycle\"):\n            return _BasePseudoAxes.get_color_cycle()\n    except:\n        pass\n    try:\n        from matplotlib.axes._base import _BaseAxes\n        if hasattr(_BaseAxes, \"get_color_cycle\"):\n            return _BaseAxes.get_color_cycle()\n    except:\n        pass\n    try:\n        from matplotlib.axes._base import _BasePseudoAxes\n        if hasattr(_BasePseudoAxes, \"get_color_cycle\"):\n            return _BasePseudoAxes.get_color_cycle()\n    except:\n        pass\n    try:\n        from matplotlib.axes._base import _BaseAxes\n        if hasattr(_BaseAxes, \"get_color_cycle\"):\n            return _BaseAxes.get_color_cycle()\n    except:\n        pass\n    try:\n        from matplotlib.axes._base import _BasePseudoAxes\n        if", "CodeBleu_score": 33.02239787570272}, {"code": "def get_color_cycle()\n    try:\n        import matplotlib\n        if matplotlib.__version__ >= \"1.5.0\":\n            from matplotlib import rcParams\n            return rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n        else:\n            from matplotlib import rcParams\n            return rcParams[\"axes.color_cycle\"]\n    except KeyError:\n        return None\n", "CodeBleu_score": 28.730058096973178}]}
{"instance_id": "f3fac557334c", "generation_result": [{"code": "def plot_knee_normalized(\n        self,\n    )\n        import matplotlib.pyplot as plt\n\n        fig, ax = plt.subplots(figsize=(8, 8))\n\n        ax.plot(self.x_normalized, self.y_normalized)\n        ax.plot(self.x_distance, self.y_distance, color=\"red\")\n\n        ax.set_xlabel(\"Normalized x\")\n        ax.set_ylabel(\"Normalized y\")\n\n        ax.set_xticks(\n            np.arange(\n                min(self.x_normalized),\n                max(self.x_normalized) + 0.1,\n                0.1,\n            )\n        )\n        ax.set_yticks(\n            np.arange(\n                min(self.y_normalized),\n                max(self.y_normalized) + 0.1,\n                0.1,\n            )\n        )\n\n        ax.grid(True)\n\n        if self.knee_point is not None:\n            ax.axvline(\n                x=self.knee_point[0],\n                color=\"black\",\n                linestyle=\"--\",\n            )\n            ax.axhline(\n                y=self.knee_point[1],\n                color=\"black\",\n                linestyle=\"--\",\n            )\n\n        plt.show()", "CodeBleu_score": 32.41621520501127}, {"code": "def plot_knee_normalized(\n        self,\n    )\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(8, 8))\n    plt.plot(self.normalized_x, self.normalized_y, label=\"Normalized\")\n    plt.plot(self.distance_x, self.distance_y, color=\"red\", label=\"Distance\")\n    plt.xlabel(\"Normalized X\")\n    plt.ylabel(\"Normalized Y\")\n    plt.xticks(np.arange(min(self.normalized_x), max(self.normalized_x)+1, 0.1))\n    plt.yticks(np.arange(min(self.normalized_y), max(self.normalized_y)+1, 0.1))\n    if self.knee_point is not None:\n        plt.axvline(x=self.knee_point[0], color=\"black\", linestyle=\"--\", label=\"Knee Point\")\n    plt.legend()\n    plt.show()", "CodeBleu_score": 41.18851695381432}, {"code": "def plot_knee_normalized(\n        self,\n    )\n        import matplotlib.pyplot as plt\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.plot(self.normalized_x, self.normalized_y)\n        ax.plot(self.x, self.y, color='red')\n        ax.set_xlabel('Distance')\n        ax.set_ylabel('Normalized Distance')\n        ax.set_title('Normalized Distance Curve')\n        ax.set_xlim(self.x.min(), self.x.max())\n        ax.set_ylim(self.y.min(), self.y.max())\n        ax.set_xticks(np.arange(self.x.min(), self.x.max(), 0.1))\n        ax.set_yticks(np.arange(self.y.min(), self.y.max(), 0.1))\n        if self.knee_point:\n            ax.axvline(x=self.knee_point, color='black', linestyle='--')\n        plt.show()\n\n\n", "CodeBleu_score": 37.55339738318721}]}
{"instance_id": "b2d48aae4fce", "generation_result": [{"code": "def plot_knee(\n        self,\n    )\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    if self.knee is not None:\n        plt.axvline(x=self.knee, color='red')\n    plt.show()", "CodeBleu_score": 57.47978473165}, {"code": "def plot_knee(\n        self,\n    )\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.xlim(0, max(self.x) + 1)\n    plt.ylim(0, max(self.y) + 1)\n    plt.plot([self.knee_point, self.knee_point], [0, max(self.y)], 'r--')\n    plt.show()", "CodeBleu_score": 57.818377918399364}, {"code": "def plot_knee(\n        self,\n    )\n\n        # Create a figure with a specified size of 8x8 inches\n        fig = plt.figure(figsize=(8, 8))\n\n        # Plot the data points defined by self.x and self.y\n        plt.plot(self.x, self.y)\n\n        # Add a vertical line at the knee point along the y-axis limits\n        plt.axvline(self.knee, color='red', linestyle='--')\n\n        # Set the x-axis label and y-axis label\n        plt.xlabel('x')\n        plt.ylabel('y')\n\n        # Display the plot\n        plt.show()\n\n    def plot_knee_and_elbow(\n        self,\n    ):\n        \"\"\"# This function plots a curve and its knee and elbow points, if they exist.\\nIt uses matplotlib to create a plot with a specified figure size of 8x8 inches.\\nThe function plots the data points defined by self.x and self.y.\\nIt then draws vertical lines at the knee and elbow points along the y-axis limits.\\nThe plot is displayed with the curve and the knee and elbow points highlighted.\"\"\"\n\n        # Create a figure with a specified size of 8x8 inches\n        fig = plt.figure(figsize=(8, 8))\n\n        # Plot the data points defined by self.x and self.y\n        plt.plot(self.x, self.y)\n\n        # Add vertical lines at the knee and elbow points along the y-axis limits\n        plt.axvline(self.knee, color='red', linestyle='--')\n        plt.axvline(self.elbow, color='green', linestyle='--')\n\n        # Set the x-axis label and y-axis label\n        plt.xlabel('x')\n        plt.ylabel('y')\n\n        # Display the plot\n        plt.show()\n\n    def plot_all_knees_and_elbows(\n        self,\n    ):\n        \"\"\"# This function plots a curve and all its knee and", "CodeBleu_score": 48.38801407089794}]}
{"instance_id": "4450ba2384c9", "generation_result": [{"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    from IPython.html.widgets import interact\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    transformed_data = pca.transform(data)\n\n    def show_decomp(i):\n        print \"Original data point:\", data[i]\n        print \"PCA decomposition:\", transformed_data[i]\n\n    interact(show_decomp, i=(0, len(data)-1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "CodeBleu_score": 62.72444236391257}, {"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    X_pca = pca.transform(data)\n\n    def show_decomp(idx):\n        plt.figure(figsize=(10, 5))\n        plt.subplot(121)\n        plt.imshow(data[idx].reshape(28, 28), cmap='gray')\n        plt.title('Original data')\n        plt.subplot(122)\n        plt.imshow(X_pca[idx].reshape(n_components, 1), cmap='gray')\n        plt.title('PCA decomposition')\n        plt.show()\n\n    interact(show_decomp, idx=(0, len(data)-1))\n\n", "CodeBleu_score": 55.21543875241984}, {"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    from IPython.html.widgets import interact\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    data_pca = pca.transform(data)\n\n    def show_decomp(i):\n        plt.figure(figsize=(10, 5))\n        plt.subplot(1, 2, 1)\n        plt.imshow(data[i].reshape(28, 28), cmap='gray')\n        plt.title('Original')\n        plt.subplot(1, 2, 2)\n        plt.imshow(np.dot(data_pca[i], pca.components_).reshape(28, 28), cmap='gray')\n        plt.title('PCA decomposition')\n        plt.show()\n\n    interact(show_decomp, i=(0, data.shape[0] - 1))\n\n", "CodeBleu_score": 67.6009325246592}]}
{"instance_id": "b09a163f3ff1", "generation_result": [{"code": "def plot_iris_knn()\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn import datasets\n    from sklearn.neighbors import KNeighborsClassifier\n\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n    # Create a kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n\n    # Train the classifier on the Iris dataset\n    knn.fit(X, y)\n\n    # Define the mesh grid over the feature space\n    h = .02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the predictions to match the mesh grid shape\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundaries and the training points\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())", "CodeBleu_score": 70.70732472317556}, {"code": "def plot_iris_knn()\n    # Load the Iris dataset\n    iris = load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n    # Create a kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n\n    # Train the kNN classifier on the Iris dataset\n    knn.fit(X, y)\n\n    # Create a mesh grid over the feature space\n    h = .02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the predictions to a 2D array\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundaries\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.show()\n\n", "CodeBleu_score": 55.82464654207242}, {"code": "def plot_iris_knn()\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn import datasets\n    from sklearn.neighbors import KNeighborsClassifier\n    \n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n    \n    # Create a kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n    \n    # Train the kNN classifier on the Iris dataset\n    knn.fit(X, y)\n    \n    # Create a meshgrid over the feature space\n    h = .02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Predict the class for each point in the meshgrid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    \n    # Reshape the predictions to match the meshgrid shape\n    Z = Z.reshape(xx.shape)\n    \n    # Plot the decision boundaries\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n    \n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max", "CodeBleu_score": 71.61591834812539}]}
{"instance_id": "8eaee3207c09", "generation_result": [{"code": "def plot_sgd_separator()\n    # Generate some data\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n\n    # Fit the SGD classifier\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n\n    # Plot the decision boundary\n    xx, yy = np.meshgrid(np.linspace(-1, 5, 500), np.linspace(-1, 5, 500))\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['dashed', 'solid', 'dashed'])\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=30)\n    plt.xlim(-1, 4)\n    plt.ylim(-1, 4)\n    plt.show()", "CodeBleu_score": 27.240496638104673}, {"code": "def plot_sgd_separator()\n    # Generate 50 separable data points with 2 centers and a standard deviation of 0.5\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.5)\n\n    # Initialize the SGD classifier with a hinge loss function, alpha=0.01, 200 iterations, and fit_intercept=True\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n\n    # Fit the SGD classifier to the data\n    clf.fit(X, y)\n\n    # Prepare a meshgrid of values for plotting the decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n\n    # Evaluate the decision function at each point in the meshgrid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n\n    # Plot the decision boundary at levels -1.0, 0.0, and 1.0 with different line styles\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, levels=[-1.0, 0.0, 1.0], colors='k', linestyles=['dashed', 'solid', 'dashed'])\n\n    # Scatter plot the original data points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolor='k')\n\n    # Set the axis limits and labels\n    plt.xlim(xx.min(), xx.max())\n", "CodeBleu_score": 34.6902124705888}, {"code": "def plot_sgd_separator()\n    from sklearn.datasets import make_blobs\n    from sklearn.linear_model import SGDClassifier\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n\n    xx, yy = np.meshgrid(np.linspace(-1, 5, 50), np.linspace(-1, 5, 50))\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['dashed', 'solid', 'dashed'])\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k', s=20)\n    plt.xlim(-1, 5)\n    plt.ylim(-1, 5)\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.title('SGD Decision Boundary')\n    plt.show()\n\n", "CodeBleu_score": 43.205624573124204}]}
{"instance_id": "8b54f999b4c0", "generation_result": [{"code": "def dump_svmlight_file(self, file)\n    X = self.data[:, :2]\n    y = self.data[:, 2]\n    dump_svmlight_file(X, y, file)", "CodeBleu_score": 40.86883949371065}, {"code": "def dump_svmlight_file(self, file)\n    X, y = self.data[:, :-1], self.data[:, -1]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 32.17044400405285}, {"code": "def dump_svmlight_file(self, file)\n    X = self.data[:, :-1]\n    y = self.data[:, -1]\n    dump_svmlight_file(X, y, file)\n", "CodeBleu_score": 38.35901271101158}]}
{"instance_id": "2419013adc6e", "generation_result": [{"code": "def fit(self)\n    print('fitting model...')\n    X = self.data[:, :-1]\n    y = self.data[:, -1]\n    if len(np.unique(y)) == 1:\n        self.model = OneClassSVM(kernel=self.kernel, gamma=self.gamma, coef0=self.coef0, degree=self.degree)\n    else:\n        self.model = SVC(kernel=self.kernel, gamma=self.gamma, coef0=self.coef0, degree=self.degree)\n    self.model.fit(X, y)\n    if hasattr(self.model, 'score'):\n        print('accuracy: {}'.format(self.model.score(X, y)))\n    self.surface = self.calculate_surface()\n    self.surface_type = 'model'\n    self.fitted = True\n    self.trigger_surface_changed()", "CodeBleu_score": 28.908384833906066}, {"code": "def fit(self)\n    print(\"Fitting model...\")\n    X = self.data[:, :-1]\n    y = self.data[:, -1]\n\n    if len(np.unique(y)) == 1:\n        self.model = OneClassSVM(kernel=self.kernel, gamma=self.gamma, coef0=self.coef0, degree=self.degree)\n    else:\n        self.model = SVC(kernel=self.kernel, gamma=self.gamma, coef0=self.coef0, degree=self.degree)\n\n    self.model.fit(X, y)\n\n    if hasattr(self.model, \"score\"):\n        print(\"Accuracy:\", self.model.score(X, y))\n\n    self.surface = self.calculate_surface()\n    self.surface_type = \"svm\"\n    self.fitted = True\n    self.trigger_event(\"surface_changed\")\n", "CodeBleu_score": 28.926296308968602}, {"code": "def fit(self)\n    print(\"Fitting model...\")\n    X, y = self.data.get_data()\n    self.complexity = self.complexity_slider.value()\n    self.gamma = self.gamma_slider.value()\n    self.coef0 = self.coef0_slider.value()\n    self.degree = self.degree_slider.value()\n    self.kernel = self.kernel_combo_box.currentText()\n    if len(np.unique(y)) == 1:\n        self.classifier = OneClassSVM(kernel=self.kernel, degree=self.degree, gamma=self.gamma, coef0=self.coef0,\n                                      C=self.complexity)\n    else:\n        self.classifier = SVC(kernel=self.kernel, degree=self.degree, gamma=self.gamma, coef0=self.coef0,\n                              C=self.complexity)\n    self.classifier.fit(X, y)\n    if hasattr(self.classifier, 'score'):\n        print(f\"Accuracy: {self.classifier.score(X, y)}\")\n    self.surface = self.calculate_surface()\n    self.surface_type = 'svm'\n    self.fitted = True\n    self.surface_changed.emit()\n", "CodeBleu_score": 36.304952752708544}]}
{"instance_id": "e887fd2a8513", "generation_result": [{"code": "def source_decode(sourcecode, verbose=0)\n    import importlib\n    import re\n    import warnings\n    from tpot.config import tpot_config\n\n    # Split the source code into its module and object components\n    module_match = re.match(r\"^tpot\\.(.*)\", sourcecode)\n    if module_match:\n        module_name = module_match.group(1)\n        object_name = module_name\n        module_name = \"tpot\"\n    else:\n        module_name = sourcecode\n        object_name = sourcecode\n\n    # Attempt to import the object from the module\n    try:\n        module = importlib.import_module(module_name)\n        obj = getattr(module, object_name)\n    except ImportError as e:\n        if verbose > 2:\n            raise\n        warnings.warn(\n            f\"Failed to import {sourcecode} from module {module_name}: {e}\"\n        )\n        obj = None\n\n    # Return the import string, object string, and the imported object\n    return f\"{module_name}.{object_name}\", object_name, obj\n\n", "CodeBleu_score": 37.36998468271342}, {"code": "def source_decode(sourcecode, verbose=0)\n    import importlib\n    import re\n    import warnings\n    import traceback\n    import sys\n    import tpot\n    import tpot.config\n    import tpot.export_utils\n    import tpot.pipeline\n    import tpot.pipeline.components\n    import tpot.pipeline.imputers\n    import tpot.pipeline.preprocessing\n    import tpot.pipeline.regressors\n    import tpot.pipeline.selectors\n    import tpot.pipeline.transformers\n    import tpot.utils\n    import tpot.utils.data\n    import tpot.utils.data.data_corruptor\n    import tpot.utils.data.data_corruptor.data_corruptor\n    import tpot.utils.data.data_corruptor.data_corruptor.data_corruptor\n    import tpot.utils.data.data_corruptor.data_corruptor.data_corruptor.data_corruptor\n    import tpot.utils.data.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor\n    import tpot.utils.data.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor\n    import tpot.utils.data.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor\n    import tpot.utils.data.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor\n    import tpot.utils.data.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor.data_corruptor\n    import tpot.utils.data.data_corruptor.data_corruptor.data_corruptor", "CodeBleu_score": 7.879282763695147}, {"code": "def source_decode(sourcecode, verbose=0)\n    import re\n    import importlib\n    import warnings\n    import sys\n    if verbose > 2:\n        warnings.filterwarnings(\"error\")\n    else:\n        warnings.filterwarnings(\"ignore\")\n    try:\n        if sourcecode.startswith(\"tpot.\"):\n            module_name = sourcecode.split(\".\")[1]\n            object_name = sourcecode.split(\".\")[2]\n        else:\n            module_name = sourcecode.split(\".\")[0]\n            object_name = sourcecode.split(\".\")[1]\n        module = importlib.import_module(module_name)\n        object = getattr(module, object_name)\n        import_string = f\"{module_name}.{object_name}\"\n        object_string = f\"{module_name}.{object_name}\"\n        return import_string, object_string, object\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(f\"Warning: Failed to import {sourcecode}\")\n            return None, None, None\n    except AttributeError as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(f\"Warning: Failed to import {sourcecode}\")\n            return None, None, None\n    except Exception as e:\n        if verbose > 2:\n            raise e\n        else:\n            print(f\"Warning: Failed to import {sourcecode}\")\n            return None, None, None\n\n", "CodeBleu_score": 35.92239321025643}]}
{"instance_id": "5d6f629c3363", "generation_result": [{"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            if hasattr(step, 'fit'):\n                if hasattr(step.fit, '__call__'):\n                    if hasattr(step.fit, '__code__'):\n                        if hasattr(step.fit.__code__, 'co_varnames'):\n                            if 'sample_weight' in step.fit.__code__.co_varnames:\n                                sample_weight_dict[step.__class__.__name__ + '_sample_weight'] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None\n", "CodeBleu_score": 42.645681452956794}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            if hasattr(step, \"fit\"):\n                if hasattr(step.fit, \"__call__\"):\n                    if hasattr(step.fit, \"__code__\"):\n                        if hasattr(step.fit.__code__, \"co_varnames\"):\n                            if \"sample_weight\" in step.fit.__code__.co_varnames:\n                                sample_weight_dict[step.__class__.__name__ + \"_sample_weight\"] = sample_weight\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None\n", "CodeBleu_score": 42.930157238204245}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n    sample_weight_dict = {}\n    if sample_weight is not None:\n        for step in pipeline_steps:\n            if hasattr(step, 'fit'):\n                if hasattr(step.fit, '__call__'):\n                    if hasattr(step.fit, '__code__'):\n                        if 'sample_weight' in step.fit.__code__.co_varnames:\n                            sample_weight_dict[step.__class__.__name__ + '_sample_weight'] = sample_weight\n    if len(sample_weight_dict) > 0:\n        return sample_weight_dict\n    else:\n        return None", "CodeBleu_score": 39.71819779360332}]}
{"instance_id": "cae17d519818", "generation_result": [{"code": "def _summary_of_best_pipeline(self, features, target):\n    if not self.fitted_pipeline_:\n        raise RuntimeError(\n            \"This TPOT instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\"\n        )\n    if self.verbosity > 0:\n        print(\n            f\"{'-' * 10} Best pipeline {'-' * 10}\\n{self.fitted_pipeline_.steps}\\n{'-' * 26}\"\n        )\n    self.fitted_pipeline_.fit(features, target)\n    if self.verbosity > 1:\n        print(\n            f\"{'-' * 10} Best pipeline {'-' * 10}\\n{self.fitted_pipeline_.steps}\\n{'-' * 26}\"\n        )\n    self.pareto_front_pipelines_ = [\n        pipeline.compile_and_fit(features, target)\n        for pipeline in self.pareto_front_pipelines_\n    ]\n    return self\n\n", "CodeBleu_score": 22.387923956959348}, {"code": "def _summary_of_best_pipeline(self, features, target):\n    if self.optimized_pipeline_ is None:\n        raise RuntimeError(\"No optimized pipeline found. Please run TPOT before calling `get_best_pipeline()`.\")\n    best_pipeline = self.optimized_pipeline_.fitted_pipeline_\n    if self.verbosity_ > 0:\n        print(\"\\n\\nBest pipeline:\")\n        if self.verbosity_ == 2:\n            print(\"\\n\\n\")\n        print(best_pipeline)\n    best_pipeline.fit(features, target)\n    if self.verbosity_ > 0:\n        print(\"\\n\\n\")\n    self.best_pipeline_ = best_pipeline\n    self.pareto_front_ = self.optimized_pipeline_.pareto_front_\n    self.pareto_front_models_ = [pipeline.fitted_pipeline_ for pipeline in self.pareto_front_]\n    return self", "CodeBleu_score": 22.26443466077838}, {"code": "def _summary_of_best_pipeline(self, features, target):\n    if self.fitted_pipeline_ is None:\n        raise RuntimeError(\"No optimized pipeline found. Please run `fit` first.\")\n    if self.verbosity_ > 0:\n        print(self.fitted_pipeline_)\n    if self.verbosity_ > 1:\n        print(\"\")\n    pipeline_str = self.fitted_pipeline_\n    pipeline_str = pipeline_str.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n    pipeline_str = \" \".join(pipeline_str.split())\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:-1])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pipeline_str.split(\" \")[1:])\n    pipeline_str = \" \".join(pip", "CodeBleu_score": 29.40580783577607}]}
{"instance_id": "638683d78167", "generation_result": [{"code": "def score(self, testing_features, testing_target)\n    if not self.optimized:\n        raise ValueError(\"The pipeline has not been optimized. Please call optimize() before calling score().\")\n    self._validate_features(testing_features)\n    self._validate_target(testing_target)\n    if callable(self.scoring):\n        scorer = self.scoring\n    else:\n        scorer = SCORERS[self.scoring]\n    score = scorer(self.pipeline, testing_features, testing_target)\n    return score", "CodeBleu_score": 35.37785289829002}, {"code": "def score(self, testing_features, testing_target)\n    if self.optimized is False:\n        raise ValueError(\"Pipeline must be optimized before scoring.\")\n    self._validate_data(testing_features, testing_target)\n    if self.scoring_function in SCORERS:\n        scorer = SCORERS[self.scoring_function]\n    else:\n        scorer = self.scoring_function\n    score = scorer(self.pipeline, testing_features, testing_target)\n    return score\n\n", "CodeBleu_score": 32.9348941573949}, {"code": "def score(self, testing_features, testing_target)\n    if not self.optimized:\n        raise ValueError(\"The pipeline has not been optimized. Please call the optimize() method before calling score().\")\n    self.validate_data(testing_features, testing_target)\n    if isinstance(self.scoring, str):\n        scorer = self.SCORERS[self.scoring]\n    else:\n        scorer = self.scoring\n    score = scorer(self.pipeline, testing_features, testing_target)\n    return score\n\n", "CodeBleu_score": 33.644253655250274}]}
{"instance_id": "a04382560333", "generation_result": [{"code": " def _impute_values(self, features)\n    if self.verbosity > 1:\n        print(\"Imputing missing values...\")\n    if self.imputer is None:\n        self.imputer = SimpleImputer(strategy=\"median\")\n        self.imputer.fit(features)\n    return self.imputer.transform(features)\n", "CodeBleu_score": 73.93042169190348}, {"code": " def _impute_values(self, features)\n    if self.verbosity > 1:\n        print(\"Imputing missing values...\")\n    if self.imputer is None:\n        self.imputer = SimpleImputer(strategy=\"median\")\n        self.imputer.fit(features)\n    return self.imputer.transform(features)", "CodeBleu_score": 73.93042169190348}, {"code": " def _impute_values(self, features)\n    if self.verbosity > 1:\n        print(\"Imputing missing values...\")\n    if self.imputer is None:\n        self.imputer = SimpleImputer(strategy=\"median\")\n        self.imputer.fit(features)\n    return self.imputer.transform(features)", "CodeBleu_score": 73.93042169190348}]}
{"instance_id": "0043d411c332", "generation_result": [{"code": "def _compile_to_sklearn(self, expr)\n    from sklearn.pipeline import Pipeline\n    from sklearn.base import clone\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.metaestimators import _BaseComposition\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.base import BaseEstimator\n    from sklearn.base import TransformerMixin\n    from sklearn.base import clone\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.metaestimators import _BaseComposition\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.base import BaseEstimator\n    from sklearn.base import TransformerMixin\n    from sklearn.base import clone\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.metaestimators import _BaseComposition\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.base import BaseEstimator\n    from sklearn.base import TransformerMixin\n    from sklearn.base import clone\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.metaestimators import _BaseComposition\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.base import BaseEstimator\n    from sklearn.base import TransformerMixin\n    from sklearn.base import clone\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.metaestimators import _BaseComposition\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.base import BaseEstimator\n    from sklearn.base import TransformerMixin\n    from sklearn.base import clone\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.metaestimators import _BaseComposition\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.base import BaseEstimator\n    from sklearn.base import TransformerMixin\n    from sklearn.base import clone", "CodeBleu_score": 14.820810428048514}, {"code": "def _compile_to_sklearn(self, expr)\n    from sklearn.pipeline import Pipeline\n    from sklearn.compose import ColumnTransformer\n    from sklearn.preprocessing import FunctionTransformer\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.preprocessing import KBinsDiscretizer\n    from sklearn.preprocessing import PowerTransformer\n    from sklearn.preprocessing import QuantileTransformer\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.preprocessing import SplineTransformer\n    from sklearn.preprocessing import PowerTransformer\n    from sklearn.preprocessing import QuantileTransformer\n    from sklearn.preprocessing import KBinsDiscretizer\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.preprocessing import SplineTransformer\n    from sklearn.preprocessing import FunctionTransformer\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.preprocessing import KBinsDiscretizer\n    from sklearn.preprocessing import PowerTransformer\n    from sklearn.preprocessing import QuantileTransformer\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.preprocessing import SplineTransformer\n    from sklearn.preprocessing import FunctionTransformer\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.preprocessing import OneHotEncoder\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.preprocessing import KBinsDiscretizer\n    from sklearn.preprocessing import PowerTransformer\n    from sklearn.preprocessing import QuantileTransformer\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.preprocessing import SplineTransformer\n    from sklearn.preprocessing import FunctionTransformer\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.pre", "CodeBleu_score": 13.231335850941072}, {"code": "def _compile_to_sklearn(self, expr)\n    import sklearn\n    import numpy as np\n    import ast\n    import astor\n    import re\n    import inspect\n    from sklearn.pipeline import Pipeline\n    from sklearn.base import BaseEstimator\n    from sklearn.base import TransformerMixin\n    from sklearn.base import clone\n    from sklearn.base import is_classifier\n    from sklearn.utils.metaestimators import available_if\n    from sklearn.utils.metaestimators import _BaseComposition\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _check_multimetric_scoring\n    from sklearn.utils.metaestimators import _safe_split\n    from sklearn.utils.metaestimators import _safe_indexing\n    from sklearn.utils.metaestimators import _get_param_names\n    from sklearn.utils.metaestimators import _get_param_grid\n    from sklearn.utils.metaestimators import _get_estimator_type\n    from sklearn.utils.metaestimators import _get_tags\n    from sklearn.utils.metaestimators import _get_valid_excludes\n    from sklearn.utils.metaestimators import _get_valid_excludes_from_estimator\n    from sklearn.utils.metaestimators import _get_valid_excludes_from_estimator_name\n    from sklearn.utils.metaestimators import _get_valid_excludes_from_estimator_name_and_estimator\n    from sklearn.utils.metaestimators import _get_valid_excludes_from_estimator_name_and_estimator_name\n    from sklearn.utils.metaestimators import _get_valid_excludes_from_estimator_name_and_estimator_name_and_estimator\n    from sklearn.utils.metaestimators import _get_valid_excludes_from_estimator_name_and_estimator_name_and_estimator_name\n    from sklearn.utils.metaestimators import _get_valid_excludes_from_estimator_name_and_estimator_name_and_estimator_name_and_", "CodeBleu_score": 7.354040801849459}]}
{"instance_id": "ca0ce462aa76", "generation_result": [{"code": "def pick_two_individuals_eligible_for_crossover(population)\n    individuals = population.individuals\n    if len(individuals) < 2:\n        return None, None\n    # Create sets of primitive nodes for each individual\n    primitive_sets = [set(individual.primitives) for individual in individuals]\n    # Convert each individual to a string representation\n    individual_strings = [str(individual) for individual in individuals]\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i in range(len(individuals)):\n        for j in range(i + 1, len(individuals)):\n            if primitive_sets[i].intersection(primitive_sets[j]) and individual_strings[i] != individual_strings[j]:\n                eligible_pairs.append((individuals[i], individuals[j]))\n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n    # Randomly select one of the eligible pairs and return the corresponding individuals from the population\n    pair = random.choice(eligible_pairs)\n    return pair[0], pair[1]", "CodeBleu_score": 31.989788951006208}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n    import random\n    import copy\n\n    # Create sets of primitive nodes for each individual\n    primitive_nodes = [set(individual.primitive_nodes) for individual in population]\n\n    # Convert each individual to a string representation\n    individual_strings = [str(individual) for individual in population]\n\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i in range(len(population)):\n        for j in range(i + 1, len(popividual_strings)):\n            if individual_strings[i] != individual_strings[j]:\n                if primitive_nodes[i] & primitive_nodes[j]:\n                    eligible_pairs.append((population[i], population[j]))\n                    eligible_pairs.append((population[j], population[i]))\n\n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n\n    # Randomly select one of the eligible pairs and return the corresponding individuals from the population\n    selected_pair = random.choice(eligible_pairs)\n    return selected_pair[0], selected_pair[1]\n\n", "CodeBleu_score": 34.32611388416611}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n    # Create sets of primitive nodes for each individual\n    primitive_sets = [set(individual.primitives) for individual in population]\n    # Convert each individual to a string representation\n    individual_strings = [str(individual) for individual in population]\n    # Find pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i in range(len(population)):\n        for j in range(i+1, len(population)):\n            if len(primitive_sets[i] & primitive_sets[j]) > 0 and individual_strings[i] != individual_strings[j]:\n                eligible_pairs.append((population[i], population[j]))\n    # If no eligible pairs are found, return None for both individuals\n    if len(eligible_pairs) == 0:\n        return None, None\n    # Randomly select one of the eligible pairs and return the corresponding individuals from the population\n    pair = random.choice(eligible_pairs)\n    return pair[0], pair[1]\n", "CodeBleu_score": 23.027836337614083}]}
{"instance_id": "4b03c843f506", "generation_result": [{"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    steps = []\n    for operator in pipeline_tree:\n        if operator in operators:\n            operator_name = operators[operator]\n            steps.append(f\"{operator_name}()\")\n    if len(steps) > 1:\n        code = \"make_pipeline(\\n\"\n        for step in steps:\n            code += f\"    {step},\\n\"\n        code += \")\"\n    else:\n        code = steps[0]\n    return code\n", "CodeBleu_score": 24.747230075201117}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    steps = []\n    for node in pipeline_tree:\n        if node.operator.name in operators:\n            steps.append(node.operator)\n\n    if len(steps) == 1:\n        return f\"{steps[0].name}()\"\n    else:\n        code = \"make_pipeline(\\n\"\n        for step in steps:\n            code += f\"    {step.name}(),\\n\"\n        code = code[:-2] + \"\\n)\"\n        return code\n", "CodeBleu_score": 24.531516488393112}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    steps = []\n    for i, operator in enumerate(operators):\n        if operator.type == 'estimator':\n            step = f\"{operator.name}(**{operator.parameters})\"\n        else:\n            step = f\"{operator.name}({operator.parameters})\"\n        steps.append(step)\n\n    if len(steps) > 1:\n        step_code = \"\\n\".join([f\"    {step}\" for step in steps])\n        code = f\"make_pipeline(\\n{step_code}\\n)\"\n    else:\n        code = steps[0]\n\n    return code", "CodeBleu_score": 27.685482177013686}]}
{"instance_id": "e9aec2ee39a4", "generation_result": [{"code": "def _process_operator(operator, operators, depth=0)\n    operator_name = operator[\"name\"]\n    if operator_name == \"CombineDFs\":\n        operators.append(\n            f\"combined_df = combine_dfs(X_train, X_test, y_train, y_test, {operator['input_matrices']})\"\n        )\n    else:\n        tpot_operator = tpot.config.operators[operator_name]\n        if tpot_operator.is_root:\n            if depth > 0:\n                operators.append(\n                    f\"{operator_name} = StackingEstimator(estimator={tpot_operator.name}({tpot_operator.get_params_string()}))\"\n                )\n            else:\n                operators.append(\n                    f\"{operator_name} = {tpot_operator.name}({tpot_operator.get_params_string()})\"\n                )\n        else:\n            for input_operator in operator[\"input_operators\"]:\n                _process_operator(input_operator, operators, depth + 1)\n            operators.append(\n                f\"{operator_name} = {tpot_operator.name}({tpot_operator.get_params_string()})\"\n            )\n\n", "CodeBleu_score": 28.150792391491137}, {"code": "def _process_operator(operator, operators, depth=0)\n    name = operator.name\n    if name == 'CombineDFs':\n        return [('combine_dfs', CombineDFs())]\n    else:\n        tpot_operator = operators[name]\n        if depth > 0:\n            tpot_operator = StackingEstimator(estimator=tpot_operator)\n        steps = []\n        for i, input_name in enumerate(tpot_operator.input_names):\n            input_operator = operator.input_operators[i]\n            if input_operator.name != 'initial_matrix':\n                input_steps = _process_operator(input_operator, operators, depth + 1)\n                steps.append(('input_{}'.format(i), input_steps))\n        steps.append(('operator', tpot_operator))\n        return steps\n", "CodeBleu_score": 38.70732068959833}, {"code": "def _process_operator(operator, operators, depth=0)\n    if operator.name == \"CombineDFs\":\n        operators.append((\"combine_dfs\", {\"input_dfs\": [\n            _process_operator(op, operators, depth+1) for op in operator.input_ops]}))\n    else:\n        tpot_operator = TPOT_OPERATORS[operator.name]\n        input_ops = [\n            _process_operator(op, operators, depth+1) for op in operator.input_ops]\n        if len(input_ops) == 1 and depth > 0:\n            input_ops = input_ops[0]\n        operators.append((tpot_operator, {\"input_matrix\": input_ops}))\n    return operators\n\n", "CodeBleu_score": 35.86070214082851}]}
{"instance_id": "0513bf969510", "generation_result": [{"code": "def transform(self, X)\n    # Select categorical features based on a threshold\n    cat_features = X.select_dtypes(include=['object', 'category']).columns\n    if len(cat_features) == 0:\n        raise ValueError('No categorical features found in the input data.')\n    # Apply one-hot encoding to selected categorical features\n    X_cat = pd.get_dummies(X[cat_features])\n    # Concatenate the transformed categorical features with the original numerical features\n    X_transformed = pd.concat([X.drop(cat_features, axis=1), X_cat], axis=1)\n    return X_transformed", "CodeBleu_score": 38.29988938375594}, {"code": "def transform(self, X)\n    # Check if input data is a pandas DataFrame\n    if not isinstance(X, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame\")\n    # Select categorical features based on threshold\n    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n    # If no categorical features are found, raise ValueError\n    if len(categorical_features) == 0:\n        raise ValueError(\"No categorical features found in input data\")\n    # Apply one-hot encoding to categorical features\n    X_transformed = pd.get_dummies(X, columns=categorical_features)\n    # Return transformed data\n    return X_transformed\n\n", "CodeBleu_score": 34.56687003650089}, {"code": "def transform(self, X)\n    # Select categorical features based on threshold\n    cat_features = self.select_features(X)\n    if len(cat_features) == 0:\n        raise ValueError(\"No categorical features found.\")\n    # Apply one-hot encoding to selected features\n    X_transformed = self.one_hot_encoder.fit_transform(X[cat_features])\n    return X_transformed", "CodeBleu_score": 23.654610327568328}]}
{"instance_id": "5b1bcf42ca9e", "generation_result": [{"code": "def balanced_accuracy(y_true, y_pred)\n    classes = np.unique(np.concatenate((y_true, y_pred)))\n    sensitivities = []\n    specificities = []\n    for c in classes:\n        true_positives = np.sum((y_true == c) & (y_pred == c))\n        false_negatives = np.sum((y_true == c) & (y_pred != c))\n        true_negatives = np.sum((y_true != c) & (y_pred != c))\n        false_positives = np.sum((y_true != c) & (y_pred == c))\n        sensitivity = true_positives / (true_positives + false_negatives)\n        specificity = true_negatives / (true_negatives + false_positives)\n        sensitivities.append(sensitivity)\n        specificities.append(specificity)\n    if len(classes) == 1:\n        return sensitivities[0]\n    else:\n        return np.mean(sensitivities + specificities)\n", "CodeBleu_score": 24.12642589555566}, {"code": "def balanced_accuracy(y_true, y_pred)\n    from sklearn.metrics import confusion_matrix\n    import numpy as np\n    # Calculate the confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n\n    # Get the unique classes in the true and predicted labels\n    classes = np.unique(y_true)\n\n    # Calculate sensitivity and specificity for each class\n    sensitivity = np.zeros(len(classes))\n    specificity = np.zeros(len(classes))\n    for i, cls in enumerate(classes):\n        tp = cm[cls, cls]\n        fn = np.sum(cm[cls, :]) - tp\n        fp = np.sum(cm[:, cls]) - tp\n        tn = np.sum(cm) - tp - fn - fp\n        sensitivity[i] = tp / (tp + fn)\n        specificity[i] = tn / (tn + fp)\n\n    # If there is only one class, set specificity to 1\n    if len(classes) == 1:\n        specificity = 1\n\n    # Calculate the mean of sensitivity and specificity\n    class_accuracy = (sensitivity + specificity) / 2\n\n    # Return the mean of class accuracies\n    return np.mean(class_accuracy)\n\n", "CodeBleu_score": 20.54442596750048}, {"code": "def balanced_accuracy(y_true, y_pred)\n    import numpy as np\n    from sklearn.metrics import confusion_matrix\n\n    classes = np.unique(y_true)\n    class_accuracies = []\n\n    for c in classes:\n        cm = confusion_matrix(y_true, y_pred, labels=[c, -c])\n        tp = cm[0, 0]\n        fp = cm[0, 1]\n        fn = cm[1, 0]\n        tn = cm[1, 1]\n\n        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n        specificity = tn / (tn + fp) if (tn + fp) > 0 else 1\n\n        class_accuracy = (sensitivity + specificity) / 2\n        class_accuracies.append(class_accuracy)\n\n    return np.mean(class_accuracies)\n\n", "CodeBleu_score": 20.58006553771108}]}
{"instance_id": "19a9ad39de0a", "generation_result": [{"code": "def transform(self, X, y=None)\n    X = check_array(X, accept_sparse='csr')\n    n_nonzero = X.sum(axis=1)\n    n_zero = X.shape[1] - n_nonzero\n    return np.c_[n_nonzero, n_zero, X]", "CodeBleu_score": 7.548255648873785}, {"code": "def transform(self, X, y=None)\n    X = check_array(X, accept_sparse='csr')\n    n_samples, n_features = X.shape\n    n_nonzero = X.getnnz(axis=1)\n    n_zero = n_features - n_nonzero\n    return np.hstack([n_nonzero[:, np.newaxis], n_zero[:, np.newaxis], X])", "CodeBleu_score": 15.504373622101122}, {"code": "def transform(self, X, y=None)\n    X = check_array(X, accept_sparse='csr')\n    n_nonzero = X.getnnz(axis=1)\n    n_zero = X.shape[1] - n_nonzero\n    return np.hstack((n_nonzero[:, np.newaxis], n_zero[:, np.newaxis], X))", "CodeBleu_score": 12.758815728142064}]}
{"instance_id": "5e8db0b5d447", "generation_result": [{"code": "def _init_pretest(self, features, target)\n    self.n_unique_targets_ = len(np.unique(target))\n    self.train_size = max(self.n_unique_targets_, self.train_size)\n    pretest_X, pretest_y = train_test_split(features, target, train_size=self.train_size, random_state=self.random_state)\n    if self.n_unique_targets_ > len(np.unique(pretest_y)):\n        unique_targets = np.unique(target)\n        for i in unique_targets:\n            if i not in pretest_y:\n                pretest_y = np.append(pretest_y, target[target == i][0])\n    return pretest_X, pretest_y\n", "CodeBleu_score": 27.656864038324287}, {"code": "def _init_pretest(self, features, target)\n    self.n_classes = len(np.unique(target))\n    if len(features) < self.n_classes:\n        raise ValueError(\"The number of unique classes is greater than the number of samples.\")\n    self.pretest_X, self.pretest_y = train_test_split(features, target, random_state=self.random_state)\n    if len(np.unique(self.pretest_y)) != self.n_classes:\n        unique_targets = np.unique(target)\n        for i in unique_targets:\n            if i not in self.pretest_y:\n                self.pretest_y = np.append(self.pretest_y, target[target == i][0])\n                self.pretest_X = np.vstack((self.pretest_X, features[target == i][0]))\n", "CodeBleu_score": 31.37026919816193}, {"code": "def _init_pretest(self, features, target)\n    self.classes = np.unique(target)\n    self.n_classes = len(self.classes)\n    if len(features) < self.n_classes:\n        raise ValueError(\"The number of unique classes is greater than the number of examples\")\n    self.pretest_X, self.pretest_y = train_test_split(features, target, random_state=self.random_state)\n    self.pretest_y = self.pretest_y[np.isin(self.pretest_y, self.classes)]\n", "CodeBleu_score": 19.535135251408857}]}
{"instance_id": "5e3d48667396", "generation_result": [{"code": "def _to_numpy_type(dtype)\n    if isinstance(dtype, str):\n        return dtype\n    if isinstance(dtype, type):\n        return dtype.__name__\n    if hasattr(dtype, \"__name__\"):\n        return dtype.__name__\n    if hasattr(dtype, \"__class__\"):\n        return dtype.__class__.__name__\n    return str(dtype)\n\n", "CodeBleu_score": 45.18018399779989}, {"code": "def _to_numpy_type(dtype)\n    if dtype == tf.float32:\n        return np.float32\n    elif dtype == tf.float64:\n        return np.float64\n    elif dtype == tf.int32:\n        return np.int32\n    elif dtype == tf.int64:\n        return np.int64\n    elif dtype == tf.uint8:\n        return np.uint8\n    elif dtype == tf.bool:\n        return np.bool\n    else:\n        raise ValueError(\"Invalid data type: {}\".format(dtype))\n\n", "CodeBleu_score": 41.27412740831248}, {"code": "def _to_numpy_type(dtype)\n    if dtype.is_numpy_compatible:\n        return dtype\n    elif dtype.is_tf_compatible:\n        return dtype.as_numpy_dtype\n    else:\n        raise ValueError(\"Unknown data type: {}\".format(dtype))\n\n", "CodeBleu_score": 18.532313859397163}]}
{"instance_id": "8f1120fdd463", "generation_result": [{"code": "def _dataset_as_numpy(ds, batch_size=None)\n    try:\n        if batch_size:\n            return np.concatenate([batch for batch in ds.batch(batch_size)])\n        else:\n            return np.concatenate([batch for batch in ds])\n    except AttributeError:\n        return np.concatenate([batch for batch in tf.data.Dataset.zip((ds, ds.map(lambda x, y: y)))])", "CodeBleu_score": 16.94183992810071}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n    try:\n        return np.concatenate([batch for batch in ds.batch(batch_size)])\n    except AttributeError:\n        return np.concatenate([batch for batch in ds])\n", "CodeBleu_score": 11.127171047799928}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n    try:\n        return np.concatenate([x.numpy() for x in ds.batch(batch_size)])\n    except AttributeError:\n        return np.concatenate([x for x in ds])\n", "CodeBleu_score": 11.393105079838325}]}
{"instance_id": "282271955871", "generation_result": [{"code": "def get_prng(seed)\n    import jax.random as jr\n    import numpy as np\n    import jax.numpy as jnp\n    if seed.ndim == 0:\n        seed = jnp.array(seed, dtype=np.uint32)\n    else:\n        raise ValueError(\n            f\"Seed must be a scalar, not a {seed.ndim}-dimensional array.\")\n    return jr.PRNGKey(seed)\n\n", "CodeBleu_score": 23.47466728988868}, {"code": "def get_prng(seed)\n    seed = jnp.asarray(seed, dtype=jnp.uint32)\n    if seed.ndim != 0:\n        raise ValueError(\"seed must be a scalar\")\n    key1 = jnp.uint32(seed)\n    key2 = jnp.uint32(seed >> 32)\n    return jax.random.PRNGKey(key1, key2)", "CodeBleu_score": 25.027164865332995}, {"code": "def get_prng(seed)\n    import jax.numpy as jnp\n    import jax\n\n    # Ensure the seed is a scalar\n    seed = jnp.asarray(seed).item()\n\n    # Split the seed into two 32-bit unsigned integers\n    key1, key2 = jax.random.split(jax.random.PRNGKey(seed))\n\n    # Concatenate the two 32-bit unsigned integers to form the PRNG key\n    return jnp.asarray(key1, dtype=jnp.uint32) * 2**32 + jnp.asarray(key2, dtype=jnp.uint32)", "CodeBleu_score": 24.716603061736368}]}
{"instance_id": "42234dccce9a", "generation_result": [{"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif isinstance(s.dtype, pd.Datetime64Dtype):\n        return s.dt.tz_localize(dtype.tz)\n    elif s.dtype.kind == 'O':\n        return pd.to_datetime(s, utc=True).dt.tz_localize(dtype.tz)\n    elif s.dtype.kind in 'biufMm':\n        return pd.to_datetime(s, unit='s', utc=True).dt.tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s, unit='ms', utc=True).dt.tz_localize(dtype.tz)", "CodeBleu_score": 64.22515467244904}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.tz_convert(dtype.tz)\n    elif isinstance(s.dtype, pd.Datetime64Dtype):\n        return s.tz_localize(dtype.tz)\n    elif isinstance(s.dtype, pandas_type):\n        return pd.to_datetime(s, utc=True).tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s, utc=True).tz_localize(dtype.tz)", "CodeBleu_score": 61.89701501949587}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(s, pd.DatetimeTZDtype):\n        return s.tz_convert(dtype.tz)\n    elif isinstance(s, pd.Datetime64Dtype):\n        return s.tz_localize(dtype.tz)\n    elif pandas_type == \"numeric\":\n        return pd.to_datetime(s, utc=True).tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s, utc=True).tz_localize(\"UTC\").tz_convert(dtype.tz)\n", "CodeBleu_score": 53.06538307943023}]}
{"instance_id": "9cee7388879b", "generation_result": [{"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if not isinstance(arg, (pd.Timestamp, pd.Timedelta)):\n        return\n    if isinstance(arg, pd.Timestamp):\n        arg = arg.tz_convert(None)\n    if isinstance(arg, pd.Timedelta):\n        arg = arg.total_seconds()\n    if isinstance(lower_bound, pd.Timestamp):\n        lower_bound = lower_bound.tz_convert(None)\n    if isinstance(lower_bound, pd.Timedelta):\n        lower_bound = lower_bound.total_seconds()\n    if isinstance(upper_bound, pd.Timestamp):\n        upper_bound = upper_bound.tz_convert(None)\n    if isinstance(upper_bound, pd.Timedelta):\n        upper_bound = upper_bound.total_seconds()\n    return op.boolean_indexer(arg, lower_bound, upper_bound)\n", "CodeBleu_score": 22.396058598231917}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if arg.tz is not None:\n        arg = arg.tz_convert(None)\n    if lower_bound.tz is not None:\n        lower_bound = lower_bound.tz_convert(None)\n    if upper_bound.tz is not None:\n        upper_bound = upper_bound.tz_convert(None)\n    indexer = (arg >= lower_bound) & (arg <= upper_bound)\n    return indexer.to_frame(index=False)", "CodeBleu_score": 26.796495773062734}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if op.tz is None:\n        lower_bound = lower_bound.tz_localize(None)\n        upper_bound = upper_bound.tz_localize(None)\n    else:\n        lower_bound = lower_bound.tz_convert(None)\n        upper_bound = upper_bound.tz_convert(None)\n    indexer = (arg >= lower_bound) & (arg <= upper_bound)\n    return indexer.to_dask_array().astype(bool).to_dask_dataframe()", "CodeBleu_score": 30.112225823266105}]}
{"instance_id": "0ab1cc8ccf71", "generation_result": [{"code": "def asseries(cls, value, like=None)\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.Scalar):\n        return dd.from_pandas(pd.Series([value.compute()]), npartitions=1)\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif like is not None:\n        if isinstance(like, tuple):\n            return dd.from_pandas(pd.Series(value), npartitions=1).repartition(npartitions=len(like))\n        elif isinstance(like, list):\n            return dd.from_pandas(pd.Series(value), npartitions=1).repartition(npartitions=len(like))\n        elif isinstance(like, dict):\n            return dd.from_pandas(pd.Series(value), npartitions=1).repartition(npartitions=len(like))\n        else:\n            return dd.from_pandas(pd.Series(value), npartitions=1).repartition(npartitions=1)\n    else:\n        return dd.from_pandas(pd.Series(value), npartitions=1)", "CodeBleu_score": 38.28895358243247}, {"code": "def asseries(cls, value, like=None)\n    if isinstance(value, (pd.Series, pd.DataFrame)):\n        value = dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, (pd.DataFrame, pd.Series)):\n        value = dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, dd.Series):\n        pass\n    elif isinstance(value, (list, tuple)):\n        value = dd.from_array(value, npartitions=1)\n    elif isinstance(value, dict):\n        value = dd.from_array(value.values(), npartitions=1)\n    else:\n        value = dd.from_array(value, npartitions=1)\n    if like is not None:\n        if isinstance(like, pd.Series):\n            value = dd.from_pandas(value, npartitions=1)\n        elif isinstance(like, pd.DataFrame):\n            value = dd.from_pandas(value, npartitions=1)\n        elif isinstance(like, dd.Series):\n            pass\n        elif isinstance(like, (list, tuple)):\n            value = dd.from_array(value, npartitions=1)\n        elif isinstance(like, dict):\n            value = dd.from_array(value.values(), npartitions=1)\n        else:\n            value = dd.from_array(value, npartitions=1)\n    return value", "CodeBleu_score": 34.275773124022734}, {"code": "def asseries(cls, value, like=None)\n    if isinstance(value, dd.Series):\n        return value\n    if isinstance(value, dd.Scalar):\n        return dd.from_pandas(pd.Series(value.compute(), index=[0]), npartitions=1)\n    if isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    if like is not None:\n        if isinstance(like, dd.Series):\n            return dd.from_pandas(pd.Series(value, index=like.index), npartitions=like.npartitions)\n        if isinstance(like, dd.DataFrame):\n            return dd.from_pandas(pd.Series(value, index=like.index), npartitions=like.npartitions)\n        if isinstance(like, pd.Series):\n            return dd.from_pandas(pd.Series(value, index=like.index), npartitions=1)\n        if isinstance(like, pd.DataFrame):\n            return dd.from_pandas(pd.Series(value, index=like.index), npartitions=1)\n        if isinstance(like, tuple):\n            return dd.from_pandas(pd.Series(value, index=pd.MultiIndex.from_tuples(like)), npartitions=1)\n        if isinstance(like, list):\n            return dd.from_pandas(pd.Series(value, index=pd.MultiIndex.from_tuples(like)), npartitions=1)\n        if isinstance(like, dict):\n            return dd.from_pandas(pd.Series(value, index=pd.MultiIndex.from_tuples(like.keys())), npartitions=1)\n        if isinstance(like, pd.MultiIndex):\n            return dd.from_pandas(pd.Series(value, index=like), npartitions=1)\n    return dd.from_pandas(pd.Series(value, index=[0]), npartitions=1)", "CodeBleu_score": 34.60589623487784}]}
{"instance_id": "926220a06a0c", "generation_result": [{"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    if not isinstance(dictionary, dict):\n        raise TypeError(\"dictionary must be a dictionary\")\n    if not all(isinstance(value, (dd.DataFrame, pd.DataFrame)) for value in dictionary.values()):\n        raise TypeError(\"all values in dictionary must be a Dask or Pandas DataFrame\")\n    super().connect(dictionary)", "CodeBleu_score": 47.92763638879528}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    for key, value in dictionary.items():\n        if not isinstance(value, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Value for key '{key}' is not a valid DataFrame: {value}\"\n            )\n    super().connect(dictionary)\n\n", "CodeBleu_score": 66.25364900986357}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    if not isinstance(dictionary, dict):\n        raise TypeError(\"Dictionary must be a dictionary\")\n    if not all(isinstance(value, (pd.DataFrame, dd.DataFrame)) for value in dictionary.values()):\n        raise TypeError(\"All values must be a Dask or Pandas DataFrame\")\n    super().connect(dictionary)", "CodeBleu_score": 45.649681617006294}]}
{"instance_id": "95afc38c9125", "generation_result": [{"code": "def get_estimator(library_name: str, estimator_name: str)\n    from sklearn.utils import get_module_members\n    from sklearn.base import BaseEstimator\n    from logging import getLogger, DEBUG, WARNING\n    from inspect import isclass\n    from importlib import import_module\n    from typing import Dict\n\n    logger = getLogger(__name__)\n\n    classes = get_module_members(import_module(f\"sklearn.{library_name}\"))\n\n    if estimator_name not in classes:\n        raise ValueError(f\"Estimator {estimator_name} could not be found in module {library_name}\")\n\n    estimators = [\n        class_name\n        for class_name, class_object in classes.items()\n        if isclass(class_object) and issubclass(class_object, BaseEstimator)\n    ]\n\n    if estimator_name not in estimators:\n        logger.debug(f\"Multiple estimators found for {estimator_name}: {estimators}\")\n        logger.warning(\n            f\"Multiple estimators found for {estimator_name}. The first one will be used.\"\n        )\n\n    estimator_class = classes[estimator_name]\n\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(\n            f\"Estimator {estimator_name} is not a subclass of sklearn.base.BaseEstimator. \"\n            \"It may not work as expected.\"\n        )\n\n    return estimator_class\n\n", "CodeBleu_score": 40.626794533493616}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n    import logging\n    from sklearn.base import BaseEstimator\n    from sklearn_evaluation.utils import get_module_members\n\n    classes = get_module_members(library_name)\n\n    if estimator_name not in classes:\n        raise ValueError(f\"Could not find estimator {estimator_name} in {library_name}\")\n\n    if len(classes[estimator_name]) > 1:\n        logging.debug(f\"Multiple classes found for {estimator_name}: {classes[estimator_name]}\")\n        logging.warning(\n            f\"Multiple classes found for {estimator_name}, using first: {classes[estimator_name][0]}\"\n        )\n\n    estimator_class = classes[estimator_name][0]\n\n    if not issubclass(estimator_class, BaseEstimator):\n        logging.info(f\"{estimator_class} is not a subclass of BaseEstimator\")\n\n    return estimator_class\n\n", "CodeBleu_score": 45.74713224486913}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n    from sklearn.base import BaseEstimator\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.metaestimators import _BaseComposition\n    from sklearn.utils.metaestimators import _BaseComposition as _BaseMetaEstimator\n    from sklearn.utils.metaestimators import _BaseComposition as _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEstimator\n    from sklearn.utils.metaestimators import _BaseEst", "CodeBleu_score": 13.413385636914715}]}
{"instance_id": "9562a9cc043b", "generation_result": [{"code": "def estimator_to_task(estimator_name: str) -> str\n    import json\n    with open('estimator_to_task_mapping.json', 'r') as f:\n        mapping = json.load(f)\n    for key, value in mapping.items():\n        if estimator_name.endswith(key):\n            return value\n    return 'unknown'\n", "CodeBleu_score": 29.5979419191781}, {"code": "def estimator_to_task(estimator_name: str) -> str\n    import json\n    import os\n    import sys\n    from pathlib import Path\n\n    # Get the path to the current file\n    current_file = Path(__file__).resolve()\n\n    # Get the path to the parent directory of the current file\n    parent_dir = current_file.parent\n\n    # Get the path to the \"data\" directory\n    data_dir = os.path.join(parent_dir, \"data\")\n\n    # Get the path to the \"mapping.json\" file\n    mapping_file = os.path.join(data_dir, \"mapping.json\")\n\n    # Open the mapping file and load the mapping data\n    with open(mapping_file, \"r\") as f:\n        mapping_data = json.load(f)\n\n    # Check if the estimator name ends with any of the postfixes in the mapping\n    for postfix, task in mapping_data.items():\n        if estimator_name.endswith(postfix):\n            return task\n\n    # If no match is found, return \"unknown\"\n    return \"unknown\"\n\n", "CodeBleu_score": 42.071566667599775}, {"code": "def estimator_to_task(estimator_name: str) -> str\n    import json\n    import os\n    import pathlib\n    import sys\n    from typing import Dict\n\n    # Get the path to the current file\n    current_file_path = pathlib.Path(__file__).parent.resolve()\n\n    # Load the mapping from the JSON file\n    mapping_file_path = os.path.join(current_file_path, \"estimator_to_task.json\")\n    with open(mapping_file_path, \"r\") as f:\n        mapping: Dict[str, str] = json.load(f)\n\n    # Check if the estimator name ends with any of the keys in the mapping\n    for key in mapping.keys():\n        if estimator_name.endswith(key):\n            return mapping[key]\n\n    # If no match is found, return \"unknown\"\n    return \"unknown\"\n\n", "CodeBleu_score": 37.0321222167312}]}
{"instance_id": "2f50195d043a", "generation_result": [{"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n    data = {}\n    for file in os.listdir(data_cache):\n        if file.startswith(data_name) and file.endswith(\".json\"):\n            continue\n        component, ext = file.split(\".\")\n        data[component] = load_data(os.path.join(data_cache, file))\n    return data", "CodeBleu_score": 30.371493027970743}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n    data = {}\n    for file in os.listdir(data_cache):\n        if file.endswith(\".json\"):\n            continue\n        if file.startswith(data_name):\n            component = file.split(\".\")[0]\n            file_extension = file.split(\".\")[-1]\n            with open(os.path.join(data_cache, file), \"r\") as f:\n                data[component] = f.read()\n    return data", "CodeBleu_score": 36.083411404833434}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n    data = {}\n    for file in os.listdir(data_cache):\n        if file.startswith(data_name) and file.endswith(\".json\"):\n            continue\n        component, ext = file.split(\".\")\n        data[component] = load_json_file(os.path.join(data_cache, file))\n    return data\n", "CodeBleu_score": 30.53841945497767}]}
{"instance_id": "e0281206029e", "generation_result": [{"code": "def retrieve(url: str, filename: str) -> None\n    import os\n    import requests\n\n    if os.path.isfile(filename):\n        return\n\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        assert response.status_code == 200\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024\n        written_size = 0\n        with open(filename, \"wb\") as file:\n            for data in response.iter_content(block_size):\n                file.write(data)\n                written_size += len(data)\n                print(\n                    f\"\\r{written_size / total_size * 100:.1f}%\",\n                    end=\"\",\n                )\n        print()\n        assert total_size == written_size\n    else:\n        raise AssertionError(f\"Unsupported URL scheme: {url}\")\n\n", "CodeBleu_score": 58.523524421312544}, {"code": "def retrieve(url: str, filename: str) -> None\n    import os\n    import requests\n    if os.path.isfile(filename):\n        return\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        assert response.status_code == 200, f\"Download failed: {response.status_code}\"\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024\n        written = 0\n        with open(filename, \"wb\") as file:\n            for data in response.iter_content(block_size):\n                written += len(data)\n                file.write(data)\n        assert written == total_size, f\"Downloaded {written} bytes, expected {total_size} bytes\"\n    else:\n        raise ValueError(f\"Invalid URL: {url}\")\n\n", "CodeBleu_score": 55.59408976118378}, {"code": "def retrieve(url: str, filename: str) -> None\n    import os\n    import requests\n    if os.path.isfile(filename):\n        return\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        assert response.status_code == 200, f\"Download failed with status code {response.status_code}\"\n        total_size = int(response.headers.get(\"Content-Length\", 0))\n        block_size = 1024\n        written_size = 0\n        with open(filename, \"wb\") as f:\n            for data in response.iter_content(block_size):\n                f.write(data)\n                written_size += len(data)\n                print(f\"Downloaded {written_size / total_size * 100:.1f}% ({written_size} / {total_size})\", end=\"\\r\")\n        print()\n        assert total_size == written_size, f\"Downloaded size ({written_size}) does not match expected size ({total_size})\"\n    else:\n        raise ValueError(\"Only HTTP URLs are supported\")\n\n", "CodeBleu_score": 57.58911074153039}]}
{"instance_id": "a69a71cc2fe8", "generation_result": [{"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\n    # Load the data\n    data = load_dataset(data_name, data_cache, raw_data_cache)\n\n    # Adjust the labels to be zero-indexed\n    data[\"train\"][\"target\"] -= 1\n    data[\"test\"][\"target\"] -= 1\n\n    # Convert the labels to binary\n    if dataset_params[\"binary\"]:\n        data[\"train\"][\"target\"] = (data[\"train\"][\"target\"] == 1).astype(int)\n        data[\"test\"][\"target\"] = (data[\"test\"][\"target\"] == 1).astype(int)\n\n    # Create a description dictionary\n    description = {\n        \"num_classes\": data[\"train\"].features[\"target\"].num_classes,\n        \"default_split\": {\n            \"train\": dataset_params[\"train_split\"],\n            \"valid\": dataset_params[\"valid_split\"],\n            \"test\": dataset_params[\"test_split\"],\n        },\n    }\n\n    return data, description\n\n", "CodeBleu_score": 35.15424271280211}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n    if data_name == \"covtype\":\n        data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n        data_path = os.path.join(raw_data_cache, \"covtype.data.gz\")\n        if not os.path.exists(data_path):\n            download_url(data_url, data_path)\n        data = pd.read_csv(data_path, header=None, sep=\",\")\n        data.columns = [\n            \"Elevation\",\n            \"Aspect\",\n            \"Slope\",\n            \"Horizontal_Distance_To_Hydrology\",\n            \"Vertical_Distance_To_Hydrology\",\n            \"Horizontal_Distance_To_Roadways\",\n            \"Hillshade_9am\",\n            \"Hillshade_Noon\",\n            \"Hillshade_3pm\",\n            \"Horizontal_Distance_To_Fire_Points\",\n            \"Wilderness_Area1\",\n            \"Wilderness_Area2\",\n            \"Wilderness_Area3\",\n            \"Wilderness_Area4\",\n            \"Soil_Type1\",\n            \"Soil_Type2\",\n            \"Soil_Type3\",\n            \"Soil_Type4\",\n            \"Soil_Type5\",\n            \"Soil_Type6\",\n            \"Soil_Type7\",\n            \"Soil_Type8\",\n            \"Soil_Type9\",\n            \"Soil_Type10\",\n            \"Soil_Type11\",\n            \"Soil_Type12\",\n            \"Soil_Type13\",\n            \"Soil_Type14\",\n            \"Soil_Type15\",\n            \"Soil_Type16\",\n            \"Soil_Type17\",\n            \"Soil_Type18\",\n            \"Soil_Type19\",\n            \"Soil_Type20\",\n            \"Soil_Type21\",\n            \"Soil_Type22\",\n            \"Soil_Type23\",\n            \"Soil_Type24\",\n            \"", "CodeBleu_score": 24.666367995205523}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n    # Load the data\n    data = pd.read_csv(\n        os.path.join(raw_data_cache, \"covtype.data\"),\n        header=None,\n        sep=\",\",\n        names=[\n            \"Elevation\",\n            \"Aspect\",\n            \"Slope\",\n            \"Horizontal_Distance_To_Hydrology\",\n            \"Vertical_Distance_To_Hydrology\",\n            \"Horizontal_Distance_To_Roadways\",\n            \"Hillshade_9am\",\n            \"Hillshade_Noon\",\n            \"Hillshade_3pm\",\n            \"Horizontal_Distance_To_Fire_Points\",\n            \"Wilderness_Area1\",\n            \"Wilderness_Area2\",\n            \"Wilderness_Area3\",\n            \"Wilderness_Area4\",\n            \"Soil_Type1\",\n            \"Soil_Type2\",\n            \"Soil_Type3\",\n            \"Soil_Type4\",\n            \"Soil_Type5\",\n            \"Soil_Type6\",\n            \"Soil_Type7\",\n            \"Soil_Type8\",\n            \"Soil_Type9\",\n            \"Soil_Type10\",\n            \"Soil_Type11\",\n            \"Soil_Type12\",\n            \"Soil_Type13\",\n            \"Soil_Type14\",\n            \"Soil_Type15\",\n            \"Soil_Type16\",\n            \"Soil_Type17\",\n            \"Soil_Type18\",\n            \"Soil_Type19\",\n            \"Soil_Type20\",\n            \"Soil_Type21\",\n            \"Soil_Type22\",\n            \"Soil_Type23\",\n            \"Soil_Type24\",\n            \"Soil_Type25\",\n            \"Soil_Type26\",\n            \"Soil_Type27\",\n            \"Soil_Type28\",\n            \"Soil_Type29\",\n            \"Soil_Type30\",\n            \"Soil_Type31\",\n            \"Soil_Type32\",\n", "CodeBleu_score": 16.341578340408578}]}
{"instance_id": "129cbd2529ef", "generation_result": [{"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    output_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            output_list.extend(flatten_list(element, ensure_type_homogeneity))\n        else:\n            output_list.append(element)\n    if ensure_type_homogeneity:\n        output_list = ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 39.68648195287447}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    output_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            output_list.extend(flatten_list(element))\n        else:\n            output_list.append(element)\n    if ensure_type_homogeneity:\n        output_list = ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 39.271901029998034}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    output_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            output_list.extend(flatten_list(element, ensure_type_homogeneity))\n        else:\n            output_list.append(element)\n    if ensure_type_homogeneity:\n        output_list = ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 39.68648195287447}]}
{"instance_id": "2d38c2c60b59", "generation_result": [{"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    return current_backend(x).logit(x=x, eps=eps, complex_mode=complex_mode, out=out)", "CodeBleu_score": 17.84153390478843}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    return current_backend(x).logit(x, eps=eps, complex_mode=complex_mode, out=out)\n\n", "CodeBleu_score": 17.472183660571833}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    return current_backend(x).logit(x, eps=eps, complex_mode=complex_mode, out=out)\n\n", "CodeBleu_score": 17.472183660571833}]}
{"instance_id": "205e296f8621", "generation_result": [{"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    x = np.asarray(x)\n    if out is None:\n        return np.where(np.abs(x) <= lambd, 0, x).astype(x.dtype)\n    else:\n        out = np.where(np.abs(x) <= lambd, 0, x).astype(out.dtype)\n        return out", "CodeBleu_score": 48.070902223290105}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    return np.where(np.abs(x) <= lambd, 0, x)", "CodeBleu_score": 23.021761690299734}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    return np.maximum(np.minimum(x, lambd), -lambd, out=out)", "CodeBleu_score": 22.677805936572863}]}
{"instance_id": "5b133f6ebe2d", "generation_result": [{"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return _cur_array_api.vorbis_window(window_length, dtype=dtype, out=out)", "CodeBleu_score": 18.37248681326899}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        out[:] = np.empty(window_length, dtype=dtype)\n\n    if window_length <= 0:\n        raise ValueError(\"Window length must be positive.\")\n\n    if window_length == 1:\n        out[0] = 1.0\n        return out\n\n    if window_length == 2:\n        out[0] = 1.0\n        out[1] = 1.0\n        return out\n\n    if window_length == 3:\n        out[0] = 1.0\n        out[1] = 1.0\n        out[2] = 1.0\n        return out\n\n    if window_length == 4:\n        out[0] = 1.0\n        out[1] = 1.0\n        out[2] = 1.0\n        out[3] = 1.0\n        return out\n\n    if window_length == 5:\n        out[0] = 1.0\n        out[1] = 1.0\n        out[2] = 1.0\n        out[3] = 1.0\n        out[4] = 1.0\n        return out\n\n    if window_length == 6:\n        out[0] = 1.0\n        out[1] = 1.0\n        out[2] = 1.0\n        out[3] = 1.0\n        out[4] = 1.0\n        out[5] = 1.0\n        return out\n\n    if window_length == 7:\n        out[0] = 1.0\n        out[1] = 1.0\n        out[2] = 1.0\n        out[3] = 1.0\n        out[4] = 1.0\n        out[5] = 1.0\n        out[6] = 1.0\n        return out\n\n    if window_length == 8:\n        out[0] = 1.0\n        out[1] = 1.0\n        out[2] = 1.0\n        out[3] = 1.0\n        out[4] = 1.0\n        out", "CodeBleu_score": 24.548570022257}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    pass\n\n", "CodeBleu_score": 15.444862155876383}]}
{"instance_id": "473296f771a9", "generation_result": [{"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    # Set the precision to 16 bits\n    np.set_printoptions(precision=16)\n\n    # Promote the data type of the coefficients and input array to a common type\n    coeffs = np.asarray(coeffs, dtype=np.promote_types(coeffs.dtype, x.dtype))\n    x = np.asarray(x, dtype=np.promote_types(coeffs.dtype, x.dtype))\n\n    # Compute the polynomial values using numpy's polyval\n    y = np.polyval(coeffs, x)\n\n    # Convert the result to an array with the promoted data type\n    y = np.asarray(y, dtype=coeffs.dtype)\n\n    return y", "CodeBleu_score": 33.041821935883505}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    coeffs = np.array(coeffs, dtype=np.promote_types(coeffs.dtype, x.dtype))\n    x = np.array(x, dtype=np.promote_types(coeffs.dtype, x.dtype))\n    return np.polyval(coeffs, x)\n\n", "CodeBleu_score": 30.92974106707564}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    coeffs = np.asarray(coeffs, dtype=np.promote_types(coeffs.dtype, x.dtype), copy=False)\n    x = np.asarray(x, dtype=np.promote_types(coeffs.dtype, x.dtype), copy=False)\n    return np.polyval(coeffs, x)", "CodeBleu_score": 31.9382838626111}]}
{"instance_id": "b8455221cb2a", "generation_result": [{"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.amax(x, axis=axis, keepdims=keepdims, out=out)", "CodeBleu_score": 26.902649231113717}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.amax(x, axis=axis, keepdims=keepdims, out=out)", "CodeBleu_score": 26.902649231113717}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.amax(x, axis=axis, keepdims=keepdims, out=out)", "CodeBleu_score": 26.902649231113717}]}
{"instance_id": "f72489f434a3", "generation_result": [{"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    return np.copysign(x1, x2, out=out)", "CodeBleu_score": 32.625925040171246}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    x1, x2 = _promote_types_of_inputs(x1, x2)\n    return np.copysign(x1, x2, out=out)", "CodeBleu_score": 29.590788039918724}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    return np.copysign(x1, x2, out=out)", "CodeBleu_score": 32.625925040171246}]}
{"instance_id": "55c0d77e4dc2", "generation_result": [{"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if axis is None:\n        axis = tuple(range(a.ndim))\n    elif isinstance(axis, int):\n        axis = (axis,)\n    elif isinstance(axis, list):\n        axis = tuple(axis)\n    if dtype is None:\n        dtype = np.int64\n    if out is None:\n        out = np.empty(tuple(a.shape[i] for i in axis), dtype=dtype)\n    else:\n        if not out.shape == tuple(a.shape[i] for i in axis):\n            raise ValueError(\n                f\"Incompatible output shape {out.shape} and specified axis {axis}\"\n            )\n        if not out.dtype == dtype:\n            raise ValueError(\n                f\"Incompatible output dtype {out.dtype} and specified dtype {dtype}\"\n            )\n    np.count_nonzero(a, axis=axis, keepdims=keepdims, out=out)\n    if out.size == 1:\n        return np.array([out.item()], dtype=dtype)\n    return out", "CodeBleu_score": 54.69489772506897}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)", "CodeBleu_score": 33.64802727171017}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return _cur_framework(count_nonzero, locals())\n\n", "CodeBleu_score": 27.648427860103563}]}
{"instance_id": "784bf7d24fc2", "generation_result": [{"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    return current_backend(x).gradient(\n        x, spacing=spacing, axis=axis, edge_order=edge_order\n    )", "CodeBleu_score": 43.5570891798783}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    return current_backend(x).gradient(x, spacing=spacing, axis=axis, edge_order=edge_order)", "CodeBleu_score": 40.81509674825832}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    return current_backend(x).gradient(x, spacing=spacing, axis=axis, edge_order=edge_order)\n\n", "CodeBleu_score": 40.81509674825832}]}
{"instance_id": "efa0a51af6bc", "generation_result": [{"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if not isinstance(x, np.ndarray):\n        x = np.asarray(x)\n    if not isinstance(y, np.ndarray):\n        y = np.asarray(y)\n    if not isinstance(out, np.ndarray):\n        out = np.asarray(out)\n    if np.all(x == 0):\n        return np.zeros_like(out)\n    return np.multiply(x, np.log(y), out=out)", "CodeBleu_score": 43.66428565431749}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    return _cur_framework(xlogy, locals())\n\n", "CodeBleu_score": 27.93470577522622}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    x, y = promote_types_of_inputs(x, y)\n    if np.all(x == 0):\n        return 0.0\n    return x * np.log(y)", "CodeBleu_score": 66.49390991730507}]}
{"instance_id": "ac1645284260", "generation_result": [{"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n    if dims == 1:\n        return x, kernel, strides, False\n    if dims == 2:\n        if data_format == \"channel_last\":\n            x = tf.transpose(x, (0, 3, 1, 2))\n        elif data_format == \"channel_first\":\n            x = tf.transpose(x, (0, 2, 3, 1))\n        else:\n            raise ValueError(\"Invalid data_format: {}\".format(data_format))\n        kernel = (1, kernel[0], kernel[1], 1)\n        strides = (1, strides[0], strides[1], 1)\n        return x, kernel, strides, True\n    if dims == 3:\n        if data_format == \"channel_last\":\n            x = tf.transpose(x, (0, 4, 1, 2, 3))\n        elif data_format == \"channel_first\":\n            x = tf.transpose(x, (0, 2, 3, 4, 1))\n        else:\n            raise ValueError(\"Invalid data_format: {}\".format(data_format))\n        kernel = (1, 1, kernel[0], kernel[1], kernel[2])\n        strides = (1, 1, strides[0], strides[1], strides[2])\n        return x, kernel, strides, True\n    raise ValueError(\"Invalid number of dimensions: {}\".format(dims))", "CodeBleu_score": 32.486973624367195}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n    if data_format == \"channel_first\":\n        x = tf.transpose(x, [0, 2, 3, 1])\n        dims = (dims[1], dims[2], dims[0])\n    kernel = _get_kernel_size_for_depth_max_pooling(kernel, dims)\n    strides = _get_strides_for_depth_max_pooling(strides, dims)\n    depth_pooling = _is_depth_pooling_needed(kernel, strides, dims)\n    return x, kernel, strides, depth_pooling\n", "CodeBleu_score": 42.58306134671649}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n    if dims == 1:\n        return x, kernel, strides, False\n    kernel_new = _get_kernel_size_strides_for_depth_pooling(kernel, strides, data_format)\n    if kernel_new == kernel:\n        return x, kernel, strides, False\n    if data_format == \"channel_last\":\n        x = tf.transpose(x, [0, 2, 1, 3])\n    else:\n        x = tf.transpose(x, [0, 3, 1, 2])\n    return x, kernel_new, strides, True\n\n", "CodeBleu_score": 33.776276638038084}]}
{"instance_id": "18ff0a4f7550", "generation_result": [{"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if not training:\n        return x\n    if data_format == \"NWC\":\n        x = x.transpose(0, 2, 1)\n    mask = np.random.binomial(1, prob, x.shape[1]).astype(x.dtype)\n    x = x * (1 / (1 - prob)) * mask\n    if data_format == \"NWC\":\n        x = x.transpose(0, 2, 1)\n    return x", "CodeBleu_score": 35.814780119372294}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if training:\n        if data_format == \"NWC\":\n            x = x.transpose(0, 2, 1)\n        mask = np.random.binomial(1, prob, x.shape)\n        x = x * (1 / (1 - prob)) * mask\n        if data_format == \"NWC\":\n            x = x.transpose(0, 2, 1)\n    return x", "CodeBleu_score": 33.01427163430199}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if not 0 <= prob <= 1:\n        raise ValueError(f\"prob must be a floating point tensor, but received {prob}\")\n    if training:\n        if data_format == \"NWC\":\n            x = x.transpose(0, 2, 1)\n        mask = np.random.binomial(1, prob, size=x.shape).astype(x.dtype)\n        if data_format == \"NWC\":\n            mask = mask.transpose(0, 2, 1)\n        x = x * mask * (1.0 / (1.0 - prob))\n        if data_format == \"NWC\":\n            x = x.transpose(0, 2, 1)\n    return x", "CodeBleu_score": 39.84727781143788}]}
{"instance_id": "0ce45eb7cbd9", "generation_result": [{"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    return current_backend(x).top_k(\n        x, k=k, axis=axis, largest=largest, sorted=sorted, out=out\n    )", "CodeBleu_score": 17.32595419961292}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    ...\n\n", "CodeBleu_score": 14.79044834307992}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    return current_backend(x).top_k(\n        x, k, axis=axis, largest=largest, sorted=sorted, out=out\n    )", "CodeBleu_score": 17.46362218270469}]}
{"instance_id": "1424f9541998", "generation_result": [{"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if axis is None:\n        x = x.reshape(-1)\n        norm = np.sum(np.abs(x))\n        if norm == 0:\n            norm = np.finfo(x.dtype).eps\n        if out is None:\n            out = np.empty_like(x)\n        out[:] = x / norm\n    else:\n        norm = np.sum(np.abs(x), axis=axis, keepdims=True)\n        if norm.size == 0:\n            norm = np.finfo(x.dtype).eps\n        if out is None:\n            out = np.empty_like(x)\n        out[:] = x / (norm + np.finfo(x.dtype).eps)\n    return out", "CodeBleu_score": 49.66367395715485}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.divide(x, np.sum(np.abs(x.reshape(-1)), axis=axis, keepdims=True) + np.finfo(x.dtype).eps, out=out)", "CodeBleu_score": 21.95526347764042}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.divide(x, np.sum(np.abs(x), axis=axis, keepdims=True), out=out)", "CodeBleu_score": 20.92457589200512}]}
{"instance_id": "4c3a386a1f05", "generation_result": [{"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return current_backend(\n        beta,\n        alpha=alpha,\n        beta=beta,\n        shape=shape,\n        device=device,\n        dtype=dtype,\n        seed=seed,\n        out=out,\n    )", "CodeBleu_score": 45.827460049034975}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return ivy.beta(alpha, beta, shape=shape, device=device, dtype=dtype, seed=seed, out=out)", "CodeBleu_score": 41.02640471835988}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return current_backend(ivy.current_backend()).beta(\n        alpha, beta, shape=shape, device=device, dtype=dtype, seed=seed, out=out\n    )\n\n", "CodeBleu_score": 45.13712812919765}]}
{"instance_id": "271200a8b261", "generation_result": [{"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    return np.argsort(np.argsort(x))", "CodeBleu_score": 20.815369772377547}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    x = np.asarray(x)\n    sorted_indices = np.argsort(x)\n    inverse = np.zeros_like(sorted_indices)\n    inverse[sorted_indices] = np.arange(len(x))\n    return np.sort(inverse)", "CodeBleu_score": 58.100091511845505}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    x = np.asarray(x)\n    sorted_indices = np.argsort(x)\n    inverse = np.zeros_like(sorted_indices)\n    inverse[sorted_indices] = np.arange(len(x))\n    return inverse[np.argsort(inverse)]", "CodeBleu_score": 60.697164168560214}]}
{"instance_id": "040d73b37229", "generation_result": [{"code": "def plot_medical_charts() -> None\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    plt.style.use(\"seaborn-v0_8-darkgrid\")\n\n    fig, axs = plt.subplots(2, 2)\n\n    x = np.linspace(0, 100, 100)\n    y = np.sin(x)\n    axs[0, 0].plot(x, y)\n    axs[0, 0].set_title(\"Total vomit per month\")\n    axs[0, 0].set_xlabel(\"Month\")\n    axs[0, 0].set_ylabel(\"Total vomit\")\n\n    x = np.linspace(0, 100, 100)\n    y = np.cos(x)\n    axs[0, 1].plot(x, y)\n    axs[0, 1].set_title(\"Days between vomit\")\n    axs[0, 1].set_xlabel(\"Days\")\n    axs[0, 1].set_ylabel(\"Days between vomit\")\n\n    x = np.linspace(0, 100, 100)\n    y = np.tan(x)\n    axs[1, 0].plot(x, y)\n    axs[1, 0].set_title(\"Days in daycare\")\n    axs[1, 0].set_xlabel(\"Days\")\n    axs[1, 0].set_ylabel(\"Days in daycare\")\n\n    x = np.linspace(0, 100, 100)\n    y = np.log(x)\n    axs[1, 1].plot(x, y)\n    axs[1, 1].set_title(\"Doctor visits\")\n    axs[1, 1].set_xlabel(\"Visits\")\n    axs[1, 1].set_ylabel(\"Doctor visits\")\n\n    fig.tight_layout()\n    plt.savefig(\"medical_charts.png\")", "CodeBleu_score": 28.373387861718214}, {"code": "def plot_medical_charts() -> None\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import seaborn as sns\n\n    sns.set_style(\"darkgrid\")\n    plt.rcParams[\"figure.figsize\"] = (12, 6)\n\n    df = pd.read_csv(\"data/medical_data.csv\")\n\n    fig, axes = plt.subplots(nrows=2, ncols=2)\n\n    sns.lineplot(\n        data=df,\n        x=\"month\",\n        y=\"total_vomit\",\n        ax=axes[0, 0],\n        color=\"red\",\n        marker=\"o\",\n        markersize=5,\n        linestyle=\"--\",\n        linewidth=2,\n    )\n    axes[0, 0].set_title(\"Total Vomit per Month\")\n    axes[0, 0].set_xlabel(\"Month\")\n    axes[0, 0].set_ylabel(\"Total Vomit\")\n\n    sns.barplot(\n        data=df,\n        x=\"month\",\n        y=\"days_between_vomit\",\n        ax=axes[0, 1],\n        color=\"green\",\n        alpha=0.7,\n    )\n    axes[0, 1].set_title(\"Days Between Vomit\")\n    axes[0, 1].set_xlabel(\"Month\")\n    axes[0, 1].set_ylabel(\"Days\")\n\n    sns.lineplot(\n        data=df,\n        x=\"month\",\n        y=\"days_in_daycare\",\n        ax=axes[1, 0],\n        color=\"blue\",\n        marker=\"o\",\n        markersize=5,\n        linestyle=\"--\",\n        linewidth=2,\n    )\n    axes[1, 0].set_title(\"Days in Daycare\")\n    axes[1, 0].set_xlabel(\"Month\")\n    axes[1, 0].set_ylabel(\"", "CodeBleu_score": 27.47173366304599}, {"code": "def plot_medical_charts() -> None\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import seaborn as sns\n    from matplotlib.ticker import MultipleLocator\n\n    plt.style.use(\"ggplot\")\n    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n    fig.suptitle(\"Medical charts\", fontsize=20)\n\n    # Total vomit per month\n    vomit_df = pd.read_csv(\"vomit.csv\")\n    vomit_df[\"date\"] = pd.to_datetime(vomit_df[\"date\"])\n    vomit_df[\"month\"] = vomit_df[\"date\"].dt.month\n    vomit_df[\"year\"] = vomit_df[\"date\"].dt.year\n    vomit_df = vomit_df.groupby([\"month\", \"year\"]).sum().reset_index()\n    vomit_df[\"date\"] = vomit_df[\"month\"].astype(str) + \"/\" + vomit_df[\"year\"].astype(str)\n    vomit_df = vomit_df.drop([\"month\", \"year\"], axis=1)\n    vomit_df = vomit_df.rename(columns={\"date\": \"Date\", \"vomit\": \"Vomit\"})\n    sns.lineplot(data=vomit_df, x=\"Date\", y=\"Vomit\", ax=axes[0, 0])\n    axes[0, 0].xaxis.set_major_locator(MultipleLocator(12))\n    axes[0, 0].set_title(\"Total vomit per month\")\n\n    # Days between vomit\n    vomit_df = pd.read_csv(\"vomit.csv\")\n    vomit_df[\"date\"] = pd.to_datetime(vomit_df[\"date\"])\n    vomit_df = vomit_df.sort_values(by=\"date\")\n    vomit_df", "CodeBleu_score": 26.458270928860085}]}
{"instance_id": "7061f4d0c3fc", "generation_result": [{"code": "def plot_feeding_24h_viz() -> None\n    # Import the necessary libraries\n    import matplotlib.pyplot as plt\n    from datetime import datetime, timedelta\n    from dateutil.relativedelta import relativedelta\n    import numpy as np\n    import pandas as pd\n    from pathlib import Path\n    from src.parse_raw_data import parse_raw_data\n    from src.export_figure import export_figure\n\n    # Extract the feeding data\n    feeding_data = parse_raw_data()\n\n    # Set up a plot with a dark grid style\n    plt.style.use(\"dark_background\")\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    # Compute the offset from the birthday based on the last solid feeding date\n    last_solid_feeding = feeding_data[\"solid_feeding\"].max()\n    birthday = datetime(2022, 11, 14)\n    offset = relativedelta(last_solid_feeding, birthday)\n    offset_days = offset.days\n\n    # Create a scatter plot of feeding times, using red for bottle feedings and blue for solid feedings\n    ax.scatter(\n        x=feeding_data[\"bottle_feeding\"].index,\n        y=feeding_data[\"bottle_feeding\"].values,\n        color=\"red\",\n        label=\"Bottle Feedings\",\n    )\n    ax.scatter(\n        x=feeding_data[\"solid_feeding\"].index,\n        y=feeding_data[\"solid_feeding\"].values,\n        color=\"blue\",\n        label=\"Solid Feedings\",\n    )\n\n    # Add a legend to distinguish between the two\n    ax.legend()\n\n    # Calculate the end date for the plot\n    end_date = last_solid_feeding + timedelta(days=7)\n\n    # Format the plot to display feeding data over a week in a 24-hour format\n    ax.set_xlim(birthday, end_date)\n    ax.set_ylim(0, 24)\n    ax.set_yticks(np.", "CodeBleu_score": 27.95343675052261}, {"code": "def plot_feeding_24h_viz() -> None\n    # Import and extract feeding data\n    df_feeding = parse_raw_data()\n    df_feeding = df_feeding[df_feeding[\"type\"] != \"bottle\"]\n\n    # Set up plot with dark grid style\n    plt.style.use(\"dark_background\")\n\n    # Compute offset from birthday based on last solid feeding date\n    last_solid_date = df_feeding[df_feeding[\"type\"] == \"solid\"][\"date\"].max()\n    offset = (last_solid_date - pd.to_datetime(\"2022-05-01\")).days\n\n    # Create scatter plot of feeding times\n    plt.scatter(\n        df_feeding[\"date\"] - pd.to_timedelta(offset, unit=\"days\"),\n        df_feeding[\"time\"],\n        c=df_feeding[\"type\"].map({\"solid\": \"blue\", \"bottle\": \"red\"}),\n        label=df_feeding[\"type\"].unique(),\n    )\n\n    # Add legend to distinguish between bottle and solid feedings\n    plt.legend()\n\n    # Calculate end date for plot\n    end_date = df_feeding[\"date\"].max() + pd.to_timedelta(7, unit=\"days\")\n\n    # Format plot to display feeding data over a week in a 24-hour format\n    plt.xlim(df_feeding[\"date\"].min(), end_date)\n    plt.xticks(\n        pd.date_range(df_feeding[\"date\"].min(), end_date, freq=\"D\"),\n        pd.date_range(df_feeding[\"date\"].min(), end_date, freq=\"D\").strftime(\"%a\"),\n    )\n    plt.ylim(0, 24)\n    plt.yticks(range(0, 25, 2))\n\n    # Export figure\n    export_figure(plt, \"feeding_24h_viz\")\n\n", "CodeBleu_score": 23.408849454040126}, {"code": "def plot_feeding_24h_viz() -> None\n    # Import and extract feeding data\n    df = parse_raw_data()\n    df = df.loc[df[\"feeding_type\"] != \"bottle_feeding\"]\n    df[\"feeding_time\"] = pd.to_datetime(df[\"feeding_time\"])\n    df = df.sort_values(by=\"feeding_time\")\n\n    # Set up plot with dark grid style\n    plt.style.use(\"dark_background\")\n\n    # Compute offset from birthday based on last solid feeding date\n    last_solid_feeding_date = df.loc[df[\"feeding_type\"] == \"solid_feeding\", \"feeding_time\"].max()\n    offset = (last_solid_feeding_date - pd.to_datetime(birthday)).days\n\n    # Create scatter plot of feeding times, using red for bottle feedings and blue for solid feedings\n    plt.scatter(\n        df[\"feeding_time\"].dt.hour + df[\"feeding_time\"].dt.dayofweek * 24,\n        df[\"feeding_time\"].dt.minute + df[\"feeding_time\"].dt.hour * 60,\n        c=df[\"feeding_type\"].map({\"bottle_feeding\": \"red\", \"solid_feeding\": \"blue\"}),\n    )\n\n    # Add legend to distinguish between bottle and solid feedings\n    plt.legend([\"Bottle Feedings\", \"Solid Feedings\"])\n\n    # Calculate end date for plot\n    end_date = last_solid_feeding_date + pd.Timedelta(days=7)\n\n    # Format plot to display feeding data over a week in a 24-hour format\n    plt.xlim([0, 168])\n    plt.xticks(range(0, 168, 24), [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n    plt.ylim([0, 1440])\n    plt.yticks(range(0, 1440, 60), [\"0:0", "CodeBleu_score": 23.10004518152538}]}
{"instance_id": "6c1ffa63d122", "generation_result": [{"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    counts = self.get_target_counts(task)\n    sns.barplot(x=counts.index, y=counts.values, ax=ax, **kwargs)\n    ax.set_title(f\"{task.value} Counts\")\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.tick_params(axis=\"x\", labelsize=8)\n    ax.tick_params(axis=\"y\", labelsize=8)\n    return ax", "CodeBleu_score": 39.13635868277249}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    target_counts = self.get_target_counts(task)\n    sns.barplot(x=target_counts.index, y=target_counts.values, ax=ax, **kwargs)\n    ax.set_title(f\"{task.name} Counts\")\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n    return ax", "CodeBleu_score": 38.71834726198804}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    if task == ClassificationTask.Result:\n        target_counts = self.get_target_counts()\n    elif task == ClassificationTask.Prediction:\n        target_counts = self.get_prediction_counts()\n    elif task == ClassificationTask.Target:\n        target_counts = self.get_target_counts()\n    else:\n        raise ValueError(f\"Invalid classification task: {task}\")\n    sns.barplot(x=target_counts.index, y=target_counts.values, ax=ax, **kwargs)\n    ax.set_title(f\"{task.name} Counts\")\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    return ax", "CodeBleu_score": 37.976223806500094}]}
{"instance_id": "2f81a906bad5", "generation_result": [{"code": "def render_to_rgb(figure)\n    import matplotlib.pyplot as plt\n    from matplotlib.backends import backend_agg as plt_backend_agg\n    import numpy as np\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = plt_backend_agg.FigureCanvasAgg(fig)\n            canvas.draw()\n            data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n            image = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n            image = image[..., :3]\n            images.append(image)\n        image = np.stack(images, axis=0)\n    else:\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n        image = data.reshape(figure.canvas.get_width_height()[::-1] + (4,))\n        image = image[..., :3]\n    plt.close(figure)\n    return image", "CodeBleu_score": 47.217317448292654}, {"code": "def render_to_rgb(figure)\n    import numpy as np\n    from matplotlib.backends import backend_agg as plt_backend_agg\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = plt_backend_agg.FigureCanvasAgg(fig)\n            canvas.draw()\n            rgba_buffer = canvas.buffer_rgba()\n            image = np.reshape(np.asarray(rgba_buffer), (rgba_buffer.height, rgba_buffer.width, 4))\n            image = image[:, :, :3]\n            images.append(image)\n        images = np.stack(images, axis=0)\n        return images\n    else:\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        rgba_buffer = canvas.buffer_rgba()\n        image = np.reshape(np.asarray(rgba_buffer), (rgba_buffer.height, rgba_buffer.width, 4))\n        image = image[:, :, :3]\n        return image", "CodeBleu_score": 39.905596373975115}, {"code": "def render_to_rgb(figure)\n    try:\n        from matplotlib.backends import backend_agg as plt_backend_agg\n    except ImportError:\n        raise ImportError('Please install matplotlib to use this function')\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = plt_backend_agg.FigureCanvasAgg(fig)\n            canvas.draw()\n            buf = canvas.buffer_rgba()\n            image = np.reshape(np.asarray(buf, dtype=np.uint8), (fig.canvas.get_width_height()[::-1] + (4,)))\n            images.append(image)\n        images = np.stack(images, axis=0)\n        return images\n    else:\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        buf = canvas.buffer_rgba()\n        image = np.reshape(np.asarray(buf, dtype=np.uint8), (figure.canvas.get_width_height()[::-1] + (4,)))\n        return image\n", "CodeBleu_score": 42.298977495017084}]}
{"instance_id": "c3037b01569c", "generation_result": [{"code": "def beholder_pytorch()\n    import torch\n    import beholder\n    import time\n    beholder.init(\"logs/beholder_pytorch\", \"pytorch\")\n    for i in range(1000):\n        tensors = [torch.randn(10, 10) for _ in range(10)]\n        names = [f\"tensor_{i}\" for i in range(10)]\n        beholder.update(tensors, names, frame=i)\n        time.sleep(0.01)\n        print(f\"Iteration {i}\")\n\n", "CodeBleu_score": 23.492144920074114}, {"code": "def beholder_pytorch()\n    import torch\n    import time\n    import beholder\n    beholder_log_dir = \"beholder_pytorch\"\n    beholder_log_dir = beholder.Beholder(beholder_log_dir)\n    for i in range(1000):\n        rand_tensor_1 = torch.rand(10, 10)\n        rand_tensor_2 = torch.rand(10, 10)\n        beholder_log_dir.update(rand_tensor_1, \"tensor_1\", frame=i)\n        beholder_log_dir.update(rand_tensor_2, \"tensor_2\", frame=i)\n        time.sleep(0.01)\n        print(i)\n\n", "CodeBleu_score": 26.43705174756138}, {"code": "def beholder_pytorch()\n    import time\n    import torch\n    import random\n    import numpy as np\n    from beholder.beholder import Beholder\n    from beholder.utils.utils import generate_random_tensor\n    beholder = Beholder(log_dir='logs/pytorch_beholder/')\n    for i in range(1000):\n        list_of_tensors = []\n        list_of_names = []\n        for j in range(10):\n            tensor = generate_random_tensor(shape=(10, 10, 10))\n            list_of_tensors.append(tensor)\n            list_of_names.append(f'tensor_{j}')\n        beholder.update(list_of_tensors, list_of_names, frame=random.randint(0, 1000))\n        time.sleep(0.01)\n        print(f'Iteration: {i}')\n\n", "CodeBleu_score": 24.575590134340665}]}
{"instance_id": "cd57b5de8a88", "generation_result": [{"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    tensor_data = summary.tensor.tensor_proto.string_val\n    tensor_data = np.frombuffer(tensor_data, dtype=np.float32)\n    tensor_data = tensor_data.reshape(summary.tensor.shape.dim)\n    thresholds = [float(i) / num_thresholds for i in range(num_thresholds)]\n    tp, fp, tn, fn, precision, recall = [], [], [], [], [], []\n    for threshold in thresholds:\n        tp.append(\n            np.sum(\n                np.logical_and(tensor_data > threshold, summary.labels == 1).flatten()\n            )\n        )\n        fp.append(\n            np.sum(\n                np.logical_and(tensor_data > threshold, summary.labels == 0).flatten()\n            )\n        )\n        tn.append(\n            np.sum(\n                np.logical_and(tensor_data <= threshold, summary.labels == 0).flatten()\n            )\n        )\n        fn.append(\n            np.sum(\n                np.logical_and(tensor_data <= threshold, summary.labels == 1).flatten()\n            )\n        )\n        precision.append(tp[-1] / (tp[-1] + fp[-1]))\n        recall.append(tp[-1] / (tp[-1] + fn[-1]))\n    pr_data = {\n        \"thresholds\": thresholds,\n        \"tp\": tp,\n        \"fp\": fp,\n        \"tn\": tn,\n        \"fn\": fn,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n    self.log_asset_data(tag, pr_data, step=step)\n\n", "CodeBleu_score": 31.973931392859622}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    tensor_data = summary.tensor.tensor_content\n    dims = summary.tensor.shape.dim\n    if len(dims) == 2:\n        tensor_data = np.reshape(tensor_data, (dims[0].size, dims[1].size))\n    elif len(dims) == 3:\n        tensor_data = np.reshape(tensor_data, (dims[0].size, dims[1].size, dims[2].size))\n    elif len(dims) == 4:\n        tensor_data = np.reshape(tensor_data, (dims[0].size, dims[1].size, dims[2].size, dims[3].size))\n    thresholds = np.linspace(0, 1, num_thresholds)\n    TP = np.zeros(num_thresholds)\n    FP = np.zeros(num_thresholds)\n    TN = np.zeros(num_thresholds)\n    FN = np.zeros(num_thresholds)\n    precision = np.zeros(num_thresholds)\n    recall = np.zeros(num_thresholds)\n    for i, threshold in enumerate(thresholds):\n        TP[i] = np.sum(tensor_data > threshold)\n        FP[i] = np.sum(tensor_data <= threshold)\n        TN[i] = np.sum(tensor_data <= threshold)\n        FN[i] = np.sum(tensor_data > threshold)\n        precision[i] = TP[i] / (TP[i] + FP[i])\n        recall[i] = TP[i] / (TP[i] + FN[i])\n    pr_data = {\n        \"thresholds\": thresholds.tolist(),\n        \"TP\": TP.tolist(),\n        \"FP\": FP.tolist(),\n        \"TN\": TN.tolist(),\n        \"FN\": FN.tolist(),\n        \"precision\": precision.tolist(),\n        \"recall\": recall.tolist()\n   ", "CodeBleu_score": 26.335082799655464}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    tensor_data = self.get_tensor_data(summary)\n    if len(tensor_data.shape) == 2:\n        tensor_data = tensor_data.reshape((-1, tensor_data.shape[1]))\n    thresholds = np.linspace(0, 1, num_thresholds + 1)[1:]\n    tp, fp, tn, fn, precision, recall = self.compute_pr_data(tensor_data, thresholds)\n    pr_data = {\n        \"thresholds\": thresholds,\n        \"tp\": tp,\n        \"fp\": fp,\n        \"tn\": tn,\n        \"fn\": fn,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n    self.log_asset_data(tag, pr_data, step=step)", "CodeBleu_score": 37.957254697147334}]}
{"instance_id": "7c4ea9beecc9", "generation_result": [{"code": "def load_mnist(images, labels, num)\n    import gzip\n    import numpy as np\n    import os\n\n    # Initialize empty arrays for the data and target labels\n    data = np.zeros((num, 784), dtype=np.uint8)\n    target = np.zeros((num,), dtype=np.uint8)\n\n    # Open the image and label files using gzip\n    with gzip.open(images, 'rb') as f_images:\n        with gzip.open(labels, 'rb') as f_labels:\n            # Skip the header bytes in both files\n            f_images.read(16)\n            f_labels.read(8)\n\n            # Read the image and label data byte by byte, storing them in the respective arrays\n            for i in range(num):\n                for j in range(784):\n                    data[i, j] = ord(f_images.read(1))\n                target[i] = ord(f_labels.read(1))\n\n    # Return the data and target arrays\n    return data, target\n", "CodeBleu_score": 58.85416142590299}, {"code": "def load_mnist(images, labels, num)\n    import gzip\n    import numpy as np\n    with gzip.open(images, 'rb') as f:\n        data = f.read()\n        data = np.frombuffer(data, np.uint8, offset=16)\n        data = data.reshape(-1, 28 * 28)\n        data = data[:num]\n    with gzip.open(labels, 'rb') as f:\n        target = f.read()\n        target = np.frombuffer(target, np.uint8, offset=8)\n        target = target[:num]\n    return data, target\n\n", "CodeBleu_score": 35.1318051250432}, {"code": "def load_mnist(images, labels, num)\n    # Initialize empty arrays for the data and target labels\n    data = np.zeros((num, 28, 28), dtype=np.uint8)\n    target = np.zeros((num,), dtype=np.uint8)\n\n    # Open the image and label files using gzip\n    with gzip.open(images, 'rb') as f_images:\n        with gzip.open(labels, 'rb') as f_labels:\n            # Skip the header bytes\n            f_images.read(16)\n            f_labels.read(8)\n\n            # Read the image and label data byte by byte, storing them in the respective arrays\n            for i in range(num):\n                data[i] = np.frombuffer(f_images.read(784), dtype=np.uint8).reshape(28, 28)\n                target[i] = np.frombuffer(f_labels.read(1), dtype=np.uint8)\n\n    # Return the data and target arrays\n    return data, target\n\n", "CodeBleu_score": 50.803484268576184}]}
{"instance_id": "bf4f1c828b25", "generation_result": [{"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.unique_values = {}\n    self.min_max_values = {}\n    self.cardinality = {}\n    self.length = len(ldf)\n\n    for col in ldf.columns:\n        self.unique_values[col] = ldf[col].unique()\n        self.cardinality[col] = len(self.unique_values[col])\n        if ldf[col].dtype == \"int64\" or ldf[col].dtype == \"float64\":\n            self.min_max_values[col] = (\n                ldf[col].min(),\n                ldf[col].max(),\n            )\n\n    if ldf.index.dtype != \"int64\":\n        self.unique_values[\"index\"] = ldf.index.unique()\n        self.cardinality[\"index\"] = len(self.unique_values[\"index\"])\n\n", "CodeBleu_score": 29.86768252089289}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.unique_values = {}\n    self.min_max_values = {}\n    self.cardinality = {}\n    self.length = len(ldf)\n\n    for col in ldf.columns:\n        if ldf.data[col].dtype == \"object\":\n            self.unique_values[col] = ldf.data[col].unique()\n        else:\n            self.unique_values[col] = ldf.data[col].unique()\n        self.cardinality[col] = len(self.unique_values[col])\n\n        if ldf.data[col].dtype == \"datetime64[ns]\":\n            self.unique_values[col] = ldf.data[col].unique().astype(str)\n\n        if ldf.data[col].dtype != \"object\":\n            self.min_max_values[col] = (\n                ldf.data[col].min(),\n                ldf.data[col].max(),\n            )\n\n    if ldf.index.dtype != \"int64\":\n        self.unique_values[ldf.index.name] = ldf.index.unique()\n        self.cardinality[ldf.index.name] = len(self.unique_values[ldf.index.name])\n\n", "CodeBleu_score": 33.06461340386373}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.unique_values = {}\n    self.min_max_values = {}\n    self.cardinality = {}\n    self.index_cardinality = {}\n    self.index_unique_values = {}\n    self.index_min_max_values = {}\n    self.length = len(ldf)\n    for col in ldf.columns:\n        self.unique_values[col] = ldf[col].unique()\n        self.cardinality[col] = ldf[col].nunique()\n        if ldf[col].dtype == \"object\":\n            self.min_max_values[col] = (\n                ldf[col].min().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                ldf[col].max().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            )\n        else:\n            self.min_max_values[col] = (ldf[col].min(), ldf[col].max())\n    if ldf.index.dtype != \"int64\":\n        self.index_unique_values = ldf.index.unique()\n        self.index_cardinality = len(ldf.index.unique())\n        self.index_min_max_values = (\n            ldf.index.min().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            ldf.index.max().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        )\n    else:\n        self.index_unique_values = ldf.index.unique()\n        self.index_cardinality = len(ldf.index.unique())\n        self.index_min_max_values = (ldf.index.min(), ldf.index.max())", "CodeBleu_score": 32.017121377984864}]}
{"instance_id": "3e6fc3d2f47c", "generation_result": [{"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    preview_query = f\"\"\"\n    SELECT *\n    FROM {tbl.name}\n    LIMIT {preview_size}\n    \"\"\"\n    df = pd.read_sql(preview_query, sql_conn)\n    return df\n\n", "CodeBleu_score": 16.36916960419701}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    if tbl.sql_connection is None:\n        raise Exception(\"Please set a SQL connection before previewing.\")\n    if tbl.sql_table_name is None:\n        raise Exception(\"Please set a SQL table name before previewing.\")\n\n    preview_query = f\"SELECT TOP {preview_size} * FROM {tbl.sql_table_name}\"\n    return tbl.sql_connection.execute_query(preview_query)", "CodeBleu_score": 25.713207738941176}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    preview_query = f\"SELECT * FROM {tbl.table_name} LIMIT {preview_size}\"\n    df = pd.read_sql(preview_query, con=sql_connection)\n    return df", "CodeBleu_score": 17.14509386599328}]}
{"instance_id": "b3dee87be4b8", "generation_result": [{"code": "def execute_sampling(tbl: LuxSQLTable)\n    sample_flag = lux.config[\"SAMPLE_FLAG\"]\n    sample_start = lux.config[\"SAMPLE_START\"]\n    sample_cap = lux.config[\"SAMPLE_CAP\"]\n    sample_frac = lux.config[\"SAMPLE_FRAC\"]\n    if sample_flag == \"auto\":\n        if tbl.data.shape[0] > sample_cap:\n            sample_flag = \"head\"\n        else:\n            sample_flag = \"frac\"\n    if sample_flag == \"head\":\n        limit = sample_cap\n    elif sample_flag == \"frac\":\n        limit = int(tbl.data.shape[0] * sample_frac)\n    elif sample_flag == \"none\":\n        limit = tbl.data.shape[0]\n    else:\n        raise Exception(\n            \"Invalid sampling flag. Please use one of the following: 'auto', 'head', 'frac', or 'none'.\"\n        )\n    if sample_start > 0:\n        limit = limit + sample_start\n    tbl._sampled = True\n    if tbl.data.shape[0] > limit:\n        tbl.data = tbl.data.head(limit)\n    else:\n        tbl.data = tbl.data\n    tbl._sampled = True\n    return tbl\n\n", "CodeBleu_score": 23.576479761314044}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n    sample_flag = lux.config.get(\"SAMPLE_FLAG\")\n    sample_start = lux.config.get(\"SAMPLE_START\")\n    sample_cap = lux.config.get(\"SAMPLE_CAP\")\n    sample_frac = lux.config.get(\"SAMPLE_FRAC\")\n    if sample_flag:\n        # get the number of rows in the table\n        num_rows = tbl.sql_query(\n            \"SELECT COUNT(*) FROM \" + tbl.table_name\n        ).iloc[0, 0]\n        # calculate the limit based on the sample fraction\n        limit = int(num_rows * sample_frac)\n        # format the sample query\n        sample_query = f\"SELECT * FROM {tbl.table_name} LIMIT {limit}\"\n        # execute the sample query and store the sampled rows in the _sampled attribute of the tbl object\n        tbl._sampled = tbl.sql_query(sample_query)", "CodeBleu_score": 29.111212444650846}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n    sample_flag = lux.config.SAMPLE_FLAG\n    sample_start = lux.config.SAMPLE_START\n    sample_cap = lux.config.SAMPLE_CAP\n    sample_frac = lux.config.SAMPLE_FRAC\n\n    # get table length\n    query = f\"SELECT COUNT(*) FROM {tbl.table_name}\"\n    length = tbl.engine.execute(query).fetchone()[0]\n\n    # set limit\n    if sample_flag:\n        if sample_cap:\n            limit = min(sample_cap, length)\n        else:\n            limit = int(sample_frac * length)\n        limit = max(limit, sample_start)\n        query = f\"SELECT * FROM {tbl.table_name} LIMIT {limit}\"\n    else:\n        query = f\"SELECT * FROM {tbl.table_name}\"\n\n    # execute query\n    tbl._sampled = tbl.engine.execute(query)", "CodeBleu_score": 26.06046578504635}]}
{"instance_id": "485253b1088a", "generation_result": [{"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n    # Override default text settings with provided parameters\n    text = params.get(\"text\", {\"text\": \"Text\"})\n    x = params.get(\"x\", {\"x\": 0.5})\n    y = params.get(\"y\", {\"y\": 0.5})\n\n    # Extract text, x, and y values\n    text = text[\"text\"]\n    x = x[\"x\"]\n    y = y[\"y\"]\n\n    # Retrieve background's bounds\n    x_min, x_max, y_min, y_max = background.bounds\n\n    # Interpolate x and y values to fit within background's bounds\n    x = x_min + (x_max - x_min) * x\n    y = y_min + (y_max - y_min) * y\n\n    # Draw text at calculated position with remaining parameters\n    plt.text(x, y, text, **params)\n\n", "CodeBleu_score": 18.00096144539235}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n    # Override default text settings with provided parameters\n    text = params.get('text', {'text': 'text', 'x': 0.5, 'y': 0.5})\n    x, y = text['x'], text['y']\n    # Retrieve background bounds\n    x_min, x_max, y_min, y_max = background.bounds\n    # Interpolate x and y values to fit within background bounds\n    x = np.interp(x, [0, 1], [x_min, x_max])\n    y = np.interp(y, [0, 1], [y_min, y_max])\n    # Draw text at calculated position with remaining parameters\n    plt.text(x, y, text['text'], **{k: v for k, v in params.items() if k != 'text'})", "CodeBleu_score": 18.583310527462864}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n    text = params[\"text\"]\n    x = params[\"x\"]\n    y = params[\"y\"]\n    x, y = background.interpolate(x, y)\n    params = {\n        \"text\": text,\n        \"x\": x,\n        \"y\": y,\n        **params\n    }\n    plt.text(**params)", "CodeBleu_score": 12.259164891229085}]}
{"instance_id": "5f70a88b6b72", "generation_result": [{"code": "def _determine_format_string( dates, datetime_format=None )\n    if datetime_format:\n        return datetime_format\n    else:\n        days_between_points = [(dates[i+1] - dates[i]).days for i in range(len(dates)-1)]\n        average_days_between_points = sum(days_between_points) / len(days_between_points)\n        if average_days_between_points < 0.33:\n            if (dates[-1] - dates[0]).days > 1:\n                return '%b %d, %H:%M'\n            else:\n                return '%H:%M'\n        else:\n            if (dates[-1] - dates[0]).days > 365:\n                return '%Y-%b-%d'\n            else:\n                return '%b %d'\n\n", "CodeBleu_score": 39.47357020826542}, {"code": "def _determine_format_string( dates, datetime_format=None )\n    # Calculate the average number of days between consecutive dates\n    dates = sorted(dates)\n    dates_diff = [(dates[i+1] - dates[i]).days for i in range(len(dates) - 1)]\n    avg_days = sum(dates_diff) / len(dates_diff)\n\n    # Return the datetime_format if provided\n    if datetime_format:\n        return datetime_format\n\n    # Determine the format string based on the average number of days\n    if avg_days < 0.33:\n        if len(dates) > 24:\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n    else:\n        if (dates[-1] - dates[0]).days > 365:\n            return '%Y-%b-%d'\n        else:\n            return '%b %d'\n\n", "CodeBleu_score": 30.044763297361186}, {"code": "def _determine_format_string( dates, datetime_format=None )\n    # Calculate the average number of days between consecutive dates\n    avg_days = sum([(dates[i+1] - dates[i]).days for i in range(len(dates)-1)]) / (len(dates)-1)\n\n    # If datetime_format is provided, return it\n    if datetime_format:\n        return datetime_format\n\n    # If the average days between points is less than 0.33, indicating intraday data, check if the data spans more than one day\n    if avg_days < 0.33:\n        if (dates[-1] - dates[0]).days > 1:\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n\n    # If the data spans multiple years, use '%Y-%b-%d' format\n    if (dates[-1] - dates[0]).days > 365:\n        return '%Y-%b-%d'\n\n    # If the data spans multiple months, use '%b %d' format\n    if (dates[-1] - dates[0]).days > 30:\n        return '%b %d'\n\n    # Otherwise, use '%Y-%b-%d' format\n    return '%Y-%b-%d'\n", "CodeBleu_score": 29.384757998737417}]}
{"instance_id": "f9589bf4666d", "generation_result": [{"code": "def _mpf_to_rgba(c, alpha=None)\n    from mpmath import mp\n    from mpmath import mpf\n    from mpmath import mpmathify\n    from mpmath import mpf_to_rgba\n    from mpmath import mpf_to_rgb\n    from mpmath import mpf_to_rgba_int\n    from mpmath import mpf_to_rgb_int\n    from mpmath import mpf_to_rgba_uint8\n    from mpmath import mpf_to_rgb_uint8\n    from mpmath import mpf_to_rgba_float\n    from mpmath import mpf_to_rgb_float\n    from mpmath import mpf_to_rgba_uint16\n    from mpmath import mpf_to_rgb_uint16\n    from mpmath import mpf_to_rgba_uint32\n    from mpmath import mpf_to_rgb_uint32\n    from mpmath import mpf_to_rgba_uint64\n    from mpmath import mpf_to_rgb_uint64\n    from mpmath import mpf_to_rgba_int8\n    from mpmath import mpf_to_rgb_int8\n    from mpmath import mpf_to_rgba_int16\n    from mpmath import mpf_to_rgb_int16\n    from mpmath import mpf_to_rgba_int32\n    from mpmath import mpf_to_rgb_int32\n    from mpmath import mpf_to_rgba_int64\n    from mpmath import mpf_to_rgb_int64\n    from mpmath import mpf_to_rgba_float16\n    from mpmath import mpf_to_rgb_float16\n    from mpmath import mpf_to_rgba_float32\n    from mpmath import mpf_to_rgb_float32\n    from mpmath import mpf_to_rgba_float64\n    from mpmath import mpf_to_rgb_float64\n    from mpmath import mpf_to_rgba_float128\n    from mpmath import mpf_to_rgb_", "CodeBleu_score": 4.454983655130216}, {"code": "def _mpf_to_rgba(c, alpha=None)\n    from matplotlib.colors import to_rgba\n    if alpha is None:\n        return to_rgba(c)\n    else:\n        return to_rgba(c, alpha)\n", "CodeBleu_score": 13.866525887364059}, {"code": "def _mpf_to_rgba(c, alpha=None)\n    if alpha is None:\n        alpha = 1.0\n    if len(c) == 4:\n        return c\n    if len(c) == 3:\n        if np.max(c) <= 1.0:\n            return (c[0], c[1], c[2], alpha)\n        else:\n            return (c[0] / 255.0, c[1] / 255.0, c[2] / 255.0, alpha)\n    else:\n        raise ValueError('Invalid color input')", "CodeBleu_score": 24.500286139782013}]}
{"instance_id": "4202e378ab0b", "generation_result": [{"code": "def subplots(self,*args,**kwargs)\n    if 'style' in kwargs:\n        self._check_for_and_apply_style(kwargs['style'])\n        kwargs.pop('style')\n    elif not hasattr(self,'mpfstyle'):\n        self._check_for_and_apply_style(self.style)\n    result = self.figure.subplots(*args,**kwargs)\n    if isinstance(result,mpl.axes.Axes):\n        result.mpfstyle = self.mpfstyle\n    elif isinstance(result,np.ndarray):\n        for ax in result:\n            ax.mpfstyle = self.mpfstyle\n    else:\n        raise TypeError(\"subplots returned an unexpected type: {}\".format(type(result)))\n    return result", "CodeBleu_score": 37.69544014889858}, {"code": "def subplots(self,*args,**kwargs)\n    if 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        self._check_for_and_apply_style(kwargs.pop('style',None))\n    result = self.figure.subplots(*args,**kwargs)\n    if isinstance(result,Axes):\n        result.mpf_style = self.mpfstyle\n    elif isinstance(result,np.ndarray):\n        for ax in result.flatten():\n            ax.mpf_style = self.mpfstyle\n    elif isinstance(result,tuple):\n        for ax in result:\n            ax.mpf_style = self.mpfstyle\n    else:\n        raise TypeError(\"Unexpected type for subplots: {}\".format(type(result)))\n    return result", "CodeBleu_score": 41.81288782033621}, {"code": "def subplots(self,*args,**kwargs)\n    if 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        self._check_for_and_apply_style(kwargs.pop('style',None))\n    if hasattr(self,'mpfstyle'):\n        kwargs['style']=self.mpfstyle\n    result=plt.subplots(*args,**kwargs)\n    if isinstance(result,tuple) and len(result)==2:\n        if isinstance(result[1],np.ndarray):\n            for ax in result[1]:\n                ax.set_mpfstyle(self.mpfstyle)\n        elif isinstance(result[1],plt.Axes):\n            result[1].set_mpfstyle(self.mpfstyle)\n        else:\n            raise TypeError(\"Unexpected type of result: {}\".format(type(result[1])))\n    return result", "CodeBleu_score": 42.37497817566374}]}
{"instance_id": "3e2b6dfe4753", "generation_result": [{"code": "def _adjust_figsize(fig,config)\n    if not fig:\n        return\n    if 'figsize' not in config:\n        fig.set_size_inches(fig.get_size_inches() * config['figsize_multiplier'])\n        return\n    figsize = config['figsize']\n    if not isinstance(figsize, tuple):\n        figsize = (figsize, figsize / config['figsize_ratio'])\n    if figsize[0] < 0.01 or figsize[1] < 0.01:\n        fig.set_size_inches(fig.get_size_inches() * config['figsize_multiplier'])\n        return\n    if figsize[0] > 10 or figsize[1] > 10:\n        fig.set_size_inches(fig.get_size_inches() * config['figsize_multiplier'])\n        return\n    fig.set_size_inches(figsize)\n", "CodeBleu_score": 27.528990981748425}, {"code": "def _adjust_figsize(fig,config)\n    if not fig:\n        return\n    figsize = config.get(\"figsize\")\n    if figsize:\n        fig.set_size_inches(figsize)\n        return\n    figsize = fig.get_size_inches()\n    if not figsize:\n        return\n    ratio = figsize[0] / figsize[1]\n    if ratio < 0.5 or ratio > 2:\n        return\n    if ratio < 1:\n        figsize = (figsize[0], figsize[0] / ratio)\n    else:\n        figsize = (figsize[1] * ratio, figsize[1])\n    fig.set_size_inches(figsize)\n", "CodeBleu_score": 29.785935018974662}, {"code": "def _adjust_figsize(fig,config)\n    if not fig:\n        return\n    if 'figsize' not in config:\n        fig_ratio = config['fig_ratio']\n        fig_width = fig.get_size_inches()[0]\n        fig_height = fig.get_size_inches()[1]\n        if fig_ratio is not None:\n            fig_ratio = np.array(fig_ratio)\n            fig_ratio = fig_ratio / np.sum(fig_ratio)\n            fig_ratio = fig_ratio * np.array([fig_width, fig_height])\n            fig.set_size_inches(fig_ratio)\n        else:\n            fig.set_size_inches(fig_width, fig_height)\n    else:\n        fig.set_size_inches(config['figsize'])\n", "CodeBleu_score": 33.26013784345931}]}
{"instance_id": "986e5804b941", "generation_result": [{"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    if apmav is None:\n        apmav = config['mav']\n    if apwidth is None:\n        apwidth = config['mavwidth']\n    if isinstance(apmav,dict):\n        apmav = list(apmav.keys())\n    if len(apmav) > 7:\n        apmav = apmav[:7]\n    mavprices = []\n    for mav in apmav:\n        mavprices.append(prices.rolling(config['mav'][mav]['period']).mean().shift(config['mav'][mav]['shift']))\n    for mav in range(len(apmav)):\n        ax.plot(xdates,mavprices[mav],linewidth=apwidth,color=config['mavcolors'][mav],label=config['mav'][apmav[mav]]['label'])\n    return mavprices\n", "CodeBleu_score": 28.39050494837358}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    if apmav is None:\n        apmav = config.get('mav')\n    if apwidth is None:\n        apwidth = config.get('mavwidth')\n    if isinstance(apmav, dict):\n        apmav = [(apmav.get(k, None), k) for k in sorted(apmav.keys())]\n        apmav = [(shift, period, label) for shift, period in apmav if shift is not None and period is not None]\n        apmav = [(shift, period, label) for shift, period, label in apmav if label is not None]\n        apmav = apmav[:7]\n    else:\n        apmav = [(None, apmav, None)]\n    mavprices = []\n    for shift, period, label in apmav:\n        if shift is not None and period is not None:\n            mavprices.append(prices.shift(shift).rolling(period).mean())\n        else:\n            mavprices.append(prices.rolling(period).mean())\n    for i, mavprice in enumerate(mavprices):\n        if label is None:\n            label = f'MAV{i+1}'\n        ax.plot(xdates, mavprice, label=label, linewidth=apwidth, color=config.get('colorcycle')[i])\n    return mavprices\n", "CodeBleu_score": 36.48880398609872}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    # Get the moving averages to plot from the config or apmav parameter\n    if apmav is None:\n        mavs = config['mav']\n    else:\n        mavs = apmav\n    # Determine the number of moving averages to plot\n    n_mavs = len(mavs)\n    if n_mavs > 7:\n        n_mavs = 7\n    # Create a list to store the calculated moving average prices\n    mav_prices = []\n    # Loop through the moving averages\n    for i in range(n_mavs):\n        # Get the current moving average\n        mav = mavs[i]\n        # If the moving average is specified as a dictionary, extract the shift and period values\n        if isinstance(mav, dict):\n            shift = mav['shift']\n            period = mav['period']\n        # Otherwise, use the default shift and period values from the config\n        else:\n            shift = config['shift']\n            period = mav\n        # Calculate the rolling mean of the prices using the current period\n        mav_price = prices.rolling(period).mean()\n        # If a shift value is specified, shift the moving average prices by the specified number of periods\n        if shift is not None:\n            mav_price = mav_price.shift(shift)\n        # Append the calculated moving average prices to the list\n        mav_prices.append(mav_price)\n        # Plot the moving average on the axis using the specified line width and color cycle from the config\n        ax.plot(xdates, mav_price, linewidth=apwidth, color=config['colors'][i], label=config['mav_labels'][i])\n    # Return the list of calculated moving average prices\n    return mav_prices", "CodeBleu_score": 35.85848981736962}]}
{"instance_id": "5308e20f3c90", "generation_result": [{"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n    kernel_height = inputs[0].shape[2]\n    kernel_width = inputs[0].shape[3]\n    num_input_channels = inputs[0].shape[1]\n    num_output_elements = outputs[0].shape[0] * outputs[0].shape[1] * outputs[0].shape[2] * outputs[0].shape[3]\n    flops = num_output_elements * num_input_channels * kernel_height * kernel_width\n    if opnode.bias is not None:\n        flops += num_output_elements\n    return flops\n\n", "CodeBleu_score": 19.24377845351419}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n    kernel_h, kernel_w = inputs[0].shape[-2:]\n    n_in = inputs[0].shape[1]\n    n_out = outputs[0].shape[1]\n    flops = n_out * n_in * kernel_h * kernel_w\n    if opnode.bias is not None:\n        flops += n_out\n    return flops\n\n", "CodeBleu_score": 17.81295666678328}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n    kh = inputs[0].shape[2]\n    kw = inputs[0].shape[3]\n    nc = inputs[0].shape[1]\n    n = outputs[0].shape[0]\n    nk = outputs[0].shape[1]\n    oh = outputs[0].shape[2]\n    ow = outputs[0].shape[3]\n    flops = n * nk * oh * ow * nc * kh * kw\n    if opnode.bias is not None:\n        flops += n * nk * oh * ow\n    return flops\n\n", "CodeBleu_score": 22.82032461514063}]}
{"instance_id": "70811d3cf578", "generation_result": [{"code": "def mge_dtype_to_ir_type(mge_dtype)\n    mge_dtype = np.dtype(mge_dtype)\n    assert mge_dtype.kind == 'f', f'mge_dtype_to_ir_type only support float type, but got {mge_dtype}'\n    return _dtype_to_ir_type[mge_dtype.type](mge_dtype)\n", "CodeBleu_score": 26.2588756252305}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype]()", "CodeBleu_score": 20.149569944707952}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype]()\n", "CodeBleu_score": 20.149569944707952}]}
{"instance_id": "a108c9ba2c22", "generation_result": [{"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = x.astype(ir.numpy_dtype_to_ir_type(x.dtype))\n    if x.dtype == np.bool_:\n        x = x.astype(np.int8)\n    elif x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n    x = np.ascontiguousarray(x)\n    attr = ir.DenseElementsAttr.get(\n        ir.RankedTensorType.get_from_shape_and_element_type(\n            x.shape, ir.numpy_dtype_to_ir_type(x.dtype)\n        ),\n        x.ravel(),\n    )\n    return [ir.ConstantOp(attr)]\n\n", "CodeBleu_score": 47.32337912381615}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = x.astype(ir.dtype_to_ir_type(x.dtype))\n    if x.dtype == np.bool_:\n        return [\n            ir.ConstantOp.get_bool_array(\n                x.shape,\n                np.unpackbits(x.astype(np.uint8), axis=None, bitorder=\"little\"),\n            )\n        ]\n    if x.dtype == np.bfloat16:\n        return [\n            ir.ConstantOp.get_bf16_array(\n                x.shape,\n                np.unpackbits(\n                    x.astype(np.uint16), axis=None, bitorder=\"little\", little_endian=True\n                ),\n            )\n        ]\n    return [\n        ir.ConstantOp.get_dense_elements_array(\n            x.shape,\n            np.ascontiguousarray(x, dtype=ir.dtype_to_ir_type(x.dtype)),\n        )\n    ]", "CodeBleu_score": 33.166631087636965}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = x.astype(np.float32)\n    if x.dtype == np.bool_:\n        x = x.astype(np.uint8)\n        dtype = ir.PrimitiveType.U8\n    elif x.dtype == np.bfloat16:\n        x = x.astype(np.float32)\n        dtype = ir.PrimitiveType.F32\n    else:\n        dtype = dtype_to_ir_type(x.dtype)\n    x = np.ascontiguousarray(x)\n    attrs = {\"dense_elements\": x.tobytes()}\n    return [hlo_module.add_constant(x.shape, dtype, attrs)]", "CodeBleu_score": 42.47146447214514}]}
{"instance_id": "3345fe4d008b", "generation_result": [{"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if val.strides[0] == 0 and val.size > 0:\n        # Identify the zero-stride and non-zero-stride axes\n        zero_stride_axes = [i for i, stride in enumerate(val.strides) if stride == 0]\n        non_zero_stride_axes = [i for i, stride in enumerate(val.strides) if stride != 0]\n\n        # Collapse the array along the zero-stride axes\n        collapsed_val = np.reshape(val, (val.shape[0], -1))\n\n        # Optionally canonicalize the data type\n        if canonicalize_types:\n            collapsed_val = collapsed_val.astype(ir.f32)\n\n        # Create a broadcast operation for the collapsed array\n        collapsed_val_ir = _numpy_array_constant(collapsed_val, canonicalize_types)\n        broadcast_op = ir.Broadcast(\n            collapsed_val_ir,\n            shape=val.shape,\n            non_zero_stride_axes=non_zero_stride_axes,\n        )\n        return broadcast_op.results\n    else:\n        return _numpy_array_constant(val, canonicalize_types)", "CodeBleu_score": 37.724393835098226}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if val.strides[0] == 0 and val.size > 0:\n        # If the array has zero strides, we need to collapse it along the\n        # zero-stride axes.\n        collapsed_shape = tuple(\n            s // val.strides[i] for i, s in enumerate(val.shape) if val.strides[i] != 0\n        )\n        collapsed_array = val.reshape(collapsed_shape)\n        if canonicalize_types:\n            collapsed_array = collapsed_array.astype(np.float32)\n\n        # Create a broadcast operation for the collapsed array, matching the\n        # original shape and non-zero-stride axes.\n        return [\n            ir.Broadcast(\n                ir.Constant(collapsed_array),\n                ir.Shape(val.shape),\n                axes=tuple(\n                    i\n                    for i, s in enumerate(val.strides)\n                    if s != 0 and val.shape[i] != 1\n                ),\n            )\n        ]\n    else:\n        return _numpy_array_constant(val, canonicalize_types)", "CodeBleu_score": 35.18858951014119}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if val.strides.any() == 0:\n        if val.size == 0:\n            return [ir.Constant(ir.Array(dtype=val.dtype, values=[]))]\n        zero_stride_axes = np.where(val.strides == 0)[0]\n        collapsed_val = val.reshape(val.shape[zero_stride_axes]).copy()\n        if canonicalize_types:\n            collapsed_val = collapsed_val.astype(np.float32)\n        non_zero_stride_axes = np.delete(\n            np.arange(val.ndim), zero_stride_axes\n        )\n        return [\n            ir.Broadcast(\n                ir.Constant(\n                    ir.Array(\n                        dtype=collapsed_val.dtype, values=collapsed_val\n                    )\n                ),\n                shape=val.shape,\n                axes=non_zero_stride_axes,\n            )\n        ]\n    return _numpy_array_constant(val, canonicalize_types)\n\n", "CodeBleu_score": 38.55011202704717}]}
{"instance_id": "91b298678351", "generation_result": [{"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    if len(xs) == 1:\n        return ir.DenseElementsAttr.get_scalar_bool(xs[0])\n    bits = np.packbits(xs, axis=None, bitorder=\"little\")\n    return ir.DenseElementsAttr.get_vector_raw(bits, ir.IntegerType.get_signless(1))\n\n", "CodeBleu_score": 40.61388109310426}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    if len(xs) == 0:\n        return ir.DenseElementsAttr.get_from_elements(ir.IntegerType.get_signless(1), [])\n    if len(xs) == 1:\n        return ir.DenseElementsAttr.get_from_elements(ir.IntegerType.get_signless(1), [int(xs[0])])\n    bits = []\n    for x in xs:\n        bits.append(int(x))\n    return ir.DenseElementsAttr.get_from_elements(ir.IntegerType.get_signless(1), bits)", "CodeBleu_score": 32.51523847461345}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    if len(xs) == 0:\n        return None\n    if len(xs) == 1:\n        return ir.DenseElementsAttr.get_from_b1(xs[0])\n    bits = [x for x in xs]\n    while len(bits) % 8 != 0:\n        bits.append(False)\n    packed_bits = [0] * (len(bits) // 8)\n    for i, bit in enumerate(bits):\n        byte_index = i // 8\n        bit_index = i % 8\n        if bit:\n            packed_bits[byte_index] |= 1 << bit_index\n    return ir.DenseElementsAttr.get_from_b1_array(packed_bits)\n\n", "CodeBleu_score": 32.82111099893862}]}
{"instance_id": "06ebabf1e0ff", "generation_result": [{"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    assert len(args) == 1\n    assert len(ctx.vars_in) == 1\n    assert len(ctx.vars_out) == 1\n    input_var = ctx.vars_in[0]\n    output_var = ctx.vars_out[0]\n    assert input_var.has_bound_data()\n    assert output_var.has_bound_data()\n    assert input_var.shape == output_var.shape\n    if output_var.has_bound_data():\n        data = output_var.get_bound_data().numpy().astype(output_var.data_type)\n    elif input_var.has_bound_data():\n        data = input_var.get_bound_data().numpy().astype(input_var.data_type)\n    else:\n        assert False, \"only tensor creation from constants is supported\"\n    return HLOTensor(data, output_var.shape, output_var.data_type)", "CodeBleu_score": 48.67764492344433}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    assert len(args) == len(ctx.vars_in) == len(ctx.vars_out) == 1\n    in_var, out_var = ctx.vars_in[0], ctx.vars_out[0]\n    assert in_var.bound_data is not None or out_var.bound_data is not None\n    if in_var.bound_data is not None:\n        in_var.data = in_var.bound_data\n        out_var.data = in_var.bound_data\n    elif out_var.bound_data is not None:\n        in_var.data = out_var.bound_data\n        out_var.data = out_var.bound_data\n    else:\n        raise AssertionError(\"only tensor creation from constants is supported\")\n    assert in_var.shape == out_var.shape\n    if out_var.data_type == np.float32:\n        return HLOTensor(out_var.data.astype(np.float32))\n    elif out_var.data_type == np.int32:\n        return HLOTensor(out_var.data.astype(np.int32))\n    else:\n        raise AssertionError(\"unsupported data type\")\n", "CodeBleu_score": 47.60809061228183}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    assert len(args) == len(ctx.vars_in) == len(ctx.vars_out) == 1\n    assert len(ctx.vars_in) == len(ctx.vars_out) == 1\n    in_var, out_var = ctx.vars_in[0], ctx.vars_out[0]\n    assert in_var.bound_data is not None or out_var.bound_data is not None\n    assert in_var.shape == out_var.shape\n    if in_var.bound_data is not None:\n        data = np.array(in_var.bound_data, dtype=out_var.data_type)\n        out_var.set_bound_data(data)\n    elif out_var.bound_data is not None:\n        data = np.array(out_var.bound_data, dtype=in_var.data_type)\n        in_var.set_bound_data(data)\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported.\")\n    return HLOTensor(out_var.bound_data, out_var.data_type, out_var.shape)\n\n", "CodeBleu_score": 53.648482798399286}]}
{"instance_id": "6cd750fa0c8d", "generation_result": [{"code": "def _device_put_scalar(x, device)\n    if isinstance(x, (int, float)):\n        if isinstance(x, float):\n            dtype = np.float32\n        else:\n            dtype = np.int32\n        x = np.array([x], dtype=dtype)\n    return _device_put(x, device)\n", "CodeBleu_score": 31.89098942743095}, {"code": "def _device_put_scalar(x, device)\n    if isinstance(x, (int, float, complex)):\n        x = np.array([x], dtype=_get_dtype(x))\n    return _device_put(x, device)\n\n", "CodeBleu_score": 30.952819089108385}, {"code": "def _device_put_scalar(x, device)\n    if not isinstance(x, (int, float, complex)):\n        raise TypeError(f\"Cannot transfer object of type {type(x)} to device {device}\")\n    if isinstance(x, (int, float)):\n        x = np.array([x], dtype=_dtype_to_numpy_dtype(x))\n    elif isinstance(x, complex):\n        x = np.array([x.real, x.imag], dtype=_dtype_to_numpy_dtype(x))\n    return _device_put(x, device)\n", "CodeBleu_score": 38.309134935260666}]}
{"instance_id": "140de7c2b6a8", "generation_result": [{"code": "def _shape_equal(lhs_shape, rhs_shape)\n    lhs_shape = list(lhs_shape)\n    rhs_shape = list(rhs_shape)\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if not lhs_shape and not rhs_shape:\n        return True\n    assert isinstance(lhs_shape[0], int)\n    assert isinstance(rhs_shape[0], int)\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for (lhs, rhs) in zip(lhs_shape, rhs_shape):\n        if lhs != rhs:\n            return False\n    return True\n\n", "CodeBleu_score": 33.00710607479818}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = list(lhs_shape)\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = list(rhs_shape)\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    assert all(isinstance(x, int) for x in lhs_shape)\n    assert all(isinstance(x, int) for x in rhs_shape)\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for lhs_dim, rhs_dim in zip(lhs_shape, rhs_shape):\n        if lhs_dim != rhs_dim:\n            return False\n    return True\n", "CodeBleu_score": 48.34182170161176}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, (tuple, list))\n    assert isinstance(rhs_shape, (tuple, list))\n    if not lhs_shape and not rhs_shape:\n        return True\n    assert isinstance(lhs_shape[0], int)\n    assert isinstance(rhs_shape[0], int)\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for lhs_dim, rhs_dim in zip(lhs_shape, rhs_shape):\n        if lhs_dim != rhs_dim:\n            return False\n    return True\n", "CodeBleu_score": 45.371678253364294}]}
{"instance_id": "55d0c71a259b", "generation_result": [{"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n    training_set = pd.read_csv(training_set_filename, dtype=dtypes)\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes)\n    training_set_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes\n    )\n    test_set_metadata = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    temp = meta_dtypes[\"target\"]\n    del meta_dtypes[\"target\"]\n    test_set_metadata = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    meta_dtypes[\"target\"] = temp\n    return training_set, test_set, training_set_metadata, test_set_metadata", "CodeBleu_score": 32.40170953920195}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\n    # Load the training set\n    training_set = pd.read_csv(training_set_filename, dtype=dtypes)\n\n    # Load the test set\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes)\n\n    # Load the training metadata\n    training_set_metadata = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n\n    # Load the test metadata\n    test_set_metadata = pd.read_csv(test_set_filename, dtype=meta_dtypes)\n\n    # Remove the 'target' column type from the metadata dictionary\n    del meta_dtypes[\"target\"]\n\n    # Load the test metadata without the 'target' column\n    test_set_metadata = pd.read_csv(test_set_filename, dtype=meta_dtypes)\n\n    # Restore the 'target' column type in the metadata dictionary\n    meta_dtypes[\"target\"] = \"int64\"\n\n    # Return the four DataFrames\n    return training_set, test_set, training_set_metadata, test_set_metadata", "CodeBleu_score": 30.647223064424654}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n    training_set = pd.read_csv(\n        training_set_filename,\n        dtype=dtypes,\n    )\n    test_set = pd.read_csv(\n        test_set_filename,\n        dtype=dtypes,\n    )\n    training_metadata = pd.read_csv(\n        training_set_metadata_filename,\n        dtype=meta_dtypes,\n    )\n    test_metadata = pd.read_csv(\n        test_set_metadata_filename,\n        dtype=meta_dtypes,\n    )\n    test_metadata.drop(\"target\", axis=1, inplace=True)\n    test_metadata[\"target\"] = training_metadata[\"target\"]\n    return training_set, test_set, training_metadata, test_metadata\n\n", "CodeBleu_score": 28.740915527081512}]}
{"instance_id": "9e5e02cd73e0", "generation_result": [{"code": "def _show_cmap(cmap)\n    from rcmod import axes_style\n    import matplotlib.pyplot as plt\n    import numpy as np\n    fig, ax = plt.subplots(figsize=(10, 1))\n    ax.set_axis_off()\n    ax.set_aspect('equal')\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.pcolormesh(np.linspace(0, 1, 100), np.linspace(0, 1, 100), np.linspace(0, 1, 100)[:, None], cmap=cmap)\n    plt.show()\n", "CodeBleu_score": 34.174359509327246}, {"code": "def _show_cmap(cmap)\n    from rcmod import axes_style\n    import matplotlib.pyplot as plt\n    import numpy as np\n    fig, ax = plt.subplots(figsize=(10, 2))\n    ax.set_title(f'{cmap.name} colormap', fontsize=16)\n    ax.set_axis_off()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect('equal')\n    ax.pcolormesh(np.linspace(0, 1, 256).reshape(1, 256), cmap=cmap)\n    plt.show()\n\n", "CodeBleu_score": 31.323880986865436}, {"code": "def _show_cmap(cmap)\n    from matplotlib.colors import LinearSegmentedColormap\n    from rcmod.axes_style import white\n    import matplotlib.pyplot as plt\n    import numpy as np\n    white()\n    fig, ax = plt.subplots(figsize=(10, 1))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    x = np.linspace(0, 1, 100)\n    y = np.zeros_like(x)\n    ax.pcolormesh(x, y, np.transpose(np.vstack([x, y])), cmap=cmap)\n    plt.show()", "CodeBleu_score": 41.05055079515406}]}
{"instance_id": "ad0781b5cc38", "generation_result": [{"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    if obj.axes is not None and obj.axes.get_transform() != obj.get_transform():\n        transform = obj.axes.transData + obj.get_transform().inverted()\n        x, y = transform.transform(xdata, ydata)\n    else:\n        x, y = xdata, ydata\n    return x, y\n", "CodeBleu_score": 48.98525126314899}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    if obj.axes is None:\n        return xdata, ydata\n    if obj.axes.get_xaxis_transform() != obj.axes.get_data_transform():\n        xdata, ydata = obj.axes.transData.inverted().transform(\n            np.vstack((xdata, ydata)).T\n        )\n    return obj.transAxes.transform(np.vstack((xdata, ydata)).T)", "CodeBleu_score": 47.37284174208176}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    if obj.axes is None:\n        return xdata, ydata\n    if obj.axes.get_transform() != obj.axes.figure.transData:\n        xdata, ydata = obj.axes.transData.inverted().transform(\n            (xdata, ydata))\n    return obj.transData.transform(xdata, ydata)\n\n", "CodeBleu_score": 42.391024169438886}]}
{"instance_id": "bf776cadef8d", "generation_result": [{"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data, 'data')\n    if data2 is not None and not is_list_like(data2):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)\n", "CodeBleu_score": 56.13783295127991}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data, 'data')\n    if data2 is not None and not is_list_like(data2):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)\n", "CodeBleu_score": 63.16056466555455}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data, 'data')\n    if data2 is not None and not is_list_like(data2):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)", "CodeBleu_score": 56.13783295127991}]}
{"instance_id": "fb44964174d9", "generation_result": [{"code": "def plot_most_used_words(df)\n    df = df.replace('programa', 'programar')\n    df = df[df['token'].str.isalpha() & df['token'].str.len() > 1 & ~df['token'].isin(stopwords)]\n    df = df.groupby('token').size().reset_index(name='count').sort_values(by='count', ascending=False)\n    df = df.head(20)\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x='token', y='count', data=df)\n    plt.title('Most Used Words')\n    plt.xlabel('Word')\n    plt.ylabel('Count')\n    plt.savefig('most_used_words.png')\n    plt.show()\n", "CodeBleu_score": 35.42570756744359}, {"code": "def plot_most_used_words(df)\n    df = df.replace(\"programa\", \"programar\")\n    df = df[df[\"token\"].str.isalpha()]\n    df = df[df[\"token\"].str.len() > 1]\n    df = df[~df[\"token\"].isin(stopwords)]\n    df = df.groupby(\"token\").size().reset_index(name=\"count\")\n    df = df.sort_values(\"count\", ascending=False)\n    df = df.head(20)\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=\"count\", y=\"token\", data=df)\n    plt.xlabel(\"Frequency\")\n    plt.ylabel(\"Word\")\n    plt.title(\"Most Frequent Words\")\n    plt.savefig(\"most_used_words.png\")\n    plt.show()", "CodeBleu_score": 38.203248961493394}, {"code": "def plot_most_used_words(df)\n    # Correct the word \"programa\" to \"programar\" in the dataframe\n    df.loc[df['token'] == 'programa', 'token'] = 'programar'\n\n    # Filter the dataframe to include only alphabetic tokens that are longer than one character and are not stop words\n    df = df[df['token'].str.isalpha() & df['token'].str.len() > 1 & ~df['token'].isin(stopwords)]\n\n    # Count the occurrences of these words\n    word_counts = df['token'].value_counts()\n\n    # Select the top 20 words\n    top_words = word_counts.head(20)\n\n    # Create a bar plot showing their frequency\n    top_words.plot(kind='bar')\n\n    # Label the plot\n    plt.xlabel('Word')\n    plt.ylabel('Frequency')\n    plt.title('Most Frequent Words in the Data')\n\n    # Save the plot as an image file\n    plt.savefig('most_used_words.png')\n\n    # Show the plot\n    plt.show()", "CodeBleu_score": 26.02045542272559}]}
{"instance_id": "13414a7eb4e3", "generation_result": [{"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    import bottleneck as bn\n    import numpy as np\n    import dask\n    a = da.asarray(a)\n    depth = (window - 1) // 2\n    if min_count is None:\n        min_count = window\n    if a.ndim == 0:\n        return moving_func(a)\n    if a.ndim == 1:\n        if axis == 0:\n            if window > a.size:\n                return moving_func(a)\n            else:\n                fill_value = a[0]\n                ag = da.overlap.overlap(a, depth=depth, boundary=fill_value)\n                return da.map_blocks(moving_func, ag, window, min_count=min_count, dtype=a.dtype)\n        else:\n            if window > a.size:\n                return moving_func(a)\n            else:\n                fill_value = a[-1]\n                ag = da.overlap.overlap(a, depth=depth, boundary=fill_value)\n                return da.map_blocks(moving_func, ag, window, min_count=min_count, dtype=a.dtype)\n    if a.ndim == 2:\n        if axis == 0:\n            if window > a.shape[0]:\n                return moving_func(a)\n            else:\n                fill_value = a[0, :]\n                ag = da.overlap.overlap(a, depth=depth, boundary=fill_value)\n                return da.map_blocks(moving_func, ag, window, min_count=min_count, dtype=a.dtype)\n        elif axis == 1:\n            if window > a.shape[1]:\n                return moving_func(a)\n            else:\n                fill_value = a[:, 0]\n                ag = da.overlap.overlap(a, depth=depth, boundary=fill_value)\n                return da.map_blocks(moving_func, ag, window, min_count=min_count, dtype=a.dtype)\n        else:\n            raise ValueError", "CodeBleu_score": 33.68765726382779}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    import numpy as np\n    from dask.array.utils import assert_axis_in_bounds\n    from dask.array.overlap import overlap, trim_internal\n    assert_axis_in_bounds(a, axis)\n    if min_count is None:\n        min_count = window\n    depth = (window - 1) // 2\n    if a.ndim == 0:\n        return moving_func(a.compute(), window, min_count)\n    if a.ndim == 1:\n        if axis == 0:\n            a = a.reshape(1, -1)\n        elif axis == 1:\n            a = a.reshape(-1, 1)\n        else:\n            raise ValueError(\"Invalid axis\")\n    if a.ndim == 2:\n        if axis == 0:\n            a = a.reshape(1, -1, a.shape[1])\n        elif axis == 1:\n            a = a.reshape(a.shape[0], 1, -1)\n        else:\n            raise ValueError(\"Invalid axis\")\n    else:\n        raise ValueError(\"Invalid number of dimensions\")\n    if a.dtype == np.dtype(\"O\"):\n        raise ValueError(\"Object dtype not supported\")\n    if a.dtype == np.dtype(\"?\"):\n        a = a.astype(np.bool_)\n    if a.dtype.kind == \"f\":\n        fill_value = np.nan\n    elif a.dtype.kind == \"i\":\n        fill_value = np.iinfo(a.dtype).min\n    else:\n        raise ValueError(\"Unsupported dtype\")\n    ag = overlap(a, depth=depth, boundary=fill_value)\n    result = da.map_blocks(moving_func, ag, window, min_count,\n                           dtype=a.dtype, chunks=a.chunks,\n                           drop_axis=axis, new_axis=axis)\n    return trim_internal(result, depth)", "CodeBleu_score": 36.27039960878589}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    import numpy as np\n    import bottleneck as bn\n    if min_count is None:\n        min_count = window\n    depth = a.shape[axis] - window + 1\n    if depth > 0:\n        if a.dtype.kind in 'iu':\n            fill_value = np.iinfo(a.dtype).min\n        elif a.dtype.kind in 'f':\n            fill_value = np.finfo(a.dtype).min\n        else:\n            fill_value = np.iinfo(np.uint8).min\n        ag = da.overlap.overlap(a, depth, boundary='reflect', trim=False,\n                                boundary_fill_value=fill_value)\n        result = da.map_blocks(moving_func, ag, window, min_count,\n                               trim=False, dtype=a.dtype)\n        result = da.overlap.trim_internal(result, depth, axis=axis)\n        return result\n    else:\n        return moving_func(a, window, min_count)", "CodeBleu_score": 36.45502289177793}]}
{"instance_id": "68ce25d325d4", "generation_result": [{"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    pass\n\n", "CodeBleu_score": 1.37987012987013}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    pass\n\n", "CodeBleu_score": 1.37987012987013}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    pass\n\n", "CodeBleu_score": 1.37987012987013}]}
{"instance_id": "c0c2c53b76e0", "generation_result": [{"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)", "CodeBleu_score": 70.27898286466454}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 70.27898286466454}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)", "CodeBleu_score": 70.27898286466454}]}
{"instance_id": "6559019d09a0", "generation_result": [{"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n    if isinstance(self.data, DaskArray):\n        return self.data.__dask_postpersist__()\n    raise AttributeError(\n        f\"{self.__class__.__name__} does not support post-persistence\"\n    )", "CodeBleu_score": 34.27873142755713}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n    if isinstance(self.data, DaskArray):\n        postpersist, args = self.data.__dask_postpersist__()\n        return (\n            lambda g, f, *a, **kw: self.finalize(g, f, *a, **kw),\n            (postpersist, args),\n        )\n    raise AttributeError(\"data is not a dask array\")", "CodeBleu_score": 47.61471936664121}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n    if isinstance(self.data, dask.array.Array):\n        finalize, args = self.data.__dask_postpersist__()\n        return (\n            lambda g, f, *a, **kw: self.finalize(\n                g,\n                f,\n                *a,\n                **kw,\n            ),\n            args,\n        )\n    raise AttributeError(\"No __dask_postpersist__ method for %r\" % self.data)", "CodeBleu_score": 46.75973814265767}]}
{"instance_id": "4a2801bb3851", "generation_result": [{"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args):\n            dask_module = import_optional_dependency(dask_module)\n            return getattr(dask_module, name)(*args, **kwargs)\n        return getattr(eager_module, name)(*args, **kwargs)\n    return f\n\n", "CodeBleu_score": 42.37876305054925}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args) or any(\n            is_duck_dask_array(val) for val in kwargs.values()\n        ):\n            dask_module = importlib.import_module(dask_module)\n            return getattr(dask_module, name)(*args, **kwargs)\n        else:\n            return getattr(eager_module, name)(*args, **kwargs)\n\n    return f", "CodeBleu_score": 48.745320337044326}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n    @wraps(getattr(eager_module, name))\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(arg) for arg in args) or any(is_duck_dask_array(val) for val in kwargs.values()):\n            dask_module = importlib.import_module(dask_module)\n            return getattr(dask_module, name)(*args, **kwargs)\n        else:\n            return getattr(eager_module, name)(*args, **kwargs)\n    return f\n", "CodeBleu_score": 47.87557290140077}]}
{"instance_id": "eef8d746d01e", "generation_result": [{"code": "def lazy_array_equiv(arr1, arr2)\n    import numpy as np\n    try:\n        import dask.array as da\n    except ImportError:\n        dask = None\n    if arr1 is arr2:\n        return True\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if dask is not None and isinstance(arr1, da.Array) and isinstance(arr2, da.Array):\n        return da.tokenize(arr1) == da.tokenize(arr2)\n    return None\n", "CodeBleu_score": 52.52756725064144}, {"code": "def lazy_array_equiv(arr1, arr2)\n    import numpy as np\n    try:\n        import dask\n    except ImportError:\n        dask = None\n    if arr1 is arr2:\n        return True\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if dask and isinstance(arr1, dask.array.Array) and isinstance(arr2, dask.array.Array):\n        return dask.tokenize(arr1, arr2) == dask.tokenize(arr2, arr1)\n    return None", "CodeBleu_score": 55.758600600406005}, {"code": "def lazy_array_equiv(arr1, arr2)\n    if arr1 is arr2:\n        return True\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if isinstance(arr1, da.Array) and isinstance(arr2, da.Array):\n        if da.tokenize(arr1) == da.tokenize(arr2):\n            return True\n        else:\n            return None\n    else:\n        return None", "CodeBleu_score": 59.395361388745336}]}
{"instance_id": "15e830ff5fcb", "generation_result": [{"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    import numpy as np\n    import dask.array as da\n    if chunks is not None:\n        if frac_nan is not None:\n            r = np.random.RandomState(seed)\n            n_nan = int(shape[0] * shape[1] * frac_nan)\n            nan_idx = r.choice(shape[0] * shape[1], n_nan, replace=False)\n            nan_idx = np.unravel_index(nan_idx, shape)\n            arr = np.zeros(shape)\n            arr[nan_idx] = np.nan\n            arr = da.from_array(arr, chunks=chunks)\n            return arr\n        else:\n            return da.random.normal(size=shape, chunks=chunks)\n    else:\n        if frac_nan is not None:\n            r = np.random.RandomState(seed)\n            n_nan = int(shape[0] * shape[1] * frac_nan)\n            nan_idx = r.choice(shape[0] * shape[1], n_nan, replace=False)\n            nan_idx = np.unravel_index(nan_idx, shape)\n            arr = np.zeros(shape)\n            arr[nan_idx] = np.nan\n            return arr\n        else:\n            return np.random.normal(size=shape)", "CodeBleu_score": 45.48245632184534}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    import numpy as np\n    import dask.array as da\n\n    if chunks:\n        rng = np.random.default_rng(seed)\n        arr = rng.standard_normal(shape)\n        if frac_nan:\n            mask = np.random.choice([False, True], size=shape, p=[1-frac_nan, frac_nan])\n            arr[mask] = np.nan\n        return da.from_array(arr, chunks=chunks)\n    else:\n        return np.random.default_rng(seed).standard_normal(shape)", "CodeBleu_score": 48.22198141894914}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    import numpy as np\n    import dask.array as da\n    if chunks is None:\n        if frac_nan is None:\n            return np.random.randn(*shape)\n        else:\n            arr = np.random.randn(*shape)\n            arr[np.random.rand(*shape) < frac_nan] = np.nan\n            return arr\n    else:\n        if frac_nan is None:\n            return da.random.randn(*shape, chunks=chunks, seed=seed)\n        else:\n            arr = da.random.randn(*shape, chunks=chunks, seed=seed)\n            arr[np.random.rand(*shape) < frac_nan] = np.nan\n            return arr\n", "CodeBleu_score": 34.469212983996066}]}
{"instance_id": "5b57abbbe38b", "generation_result": [{"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target", "CodeBleu_score": 81.06081450739325}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target", "CodeBleu_score": 81.06081450739325}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target", "CodeBleu_score": 81.06081450739325}]}
{"instance_id": "592e414671e9", "generation_result": [{"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    if name not in self.allowed_financial_statements:\n        raise ValueError(f\"Invalid financial statement: {name}\")\n    if timescale not in self.allowed_timescales:\n        raise ValueError(f\"Invalid timescale: {timescale}\")\n    try:\n        financials_table = self._create_financials_table(name, timescale, proxy)\n        return financials_table\n    except Exception as e:\n        logger.error(f\"Error fetching time series for {name}: {e}\")\n        return pd.DataFrame()\n", "CodeBleu_score": 39.51187043524031}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    if name not in self._allowed_financial_statements:\n        raise ValueError(\n            f\"Invalid financial statement: {name}. Must be one of {self._allowed_financial_statements}\"\n        )\n    if timescale not in self._allowed_timescales:\n        raise ValueError(\n            f\"Invalid timescale: {timescale}. Must be one of {self._allowed_timescales}\"\n        )\n    try:\n        table = self._create_financials_table(name, timescale, proxy=proxy)\n        return table\n    except Exception as e:\n        self._logger.error(f\"Error fetching time series for {name} {timescale}: {e}\")\n        return pd.DataFrame()", "CodeBleu_score": 39.40585204737698}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    if name not in self.financial_statements:\n        raise ValueError(f\"Invalid financial statement: {name}\")\n    if timescale not in self.financial_statements[name][\"timescale\"]:\n        raise ValueError(f\"Invalid timescale: {timescale}\")\n    try:\n        table = self._create_financials_table(name, timescale, proxy)\n    except Exception as e:\n        self.log.error(f\"Error creating financials table: {e}\")\n        return pd.DataFrame()\n    return table\n", "CodeBleu_score": 40.70088840536552}]}
{"instance_id": "af01d9bae091", "generation_result": [{"code": "def _parse_fund_ownership(self, data)\n    # Extract ownership data\n    ownership_data = data['fundOwnership']\n\n    # Initialize an empty list to store ownership details\n    ownership_details = []\n\n    # Iterate through each owner's data\n    for owner in ownership_data:\n        # Parse raw values and remove the 'maxAge' field\n        ownership_details.append({\n            'name': owner['name'],\n            'ownership': owner['ownership'],\n            'reportDate': owner['reportDate']\n        })\n\n    # Convert ownership details into a pandas DataFrame\n    ownership_df = pd.DataFrame(ownership_details)\n\n    # If the DataFrame is not empty\n    if not ownership_df.empty:\n        # Convert the 'reportDate' to a datetime format\n        ownership_df['reportDate'] = pd.to_datetime(\n            ownership_df['reportDate'], format='%Y-%m-%d')\n\n        # Rename specific columns for clarity\n        ownership_df = ownership_df.rename(columns={\n            'name': 'Owner Name',\n            'ownership': 'Ownership',\n            'reportDate': 'Report Date'\n        })\n\n    # Assign the DataFrame to the instance variable _mutualfund\n    self._mutualfund = ownership_df", "CodeBleu_score": 26.193313825850094}, {"code": "def _parse_fund_ownership(self, data)\n    self._mutualfund = pd.DataFrame(data['ownership']['ownership'])\n    self._mutualfund['reportDate'] = pd.to_datetime(self._mutualfund['reportDate'])\n    self._mutualfund.rename(columns={'reportDate': 'date', 'ownership': 'ownership_percentage'}, inplace=True)\n    self._mutualfund.drop(columns=['maxAge'], inplace=True)\n    return self._mutualfund\n\n", "CodeBleu_score": 21.26782494432019}, {"code": "def _parse_fund_ownership(self, data)\n    # Extract ownership data from the input data\n    ownership_data = data.get('ownership', {}).get('ownershipList', [])\n\n    # Initialize an empty list to store processed ownership details\n    processed_ownership_details = []\n\n    # Iterate through each owner's data\n    for ownership_detail in ownership_data:\n        # Extract raw values from the ownership detail\n        raw_values = ownership_detail.get('rawValues', {})\n\n        # Remove the 'maxAge' field from the raw values\n        if 'maxAge' in raw_values:\n            del raw_values['maxAge']\n\n        # Add the processed ownership detail to the list\n        processed_ownership_details.append(raw_values)\n\n    # Convert the ownership details into a pandas DataFrame\n    ownership_df = pd.DataFrame(processed_ownership_details)\n\n    # Convert the 'reportDate' to a datetime format\n    if 'reportDate' in ownership_df.columns:\n        ownership_df['reportDate'] = pd.to_datetime(ownership_df['reportDate'], format='%Y-%m-%d')\n\n    # Rename specific columns for clarity\n    if 'reportDate' in ownership_df.columns:\n        ownership_df.rename(columns={'reportDate': 'ownership_report_date'}, inplace=True)\n    if 'ownershipPercentage' in ownership_df.columns:\n        ownership_df.rename(columns={'ownershipPercentage': 'ownership_percentage'}, inplace=True)\n\n    # Assign the DataFrame to the instance variable\n    self._mutualfund.ownership = ownership_df", "CodeBleu_score": 35.728364999740755}]}
{"instance_id": "2b5c6f1253db", "generation_result": [{"code": "def currency(self)\n    if hasattr(self, \"currency\"):\n        return self.currency\n    else:\n        self.metadata = self.get_metadata(self.ticker)\n        self.currency = self.metadata[\"currency\"]\n        return self.currency", "CodeBleu_score": 39.13834087204795}, {"code": "def currency(self)\n    if self.currency:\n        return self.currency\n    else:\n        history = self.history(period='max')\n        self.currency = history.meta['currency']\n        return self.currency", "CodeBleu_score": 42.723505990774186}, {"code": "def currency(self)\n    if hasattr(self, '_currency'):\n        return self._currency\n    else:\n        self._currency = self._get_meta_data()['currency']\n        return self._currency", "CodeBleu_score": 37.190308869081726}]}
{"instance_id": "16b80be60e3b", "generation_result": [{"code": "def recommendations(self) -> pd.DataFrame\n    if self._recommendations is None:\n        try:\n            self._recommendations = self.proxy.recommendationTrend(self.ticker)\n        except Exception as e:\n            raise YFDataException(\"Error fetching recommendation data: \" + str(e))\n        if self._recommendations is None:\n            self._recommendations = pd.DataFrame()\n        else:\n            try:\n                self._recommendations = pd.DataFrame(self._recommendations)\n            except Exception as e:\n                raise YFDataException(\"Error parsing recommendation data: \" + str(e))\n    return self._recommendations", "CodeBleu_score": 45.02175990392594}, {"code": "def recommendations(self) -> pd.DataFrame\n    if self._recommendations is None:\n        try:\n            self._recommendations = self._proxy.recommendationTrend(\n                self._ticker\n            )\n        except Exception as e:\n            raise YFDataException(\n                f\"Error fetching recommendations for {self._ticker}: {e}\"\n            )\n    if self._recommendations is None:\n        self._recommendations = pd.DataFrame()\n    else:\n        try:\n            self._recommendations = pd.DataFrame(\n                self._recommendations[\"trend\"]\n            )\n        except Exception as e:\n            raise YFDataException(\n                f\"Error parsing recommendations for {self._ticker}: {e}\"\n            )\n    return self._recommendations", "CodeBleu_score": 41.387656232873525}, {"code": "def recommendations(self) -> pd.DataFrame\n    if self._recommendations is None:\n        try:\n            self._recommendations = self.proxy.recommendationTrend(self.ticker)\n        except Exception as e:\n            raise YFDataException(f\"Error fetching recommendations for {self.ticker}: {e}\")\n        if self._recommendations is None:\n            self._recommendations = pd.DataFrame()\n        else:\n            try:\n                self._recommendations = pd.DataFrame(self._recommendations)\n            except Exception as e:\n                raise YFDataException(f\"Error parsing recommendations for {self.ticker}: {e}\")\n    return self._recommendations\n\n", "CodeBleu_score": 42.96480872396158}]}
{"instance_id": "abaf8c2a3ecc", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n    if not hasattr(clf, 'n_clusters'):\n        raise AttributeError(\"Classifier must have n_clusters attribute.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(2, clf.n_clusters + 1)\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.set_ylabel('Clustering Duration (s)', fontsize=text_fontsize)\n        ax2.grid(False)\n\n    ax.set_xlabel('Number of Clusters', fontsize=text_fontsize)\n    ax.set_ylabel('SSE', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.grid()\n\n    sse = []\n    cluster_times = []\n    for n_clusters in cluster_ranges:\n        clf.n_clusters = n_clusters\n        t0 = time.time()\n        clf.fit(X)\n        t1 = time.time()\n        sse.append(clf.inertia_)\n        cluster_times.append(t1 - t0)\n\n    ax.plot(cluster_ranges, sse, 'b*-')\n    if show_cluster_time:\n        ax2.plot(cluster_ranges, cluster_times, 'r*-')\n\n    return ax", "CodeBleu_score": 47.80415931243201}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n    if not hasattr(clf, 'n_clusters'):\n        raise AttributeError(\"Classifier must have n_clusters attribute\")\n    if cluster_ranges is None:\n        cluster_ranges = range(2, clf.n_clusters + 1)\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Number of Clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Sum of Squared Errors\", fontsize=text_fontsize)\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.set_ylabel(\"Clustering Duration (s)\", fontsize=text_fontsize)\n    errors = []\n    times = []\n    for n_clusters in cluster_ranges:\n        clf.set_params(n_clusters=n_clusters)\n        start = time.time()\n        clf.fit(X)\n        end = time.time()\n        errors.append(clf.inertia_)\n        times.append(end - start)\n    ax.plot(cluster_ranges, errors, 'o-')\n    if show_cluster_time:\n        ax2.plot(cluster_ranges, times, 'o-')\n    return ax", "CodeBleu_score": 49.0518900223948}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\n    if not hasattr(clf, 'n_clusters'):\n        raise AttributeError(\n            \"The classifier object doesn't have a 'n_clusters' attribute\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, clf.n_clusters + 1)\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Number of clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Sum of squared errors\", fontsize=text_fontsize)\n\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.set_ylabel(\"Clustering duration [s]\", fontsize=text_fontsize)\n\n    sse = []\n    cluster_times = []\n    for n_clusters in cluster_ranges:\n        clf.set_params(n_clusters=n_clusters)\n        start_time = time.time()\n        clf.fit(X)\n        cluster_times.append(time.time() - start_time)\n        sse.append(clf.inertia_)\n\n    ax.plot(cluster_ranges, sse, 'b*-')\n\n    if show_cluster_time:\n        ax2.plot(cluster_ranges, cluster_times, 'r*-')\n        ax2.set_ylim(0, max(cluster_times) * 1.1)\n\n    return ax", "CodeBleu_score": 50.375154342518904}]}
{"instance_id": "8a90815c4ee1", "generation_result": [{"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    from time import time\n    start_time = time()\n    clf_ = clf.clone()\n    clf_.set_params(n_clusters=n_clusters)\n    clf_.fit(X)\n    return clf_.score(X), time() - start_time\n", "CodeBleu_score": 43.29956227662507}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    import time\n    start_time = time.time()\n    clf_clone = clone(clf)\n    clf_clone.set_params(n_clusters=n_clusters)\n    clf_clone.fit(X)\n    return clf_clone.inertia_, time.time() - start_time\n", "CodeBleu_score": 29.27160778436104}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    import time\n    start_time = time.time()\n    clf = clf.set_params(n_clusters=n_clusters)\n    clf.fit(X)\n    return clf.score(X), time.time() - start_time\n", "CodeBleu_score": 38.34738783591207}]}
{"instance_id": "da40bd5dd81a", "generation_result": [{"code": "def clustering_factory(clf)\n    required_methods = [\"fit\", \"fit_predict\"]\n    for method in required_methods:\n        if not hasattr(clf, method):\n            raise TypeError(f\"{clf.__class__.__name__} does not have method '{method}'\")\n    def plot_silhouette(X, labels, ax=None):\n        \"\"\"The function plot_silhouette takes three arguments: X, labels, and ax.\\nX is the data to be clustered.\\nlabels is the list of labels assigned to each data point.\\nax is the matplotlib axis object where the silhouette plot will be drawn.\\nThe function first imports the necessary libraries: numpy, matplotlib, and sklearn.metrics.\\nIt then calculates the silhouette scores for each data point and stores them in the variable silhouette_scores.\\nIt then calculates the average silhouette score for each cluster and stores them in the variable silhouette_avg.\\nIt then plots the silhouette scores for each cluster on the given axis.\\nIf no axis is provided, it creates a new figure and axis.\\nIt returns the axis object.\"\"\"\n        import numpy as np\n        import matplotlib.pyplot as plt\n        from sklearn.metrics import silhouette_samples, silhouette_score\n        silhouette_scores = silhouette_samples(X, labels)\n        silhouette_avg = silhouette_score(X, labels)\n        if ax is None:\n            fig, ax = plt.subplots()\n        ax.scatter(silhouette_scores, np.arange(len(silhouette_scores)), c=labels, cmap=\"viridis\", s=5)\n        ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n        ax.set_title(f\"Silhouette Plot (average={silhouette_avg:.2f})\")\n        ax.set_xlabel(\"Silhouette Score\")\n        ax.set_ylabel(\"Data Point\")\n        ax.set_yticks([])\n        return ax\n    def plot_elbow", "CodeBleu_score": 28.98062514907614}, {"code": "def clustering_factory(clf)\n    if not hasattr(clf, \"fit\"):\n        raise TypeError(\"Clusterer object must have a 'fit' method\")\n    if not hasattr(clf, \"fit_predict\"):\n        raise TypeError(\"Clusterer object must have a 'fit_predict' method\")\n\n    def plot_silhouette(X, labels, n_clusters=None, ax=None):\n        \"\"\"The function plot_silhouette takes in the following arguments:\\nX: A numpy array or pandas DataFrame containing the data points to be clustered.\\nlabels: A numpy array or pandas Series containing the cluster labels for each data point.\\nn_clusters: An integer representing the number of clusters in the data. If None, the number of clusters is inferred from the unique values in the labels array.\\nax: A matplotlib Axes object to plot the silhouette plot on. If None, a new Axes object is created.\\nThe function calculates the silhouette score for each data point and plots the silhouette plot using matplotlib.\\nIt returns the silhouette score as a float.\"\"\"\n        from sklearn.metrics import silhouette_score, silhouette_samples\n        import matplotlib.pyplot as plt\n        import numpy as np\n\n        if n_clusters is None:\n            n_clusters = len(np.unique(labels))\n\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        silhouette_avg = silhouette_score(X, labels)\n        sample_silhouette_values = silhouette_samples(X, labels)\n\n        y_lower = 10\n        for i in range(n_clusters):\n            ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n            ith_cluster_silhouette_values.sort()\n\n            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n            y_upper = y_lower + size_cluster_i\n\n            color = plt.cm.nipy_spectral(float(i) / n_clust", "CodeBleu_score": 29.32407969694416}, {"code": "def clustering_factory(clf)\n    if not hasattr(clf, 'fit') or not hasattr(clf, 'fit_predict'):\n        raise TypeError(\"The object is not a valid clusterer instance.\")\n\n    def plot_silhouette(X, y_pred, metric='euclidean', figsize=(10, 7)):\n        \"\"\"The function plot_silhouette takes four arguments: X, y_pred, metric, and figsize.\\nX is a dataframe or array-like object containing the features.\\ny_pred is a list or array-like object containing the predicted cluster labels for each data point.\\nmetric is a string indicating the distance metric to use for calculating the silhouette scores.\\nfigsize is a tuple indicating the size of the figure to be generated.\\nThe function first imports the necessary libraries: matplotlib.pyplot as plt, numpy as np, and silhouette_score from sklearn.metrics.\\nIt then calculates the silhouette scores for each data point using the silhouette_score function.\\nThe function then plots the silhouette scores using a bar plot.\\nThe x-axis shows the data points, and the y-axis shows the silhouette scores.\\nThe color of the bars indicates the cluster to which the data point belongs.\\nThe function also plots the average silhouette score for each cluster.\\nThe function returns the silhouette scores for each data point.\"\"\"\n        import matplotlib.pyplot as plt\n        import numpy as np\n        from sklearn.metrics import silhouette_score\n\n        silhouette_scores = silhouette_score(X, y_pred, metric=metric)\n        plt.figure(figsize=figsize)\n        plt.bar(range(len(X)), silhouette_scores)\n        plt.xlabel('Data Points')\n        plt.ylabel('Silhouette Score')\n        plt.title('Silhouette Scores')\n        plt.show()\n        return silhouette_scores\n\n    def plot_elbow_curve(X, max_clusters=10, metric='euclidean', figsize=(10, 7)):\n        \"\"\"The function plot_el", "CodeBleu_score": 24.254100423205244}]}
{"instance_id": "b8d8ba3c0b1b", "generation_result": [{"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    from sklearn.model_selection import learning_curve\n    from sklearn.utils import resample\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, train_sizes=train_sizes, n_jobs=n_jobs,\n        scoring=scoring)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 74.93966598867348}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    from sklearn.model_selection import learning_curve\n    from matplotlib import pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, train_sizes=train_sizes, n_jobs=n_jobs, scoring=scoring)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"Score\")\n\n    ax.grid()\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n    return ax", "CodeBleu_score": 74.56921953418333}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import learning_curve\n    from sklearn.model_selection import ShuffleSplit\n    import numpy as np\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    if train_sizes is None:\n        train_sizes = np.linspace(0.1, 1.0, 5)\n\n    if scoring is None:\n        scoring = 'accuracy'\n\n    if cv is None:\n        cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, train_sizes=train_sizes, scoring=scoring, n_jobs=n_jobs)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\")\n    ax.set_ylabel(\"Score\")\n\n    ax.grid()\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label", "CodeBleu_score": 71.5217957959092}]}
{"instance_id": "e6e76eb35945", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"Classifier must have n_clusters attribute\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n    classifiers = [type(clf)(n_clusters=k) for k in cluster_ranges]\n    scores = [abs(clf.fit(X).score(X)) for clf in classifiers]\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(cluster_ranges, scores, 'bx-')\n    ax.grid(True)\n    ax.set_xlabel('Number of clusters')\n    ax.set_ylabel('Score')\n    ax.set_title(title, fontsize=title_fontsize)\n    return ax", "CodeBleu_score": 48.32419693196059}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(f\"{clf.__class__.__name__} is not a clustering algorithm\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n\n    classifiers = [type(clf)(n_clusters=n) for n in cluster_ranges]\n    scores = [abs(clf.fit(X).score(X)) for clf in classifiers]\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    ax.plot(cluster_ranges, scores, 'bo-')\n    ax.grid(True)\n    ax.set_xlabel('Number of Clusters')\n    ax.set_ylabel('Score')\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n\n    return ax", "CodeBleu_score": 50.58010597366732}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"clf must have an n_clusters attribute\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11, 2)\n    classifiers = [type(clf)(n_clusters=n) for n in cluster_ranges]\n    scores = [abs(clf.fit(X).score(X)) for clf in classifiers]\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(cluster_ranges, scores, 'bx-')\n    ax.grid(True)\n    ax.set_xlabel('Number of clusters')\n    ax.set_ylabel('Score')\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n    return ax", "CodeBleu_score": 50.402137211722966}]}
{"instance_id": "6f4530c8a20f", "generation_result": [{"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap,\n               edgecolor='none', alpha=0.5, s=40)\n    ax.set_xlabel('first principal component', fontsize=text_fontsize)\n    ax.set_ylabel('second principal component', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.legend(loc=0, fontsize=text_fontsize)\n    return ax\n\n", "CodeBleu_score": 40.78795829710471}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    from sklearn.decomposition import PCA\n    import matplotlib.pyplot as plt\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap, edgecolor='none', alpha=0.5,\n               linewidth=0.5)\n    ax.set_xlabel('First principal component', fontsize=text_fontsize)\n    ax.set_ylabel('Second principal component', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.legend(handles=ax.lines[0:len(np.unique(y))])\n    return ax", "CodeBleu_score": 41.908543140892554}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    from sklearn.decomposition import PCA\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap,\n               edgecolor='none', alpha=0.5, s=40)\n\n    ax.set_xlabel('PC 1', fontsize=text_fontsize)\n    ax.set_ylabel('PC 2', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.legend(np.unique(y), fontsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 43.58710854697346}]}
{"instance_id": "e7c2775d024c", "generation_result": [{"code": "def barplot(ax)\n    ax.set_title('Bar plot')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_xlim(-1, 16)\n    ax.set_ylim(0, 10)\n    ax.set_xticks(range(16))\n    ax.set_xticklabels(range(16))\n    ax.set_yticks(range(1, 10))\n    ax.set_yticklabels(range(1, 10))\n    ax.grid(True)\n    ax.set_axisbelow(True)\n    ax.set_facecolor('white')\n    ax.set_frame_on(False)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)\n    ax.set_axisbelow(True)", "CodeBleu_score": 12.43870489264772}, {"code": "def barplot(ax)\n    import numpy as np\n    import matplotlib.patches as patches\n    import matplotlib.transforms as transforms\n\n    # Create two clipping paths\n    clip1 = patches.Rectangle((0, 0), 10, 10, transform=ax.transAxes)\n    clip2 = patches.Rectangle((0, 0), 10, 10, transform=ax.transAxes)\n\n    # Create data for 16 bars\n    Y1 = np.random.randint(1, 10, 16)\n    Y2 = np.random.randint(1, 10, 16)\n\n    # Plot four sets of bars\n    ax.bar(np.arange(1, 17), Y1, color='gray', alpha=0.5, clip_path=clip1)\n    ax.bar(np.arange(1, 17), Y2, color='red', alpha=0.5, clip_path=clip2)\n    ax.bar(np.arange(1, 17), Y1, color='gray', alpha=0.5, clip_path=clip2)\n    ax.bar(np.arange(1, 17), Y2, color='red', alpha=0.5, clip_path=clip1)\n\n    # Set x-axis limits\n    ax.set_xlim(-1, 16)\n\n    # Add two text labels\n    ax.text(0.5, 0.95, 'Bar plot', transform=ax.transAxes, ha='center', va='top', fontsize=16)\n    ax.text(0.5, 0.9, 'ax.bar()', transform=ax.transAxes, ha='center', va='top', fontsize=16, color='blue', fontfamily='monospace')\n\n    return ax\n", "CodeBleu_score": 40.96073328823119}, {"code": "def barplot(ax)\n    from matplotlib.path import Path\n    import matplotlib.patches as patches\n    import matplotlib.transforms as transforms\n    import numpy as np\n\n    # create two clipping paths\n    clip1 = Path([[0, 0], [0, 1], [1, 1], [1, 0]])\n    clip2 = Path([[0, 0], [0, 1], [1, 1], [1, 0]])\n\n    # create data for 16 bars\n    Y1 = np.random.uniform(0, 1, 16)\n    Y2 = np.random.uniform(0, 1, 16)\n\n    # plot four sets of bars on the provided axis\n    ax.bar(np.arange(1, 17), Y1, color='gray', alpha=0.5, clip_path=clip1)\n    ax.bar(np.arange(1, 17), Y2, color='red', alpha=0.5, clip_path=clip2)\n    ax.bar(np.arange(1, 17), Y1, color='gray', alpha=0.5, clip_path=clip1)\n    ax.bar(np.arange(1, 17), Y2, color='red', alpha=0.5, clip_path=clip2)\n\n    # set x-axis limits\n    ax.set_xlim(-1, 16)\n\n    # add two text labels below the plot\n    ax.text(0.5, -0.1, 'Bar plot', ha='center', va='center', transform=ax.transAxes)\n    ax.text(0.5, -0.15, 'ax.bar()', ha='center', va='center', transform=ax.transAxes, fontfamily='monospace', color='blue')\n\n    return ax\n\n", "CodeBleu_score": 42.179395194122776}]}
{"instance_id": "0ec1d146a32b", "generation_result": [{"code": "def split(n_segment)\n    # Calculate the width of each segment\n    width_segment = int(n_segment / 2)\n    # Calculate the padding between segments\n    padding_segment = int((n_segment - 1) / 2)\n    # Initialize the starting and ending positions of segments\n    X0 = []\n    X1 = []\n    # Loop through the number of segments\n    for i in range(n_segment):\n        # Calculate the starting position of the segment\n        x0 = i * width_segment\n        # Calculate the ending position of the segment\n        x1 = x0 + width_segment\n        # Add the starting position of the segment to the X0 array\n        X0.append(x0)\n        # Add the ending position of the segment to the X1 array\n        X1.append(x1)\n    # Return the starting and ending positions of segments\n    return X0, X1\n\n", "CodeBleu_score": 20.68886618982711}, {"code": "def split(n_segment)\n    width = 1.0 / n_segment\n    padding = width / 2.0\n    X0 = [padding + i * width for i in range(n_segment)]\n    X1 = [1.0 - padding + i * width for i in range(n_segment)]\n    return X0, X1\n", "CodeBleu_score": 25.720676987531156}, {"code": "def split(n_segment)\n    width = int(n_segment)\n    if width > 0:\n        # Calculate the width of each segment\n        segment_width = int(100 / width)\n        # Calculate the padding between segments\n        padding = int((100 - (segment_width * width)) / 2)\n        # Calculate the starting and ending positions of each segment\n        X0 = [padding + i * segment_width for i in range(width)]\n        X1 = [padding + i * segment_width + segment_width - 1 for i in range(width)]\n        return X0, X1\n    else:\n        return None, None\n\n", "CodeBleu_score": 32.8339374850071}]}
{"instance_id": "604a3e93ac0d", "generation_result": [{"code": "def pixelated_text(dpi=100)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    fig = plt.figure(figsize=(1, 1), dpi=dpi)\n    canvas = fig.canvas\n    ax = fig.add_subplot(111)\n    ax.text(0.5, 0.5, 'a', ha='center', va='center', fontsize=75)\n    ax.set_axis_off()\n    canvas.draw()\n    data = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    data = np.roll(data, 3, axis=2)\n    return data\n", "CodeBleu_score": 50.73294042802685}, {"code": "def pixelated_text(dpi=100)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    fig, ax = plt.subplots(figsize=(1, 1))\n    ax.text(0.5, 0.5, 'a', fontsize=75, ha='center', va='center')\n    ax.axis('off')\n    fig.canvas.draw()\n    data = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n    data = np.roll(data, 3, axis=2)\n    return data.reshape((dpi, dpi, 4))\n\n", "CodeBleu_score": 49.029645124896106}, {"code": "def pixelated_text(dpi=100)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    fig = plt.figure(figsize=(1, 1), dpi=dpi)\n    canvas = fig.canvas\n    ax = fig.add_subplot(111)\n    ax.text(0.5, 0.5, 'a', fontsize=75, ha='center', va='center')\n    ax.axis('off')\n    canvas.draw()\n    data = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8).reshape(canvas.get_width_height()[::-1] + (3,))\n    data = np.roll(data, 3, axis=-1)\n    return data", "CodeBleu_score": 50.22956115765602}]}
{"instance_id": "2a7fc7d560ea", "generation_result": [{"code": "def curve()\n    import numpy as np\n    import random\n    from scipy.stats import norm\n    import matplotlib.pyplot as plt\n\n    # Initialize the array with zeros\n    curve = np.zeros(1000)\n\n    # Determine the number of components\n    num_components = random.randint(2, 10)\n\n    # Determine the centers of the components\n    centers = np.random.uniform(0, 1000, num_components)\n\n    # Determine the widths of the components\n    widths = np.random.uniform(100, 500, num_components)\n    widths /= np.sum(widths)\n\n    # Determine the scales of the components\n    scales = np.random.uniform(0.5, 1.5, num_components)\n    scales /= np.sum(scales)\n\n    # Iteratively add scaled Gaussian components\n    for i in range(num_components):\n        curve += norm.pdf(np.arange(1000), centers[i], widths[i]) * scales[i]\n\n    # Normalize the curve\n    curve /= np.max(curve)\n\n    # Plot the curve\n    plt.plot(curve)\n    plt.title('Composite Curve')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.show()\n\n    return curve\n\n", "CodeBleu_score": 35.52692863517629}, {"code": "def curve()\n    # Initialize an array of zeros to store the composite curve\n    curve = np.zeros(100)\n\n    # Randomly determine the number of components\n    num_components = np.random.randint(1, 6)\n\n    # Randomly determine the centers, widths, and scales of the components\n    centers = np.random.uniform(0, 100, num_components)\n    widths = np.random.uniform(0, 10, num_components)\n    scales = np.random.uniform(0, 10, num_components)\n\n    # Normalize the widths and scales to ensure they sum to specific values\n    total_width = np.sum(widths)\n    total_scale = np.sum(scales)\n    widths = widths / total_width\n    scales = scales / total_scale\n\n    # Iteratively add scaled Gaussian components to the composite curve\n    for i in range(num_components):\n        center = centers[i]\n        width = widths[i]\n        scale = scales[i]\n        curve += scale * np.exp(-((np.arange(100) - center) ** 2) / (2 * width ** 2))\n\n    # Return the resulting composite curve\n    return curve\n", "CodeBleu_score": 31.056685674657498}, {"code": "def curve()\n    import numpy as np\n    import random\n\n    # Set the number of components\n    n_components = random.randint(2, 10)\n\n    # Generate random centers, widths, and scales\n    centers = np.random.uniform(0, 1, n_components)\n    widths = np.random.uniform(0.1, 0.5, n_components)\n    scales = np.random.uniform(0.1, 1, n_components)\n\n    # Normalize the widths and scales to ensure they sum to specific values\n    widths /= np.sum(widths)\n    scales /= np.sum(scales)\n\n    # Initialize an array of zeros\n    curve = np.zeros(100)\n\n    # Iteratively add scaled Gaussian components\n    for i in range(n_components):\n        curve += scales[i] * np.exp(-((np.arange(100) - centers[i]) ** 2) / (2 * widths[i] ** 2))\n\n    return curve\n\n", "CodeBleu_score": 36.70360692679062}]}
{"instance_id": "be694335ebfc", "generation_result": [{"code": "def difference(derivative, accuracy=1)\n    # Calculate the radius of the finite difference formula based on the accuracy and derivative order\n    radius = accuracy / (2 * derivative + 1)\n\n    # Define the range of points centered around zero\n    points = np.arange(-radius, radius + accuracy, accuracy)\n\n    # Calculate the inverse of the Vandermonde matrix for the points\n    vandermonde_inverse = np.linalg.inv(np.vander(points))\n\n    # Calculate the coefficients for the specified derivative order\n    coefficients = vandermonde_inverse[derivative] / np.math.factorial(derivative)\n\n    # Return the coefficients, scaled by the factorial of one less than the derivative order, and the points\n    return coefficients, points\n\n", "CodeBleu_score": 45.842866272148385}, {"code": "def difference(derivative, accuracy=1)\n    order = derivative + 1\n    radius = accuracy * order\n    points = np.linspace(-radius, radius, order)\n    inverse = np.linalg.inv(np.vander(points))\n    coefficients = inverse[0] / np.math.factorial(derivative)\n    return coefficients, points\n\n", "CodeBleu_score": 46.851895586682225}, {"code": "def difference(derivative, accuracy=1)\n    # Calculate the radius of the finite difference points\n    radius = accuracy * (1 + derivative)\n    # Calculate the number of points to use in the finite difference\n    points = 2 * radius + 1\n    # Create a range of points centered around zero\n    x = np.linspace(-radius, radius, points)\n    # Calculate the inverse of the Vandermonde matrix for these points\n    inv = np.linalg.inv(np.vander(x))\n    # Return the coefficients for the specified derivative order, scaled by the factorial of one less than the derivative order, along with the points\n    return inv[derivative] / np.math.factorial(derivative), x\n", "CodeBleu_score": 44.74236418941513}]}
{"instance_id": "fa3fb47f33fa", "generation_result": [{"code": "def interpolate(X, Y, T)\n    # Calculate the distances between consecutive points\n    distances = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n    # Calculate the cumulative distances\n    cumulative_distances = np.cumsum(distances)\n    # Normalize the cumulative distances\n    cumulative_distances /= cumulative_distances[-1]\n    # Interpolate the X and Y coordinates at the positions specified by T\n    X_interpolated = np.interp(T, cumulative_distances, X[:-1])\n    Y_interpolated = np.interp(T, cumulative_distances, Y[:-1])\n    # Calculate the total distance covered by the original coordinates\n    total_distance = cumulative_distances[-1]\n    return X_interpolated, Y_interpolated, total_distance\n", "CodeBleu_score": 25.50058290416909}, {"code": "def interpolate(X, Y, T)\n    # Calculate the distances between consecutive points\n    distances = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n    # Calculate the cumulative distances\n    cumulative_distances = np.insert(np.cumsum(distances), 0, 0)\n    # Normalize the cumulative distances to the range [0, 1]\n    cumulative_distances = (cumulative_distances - cumulative_distances.min()) / (cumulative_distances.max() - cumulative_distances.min())\n    # Interpolate the X and Y coordinates based on the target array T\n    interpolated_X = np.interp(T, cumulative_distances, X)\n    interpolated_Y = np.interp(T, cumulative_distances, Y)\n    # Calculate the total distance covered by the original coordinates\n    total_distance = cumulative_distances[-1]\n    return interpolated_X, interpolated_Y, total_distance\n", "CodeBleu_score": 28.03028743788008}, {"code": "def interpolate(X, Y, T)\n    # Calculate the distances between consecutive points\n    dist = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n    # Calculate the cumulative distances\n    cum_dist = np.cumsum(dist)\n    # Normalize the cumulative distances\n    cum_dist = cum_dist / cum_dist[-1]\n    # Interpolate the X and Y coordinates at the positions specified by T\n    X_interp = np.interp(T, cum_dist, X)\n    Y_interp = np.interp(T, cum_dist, Y)\n    # Calculate the total distance covered by the original coordinates\n    total_dist = cum_dist[-1]\n    return X_interp, Y_interp, total_dist", "CodeBleu_score": 29.42122277494429}]}
{"instance_id": "870a2dad3c66", "generation_result": [{"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n    # Create a new complex plane with the given ranges and number of points in each direction\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None] * 1j\n\n    # Initialize the iteration count array N and complex array Z\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n\n    # Iterate over each point in the complex plane\n    for n in range(maxiter):\n        # Compute the Mandelbrot sequence for each point in the complex plane\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n\n    # Set all points that remain bounded after maxiter iterations to 0\n    N[N == maxiter-1] = 0\n\n    # Return the final complex values Z and the iteration counts N\n    return Z, N\n", "CodeBleu_score": 73.87064790805528}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n    # Create a new figure with the given size\n    fig = plt.figure(figsize=(8, 8))\n    # Generate a grid of complex numbers C from the specified ranges\n    c = np.linspace(xmin, xmax, xn) + np.linspace(ymin, ymax, yn)[:, None] * 1j\n    # Initialize the iteration count array N with zeros\n    n = np.zeros(c.shape, dtype=int)\n    # Initialize the complex array Z with zeros\n    z = np.zeros(c.shape, np.complex64)\n    # Iterate over the complex numbers C\n    for i in range(maxiter):\n        # Compute the Mandelbrot sequence\n        z[n == 0] = z[n == 0] * z[n == 0] + c[n == 0]\n        # Update the iteration count array N with the iteration count where the magnitude of Z exceeds the horizon\n        n[np.abs(z) > horizon] = i\n        # Update the complex array Z with the new values\n        z[n == 0] = z[n == 0] * z[n == 0] + c[n == 0]\n    # Set the points that remain bounded after maxiter iterations to 0 in N\n    n[n == maxiter - 1] = 0\n    # Plot the Mandelbrot set using a contour plot\n    plt.contour(c.real, c.imag, n, colors=\"black\")\n    # Add a colorbar to the plot\n    plt.colorbar()\n    # Set the title of the plot\n    plt.title(f\"Mandelbrot set with {maxiter} iterations\")\n    # Show the plot\n    plt.show()\n    # Return the final complex values Z and the iteration counts N\n    return z, n\n\n", "CodeBleu_score": 38.567333701043}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n    # Create complex grid\n    x = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    c = x + y[:, None] * 1j\n    # Init output\n    N = np.zeros(c.shape, dtype=int)\n    Z = np.zeros(c.shape, np.complex64)\n    # Iterate\n    for i in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = i\n        Z[I] = Z[I]**2 + c[I]\n    # Output\n    return Z, N\n\n", "CodeBleu_score": 57.82644813279273}]}
{"instance_id": "e1a908945acf", "generation_result": [{"code": "def smooth2d(A, sigma=3)\n    import numpy as np\n    from scipy.ndimage.filters import gaussian_filter1d\n    w = int(np.ceil(sigma * 3))\n    w = w if w % 2 == 1 else w + 1\n    A = gaussian_filter1d(A, sigma, axis=0)\n    A = gaussian_filter1d(A.T, sigma, axis=0).T\n    return A\n\n", "CodeBleu_score": 22.21168935806729}, {"code": "def smooth2d(A, sigma=3)\n    from scipy.ndimage.filters import gaussian_filter1d\n    window_length = int(np.ceil(sigma * 3))\n    if window_length % 2 == 0:\n        window_length += 1\n    A = gaussian_filter1d(A, sigma, axis=1, mode='reflect')\n    A = A.T\n    A = gaussian_filter1d(A, sigma, axis=1, mode='reflect')\n    A = A.T\n    return A", "CodeBleu_score": 23.646767115382573}, {"code": "def smooth2d(A, sigma=3)\n    from scipy.ndimage.filters import gaussian_filter1d\n    window_length = int(2 * np.ceil(sigma) + 1)\n    if window_length % 2 == 0:\n        window_length += 1\n    A = gaussian_filter1d(A, sigma, axis=1, mode='reflect')\n    A = gaussian_filter1d(A.T, sigma, axis=1, mode='reflect').T\n    return A\n", "CodeBleu_score": 28.30596407454821}]}
{"instance_id": "94563c54b483", "generation_result": [{"code": "def process_image(self, padded_src, dpi)\n    ox, oy = self.offsets\n    ox_pixels = ox * dpi / 25.4\n    oy_pixels = oy * dpi / 25.4\n    return np.roll(np.roll(padded_src, int(ox_pixels), axis=1), int(oy_pixels), axis=0)", "CodeBleu_score": 31.1548635775844}, {"code": "def process_image(self, padded_src, dpi)\n    ox = self.ox * dpi / 25.4\n    oy = self.oy * dpi / 25.4\n    return np.roll(np.roll(padded_src, int(ox), axis=1), int(oy), axis=0)", "CodeBleu_score": 28.25975238234846}, {"code": "def process_image(self, padded_src, dpi)\n    ox = self.ox\n    oy = self.oy\n    ox_pix = int(ox * dpi / 25.4)\n    oy_pix = int(oy * dpi / 25.4)\n    return np.roll(np.roll(padded_src, ox_pix, axis=1), oy_pix, axis=0)\n", "CodeBleu_score": 26.669457553188185}]}
{"instance_id": "d71d232872fe", "generation_result": [{"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n    x, y = [(dim - 1.) / 2. for dim in shape]\n    y, x = np.ogrid[-y:y + 1, -x:x + 1]\n    h = np.exp(-(x * x + y * y) / (2. * width * width))\n    h[h < np.finfo(h.dtype).eps * h.max()] = 0\n    return h\n", "CodeBleu_score": 10.960787717342928}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n    x, y = [(i - c) / w for i, c, w in zip(np.indices(shape), center, width)]\n    return np.exp(-(x ** 2 + y ** 2))\n\n", "CodeBleu_score": 10.784580119096047}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n    # Initialize the shape as a tuple if it is a single value\n    shape = tuple(shape)\n\n    # Compute the squared distance of each grid point from the center\n    center = np.array(center)\n    indices = np.array(np.meshgrid(*[np.arange(s) for s in shape]))\n    indices = np.stack(indices, axis=-1)\n    squared_distance = np.sum((indices - center) ** 2, axis=-1)\n\n    # Compute the Gaussian values\n    gaussian_values = np.exp(-squared_distance / (2 * width ** 2))\n\n    return gaussian_values\n\n", "CodeBleu_score": 21.191229051737153}]}
{"instance_id": "7bbd08c987bc", "generation_result": [{"code": "def stimulus(position, size, intensity)\n    x, y = position\n    x_mesh, y_mesh = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n    R = np.sqrt((x_mesh - x)**2 + (y_mesh - y)**2)\n    return np.exp(-R**2 / size**2) * intensity\n", "CodeBleu_score": 14.741945389547748}, {"code": "def stimulus(position, size, intensity)\n    # Convert polar coordinates to Cartesian coordinates and normalize them\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n    x = x / np.sqrt(x**2 + y**2)\n    y = y / np.sqrt(x**2 + y**2)\n    \n    # Create a mesh grid representing the spatial domain, normalized and centered\n    x_grid, y_grid = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n    x_grid = x_grid - x\n    y_grid = y_grid - y\n    \n    # Compute the squared Euclidean distance (R) from each point in the grid to the stimulus center\n    R = x_grid**2 + y_grid**2\n    \n    # Return an exponential decay function of these distances, scaled by the size parameter\n    return np.exp(-R / size**2) * intensity\n", "CodeBleu_score": 15.378780175016882}, {"code": "def stimulus(position, size, intensity)\n    # Convert polar coordinates to Cartesian coordinates\n    x, y = position\n    x = x * np.cos(np.deg2rad(90 - y))\n    y = x * np.sin(np.deg2rad(90 - y))\n\n    # Normalize the stimulus\n    x = x / np.sqrt(x**2 + y**2)\n    y = y / np.sqrt(x**2 + y**2)\n\n    # Create a mesh grid representing the spatial domain\n    x_grid, y_grid = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n\n    # Compute the squared Euclidean distance (R) from each point in the grid to the stimulus center\n    R = (x_grid - x)**2 + (y_grid - y)**2\n\n    # Return the exponential decay function of R, scaled by the size parameter\n    return np.exp(-R / (2 * size**2)) * intensity\n", "CodeBleu_score": 31.2225558608678}]}
{"instance_id": "2b5806238d5d", "generation_result": [{"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 0.5, 1])\n    ax.set_yticks([0, 0.5, 1])\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.text(0.5, 0.5, text, ha=\"center\", va=\"center\")\n    ax.set_title(\"My Plot\", fontfamily=\"serif\", fontweight=\"bold\")\n\n", "CodeBleu_score": 38.68310151167452}, {"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.text(0.5, 0.5, text, ha='center', va='center', fontsize=20, fontweight='bold')\n    ax.set_title('Plot Title', fontfamily='serif', fontweight='bold')", "CodeBleu_score": 41.73198054575715}, {"code": "def plot(ax, text)\n    ax.set(xlim=(0, 1), ylim=(0, 1), xticks=[0, 0.25, 0.5, 0.75, 1], yticks=[0, 0.25, 0.5, 0.75, 1], xlabel='x', ylabel='y')\n    ax.text(0.5, 0.5, text, ha='center', va='center', fontsize=20, fontfamily='serif', fontweight='bold')\n    ax.set_title(text, fontfamily='serif', fontweight='bold')", "CodeBleu_score": 25.387697713180785}]}
{"instance_id": "0b42aac61e21", "generation_result": [{"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    X = self._check_X(X)\n    X_t = X.copy()\n    for name, transform in self.named_transformers_.items():\n        X_t = transform.transform(X_t)\n    return self.final_estimator_.predict_cumulative_hazard_function(X_t, **kwargs)", "CodeBleu_score": 34.46995033040871}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    X = self._check_transform_input(X)\n    Xt = X\n    for transformer in self.transformers_:\n        Xt = transformer.transform(Xt)\n    return self.final_estimator_.predict_cumulative_hazard_function(Xt, **kwargs)", "CodeBleu_score": 40.145445464708786}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    X_transformed = self.transform(X, **kwargs)\n    return self.final_estimator.predict_cumulative_hazard_function(X_transformed)", "CodeBleu_score": 19.935237920913064}]}
{"instance_id": "c8439e4918ad", "generation_result": [{"code": "def predict_survival_function(self, X, **kwargs)\n    X = self.transform(X)\n    return self.steps[-1][1].predict_survival_function(X, **kwargs)\n", "CodeBleu_score": 18.095661485827403}, {"code": "def predict_survival_function(self, X, **kwargs)\n    X_transformed = X.copy()\n    for step in self.pipeline[:-1]:\n        X_transformed = step.transform(X_transformed)\n    return self.pipeline[-1].predict(X_transformed, **kwargs)\n", "CodeBleu_score": 29.34180396843944}, {"code": "def predict_survival_function(self, X, **kwargs)\n    X = self._check_X(X)\n    X_transformed = self._pipeline.transform(X)\n    return self._final_step.predict_survival_function(X_transformed, **kwargs)\n", "CodeBleu_score": 21.64525973386318}]}
{"instance_id": "216662ca7db4", "generation_result": [{"code": "def _check_success(self, results)\n    if results.success:\n        return\n    if results.status == 'maxiter':\n        self.logger.warning('Optimization terminated successfully. '\n                            'The maximum number of iterations has been exceeded.')\n    elif results.status == 'primal infeasible':\n        raise RuntimeError('Optimization terminated successfully. '\n                           'The problem is primal infeasible.')\n    elif results.status == 'dual infeasible':\n        raise RuntimeError('Optimization terminated successfully. '\n                           'The problem is dual infeasible.')\n    else:\n        raise RuntimeError('Optimization failed. The solver returned '\n                           'status {}.'.format(results.status))\n\n", "CodeBleu_score": 23.182502267636178}, {"code": "def _check_success(self, results)\n    if results['status'] == 'optimal':\n        return\n    elif results['status'] == 'maxiter':\n        print('Maximum iterations reached')\n        return\n    elif results['status'] == 'primal infeasible':\n        raise RuntimeError('Primal infeasible')\n    elif results['status'] == 'dual infeasible':\n        raise RuntimeError('Dual infeasible')\n    else:\n        raise RuntimeError('Unknown exit status: ' + str(results['status']))", "CodeBleu_score": 18.467972525084082}, {"code": "def _check_success(self, results)\n    if results.success:\n        return\n    if results.status == \"maxiter\":\n        warnings.warn(\"Optimization terminated successfully. \"\n                      \"The maximum number of iterations is exceeded.\")\n    elif results.status == \"primal infeasible\":\n        raise RuntimeError(\"Optimization was primal infeasible.\")\n    elif results.status == \"dual infeasible\":\n        raise RuntimeError(\"Optimization was dual infeasible.\")\n    else:\n        raise RuntimeError(\"Optimization failed with status: %s\" %\n                           results.status)", "CodeBleu_score": 24.16024697980761}]}
{"instance_id": "1fe2e379fca0", "generation_result": [{"code": "def _decompose(self, P)\n    # Compute eigenvalues and eigenvectors\n    eigvals, eigvecs = linalg.eigh(P)\n\n    # Identify the largest eigenvalue\n    largest_eigval = max(eigvals)\n\n    # Compute the condition number\n    cond_num = largest_eigval / (eigvals.shape[0] * np.finfo(np.float64).eps)\n\n    # Check if all eigenvalues are above the negative cutoff\n    if all(eigvals > -self.cutoff):\n        # Filter out eigenvalues below the positive cutoff\n        eigvals = eigvals[eigvals > self.cutoff]\n\n        # Normalize the eigenvalues so that the maximum eigenvalue is set to 1\n        eigvals /= largest_eigval\n\n        # Compute the transposed decomposed matrix\n        P_decomp = eigvecs.T @ np.diag(eigvals) @ eigvecs\n\n        # Return the transposed decomposed matrix and the largest eigenvalue\n        return P_decomp, largest_eigval\n    else:\n        # Return None if all eigenvalues are below the negative cutoff\n        return None, None\n\n", "CodeBleu_score": 24.443166513064778}, {"code": "def _decompose(self, P)\n    # Calculate the eigenvalues and eigenvectors of P using scipy.linalg.eigh\n    eigvals, eigvecs = linalg.eigh(P)\n\n    # Identify the largest eigenvalue and calculate a condition number based on it\n    max_eigval = np.max(eigvals)\n    cond_num = max_eigval * np.sqrt(P.shape[0]) * np.finfo(float).eps\n\n    # Filter out eigenvalues below the cutoff\n    eigvals = eigvals[eigvals > -cond_num]\n\n    # Normalize the eigenvalues so that the maximum eigenvalue is set to 1\n    eigvals = eigvals / max_eigval\n\n    # Decompose P using the eigenvalues and eigenvectors\n    P_decomposed = eigvecs.T.dot(np.diag(eigvals)).dot(eigvecs)\n\n    # Return the transposed decomposed matrix and the largest eigenvalue\n    return P_decomposed.T, max_eigval\n", "CodeBleu_score": 23.686186080044763}, {"code": "def _decompose(self, P)\n    # Compute the eigenvalues and eigenvectors of P\n    eigvals, eigvecs = linalg.eigh(P)\n\n    # Identify the largest eigenvalue and calculate a condition number\n    largest_eigval = eigvals.max()\n    cond_num = largest_eigval / (P.shape[0] * np.finfo(float).eps)\n\n    # Filter out eigenvalues below the negative cutoff\n    eigvals = eigvals[eigvals >= -self.cutoff]\n\n    # Normalize the eigenvalues so that the maximum eigenvalue is set to 1\n    eigvals /= largest_eigval\n\n    # Return the transposed decomposed matrix and the largest eigenvalue\n    return eigvecs.T, largest_eigval", "CodeBleu_score": 15.834997635112162}]}
{"instance_id": "20c3f5d05d79", "generation_result": [{"code": "def _update_coef(self, coef, D)\n    self.coef_ = coef[np.abs(coef) > D] * self.scale_\n    self.coef_ = self.coef_[np.abs(self.coef_) > self.D]\n", "CodeBleu_score": 27.062290531081086}, {"code": "def _update_coef(self, coef, D)\n    if coef is None:\n        coef = np.ones(D.shape[1])\n    else:\n        coef = coef.copy()\n    coef[np.abs(coef) < self.tol] = 0\n    coef *= D\n    self.coef_ = coef\n", "CodeBleu_score": 27.33015403444709}, {"code": "def _update_coef(self, coef, D)\n    coef = coef[np.abs(coef) > self.threshold]\n    D = D[np.abs(coef) > self.threshold]\n    self.coef = coef / D\n", "CodeBleu_score": 20.520151259900775}]}
{"instance_id": "37c347d00d35", "generation_result": [{"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n    # Extract event type and time from the true event data\n    event_type = y_true[:, 0]\n    event_time = y_true[:, 1]\n    \n    # Compute the gradient using event type and time\n    gradient = self._compute_gradient(event_type, event_time)\n    \n    # Apply sample weights if provided\n    if sample_weight is not None:\n        gradient *= sample_weight[:, None]\n    \n    # Return the weighted or unweighted gradient\n    return gradient\n", "CodeBleu_score": 42.73352478250683}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n    if sample_weight is not None:\n        sample_weight = tf.cast(sample_weight, self.dtype)\n    # Compute the negative gradient\n    negative_gradient = self._compute_negative_gradient(y_true, raw_prediction)\n    # Apply sample weights if provided\n    if sample_weight is not None:\n        negative_gradient = negative_gradient * sample_weight\n    return negative_gradient\n", "CodeBleu_score": 40.23230851096571}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n    # Compute the gradient of the Cox PH model\n    # Calculate the negative gradient of the Cox PH model using event type and time\n    event_type = y_true[:, 0]\n    event_time = y_true[:, 1]\n    n_samples = len(event_type)\n    n_features = raw_prediction.shape[1]\n    gradient = np.zeros((n_samples, n_features))\n    for i in range(n_samples):\n        if event_type[i] == 0:\n            gradient[i] = -np.exp(raw_prediction[i]) / (np.exp(raw_prediction[i]) - 1)\n        else:\n            gradient[i] = -np.exp(raw_prediction[i]) * (event_time[i] - np.exp(raw_prediction[i])) / (np.exp(raw_prediction[i]) - 1)\n    # Apply sample weights if provided\n    if sample_weight is not None:\n        gradient = gradient * sample_weight[:, np.newaxis]\n    # Return the weighted or unweighted gradient\n    if sample_weight is None:\n        return gradient\n    else:\n        return gradient / np.sum(sample_weight)\n", "CodeBleu_score": 39.88181465756221}]}
{"instance_id": "6be432d27e52", "generation_result": [{"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    if not self.supports_missing_values:\n        if np.all(np.isfinite(X)):\n            return None\n        else:\n            raise ValueError(\n                \"The dataset does not support missing values and contains \"\n                \"infinite or NaN values.\"\n            )\n    else:\n        if estimator_name is None:\n            estimator_name = \"estimator\"\n        mask = np.isnan(X)\n        if np.any(mask):\n            # Check that there are no infinite values\n            with np.errstate(invalid=\"ignore\"):\n                sum_of_X = np.sum(X, axis=0)\n            if np.any(np.logical_not(np.isfinite(sum_of_X))):\n                raise ValueError(\n                    f\"{estimator_name} does not support missing values and \"\n                    \"contains infinite or NaN values.\"\n                )\n            return mask\n        else:\n            return None\n", "CodeBleu_score": 35.39627625864625}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    if not self.supports_missing_values:\n        if not np.all(np.isfinite(X)):\n            raise ValueError(\"The dataset does not support missing values.\")\n        return None\n    if estimator_name is None:\n        estimator_name = self.estimator_name\n    if estimator_name is None:\n        estimator_name = \"estimator\"\n    if self.missing_values_mask is not None:\n        return self.missing_values_mask\n    with np.errstate(over=\"ignore\"):\n        X_sum = np.sum(X, axis=0)\n    if not np.all(np.isfinite(X_sum)):\n        raise ValueError(\n            \"The dataset contains infinite values in the feature matrix \"\n            f\"of the {estimator_name}.\"\n        )\n    if not np.all(np.isfinite(X_sum)):\n        raise ValueError(\n            \"The dataset contains NaN values in the feature matrix \"\n            f\"of the {estimator_name}.\"\n        )\n    self.missing_values_mask = None\n    return self.missing_values_mask", "CodeBleu_score": 34.86165593410594}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    if not self.supports_missing_values:\n        if not np.all(np.isfinite(X)):\n            raise ValueError(\n                \"The dataset does not support missing values. \"\n                \"All elements must be finite.\"\n            )\n        return None\n\n    if estimator_name is None:\n        estimator_name = self.__class__.__name__\n\n    with np.errstate(over=\"ignore\"):\n        sum_ = np.sum(X, axis=0)\n\n    if not np.all(np.isfinite(sum_)):\n        raise ValueError(\n            f\"{estimator_name} does not support missing values. \"\n            \"The sum of the dataset is not finite.\"\n        )\n\n    if np.any(np.isnan(sum_)):\n        return np.any(np.isnan(X), axis=0)\n\n    return None", "CodeBleu_score": 39.01453559880338}]}
{"instance_id": "dc5bda895a07", "generation_result": [{"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    if self.memory is None:\n        raise ValueError(\"The model has not been fitted yet.\")\n    if not self.fitted:\n        raise ValueError(\"The model has not been fitted yet.\")\n    if check_input:\n        X = check_array(X, accept_sparse=True)\n    if X.shape[1] != self.n_features:\n        raise ValueError(\n            f\"X has {X.shape[1]} features, but this model has {self.n_features} features.\"\n        )\n    if X.shape[0] == 0:\n        raise ValueError(\"X is empty.\")\n    if X.shape[0] == 1:\n        X = X.reshape(1, -1)\n    X = self._validate_data(X, reset=False)\n    if self.memory.X_val is not None:\n        if not np.allclose(X, self.memory.X_val):\n            raise ValueError(\n                \"X does not match the validation set used to fit the model.\"\n            )\n    if self.memory.y_val is not None:\n        if not np.allclose(self.memory.y_val, self.memory.y_val):\n            raise ValueError(\n                \"y does not match the validation set used to fit the model.\"\n            )\n    if self.memory.times is not None:\n        if not np.allclose(self.memory.times, self.memory.times):\n            raise ValueError(\n                \"times does not match the validation set used to fit the model.\"\n            )\n    if self.memory.X_val is None:\n        raise ValueError(\n            \"The model has not been fitted yet. Please fit the model before predicting.\"\n        )\n    if self.memory.y_val is None:\n        raise ValueError(\n            \"The model has not been fitted yet. Please fit the model before predicting.\"\n        )\n    if self.memory.times is None:\n        raise ValueError(\n            \"The model has not been fitted yet. Please fit the model before predicting.\"\n        )\n    if self.memory.X_val.shape[0] != self.memory.y_", "CodeBleu_score": 28.023258860354904}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    if not self._is_fitted:\n        raise ValueError(\"Model is not fitted yet\")\n    if check_input:\n        X = check_array(X, accept_sparse=True)\n    X = self._validate_data(X, reset=False)\n    X = self._convert_to_sparse_matrix(X)\n    predictions = self._decision_tree.predict(X)\n    predictions = predictions[:, 1]\n    if return_array:\n        return predictions\n    else:\n        return self._convert_to_step_function(predictions)\n\n", "CodeBleu_score": 44.24285624871606}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    if self.memory is None:\n        raise ValueError(\"The model has not been fitted yet.\")\n    if check_input:\n        X = self._validate_X(X)\n    if self.memory.is_sparse:\n        X = self.memory.sparse_to_dense(X)\n    predictions = self.memory.decision_tree.predict(X)\n    predictions = predictions[:, 1]\n    if return_array:\n        return predictions\n    else:\n        return self.memory.step_function(predictions)", "CodeBleu_score": 41.68053843335478}]}
{"instance_id": "84ae915b5933", "generation_result": [{"code": "def _pre_fit(self, X, y)\n    X = check_array(X, accept_sparse='csr', dtype=np.float64, order='F')\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    if y.shape[1] != 2:\n        raise ValueError(\"y must have shape (n_samples, 2)\")\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"y and X must have the same number of samples\")\n    if y[:, 0].min() < 0:\n        raise ValueError(\"y[:, 0] must be non-negative\")\n    if y[:, 1].min() < 0:\n        raise ValueError(\"y[:, 1] must be non-negative\")\n    if y[:, 0].max() > 1:\n        raise ValueError(\"y[:, 0] must be in the range [0, 1]\")\n    if y[:, 1].max() > 1:\n        raise ValueError(\"y[:, 1] must be in the range [0, 1]\")\n    if y[:, 0].sum() == 0:\n        raise ValueError(\"y[:, 0] must have at least one positive value\")\n    if y[:, 1].sum() == 0:\n        raise ValueError(\"y[:, 1] must have at least one positive value\")\n    if X.shape[0] < self.min_samples:\n        raise ValueError(\"X must have at least %d samples\" % self.min_samples)\n    if self.normalize:\n        self.feature_means_ = X.mean(axis=0)\n        X -= self.feature_means_\n        if self.scale:\n            self.feature_scales_ = X.std(axis=0)\n            X /= self.feature_scales_\n    return X, y[:, 0], y[:, 1], self.feature_means_, self.feature_scales_\n", "CodeBleu_score": 17.641801132834185}, {"code": "def _pre_fit(self, X, y)\n    X, y = check_X_y(X, y, accept_sparse=False)\n    X = check_array(X, accept_sparse=False, dtype=np.float64)\n\n    # validate y\n    y = check_array(y, ensure_2d=False, dtype=np.float64)\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1d array\")\n\n    # convert to Fortran-contiguous arrays\n    X = np.asfortranarray(X)\n    y = np.asfortranarray(y)\n\n    # sort by survival time\n    order = np.argsort(y, kind=\"mergesort\")\n    X = X[order]\n    y = y[order]\n\n    # convert to Fortran-contiguous arrays\n    X = np.asfortranarray(X)\n    y = np.asfortranarray(y)\n\n    # validate y\n    y = check_array(y, ensure_2d=False, dtype=np.float64)\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1d array\")\n\n    # convert to Fortran-contiguous arrays\n    X = np.asfortranarray(X)\n    y = np.asfortranarray(y)\n\n    # validate y\n    y = check_array(y, ensure_2d=False, dtype=np.float64)\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1d array\")\n\n    # convert to Fortran-contiguous arrays\n    X = np.asfortranarray(X)\n    y = np.asfortranarray(y)\n\n    # validate y\n    y = check_array(y, ensure_2d=False, dtype=np.float64)\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1d array\")\n\n    # convert to Fortran-contiguous arrays\n    X = np.asfortranarray(X)\n    y = np.as", "CodeBleu_score": 23.435774259414856}, {"code": "def _pre_fit(self, X, y)\n    if X.dtype != np.float64:\n        X = X.astype(np.float64)\n    if y.dtype != np.float64:\n        y = y.astype(np.float64)\n    if len(X) < self.min_samples:\n        raise ValueError(\n            f\"Number of samples ({len(X)}) must be greater than or equal to {self.min_samples}.\"\n        )\n    if len(y) < self.min_samples:\n        raise ValueError(\n            f\"Number of samples ({len(y)}) must be greater than or equal to {self.min_samples}.\"\n        )\n    if len(X) != len(y):\n        raise ValueError(\n            f\"Number of samples in X ({len(X)}) must be equal to number of samples in y ({len(y)}).\"\n        )\n    X, y = np.asfortranarray(X), np.asfortranarray(y)\n    X, y = X[y.argsort()[::-1]], y[y.argsort()[::-1]]\n    if self.center:\n        self.feature_means = X.mean(axis=0)\n        X -= self.feature_means\n    if self.scale:\n        self.feature_scales = X.std(axis=0)\n        X /= self.feature_scales\n    return X, y, self.feature_means, self.feature_scales", "CodeBleu_score": 24.905317152303972}]}
{"instance_id": "c70bb013f85f", "generation_result": [{"code": "def _check_alphas(self)\n    if self.alphas is None:\n        if self.alpha_range is None:\n            self.alpha_range = [0.0, 1.0]\n        if self.alpha_range[0] >= self.alpha_range[1]:\n            raise ValueError(\n                \"alpha_range[0] must be less than alpha_range[1].\"\n            )\n        if self.alpha_range[0] < 0.0:\n            raise ValueError(\n                \"alpha_range[0] must be non-negative.\"\n            )\n        if self.alpha_range[1] < 0.0:\n            raise ValueError(\n                \"alpha_range[1] must be non-negative.\"\n            )\n        if self.alpha_range[1] == 0.0:\n            raise ValueError(\n                \"alpha_range[1] must be non-zero.\"\n            )\n        if self.alpha_range[0] == self.alpha_range[1]:\n            raise ValueError(\n                \"alpha_range[0] must be different from alpha_range[1].\"\n            )\n        if self.num_alphas is None:\n            self.num_alphas = 100\n        if self.num_alphas <= 0:\n            raise ValueError(\n                \"num_alphas must be a positive integer.\"\n            )\n        self.alphas = np.linspace(\n            self.alpha_range[0], self.alpha_range[1], self.num_alphas\n        )\n        return self.alphas, True\n    else:\n        if not isinstance(self.alphas, np.ndarray):\n            raise TypeError(\n                \"alphas must be a numpy array.\"\n            )\n        if len(self.alphas.shape) != 1:\n            raise ValueError(\n                \"alphas must be a 1D array.\"\n            )\n        if not np.all(np.isfinite(self.alphas)):\n            raise ValueError(\n                \"alphas must be finite.\"\n            )\n        if not np.all(self.alphas >= 0.0):\n            raise ValueError(\n                \"alphas must be non", "CodeBleu_score": 30.26346780711206}, {"code": "def _check_alphas(self)\n    if self.alphas is None:\n        if self.alpha_min is None or self.alpha_max is None:\n            raise ValueError(\"alpha_min and alpha_max must be set if alphas is None\")\n        if self.alpha_min >= self.alpha_max:\n            raise ValueError(\"alpha_min must be less than alpha_max\")\n        if self.n_alphas <= 0:\n            raise ValueError(\"n_alphas must be a positive integer\")\n        self.alphas = np.linspace(self.alpha_min, self.alpha_max, self.n_alphas)\n        self._new_alphas = True\n    else:\n        self._new_alphas = False\n        if not isinstance(self.alphas, np.ndarray):\n            self.alphas = np.array(self.alphas)\n        if len(self.alphas.shape) != 1:\n            raise ValueError(\"alphas must be a 1D array\")\n        if not np.all(np.isfinite(self.alphas)):\n            raise ValueError(\"alphas must be finite\")\n        if not np.all(self.alphas >= 0):\n            raise ValueError(\"alphas must be non-negative\")\n    return self.alphas, self._new_alphas", "CodeBleu_score": 41.406366143120046}, {"code": "def _check_alphas(self)\n    if self.alphas is None:\n        # If alphas is None, we need to create a new path\n        if self.alpha_path_length <= 0:\n            raise ValueError(\"alpha_path_length must be a positive integer\")\n        self.alphas = np.linspace(0, 1, self.alpha_path_length)\n        self.new_alpha_path = True\n    else:\n        # If alphas is not None, we need to validate the values\n        if not isinstance(self.alphas, (list, tuple, np.ndarray)):\n            raise ValueError(\"alphas must be a list, tuple, or numpy array\")\n        if len(self.alphas) == 0:\n            raise ValueError(\"alphas must not be empty\")\n        if not np.all(np.isfinite(self.alphas)):\n            raise ValueError(\"alphas must be finite\")\n        if not np.all(self.alphas >= 0):\n            raise ValueError(\"alphas must be non-negative\")\n        self.alphas = np.array(self.alphas)\n        self.new_alpha_path = False\n    return self.alphas, self.new_alpha_path\n", "CodeBleu_score": 34.65846374249814}]}
{"instance_id": "7b54a0554ba4", "generation_result": [{"code": "def nelson_aalen_estimator(event, time)\n    import numpy as np\n\n    # Validate inputs\n    if not isinstance(event, np.ndarray):\n        raise ValueError(\"event must be a numpy array\")\n    if not isinstance(time, np.ndarray):\n        raise ValueError(\"time must be a numpy array\")\n    if len(event) != len(time):\n        raise ValueError(\"event and time must have the same length\")\n    if not np.issubdtype(event.dtype, np.bool_):\n        raise ValueError(\"event must be a boolean array\")\n    if not np.issubdtype(time.dtype, np.number):\n        raise ValueError(\"time must be a numeric array\")\n\n    # Ensure inputs have consistent lengths\n    event = event.astype(int)\n    time = time.astype(int)\n\n    # Calculate unique times\n    unique_times = np.unique(time)\n\n    # Calculate number of events and subjects at risk at each time point\n    num_events = np.zeros(len(unique_times))\n    num_subjects_at_risk = np.zeros(len(unique_times))\n    for i, t in enumerate(unique_times):\n        num_events[i] = np.sum(event[time == t])\n        num_subjects_at_risk[i] = np.sum(time >= t)\n\n    # Calculate cumulative hazard\n    cumulative_hazard = np.zeros(len(unique_times))\n    for i in range(len(unique_times)):\n        cumulative_hazard[i] = np.sum(num_events[:i+1] / num_subjects_at_risk[:i+1])\n\n    return unique_times, cumulative_hazard", "CodeBleu_score": 20.91579164944466}, {"code": "def nelson_aalen_estimator(event, time)\n    # Validate inputs\n    if not isinstance(event, list) or not isinstance(time, list):\n        raise TypeError(\"event and time must be lists\")\n    if len(event) != len(time):\n        raise ValueError(\"event and time must have the same length\")\n    if any(not isinstance(e, int) or not isinstance(t, float) for e, t in zip(event, time)):\n        raise TypeError(\"event and time must contain only integers and floats\")\n    if any(e < 0 or t < 0 for e, t in zip(event, time)):\n        raise ValueError(\"event and time must contain only non-negative values\")\n\n    # Ensure inputs are sorted\n    time, event = zip(*sorted(zip(time, event)))\n\n    # Compute unique times\n    unique_times = list(set(time))\n    unique_times.sort()\n\n    # Compute number of events, number of subjects at risk, and cumulative hazard\n    cumulative_hazard = []\n    num_events = 0\n    num_subjects_at_risk = 0\n    for t in unique_times:\n        num_events += sum(e == 1 and t == t_i for e, t_i in zip(event, time))\n        num_subjects_at_risk += sum(t == t_i for t_i in time)\n        cumulative_hazard.append(num_events / num_subjects_at_risk)\n\n    return unique_times, cumulative_hazard", "CodeBleu_score": 35.45071334579454}, {"code": "def nelson_aalen_estimator(event, time)\n    # Validate inputs\n    if not isinstance(event, list) or not isinstance(time, list):\n        raise ValueError(\"Inputs must be lists\")\n    if len(event) != len(time):\n        raise ValueError(\"Inputs must have the same length\")\n    if any(not isinstance(e, (int, float)) for e in event) or any(not isinstance(t, (int, float)) for t in time):\n        raise ValueError(\"Inputs must be numeric\")\n    if any(e < 0 for e in event):\n        raise ValueError(\"Event indicators must be non-negative\")\n    if any(t < 0 for t in time):\n        raise ValueError(\"Times must be non-negative\")\n    # Compute unique times\n    unique_times = sorted(set(time))\n    # Compute number of events, number of subjects at risk, and cumulative hazard\n    cum_hazard = []\n    for t in unique_times:\n        num_events = sum(1 for e, t_i in zip(event, time) if e == 1 and t_i <= t)\n        num_subjects = sum(1 for t_i in time if t_i <= t)\n        cum_hazard.append(num_events / num_subjects)\n    # Return unique times and cumulative hazard\n    return unique_times, cum_hazard\n", "CodeBleu_score": 32.90515071538556}]}
{"instance_id": "35af806300cc", "generation_result": [{"code": "def ipc_weights(event, time)\n    # Check if all events have occurred\n    if event.all():\n        return np.ones(len(time))\n    # Calculate the Kaplan-Meier estimator in reverse\n    km_estimator = km_estimator_reverse(event, time)\n    # Find the index positions of event times within the unique time values\n    idx = np.searchsorted(np.unique(time), time)\n    # Retrieve the corresponding survival probabilities\n    survival_probabilities = km_estimator[idx - 1]\n    # Ensure all retrieved probabilities are positive\n    survival_probabilities = np.maximum(survival_probabilities, 0)\n    # Create a weights array\n    weights = 1 / survival_probabilities\n    return weights\n\n", "CodeBleu_score": 32.47147308445248}, {"code": "def ipc_weights(event, time)\n    from lifelines.utils import concordance_index\n    from lifelines.utils import KaplanMeierFitter\n    import numpy as np\n\n    # Check if all events have occurred\n    if np.all(event):\n        return np.ones(len(event))\n\n    # Calculate Kaplan-Meier estimator for event and time data in reverse\n    kmf = KaplanMeierFitter()\n    kmf.fit(time, event, label='Kaplan-Meier Estimator')\n    survival_prob = kmf.survival_function_at_times(time[::-1])\n\n    # Find index positions of event times within unique time values\n    unique_times = np.unique(time)\n    index_positions = [np.where(unique_times == t)[0][0] for t in time]\n\n    # Retrieve corresponding survival probabilities\n    survival_prob = survival_prob[index_positions]\n\n    # Ensure all retrieved probabilities are positive\n    survival_prob = np.maximum(survival_prob, 1e-10)\n\n    # Create weights array\n    weights = 1 / survival_prob\n\n    return weights", "CodeBleu_score": 33.67064539686114}, {"code": "def ipc_weights(event, time)\n    # Check if all events have occurred\n    if event.all():\n        return np.ones(len(event))\n\n    # Calculate the Kaplan-Meier estimator\n    kmf = KaplanMeierFitter()\n    kmf.fit(time, event)\n    unique_times = np.unique(time)\n    unique_survival_probs = kmf.survival_function_at_times(unique_times)\n\n    # Find the index positions of event times within the unique time values\n    event_indices = np.searchsorted(unique_times, time[event])\n\n    # Retrieve the corresponding survival probabilities\n    survival_probs = unique_survival_probs[event_indices]\n\n    # Ensure all retrieved probabilities are positive\n    survival_probs = np.maximum(survival_probs, 1e-10)\n\n    # Create the weights array\n    weights = 1 / survival_probs\n\n    return weights", "CodeBleu_score": 38.35906809174591}]}
{"instance_id": "72b19985ea11", "generation_result": [{"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    if isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    if isinstance(dfs, list):\n        dfs = [dfs]\n    if isinstance(dfs, tuple):\n        dfs = list(dfs)\n    if isinstance(dfs, np.ndarray):\n        dfs = [dfs]\n    if isinstance(dfs, dict):\n        dfs = [dfs]\n    if isinstance(dfs, set):\n        dfs = [dfs]\n    if isinstance(dfs, str):\n        dfs = [dfs]\n    if isinstance(dfs, int):\n        dfs = [dfs]\n    if isinstance(dfs, float):\n        dfs = [dfs]\n    if isinstance(dfs, bool):\n        dfs = [dfs]\n    if isinstance(dfs, object):\n        dfs = [dfs]\n    if isinstance(dfs, bytes):\n        dfs = [dfs]\n    if isinstance(dfs, bytearray):\n        dfs = [dfs]\n    if isinstance(dfs, memoryview):\n        dfs = [dfs]\n    if isinstance(dfs, np.void):\n        dfs = [dfs]\n    if isinstance(dfs, np.bool_):\n        dfs = [dfs]\n    if isinstance(dfs, np.int8):\n        dfs = [dfs]\n    if isinstance(dfs, np.int16):\n        dfs = [dfs]\n    if isinstance(dfs, np.int32):\n        dfs = [dfs]\n    if isinstance(dfs, np.int64):\n        dfs = [dfs]\n    if isinstance(dfs, np.uint8):\n        dfs = [dfs]\n    if isinstance(dfs, np.uint16):\n        dfs = [dfs]\n    if isinstance(dfs, np.uint3", "CodeBleu_score": 27.886140957073412}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    elif isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        pass\n    else:\n        raise ValueError(\"Invalid input type\")\n    dfs = [\n        self._convert_to_connector(df) if isinstance(df, pd.DataFrame) else df\n        for df in dfs\n    ]\n    return dfs\n\n", "CodeBleu_score": 20.045960544013592}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    if isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    if isinstance(dfs, list):\n        dfs = [dfs]\n    if isinstance(dfs, tuple):\n        dfs = list(dfs)\n    if isinstance(dfs, np.ndarray):\n        dfs = [dfs]\n    if isinstance(dfs, dict):\n        dfs = [dfs]\n    if isinstance(dfs, set):\n        dfs = [dfs]\n    if isinstance(dfs, str):\n        dfs = [dfs]\n    if isinstance(dfs, int):\n        dfs = [dfs]\n    if isinstance(dfs, float):\n        dfs = [dfs]\n    if isinstance(dfs, bool):\n        dfs = [dfs]\n    if isinstance(dfs, object):\n        dfs = [dfs]\n    if isinstance(dfs, pd.Series):\n        dfs = [dfs]\n    if isinstance(dfs, pd.Index):\n        dfs = [dfs]\n    if isinstance(dfs, pd.MultiIndex):\n        dfs = [dfs]\n    if isinstance(dfs, pd.MultiIndex):\n        dfs = [dfs]\n    if isinstance(dfs, pd.MultiIndex):\n        dfs = [dfs]\n    if isinstance(dfs, pd.MultiIndex):\n        dfs = [dfs]\n    if isinstance(dfs, pd.MultiIndex):\n        dfs = [dfs]\n    if isinstance(dfs, pd.MultiIndex):\n        dfs = [dfs]\n    if isinstance(dfs, pd.MultiIndex):\n        dfs = [dfs]\n    if isinstance(dfs, pd.MultiIndex):\n        dfs = [dfs]\n    if isinstance(dfs, pd.MultiIndex):\n        dfs = [dfs]", "CodeBleu_score": 27.857976478630995}]}
{"instance_id": "cf22fb696e90", "generation_result": [{"code": "def execute(self) -> pd.DataFrame\n    if isinstance(self.data, pd.DataFrame):\n        return self.data\n    elif self.cache_path is not None:\n        try:\n            return pd.read_csv(self.cache_path)\n        except FileNotFoundError:\n            pass\n    self.data = self.fetch()\n    if self.cache_path is not None:\n        self.data.to_csv(self.cache_path, index=False)\n    return self.data\n\n", "CodeBleu_score": 44.38678542887251}, {"code": "def execute(self) -> pd.DataFrame\n    if self._data is not None:\n        return self._data\n    if self._filters is None:\n        self._data = self._fetch_data()\n    else:\n        self._data = self._fetch_data_with_filters()\n    return self._data", "CodeBleu_score": 41.53802294504831}, {"code": "def execute(self) -> pd.DataFrame\n    if self._df is not None:\n        return self._df\n    else:\n        self._df = self.fetch()\n        return self._df\n", "CodeBleu_score": 30.261331638780693}]}
{"instance_id": "21df5ecea95f", "generation_result": [{"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.df()\n    col_names = \"|\".join(self.columns)\n    formula = \"|\".join(self.formula)\n    hash_str = col_names + formula\n    return hashlib.sha256(hash_str.encode()).hexdigest()\n", "CodeBleu_score": 41.616772669570814}, {"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.get_df()\n    col_names = \"|\".join(self.columns)\n    return hashlib.sha256(col_names.encode() + b\"|formula\").hexdigest()\n\n", "CodeBleu_score": 36.72036601597088}, {"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.to_dataframe()\n    columns = \"|\".join(self.columns)\n    formula = \"|\".join([\"formula\", \"column_hash\"])\n    combined = columns + formula\n    return hashlib.sha256(combined.encode()).hexdigest()\n\n", "CodeBleu_score": 42.24694594510673}]}
{"instance_id": "27ee5f415000", "generation_result": [{"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n    if isinstance(df, pd.DataFrame):\n        self._df = df\n    elif isinstance(df, pd.Series):\n        self._df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        if df.endswith(\".csv\"):\n            self._df = pd.read_csv(df)\n        elif df.endswith(\".xlsx\"):\n            self._df = pd.read_excel(df)\n        else:\n            raise ValueError(\"Unsupported file type\")\n    elif isinstance(df, list) or isinstance(df, dict):\n        self._df = pd.DataFrame(df)\n    else:\n        raise ValueError(\"Unsupported data type\")\n", "CodeBleu_score": 49.84822845080552}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n    if isinstance(df, pd.DataFrame):\n        self._df = df\n    elif isinstance(df, pd.Series):\n        self._df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        self._df = pd.read_csv(df, sep=self.sep, header=self.header)\n    elif isinstance(df, list):\n        self._df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        self._df = pd.DataFrame(df)\n    else:\n        raise ValueError(f\"Unsupported type {type(df)}\")\n", "CodeBleu_score": 42.884421503424996}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n    if isinstance(df, pd.DataFrame):\n        self.df = df\n    elif isinstance(df, pd.Series):\n        self.df = df.to_frame()\n    elif isinstance(df, str):\n        self.df = pd.read_csv(df)\n    elif isinstance(df, list) or isinstance(df, dict):\n        self.df = pd.DataFrame(df)\n    else:\n        raise ValueError(\"Unsupported type for df\")\n", "CodeBleu_score": 46.1145564015097}]}
{"instance_id": "38f48a358606", "generation_result": [{"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    if n > self.shape[0]:\n        return self.sample(frac=1).reset_index(drop=True)\n    else:\n        sample = self.sample(n).reset_index(drop=True)\n        for col in sample.columns:\n            if sample[col].dtype == \"object\":\n                sample[col] = sample[col].apply(lambda x: hash(x))\n            elif sample[col].dtype == \"datetime64[ns]\":\n                sample[col] = sample[col].apply(\n                    lambda x: x.replace(year=x.year - x.year % 10)\n                )\n            elif sample[col].dtype == \"int64\":\n                sample[col] = sample[col].apply(lambda x: x % 100)\n            elif sample[col].dtype == \"float64\":\n                sample[col] = sample[col].apply(lambda x: x % 100)\n        return sample\n\n", "CodeBleu_score": 28.756619115875765}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    if n > len(self):\n        return self.sample(frac=1, random_state=42).reset_index(drop=True)\n    else:\n        samples = []\n        for col in self.columns:\n            if self[col].dtype == \"object\":\n                samples.append(self[col].sample(n, random_state=42))\n            else:\n                samples.append(self[col].sample(n, random_state=42).sort_index())\n        df = pd.concat(samples, axis=1)\n        df = df.iloc[:n, :]\n        df = df.replace(\n            {\n                \"name\": \"Anonymous\",\n                \"email\": \"anonymous@example.com\",\n                \"phone\": \"1234567890\",\n                \"address\": \"123 Main St\",\n                \"city\": \"City\",\n                \"state\": \"State\",\n                \"zip\": \"12345\",\n                \"country\": \"Country\",\n            }\n        )\n        return df\n\n", "CodeBleu_score": 26.750632271628195}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    if n > self.shape[0]:\n        return self.sample(frac=1).reset_index(drop=True)\n    else:\n        sample_df = pd.DataFrame()\n        for col in self.columns:\n            if self[col].dtype == \"object\":\n                sample_df[col] = self[col].sample(n, random_state=42)\n            else:\n                sample_df[col] = self[col].sample(n, random_state=42)\n        sample_df = sample_df.sample(frac=1, random_state=42)\n        sample_df.loc[:, :5] = sample_df.loc[:, :5].applymap(\n            lambda x: x[0] + \"*\" * (len(str(x)) - 1)\n        )\n        return sample_df", "CodeBleu_score": 31.987732984462447}]}
{"instance_id": "8bddad2e31ca", "generation_result": [{"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    # Create an empty string to store the CSV-formatted DataFrame\n    csv_df = \"\"\n\n    # If the DataFrame has a name, add it to the opening tag\n    if \"name\" in extras:\n        csv_df += f'<data name=\"{extras[\"name\"]}\"'\n    else:\n        csv_df += \"<data\"\n\n    # If the DataFrame has a description, add it to the opening tag\n    if \"description\" in extras:\n        csv_df += f' description=\"{extras[\"description\"]}\"'\n\n    # Close the opening tag\n    csv_df += \">\\n\"\n\n    # Add details about the DataFrame's dimensions and content in CSV format\n    csv_df += f\"{df.shape[0]} rows, {df.shape[1]} columns\\n\"\n    csv_df += df.to_csv(index=False)\n\n    # Close the tag\n    csv_df += \"</data>\\n\"\n\n    # Return the CSV-formatted DataFrame as a string\n    return csv_df\n\n", "CodeBleu_score": 37.995622600660525}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    name = extras.get(\"name\", \"\")\n    description = extras.get(\"description\", \"\")\n    csv_str = f\"<{name} description=\\\"{description}\\\">\\n\"\n    csv_str += df.to_csv(index=False)\n    csv_str += f\"</{name}>\\n\"\n    return csv_str", "CodeBleu_score": 26.40024126704275}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    name = extras.get(\"name\", \"\")\n    description = extras.get(\"description\", \"\")\n    csv = df.to_csv(index=False, header=False)\n    return f\"<{name} {description}>\\n{csv}\\n</{name}>\"", "CodeBleu_score": 23.314739433809727}]}
{"instance_id": "5380bb60fd39", "generation_result": [{"code": "def convert_df_sql_connector_to_str\n    # Create the opening table tag with the table name and optional description\n    html_str = f'<table border=\"1\" class=\"dataframe\"><caption>{df.name}</caption>\\n'\n\n    # Add the CSV representation of the DataFrame's head\n    html_str += df.head().to_csv(index=False)\n\n    # Close the table tag\n    html_str += \"</table>\"\n\n    # Return the complete HTML string\n    return html_str\n\n", "CodeBleu_score": 8.177497769854574}, {"code": "def convert_df_sql_connector_to_str\n    # Initialize an empty list to store the rows of the table\n    rows = []\n\n    # Loop through each row in the DataFrame\n    for i, row in df.iterrows():\n        # Initialize an empty list to store the cells of the row\n        cells = []\n\n        # Loop through each column in the DataFrame\n        for col in df.columns:\n            # Get the value of the cell at the current row and column\n            val = row[col]\n\n            # If the value is a string, wrap it in <td> tags\n            if isinstance(val, str):\n                val = f\"<td>{val}</td>\"\n\n            # If the value is a number, wrap it in <td> tags and add a class attribute to indicate that it's a number\n            elif isinstance(val, (int, float)):\n                val = f'<td class=\"number\">{val}</td>'\n\n            # If the value is a datetime object, wrap it in <td> tags and add a class attribute to indicate that it's a datetime\n            elif isinstance(val, (datetime.datetime, datetime.date)):\n                val = f'<td class=\"datetime\">{val}</td>'\n\n            # If the value is a boolean, wrap it in <td> tags and add a class attribute to indicate that it's a boolean\n            elif isinstance(val, bool):\n                val = f'<td class=\"boolean\">{val}</td>'\n\n            # If the value is a list, wrap it in <td> tags and add a class attribute to indicate that it's a list\n            elif isinstance(val, list):\n                val = f'<td class=\"list\">{val}</td>'\n\n            # If the value is a dictionary, wrap it in <td> tags and add a class attribute to indicate that it's a dictionary\n            elif isinstance(val, dict):\n                val = f'<td class=\"dict\">{val}</td>'\n\n            # If the value is None, wrap it in <td> tags and add a class attribute to indicate that it's None", "CodeBleu_score": 26.159672508000586}, {"code": "def convert_df_sql_connector_to_str\n    df = pd.DataFrame(data=data)\n    df.columns = df.columns.str.replace(\" \", \"_\")\n    df.columns = df.columns.str.replace(\"-\", \"_\")\n    df.columns = df.columns.str.lower()\n    df.columns = df.columns.str.replace(\".\", \"_\")\n    df.columns = df.columns.str.replace(\"(\", \"_\")\n    df.columns = df.columns.str.replace(\")\", \"_\")\n    df.columns = df.columns.str.replace(\"/\", \"_\")\n    df.columns = df.columns.str.replace(\"\\\\\", \"_\")\n    df.columns = df.columns.str.replace(\"&\", \"_\")\n    df.columns = df.columns.str.replace(\"*\", \"_\")\n    df.columns = df.columns.str.replace(\":\", \"_\")\n    df.columns = df.columns.str.replace(\"?\", \"_\")\n    df.columns = df.columns.str.replace(\"+\", \"_\")\n    df.columns = df.columns.str.replace(\"=\", \"_\")\n    df.columns = df.columns.str.replace(\"|\", \"_\")\n    df.columns = df.columns.str.replace(\"<\", \"_\")\n    df.columns = df.columns.str.replace(\">\", \"_\")\n    df.columns = df.columns.str.replace(\"[\", \"_\")\n    df.columns = df.columns.str.replace(\"]\", \"_\")\n    df.columns = df.columns.str.replace(\"{\", \"_\")\n    df.columns = df.columns.str.replace(\"}\", \"_\")\n    df.columns = df.columns.str.replace(\"'\", \"_\")\n    df.columns = df.columns.str.replace(\"\\\"\", \"_\")\n    df.columns = df.columns.str.replace(\"\\\\\", \"_\")\n    df.columns = df.columns.str.replace(\"\\\\\", \"_\")\n    df.columns = df.columns.str.replace(\"\\\\\", \"_\")\n    df.columns = df.columns.str.replace(\"\\\\\", \"_\")\n    df.columns = df.columns.str.replace(\"\\\\\", \"_\")\n   ", "CodeBleu_score": 26.389628728019176}]}
{"instance_id": "0afd995716c5", "generation_result": [{"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    # Convert the DataFrame to JSON\n    json_data = df.to_json(orient='records')\n\n    # Convert the JSON to YAML\n    yaml_data = yaml.safe_dump(json.loads(json_data))\n\n    # Wrap the YAML string in HTML table tags if is_direct_sql is True\n    if extras.get('is_direct_sql', False):\n        html_table = f'<table><tr><th>{yaml_data}</th></tr></table>'\n        return html_table\n\n    # Return the YAML string\n    return yaml_data", "CodeBleu_score": 42.73665088640987}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    json_str = convert_df_to_json(df, extras)\n    yaml_str = yaml.dump(json.loads(json_str), sort_keys=False)\n    if extras.get(\"is_direct_sql\", False):\n        return f\"<table>{yaml_str}</table>\"\n    return yaml_str", "CodeBleu_score": 50.96578047475583}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    json_str = self.convert_df_to_json(df, extras)\n    yaml_str = yaml.safe_dump(json.loads(json_str), sort_keys=False)\n    if extras.get(\"is_direct_sql\", False):\n        return f\"<table>{yaml_str}</table>\"\n    else:\n        return yaml_str", "CodeBleu_score": 56.67011071294993}]}
{"instance_id": "e2b90ebd4e5d", "generation_result": [{"code": "def validate_value(self, expected_type: str) -> bool\n    if expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        if isinstance(self.value, (int, float)):\n            return True\n        else:\n            return False\n    elif expected_type == \"string\":\n        if isinstance(self.value, str):\n            return True\n        else:\n            return False\n    elif expected_type == \"dataframe\":\n        if isinstance(self.value, (pd.DataFrame, pd.Series)):\n            return True\n        else:\n            return False\n    elif expected_type == \"plot\":\n        if isinstance(self.value, (str, dict)):\n            return True\n        else:\n            return False\n    else:\n        return False", "CodeBleu_score": 52.25036711557293}, {"code": "def validate_value(self, expected_type: str) -> bool\n    if expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        return isinstance(self.value, (str, dict))\n    else:\n        return False", "CodeBleu_score": 54.11956938032165}, {"code": "def validate_value(self, expected_type: str) -> bool\n    if expected_type == \"\":\n        return True\n    if expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    if expected_type == \"string\":\n        return isinstance(self.value, str)\n    if expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    if expected_type == \"plot\":\n        return isinstance(self.value, (str, dict))\n    return False", "CodeBleu_score": 47.32138138733279}]}
{"instance_id": "d95828fdcbcb", "generation_result": [{"code": " def _distance(self, X, X2=None)\n    if X2 is None:\n        if isinstance(self.distance_metric, str):\n            if inspect.signature(pairwise_distance).parameters.get('metric') is not None:\n                return pairwise_distance(X, metric=self.distance_metric)\n            else:\n                return pairwise_distance(X, metric=self.distance_metric, **self.distance_metric_params)\n        else:\n            if inspect.signature(self.distance_metric).parameters.get('metric') is not None:\n                return self.distance_metric(X, metric=self.distance_metric)\n            else:\n                return self.distance_metric(X, metric=self.distance_metric, **self.distance_metric_params)\n    else:\n        if isinstance(self.distance_metric, str):\n            if inspect.signature(pairwise_distance).parameters.get('metric') is not None:\n                return pairwise_distance(X, X2, metric=self.distance_metric)\n            else:\n                return pairwise_distance(X, X2, metric=self.distance_metric, **self.distance_metric_params)\n        else:\n            if inspect.signature(self.distance_metric).parameters.get('metric') is not None:\n                return self.distance_metric(X, X2, metric=self.distance_metric)\n            else:\n                return self.distance_metric(X, X2, metric=self.distance_metric, **self.distance_metric_params)\n", "CodeBleu_score": 22.397543445420002}, {"code": " def _distance(self, X, X2=None)\n    if X2 is None:\n        if inspect.signature(self.distance).parameters:\n            return self.distance(X)\n        else:\n            return self.distance()\n    else:\n        if isinstance(self.distance, str):\n            return pairwise_distance(X, X2, metric=self.distance, metric_params=self.distance_params)\n        else:\n            return self.distance(X, X2, **self.distance_params)\n", "CodeBleu_score": 17.14478114380726}, {"code": " def _distance(self, X, X2=None)\n    if X2 is None:\n        if inspect.signature(self._distance).parameters.get(\"X2\") is not None:\n            return self._distance(X)\n        else:\n            return self._distance(X, X)\n    else:\n        return self._distance(X, X2)", "CodeBleu_score": 12.105365833559995}]}
{"instance_id": "497d73b298ee", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n    import pandas as pd\n    import numpy as np\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    if type(X) == pd.core.frame.DataFrame:\n        X = X.reset_index(drop=True)\n        X = X.pivot(index='index', columns='series', values='value')\n        X = X.fillna(0)\n        X = X.to_numpy()\n        lengths = np.array([len(X[i]) for i in range(len(X))])\n        X = np.concatenate((lengths.reshape(-1, 1), X), axis=1)\n    elif type(X) == np.ndarray:\n        if len(X.shape) == 1:\n            X = X.reshape(-1, 1)\n        elif len(X.shape) == 2:\n            if X.shape[1] == 1:\n                X = X.reshape(-1, 1)\n    else:\n        raise ValueError('Input data must be a 2D numpy array or a pandas dataframe.')\n    return X\n", "CodeBleu_score": 36.32772099617857}, {"code": "def _convert_X_to_sklearn(self, X)\n    import pandas as pd\n    import numpy as np\n    if isinstance(X, pd.Series):\n        X = X.values\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n    if isinstance(X, pd.Series):\n        X = X.values\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n    if isinstance(X, pd.Series):\n        X = X.values\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n    if isinstance(X, pd.Series):\n        X = X.values\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n    if isinstance(X, pd.Series):\n        X = X.values\n    if isinstance(X, np.ndarray):\n        if X", "CodeBleu_score": 15.700818937642087}, {"code": "def _convert_X_to_sklearn(self, X)\n    import numpy as np\n    if len(X) > 0:\n        if len(X) != len(X[0]):\n            X_new = pd.DataFrame(X).reset_index()\n            X_new = X_new.pivot(index='index', columns='level_0', values=0)\n            X_new = X_new.fillna(0)\n            X_new = X_new.to_numpy()\n            lengths = np.array([len(x) for x in X])\n            X_new = np.concatenate((lengths.reshape(-1, 1), X_new), axis=1)\n        else:\n            if type(X[0]) == list:\n                X_new = np.array(X).flatten()\n            else:\n                X_new = np.array(X)\n    else:\n        X_new = np.array([])\n    return X_new\n\n", "CodeBleu_score": 37.70114817593825}]}
{"instance_id": "8798260a6f12", "generation_result": [{"code": "def is_sklearn_estimator(obj)\n    from sklearn.base import is_classifier, is_regressor\n    return is_classifier(obj) or is_regressor(obj)\n\n", "CodeBleu_score": 12.727043674699518}, {"code": "def is_sklearn_estimator(obj)\n    from sklearn.base import BaseEstimator\n    from sktime.base import BaseObject\n    return isinstance(obj, type) and issubclass(obj, BaseEstimator) and not issubclass(obj, BaseObject)", "CodeBleu_score": 23.856042154354796}, {"code": "def is_sklearn_estimator(obj)\n    from sktime.base import SklearnBaseEstimator\n\n    return isinstance(obj, type) and issubclass(obj, SklearnBaseEstimator)", "CodeBleu_score": 16.142863587086982}]}
{"instance_id": "2ab2596d8520", "generation_result": [{"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return TransformerPipeline(self.transformers + other.transformers)\n    else:\n        return NotImplemented", "CodeBleu_score": 16.318171387832432}, {"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return TransformerPipeline(self.transformers + other.transformers)\n    elif isinstance(other, SklearnRegressorPipeline):\n        return SklearnRegressorPipeline(regressor=other.regressor, transformers=self.transformers)\n    else:\n        return NotImplemented\n", "CodeBleu_score": 35.450227290316725}, {"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return TransformerPipeline(self.transformers + other.transformers)\n    return NotImplemented\n", "CodeBleu_score": 16.286605234397754}]}
{"instance_id": "5c1c6d14ca93", "generation_result": [{"code": "def prep_skl_df(df, copy_df=False)\n    if copy_df:\n        df = df.copy()\n    df.columns = df.columns.astype(str)\n    if not all(df.columns.dtype == 'O'):\n        df.columns = df.columns.astype(str)\n    return df", "CodeBleu_score": 51.83159366131734}, {"code": "def prep_skl_df(df, copy_df=False)\n    df.columns = df.columns.astype(str)\n    if copy_df:\n        df = df.copy()\n    return df\n\n", "CodeBleu_score": 32.69385129482958}, {"code": "def prep_skl_df(df, copy_df=False)\n    if copy_df:\n        df = df.copy()\n    df.columns = df.columns.astype(str)\n    if not all(df.columns.dtype == \"O\"):\n        raise ValueError(\"All columns must be of type object\")\n    return df\n\n", "CodeBleu_score": 49.512722875441064}]}
{"instance_id": "d5e567c20361", "generation_result": [{"code": "def predict(self, X)\n    rng = np.random.RandomState(self.random_state)\n    probs = self.predict_proba(X)\n    return rng.choice(self.classes_, size=len(X), p=probs.T)", "CodeBleu_score": 33.00638345760848}, {"code": "def predict(self, X)\n    rng = check_random_state(self.random_state)\n    y_pred = np.empty(X.shape[0], dtype=np.int)\n    for i, x in enumerate(X):\n        probs = self.predict_proba(x)\n        y_pred[i] = rng.choice(np.flatnonzero(probs == probs.max()))\n    return y_pred", "CodeBleu_score": 50.46096100150352}, {"code": "def predict(self, X)\n    rng = np.random.RandomState(self.random_state)\n    y_pred = np.empty((X.shape[0],), dtype=np.int)\n    for i, x in enumerate(X):\n        y_pred[i] = rng.choice(np.flatnonzero(self.predict_proba(x)))\n    return y_pred", "CodeBleu_score": 45.22628012204106}]}
{"instance_id": "210b60dffd55", "generation_result": [{"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    # Transform the input data by applying the corresponding PCA to each group and concatenating the results\n    X_transformed = np.concatenate([pca.transform(X[:, group]) for pca, group in zip(pcas, groups)], axis=1)\n    X_transformed = np.nan_to_num(X_transformed, copy=False)\n    # Compute class probabilities using the classifier\n    y_proba = clf.predict_proba(X_transformed)\n    # Adjust the probabilities if the number of predicted classes does not match the expected number of classes\n    if y_proba.shape[1] != self.n_classes:\n        y_proba = y_proba[:, :self.n_classes]\n        y_proba = y_proba / y_proba.sum(axis=1, keepdims=True)\n    return y_proba", "CodeBleu_score": 23.987232830149697}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    # Transform the input data by applying the corresponding PCA to each group\n    X_transformed = []\n    for i in range(len(groups)):\n        if pcas[i] is not None:\n            X_transformed.append(pcas[i].transform(X[:, groups[i]]))\n        else:\n            X_transformed.append(X[:, groups[i]])\n    X_transformed = np.concatenate(X_transformed, axis=1)\n\n    # Replace missing values with zeros\n    X_transformed[np.isnan(X_transformed)] = 0\n\n    # Compute class probabilities using the classifier\n    if clf.n_classes_ == 1:\n        # If the classifier is a binary classifier, predict the probability of the positive class\n        y_proba = clf.predict_proba(X_transformed)[:, 1]\n    else:\n        # If the classifier is a multiclass classifier, predict the probabilities for all classes\n        y_proba = clf.predict_proba(X_transformed)\n\n    # Adjust the probabilities to match the expected class structure\n    if clf.n_classes_ != len(self.classes_):\n        # If the number of predicted classes does not match the expected number of classes, adjust the probabilities\n        y_proba = self._adjust_proba_for_class_structure(y_proba, self.classes_)\n\n    return y_proba\n", "CodeBleu_score": 33.753556130291386}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    # Transform the data by applying the corresponding PCA to each group\n    X_transformed = np.concatenate([X[:, group] @ pcas[group] for group in groups], axis=1)\n\n    # Replace missing values with zeros\n    X_transformed = np.nan_to_num(X_transformed)\n\n    # Compute class probabilities using the classifier\n    y_pred_proba = clf.predict_proba(X_transformed)\n\n    # Adjust the probabilities if the number of predicted classes does not match the expected number of classes\n    if y_pred_proba.shape[1] != self.n_classes_:\n        y_pred_proba = self._adjust_proba(y_pred_proba)\n\n    return y_pred_proba\n", "CodeBleu_score": 13.968849152191169}]}
{"instance_id": "9436ee0860bb", "generation_result": [{"code": "def predict_proba(self, X)\n    if not self.is_fitted_:\n        raise NotFittedError(\"Model is not fitted yet.\")\n    if self.n_classes_ == 1:\n        return np.ones((X.shape[0], 1))\n    if isinstance(X, np.ndarray) and X.ndim == 3 and X.shape[1] == 1:\n        X = X.reshape(-1)\n    if not isinstance(X, np.ndarray) or X.ndim != 1:\n        raise ValueError(\"Invalid input type or shape.\")\n    if X.shape[0] == 0:\n        raise ValueError(\"Invalid input shape.\")\n    X = self._validate_X(X)\n    probabilities = np.zeros((X.shape[0], self.n_classes_))\n    for i, x in enumerate(X):\n        probabilities[i, :] = self.root_node.predict_proba(x)\n    return probabilities", "CodeBleu_score": 41.21670518941238}, {"code": "def predict_proba(self, X)\n    if not self.fitted:\n        raise ValueError(\"The model is not fitted yet.\")\n    if X.ndim == 3 and X.shape[1] == 1:\n        X = X.reshape(-1)\n    elif X.ndim != 1:\n        raise ValueError(\"Invalid input type or shape.\")\n    if np.isnan(X).any():\n        raise ValueError(\"Invalid input: NaN values are not allowed.\")\n    probs = []\n    for x in X:\n        probs.append(self.root.predict_proba(x))\n    return np.array(probs)", "CodeBleu_score": 20.41830623662592}, {"code": "def predict_proba(self, X)\n    if not self.is_fitted_:\n        raise ValueError(\"Model is not fitted yet. Call fit method first.\")\n\n    # Validate the input data\n    if not isinstance(X, np.ndarray):\n        raise TypeError(\"Input data must be a numpy array.\")\n\n    if len(X.shape) == 3 and X.shape[2] == 1:\n        # Reshape the input to 2D if it is a 3D array with a single feature\n        X = X.reshape((X.shape[0], X.shape[1]))\n    elif len(X.shape) != 2:\n        raise ValueError(\"Invalid input data shape. Expected 2D array or 3D array with a single feature.\")\n\n    # Validate the input data\n    if not np.all(np.isfinite(X)):\n        raise ValueError(\"Input data contains NaN or infinite values.\")\n\n    # Calculate the probability distributions for each input sample\n    proba = np.zeros((X.shape[0], len(self.classes_)))\n    for i, x in enumerate(X):\n        proba[i] = self.root_node.predict_proba(x)\n\n    return proba", "CodeBleu_score": 38.7691530008347}]}
{"instance_id": "d7cb1cb66840", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n    if self.transformers_ is None:\n        raise ValueError(\"The transformers_ attribute is not set. Please call fit before calling transform.\")\n    if self.transformers_[\"output_scitype\"] == \"Primitives\":\n        if isinstance(X, pd.DataFrame):\n            return X.to_numpy()\n        elif isinstance(X, pd.Series):\n            return X.to_numpy().reshape(-1, 1)\n        elif isinstance(X, np.ndarray):\n            return X\n        else:\n            raise TypeError(\"Unexpected output type: \" + str(type(X)))\n    elif self.transformers_[\"output_scitype\"] == \"Series\":\n        if isinstance(X, pd.DataFrame):\n            return X.to_numpy().reshape(-1, X.shape[1])\n        elif isinstance(X, pd.Series):\n            return X.to_numpy().reshape(-1, 1)\n        elif isinstance(X, np.ndarray):\n            return X\n        else:\n            raise TypeError(\"Unexpected output type: \" + str(type(X)))\n    else:\n        raise TypeError(\"Unexpected output type: \" + str(type(X)))", "CodeBleu_score": 17.096080296802192}, {"code": "def _convert_X_to_sklearn(self, X)\n    if self.transformers_ is None:\n        raise ValueError(\"The model has not been fitted yet.\")\n    if self.transformers_[0].scitype() == \"Primitives\":\n        X = self._convert_X_to_sklearn_primitives(X)\n    elif self.transformers_[0].scitype() == \"Series\":\n        X = self._convert_X_to_sklearn_series(X)\n    else:\n        raise TypeError(f\"Unexpected scitype {self.transformers_[0].scitype()}\")\n    return X\n", "CodeBleu_score": 23.335071027597465}, {"code": "def _convert_X_to_sklearn(self, X)\n    if self.transformers_ is not None:\n        scitype = self.transformers_[0].scitype\n        if scitype == \"Primitives\":\n            if isinstance(X, pd.DataFrame):\n                return X.values\n            elif isinstance(X, np.ndarray):\n                return X\n            elif isinstance(X, pd.Series):\n                return X.values.reshape(-1, 1)\n            elif isinstance(X, list):\n                return np.array(X)\n            elif isinstance(X, pd.Panel):\n                return X.values\n            else:\n                raise TypeError(\n                    \"X should be a Pandas DataFrame, Numpy array, Pandas Series, list, or Pandas Panel\"\n                )\n        elif scitype == \"Series\":\n            if isinstance(X, pd.DataFrame):\n                return X.values.flatten()\n            elif isinstance(X, np.ndarray):\n                return X.flatten()\n            elif isinstance(X, pd.Series):\n                return X.values\n            elif isinstance(X, list):\n                return np.array(X).flatten()\n            elif isinstance(X, pd.Panel):\n                return X.values.flatten()\n            else:\n                raise TypeError(\n                    \"X should be a Pandas DataFrame, Numpy array, Pandas Series, list, or Pandas Panel\"\n                )\n        else:\n            raise TypeError(\n                \"Unexpected output type from transformers_. Expected Primitives or Series, got {}\".format(\n                    scitype\n                )\n            )\n    else:\n        if isinstance(X, pd.DataFrame):\n            return X.values\n        elif isinstance(X, np.ndarray):\n            return X\n        elif isinstance(X, pd.Series):\n            return X.values.reshape(-1, 1)\n        elif isinstance(X, list):\n            return np.array(X)\n        elif isinstance(X, pd.Panel):\n            return X.values\n        else:\n            raise TypeError(", "CodeBleu_score": 32.0649298470378}]}
{"instance_id": "497cb75f2b4d", "generation_result": [{"code": "def set_random_state(estimator, random_state=0)\n    if random_state is None:\n        random_state = np.random.RandomState()\n    elif isinstance(random_state, int):\n        random_state = np.random.RandomState(random_state)\n    elif not isinstance(random_state, np.random.RandomState):\n        raise ValueError(\"random_state must be an integer or a numpy random state object\")\n    params = {}\n    for param_name, param_value in estimator.get_params().items():\n        if param_name.endswith(\"random_state\"):\n            params[param_name] = random_state.randint(1000000000)\n    if params:\n        estimator.set_params(**params)\n    return estimator\n\n", "CodeBleu_score": 31.276735930066685}, {"code": "def set_random_state(estimator, random_state=0)\n    import numpy as np\n    import warnings\n    warnings.filterwarnings('ignore')\n    if random_state is None:\n        return estimator\n    if isinstance(random_state, int):\n        random_state = np.random.RandomState(random_state)\n    elif isinstance(random_state, np.random.RandomState):\n        pass\n    else:\n        raise ValueError(\"Invalid random state\")\n    params = {}\n    for param_name, param_value in estimator.get_params().items():\n        if 'random_state' in param_name:\n            params[param_name] = random_state.randint(0, 1000000)\n    if params:\n        estimator.set_params(**params)\n    return estimator\n", "CodeBleu_score": 31.412316963992993}, {"code": "def set_random_state(estimator, random_state=0)\n    import random\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import scipy.stats as stats\n    import sklearn\n    import sklearn.linear_model\n    import sklearn.ensemble\n    import sklearn.preprocessing\n    import sklearn.metrics\n    import sklearn.model_selection\n    import sklearn.pipeline\n    import sklearn.compose\n    import sklearn.impute\n    import sklearn.feature_selection\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction.text\n    import sklearn.feature_selection\n    import sklearn.decomposition\n    import sklearn.cluster\n    import sklearn.metrics.pairwise\n    import sklearn.metrics.pairwise import cosine_similarity\n    import sklearn.metrics.pairwise import euclidean_distances\n    import sklearn.metrics.pairwise import manhattan_distances\n    import sklearn.metrics.pairwise import haversine_distances\n    import sklearn.metrics.pairwise import chebyshev_distances\n    import sklearn.metrics.pairwise import canberra_distances\n    import sklearn.metrics.pairwise import braycurtis_distances\n    import sklearn.metrics.pairwise import mahalanobis_distances\n    import sklearn.metrics.pairwise import wminkowski_distances\n    import sklearn.metrics.pairwise import seuclidean_distances\n    import sklearn.metrics.pairwise import cosine_similarity\n    import sklearn.metrics.pairwise import euclidean_distances\n    import sklearn.metrics.pairwise import manhattan_distances\n    import sklearn.metrics.pairwise import haversine_distances\n    import sklearn.metrics.pairwise import chebyshev_distances\n    import sklearn.metrics.pairwise import canberra_distances\n    import sklearn.metrics.pairwise import braycurtis_distances\n    import sklearn.metrics.pairwise import", "CodeBleu_score": 9.020690206912308}]}
{"instance_id": "59ffd1d596a1", "generation_result": [{"code": "def _transform(X, intervals)\n    n_instances, n_intervals = X.shape[0], len(intervals)\n    transformed = np.zeros((n_intervals, n_instances))\n    for i, (start, end) in enumerate(intervals):\n        X_slice = X[:, start:end]\n        mean = np.mean(X_slice, axis=1)\n        std = np.std(X_slice, axis=1)\n        slope = np.diff(X_slice, axis=1)\n        transformed[i, :] = np.concatenate((mean, std, slope))\n    return transformed.T\n", "CodeBleu_score": 32.75335515786}, {"code": "def _transform(X, intervals)\n    n_instances, n_intervals = X.shape[0], len(intervals)\n    transformed = np.empty((n_intervals, n_instances))\n    for i, interval in enumerate(intervals):\n        start, end = interval\n        slice = X[:, start:end]\n        mean = np.mean(slice, axis=1)\n        std = np.std(slice, axis=1)\n        slope = np.diff(slice, axis=1)\n        transformed[i, :] = np.concatenate((mean, std, slope))\n    return transformed.T", "CodeBleu_score": 32.51249928646621}, {"code": "def _transform(X, intervals)\n    n_instances, n_features = X.shape\n    n_intervals = len(intervals)\n    transformed = np.zeros((n_features, n_intervals))\n\n    for i, interval in enumerate(intervals):\n        start, end = interval\n        X_slice = X[:, start:end]\n        mean = np.mean(X_slice, axis=0)\n        std = np.std(X_slice, axis=0)\n        slope = np.mean(np.diff(X_slice, axis=0), axis=0)\n        transformed[:, i] = np.concatenate([mean, std, slope])\n\n    return transformed.T\n", "CodeBleu_score": 33.043726130334115}]}
{"instance_id": "5a01594eebfb", "generation_result": [{"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n    if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n    if plot_params is None:\n        plot_params = {}\n    if configuration is None:\n        configuration = {}\n    if not isinstance(plot, str):\n        raise TypeError(\"plot must be a string\")\n    if not isinstance(plot_params, dict):\n        raise TypeError(\"plot_params must be a dictionary\")\n    if not isinstance(configuration, dict):\n        raise TypeError(\"configuration must be a dictionary\")\n    if not isinstance(annotation_func, str):\n        raise TypeError(\"annotation_func must be a string\")\n    if not isinstance(annotation_params, dict):\n        raise TypeError(\"annotation_params must be a dictionary\")\n    if not isinstance(ax_op_before, list):\n        raise TypeError(\"ax_op_before must be a list\")\n    if not isinstance(ax_op_after, list):\n        raise TypeError(\"ax_op_after must be a list\")\n    if not isinstance(annotate_params, dict):\n        raise TypeError(\"annotate_params must be a dictionary\")\n    if not isinstance(kwargs, dict):\n        raise TypeError(\"kwargs must be a dictionary\")\n    if not isinstance(args, tuple):\n        raise TypeError(\"args must be a tuple\")\n    if not isinstance(plot_params, dict):\n        raise TypeError(\"plot_params must be a dictionary\")\n    if not isinstance(configuration, dict):\n        raise TypeError(\"configuration must be a dictionary\")\n    if not isinstance(annotation_func, str):\n        raise TypeError(\"annotation_func must be a string\")\n    if not isinstance(annotation_params, dict):\n        raise TypeError(\"annotation_params must be a dictionary\")\n    if not isinstance(ax_op_before, list):\n        raise TypeError(\"ax_op_before must be a list\")\n    if not isinstance(ax_", "CodeBleu_score": 33.53506208112061}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n    # Initialize annotation and operation parameters\n    if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n\n    # Create the plot\n    plot_func = getattr(sns, plot)\n    ax = plot_func(**plot_params)\n\n    # Configure the plot\n    for key, value in configuration.items():\n        if key == 'colors':\n            sns.set_palette(value)\n        elif key == 'style':\n            sns.set_style(value)\n        elif key == 'context':\n            sns.set_context(value)\n        elif key == 'font_scale':\n            sns.set_context(font_scale=value)\n        elif key == 'font':\n            sns.set_context(font=value)\n        elif key == 'font_scale':\n            sns.set_context(font_scale=value)\n        elif key == 'font':\n            sns.set_context(font=value)\n        elif key == 'rc':\n            sns.set_context(rc=value)\n        elif key == 'rc_params':\n            sns.set_context(rc_params=value)\n        elif key == 'axes_style':\n            sns.set_axes_style(value)\n        elif key == 'axes_style_params':\n            sns.set_axes_style(**value)\n        elif key == 'axes_style_context':\n            sns.set_axes_style(context=value)\n        elif key == 'axes_style_context_params':\n            sns.set_axes_style(context=value, **kwargs)\n        elif key == 'axes_style_context_rc':\n            sns.set_axes_style(context=value, rc=value)\n        elif key == 'axes_style_context_rc_params':\n            sns.set_axes_", "CodeBleu_score": 43.51435606993762}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\n    # Initialize annotation and operation parameters if they are not provided\n    if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n\n    # Create the plot with given parameters and apply pre-plot axis operations\n    ax = sns.catplot(**plot_params)\n    if ax_op_before is not None:\n        for op in ax_op_before:\n            if isinstance(op, str):\n                getattr(ax, op)(**kwargs)\n            elif isinstance(op, list):\n                getattr(ax, op[0])(*op[1], **kwargs)\n            elif isinstance(op, dict):\n                getattr(ax, op['op'])(*op['args'], **op['kwargs'])\n\n    # Configure the plot based on provided configuration\n    for key, value in configuration.items():\n        if isinstance(value, dict):\n            getattr(ax, key)(**value)\n        else:\n            getattr(ax, key)(value)\n\n    # Annotate the plot based on provided annotation function and parameters\n    if annotation_func is not None:\n        annotation_func(ax, **annotation_params)\n\n    # Apply post-plot axis operations\n    if ax_op_after is not None:\n        for op in ax_op_after:\n            if isinstance(op, str):\n                getattr(ax, op)(**kwargs)\n            elif isinstance(op, list):\n                getattr(ax, op[0])(*op[1], **kwargs)\n            elif isinstance(op, dict):\n                getattr(ax, op['op'])(*op['args'], **op['kwargs'])\n\n    # Return the output of the plotting process\n    return ax", "CodeBleu_score": 42.833187532997506}]}
{"instance_id": "b5b17d312c74", "generation_result": [{"code": "def predict_proba(self, X)\n    if not hasattr(self, '_program'):\n        raise ValueError('Model not fitted yet.')\n    X = self._validate_X(X)\n    if X.shape[1] != self._feature_count:\n        raise ValueError(f'Expected {self._feature_count} features, got {X.shape[1]}')\n    scores = self._program(X)\n    probs = np.exp(scores) / (1 + np.exp(scores))\n    return np.hstack((1 - probs, probs))", "CodeBleu_score": 33.82739722312873}, {"code": "def predict_proba(self, X)\n    if not hasattr(self, '_program'):\n        raise ValueError(\"Model has not been fitted yet.\")\n    X = self._validate_X(X)\n    if X.shape[1] != self.n_features_:\n        raise ValueError(\"X has {} features per sample, expected {}\".format(X.shape[1], self.n_features_))\n    scores = self._program.run(X)\n    probas = self._sigmoid(scores)\n    return np.vstack([1 - probas, probas]).T", "CodeBleu_score": 38.02325485884485}, {"code": "def predict_proba(self, X)\n    if not hasattr(self, \"_program\"):\n        raise ValueError(\"Model has not been fitted yet\")\n    if not isinstance(X, pd.DataFrame):\n        raise ValueError(\"Input data must be a pandas DataFrame\")\n    if X.shape[1] != self._expected_feature_count:\n        raise ValueError(\n            f\"Expected {self._expected_feature_count} features, got {X.shape[1]}\"\n        )\n    scores = self._program.execute(X)\n    probs = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    return probs", "CodeBleu_score": 41.53360604310855}]}
{"instance_id": "83f71c959df7", "generation_result": [{"code": "def point_mutation(self, random_state)\n    # Create a copy of the program\n    mutated_program = copy.deepcopy(self)\n    # Get the list of all nodes in the program\n    nodes = mutated_program.get_nodes()\n    # Get the number of nodes in the program\n    num_nodes = len(nodes)\n    # Generate a uniform random distribution for node selection\n    node_selection = random_state.uniform(size=num_nodes)\n    # Define the probability threshold for node selection\n    threshold = 0.5\n    # Identify nodes to mutate based on the uniform random distribution and the threshold\n    mutation_nodes = [i for i, node in enumerate(nodes) if node_selection[i] < threshold]\n    # Iterate over the nodes to mutate\n    for node_index in mutation_nodes:\n        # Get the node to be mutated\n        node = nodes[node_index]\n        # If the node is a function, find a valid replacement function with the same arity\n        if node.is_function():\n            # Get the arity of the function\n            arity = node.get_arity()\n            # Find a valid replacement function with the same arity\n            replacement_function = self.get_valid_replacement_function(arity, random_state)\n            # Update the node with the replacement function\n            nodes[node_index] = replacement_function\n        # If the node is a terminal, replace it with either a constant or a variable\n        else:\n            # Generate a uniform random distribution for terminal selection\n            terminal_selection = random_state.uniform(size=2)\n            # Define the probability threshold for terminal selection\n            threshold = 0.5\n            # If the first random number is less than the threshold, replace the terminal with a constant\n            if terminal_selection[0] < threshold:\n                # Generate a uniform random distribution for constant selection\n                constant_selection = random_state.uniform(size=len(self.constants))\n                # Define the probability threshold for constant selection\n                threshold = 0.5\n                # Identify the constant to be used for replacement\n                constant_index = np.argmax(constant_selection < threshold)\n                # Update the node with the selected constant\n                nodes[node_index] =", "CodeBleu_score": 24.631779793712933}, {"code": "def point_mutation(self, random_state)\n    mutated_program = self.program.copy()\n    modified_nodes = []\n    for node in mutated_program.nodes:\n        if random_state.uniform() < self.mutation_probability:\n            if node.is_function():\n                replacement_function = self.get_replacement_function(node.function_name, node.arity)\n                if replacement_function is not None:\n                    node.function_name = replacement_function\n                    modified_nodes.append(node.id)\n            else:\n                replacement_value = self.get_replacement_value()\n                node.value = replacement_value\n                modified_nodes.append(node.id)\n    return mutated_program, modified_nodes\n", "CodeBleu_score": 18.7493434617484}, {"code": "def point_mutation(self, random_state)\n    # Create a copy of the program\n    mutated_program = self.copy()\n    # Initialize the list of modified node indices\n    modified_nodes = []\n    # Iterate through all nodes in the program\n    for node in mutated_program.nodes:\n        # Check if the node is a function\n        if isinstance(node, Function):\n            # Check if the node should be mutated based on the probability threshold\n            if random_state.uniform(0, 1) < self.mutation_probability:\n                # Find a replacement function with the same arity\n                replacement_function = self.get_function_by_arity(node.arity)\n                # If a replacement function was found, update the node\n                if replacement_function is not None:\n                    node.function = replacement_function\n                    modified_nodes.append(node.index)\n        # Check if the node is a terminal\n        elif isinstance(node, Terminal):\n            # Check if the node should be mutated based on the probability threshold\n            if random_state.uniform(0, 1) < self.mutation_probability:\n                # Replace the terminal with a constant or a variable\n                if random_state.uniform(0, 1) < self.constant_probability:\n                    # Choose a random constant from the defined constant range\n                    node.value = random_state.choice(self.constant_range)\n                else:\n                    # Choose a random feature from the list of features\n                    node.value = random_state.choice(self.features)\n                modified_nodes.append(node.index)\n    # Return the mutated program and the list of modified node indices\n    return mutated_program, modified_nodes", "CodeBleu_score": 24.61211827248565}]}
{"instance_id": "f1853251e671", "generation_result": [{"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate arguments\n    length = validate_int(length, 20)\n    mamode = validate_ma_mode(mamode)\n    offset = validate_offset(offset, length)\n    close = verify_series(close)\n    # Calculate Result\n    ma = close.rolling(length, min_periods=length).mean()\n    bias = (close / ma) * 100 - 100\n    bias = bias.shift(offset)\n    # Handle fills\n    if 'fillna' in kwargs:\n        bias.fillna(kwargs['fillna'], inplace=True)\n    if 'fill_method' in kwargs:\n        bias.fillna(method=kwargs['fill_method'], inplace=True)\n    # Name and Categorize it\n    bias.name = f\"BIAS_{length}\"\n    bias.category = 'momentum'\n    return bias", "CodeBleu_score": 34.062888867632736}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate arguments\n    length = validate_int(length, 20)\n    mamode = validate_ma_mode(mamode)\n    offset = validate_int(offset, 0)\n\n    # Calculate Bias\n    ma = ma(close, length=length, mamode=mamode, **kwargs)\n    bias = close - ma\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n    # Name and Categorize it\n    bias.name = f\"BIAS_{length}\"\n    bias.category = \"momentum\"\n\n    return bias", "CodeBleu_score": 43.86238138586568}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate inputs\n    length = int(length) if length and length > 0 else 26\n    mamode = mamode if mamode in ['sma', 'ema', 'wma'] else 'sma'\n    offset = int(offset) if offset and offset > 0 else 0\n\n    # Calculate moving average\n    ma = ta.ma(close, length=length, mamode=mamode, **kwargs)\n\n    # Calculate bias\n    bias = (close - ma) / ma * 100\n\n    # Shift bias\n    bias = bias.shift(offset)\n\n    # Validate offset\n    if offset != 0:\n        bias = ta.offset(bias, offset, method=kwargs.get('fillna'))\n\n    # Name and categorize series\n    bias.name = f\"BIAS_{length}\"\n    bias.category = 'momentum'\n\n    return bias", "CodeBleu_score": 37.4485908451618}]}
{"instance_id": "77825be37c24", "generation_result": [{"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    import pandas as pd\n    import requests\n    import json\n    import traceback\n    try:\n        if broker == 'hb':\n            url = f'https://api.huobi.pro/market/history/kline?symbol={code.upper()}&period={ktype}&size={size}'\n            data = requests.get(url).json()['data']\n            df = pd.DataFrame(data)\n            df.columns = ['date', 'open', 'close', 'low', 'high', 'amount', 'vol', 'count']\n            df['date'] = df['date'].apply(lambda x: pd.to_datetime(x, unit='ms'))\n            df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n            df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S')\n            df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n            df['open'] = df['open'].astype(float)\n            df['close'] = df['close'].astype(float)\n            df['low'] = df['low'].astype(float)\n            df['high'] = df['high'].astype(float)\n            df['amount'] = df['amount'].astype(float)\n            df['vol'] = df['vol'].astype(float)\n            df['count'] = df['count'].astype(float)\n            df = df.set_index('date')\n            return df\n        elif broker == 'binance':\n            url = f'https://api.binance.com/api/v3/klines?symbol={code.upper()}{ktype}&limit={size}'\n            data = requests.get(url).json()\n            df = pd.DataFrame(data)\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'vol', 'close_time', 'quote_asset_vol', 'num_trades', '", "CodeBleu_score": 34.80419606114283}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    try:\n        if broker == 'hb':\n            url = f'https://api.huobi.pro/market/history/kline?symbol={code.upper()}&period={ktype.lower()}&size={size}'\n            data = requests.get(url).json()['data']\n            df = pd.DataFrame(data)\n            df.columns = ['date', 'open', 'close', 'low', 'high', 'amount', 'vol', 'count']\n            df['date'] = pd.to_datetime(df['date'], unit='ms')\n            df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n            df['date'] = pd.to_datetime(df['date'])\n            df = df.set_index('date')\n            df.index.name = None\n        elif broker == 'binance':\n            url = f'https://api.binance.com/api/v3/klines?symbol={code.upper()}{ktype.upper()}&limit={size}'\n            data = requests.get(url).json()\n            df = pd.DataFrame(data)\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'amount', 'vol', 'count', 'taker_base_vol', 'taker_quote_vol', 'ignore']\n            df['date'] = pd.to_datetime(df['date'], unit='ms')\n            df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n            df['date'] = pd.to_datetime(df['date'])\n            df = df.set_index('date')\n            df.index.name = None\n        elif broker == 'okex':\n            url = f'https://www.okex.com/api/v5/market/candles?instId={code.upper()}-{ktype.upper()}&limit={size}'\n            data = requests.get(url).json()['data']\n            df = pd.DataFrame(data", "CodeBleu_score": 32.8104320809247}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    try:\n        # Define the URL for the specified broker, cryptocurrency code, kline type, and size\n        url = f'https://api.{broker}.com/market/history/kline?symbol={code}&period={ktype}&size={size}'\n\n        # Fetch the data from the URL\n        data = requests.get(url).json()['data']\n\n        # Handle any broker-specific data structures\n        if broker == 'hb':\n            data = data['items']\n        elif broker == 'binance':\n            data = data['data']\n\n        # Convert the data into a pandas DataFrame with specific columns\n        df = pd.DataFrame(data)\n        df.columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'amount']\n\n        # Adjust date format based on the broker and kline type\n        if broker == 'hb':\n            df['date'] = pd.to_datetime(df['date'], unit='ms')\n        elif broker == 'binance':\n            df['date'] = pd.to_datetime(df['date'], unit='ms')\n\n        # Convert the date column to datetime format\n        df['date'] = pd.to_datetime(df['date'])\n\n        # Return the DataFrame\n        return df\n    except Exception as e:\n        # Print the traceback if an exception occurs\n        print(e)\n        traceback.print_exc()\n", "CodeBleu_score": 34.14370807216757}]}
{"instance_id": "c6552851da8a", "generation_result": [{"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    try:\n        from pandas import Panel\n        from pandas import DataFrame\n        from pandas import Timestamp\n        from pandas import to_datetime\n        from pandas import concat\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime\n        from pandas import to_datetime", "CodeBleu_score": 10.403785961773327}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    import pandas as pd\n    import traceback\n    try:\n        if broker == 'hb':\n            data = helper_hb(code, size)\n            ask = pd.DataFrame(data['asks'], columns=['price', 'amount'])\n            bid = pd.DataFrame(data['bids'], columns=['price', 'amount'])\n            ask['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n            bid['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n            return pd.Panel({'ask': ask, 'bid': bid})\n        elif broker == 'btcchina':\n            data = helper_btcchina(code, size)\n            ask = pd.DataFrame(data['asks'], columns=['price', 'amount'])\n            bid = pd.DataFrame(data['bids'], columns=['price', 'amount'])\n            ask['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n            bid['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n            return pd.Panel({'ask': ask, 'bid': bid})\n        elif broker == 'bitstamp':\n            data = helper_bitstamp(code, size)\n            ask = pd.DataFrame(data['asks'], columns=['price', 'amount'])\n            bid = pd.DataFrame(data['bids'], columns=['price', 'amount'])\n            ask['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n            bid['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n            return pd.Panel({'ask': ask, 'bid': bid})\n        elif broker == 'bitfinex':\n            data = helper_bitfinex(code, size)\n            ask = pd.DataFrame(data['asks'], columns=['price', 'amount'])\n            bid = pd.DataFrame(data['bids'], columns=['price', 'amount'])\n            ask['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n           ", "CodeBleu_score": 31.27116045724172}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    try:\n        data = get_snapshot(broker, code)\n        ask = pd.DataFrame(data['asks'], columns=['price', 'amount'])\n        bid = pd.DataFrame(data['bids'], columns=['price', 'amount'])\n        if broker == 'hb':\n            timestamp = pd.to_datetime(data['time'], unit='ms')\n        elif broker == 'ok':\n            timestamp = pd.to_datetime(data['timestamp'], unit='ms')\n        ask['timestamp'] = timestamp\n        bid['timestamp'] = timestamp\n        return pd.Panel(dict(ask=ask, bid=bid))\n    except Exception as e:\n        print(e)\n        traceback.print_exc()", "CodeBleu_score": 36.45244182401685}]}
{"instance_id": "8b5fee723398", "generation_result": [{"code": "def coins_trade(broker='hb', code='btc')\n    import pandas as pd\n    from datetime import datetime\n    import requests\n    import json\n    import time\n    import numpy as np\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    url = 'https://api.huobi.pro/market/history/trade?symbol=' + code.upper()\n    r = requests.get(url)\n    data = r.json()\n    if data is not None:\n        if broker == 'hb':\n            df = pd.DataFrame(data['data'])\n            df = df.loc[:, ['id', 'data-time', 'price', 'amount', 'direction']]\n            df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            df['datetime'] = df['datetime'].apply(lambda x: datetime.fromtimestamp(x / 1000))\n            df['type'] = df['type'].apply(lambda x: 'buy' if x == 'buy-market' else 'sell')\n            df['price'] = df['price'].astype(float)\n            df['amount'] = df['amount'].astype(float)\n            df = df.drop_duplicates(subset=['tid'])\n            df = df.dropna()\n            return df\n        elif broker == 'ok':\n            df = pd.DataFrame(data['data'])\n            df = df.loc[:, ['tid', 'datetime', 'price', 'amount', 'type']]\n            df['datetime'] = df['datetime'].apply(lambda x: datetime.fromtimestamp(x / 1000))\n            df['price'] = df['price'].astype(float)\n            df['amount'] = df['amount'].astype(float)\n            df = df.drop_duplicates(subset=['tid'])\n            df = df.dropna()\n            return df\n        elif broker == 'chbtc':\n            df = pd.DataFrame(data['data'])\n            df = df.loc[:, ['tid', 'datetime', 'price', 'amount', 'type']]\n            df['datetime'] = df['datetime'].apply(lambda x: datetime.fromtimestamp(x", "CodeBleu_score": 45.22931093643805}, {"code": "def coins_trade(broker='hb', code='btc')\n    import pandas as pd\n    import requests\n    import json\n    import time\n    import datetime\n    import pytz\n    from datetime import datetime\n    from datetime import timedelta\n    import numpy as np\n\n    # Set the base URL for the specified broker\n    if broker == 'hb':\n        base_url = f'https://api.huobi.pro/market/history/trade?symbol={code.upper()}'\n    elif broker == 'ok':\n        base_url = f'https://www.okcoin.com/api/v5/market/trades?instId={code.upper()}-USDT'\n    elif broker == 'chbtc':\n        base_url = f'https://api.chbtc.com/api/v3/trades?symbol={code.upper()}'\n    else:\n        raise ValueError(f\"Unsupported broker: {broker}\")\n\n    # Fetch the data from the URL\n    response = requests.get(base_url)\n    data = response.json()\n\n    # If the data is not None, process it\n    if data is not None:\n        # Process the data differently depending on the broker\n        if broker == 'hb':\n            # Select and rename specific columns\n            df = pd.DataFrame(data['data'])\n            df = df[['id', 'amount', 'price', 'direction', 'ts']]\n            df.columns = ['tid', 'amount', 'price', 'type', 'datetime']\n\n            # Convert timestamps to datetime objects\n            df['datetime'] = pd.to_datetime(df['datetime'], unit='ms')\n\n            # Set the timezone to UTC\n            df['datetime'] = df['datetime'].dt.tz_localize('UTC')\n\n            # Convert the datetime objects to the desired timezone\n            df['datetime'] = df['datetime'].dt.tz_convert(pytz.timezone('America/New_York'))\n\n        elif broker == 'ok':\n            # Select, rename columns, and convert timestamps\n            df = pd.DataFrame(data)\n            df = df[['data']]\n            df =", "CodeBleu_score": 39.80975682837404}, {"code": "def coins_trade(broker='hb', code='btc')\n    import pandas as pd\n    import requests\n    from datetime import datetime\n    import json\n    import numpy as np\n    from datetime import datetime\n    import time\n\n    if broker == 'hb':\n        url = f'https://api.huobi.pro/market/history/trade?symbol={code.upper()}&size=100'\n        data = requests.get(url).json()\n        data = pd.DataFrame(data['data'])\n        data = data[['id', 'price', 'amount', 'direction', 'ts']]\n        data.columns = ['tid', 'price', 'amount', 'type', 'datetime']\n        data['datetime'] = data['datetime'].apply(lambda x: datetime.fromtimestamp(x / 1000))\n        data['amount'] = data['amount'].astype(float)\n        data['price'] = data['price'].astype(float)\n        data['type'] = data['type'].map({'buy': 'buy', 'sell': 'sell'})\n        data['tid'] = data['tid'].astype(int)\n        data = data[['tid', 'datetime', 'price', 'amount', 'type']]\n    elif broker == 'ok':\n        url = f'https://www.okcoin.com/api/v5/market/trades?instId={code.upper()}-USDT'\n        data = requests.get(url).json()\n        data = pd.DataFrame(data['data'])\n        data = data[['tradeId', 'price', 'size', 'side', 'ts']]\n        data.columns = ['tid', 'price', 'amount', 'type', 'datetime']\n        data['datetime'] = data['datetime'].apply(lambda x: datetime.fromtimestamp(x / 1000))\n        data['amount'] = data['amount'].astype(float)\n        data['price'] = data['price'].astype(float)\n        data['type'] = data['type'].map({'buy': 'buy', 'sell': 'sell'})\n        data['tid'] = data['tid'].astype(int", "CodeBleu_score": 39.421524476605626}]}
{"instance_id": "25e0a134c45b", "generation_result": [{"code": "def _parse_fund_data(url, fund_type='open')\n    import requests\n    import pandas as pd\n    import json\n    import io\n    import sys\n    import re\n    from datetime import datetime\n    import time\n    import os\n    import logging\n    from bs4 import BeautifulSoup\n    from dateutil import parser\n    from dateutil.relativedelta import relativedelta\n    from datetime import datetime\n    import numpy as np\n    import pandas as pd\n    import requests\n    import json\n    import io\n    import sys\n    import re\n    from datetime import datetime\n    import time\n    import os\n    import logging\n    from bs4 import BeautifulSoup\n    from dateutil import parser\n    from dateutil.relativedelta import relativedelta\n    from datetime import datetime\n    import numpy as np\n    import pandas as pd\n    import requests\n    import json\n    import io\n    import sys\n    import re\n    from datetime import datetime\n    import time\n    import os\n    import logging\n    from bs4 import BeautifulSoup\n    from dateutil import parser\n    from dateutil.relativedelta import relativedelta\n    from datetime import datetime\n    import numpy as np\n    import pandas as pd\n    import requests\n    import json\n    import io\n    import sys\n    import re\n    from datetime import datetime\n    import time\n    import os\n    import logging\n    from bs4 import BeautifulSoup\n    from dateutil import parser\n    from dateutil.relativedelta import relativedelta\n    from datetime import datetime\n    import numpy as np\n    import pandas as pd\n    import requests\n    import json\n    import io\n    import sys\n    import re\n    from datetime import datetime\n    import time\n    import os\n    import logging\n    from bs4 import BeautifulSoup\n    from dateutil import parser\n    from dateutil.relativedelta import relativedelta\n    from datetime import datetime\n    import numpy as np\n    import pandas as pd\n    import requests\n    import json\n    import io\n    import sys\n    import re\n    from datetime import datetime\n    import time\n    import os\n    import logging\n    from bs4 import BeautifulSoup", "CodeBleu_score": 11.446436105337849}, {"code": "def _parse_fund_data(url, fund_type='open')\n    import requests\n    import pandas as pd\n    import json\n\n    try:\n        response = requests.get(url)\n        response.encoding = 'utf-8'\n        text = response.text\n        text = text.replace('\\n', '')\n        text = text.replace('\\t', '')\n        text = text.replace('\\r', '')\n        text = text.replace('\\xa0', '')\n        text = text.replace('\\u3000', '')\n        text = text.replace('\\u200b', '')\n        text = text.replace('\\u200c', '')\n        text = text.replace('\\u200d', '')\n        text = text.replace('\\u200e', '')\n        text = text.replace('\\u200f', '')\n        text = text.replace('\\u202f', '')\n        text = text.replace('\\u205f', '')\n        text = text.replace('\\u2060', '')\n        text = text.replace('\\u2061', '')\n        text = text.replace('\\u2062', '')\n        text = text.replace('\\u2063', '')\n        text = text.replace('\\u2064', '')\n        text = text.replace('\\u2065', '')\n        text = text.replace('\\u2066', '')\n        text = text.replace('\\u2067', '')\n        text = text.replace('\\u2068', '')\n        text = text.replace('\\u2069', '')\n        text = text.replace('\\u206a', '')\n        text = text.replace('\\u206b', '')\n        text = text.replace('\\u206c', '')\n        text = text.replace('\\u206d', '')\n        text = text.replace('\\u206e', '')\n        text = text.replace('\\u206f', '')\n       ", "CodeBleu_score": 24.68836535571036}, {"code": "def _parse_fund_data(url, fund_type='open')\n    import pandas as pd\n    import requests\n    import json\n    import io\n    import re\n    import sys\n    import time\n    import datetime\n    import numpy as np\n\n    # Define the headers for the request\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n        'Accept-Language': 'en-US,en;q=0.9'\n    }\n\n    # Try to request the URL\n    try:\n        response = requests.get(url, headers=headers)\n    except requests.exceptions.RequestException as e:\n        print(f\"Error: {e}\")\n        return None\n\n    # Handle text encoding\n    if response.encoding == 'ISO-8859-1':\n        response.encoding = response.apparent_encoding\n\n    # Extract relevant data\n    text = response.text\n    text = re.sub(r'<script type=\"text/javascript\">.*?</script>', '', text, flags=re.DOTALL)\n    text = re.sub(r'<script type=\"text/javascript\" src=.*?</script>', '', text, flags=re.DOTALL)\n    text = re.sub(r'<script type=\"text/javascript\">.*?</script>', '', text, flags=re.DOTALL)\n    text = re.sub(r'<script type=\"text/javascript\" src=.*?</script>', '', text, flags=re.DOTALL)\n    text = re.sub(r'<script type=\"text/javascript\">.*?</script>', '', text, flags=re.DOTALL)\n    text = re.sub(r'<script type=\"text/javascript\" src=.*?</script>', '', text, flags=re.DOTALL)\n    text = re.sub(r'<script type=\"text/javascript", "CodeBleu_score": 20.998923214409878}]}
{"instance_id": "401d18e122d8", "generation_result": [{"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import ccxt\n    \n    # Determine the appropriate function to call based on the market parameter\n    if market == 'CFFEX':\n        # Initialize an empty list to hold data frames\n        data = []\n        # Iterate over each date in the range\n        for date in pd.date_range(start=start, end=end):\n            # Convert the date to the correct format\n            date_str = date.strftime('%Y%m%d')\n            # Retrieve daily data using the tushare API\n            df = ts.pro_bar(ts_code='CFFEX.IF1903.Daily', start_date=date_str, end_date=date_str)\n            # Append the data frame to the list if data is found\n            if not df.empty:\n                data.append(df)\n        # Concatenate and return the list of data frames if any data was collected\n        return pd.concat(data, ignore_index=True)\n    elif market == 'SHFE':\n        # Initialize an empty list to hold data frames\n        data = []\n        # Iterate over each date in the range\n        for date in pd.date_range(start=start, end=end):\n            # Convert the date to the correct format\n            date_str = date.strftime('%Y-%m-%d')\n            # Retrieve daily data using the ccxt API\n            df = ccxt.shfe().fetch_ohlcv('SHFE/AG2103', timeframe='1d', since=date_str)\n            # Append the data frame to the list if data is found\n            if not df.empty:\n                data.append(df)\n        # Concatenate and return the list of data frames if any data was collected\n        return pd.concat(data, ignore_index=True)\n    else:\n        # Raise an exception if an unsupported market is specified\n        raise ValueError('Unsupported market: {}'.format(market))\n", "CodeBleu_score": 34.47709710747302}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import tushare.pro as pro\n    import os\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import tushare.pro as pro\n    import os\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import tushare.pro as pro\n    import os\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import tushare.pro as pro\n    import os\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import tushare.pro as pro\n    import os\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import tushare.pro as pro\n    import os\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import tushare.pro as pro\n    import os\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import tushare.pro as pro\n    import os\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import tushare.pro as pro\n    import os\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import tushare.pro as pro\n    import os\n", "CodeBleu_score": 12.837980098937042}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    # Import necessary libraries\n    import pandas as pd\n    import datetime\n    import tushare as ts\n    import requests\n    import json\n    import numpy as np\n    import os\n    import time\n    # Initialize an empty list to hold data frames\n    df_list = []\n    # Determine the appropriate function to call based on the market parameter\n    if market == 'CFFEX':\n        # Convert the start and end dates to the correct format\n        if start is None:\n            start = datetime.datetime.now().strftime('%Y-%m-%d')\n        if end is None:\n            end = datetime.datetime.now().strftime('%Y-%m-%d')\n        # Iterate over each date in the range\n        for date in pd.date_range(start, end):\n            # Retrieve daily data using the tushare library\n            df = ts.pro_bar(ts_code='CFFEX.IF1903', start_date=date.strftime('%Y-%m-%d'), end_date=date.strftime('%Y-%m-%d'), freq='D', asset='FUTURE')\n            # Append the data frame to the list if data is found\n            if not df.empty:\n                df_list.append(df)\n    elif market == 'SHFE':\n        # Convert the start and end dates to the correct format\n        if start is None:\n            start = datetime.datetime.now().strftime('%Y-%m-%d')\n        if end is None:\n            end = datetime.datetime.now().strftime('%Y-%m-%d')\n        # Iterate over each date in the range\n        for date in pd.date_range(start, end):\n            # Retrieve daily data using the tushare library\n            df = ts.pro_bar(ts_code='SHFE.AG1905', start_date=date.strftime('%Y-%m-%d'), end_date=date.strftime('%Y-%m-%d'), freq='D', asset='FUTURE')", "CodeBleu_score": 35.65779511262988}]}
{"instance_id": "697779825988", "generation_result": [{"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n    url = 'https://movie.daum.net/api/movieboxoffice/realtime'\n    for i in range(retry_count):\n        try:\n            response = requests.get(url)\n            if response.status_code == 200:\n                data = response.json()\n                if 'data2' in data:\n                    df = pd.DataFrame(data['data2'])\n                    df = df.drop(['MovieImg', 'mId'], axis=1)\n                    df['timestamp'] = pd.to_datetime('now')\n                    return df\n        except Exception as e:\n            print(f\"Error fetching data: {e}\")\n        time.sleep(pause)\n    return None\n", "CodeBleu_score": 41.71250847849512}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n    import requests\n    import pandas as pd\n    import time\n    from datetime import datetime\n    for i in range(retry_count):\n        try:\n            url = \"https://movie.daum.net/api/movieboxoffice/realtime\"\n            response = requests.get(url)\n            if response.status_code == 200:\n                data = response.json()\n                if 'data2' in data:\n                    df = pd.DataFrame(data['data2'])\n                    df = df.drop(['MovieImg', 'mId'], axis=1)\n                    df['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                    return df\n            else:\n                print(f\"Error: {response.status_code}\")\n        except Exception as e:\n            print(f\"Error: {e}\")\n        time.sleep(pause)\n    print(\"Error: Failed to fetch real-time box office data.\")\n    return None", "CodeBleu_score": 38.7321938905499}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n    url = 'https://boxoffice.today/api/v1/get_realtime_boxoffice'\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n    for i in range(retry_count):\n        try:\n            response = requests.get(url, headers=headers)\n            if response.status_code == 200:\n                data = response.json()\n                if 'data2' in data:\n                    df = pd.DataFrame(data['data2'])\n                    df = df.drop(['MovieImg', 'mId'], axis=1)\n                    df['timestamp'] = datetime.now()\n                    return df\n                else:\n                    print(f'No data found in response. Retrying in {pause} seconds...')\n                    time.sleep(pause)\n            else:\n                print(f'Request failed with status code {response.status_code}. Retrying in {pause} seconds...')\n                time.sleep(pause)\n        except Exception as e:\n            print(f'An error occurred: {e}. Retrying in {pause} seconds...')\n            time.sleep(pause)\n    raise Exception('Failed to fetch real-time box office data after multiple attempts.')\n", "CodeBleu_score": 34.93836613504819}]}
