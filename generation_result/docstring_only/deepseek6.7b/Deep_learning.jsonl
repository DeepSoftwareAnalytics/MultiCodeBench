{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n    # Check if a cached model is available and valid\n    if self.cache_path and os.path.exists(self.cache_path):\n        with open(self.cache_path, \"rb\") as f:\n            cached_model = pickle.load(f)\n            if cached_model[\"model\"] is not None and cached_model[\"model\"].training:\n                # If the cached model is valid and in training mode, return it\n                return cached_model\n\n    # Load the model from the specified path\n    model = torch.load(self.model_path, map_location=self.device)\n\n    # Configure the model using settings from the YAML file\n    with open(self.config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n    model.configure(**config)\n\n    # Move the model to the appropriate device\n    model = model.to(self.device)\n\n    # Enable half-precision attention if requested\n    if half_attention:\n        model.enable_half_precision_attention()\n\n    # Enable memory format optimization if requested\n    if self.memory_format == \"channels_last\":\n        model.enable_memory_format_optimization()\n\n    # Put the model in evaluation mode\n    model.eval()\n\n    # Cache the loaded model for future use\n    if self.cache_path:\n        with open(self.cache_path, \"wb\") as f:\n            pickle.dump({\"model\": model, \"config\": config}, f)\n\n    return {\"model\": model, \"config\": config}", "CodeBleu_score": 28.831982955509567}, {"code": "def load_model_from_config(self, half_attention)\n    # Check if a cached model is available and valid\n    cache_path = os.path.join(self.model_cache_dir, self.model_name + \".pt\")\n    if os.path.exists(cache_path):\n        try:\n            model = torch.load(cache_path)\n            if model[\"config\"] == self.model_config:\n                print(f\"Loading model from cache: {cache_path}\")\n                model = model[\"model\"]\n                model.eval()\n                return model\n        except Exception as e:\n            print(f\"Error loading cached model: {e}\")\n\n    # Load the model from the specified path\n    model = load_model_from_path(self.model_path, self.model_config, self.device)\n\n    # Configure the model using settings from the YAML file\n    model.config.update(self.model_settings)\n\n    # Move the model to the appropriate device\n    model.to(self.device)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Optionally cache the loaded model for future use\n    if self.cache_models:\n        torch.save({\"config\": self.model_config, \"model\": model}, cache_path)\n\n    return model\n\n", "CodeBleu_score": 28.655197496862}, {"code": "def load_model_from_config(self, half_attention)\n    if self.cache_path and os.path.exists(self.cache_path):\n        if os.path.getmtime(self.cache_path) > os.path.getmtime(self.model_path):\n            try:\n                model = torch.load(self.cache_path, map_location=self.device)\n                self.logger.info(f\"Loaded model from cache at {self.cache_path}\")\n                return model\n            except Exception as e:\n                self.logger.warning(f\"Failed to load model from cache: {e}\")\n\n    model = torch.load(self.model_path, map_location=self.device)\n    self.logger.info(f\"Loaded model from {self.model_path}\")\n\n    if half_attention:\n        model = model.half()\n        self.logger.info(\"Using half precision attention\")\n\n    if self.memory_format_optimization:\n        model = model.to(memory_format=torch.channels_last)\n        self.logger.info(\"Using channels last memory format\")\n\n    model.eval()\n    self.logger.info(\"Model is in evaluation mode\")\n\n    if self.cache_path:\n        torch.save(model, self.cache_path)\n        self.logger.info(f\"Cached model at {self.cache_path}\")\n\n    return model", "CodeBleu_score": 36.21107769898945}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    batch_size = shape[0]\n    print(f\"sampling steps: {steps}\")\n    print(f\"eta: {eta}\")\n    if callback:\n        callback(0)\n    alg = DDIMSampler(model)\n    return alg.sample(S=steps,\n                      conditioning=cond,\n                      batch_size=batch_size,\n                      shape=shape,\n                      verbose=False,\n                      eta=eta,\n                      callback=callback,\n                      normals_sequence=normals_sequence,\n                      mask=mask,\n                      x0=x0,\n                      quantize_x0=quantize_x0,\n                      temperature=temperature,\n                      score_corrector=score_corrector,\n                      corrector_kwargs=corrector_kwargs,\n                      x_T=x_t)", "CodeBleu_score": 43.13856948803554}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\n    model.to(device)\n    batch_size = shape[0]\n    print(f'sampling steps: {steps}')\n    print(f'eta: {eta}')\n    if steps % model.diffusion_steps != 0:\n        raise ValueError(f'`steps` ({steps}) must be a multiple of `diffusion_steps` ({model.diffusion_steps})')\n    if eta != 1.0 and model.n_timesteps != model.diffusion_steps:\n        raise ValueError(f'`eta` ({eta}) must be 1.0 when `n_timesteps` ({model.n_timesteps}) is equal to `diffusion_steps` ({model.diffusion_steps})')\n\n    if isinstance(shape, torch.Size):\n        shape = tuple(shape)\n\n    if not isinstance(shape, tuple):\n        raise ValueError(f'`shape` must be a tuple, but got {type(shape)}')\n\n    if not all(isinstance(dim, int) for dim in shape):\n        raise ValueError(f'`shape` must be a tuple of ints, but got {shape}')\n\n    if not all(dim > 0 for dim in shape):\n        raise ValueError(f'all dimensions in `shape` must be positive, but found {shape}')\n\n    if not all(dim % 2 == 0 for dim in shape):\n        raise ValueError(f'all dimensions in `shape` must be even, but found {shape}')\n\n    if len(shape) == 2:\n        shape = shape + (3,)\n\n    if len(shape) != 3:\n        raise ValueError(f'`shape` must have 2 or 3 dimensions, but got {len(shape)}')\n\n    if not all(dim % 4 == 0 for dim in shape):\n        raise ValueError(f'all dimensions in `shape` must be divisible by 4, but found {shape}')\n\n    if cond is not None and cond.shape != (batch_size, model.cond_dim):\n        raise ValueError(f'`cond` must have shape (batch_", "CodeBleu_score": 23.72246605281968}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    batch_size = shape[0]\n    print(f\"eta: {eta}, steps: {steps}\")\n    if callback is not None and not isinstance(callback, Callback):\n        raise TypeError(f\"`callback` should be a subclass of `Callback`, not {type(callback)}\")\n\n    # Initialize the sampler\n    sampler = DDIMSampler(model)\n\n    # Sample from the model\n    samples, intermediates = sampler.sample(steps, batch_size, shape, cond, eta,\n                                            callback=callback, normals_sequence=normals_sequence,\n                                            mask=mask, x0=x0, quantize_x0=quantize_x0,\n                                            temperature=temperature, score_corrector=score_corrector,\n                                            corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n    return samples, intermediates\n", "CodeBleu_score": 56.125040885714085}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if context is not None:\n        print(f\"{context}: Using EMA weights\")\n    self.ema_weights = self.model.get_weights()\n    self.model.set_weights(self.ema_weights)\n    if context is not None:\n        print(f\"{context}: Switched to EMA weights\")\n    yield\n    if context is not None:\n        print(f\"{context}: Restoring original weights\")\n    self.model.set_weights(self.original_weights)\n    if context is not None:\n        print(f\"{context}: Restored original weights\")\n", "CodeBleu_score": 61.59989918397205}, {"code": "def ema_scope(self, context=None)\n    self.ema_enabled = True\n    self.ema_weights = copy.deepcopy(self.model.get_weights())\n    if context is not None:\n        print(\"EMA weights enabled\")\n    self.model.set_weights(self.ema_weights)\n    if context is not None:\n        print(\"EMA weights set\")\n    yield\n    if context is not None:\n        print(\"Restoring original weights\")\n    self.model.set_weights(self.original_weights)\n    if context is not None:\n        print(\"Original weights restored\")\n    self.ema_enabled = False\n\n", "CodeBleu_score": 41.72799079871062}, {"code": "def ema_scope(self, context=None)\n    if self.ema_enabled:\n        if context is not None:\n            print(f\"[{context}] Using EMA weights\")\n        self.model.set_weights(self.ema_model.get_weights())\n        if context is not None:\n            print(f\"[{context}] Restoring original weights\")\n        self.ema_model.set_weights(self.model.get_weights())\n    else:\n        if context is not None:\n            print(f\"[{context}] Using original weights\")\n\n", "CodeBleu_score": 52.7412857202303}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    state_dict = torch.load(path)\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            state_dict.pop(key)\n    self.load_state_dict(state_dict)\n    missing_keys = []\n    unexpected_keys = []\n    for key, value in self.state_dict().items():\n        if key not in state_dict:\n            missing_keys.append(key)\n    for key in state_dict.keys():\n        if key not in self.state_dict():\n            unexpected_keys.append(key)\n    if len(missing_keys) > 0:\n        print(\"Missing keys:\")\n        print(missing_keys)\n    if len(unexpected_keys) > 0:\n        print(\"Unexpected keys:\")\n        print(unexpected_keys)", "CodeBleu_score": 28.117919417697113}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    state_dict = torch.load(path)\n    if ignore_keys is not None:\n        state_dict = {k: v for k, v in state_dict.items() if k not in ignore_keys}\n    self.load_state_dict(state_dict)\n    missing_keys = []\n    unexpected_keys = []\n    for k, v in self.state_dict().items():\n        if k not in state_dict:\n            missing_keys.append(k)\n        elif v.shape != state_dict[k].shape:\n            unexpected_keys.append(k)\n    if len(missing_keys) > 0:\n        print(f\"Missing keys: {missing_keys}\")\n    if len(unexpected_keys) > 0:\n        print(f\"Unexpected keys: {unexpected_keys}\")", "CodeBleu_score": 39.089620236144896}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    if ignore_keys is None:\n        ignore_keys = []\n    ckpt = torch.load(path, map_location=\"cpu\")\n    if \"state_dict\" in ckpt:\n        ckpt = ckpt[\"state_dict\"]\n    ckpt = {k: v for k, v in ckpt.items() if not any(ignore_key in k for ignore_key in ignore_keys)}\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, \"_metadata\", None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n    # use strict loading\n    self.load_state_dict(state_dict, strict=False)\n    # convert relative_position_index from relative to absolute indexing\n    if hasattr(self, \"relative_position_index_keys\"):\n        for rel_pos_key in self.relative_position_index_keys:\n            key = rel_pos_key\n            new_key = rel_pos_key.replace(\"relative_position_index\", \"relative_position_index_transpose\")\n            state_dict[new_key] = torch.transpose(state_dict[key], 0, 1)\n            del state_dict[key]\n    # get missing keys\n    current_keys = []\n    for key in state_dict.keys():\n        if not hasattr(self, key):\n            missing_keys.append(key)\n        else:\n            current_keys.append(key)\n    # get unexpected keys\n    for key in current_keys:\n        if not hasattr(self, key):\n            unexpected_keys.append(key)\n    # print info\n    if len(missing_keys) > 0:\n        _logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(self.__class__.__name__, missing_keys))\n    if len(unexpected_keys) > 0:\n        _logger.info(\"Weights of {}", "CodeBleu_score": 31.650838748381272}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n    self.encoder.eval()\n    self.decoder.eval()\n\n    input_embed = self.embedding(input)\n    input_embed = input_embed.permute(1, 0, 2)\n    input_embed = input_embed.to(self.device)\n\n    with torch.no_grad():\n        encoder_outputs, hidden = self.encoder(input_embed)\n\n    decoder_input = torch.tensor([[self.SOS_token]], device=self.device)\n    decoder_input = self.embedding(decoder_input)\n\n    decoded_words = []\n    pred_indices = []\n    for _ in range(self.max_length):\n        decoder_output, hidden = self.decoder(decoder_input, hidden, encoder_outputs)\n        decoder_output = decoder_output.squeeze(1)\n        _, topi = decoder_output.data.topk(1)\n        if topi.item() == self.EOS_token:\n            decoded_words.append('<EOS>')\n            break\n        else:\n            decoded_words.append(self.output_lang.index2word[topi.item()])\n            pred_indices.append(topi.item())\n\n        decoder_input = self.embedding(topi.squeeze().view(1, -1))\n\n    if return_pred_indices:\n        return ' '.join(decoded_words), pred_indices\n    else:\n        return ' '.join(decoded_words)", "CodeBleu_score": 28.644882295403264}, {"code": "def forward(self, input, return_pred_indices=False)\n    # Encoding\n    if self.encoder is not None:\n        self.encoder.eval()\n        with torch.no_grad():\n            encoding = self.encoder(input)\n    else:\n        encoding = input\n\n    # Decoding\n    if self.decoder is not None:\n        self.decoder.eval()\n        with torch.no_grad():\n            decoded = self.decoder(encoding)\n    else:\n        decoded = encoding\n\n    # Difference metric\n    if self.diff_metric is not None:\n        diff = self.diff_metric(decoded, input)\n    else:\n        diff = None\n\n    # Prediction indices\n    if return_pred_indices:\n        if self.decoder is not None:\n            pred_indices = self.decoder.pred_indices\n        else:\n            pred_indices = None\n    else:\n        pred_indices = None\n\n    return decoded, diff, pred_indices\n\n", "CodeBleu_score": 29.92040403108367}, {"code": "def forward(self, input, return_pred_indices=False)\n    # Encoding\n    encoding_output = self.encoder(input)\n\n    # Decoding\n    decoding_output = self.decoder(encoding_output)\n\n    # Difference metric\n    difference_metric = self.difference_metric(input, decoding_output)\n\n    # Prediction indices\n    if return_pred_indices:\n        prediction_indices = self.prediction_indices(input, decoding_output)\n        return decoding_output, difference_metric, prediction_indices\n    else:\n        return decoding_output, difference_metric", "CodeBleu_score": 19.227357823773882}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n    if self.batch_resizing:\n        batch_size = batch.shape[0]\n        input_size = batch.shape[2]\n        if self.resize_range is not None:\n            resize_factor = self.resize_range[0] + (self.resize_range[1] - self.resize_range[0]) * (\n                self.current_step / self.total_steps\n            )\n            resize_size = int(input_size * resize_factor)\n            batch = F.interpolate(batch, size=(resize_size, resize_size), mode=\"bicubic\", align_corners=True)\n    batch = batch[..., k]\n    if len(batch.shape) < 4:\n        batch = batch.unsqueeze(-1)\n    if len(batch.shape) < 5:\n        batch = batch.unsqueeze(-1)\n    if batch.shape[1] != 3:\n        batch = batch.permute(0, 3, 1, 2)\n    if batch.shape[1] == 4:\n        batch = batch[:, :3]\n    batch = batch.contiguous().float()\n    batch = batch.detach()\n    return batch", "CodeBleu_score": 41.43631757912637}, {"code": "def get_input(self, batch, k)\n    if batch.get('resize') is not None:\n        batch_size = batch['data'].size(0)\n        batch_size_f = float(batch_size)\n        batch['data'] = batch['data'].view(batch_size, -1, batch['data'].size(2), batch['data'].size(3))\n        batch['data'] = batch['data'].permute(0, 2, 3, 1)\n        batch['data'] = batch['data'].contiguous()\n        batch['data'] = batch['data'].view(batch_size, -1, batch['data'].size(2), batch['data'].size(3))\n        batch['data'] = batch['data'].permute(0, 2, 3, 1)\n        batch['data'] = batch['data'].contiguous()\n        batch['data'] = batch['data'].view(batch_size, -1, batch['data'].size(2), batch['data'].size(3))\n        batch['data'] = batch['data'].permute(0, 2, 3, 1)\n        batch['data'] = batch['data'].contiguous()\n        batch['data'] = batch['data'].view(batch_size, -1, batch['data'].size(2), batch['data'].size(3))\n        batch['data'] = batch['data'].permute(0, 2, 3, 1)\n        batch['data'] = batch['data'].contiguous()\n        batch['data'] = batch['data'].view(batch_size, -1, batch['data'].size(2), batch['data'].size(3))\n        batch['data'] = batch['data'].permute(0, 2, 3, 1)\n        batch['data'] = batch['data'].contiguous()\n        batch['data'] = batch['data'].view(batch_size, -1, batch['data'].size(2), batch['data'].size(3))\n        batch['data'] = batch['data'].permute(0, 2, 3, 1)\n        batch['data'] = batch['data'].contiguous()\n        batch['data'] = batch['", "CodeBleu_score": 17.080765616697764}, {"code": "def get_input(self, batch, k)\n    if batch.ndim == 4:\n        batch = batch[:, k, :, :]\n    elif batch.ndim == 3:\n        batch = batch[:, :, :]\n    elif batch.ndim == 2:\n        batch = batch[:, :]\n    else:\n        raise ValueError(f\"Invalid number of dimensions: {batch.ndim}\")\n    if self.batch_resize:\n        batch = batch.permute(0, 3, 1, 2)\n        batch = self.batch_resize_fn(batch, self.batch_resize_range[0], self.batch_resize_range[1])\n        batch = batch.permute(0, 2, 3, 1)\n    batch = batch.contiguous().float()\n    batch = batch.detach()\n    return batch\n", "CodeBleu_score": 31.63069503733279}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    # Retrieve input data and predictions\n    x, y = batch\n    y_hat = self(x)\n\n    # Compute autoencoding loss\n    if optimizer_idx == 0:\n        loss = self.autoencoder_loss(y_hat, y)\n        self.log(\"autoencoder_loss\", loss)\n        return loss\n\n    # Compute discriminator loss\n    if optimizer_idx == 1:\n        loss = self.discriminator_loss(y_hat, y)\n        self.log(\"discriminator_loss\", loss)\n        return loss", "CodeBleu_score": 17.035789034602857}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    y_hat = self.forward(x)\n    if optimizer_idx == 0:\n        loss = self.autoencoder_loss(y_hat, y)\n        self.log(\"autoencoder_loss\", loss)\n        return loss\n    elif optimizer_idx == 1:\n        loss = self.discriminator_loss(y_hat, y)\n        self.log(\"discriminator_loss\", loss)\n        return loss\n    else:\n        raise ValueError(\"Invalid optimizer index\")\n", "CodeBleu_score": 17.84479143260391}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    y_hat = self.forward(x)\n    if optimizer_idx == 0:\n        loss = self.autoencoder_loss(y_hat, y)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n        return loss\n    elif optimizer_idx == 1:\n        loss = self.discriminator_loss(y_hat, y)\n        self.log(\"discriminator_loss\", loss, on_step=True, on_epoch=True)\n        return loss\n", "CodeBleu_score": 12.95866552051606}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    inputs = batch['inputs']\n    labels = batch['labels']\n    if self.ema_model is not None and plot_ema:\n        ema_inputs = self.ema_model.ema_inputs(inputs)\n    if self.log_inputs:\n        self.logger.log_images(inputs, 'inputs', **kwargs)\n        if self.ema_model is not None and plot_ema:\n            self.logger.log_images(ema_inputs, 'ema_inputs', **kwargs)\n    if self.log_reconstructions:\n        reconstructions = self.model(inputs)\n        if self.ema_model is not None and plot_ema:\n            ema_reconstructions = self.ema_model(ema_inputs)\n        if self.log_inputs:\n            self.logger.log_images(reconstructions, 'reconstructions', **kwargs)\n            if self.ema_model is not None and plot_ema:\n                self.logger.log_images(ema_reconstructions, 'ema_reconstructions', **kwargs)\n        else:\n            self.logger.log_images(reconstructions, 'reconstructions', **kwargs)\n            if self.ema_model is not None and plot_ema:\n                self.logger.log_images(ema_reconstructions, 'ema_reconstructions', **kwargs)\n    if self.log_labels:\n        self.logger.log_images(labels, 'labels', **kwargs)\n    if self.log_inputs and self.log_reconstructions and self.log_labels:\n        self.logger.log_images(inputs, 'inputs_reconstructions_labels', **kwargs)\n    if self.log_inputs and self.log_reconstructions and self.log_labels and self.ema_model is not None and plot_ema:\n        self.logger.log_images(ema_inputs, 'ema_inputs_reconstructions_labels', **kwargs)\n", "CodeBleu_score": 23.525745586640056}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if self.log_images_called:\n        return\n    self.log_images_called = True\n\n    # Get inputs and reconstructions\n    inputs = batch[0]\n    if only_inputs:\n        reconstructions = inputs\n    else:\n        reconstructions = self.forward(inputs)\n\n    # Apply colorization for multi-channel images\n    if inputs.shape[1] > 1:\n        inputs = self.colorizer(inputs)\n        reconstructions = self.colorizer(reconstructions)\n\n    # Log inputs and reconstructions\n    self.logger.experiment.add_images(\"inputs\", inputs, self.global_step, dataformats=\"NCHW\")\n    self.logger.experiment.add_images(\"reconstructions\", reconstructions, self.global_step, dataformats=\"NCHW\")\n\n    # Log reconstructions using EMA weights if enabled\n    if plot_ema:\n        if self.ema_weights is not None:\n            self.ema_weights.load_state_dict(self.state_dict())\n            reconstructions = self.forward(inputs)\n            self.logger.experiment.add_images(\"ema_reconstructions\", reconstructions, self.global_step, dataformats=\"NCHW\")\n\n    # Log images using the logger's log_images function\n    self.logger.log_images(inputs, reconstructions, **kwargs)", "CodeBleu_score": 30.27761789089099}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    # Get inputs and reconstructions\n    inputs, reconstructions = batch\n    if only_inputs:\n        images = inputs\n    else:\n        images = torch.cat([inputs, reconstructions], dim=0)\n    # Colorize images if needed\n    if self.num_channels > 1:\n        images = self.colorize(images)\n    # Log images\n    self.logger.experiment.add_images(\n        \"images\", images, self.current_epoch, dataformats=\"NCHW\"\n    )\n    # Log reconstructions using EMA weights if needed\n    if plot_ema:\n        ema_weights = self.ema_model.module.state_dict()\n        self.ema_model.load_state_dict(ema_weights)\n        reconstructions = self.ema_model.reconstruct(inputs)\n        reconstructions = torch.cat([inputs, reconstructions], dim=0)\n        if self.num_channels > 1:\n            reconstructions = self.colorize(reconstructions)\n        self.logger.experiment.add_images(\n            \"ema_images\", reconstructions, self.current_epoch, dataformats=\"NCHW\"\n        )\n", "CodeBleu_score": 31.4369517822444}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError(\"Unknown loss type: {}\".format(self.loss_type))", "CodeBleu_score": 33.99461313903018}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        return torch.mean(torch.abs(pred - target))\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean(torch.pow(pred - target, 2))\n        else:\n            return torch.pow(pred - target, 2)\n    else:\n        raise ValueError(f\"Unknown loss type: {self.loss_type}\")\n\n", "CodeBleu_score": 32.7934089722426}, {"code": "def get_loss(self, pred, target, mean=True)\n    loss_type = self.loss_type\n    if loss_type == 'l1':\n        loss = F.l1_loss(pred, target, reduction='none')\n        if mean:\n            return torch.mean(loss)\n        else:\n            return loss\n    elif loss_type == 'l2':\n        loss = F.mse_loss(pred, target, reduction='none')\n        if mean:\n            return torch.mean(loss)\n        else:\n            return loss\n    else:\n        raise ValueError(f\"Unknown loss type: {loss_type}\")", "CodeBleu_score": 38.30557344184992}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n    # Select time steps\n    if self.time_embedding_type == \"positional\":\n        # Positional encoding\n        t = torch.arange(0, x.shape[1], device=x.device)\n        t = t.type_as(x)\n        t = t.view(-1, 1)\n        t = self.time_embedding(t)\n        x = x + t\n    elif self.time_embedding_type == \"learned\":\n        # Learned time embedding\n        t = torch.randint(0, self.time_embedding.shape[0], (x.shape[0],), device=x.device)\n        t = t.type_as(x)\n        t = self.time_embedding[t]\n        x = x + t\n    elif self.time_embedding_type == \"none\":\n        # No time embedding\n        pass\n    else:\n        raise ValueError(f\"Invalid time embedding type: {self.time_embedding_type}\")\n\n    # Apply conditioning if applicable\n    if c is not None:\n        if self.conditioning_type == \"learned\":\n            # Learned conditioning\n            x = x + c\n        elif self.conditioning_type == \"embedding\":\n            # Embedding conditioning\n            x = x + self.conditioning_embedding(c)\n        elif self.conditioning_type == \"none\":\n            # No conditioning\n            pass\n        else:\n            raise ValueError(f\"Invalid conditioning type: {self.conditioning_type}\")\n\n    # Process inputs according to conditioning requirements\n    if self.conditioning_type == \"learned\":\n        # Learned conditioning\n        x = self.model(x)\n    elif self.conditioning_type == \"embedding\":\n        # Embedding conditioning\n        x = self.model(x, c)\n    elif self.conditioning_type == \"none\":\n        # No conditioning\n        x = self.model(x)\n    else:\n        raise ValueError(f\"Invalid conditioning type: {self.conditioning_type}\")\n\n    # Compute loss\n    loss = self.loss(x)\n\n    return", "CodeBleu_score": 38.285593276416044}, {"code": "def forward(self, x, c, *args, **kwargs)\n    # Randomly select time steps\n    t_rand = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n    # Compute the time step embeddings\n    t_emb = self.time_embed(t_rand)\n    # Apply learned conditioning if applicable\n    if c is not None:\n        c_emb = self.condition_embed(c)\n        t_emb = t_emb + c_emb\n    # Process the inputs according to the model's conditioning requirements\n    x = self.input_blocks[0](x, t_emb)\n    for i in range(1, self.num_blocks):\n        x = self.input_blocks[i](x, t_emb)\n    # Compute the loss\n    loss = self.loss(x)\n    return loss", "CodeBleu_score": 32.97648846282192}, {"code": "def forward(self, x, c, *args, **kwargs)\n    # Select time steps\n    if self.time_embedding_type == \"positional\":\n        time_steps = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n    elif self.time_embedding_type == \"learned\":\n        time_steps = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device)\n    elif self.time_embedding_type == \"none\":\n        time_steps = torch.zeros((x.shape[0],), device=x.device, dtype=torch.long)\n    else:\n        raise ValueError(f\"Unknown time embedding type: {self.time_embedding_type}\")\n\n    # Apply learned conditioning if applicable\n    if c is not None:\n        c = self.conditioning_layer(c)\n\n    # Process inputs according to conditioning requirements\n    if self.conditioning_type == \"learned\":\n        x = self.conditioning_layer(x)\n    elif self.conditioning_type == \"concatenate\":\n        x = torch.cat([x, c], dim=1)\n    elif self.conditioning_type == \"none\":\n        pass\n    else:\n        raise ValueError(f\"Unknown conditioning type: {self.conditioning_type}\")\n\n    # Compute loss\n    loss = self.compute_loss(x, c, time_steps)\n\n    return loss\n", "CodeBleu_score": 41.2619784191195}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n    if self.codebook_dim is None:\n        raise ValueError(\"Codebook dimension is not set\")\n    if self.codebook_dim != shape[0]:\n        raise ValueError(f\"Codebook dimension {self.codebook_dim} does not match the input shape {shape}\")\n    if self.codebook_dim != indices.shape[1]:\n        raise ValueError(f\"Codebook dimension {self.codebook_dim} does not match the indices shape {indices.shape}\")\n    if self.remap_indices:\n        indices = self.remap_indices_fn(indices)\n    if self.permute_indices:\n        indices = self.permute_indices_fn(indices)\n    if self.permute_codebook:\n        indices = self.permute_codebook_fn(indices)\n    if self.remap_codebook:\n        indices = self.remap_codebook_fn(indices)\n    return self.codebook(indices).reshape(indices.shape[0], *shape)", "CodeBleu_score": 29.344783667739836}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.remap_indices:\n        indices = self.remap_indices(indices)\n    if self.permute_indices:\n        indices = self.permute_indices(indices)\n    latent_vector = self.codebook(indices)\n    latent_vector = latent_vector.view(shape)\n    return latent_vector", "CodeBleu_score": 31.427814498648925}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.remap_indices:\n        indices = self.remap_indices(indices)\n    if self.codebook_dim is not None:\n        indices = indices.view(-1, self.codebook_dim)\n    if self.use_residual:\n        indices = indices + self.embedding.weight.t()\n    embed_shape = (indices.shape[0], self.num_embeddings) + (1,) * (len(shape) - 1)\n    embed_shape = embed_shape[1:] + embed_shape[:1]\n    embed = self.embedding(indices).view(*embed_shape)\n    if self.permute_codebook:\n        embed = embed.permute(1, 0, 2, 3)\n    return embed.view(indices.shape + embed.shape[2:])\n", "CodeBleu_score": 46.00872677375861}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor > 0:\n        raise ValueError(f\"The specified factor {factor} does not divide the dimension {dimension}.\")\n\n    m, n = 1, dimension\n    while m < n:\n        m += 1\n        if dimension % m == 0:\n            n = dimension // m\n            if m <= n:\n                return m, n\n    raise ValueError(f\"No factorization found for dimension {dimension}.\")\n\n", "CodeBleu_score": 37.60998048140532}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    if factor < 0:\n        m = n = 1\n        while m * n < dimension:\n            if m * n == dimension:\n                return m, n\n            m += 1\n            n = dimension // m\n        return m, n\n    raise ValueError(\"Invalid factor\")\n\n", "CodeBleu_score": 34.72921520156459}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor < 0:\n        for i in range(2, int(dimension**0.5)+1):\n            if dimension % i == 0:\n                m = i\n                n = dimension // i\n                if m <= n and m <= abs(factor):\n                    return m, n\n        return 1, dimension\n    else:\n        return 1, dimension\n\n\n", "CodeBleu_score": 38.217163093903025}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n    # Get the device and data type of the original weight\n    device = orig_weight.device\n    dtype = orig_weight.dtype\n\n    # Move the input weight and original weight to the same device and data type\n    weight = updown.to(device, dtype)\n    orig_weight = orig_weight.to(device, dtype)\n\n    # Calculate the combined weight by adding the input weight and the original weight\n    combined_weight = weight + orig_weight\n\n    # Normalize the combined weight\n    combined_weight = combined_weight / torch.norm(combined_weight)\n\n    # Scale the combined weight using the predefined scale factor\n    combined_weight = combined_weight * self.scale_factor\n\n    # Calculate the final adjusted weight by subtracting the original weight from the scaled and normalized combined weight\n    adjusted_weight = combined_weight - orig_weight\n\n    # Return the final adjusted weight\n    return adjusted_weight", "CodeBleu_score": 12.28827782245875}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n    if self.weight_decompose_type == \"weight_decompose_scale_factor\":\n        self.weight_decompose_scale_factor = updown.weight_decompose_scale_factor\n    elif self.weight_decompose_type == \"weight_decompose_scale_factor_with_adjust\":\n        self.weight_decompose_scale_factor = updown.weight_decompose_scale_factor\n        self.weight_decompose_adjust_scale_factor = updown.weight_decompose_adjust_scale_factor\n    elif self.weight_decompose_type == \"weight_decompose_scale_factor_with_adjust_and_clip\":\n        self.weight_decompose_scale_factor = updown.weight_decompose_scale_factor\n        self.weight_decompose_adjust_scale_factor = updown.weight_decompose_adjust_scale_factor\n        self.weight_decompose_clip_min = updown.weight_decompose_clip_min\n        self.weight_decompose_clip_max = updown.weight_decompose_clip_max\n    else:\n        raise ValueError(f\"Unknown weight_decompose_type: {self.weight_decompose_type}\")\n    if self.weight_decompose_type == \"weight_decompose_scale_factor\":\n        return orig_weight\n    elif self.weight_decompose_type == \"weight_decompose_scale_factor_with_adjust\":\n        return orig_weight - self.weight_decompose_adjust_scale_factor * self.weight_decompose_scale_factor * (\n            updown.weight - orig_weight\n        )\n    elif self.weight_decompose_type == \"weight_decompose_scale_factor_with_adjust_and_clip\":\n        return torch.clamp(\n            orig_weight - self.weight_decompose_adjust_scale_factor * self.weight_decompose_scale_factor * (updown.weight - orig_weight),\n            min=self.weight", "CodeBleu_score": 16.039950968129084}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n    if updown == \"up\":\n        if self.updown == \"down\":\n            weight = self.weight_up\n            weight_orig = self.weight_orig_up\n            weight_scale = self.weight_scale_up\n        else:\n            weight = self.weight_up\n            weight_orig = self.weight_orig_up\n            weight_scale = self.weight_scale_up\n    elif updown == \"down\":\n        if self.updown == \"up\":\n            weight = self.weight_down\n            weight_orig = self.weight_orig_down\n            weight_scale = self.weight_scale_down\n        else:\n            weight = self.weight_down\n            weight_orig = self.weight_orig_down\n            weight_scale = self.weight_scale_down\n    else:\n        raise ValueError(\"Invalid updown value. Must be 'up' or 'down'.\")\n    # Check if weight and weight_orig have the same device and dtype\n    if weight.device != weight_orig.device or weight.dtype != weight_orig.dtype:\n        weight = weight.to(weight_orig.device).type(weight_orig.dtype)\n        weight_orig = weight_orig.to(weight.device).type(weight.dtype)\n        weight_scale = weight_scale.to(weight.device).type(weight.dtype)\n    # Check if weight and weight_orig have the same shape\n    if weight.shape != weight_orig.shape:\n        raise ValueError(\"Weight and weight_orig must have the same shape.\")\n    # Check if weight_scale is a scalar\n    if not torch.is_scalar(weight_scale):\n        raise ValueError(\"weight_scale must be a scalar.\")\n    # Check if weight and weight_scale have the same device and dtype\n    if weight.device != weight_scale.device or weight.dtype != weight_scale.dtype:\n        weight = weight.to(weight_scale.device).type(weight_scale.dtype)\n        weight_scale = weight_scale.to(weight.device).type(weight.dtype)\n    # Normalize the combined weight\n   ", "CodeBleu_score": 24.4548313736304}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if updown.shape[0] == orig_weight.shape[0]:\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier\n    if self.scalar is not None:\n        updown = updown * self.scalar\n    if self.weight_decomposition is not None:\n        updown = updown.reshape(self.weight_decomposition)\n        updown = updown.sum(axis=-1)\n    if ex_bias is not None:\n        updown = updown + ex_bias\n    return updown, ex_bias\n", "CodeBleu_score": 42.94696559298398}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    bias = ex_bias\n    if bias is None:\n        bias = 0\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    elif orig_weight.numel() > updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(orig_weight.shape)\n    if self.scalar is not None:\n        updown = updown * self.scalar\n    if self.weight_decomposition is not None:\n        updown = self.weight_decomposition(updown)\n    bias = bias * self.scalar\n    return updown, bias", "CodeBleu_score": 47.108142599890726}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if ex_bias is None:\n        ex_bias = tf.zeros(orig_weight.shape[0], dtype=tf.float32)\n    updown = tf.reshape(updown, [-1])\n    updown = tf.add(updown, ex_bias)\n    updown = tf.reshape(updown, output_shape)\n    if orig_weight.shape[0] == updown.shape[0]:\n        updown = tf.reshape(updown, orig_weight.shape)\n    if self.scale_factor is not None:\n        updown = tf.reshape(updown, [-1, self.scale_factor])\n        updown = tf.reduce_sum(updown, axis=1)\n    return updown, ex_bias\n", "CodeBleu_score": 40.41503794511187}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n    # Transfer the current weight tensor and optional bias to the device of the original weight tensor\n    weight = orig_weight.to(self.device)\n    if self.bias is not None:\n        bias = self.bias.to(self.device)\n    # Call the finalize_weights function to finalize the updated weights\n    weight = self.finalize_weights(weight, bias)\n    # Return the updated weight tensor\n    return weight\n", "CodeBleu_score": 38.47969195804426}, {"code": "def calc_updown(self, orig_weight)\n    if self.device == \"cpu\":\n        weight = orig_weight.to(self.device)\n    else:\n        weight = orig_weight.to(self.device)\n        weight = weight.to(self.device)\n    weight = self.calc_updown_final(weight)\n    return weight\n", "CodeBleu_score": 31.299027775557022}, {"code": "def calc_updown(self, orig_weight)\n    orig_weight = orig_weight.to(self.device)\n    self.weight = self.weight.to(self.device)\n    if self.bias is not None:\n        self.bias = self.bias.to(self.device)\n    self.weight = self.finalize_updown(orig_weight)\n    return self.weight\n", "CodeBleu_score": 38.793798931406656}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n    import gc\n    import sys\n    import os\n    import psutil\n    import subprocess\n    import platform\n    import re\n\n    def get_memory_usage():\n        \"\"\"Get the memory usage of the current process in bytes.\"\"\"\n        if platform.system() == 'Windows':\n            process = psutil.Process(os.getpid())\n            return process.memory_info().rss\n        else:\n            output = subprocess.check_output(['ps', '-o', 'rss', '-p', str(os.getpid())])\n            return int(output.split()[1]) * 1024\n\n    def get_memory_usage_percent():\n        \"\"\"Get the memory usage of the current process in percentage.\"\"\"\n        return get_memory_usage() / psutil.virtual_memory().total * 100\n\n    def get_memory_usage_string():\n        \"\"\"Get the memory usage of the current process in a human-readable format.\"\"\"\n        return str(round(get_memory_usage() / 1024 / 1024, 2)) + ' MB'\n\n    def get_memory_usage_percent_string():\n        \"\"\"Get the memory usage of the current process in a human-readable format.\"\"\"\n        return str(round(get_memory_usage_percent(), 2)) + ' %'\n\n    def get_memory_usage_percent_string_with_limit():\n        \"\"\"Get the memory usage of the current process in a human-readable format.\"\"\"\n        return str(round(get_memory_usage_percent(), 2)) + ' %' + ' (limit: ' + str(round(limit, 2)) + ' %)'\n\n    def get_memory_usage_string_with_limit():\n        \"\"\"Get the memory usage of the current process in a human-readable format.\"\"\"\n        return str(round(get_memory_usage() / 1024 / 1024, 2)) + ' MB' + ' (limit: ' + str(round(limit / 1024 / 1024, 2)) + ' MB)'\n\n    def get_memory_usage_string_with_limit_and_process_count", "CodeBleu_score": 15.827471423719846}, {"code": "def purge_networks_from_memory()\n    import gc\n    import torch\n    import torchvision\n    import torchvision.models\n    import torchvision.models.resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n    import torchvision.models.resnet as resnet\n", "CodeBleu_score": 11.30904234035094}, {"code": "def purge_networks_from_memory()\n    from . import get_networks\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from . import get_network_count\n    from . import get_network_limit\n    from", "CodeBleu_score": 8.711447931644074}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if hasattr(self, 'backup_weight') and self.backup_weight is not None:\n        self.weight = self.backup_weight\n    if hasattr(self, 'backup_bias') and self.backup_bias is not None:\n        self.bias = self.backup_bias\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, 'backup_proj_weight') and self.backup_proj_weight is not None:\n            self.in_proj_weight = self.backup_proj_weight\n        if hasattr(self, 'backup_proj_bias') and self.backup_proj_bias is not None:\n            self.in_proj_bias = self.backup_proj_bias\n    if hasattr(self, 'backup_weight') and self.backup_weight is None:\n        self.weight = None\n    if hasattr(self, 'backup_bias') and self.backup_bias is None:\n        self.bias = None\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, 'backup_proj_weight') and self.backup_proj_weight is None:\n            self.in_proj_weight = None\n        if hasattr(self, 'backup_proj_bias') and self.backup_proj_bias is None:\n            self.in_proj_bias = None", "CodeBleu_score": 39.51534665925494}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if hasattr(self, \"backup_weights\"):\n        self.weight.data = self.backup_weights.data\n        if hasattr(self, \"backup_bias\"):\n            self.bias.data = self.backup_bias.data\n        else:\n            self.bias = None\n    elif hasattr(self, \"backup_projection_weights\"):\n        self.projection.weight.data = self.backup_projection_weights.data\n        if hasattr(self, \"backup_projection_bias\"):\n            self.projection.bias.data = self.backup_projection_bias.data\n        else:\n            self.projection.bias = None\n    elif hasattr(self, \"backup_out_projection_weights\"):\n        self.out_projection.weight.data = self.backup_out_projection_weights.data\n        if hasattr(self, \"backup_out_projection_bias\"):\n            self.out_projection.bias.data = self.backup_out_projection_bias.data\n        else:\n            self.out_projection.bias = None\n    elif hasattr(self, \"backup_q_projection_weights\"):\n        self.q_proj_weight.data = self.backup_q_projection_weights.data\n        if hasattr(self, \"backup_q_projection_bias\"):\n            self.q_proj_bias.data = self.backup_q_projection_bias.data\n        else:\n            self.q_proj_bias = None\n    elif hasattr(self, \"backup_k_projection_weights\"):\n        self.k_proj_weight.data = self.backup_k_projection_weights.data\n        if hasattr(self, \"backup_k_projection_bias\"):\n            self.k_proj_bias.data = self.backup_k_projection_bias.data\n        else:\n            self.k_proj_bias = None\n    elif hasattr(self, \"backup_v_projection_we", "CodeBleu_score": 33.148305567843764}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if hasattr(self, 'backup_weight'):\n        self.weight.data = self.backup_weight.data\n    if hasattr(self, 'backup_bias'):\n        self.bias.data = self.backup_bias.data\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, 'backup_projection_weight'):\n            self.projection_weight.data = self.backup_projection_weight.data\n        if hasattr(self, 'backup_projection_bias'):\n            self.projection_bias.data = self.backup_projection_bias.data\n    if not hasattr(self, 'backup_bias'):\n        self.bias = None", "CodeBleu_score": 32.32482684027064}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n    # Check if any additional networks are loaded\n    if len(org_module.additional_networks) > 0:\n        # Cast input to appropriate device\n        input = input.to(org_module.device)\n        # Restore original weights\n        org_module.restore_weights()\n        # Call original forward method\n        output = original_forward(input)\n        # Apply each loaded network sequentially\n        for network in org_module.additional_networks:\n            if network.layer == org_module.current_layer:\n                # Apply network\n                output = network.apply_network(output)\n        # Return output\n        return output\n    else:\n        # No additional networks loaded, use original forward method\n        return original_forward(input)", "CodeBleu_score": 30.68119299905681}, {"code": "def network_forward(org_module, input, original_forward)\n    if len(networks) == 0:\n        return original_forward(input)\n    else:\n        input = input.to(device)\n        restore_weights()\n        output = original_forward(input)\n        for net in networks:\n            if net.layer == org_module._get_name():\n                output = net.apply(output)\n        return output", "CodeBleu_score": 27.050899881768263}, {"code": "def network_forward(org_module, input, original_forward)\n    if not hasattr(org_module, 'loaded_modules'):\n        return original_forward(input)\n    input = input.to(org_module.device)\n    org_module.restore_weights()\n    output = original_forward(input)\n    for module in org_module.loaded_modules:\n        if module.name == org_module.name:\n            output = module.forward(output)\n    return output", "CodeBleu_score": 30.347234942112866}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n    return [param for layer in self.layers for param in layer.parameters() if isinstance(layer, (torch.nn.Linear, torch.nn.LayerNorm))]", "CodeBleu_score": 18.764551609128798}, {"code": "def trainables(self)\n    return [param for layer in self.layers for param in layer.parameters()]\n", "CodeBleu_score": 9.735081357744765}, {"code": "def trainables(self)\n    trainable = []\n    for layer in self.layers:\n        if isinstance(layer, torch.nn.Linear) or isinstance(layer, torch.nn.LayerNorm):\n            trainable.extend(layer.parameters())\n    return trainable\n", "CodeBleu_score": 38.62425912033203}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n    hypernetworks = []\n    for name in names:\n        hypernetwork = hypernetworks_loaded.get(name)\n        if hypernetwork is None:\n            hypernetwork = Hypernetwork(name)\n            hypernetworks_loaded[name] = hypernetwork\n        if multipliers is not None:\n            hypernetwork.multiplier = multipliers[name]\n        hypernetworks.append(hypernetwork)\n    return hypernetworks", "CodeBleu_score": 31.54421873441766}, {"code": "def load_hypernetworks(names, multipliers=None)\n    from hypernetworks.hypernetwork import Hypernetwork\n    from hypernetworks.hypernetworks_list import hypernetworks_list\n\n    # Check if the specified hypernetworks have already been loaded\n    already_loaded = []\n    for name in names:\n        if name in hypernetworks_list:\n            already_loaded.append(name)\n\n    # Clear the current list of hypernetworks\n    hypernetworks_list.clear()\n\n    # Load or retrieve each hypernetwork by name, setting its multiplier if provided\n    for name in names:\n        if name in already_loaded:\n            hypernetwork = hypernetworks_list[name]\n        else:\n            hypernetwork = Hypernetwork(name)\n            hypernetwork.load()\n            if multipliers and name in multipliers:\n                hypernetwork.multiplier = multipliers[name]\n\n        # Append the loaded hypernetwork to the shared list\n        hypernetworks_list.append(hypernetwork)", "CodeBleu_score": 33.94753499917896}, {"code": "def load_hypernetworks(names, multipliers=None)\n    # Check for already loaded hypernetworks and clear the current list\n    if \"hypernetworks\" in globals():\n        globals()[\"hypernetworks\"].clear()\n\n    # Load or retrieve each hypernetwork by name, setting its multiplier if provided\n    for name in names:\n        if name in globals():\n            hypernetwork = globals()[name]\n        else:\n            hypernetwork = Hypernetwork(name)\n        if multipliers is not None:\n            hypernetwork.multiplier = multipliers[name]\n        globals()[\"hypernetworks\"].append(hypernetwork)\n\n    # Append each loaded hypernetwork to the shared list\n    return globals()[\"hypernetworks\"]\n\n", "CodeBleu_score": 25.66425882646193}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 61.23443109506348}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 61.23443109506348}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 61.23443109506348}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    q = self.query(x)\n    if context is not None:\n        context = self.context(context)\n    if self.use_hypernet:\n        q = q + self.hq(context)\n    if self.use_hypernet:\n        k = k + self.hk(context)\n    if self.use_hypernet:\n        v = v + self.hv(context)\n    q = self.reshape_qk(q)\n    k = self.reshape_qk(k)\n    v = self.reshape_qk(v)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n        scores = scores.masked_fill(mask == 0, -1e9)\n    scores = F.softmax(scores, dim=-1)\n    if self.dropout is not None:\n        scores = self.dropout(scores)\n    o = torch.matmul(scores, v)\n    o = o.transpose(1, 2).contiguous()\n    o = o.view(o.shape[0], -1, self.head_dim * self.num_heads)\n    o = self.out(o)\n    return o\n\n", "CodeBleu_score": 38.32929847311751}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    # type: (Tensor, Optional[Tensor], Optional[Tensor], **Any) -> Tensor\n    q = self.q_proj(x)\n    k = self.k_proj(context) if context is not None else self.k_proj(x)\n    v = self.v_proj(context) if context is not None else self.v_proj(x)\n\n    q = self.q_net(q) if self.q_net is not None else q\n    k = self.k_net(k) if self.k_net is not None else k\n    v = self.v_net(v) if self.v_net is not None else v\n\n    q = q.view(q.size(0), q.size(1), self.num_heads, -1).transpose(1, 2)\n    k = k.view(k.size(0), k.size(1), self.num_heads, -1).transpose(1, 2)\n    v = v.view(v.size(0), v.size(1), self.num_heads, -1).transpose(1, 2)\n\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n    if mask is not None:\n        mask = mask.unsqueeze(1).expand_as(scores)\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim=-1)\n\n    if self.dropout is not None:\n        p_attn = self.dropout(p_attn)\n\n    x = torch.matmul(p_attn, v).transpose(1, 2).contiguous()\n    x = x.view(x.size(0), x.size(1), -1)\n    x = self.out_proj(x)\n    return x\n\n", "CodeBleu_score": 33.01528346496097}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    # x: (batch_size, seq_len, hidden_size)\n    # context: (batch_size, context_len, hidden_size)\n    # mask: (batch_size, seq_len, context_len)\n    # kwargs:\n    #   key_padding_mask: (batch_size, context_len)\n    #   need_weights: bool\n    #   attn_mask: (batch_size, seq_len, seq_len)\n    #   average_attn_weights: bool\n    #   self_attention: bool\n    #   need_head_weights: bool\n    #   need_key_padding_mask: bool\n    #   need_value_weights: bool\n    #   need_context_weights: bool\n    #   need_context_weights_sum: bool\n    #   need_context_weights_mean: bool\n    #   need_context_weights_max: bool\n    #   need_context_weights_min: bool\n    #   need_context_weights_sum_squared: bool\n    #   need_context_weights_mean_squared: bool\n    #   need_context_weights_max_squared: bool\n    #   need_context_weights_min_squared: bool\n    #   need_context_weights_sum_cubed: bool\n    #   need_context_weights_mean_cubed: bool\n    #   need_context_weights_max_cubed: bool\n    #   need_context_weights_min_cubed: bool\n    #   need_context_weights_sum_fourth: bool\n    #   need_context_weights_mean_fourth: bool\n    #   need_context_weights_max_fourth: bool\n    #   need_context_weights_min_fourth: bool\n    #   need_context_weights_sum_fifth: bool\n    #   need_context_weights_mean_fifth: bool\n    #   need_context_weights_max_fifth: bool\n    #   need_context_weights", "CodeBleu_score": 1.9834764933446987}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    hypernetwork_attributes = {\n        attr: getattr(hypernetwork, attr) for attr in dir(hypernetwork) if not attr.startswith('_')\n    }\n    hypernetwork.checkpoint = checkpoint\n    try:\n        hypernetwork.save(hypernetwork_name, filename)\n    except Exception as e:\n        for attr, value in hypernetwork_attributes.items():\n            setattr(hypernetwork, attr, value)\n        raise e\n    for attr, value in hypernetwork_attributes.items():\n        setattr(hypernetwork, attr, value)\n\n", "CodeBleu_score": 27.014062447162956}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    try:\n        original_checkpoint = hypernetwork.checkpoint\n        original_hypernetwork_name = hypernetwork.name\n        original_filename = hypernetwork.filename\n        hypernetwork.checkpoint = checkpoint\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.filename = filename\n        hypernetwork.save()\n    except Exception as e:\n        hypernetwork.checkpoint = original_checkpoint\n        hypernetwork.name = original_hypernetwork_name\n        hypernetwork.filename = original_filename\n        raise e", "CodeBleu_score": 38.23138040110879}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    hypernetwork_attributes = {\n        \"hypernetwork_name\": hypernetwork.hypernetwork_name,\n        \"hypernetwork_id\": hypernetwork.hypernetwork_id,\n        \"hypernetwork_type\": hypernetwork.hypernetwork_type,\n        \"hypernetwork_version\": hypernetwork.hypernetwork_version,\n        \"hypernetwork_status\": hypernetwork.hypernetwork_status,\n        \"hypernetwork_description\": hypernetwork.hypernetwork_description,\n        \"hypernetwork_creation_date\": hypernetwork.hypernetwork_creation_date,\n        \"hypernetwork_last_modified_date\": hypernetwork.hypernetwork_last_modified_date,\n        \"hypernetwork_last_modified_by\": hypernetwork.hypernetwork_last_modified_by,\n        \"hypernetwork_last_modified_reason\": hypernetwork.hypernetwork_last_modified_reason,\n        \"hypernetwork_last_modified_date\": hypernetwork.hypernetwork_last_modified_date,\n        \"hypernetwork_last_modified_by\": hypernetwork.hypernetwork_last_modified_by,\n        \"hypernetwork_last_modified_reason\": hypernetwork.hypernetwork_last_modified_reason,\n        \"hypernetwork_last_modified_date\": hypernetwork.hypernetwork_last_modified_date,\n        \"hypernetwork_last_modified_by\": hypernetwork.hypernetwork_last_modified_by,\n        \"hypernetwork_last_modified_reason\": hypernetwork.hypernetwork_last_modified_reason,\n        \"hypernetwork_last_modified_date\": hypernetwork.hypernetwork_last_modified_date,\n        \"hypernetwork_last_modified_by\": hypernetwork.hypernetwork_last_modified_by,\n        \"hypernetwork_last_modified_reason\": hypernetwork.hypernetwork_last_modified_reason,\n        \"hypernetwork_last_modified_date\": hypernetwork.hypernetwork_last_modified_date,\n        \"hypernetwork_last_modified_by\": hypernetwork.hypernetwork_last_modified_by,\n        \"hypernetwork_last_modified_reason\": hypernetwork.hypernetwork_last_mod", "CodeBleu_score": 17.855278639462252}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n    pass\n\n", "CodeBleu_score": 3.051106025934401}, {"code": "def ema_scope(self, context=None)\n    pass\n\n", "CodeBleu_score": 3.051106025934401}, {"code": "def ema_scope(self, context=None)\n    pass", "CodeBleu_score": 3.051106025934401}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    model_output = self(x, t, clip_denoised=clip_denoised)\n    if isinstance(model_output, tuple):\n        model_output, posterior_variance = model_output\n    else:\n        posterior_variance, posterior_log_variance = None, None\n    if isinstance(model_output, dict):\n        model_output = model_output[\"pred_noise\"]\n    assert model_output.shape == x.shape\n    model_variance, model_log_variance = (None, None)\n    if posterior_variance is not None:\n        assert posterior_variance.shape == x.shape\n        posterior_variance = posterior_variance.to(x.dtype)\n        posterior_log_variance = posterior_variance.log()\n        model_variance = posterior_variance\n        model_log_variance = posterior_log_variance\n    elif posterior_log_variance is not None:\n        assert posterior_log_variance.shape == x.shape\n        posterior_log_variance = posterior_log_variance.to(x.dtype)\n        posterior_variance = posterior_log_variance.exp()\n        model_variance = posterior_variance\n        model_log_variance = posterior_log_variance\n    return model_output, model_variance, model_log_variance\n\n", "CodeBleu_score": 29.67083252557923}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    model_output = self.model(x, t)\n    if clip_denoised:\n        x_recon = torch.clamp(model_output, -1, 1)\n    else:\n        x_recon = model_output\n    model_variance, model_log_variance = self.variance(t, x_recon, x)\n    return model_mean, model_variance, model_log_variance\n", "CodeBleu_score": 31.873489763153007}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    model_output = self.model(x, t)\n    if self.model_var_type in [\"learned\", \"learned_range\"]:\n        model_var, model_log_var = self.variance(t, model_output, x)\n    else:\n        model_var, model_log_var = self.model_var, self.model_log_var\n    assert model_output.shape == x.shape\n    if clip_denoised:\n        x_recon = torch.clamp(model_output, -1.0, 1.0)\n    else:\n        x_recon = model_output\n    x_recon.requires_grad = True\n    model_output.requires_grad = True\n    posterior_mean, posterior_variance, posterior_log_variance = self.q_posterior(\n        x_start=x, x_t=x_recon, t=t\n    )\n    assert (\n        posterior_mean.shape == posterior_variance.shape == posterior_log_variance.shape\n    )\n    return posterior_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 43.66610850834921}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n    loss_type = self.loss_type\n    if loss_type == \"l1\":\n        if mean:\n            loss = torch.mean(torch.abs(pred - target))\n        else:\n            loss = torch.abs(pred - target)\n    elif loss_type == \"l2\":\n        if mean:\n            loss = torch.mean(torch.pow(pred - target, 2))\n        else:\n            loss = torch.pow(pred - target, 2)\n    else:\n        raise ValueError(\"Unknown loss type: {}\".format(loss_type))\n    return loss", "CodeBleu_score": 42.53003221117528}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        if mean:\n            return torch.mean((pred - target) ** 2)\n        else:\n            return (pred - target) ** 2\n    else:\n        raise ValueError('Unknown loss type: {}'.format(self.loss_type))", "CodeBleu_score": 33.96004812577799}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n        return loss\n    elif self.loss_type == 'l2':\n        loss = torch.nn.functional.mse_loss(pred, target, reduction='none')\n        if mean:\n            loss = torch.mean(loss)\n        return loss\n    else:\n        raise ValueError(f\"Unknown loss type: {self.loss_type}\")", "CodeBleu_score": 56.54132828353008}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.ones(batch_size, device=x_start.device, dtype=torch.long)\n    mean, log_var = self.q_posterior(x_start, t)\n    kl = mean.pow(2).add_(log_var.exp()).mul_(-1).add_(1).add_(log_var)\n    kl = kl.sum(dim=[1, 2, 3]).mul_(-0.5)\n    bpd = kl.div_(np.log(2) * self.channels * self.image_size ** 2)\n    return bpd\n\n", "CodeBleu_score": 29.330537406679184}, {"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.ones((batch_size,), device=x_start.device, dtype=torch.long)\n    model_kwargs = dict(y=None)\n    q_t = self.q_sample(x_start, t, noise=None)\n    mean, log_var = self.q_mean_variance(x_start=x_start, t=t)\n    kl = normal_kl(mean, log_var, post_mean=0, post_var=1)\n    kl = kl.div(np.log(2))\n    kl = kl.sum(dim=[1, 2, 3]).mean(dim=0)\n    return kl", "CodeBleu_score": 30.558484942458254}, {"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.ones(batch_size, device=x_start.device) * (self.num_timesteps - 1)\n    t = t.long()\n\n    model_output = self.q_posterior(x_start, t)\n    model_mean, model_log_variance = torch.chunk(model_output, 2, dim=-1)\n    posterior_mean, posterior_log_variance = model_mean, model_log_variance\n\n    log_variance = posterior_log_variance - self.posterior_mean_type\n    variance = torch.exp(log_variance)\n    kl = log_normal_density(posterior_mean, variance, 0, 1)\n    kl = kl.sum(dim=-1)\n    bpd = -kl / np.log(2)\n\n    return bpd\n\n", "CodeBleu_score": 33.65592627914089}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n    return diskcache.Cache(\n        path=f\"{CACHE_DIR}/{subsection}\",\n        size_limit=4 * 1024 * 1024 * 1024,\n        eviction_policy=\"least-recently-used\",\n        disk_min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 52.903983577507695}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n    return diskcache.Cache(\n        cache_dir / f\"{subsection}\",\n        size_limit=4 * 1024 * 1024 * 1024,\n        disk_min_file_size=256 * 1024,\n    )\n\n", "CodeBleu_score": 52.288376431604775}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n    cache_dir = os.path.join(CACHE_DIR, subsection)\n    cache = diskcache.Cache(cache_dir, size_limit=4e9, eviction_policy='least-recently-used', disk_min_file_size=256e3)\n    return cache\n", "CodeBleu_score": 47.08964028572959}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n\n    # Retrieve the cache object from the global caches dictionary\n    cache_object = caches.get(subsection)\n\n    # If the cache object does not exist, acquire a lock to ensure thread-safety\n    with cache_lock:\n        # Check if the cache directory exists and if the cache filename is a valid file\n        if not os.path.exists(cache_directory) or not os.path.isfile(cache_filename):\n            # Convert legacy cached data if necessary\n            convert_old_cached_data()\n\n        # Re-check the caches dictionary for the subsection\n        cache_object = caches.get(subsection)\n\n        # If the cache object still does not exist, create a new cache object\n        if cache_object is None:\n            cache_object = make_cache(subsection)\n            caches[subsection] = cache_object\n\n    return cache_object", "CodeBleu_score": 55.532597266764924}, {"code": "def cache(subsection)\n\n    import os\n    import json\n    from threading import Lock\n    from . import config\n    from . import convert_old_cached_data\n    from . import make_cache\n\n    caches = config.caches\n    cache_lock = config.cache_lock\n\n    if subsection in caches:\n        return caches[subsection]\n\n    with cache_lock:\n        if subsection in caches:\n            return caches[subsection]\n\n        cache_dir = config.cache_dir\n        cache_filename = os.path.join(cache_dir, subsection + \".json\")\n\n        if os.path.exists(cache_dir) and os.path.isfile(cache_filename):\n            with open(cache_filename, \"r\") as f:\n                try:\n                    cache_data = json.load(f)\n                except json.JSONDecodeError:\n                    convert_old_cached_data(subsection)\n                    return cache(subsection)\n\n            if \"data\" in cache_data:\n                caches[subsection] = cache_data[\"data\"]\n                return caches[subsection]\n\n        caches[subsection] = make_cache(subsection)\n\n    return caches[subsection]", "CodeBleu_score": 33.72237649141254}, {"code": "def cache(subsection)\n\n    from . import caches, cache_lock\n\n    try:\n        return caches[subsection]\n    except KeyError:\n        with cache_lock:\n            try:\n                return caches[subsection]\n            except KeyError:\n                pass\n\n        from . import convert_old_cached_data\n\n        convert_old_cached_data(subsection)\n\n        from . import make_cache\n\n        caches[subsection] = make_cache(subsection)\n\n        return caches[subsection]\n\n", "CodeBleu_score": 31.12866064866274}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    if title in cache and cache[title][\"mtime\"] >= mtime:\n        return cache[title][\"data\"]\n    data = func()\n    cache[title] = {\"mtime\": mtime, \"data\": data}\n    dump_cache(subsection, cache)\n    return data", "CodeBleu_score": 22.076122113945658}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    if title in cache and mtime <= cache[title]['mtime']:\n        return cache[title]['data']\n    data = func()\n    cache[title] = {'mtime': mtime, 'data': data}\n    dump_cache(subsection, cache)\n    return data\n", "CodeBleu_score": 22.327620602182012}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.stat(filename).st_mtime\n    if title in cache:\n        if mtime > cache[title]['mtime']:\n            del cache[title]\n    if title not in cache:\n        cache[title] = {'mtime': mtime, 'data': func()}\n        dump_cache(subsection, cache)\n    return cache[title]['data']\n", "CodeBleu_score": 23.006500190605088}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception()\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n", "CodeBleu_score": 52.94417170818152}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception()\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n", "CodeBleu_score": 63.65335006274419}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception()\n    lines = message.splitlines()\n    for line in lines:\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n", "CodeBleu_score": 49.02456257058659}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    try:\n        positive_prompt = extract_style_text(style.prompt, prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    try:\n        negative_prompt = extract_style_text(style.negative_prompt, negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 54.23618178758871}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    positive_prompt = extract_style_text(style, prompt)\n    negative_prompt = extract_style_text(style, negative_prompt)\n    if positive_prompt == \"\" or negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    return True, positive_prompt, negative_prompt\n\n", "CodeBleu_score": 44.788666745299516}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n\n    extracted_prompt = extract_style_text(style, prompt)\n    if extracted_prompt is None:\n        return False, prompt, negative_prompt\n\n    extracted_negative_prompt = extract_style_text(style, negative_prompt)\n    if extracted_negative_prompt is None:\n        return False, prompt, negative_prompt\n\n    return True, extracted_prompt, extracted_negative_prompt\n\n", "CodeBleu_score": 52.00488398778508}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]", "CodeBleu_score": 55.89879214825755}, {"code": "def random_partial(self, count, n_frames)\n    self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]", "CodeBleu_score": 36.79237623664877}, {"code": "def random_partial(self, count, n_frames)\n    if not self.utterances:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in utterances]", "CodeBleu_score": 55.89879214825755}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only cannot be set to True.\")\n    dBFS_change = target_dBFS - wav.dBFS\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav.apply_gain(dBFS_change)", "CodeBleu_score": 55.876291776903045}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only cannot be set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if dBFS_change < 0 and increase_only:\n        return wav\n    if dBFS_change > 0 and decrease_only:\n        return wav\n    return wav * 10 ** (dBFS_change / 20)\n", "CodeBleu_score": 72.79560473578768}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    dBFS_change = target_dBFS - wav.dBFS\n    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n        return wav\n    return wav.apply_gain(dBFS_change)\n\n", "CodeBleu_score": 26.87763644665227}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n    global _model\n    _model = SpeakerEncoder(device=device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location='cpu')['model_state'])\n    _model.eval()\n    print('Loaded model from epoch {}'.format(torch.load(weights_fpath, map_location='cpu')['epoch']))\n\n", "CodeBleu_score": 20.757085672360947}, {"code": "def load_model(weights_fpath: Path, device=None)\n    global _model\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    _model = SpeakerEncoder(device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, map_location=\"cpu\")\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded pre-trained model from epoch {}\".format(checkpoint[\"epoch\"]))\n", "CodeBleu_score": 56.17587720788697}, {"code": "def load_model(weights_fpath: Path, device=None)\n    global _model\n    device = torch.device(device) if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    _model = SpeakerEncoder(device=device)\n    checkpoint = torch.load(weights_fpath)\n    _model.load_state_dict(checkpoint['model_state'])\n    _model.eval()\n    print('Loaded pretrained model from epoch %d' % checkpoint['epoch'])\n\n", "CodeBleu_score": 36.188184908913065}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n    if not hasattr(embed_frames_batch, '_model'):\n        raise Exception('Model not loaded')\n    if not hasattr(embed_frames_batch, '_device'):\n        raise Exception('Device not set')\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model.forward(frames_batch).detach().cpu().numpy()\n    return embeddings", "CodeBleu_score": 57.209665374558504}, {"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception(\"Model not loaded\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    embeddings = _model(frames_batch).detach().cpu().numpy()\n    return embeddings", "CodeBleu_score": 62.11983948483195}, {"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception(\"Model not loaded\")\n    frames_batch = torch.from_numpy(frames_batch)\n    frames_batch = frames_batch.to(_device)\n    embeddings = _model.forward(frames_batch)\n    embeddings = embeddings.detach().cpu()\n    embeddings = embeddings.numpy()\n    return embeddings", "CodeBleu_score": 55.593522062763846}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0 <= min_pad_coverage <= 1\n    assert 0 < overlap < 1\n\n    # Compute how many frames will result in (N + partial_utterance_n_frames) frames after padding\n    total_frames = int(np.ceil(n_samples / frame_step))\n    total_frames += int(np.ceil(partial_utterance_n_frames / frame_step))\n\n    # Compute how many frames will result in (N + partial_utterance_n_frames) frames after overlap + padding\n    total_samples = frame_step * (total_frames - partial_utterance_n_frames)\n    total_samples = int(np.ceil(total_samples))\n\n    # Re-compute the number of frames after padding and make sure we have at least (partial_utterance_n_frames + 1)\n    total_frames = int(np.ceil(total_samples / frame_step))\n    total_frames = max(total_frames, partial_utterance_n_frames + 1)\n\n    # Re-compute the number of samples to match the new total_frames\n    total_samples = frame_step * (total_frames - 1)\n    total_samples = int(np.ceil(total_samples))\n\n    # Ensure we have at least partial_utterance_n_frames + 1 frames\n    total_frames = max(total_frames, partial_utterance_n_frames + 1)\n\n    # Compute frame step\n    frame_step = int(np.floor(total_samples / (total_frames - 1)))\n\n    # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = np.arange(0, total_frames, frame_step)\n    for i in range(len(steps) - 1):\n        start_sample = int(steps[i])\n        end_sample = int(steps[i + 1])\n\n        # Perform overlap and add partial at the end if necessary\n        if i + 1 == len(steps) - 1:\n            if end_sample + partial_utterance_n_frames < n_samples:", "CodeBleu_score": 36.277109507306314}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    # Check that the partial utterance n_frames is smaller than the total number of frames\n    assert partial_utterance_n_frames < n_samples\n    # Check that the overlap is smaller than 1 and larger than 0\n    assert overlap < 1 and overlap > 0\n    # Check that the min pad coverage is smaller than 1 and larger than 0\n    assert min_pad_coverage < 1 and min_pad_coverage > 0\n\n    # Calculate the number of frames and the frame step based on the sampling rate and overlap\n    frame_step = int(config.SAMPLE_RATE * (1 - overlap))\n    n_frames_total = int(config.SAMPLE_RATE * partial_utterance_n_frames)\n\n    # Compute the number of mel and linear spectrogram frames\n    n_mels = int(config.MEL_BINS)\n    n_frames = int(np.ceil(n_frames_total / frame_step))\n\n    # Compute the start and end sample for the current partial utterance\n    start_sample = int(frame_step * (n_frames - n_frames_total))\n    end_sample = start_sample + n_frames_total\n\n    # Compute mel and linear spectrogram slices for the current partial utterance\n    mel_slices = np.expand_dims(np.linspace(0, n_mels - 1, n_mels), 0).astype(np.int)\n    wav_slices = np.expand_dims(np.arange(start_sample, end_sample), 0).astype(np.int)\n\n    # Evaluate whether extra padding is warranted or not\n    coverage = (wav_slices.shape[1] - start_sample) / n_frames_total\n    if coverage < min_pad_coverage and wav_slices.shape[1] - start_sample > 0:\n        # Remove last slice if too short\n        mel_slices = mel_slices[:, :-1]\n        wav_slices = wav_slices[:, :-1]\n\n    return wav", "CodeBleu_score": 35.52769148754673}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0 <= overlap < 1, \"Invalid value encountered in argument 'overlap'\"\n    assert 0 <= min_pad_coverage < 1, \"Invalid value encountered in argument 'min_pad_coverage'\"\n\n    # Compute how many frames will result in `partial_utterance_n_frames` if we sample\n    # `partial_utterance_n_frames` frames, overlaping by `overlap`, from a sine wave\n    n_frames_per_step = int(\n        math.ceil(\n            (1 + overlap) * partial_utterance_n_frames / sampling_rate\n        )\n    )\n\n    # Compute how many steps will result in `partial_utterance_n_frames` if we sample\n    # `partial_utterance_n_frames` frames, overlaping by `overlap`, from a sine wave\n    n_steps = int(\n        math.ceil(\n            partial_utterance_n_frames / n_frames_per_step\n        )\n    )\n\n    # Compute the number of frames that will result in `partial_utterance_n_frames`\n    # if we start with a sine wave amplitude of 1 and decay it by 0.5 to 0, using\n    # `n_steps`\n    n_pad_frames = int(\n        math.ceil(\n            n_steps / 2\n        ) * n_frames_per_step\n    )\n\n    # We now have enough information to compute the start and end frame of the first\n    # slice\n    start_frame = 0\n    end_frame = min(n_pad_frames, n_samples)\n\n    slices = []\n    mel_slices = []\n    while start_frame < n_samples:\n        # Append a slice\n        slices += [(start_frame, end_frame)]\n        mel_slices += [(start_frame, end_frame)]\n\n        # Compute the starting frame of the next slice\n        start_frame = end_frame + n_frames_per_step\n\n        # Compute the ending frame of the next slice\n        end_frame = min(start_frame + n_pad_frames, n_samples", "CodeBleu_score": 26.09813687359688}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n    embed = embed.reshape((shape, shape))\n    im = ax.imshow(embed, cmap=\"coolwarm\", vmin=color_range[0], vmax=color_range[1])\n    ax.figure.colorbar(im, ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    return ax\n\n", "CodeBleu_score": 49.97982255049653}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = (int(np.sqrt(len(embed))), -1)\n    embed = embed.reshape(shape)\n    im = ax.imshow(embed, cmap=\"coolwarm\", vmin=color_range[0], vmax=color_range[1])\n    ax.figure.colorbar(im, ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n\n", "CodeBleu_score": 53.24878807026872}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        shape = (int(np.sqrt(embed.shape[0])), int(np.sqrt(embed.shape[0])))\n\n    embed = embed.reshape(shape)\n\n    im = ax.imshow(embed, cmap=plt.cm.hot)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.axis(\"off\")\n    plt.tight_layout()\n    return ax\n\n", "CodeBleu_score": 55.95138053285338}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n    centroids_incl = torch.zeros(num_speakers, self.embed_dim, device=self.device)\n    centroids_excl = torch.zeros(num_speakers, self.embed_dim, device=self.device)\n    mask = torch.ones(num_speakers, num_speakers, device=self.device)\n    for i in range(num_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0)\n        centroids_excl[i] = torch.mean(torch.cat([embeds[i, :i], embeds[i, i + 1:]], dim=0), dim=0)\n        mask[i, i] = 0\n    sim_matrix = torch.zeros(num_speakers, num_speakers, device=self.device)\n    for i in range(num_speakers):\n        sim_matrix[i] = torch.matmul(centroids_incl[i], centroids_excl.t())\n    sim_matrix = sim_matrix * mask\n    sim_matrix = sim_matrix / torch.norm(sim_matrix, dim=1, keepdim=True)\n    sim_matrix = sim_matrix / torch.norm(sim_matrix, dim=0, keepdim=True)\n    sim_matrix = sim_matrix * (self.scale - self.bias) + self.bias\n    return sim_matrix\n\n", "CodeBleu_score": 36.629705890011465}, {"code": "def similarity_matrix(self, embeds)\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n    centroids_incl = torch.zeros(num_speakers, self.dim, device=embeds.device)\n    centroids_excl = torch.zeros(num_speakers, self.dim, device=embeds.device)\n    mask = torch.ones(num_speakers, num_speakers, device=embeds.device)\n    mask = mask.triu(1)\n    for i in range(num_speakers):\n        centroids_incl[i] = torch.mean(embeds[i], dim=0)\n        centroids_incl[i] = F.normalize(centroids_incl[i], p=2, dim=0)\n        centroids_excl[i] = torch.mean(torch.cat([embeds[i, :i], embeds[i, i + 1:]], dim=0), dim=0)\n        centroids_excl[i] = F.normalize(centroids_excl[i], p=2, dim=0)\n    similarity_matrix = torch.zeros(num_speakers, num_speakers, device=embeds.device)\n    for i in range(num_speakers):\n        similarity_matrix[i] = torch.matmul(centroids_incl[i], centroids_incl.t())\n        similarity_matrix[i] = torch.matmul(centroids_excl[i], centroids_excl.t())\n    similarity_matrix = similarity_matrix * self.scale + self.bias\n    similarity_matrix = similarity_matrix * mask\n    return similarity_matrix", "CodeBleu_score": 34.117099561822386}, {"code": "def similarity_matrix(self, embeds)\n    batch_size, num_speakers, num_utterances, _ = embeds.shape\n    centroids_incl = embeds.reshape(batch_size, num_speakers, num_utterances, -1).mean(dim=2)\n    centroids_excl = (embeds.reshape(batch_size, num_speakers, num_utterances, -1) - embeds.reshape(batch_size, num_speakers, 1, num_utterances, -1)).mean(dim=2)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=-1)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=-1)\n    sim_matrix = torch.zeros(num_speakers, num_speakers, device=embeds.device)\n    mask = torch.ones(num_speakers, num_speakers, device=embeds.device) - torch.eye(num_speakers, device=embeds.device)\n    for i in range(num_speakers):\n        sim_matrix[i] = torch.matmul(centroids_incl[i], centroids_incl.transpose(0, 1)) + torch.matmul(centroids_excl[i], centroids_excl.transpose(0, 1))\n    sim_matrix = sim_matrix * mask\n    sim_matrix = sim_matrix / torch.max(sim_matrix)\n    sim_matrix = sim_matrix * 0.5 + 0.5\n    return sim_matrix", "CodeBleu_score": 29.053337857380612}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n    embeds = embeds.cpu().detach().numpy()\n    n_speakers = embeds.shape[0]\n    n_utts = embeds.shape[1]\n    embeds = embeds.reshape(n_speakers * n_utts, -1)\n    sim_mat = self.cosine_similarity(embeds, embeds)\n    sim_mat = sim_mat.reshape(n_speakers, n_utts, n_speakers, n_utts)\n    sim_mat = sim_mat.transpose(2, 0, 3, 1).reshape(n_speakers * n_speakers, n_utts * n_utts)\n\n    gt_labels = np.zeros((n_speakers, n_speakers, n_utts, n_utts))\n    for i in range(n_speakers):\n        for j in range(n_speakers):\n            gt_labels[i, j, :, :] = np.eye(n_utts)\n    gt_labels = gt_labels.reshape(n_speakers, n_speakers, n_utts * n_utts).transpose(0, 2, 1, 3)\n    gt_labels = gt_labels.reshape(n_speakers * n_utts, n_speakers * n_utts)\n\n    loss = self.criterion(sim_mat, gt_labels)\n\n    pred_labels = np.zeros((n_speakers, n_speakers, n_utts, n_utts))\n    for i in range(n_speakers):\n        for j in range(n_speakers):\n            pred_labels[i, j, :, :] = np.eye(n_utts)\n    pred_labels = pred_labels.reshape(n_speakers, n_speakers, n_utts * n_utts).transpose(0, 2, 1, 3)\n    pred_labels = pred_labels.reshape(n_speakers * n_utts, n_speakers *", "CodeBleu_score": 24.51720304695663}, {"code": "def loss(self, embeds)\n    # Get the number of speakers and utterances per speaker from the shape of the embeddings\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Compute a similarity matrix of shape (num_speakers, num_speakers, num_utterances)\n    # by taking the dot product of the embeddings with their transpose\n    similarity_matrix = torch.bmm(embeds, embeds.transpose(1, 2))\n\n    # Reshape the similarity matrix to be of shape (num_speakers * num_utterances, num_speakers * num_utterances)\n    similarity_matrix = similarity_matrix.view(num_speakers * num_utterances, num_speakers * num_utterances)\n\n    # Generate ground truth labels for the similarity matrix\n    # 1 if the two speakers are the same, 0 otherwise\n    ground_truth = torch.eye(num_speakers * num_utterances).to(self.device)\n\n    # Compute the loss using the ground truth labels and the similarity matrix\n    loss = self.criterion(similarity_matrix, ground_truth)\n\n    # Compute the EER without backpropagation\n    # Compute the predicted labels by taking the argmax of the similarity matrix along the second dimension\n    predicted = torch.argmax(similarity_matrix, dim=1)\n\n    # Compute the true labels by taking the diagonal of the ground truth matrix\n    true = torch.diag(ground_truth)\n\n    # Compute the false positive rate and false negative rate for different thresholds\n    fpr, tpr, thresholds = roc_curve(true, predicted)\n\n    # Find the threshold where the false positive rate equals the false negative rate\n    eer = fpr[np.nanargmin(np.absolute((fpr - tpr)))]\n\n    return loss, eer", "CodeBleu_score": 19.57626879402502}, {"code": "def loss(self, embeds)\n    # Calculate the number of speakers and utterances per speaker\n    n_speakers = embeds.shape[0]\n    n_utts = embeds.shape[1]\n\n    # Reshape the embeddings to a 2D matrix\n    embeds = embeds.reshape(n_speakers * n_utts, -1)\n\n    # Compute the similarity matrix\n    sim_matrix = torch.matmul(embeds, embeds.T)\n\n    # Generate ground truth labels\n    labels = torch.eye(n_speakers * n_utts, dtype=torch.float32, device=self.device)\n\n    # Compute the loss\n    loss = self.criterion(sim_matrix, labels)\n\n    # Compute the EER\n    with torch.no_grad():\n        # Compute the predicted labels\n        preds = sim_matrix.argmax(dim=1)\n\n        # Compute the true labels\n        true_labels = torch.arange(n_speakers * n_utts, dtype=torch.long, device=self.device)\n\n        # Compute the ROC curve\n        fpr, tpr, thresholds = roc_curve(true_labels, preds, pos_label=1)\n\n        # Find the threshold where the false positive rate equals the false negative rate\n        eer = thresholds[np.argmin(np.abs(fpr - tpr))]\n\n    return loss, eer", "CodeBleu_score": 25.205508785787075}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n    import configparser\n    import os\n    from IPython.display import display, Markdown\n    config = configparser.ConfigParser()\n    config.read(os.path.join(os.getcwd(), \"config.ini\"))\n    if config[\"LOGGING\"][\"log_params\"] == \"True\":\n        params = \"\"\n        for param in self.model.parameters():\n            params += f\"{param.name}: {param.data}\\n\"\n        for param in self.data.parameters():\n            params += f\"{param.name}: {param.data}\\n\"\n        display(Markdown(f\"## Parameters\\n{params}\"))\n\n", "CodeBleu_score": 19.678622457646032}, {"code": "def log_params(self)\n    import json\n    import os\n    import sys\n    import inspect\n    from rich.console import Console\n    from rich.table import Table\n    from rich.panel import Panel\n    from rich.theme import Theme\n    from rich.style import Style\n\n    # Import parameter configurations\n    with open(\"config/params.json\", \"r\") as f:\n        params = json.load(f)\n\n    # Get current function name\n    func_name = inspect.currentframe().f_code.co_name\n\n    # Iterate through model and data parameters\n    model_params = []\n    data_params = []\n    for param in params[\"model_params\"]:\n        model_params.append(f\"{param['name']}: {param['value']}\")\n    for param in params[\"data_params\"]:\n        data_params.append(f\"{param['name']}: {param['value']}\")\n\n    # Create a table with the model and data parameters\n    table = Table(show_header=True, header_style=\"bold magenta\")\n    table.add_column(\"Model Parameters\", style=\"dim\")\n    table.add_column(\"Data Parameters\", style=\"dim\")\n    for i in range(len(model_params)):\n        table.add_row(model_params[i], data_params[i])\n\n    # Create a rich panel with the table and function name\n    panel = Panel(\n        table,\n        title=f\"[bold magenta]{func_name}[/bold magenta]\",\n        title_align=\"left\",\n        padding=(1, 2),\n        style=\"on blue\",\n    )\n\n    # Create a rich console and add the panel to it\n    console = Console()\n    console.print(panel)", "CodeBleu_score": 21.92171782196468}, {"code": "def log_params(self)\n    import config\n    from rich.console import Console\n    from rich.table import Table\n    from rich.panel import Panel\n    from rich.theme import Theme\n    from rich.box import ROUNDED\n    from rich.padding import Padding\n    from rich.style import Style\n    from rich.console import Console\n    from rich.text import Text\n\n    # Create a table to display the parameters\n    table = Table(title=\"Parameters\", show_header=True, header_style=\"bold white\", box=ROUNDED)\n\n    # Add columns for the parameter names and values\n    table.add_column(\"Parameter\", style=\"cyan\", no_wrap=True)\n    table.add_column(\"Value\", style=\"magenta\")\n\n    # Iterate through the model parameters and add them to the table\n    for name, value in self.model.named_parameters():\n        table.add_row(name, str(value.data.cpu().numpy()))\n\n    # Iterate through the data parameters and add them to the table\n    for name, value in self.data.named_parameters():\n        table.add_row(name, str(value.data.cpu().numpy()))\n\n    # Add the table to the console\n    console = Console()\n    console.print(table)\n\n    # Print the model and data parameters to the console\n    print(\"Model parameters:\")\n    for name, value in self.model.named_parameters():\n        print(f\"{name}: {value.data.cpu().numpy()}\")\n    print(\"Data parameters:\")\n    for name, value in self.data.named_parameters():\n        print(f\"{name}: {value.data.cpu().numpy()}\")\n\n    # Print the model and data parameters to the console\n    print(\"Model parameters:\")\n    for name, value in self.model.named_parameters():\n        print(f\"{name}: {value.data.cpu().numpy()}\")\n    print(\"Data parameters:\")\n    for name, value in self.data.named_parameters():\n        print(f\"{name}: {value.data.cpu().numpy()}\")\n\n   ", "CodeBleu_score": 23.197554452624942}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    if self.projections is None:\n        self.projections = {}\n    if self.projections_fig is None:\n        self.projections_fig = plt.figure(figsize=(12, 12))\n    if self.projections_ax is None:\n        self.projections_ax = self.projections_fig.add_subplot(111)\n    if self.projections_ax.collections:\n        self.projections_ax.collections = []\n    if self.projections_ax.lines:\n        self.projections_ax.lines = []\n    if self.projections_ax.patches:\n        self.projections_ax.patches = []\n    if self.projections_ax.texts:\n        self.projections_ax.texts = []\n    if self.projections_ax.images:\n        self.projections_ax.images = []\n    if self.projections_ax.legends:\n        self.projections_ax.legends = []\n    if self.projections_ax.artists:\n        self.projections_ax.artists = []\n    if self.projections_ax.containers:\n        self.projections_ax.containers = []\n    if self.projections_ax.collections:\n        self.projections_ax.collections = []\n    if self.projections_ax.lines:\n        self.projections_ax.lines = []\n    if self.projections_ax.patches:\n        self.projections_ax.patches = []\n    if self.projections_ax.texts:\n        self.projections_ax.texts = []\n    if self.projections_ax.images:\n        self.projections_ax.images = []\n    if self.projections_ax.legends:\n        self.projections_ax.legends = []\n    if self.projections_ax.artists:\n        self.projections_ax.artists = []\n    if self.projections_ax.containers:\n        self.projections_ax.containers = []\n    if self.projections_ax.collections", "CodeBleu_score": 17.983009529003954}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    # Limit the number of speakers to display\n    embeds = embeds[:max_speakers]\n    utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    # Flatten the embeddings\n    embeds = embeds.reshape(embeds.shape[0] * embeds.shape[1], embeds.shape[2])\n    # Assign colors based on speaker identity\n    colors = [self.speaker_colors[i] for i in range(embeds.shape[0])]\n    # Reduce the dimensionality of the embeddings using UMAP\n    proj = umap.UMAP(n_components=2, random_state=0,).fit_transform(embeds)\n    # Plot the embeddings\n    fig = plt.figure(figsize=(4, 4))\n    plt.scatter(proj[:, 0], proj[:, 1], s=10, c=colors, marker=\".\")\n    plt.title(f\"step {step} speaker embeddings\", fontsize=8)\n    plt.axis(\"off\")\n    # Save the plot to a file if specified\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    # Clear the current figure\n    plt.clf()\n    plt.close(fig)\n    # Update the visualization\n    if not self.disable_visualization:\n        self.visualization.update(fig)\n    # Wait for a short time before continuing\n    time.sleep(0.1)\n", "CodeBleu_score": 48.93839497605146}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    # limit the number of speakers\n    if len(embeds) > max_speakers:\n        embeds = embeds[:max_speakers]\n        utterances_per_speaker = utterances_per_speaker[:max_speakers]\n    # flatten the embeddings\n    embeds = embeds.reshape(len(embeds) * utterances_per_speaker.sum(), -1)\n    # assign colors based on speaker identity\n    colors = np.zeros((len(embeds), 3))\n    cmap = plt.get_cmap(\"tab10\")\n    for i, num_utterances in enumerate(utterances_per_speaker):\n        colors[i * num_utterances:(i + 1) * num_utterances] = cmap(i)\n    # reduce the dimensionality of the embeddings using UMAP\n    reducer = umap.UMAP(n_components=2, random_state=42)\n    projections = reducer.fit_transform(embeds)\n    # plot the projections\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.scatter(projections[:, 0], projections[:, 1], c=colors)\n    ax.set_title(f\"UMAP projections (step {step})\")\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    else:\n        plt.show()\n    plt.close(fig)", "CodeBleu_score": 46.80937452574822}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    # Save the dimensions of the input tensor as human-readable names\n    input_dims = x.shape\n    input_dims_names = ['batch', 'time', 'dim']\n\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_dim_index = input_dims_names.index('dim')\n\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(input_dims[speaker_dim_index], 1)\n\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(input_dims[0], input_dims[1], input_dims[2])\n    speaker_embedding = speaker_embedding.transpose(0, 2, 1)\n\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat((x, speaker_embedding), dim=-1)\n\n    return x", "CodeBleu_score": 31.812608772135054}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    # Save the dimensions of the input tensor as human-readable names\n    batch_size, seq_len, _ = x.shape\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_dim_index = self.speaker_dim_index\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(seq_len, axis=0)\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(seq_len, batch_size, -1)\n    speaker_embedding = speaker_embedding.transpose(1, 0, 2)\n    # Concatenate the speaker embedding with the input tensor\n    x = tf.concat([x, speaker_embedding], axis=-1)\n    return x", "CodeBleu_score": 19.13439467392116}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    # Save the dimensions of the input tensor as human-readable names\n    input_dims = x.shape\n    input_dims_str = str(input_dims)\n    input_dims_str = input_dims_str.replace(\"torch.Size(\", \"\")\n    input_dims_str = input_dims_str.replace(\")\", \"\")\n    input_dims_str = input_dims_str.replace(\",\", \"x\")\n    input_dims_str = input_dims_str.replace(\" \", \"\")\n\n    # Determine the appropriate dimension index for the speaker embedding\n    dim_index = -1\n    for i, dim in enumerate(input_dims):\n        if dim == 1:\n            dim_index = i\n            break\n\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(input_dims[dim_index])\n\n    # Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(input_dims[dim_index], -1)\n    speaker_embedding = speaker_embedding.transpose(0, 1)\n\n    # Concatenate the speaker embedding with the input tensor along the last dimension\n    x = torch.cat([x, speaker_embedding], dim=-1)\n\n    return x", "CodeBleu_score": 32.77111282210486}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n    # Get device from model parameters\n    device = next(self.parameters()).device\n\n    # Get shape of encoder sequence projection\n    batch_size, seq_len, _ = encoder_seq_proj.shape\n\n    # Create and zero cumulative and attention tensors\n    cumulative_attention = torch.zeros(batch_size, seq_len, device=device)\n    attention = torch.zeros(batch_size, seq_len, device=device)\n\n    return cumulative_attention, attention\n\n", "CodeBleu_score": 21.507849619700238}, {"code": "def init_attention(self, encoder_seq_proj)\n    device = next(self.parameters()).device\n    batch_size, seq_len = encoder_seq_proj.shape[0], encoder_seq_proj.shape[1]\n    self.cum_attn = torch.zeros(batch_size, seq_len, device=device)\n    self.attn = torch.zeros(batch_size, seq_len, device=device)", "CodeBleu_score": 22.899233096952955}, {"code": "def init_attention(self, encoder_seq_proj)\n    device = next(self.parameters()).device\n    batch_size, seq_len = encoder_seq_proj.shape[0], encoder_seq_proj.shape[1]\n    self.cumulative_attention = torch.zeros(batch_size, seq_len, device=device)\n    self.attention = torch.zeros(batch_size, seq_len, device=device)\n", "CodeBleu_score": 23.669962119294947}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Checkpoint not found at {path}\")\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint[\"model_state_dict\"])\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    return checkpoint", "CodeBleu_score": 44.13332938541403}, {"code": "def load(self, path, optimizer=None)\n    self.to(self.device)\n    checkpoint = torch.load(path)\n    self.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return self\n", "CodeBleu_score": 39.64602315434154}, {"code": "def load(self, path, optimizer=None)\n    state = torch.load(path, map_location=self.device)\n    self.load_state_dict(state['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n", "CodeBleu_score": 29.878525300646704}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n    if optimizer:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)\n    else:\n        torch.save(self.state_dict(), path)\n", "CodeBleu_score": 55.26145593616825}, {"code": "def save(self, path, optimizer=None)\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)", "CodeBleu_score": 57.651555886258265}, {"code": "def save(self, path, optimizer=None)\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, path)\n", "CodeBleu_score": 57.651555886258265}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n    from string import punctuation\n    from inflect import engine\n    from soynlp.normalizer import repeat_normalize\n    from soynlp.word import Word\n    from soynlp.tokenizer import LTokenizer\n    import re\n\n    def remove_special_chars(text):\n        return re.sub(r\"[^A-Za-z0-9(),\\.\\'\\\"\\`]\", \" \", text)\n\n    def remove_punctuation(text):\n        return text.translate(str.maketrans(\"\", \"\", punctuation))\n\n    def expand_numbers(text):\n        return inflect.number_to_words(text)\n\n    def expand_abbreviations(text):\n        return text.replace(\"i.e.\", \"that is\").replace(\"e.g.\", \"for example\")\n\n    def remove_stopwords(text):\n        return \" \".join([word for word in text.split() if word not in stopwords])\n\n    def remove_whitespace(text):\n        return \" \".join(text.split())\n\n    def remove_non_ascii(text):\n        return text.encode(\"ascii\", \"ignore\").decode()\n\n    def remove_non_english(text):\n        return text.encode(\"ascii\", \"ignore\").decode()\n\n    def remove_extra_whitespace(text):\n        return re.sub(r\"\\s+\", \" \", text).strip()\n\n    def remove_extra_whitespace(text):\n        return re.sub(r\"\\s+\", \" \", text).strip()\n\n    def remove_extra_whitespace(text):\n        return re.sub(r\"\\s+\", \" \", text).strip()\n\n    def remove_extra_whitespace(text):\n        return re.sub(r\"\\s+\", \" \", text).strip()\n\n    def remove_extra_whitespace(text):\n        return re.sub(r\"\\s+\", \" \", text).strip()\n\n    def remove_extra_whitespace(text):\n        return re.sub(r\"\\s+\", \" \", text).strip()\n\n    def remove", "CodeBleu_score": 24.517601282388515}, {"code": "def english_cleaners(text)\n    text = text.lower()\n    text = text.replace(\"'\", \"\")\n    text = text.replace(\"-\", \" \")\n    text = text.replace(\"\u2014\", \" \")\n    text = text.replace(\"\u2013\", \" \")\n    text = text.replace(\"_\", \" \")\n    text = text.replace(\"+\", \" \")\n    text = text.replace(\"=\", \" \")\n    text = text.replace(\"(\", \" \")\n    text = text.replace(\")\", \" \")\n    text = text.replace(\"[\", \" \")\n    text = text.replace(\"]\", \" \")\n    text = text.replace(\"{\", \" \")\n    text = text.replace(\"}\", \" \")\n    text = text.replace(\"<\", \" \")\n    text = text.replace(\">\", \" \")\n    text = text.replace(\"|\", \" \")\n    text = text.replace(\"\\\\\", \" \")\n    text = text.replace(\"/\", \" \")\n    text = text.replace(\"*\", \" \")\n    text = text.replace(\"&\", \" \")\n    text = text.replace(\"@\", \" \")\n    text = text.replace(\"$\", \" \")\n    text = text.replace(\"^\", \" \")\n    text = text.replace(\"%\", \" \")\n    text = text.replace(\"`\", \" \")\n    text = text.replace(\"~\", \" \")\n    text = text.replace(\"?\", \" \")\n    text = text.replace(\":\", \" \")\n    text = text.replace(\";\", \" \")\n    text = text.replace(\",\", \" \")\n    text = text.replace(\".\", \" \")\n    text = text.replace(\"+\", \" \")\n    text = text.replace(\"-\", \" \")\n    text = text.replace(\"=\", \" \")\n    text = text.replace(\"_\", \" \")\n    text = text.replace(\"|\", \" \")\n    text = text.replace(\"&\", \" \")\n    text = text.replace(\"*\", \" \")\n    text = text.replace(\"$\", \" \")\n    text = text.replace(\"@\", \" \")\n    text = text.replace(\"^\", \" \")\n    text = text.replace(\"%\", \" \")\n    text = text", "CodeBleu_score": 20.459127990777343}, {"code": "def english_cleaners(text)\n    text = text.lower()\n    text = text.replace(\"'\", \"\")\n    text = text.replace(\"`\", \"\")\n    text = text.replace(\"-\", \" \")\n    text = text.replace(\"\u2013\", \" \")\n    text = text.replace(\"\u2014\", \" \")\n    text = text.replace(\"\u2026\", \".\")\n    text = text.replace(\"\u2022\", \".\")\n    text = text.replace(\"?\", \".\")\n    text = text.replace(\".\", \". \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n", "CodeBleu_score": 19.50871837123512}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        text_without_braces = text.split('{')\n        for i in range(len(text_without_braces)):\n            if i % 2 == 1:\n                sequence += cleaners.english_cleaners(text_without_braces[i])\n            else:\n                sequence += symbols_to_sequence(text_without_braces[i])\n        break\n\n    # Append EOS token\n    sequence.append(symbols.eos_id)\n    return sequence\n\n", "CodeBleu_score": 32.27157569904687}, {"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        text_part = text[:1]\n        text = text[1:]\n\n        if text_part == \" \":\n            continue\n        elif text_part == \"|\":\n            sequence.append(punctuation_to_arpabet[\"|\"])\n        elif text_part == \"'\":\n            if len(text) and text[0] == \" \":\n                text = text[1:]\n                sequence.append(punctuation_to_arpabet[\"|\"])\n            else:\n                sequence.append(punctuation_to_arpabet[\"'\"])\n        elif text_part == \"`\":\n            sequence.append(punctuation_to_arpabet[\"`\"])\n        elif text_part == \",\":\n            sequence.append(punctuation_to_arpabet[\",\"])\n        elif text_part == \".\":\n            sequence.append(punctuation_to_arpabet[\".\"])\n        elif text_part == \"-\":\n            if len(text) and text[0] == \" \":\n                text = text[1:]\n                sequence.append(punctuation_to_arpabet[\"-\"])\n            else:\n                sequence.append(punctuation_to_arpabet[\" \"])\n        elif text_part == \"(\":\n            sequence.append(punctuation_to_arpabet[\"(\"])\n        elif text_part == \")\":\n            sequence.append(punctuation_to_arpabet[\")\"])\n        elif text_part == \"\\\"\":\n            sequence.append(punctuation_to_arpabet[\"\\\"\"])\n        elif text_part == \":\":\n            sequence.append(punctuation_to_arpabet[\":\"])\n        elif text_part == \";\":\n            sequence.append(punctuation_to_arpabet[\";\"])\n        elif text_part == \"+\":\n            sequence.append(punctuation_to_arpabet[\"+\"])\n        elif text_part == \"@\":\n            sequence.append(punctuation_to_arpabet[\"@\"])\n        elif", "CodeBleu_score": 17.01151728011481}, {"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        text_part = text[:1]\n        text = text[1:]\n\n        if text_part == \" \":\n            continue\n        elif text_part == \"|\":\n            sequence.append(arpabet_to_sequence(\" \", cleaner_names))\n        elif text_part == \"{\":\n            brace_closed = False\n            while not brace_closed:\n                text_part = text[:1]\n                text = text[1:]\n                sequence += arpabet_to_sequence(text_part, cleaner_names)\n                if text_part == \"}\":\n                    brace_closed = True\n\n        # The text is a sequence of phonemes (not ARPAbet)\n        else:\n            sequence += arpabet_to_sequence(text_part, cleaner_names)\n\n    sequence.append(_symbols_to_sequence(\"", "CodeBleu_score": 25.158480481892596}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in ARPABET_TO_ID:\n            symbol = ARPABET_TO_ID[symbol_id]\n            # Enclose ARPAbet symbols in curly braces.\n            if len(symbol.split()) > 1:\n                symbol = \"{%s}\" % symbol\n        else:\n            symbol = \"@\" + str(symbol_id) + \"@\"\n        # Ensure that ARPAbet is separated from the symbols surrounding it with a space.\n        if len(result) > 0 and symbol[0] != \"{\" and result[-1] != \"}\":\n            result += \" \"\n        result += symbol\n    return result.replace(\"}{\", \" } { \").replace(\"@\", \"\").strip()\n", "CodeBleu_score": 45.13772621668012}, {"code": "def sequence_to_text(sequence)\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _arpabet:\n            result += _arpabet[symbol_id]\n        else:\n            result += _symbols[symbol_id]\n    return result.replace('}{', ' ')\n\n", "CodeBleu_score": 33.182275007770066}, {"code": "def sequence_to_text(sequence)\n    text = ''\n    for i in sequence:\n        if i in arpabet:\n            text += arpabet[i]\n        else:\n            text += '{' + arpabet[i] + '}'\n    text = re.sub(r'{(.+?)}', r'\\1', text)\n    return text\n", "CodeBleu_score": 26.147798024482764}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\n    # load the models\n    encoder, _ = load_models(encoder_model_fpath)\n\n    # load the wav file\n    wav_fpath = fpaths[0]\n    wav = load_wav(wav_fpath)\n\n    # the wav file should be <= ~4.5 seconds\n    if len(wav) / sampling_rate > 4.5:\n        wav = wav[:int(4.5 * sampling_rate)]\n\n    # the wav file should be >= ~1.5 seconds\n    if len(wav) / sampling_rate < 1.5:\n        wav = np.pad(wav, (0, int(1.5 * sampling_rate) - len(wav)), mode='constant', constant_values=0)\n\n    # extract speaker embedding\n    embed = encoder.embed_utterance(wav)\n\n    # save the embedding\n    np.save(fpaths[1], embed)\n\n", "CodeBleu_score": 43.14727372106601}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    # Load the encoder\n    print('Loading the encoder')\n    encoder = load_model(encoder_model_fpath)\n    # Compute the speaker embedding of the utterance\n    wav = load_wav_to_torch(fpaths[0]).unsqueeze(0)\n    wav = wav.cuda()\n    embed = encoder.forward_eval(wav)\n    # Save the embedding\n    fname = os.path.basename(fpaths[0])\n    speaker_id = fname.split('_')[0]\n    embed_fpath = speaker_id + '_embed.pt'\n    torch.save(embed, embed_fpath)\n    print('Saved the speaker embedding to %s' % embed_fpath)\n\n", "CodeBleu_score": 42.12902258946375}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\n    # Load the encoder\n    print('Loading encoder \"{}\"'.format(encoder_model_fpath))\n    encoder = load_encoder(encoder_model_fpath)\n\n    # Extract embedding\n    embed = encoder.embed_utterance(fpaths[0])\n\n    # Save the embedding\n    d_name = os.path.dirname(fpaths[0])\n    fname = os.path.basename(fpaths[0])\n    save_path = os.path.join(d_name, \"embeddings\", fname + \".npy\")\n    print(\"Saving embedding to {}\".format(save_path))\n    np.save(save_path, embed, allow_pickle=False)\n\n", "CodeBleu_score": 36.73944983344009}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n    # Check if the required directories and files exist\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(\"Encoder model file not found\")\n    if not synthesizer_root.exists():\n        raise NotADirectoryError(\"Synthesizer root directory not found\")\n    # Get the list of audio files\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(\"Metadata file not found\")\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        fpaths = [synthesizer_root.joinpath(m[0]) for m in metadata]\n    # Create the embedding directory if it doesn't exist\n    embedding_dir = synthesizer_root.joinpath(\"embeddings\")\n    embedding_dir.mkdir(exist_ok=True)\n    # Embed the audio files\n    embed_fpaths = [embedding_dir.joinpath(fpath.name.replace(\".wav\", \".npy\")) for fpath in fpaths]\n    embed_existing_fpaths = [fpath for fpath in embed_fpaths if fpath.exists()]\n    fpaths = [fpath for fpath in fpaths if fpath.name not in [f.name for f in embed_existing_fpaths]]\n    embed_fpaths = [fpath for fpath in embed_fpaths if fpath.name not in [f.name for f in embed_existing_fpaths]]\n    encoder.embed_utterance_dir(encoder_model_fpath, fpaths, embed_fpaths, n_processes)\n    # Save the metadata to the embedding directory\n    metadata_fpath = embedding_dir.joinpath(\"train.txt\")\n    with metadata_fpath.open(\"w\") as metadata_file:\n        for embedding_fpath, wav_fpath in zip(embed_fpaths, fpaths):\n            metadata_file.write(\"|\".join([str(wav_fpath), str(embedding_fpath)]) + \"\\n\")\n    return", "CodeBleu_score": 52.141006145646806}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n    # Check if encoder model is given\n    if not encoder_model_fpath.exists():\n        raise Exception(\"Encoder model not found\")\n\n    # Check if path to save embeddings is given\n    if not synthesizer_root.exists():\n        raise Exception(\"Synthesizer root directory not found\")\n\n    # Get the list of audio files\n    metadata_fpath = synthesizer_root / \"train.txt\"\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        audio_paths = [synthesizer_root / \"audio\" / m[0] for m in metadata]\n\n    # Create the embedding directory if it doesn't exist\n    embedding_dir = synthesizer_root / \"embeddings\"\n    embedding_dir.mkdir(exist_ok=True)\n\n    # Verify that all audio files exist\n    assert all(\n        [f.exists() for f in audio_paths]\n    ), \"Not all audio files exist. Make sure you are providing paths to correct audio files.\"\n\n    # Embed the existing audio files\n    embedding_fpaths = [embedding_dir / f.stem for f in audio_paths]\n    embedding_exists = [f.exists() for f in embedding_fpaths]\n    embeddings = [f.read_text().split(\"\\n\")[:-1] for f, e in zip(embedding_fpaths, embedding_exists) if e]\n\n    # We use multiprocessing to embed the audio files in parallel\n    # We embed files that do not exist in the embedding dir with a pool of workers\n    with Pool(n_processes) as p:\n        func = partial(embed_utterance, encoder_model_fpath=encoder_model_fpath)\n        embed = list(\n            tqdm(\n                p.imap(func, audio_paths, embedding_fpaths),\n                total=len(audio_paths),\n                desc=\"Embedding\",\n            )\n        )\n\n    # Store the embeddings, one per line\n    for i in range(len(embedding_fpaths)):\n        embedding", "CodeBleu_score": 51.332517686469274}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\n    # Check if the synthesizer root exists\n    if not synthesizer_root.exists():\n        raise Exception(\n            f\"The synthesizer root directory '{synthesizer_root}' does not exist.\")\n\n    # Check if the encoder model exists\n    if not encoder_model_fpath.exists():\n        raise Exception(\n            f\"The encoder model file '{encoder_model_fpath}' does not exist.\")\n\n    # Get the metadata filepath\n    metadata_fpath = synthesizer_root / \"train.txt\"\n\n    # Check if the metadata file exists\n    if not metadata_fpath.exists():\n        raise Exception(\n            f\"The metadata file '{metadata_fpath}' does not exist.\")\n\n    # Get the output embedding filepath\n    embed_fpath = synthesizer_root / \"mels\"\n\n    # Create the output embedding directory\n    embed_fpath.mkdir(exist_ok=True, parents=True)\n\n    # Get the basename of the embedding file\n    embed_fname = encoder_model_fpath.stem\n\n    # Embedding configuration\n    embed_config = AttrDict({\n        \"seed\": 42,\n        \"trim_silence\": True,\n        \"ref_mels_dir\": synthesizer_root / \"mels\",\n        \"mel_ext\": \".npy\",\n        \"embed_ext\": \".pt\",\n        \"num_workers\": n_processes,\n        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    })\n\n    # Create the embeddings\n    create_embeddings_from_metadata(\n        metadata_fpath, embed_config, embed_fpath, embed_fname, encoder_model_fpath)\n\n", "CodeBleu_score": 30.7467025737467}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    # Save attention plot\n    attention_plot = attention.cpu().detach().numpy()\n    attention_plot = np.transpose(attention_plot, (0, 2, 1))\n    plot_attention(attention_plot, sample_num, step, plot_dir)\n\n    # Save predicted mel spectrogram\n    mel_prediction = mel_prediction.cpu().detach().numpy()\n    mel_prediction = np.transpose(mel_prediction, (0, 2, 1))\n    plot_mel_spectrogram(mel_prediction, sample_num, step, mel_output_dir)\n\n    # Reconstruct waveform from mel spectrogram\n    mel_prediction = torch.from_numpy(mel_prediction).to(hparams.device)\n    wav_prediction = mel_to_wave(mel_prediction)\n\n    # Save reconstructed waveform\n    wav_prediction = wav_prediction.cpu().detach().numpy()\n    wav_prediction = np.transpose(wav_prediction, (0, 2, 1))\n    plot_waveform(wav_prediction, sample_num, step, wav_dir)\n\n    # Generate and save plot comparing predicted and target mel spectrograms\n    mel_prediction = mel_prediction.cpu().detach().numpy()\n    mel_prediction = np.transpose(mel_prediction, (0, 2, 1))\n    mel_prediction = mel_prediction[0]\n    target_spectrogram = target_spectrogram.cpu().detach().numpy()\n    target_spectrogram = np.transpose(target_spectrogram, (0, 2, 1))\n    target_spectrogram = target_spectrogram[0]\n    plot_mel_spectrogram_comparison(mel_prediction, target_spectrogram, sample_num, step, loss, plot_dir)\n\n    # Print input sequence\n    print(f\"Input sequence at step {step}:\")\n    print(input_seq)\n\n", "CodeBleu_score": 27.061321108777847}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n\n    # Save attention plot\n    attention_plot = attention.detach().cpu().numpy()\n    attention_plot = np.squeeze(attention_plot, axis=0)\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_plot = attention_plot.transpose()\n    attention_plot = attention_plot[:, :, :hparams.num_mels]\n    attention_", "CodeBleu_score": 15.005424652588026}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    # Save attention plot\n    attention_plot_path = os.path.join(plot_dir, 'attention_plot_step_{}.png'.format(step))\n    plot_attention(attention, attention_plot_path)\n\n    # Save predicted mel spectrogram\n    mel_prediction_path = os.path.join(mel_output_dir, 'mel_prediction_step_{}.npy'.format(step))\n    np.save(mel_prediction_path, mel_prediction.cpu().numpy())\n\n    # Reconstruct waveform from predicted mel spectrogram\n    waveform = mel_to_waveform(mel_prediction, hparams)\n\n    # Save reconstructed waveform\n    wav_path = os.path.join(wav_dir, 'waveform_step_{}.wav'.format(step))\n    save_wav(waveform, wav_path, hparams)\n\n    # Generate and save plot comparing predicted and target mel spectrograms\n    plot_title = 'Model Type: {}, Step: {}, Loss: {:.4f}'.format(hparams.model_type, step, loss.item())\n    plot_mel_spectrogram(target_spectrogram, mel_prediction, plot_title, plot_dir, sample_num)\n\n    # Print input sequence\n    print('Input Sequence:')\n    print(input_seq)", "CodeBleu_score": 37.642057889383445}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n    total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"{total_params/1000000:.3f}M parameters\")", "CodeBleu_score": 12.495762412061866}, {"code": "def num_params(self)\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f'The model has {num_params/1e6:.3f} million trainable parameters')", "CodeBleu_score": 14.714815696159544}, {"code": "def num_params(self)\n    num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    num_params_m = num_params / 1e6\n    print(f\"Number of trainable parameters: {num_params_m:.3f}M\")\n\n", "CodeBleu_score": 16.671026044026465}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n    if side == 'both':\n        return torch.cat((pad, x, pad), dim=0)\n    elif side == 'before':\n        return torch.cat((pad, x), dim=0)\n    elif side == 'after':\n        return torch.cat((x, pad), dim=0)\n    else:\n        raise ValueError('side must be one of \"before\", \"after\", or \"both\"')\n", "CodeBleu_score": 22.98846062812776}, {"code": "def pad_tensor(self, x, pad, side='both')\n    if side == 'both':\n        new_size = x.size(1) + pad\n        new_x = x.new_zeros(x.size(0), new_size, x.size(2), x.size(3))\n        new_x[:, pad//2:pad//2+x.size(1), :, :] = x\n        return new_x\n    elif side == 'before':\n        new_size = x.size(1) + pad\n        new_x = x.new_zeros(x.size(0), new_size, x.size(2), x.size(3))\n        new_x[:, pad:, :, :] = x\n        return new_x\n    elif side == 'after':\n        new_size = x.size(1) + pad\n        new_x = x.new_zeros(x.size(0), new_size, x.size(2), x.size(3))\n        new_x[:, :-pad, :, :] = x\n        return new_x\n    else:\n        raise ValueError('Invalid side. Must be \"before\", \"after\", or \"both\".')", "CodeBleu_score": 30.165987531995576}, {"code": "def pad_tensor(self, x, pad, side='both')\n    if side == 'both':\n        return torch.cat((torch.zeros(x.size(0), pad, x.size(2), x.size(3), device=x.device), x, torch.zeros(x.size(0), pad, x.size(2), x.size(3), device=x.device)), dim=1)\n    elif side == 'before':\n        return torch.cat((torch.zeros(x.size(0), pad, x.size(2), x.size(3), device=x.device), x), dim=1)\n    elif side == 'after':\n        return torch.cat((x, torch.zeros(x.size(0), pad, x.size(2), x.size(3), device=x.device)), dim=1)\n    else:\n        raise ValueError('side must be either \"before\", \"after\", or \"both\"')\n", "CodeBleu_score": 21.85738836901879}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.imshow(attn, cmap='jet', alpha=0.5)\n    ax.set_xlabel('Attention')\n    ax.set_ylabel('Attention')\n    ax.set_title('Attention')\n    plt.savefig(path + '.png')\n    plt.close(fig)\n\n", "CodeBleu_score": 49.258157512278565}, {"code": "def save_attention(attn, path)\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    plt.imshow(attn, cmap='Blues', interpolation='nearest')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.colorbar()\n    plt.tight_layout()\n    plt.savefig(path, dpi=300)\n    plt.close(fig)\n\n", "CodeBleu_score": 50.63669141181484}, {"code": "def save_attention(attn, path)\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.imshow(attn, cmap='bone')\n    ax.set_xlabel('Head number')\n    ax.set_ylabel('Layer number')\n    ax.set_title('Attention Heatmap')\n    fig.savefig(path)\n    plt.close(fig)", "CodeBleu_score": 50.88397759876801}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('x', color='#777777', fontsize=18)\n    ax.set_ylabel('y', color='#777777', fontsize=18)\n    ax.tick_params(axis='x', colors='#777777', labelsize=16)\n    ax.tick_params(axis='y', colors='#777777', labelsize=16)\n    ax.plot(array, color='#000000', linewidth=2)\n    plt.show()\n\n", "CodeBleu_score": 52.147844059069456}, {"code": "def plot(array)\n    import matplotlib.pyplot as plt\n    import matplotlib.colors as mcolors\n    import numpy as np\n    from matplotlib.ticker import MultipleLocator\n\n    fig, ax = plt.subplots(figsize=(12, 10))\n    ax.set_xlabel(\"x\", color=\"black\", fontsize=20)\n    ax.set_ylabel(\"y\", color=\"black\", fontsize=20)\n    ax.tick_params(axis=\"x\", colors=\"black\", labelsize=15)\n    ax.tick_params(axis=\"y\", colors=\"black\", labelsize=15)\n    ax.xaxis.set_major_locator(MultipleLocator(1))\n    ax.yaxis.set_major_locator(MultipleLocator(1))\n    ax.set_title(\"Solution\", fontsize=20)\n    cmap = mcolors.ListedColormap([\"white\", \"black\"])\n    ax.imshow(array, cmap=cmap)\n    plt.show()\n\n", "CodeBleu_score": 45.940036961940386}, {"code": "def plot(array)\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(\"X\", fontsize=15, color=\"red\")\n    ax.set_ylabel(\"Y\", fontsize=15, color=\"red\")\n    ax.tick_params(axis=\"x\", colors=\"red\", labelsize=15)\n    ax.tick_params(axis=\"y\", colors=\"red\", labelsize=15)\n    plt.plot(array)\n    plt.show()", "CodeBleu_score": 36.056312466045405}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n    import csv\n    import os\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        # Create a CSV writer object\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        # Write the header if the file is new\n        if os.stat('predictions.csv').st_size == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n\n    print(\"Prediction data written to CSV file.\")\n\n", "CodeBleu_score": 46.71876959401599}, {"code": "def write_to_csv(image_name, prediction, confidence)\n    import csv\n    import os\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the CSV file in append mode\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        # Create a CSV writer object\n        fieldnames = ['image_name', 'prediction', 'confidence']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        # Write the header if the file is new\n        if os.stat('predictions.csv').st_size == 0:\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n", "CodeBleu_score": 47.915021294632375}, {"code": "def write_to_csv(image_name, prediction, confidence)\n    import csv\n    import os\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\"image_name\": image_name, \"prediction\": prediction, \"confidence\": confidence}\n\n    # Open the CSV file in append mode\n    with open(\"predictions.csv\", \"a\", newline=\"\") as csvfile:\n        # Create a CSV writer object\n        writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n\n        # Write the header if the file is new\n        if not os.path.exists(\"predictions.csv\"):\n            writer.writeheader()\n\n        # Write the data as a new row\n        writer.writerow(data)\n", "CodeBleu_score": 56.35908445988483}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    # Export torchscript\n    print(f\"{prefix} Exporting {file}\")\n    model.model[-1].export = True  # set Detect() layer export=True\n    model = model.fuse()  # model.fuse() converts any nn.Sequential() layers to nn.ModuleList()\n    y = model(im)  # dry run\n    torch.onnx.export(\n        model,\n        im,\n        file,\n        verbose=False,\n        opset_version=11,\n        input_names=[\"images\"],\n        output_names=[\"classes\", \"boxes\"] if y is None else [\"output0\", \"output1\", \"output2\"],\n    )  # export torchscript\n    model.model[-1].export = False  # set Detect() layer export=False\n    if optimize:\n        try:\n            from torch.utils.mobile_optimizer import optimize_for_mobile\n\n            print(f\"{prefix} Optimizing for mobile...\")\n            model_optimized = optimize_for_mobile(torch.jit.load(file))\n            model_optimized._script_module.save(file)\n        except Exception as e:\n            print(f\"{prefix} Optimization failed: {e}\")\n    else:\n        print(f\"{prefix} Export complete ({Path(file).stat().st_size / 1E6:.1f} MB)\")\n    return file\n\n", "CodeBleu_score": 31.40848497606359}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    # Export torchscript and torchscript.pt to CoreML format (deployment only)\n    print(f\"{prefix} Exporting {file}\")\n    model.eval()\n    model.model[-1].export = False  # set Detect() layer export=False\n    y = model(im)  # dry run\n    torch.onnx.export(model, im, file, verbose=False, opset_version=11)  # export torchscript\n    if optimize:\n        try:\n            import onnx\n            from onnxsim import simplify\n\n            model_onnx = onnx.load(file)\n            model_onnx, check = simplify(model_onnx)\n            assert check, \"Simplified ONNX model could not be validated\"\n            onnx.save(model_onnx, file)\n            print(f\"{prefix} Optimized {file}\")\n            return file\n        except Exception as e:\n            print(f\"{prefix} {e}\")\n    return file", "CodeBleu_score": 33.08272205725626}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    # Export torchscript and torchscript (JIT) code to $dir/model.torchscript.pt\n    # (Idea from https://github.com/rwightman/pytorch-image-models/pull/490)\n    try:\n        import torch\n        from torch import jit\n        from torch.utils.mobile_optimizer import optimize_for_mobile\n        from yolov5.utils.general import LOGGER, colorstr\n        from yolov5.utils.torch_utils import fuse_model\n\n        LOGGER.info(f\"{prefix} Exporting torchscript and torchscript (JIT) code\")\n        model = fuse_model(model)  # fuse model to reduce file size\n        im = im.to(next(model.parameters()).device)\n        traced_script_module = jit.trace(model, [im])\n        if optimize:\n            LOGGER.info(f\"{prefix} Optimizing for mobile...\")\n            traced_script_module = optimize_for_mobile(traced_script_module)\n        torch.jit.save(traced_script_module, file)\n        LOGGER.info(f\"{prefix} TorchScript export success, saved as {file}\")\n        return file\n    except Exception as e:\n        LOGGER.info(f\"{prefix} TorchScript export failure: {e}\")\n        return None", "CodeBleu_score": 37.938190988176814}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    # Check coremltools requirement\n    import coremltools\n\n    # Log export start\n    print(f\"{prefix} Exporting {file}\")\n\n    # Set file suffix to .mlmodel\n    file = Path(file).with_suffix(\".mlmodel\")\n\n    # Wrap model with iOSModel for NMS support\n    if nms:\n        model = model.export()  # torch.jit.trace(model, im)  # model.fuse().eval()\n        model = model.model  # model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model.model  # model = model.model\n        model = model.model  # model = model", "CodeBleu_score": 19.667160685972686}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    import coremltools as ct\n    from torch import nn\n    from torch.utils.mobile_optimizer import optimize_for_mobile\n    from yolov5.utils.coreml import iOSModel\n    from yolov5.utils.general import LOGGER, colorstr\n    from yolov5.utils.torch_utils import fuse_conv_and_bn, traced_model\n\n    # Check coremltools requirement\n    try:\n        import coremltools\n    except ImportError:\n        LOGGER.info(f\"{prefix} Please install coremltools, see https://github.com/apple/coremltools\")\n        return\n\n    # Log start\n    LOGGER.info(f\"{prefix} Exporting {file}\")\n\n    # Set file suffix to .mlmodel\n    file = file.replace(\".pt\", \".mlmodel\")\n\n    # Wrap model with iOSModel for NMS support\n    if nms:\n        model = iOSModel(model)\n\n    # Trace model\n    model = traced_model(model, im, fuse=fuse_conv_and_bn)\n\n    # Convert to CoreML\n    try:\n        model = ct.convert(model, inputs=[ct.ImageType(name=\"image\", shape=im.shape, scale=1 / 255)])\n    except Exception as e:\n        LOGGER.info(f\"{prefix} Error converting model to CoreML: {e}\")\n        return\n\n    # Apply quantization if needed and supported\n    if int8 or half:\n        model = ct.utils.convert_neural_network_float32_to_float16(model)\n\n    # Optimize model for mobile\n    model = optimize_for_mobile(model)\n\n    # Save model\n    model.save(file)\n\n    # Return file path and model\n    return file, model", "CodeBleu_score": 40.025666213227574}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    assert model.training == False, \"Exporting to CoreML requires the model to be in eval mode (model.train(False)).\"\n    assert im.shape[1] == 3 and im.shape[2] > 16 and im.shape[3] > 16, \"Image size must be at least 16x16.\"\n    assert im.shape[1] == im.shape[3], \"Image must be square.\"\n    assert im.shape[1] % 8 == 0, \"Image size must be divisible by 8.\"\n    assert im.shape[1] <= 224, \"Image size must be at most 224x224.\"\n    assert im.shape[1] / 8 <= 16, \"Image size must be at most 16x16.\"\n    assert im.shape[1] / 8 <= 32, \"Image size must be at most 32x32.\"\n    assert im.shape[1] / 8 <= 64, \"Image size must be at most 64x64.\"\n    assert im.shape[1] / 8 <= 128, \"Image size must be at most 128x128.\"\n    assert im.shape[1] / 8 <= 256, \"Image size must be at most 256x256.\"\n    assert im.shape[1] / 8 <= 512, \"Image size must be at most 512x512.\"\n    assert im.shape[1] / 8 <= 1024, \"Image size must be at most 1024x1024.\"\n    assert im.shape[1] / 8 <= 2048, \"Image size must be at most 2048x2048.\"\n    assert im.shape[1] / 8 <= 4096, \"Image size must be at most 4096x4096.\"\n    assert im.shape[1] / 8 <= 8192, \"Image size must be at most 8192x8192.\"\n    assert im.shape[1] / 8 <= 16384, \"Image size must", "CodeBleu_score": 13.02030371798047}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n    import random\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n", "CodeBleu_score": 46.16791810180864}, {"code": "def generate_individual(input_ranges, individual_length)\n    import random\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n", "CodeBleu_score": 48.609071015262195}, {"code": "def generate_individual(input_ranges, individual_length)\n    import random\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n", "CodeBleu_score": 48.609071015262195}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule == 'discrete':\n        return self.log_alpha_0\n    elif self.schedule == 'linear':\n        return self.log_alpha_0 + (t - self.T_max) / self.T_max * (self.log_alpha_1 - self.log_alpha_0)\n    elif self.schedule == 'cosine':\n        return self.log_alpha_0 + (1 + np.cos(t / self.T_max * np.pi)) / 2 * (self.log_alpha_1 - self.log_alpha_0)\n    else:\n        raise ValueError('Schedule type not recognized.')\n\n", "CodeBleu_score": 38.70198006236395}, {"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == 'discrete':\n        return self.log_mean_coeffs[t]\n    elif self.schedule_type == 'linear':\n        return self.log_mean_coeffs[t] - self.log_mean_coeffs[t + 1]\n    elif self.schedule_type == 'cosine':\n        return self.log_mean_coeffs[t] + self.log_mean_coeffs[0] - self.log_mean_coeffs[t + 1]\n    else:\n        raise ValueError('Unknown schedule type: {}'.format(self.schedule_type))", "CodeBleu_score": 29.149536387775665}, {"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == 'discrete':\n        return self.log_alpha_0\n    elif self.schedule_type == 'linear':\n        return self.log_alpha_0 + (t / self.num_steps) * (self.log_alpha_1 - self.log_alpha_0)\n    elif self.schedule_type == 'cosine':\n        return self.log_alpha_0 + (1 + np.cos(t * np.pi / self.num_steps)) / 2 * (self.log_alpha_1 - self.log_alpha_0)\n    else:\n        raise ValueError('Invalid schedule type.')\n", "CodeBleu_score": 34.83908671271042}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n    if self.schedule == 'linear':\n        beta = lamb / self.beta_max\n        t = self.T * (1 - beta)\n    elif self.schedule == 'discrete':\n        t = np.interp(lamb, self.log_alpha, self.t)\n    else:\n        log_alpha = np.arccos(1 - 2 * lamb / self.beta_max)\n        t = self.T * (1 - log_alpha / np.pi)\n    return t\n", "CodeBleu_score": 17.15322099204721}, {"code": "def inverse_lambda(self, lamb)\n    if self.schedule == 'linear':\n        # Calculate the inverse of the lambda function for a linear schedule\n        beta = self.beta\n        temp = (lamb - beta[0]) / (beta[1] - beta[0])\n        t = temp / (1 - temp)\n    elif self.schedule == 'discrete':\n        # Calculate the inverse of the lambda function for a discrete schedule\n        alpha = self.alpha\n        t = np.interp(lamb, alpha, np.linspace(0, 1, len(alpha)))\n    else:\n        # Calculate the inverse of the lambda function for other schedules\n        alpha = self.alpha\n        log_alpha = np.log(alpha)\n        t = np.arccos(1 - 2 * lamb) / np.pi\n        t = np.interp(t, log_alpha, np.linspace(0, 1, len(alpha)))\n    return t", "CodeBleu_score": 24.695373574607295}, {"code": "def inverse_lambda(self, lamb)\n    if self.schedule == \"linear\":\n        # Linear schedule\n        if lamb > 0:\n            t = self.T * (1 - np.exp(-lamb * self.beta))\n        else:\n            t = 0\n    elif self.schedule == \"discrete\":\n        # Discrete schedule\n        t = np.interp(lamb, self.log_alpha, self.t)\n    else:\n        # Other schedules\n        log_alpha = np.arccos(1 - 2 * lamb)\n        t = np.interp(log_alpha, self.log_alpha, self.t)\n    return t\n", "CodeBleu_score": 18.53925261518799}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    if cond is None:\n        cond = []\n    batch_size = t_continuous.shape[0]\n    t_exp = t_continuous.expand(batch_size, *([-1] * (len(x.ndim) - 1)))\n    model_output = self.model(x, t_exp, cond)\n    if self.model_type == 'ddpm':\n        return self.apply_ddpm_noise_pred_fn(model_output, t_continuous)\n    elif self.model_type == 'ddim':\n        return self.apply_ddim_noise_pred_fn(model_output, t_continuous)\n    elif self.model_type == 'ddpm-v2':\n        return self.apply_ddpm_v2_noise_pred_fn(model_output, t_continuous)\n    else:\n        raise ValueError(f'Unknown model type: {self.model_type}')", "CodeBleu_score": 22.77933898607275}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    t_discrete = torch.floor(t_continuous)\n    t_discrete = t_discrete.type(torch.int64)\n    t_discrete = t_discrete.unsqueeze(1)\n    t_discrete = t_discrete.expand(x.shape[0], 1)\n    t_discrete = t_discrete.to(x.device)\n\n    if model_type == 'DDPM':\n        alpha_t = noise_schedule.get_alpha_t(t_discrete)\n        alpha_t = alpha_t.unsqueeze(1)\n        alpha_t = alpha_t.expand(x.shape[0], 1)\n        alpha_t = alpha_t.to(x.device)\n\n        if cond is not None:\n            cond = cond.to(x.device)\n            cond = cond.unsqueeze(1)\n            cond = cond.expand(x.shape[0], 1)\n\n        noise_pred = model(x, t_discrete, cond)\n        noise_pred = noise_pred * (1 - alpha_t) ** 0.5\n\n    elif model_type == 'DDIM':\n        sigma_t = noise_schedule.get_sigma_t(t_discrete)\n        sigma_t = sigma_t.unsqueeze(1)\n        sigma_t = sigma_t.expand(x.shape[0], 1)\n        sigma_t = sigma_t.to(x.device)\n\n        if cond is not None:\n            cond = cond.to(x.device)\n            cond = cond.unsqueeze(1)\n            cond = cond.expand(x.shape[0], 1)\n\n        noise_pred = model(x, t_discrete, cond)\n        noise_pred = noise_pred * sigma_t\n\n    elif model_type == 'DDPM_CLIP':\n        alpha_t = noise_schedule.get_alpha_t(t_discrete)\n        alpha_t = alpha_t.unsqueeze(1)\n        alpha", "CodeBleu_score": 32.55586267554864}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    t = t_continuous.view(x.shape[0], 1)\n    t = t.expand(x.shape[0], t.shape[1])\n    if cond is not None:\n        x = torch.cat((x, cond), dim=1)\n    noise_pred = model(x, t)\n    if model_type == \"DDPM\":\n        return noise_pred\n    elif model_type == \"DDIM\":\n        alpha_t = torch.gather(alphas_cumprod, 0, t)\n        alpha_t_prev = torch.gather(alphas_cumprod, 0, t - 1)\n        sigma_t = torch.sqrt((1 - alpha_t_prev) / (1 - alpha_t))\n        sigma_t_prev = torch.sqrt((1 - alpha_t) / (1 - alpha_t_prev))\n        noise_pred = sigma_t * noise_pred + sigma_t_prev * noise_pred_prev\n        return noise_pred\n    elif model_type == \"DPM2\":\n        alpha_t = torch.gather(alphas_cumprod, 0, t)\n        sigma_t = torch.sqrt(1 - alpha_t)\n        sigma_t_prev = torch.sqrt(1 - torch.gather(alphas_cumprod, 0, t - 1))\n        noise_pred = sigma_t * noise_pred + sigma_t_prev * noise_pred_prev\n        return noise_pred\n    elif model_type == \"DPM2+\":\n        alpha_t = torch.gather(alphas_cumprod, 0, t)\n        sigma_t = torch.sqrt(1 - alpha_t)\n        sigma_t_prev = torch.sqrt(1 - torch.gather(alphas_cumprod, 0, t - 1))\n        noise_pred = sigma_t * noise_pred + sigma_t_prev * noise_pred_prev\n        return noise_pred\n    elif model_type == \"DPM++\":\n        alpha_t = torch", "CodeBleu_score": 30.87816758584957}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    x0 = x0.reshape(-1)\n    q = np.quantile(np.abs(x0), self.q)\n    scale = np.max(np.abs(x0)) / q\n    x0 = np.clip(x0, -scale, scale)\n    x0 = x0.reshape(self.shape)\n    return x0", "CodeBleu_score": 13.73512348366434}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    return self.dynamic_thresholding_fn(x0, t)\n\n", "CodeBleu_score": 2.9674036720612107}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    if t is None:\n        t = self.t\n    x = x0.reshape(-1)\n    q = np.quantile(np.abs(x), t)\n    scale = (q / 0.75) * 0.25\n    x = np.clip(x, -scale, scale)\n    x = x / scale\n    return x.reshape(x0.shape)", "CodeBleu_score": 29.777557537141963}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n    t = t.to(self.device)\n    x0 = x.to(self.device)\n    threshold = self.dynamic_thresholding_ratio * torch.max(x0)\n    x0 = torch.clamp(x0, -threshold, threshold)\n    x0 = x0 / (threshold + self.eps)\n    return x0", "CodeBleu_score": 14.780048458132528}, {"code": "def data_prediction_fn(self, x, t)\n    x0 = x\n    x0 = tf.clip_by_value(x0, -self.max_threshold, self.max_threshold)\n    x0 = tf.sign(x0) * tf.minimum(tf.abs(x0), self.threshold)\n    x0 = x0 / self.max_threshold\n    return x0\n\n", "CodeBleu_score": 5.209794115156482}, {"code": "def data_prediction_fn(self, x, t)\n    x0 = x[0]\n    x1 = x[1]\n    x2 = x[2]\n    x3 = x[3]\n    x4 = x[4]\n    x5 = x[5]\n    x6 = x[6]\n    x7 = x[7]\n    x8 = x[8]\n    x9 = x[9]\n    x10 = x[10]\n    x11 = x[11]\n    x12 = x[12]\n    x13 = x[13]\n    x14 = x[14]\n    x15 = x[15]\n    x16 = x[16]\n    x17 = x[17]\n    x18 = x[18]\n    x19 = x[19]\n    x20 = x[20]\n    x21 = x[21]\n    x22 = x[22]\n    x23 = x[23]\n    x24 = x[24]\n    x25 = x[25]\n    x26 = x[26]\n    x27 = x[27]\n    x28 = x[28]\n    x29 = x[29]\n    x30 = x[30]\n    x31 = x[31]\n    x32 = x[32]\n    x33 = x[33]\n    x34 = x[34]\n    x35 = x[35]\n    x36 = x[36]\n    x37 = x[37]\n    x38 = x[38]\n    x39 = x[39]\n    x40 = x[40]\n    x41 = x[41]\n    x42 = x[42]\n    x43 = x[43]\n    x44 = x[44]\n    x45 = x[45]\n    x46 = x[46]\n    x47 = x[47]\n    x4", "CodeBleu_score": 6.372024227251734}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    sigma_in = sigma_in.reshape(sigma_in.shape[0], 1, 1, 1)\n    scaled_input = input * sigma_in\n    prediction = model(scaled_input, **kwargs)\n    return prediction * sigma_in\n\n", "CodeBleu_score": 25.703284529423577}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    sigma_in = sigma_in.reshape(sigma_in.shape[0], sigma_in.shape[1], 1, 1)\n    input = input * sigma_in\n    output = model(input, **kwargs)\n    output = output * sigma_in\n    return output", "CodeBleu_score": 29.303445611516153}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    sigma_in = sigma_in.reshape(input.shape)\n    input_scaled = input * sigma_in\n    output = model(input_scaled, **kwargs)\n    return output * sigma_in\n\n", "CodeBleu_score": 24.448937066689428}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n    alpha = taus + beta_0\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(retain_graph=True)\n    grad_log_alpha = taus.grad\n    taus.grad = None\n    integrand = grad_log_alpha / alpha\n    return integrand\n", "CodeBleu_score": 18.193579069130735}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n        tape.watch(taus)\n        alpha = tf.exp(beta_0 + beta_1 * taus)\n        log_alpha = tf.math.log(alpha)\n    log_alpha_grad = tape.gradient(log_alpha, taus)\n    integrand = log_alpha_grad * alpha\n    return integrand\n\n", "CodeBleu_score": 24.976374661417957}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n    alpha = taus * beta_1\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(retain_graph=True)\n    grad_log_alpha = taus.grad\n    taus.grad = None\n    integrand = grad_log_alpha * alpha + beta_0\n    return integrand\n\n", "CodeBleu_score": 19.210379051337238}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda: torch.randn_like(x)\n    if disable is None:\n        disable = False\n    if s_noise == 0.:\n        s_noise = 1.\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5\n    if s_noise == 1.:\n        s_noise = 1. - 1e-5", "CodeBleu_score": 16.022449643275255}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda x: torch.randn_like(x)\n    x_in = x\n    for sigma in sigmas:\n        x_in = model(x_in, sigma, **extra_args)\n        if callback:\n            callback({'x_in': x_in, 'sigma': sigma})\n        if sigma > 0:\n            noise = noise_sampler(x_in)\n            x_in = x_in + sigma * eta * noise\n            x_in = x_in + sigma * s_noise * noise_sampler(x_in)\n    return x_in\n", "CodeBleu_score": 25.13171154933603}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda: torch.randn_like(x)\n    x_t = x\n    for i, sigma in enumerate(sigmas):\n        if callback:\n            callback({'x_t': x_t, **extra_args})\n        if i > 0:\n            x_t = x_t - eta * model(x_t, sigma, **extra_args)\n        if sigma > 0:\n            noise = s_noise * sigma * noise_sampler()\n            x_t = x_t + noise\n    return x_t\n\n", "CodeBleu_score": 24.885189448251825}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n    def product_term(t, i, j):\n        if j == i:\n            return 1\n        elif j == i-1:\n            return t[i] - t[j]\n        else:\n            return (t[i] - t[j]) / (t[i] - t[j+1])\n\n    if order > i:\n        raise ValueError(\"Order is too high for the step index.\")\n\n    result = 0\n    for k in range(i-order+1, i+1):\n        result += product_term(t, i, k)\n\n    return result\n", "CodeBleu_score": 33.4756134994686}, {"code": "def linear_multistep_coeff(order, t, i, j)\n    def product(i, j):\n        if j == i:\n            return 1\n        else:\n            return (t[i] - t[j]) / (t[i + 1] - t[j])\n    if order > i:\n        raise ValueError(\"Order too high for step index\")\n    return sum([product(i, j) for j in range(i - order + 1, i + 1)])\n", "CodeBleu_score": 28.899304942694233}, {"code": "def linear_multistep_coeff(order, t, i, j)\n    if order > i:\n        raise ValueError(\"Order is too high for the step index.\")\n    def product_term(k, i, j):\n        if k == j:\n            return 1\n        elif k < j:\n            return (t[i] - t[k]) / (t[i+1] - t[k])\n        else:\n            return (t[i+1] - t[k]) / (t[i+1] - t[k-1])\n    return sum(product_term(k, i, j) for k in range(order+1))\n", "CodeBleu_score": 28.28332378638223}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = model.get_noise_sampler()\n    x = x.clone()\n    for i, sigma in enumerate(sigmas):\n        if disable is not None:\n            disable(f\"LCM sampling {i+1}/{len(sigmas)}\")\n        if callback is not None:\n            callback(x, i)\n        if sigma > 0:\n            noise = noise_sampler(x)\n            x = model(x, sigma, extra_args) + sigma * noise\n    return x", "CodeBleu_score": 29.47072148406507}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = model.get_noise_sampler()\n    if disable is None:\n        disable = False\n    if disable:\n        pbar = None\n    else:\n        pbar = tqdm(total=len(sigmas))\n    for i, sigma in enumerate(sigmas):\n        if sigma > 0:\n            x = model.apply_noise(x, t=torch.tensor([i]), sigma=sigma, noise_sampler=noise_sampler, extra_args=extra_args)\n        if callback is not None:\n            callback(x, i)\n        if pbar is not None:\n            pbar.update()\n    if pbar is not None:\n        pbar.close()\n    return x", "CodeBleu_score": 35.25593409902494}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = model.make_noise_sampler(model.timesteps)\n    x_t = x\n    for i, sigma in enumerate(sigmas):\n        if callback is not None and callback.on_sample(x_t, i, len(sigmas)):\n            return None\n        if sigma > 0:\n            x_t = model.apply_model(x_t, sigma, **extra_args)\n            x_t = model.denoise(x_t, sigma, **extra_args)\n            x_t = noise_sampler(x_t, sigma)\n    return x_t", "CodeBleu_score": 29.991469670192462}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n    # Calculate the number of dimensions to append\n    num_dims_to_append = target_dims - x.ndim\n\n    # Check if the target dimensions are fewer than the current dimensions\n    if num_dims_to_append < 0:\n        raise ValueError(\n            f\"Target dimensions ({target_dims}) must be greater than or equal to the current dimensions ({x.ndim})\"\n        )\n\n    # Append new dimensions to the tensor\n    for _ in range(num_dims_to_append):\n        x = x.unsqueeze(-1)\n\n    # Detach and clone the tensor if it is on an 'mps' device\n    if x.device.type == \"mps\":\n        x = x.detach().clone()\n\n    return x", "CodeBleu_score": 41.58949293086614}, {"code": "def append_dims(x, target_dims)\n    if x.device.type == 'mps':\n        return x.expand(*target_dims).detach().clone()\n    else:\n        return x.expand(*target_dims)", "CodeBleu_score": 14.458674736245102}, {"code": "def append_dims(x, target_dims)\n    current_dims = len(x.shape)\n    if target_dims <= current_dims:\n        raise ValueError(\"Target dimensions must be greater than current dimensions.\")\n    num_dims_to_append = target_dims - current_dims\n    for _ in range(num_dims_to_append):\n        x = x.unsqueeze(-1)\n    if x.device.type == \"mps\":\n        return x.detach()\n    return x", "CodeBleu_score": 32.23199575202849}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n    import os\n    import requests\n    import hashlib\n    from tqdm import tqdm\n\n    # Ensure the target directory exists\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n    # Check if the file already exists\n    if os.path.isfile(path):\n        # Calculate the SHA-256 hash of the existing file\n        existing_hash = hashlib.sha256(open(path, 'rb').read()).hexdigest()\n        if digest is not None and existing_hash != digest:\n            # The existing file's hash does not match the expected hash\n            raise ValueError(f\"Existing file '{path}' has hash '{existing_hash}' but expected '{digest}'\")\n        return path\n\n    # Download the file\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get('content-length', 0))\n\n    with open(path, 'wb') as file, tqdm(\n            desc=os.path.basename(path),\n            total=total_size,\n            unit='iB',\n            unit_scale=True,\n            unit_divisor=1024,\n    ) as bar:\n        for data in response.iter_content(chunk_size=1024):\n            size = file.write(data)\n            bar.update(size)\n\n    # Verify the file's hash\n    if digest is not None:\n        downloaded_hash = hashlib.sha256(open(path, 'rb').read()).hexdigest()\n        if downloaded_hash != digest:\n            raise ValueError(f\"Downloaded file '{path}' has hash '{downloaded_hash}' but expected '{digest}'\")\n\n    return path", "CodeBleu_score": 49.52853093705707}, {"code": "def download_file(path, url, digest=None)\n    import os\n    import hashlib\n    import requests\n    from tqdm import tqdm\n\n    # Ensure the target directory exists\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n    # Download the file if necessary\n    if not os.path.exists(path):\n        response = requests.get(url, stream=True)\n        total_size = int(response.headers.get('content-length', 0))\n        block_size = 1024\n        with open(path, 'wb') as file:\n            for data in tqdm(response.iter_content(block_size), total=total_size, unit='B', unit_scale=True):\n                file.write(data)\n\n    # Verify the file's hash if provided\n    if digest is not None:\n        with open(path, 'rb') as file:\n            file_hash = hashlib.sha256(file.read()).hexdigest()\n            if file_hash != digest:\n                raise ValueError(f'File hash does not match expected value: {file_hash}')\n\n    return path\n\n", "CodeBleu_score": 46.845844284698046}, {"code": "def download_file(path, url, digest=None)\n    import os\n    import hashlib\n    import requests\n    from tqdm import tqdm\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    file_path = os.path.join(path, os.path.basename(url))\n\n    if os.path.exists(file_path):\n        if digest is not None:\n            with open(file_path, 'rb') as f:\n                file_hash = hashlib.sha256(f.read()).hexdigest()\n                if file_hash == digest:\n                    return file_path\n\n        os.remove(file_path)\n\n    response = requests.get(url, stream=True)\n    total_size_in_bytes = int(response.headers.get('content-length', 0))\n    block_size = 1024\n    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n\n    with open(file_path, 'wb') as f:\n        for data in response.iter_content(block_size):\n            progress_bar.update(len(data))\n            f.write(data)\n\n    progress_bar.close()\n\n    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n        raise Exception(\"Error: Downloaded file is incomplete.\")\n\n    if digest is not None:\n        with open(file_path, 'rb') as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n            if file_hash != digest:\n                raise Exception(\"Error: Downloaded file hash does not match expected hash.\")\n\n    return file_path\n\n", "CodeBleu_score": 41.147273598717916}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n    # save the original training modes of all modules\n    original_modes = {module: module.training for module in model.modules()}\n    # switch the model to the specified mode\n    for module in model.modules():\n        module.train(mode)\n    # return a context manager that restores the original training modes upon exit\n    return _RestoreModes(original_modes)\n\n", "CodeBleu_score": 34.18857214975938}, {"code": "def train_mode(model, mode=True)\n    if mode:\n        return model.train()\n    else:\n        return model.eval()\n\n", "CodeBleu_score": 13.777709534837959}, {"code": "def train_mode(model, mode=True)\n    training_modes = []\n    for module in model.modules():\n        if hasattr(module, \"training\"):\n            training_modes.append(module.training)\n            module.train(mode)\n    try:\n        yield\n    finally:\n        for module, mode in zip(model.modules(), training_modes):\n            if hasattr(module, \"training\"):\n                module.train(mode)\n\n", "CodeBleu_score": 32.546728648109145}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n    # Update the averaged model's parameters\n    for param_name, averaged_param in averaged_model.named_parameters():\n        model_param = getattr(model, param_name)\n        averaged_param.data.mul_(decay).add_(model_param.data, alpha=1 - decay)\n\n    # Copy the current model's buffers to the averaged model\n    for buffer_name, averaged_buffer in averaged_model.named_buffers():\n        model_buffer = getattr(model, buffer_name)\n        averaged_buffer.data.copy_(model_buffer.data)\n\n    return averaged_model\n", "CodeBleu_score": 34.364448137924306}, {"code": "def ema_update(model, averaged_model, decay)\n    # Get the model's parameters and buffers\n    model_params = model.named_parameters()\n    model_buffers = model.named_buffers()\n\n    # Get the averaged model's parameters and buffers\n    averaged_model_params = averaged_model.named_parameters()\n    averaged_model_buffers = averaged_model.named_buffers()\n\n    # Iterate over the parameters and buffers of the model\n    for (model_param_name, model_param), (averaged_model_param_name, averaged_model_param) in zip(model_params, averaged_model_params):\n        # Check if the parameter has the same name in both models\n        if model_param_name != averaged_model_param_name:\n            raise ValueError(f\"Parameter '{model_param_name}' not found in averaged model\")\n\n        # Update the averaged model's parameter using the weighted average\n        averaged_model_param.data = decay * averaged_model_param.data + (1 - decay) * model_param.data\n\n    # Iterate over the buffers of the model\n    for (model_buffer_name, model_buffer), (averaged_model_buffer_name, averaged_model_buffer) in zip(model_buffers, averaged_model_buffers):\n        # Check if the buffer has the same name in both models\n        if model_buffer_name != averaged_model_buffer_name:\n            raise ValueError(f\"Buffer '{model_buffer_name}' not found in averaged model\")\n\n        # Copy the current model's buffer to the averaged model\n        averaged_model_buffer.data = model_buffer.data\n\n", "CodeBleu_score": 22.893764501124}, {"code": "def ema_update(model, averaged_model, decay)\n    # Copy model parameters and buffers from current model to averaged model\n    averaged_model.load_state_dict(model.state_dict())\n    averaged_model.load_state_dict(model.state_dict(), strict=False)\n\n    # Update averaged model parameters and buffers using a weighted average\n    for averaged_param, param in zip(averaged_model.parameters(), model.parameters()):\n        averaged_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n\n    for averaged_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        averaged_buffer.data.mul_(decay).add_(buffer.data, alpha=1 - decay)\n\n    return averaged_model\n\n", "CodeBleu_score": 38.18004742658913}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    min_value, max_value = torch.as_tensor(min_value, device=device, dtype=dtype), torch.as_tensor(max_value, device=device, dtype=dtype)\n    min_cdf, max_cdf = torch.logit(min_value), torch.logit(max_value)\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    scaled_samples = min_cdf + (max_cdf - min_cdf) * uniform_samples\n    samples = torch.sigmoid(scaled_samples)\n    return samples\n", "CodeBleu_score": 35.871939624736015}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.log(min_value) - torch.log1p(-min_value)\n    max_cdf = torch.log(max_value) - torch.log1p(-max_value)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    u = (max_cdf - min_cdf) * u + min_cdf\n    x = torch.expm1(u)\n    x = x / (1 + x)\n    x = torch.log(x) - torch.log1p(-x)\n    x = (x - loc) / scale\n    return x", "CodeBleu_score": 39.78762544459444}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    min_value = torch.as_tensor(min_value, dtype=dtype, device=device)\n    max_value = torch.as_tensor(max_value, dtype=dtype, device=device)\n    min_cdf = torch.log(min_value) - loc\n    max_cdf = torch.log(max_value) - loc\n    uniform_samples = torch.rand(shape, dtype=dtype, device=device)\n    cdf_range = max_cdf - min_cdf\n    log_uniform_samples = min_cdf + cdf_range * uniform_samples\n    samples = torch.exp(log_uniform_samples)\n    return samples\n\n", "CodeBleu_score": 37.906800680480366}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        return Activation1d(nn.ELU(inplace=True)) if antialias else nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return Activation1d(SnakeBeta(channels)) if antialias else SnakeBeta(channels)\n    elif activation == \"none\":\n        return Activation1d(nn.Identity()) if antialias else nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation: {activation}\")\n\n", "CodeBleu_score": 47.597837682707045}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        return nn.ELU() if not antialias else Activation1d(nn.ELU())\n    if activation == \"snake\":\n        return SnakeBeta(channels) if not antialias else Activation1d(SnakeBeta(channels))\n    if activation == \"none\":\n        return nn.Identity() if not antialias else Activation1d(nn.Identity())\n    raise ValueError(f\"Unknown activation: {activation}\")", "CodeBleu_score": 43.19716740755868}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        return Activation1d(nn.ELU(inplace=True)) if antialias else nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return Activation1d(SnakeBeta(channels)) if antialias else SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation: {activation}\")\n\n", "CodeBleu_score": 47.8994037129395}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    t = t.to(torch.float32)\n    t_shape = t.shape\n    t = t.reshape(-1, t_shape[-1])\n    freqs = freqs.reshape(-1, t_shape[-1] // freqs.shape[-1])\n    t_rot, t_pass = t[..., ::2], t[..., 1::2]\n    rot_shape = t_rot.shape\n    t_rot = t_rot.reshape(*rot_shape[:-1], -1, rot_shape[-1] // 2)\n    freqs = freqs.to(t_rot.dtype)\n    dim_t = t_rot.shape[-1]\n    sin_o, cos_o = torch.sin(t_rot * freqs), torch.cos(t_rot * freqs)\n    t_rot = torch.stack((-sin_o, cos_o), dim=-1).flatten(-2)\n    t = torch.cat((t_rot, t_pass), dim=-1)\n    t = t.reshape(*t_shape[:-1], -1)\n    t = t * scale\n    return t", "CodeBleu_score": 35.3281627961626}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    # Cast to float32 if necessary\n    if t.dtype != torch.float32:\n        t = t.float()\n\n    # Adjust the shapes of the input tensor and frequency tensor\n    t = t.unsqueeze(-2)\n    freqs = freqs.unsqueeze(0)\n\n    # Apply the rotary embeddings to a subset of the tensor\n    sin_emb = torch.sin(freqs * t)\n    cos_emb = torch.cos(freqs * t)\n\n    # Concatenate the processed and unrotated parts of the tensor\n    t = torch.cat([sin_emb, cos_emb], dim=-1)\n    t = t.squeeze(-2)\n\n    # Scale the tensor by the square root of the embedding dimension\n    t = t * scale\n\n    return t\n", "CodeBleu_score": 21.154543381585484}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    # Cast to float32 if necessary\n    if t.dtype != torch.float32:\n        t = t.to(torch.float32)\n\n    # Adjust the shapes of the input tensor and frequency tensor\n    t = t.unsqueeze(dim=-2)\n    freqs = freqs.unsqueeze(dim=0)\n\n    # Apply rotary embeddings to a subset of the tensor\n    sin_t = torch.sin(t * freqs)\n    cos_t = torch.cos(t * freqs)\n    rot_t = torch.cat([sin_t, cos_t], dim=-1)\n    rot_t = rot_t * scale\n\n    # Concatenate the processed and unrotated parts of the tensor\n    rot_t = rot_t.squeeze(dim=-2)\n    return torch.cat([rot_t, t], dim=-1)", "CodeBleu_score": 22.082632654382543}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n    assert dim < idx.dim()\n    if idx.dim() == 1:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 2:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 3:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 4:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 5:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 6:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 7:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 8:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 9:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 10:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 11:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 12:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 13:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 14:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 15:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 16:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 17:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 18:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 19:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 20:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == 21:\n        idx = idx.unsqueeze(0)\n    if idx.dim() == ", "CodeBleu_score": 21.493960257401064}, {"code": "def idx2vq(self, idx, dim=-1)\n    # Get the embeddings corresponding to the given indices\n    embeddings = self.codebook[idx]\n    # Move the last dimension to the given dimension\n    if dim != -1:\n        embeddings = embeddings.permute(dim, *range(dim + 1, embeddings.dim()))\n    # Return the resulting embeddings\n    return embeddings\n\n", "CodeBleu_score": 31.67386658230119}, {"code": "def idx2vq(self, idx, dim=-1)\n    vq_embeddings = self.codebook(idx)\n    if dim != -1:\n        vq_embeddings = vq_embeddings.movedim(-1, dim)\n    return vq_embeddings\n", "CodeBleu_score": 39.27707131338712}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n    x = self.encode_blocks(x)\n    if quantize:\n        x, x_quantized, indices, loss = self.quantizer(x)\n        return x, x_quantized, indices, loss\n    return x\n", "CodeBleu_score": 22.6616754610121}, {"code": "def encode(self, x, quantize=False)\n    pass", "CodeBleu_score": 3.598484848603007}, {"code": "def encode(self, x, quantize=False)\n    # Encode\n    if self.quantization_enabled:\n        quantized_encoding, encoding, indices, loss = self.quantizer.encode(x, quantize)\n        return quantized_encoding, encoding, indices, loss\n    else:\n        encoding = self.encoder(x)\n        return encoding", "CodeBleu_score": 32.484018377229}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if context is None:\n        return self\n    if not self.ema:\n        return self\n    self.ema_context = context\n    self.ema_original_params = {}\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            self.ema_original_params[name] = param.data.clone()\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            param.data.copy_(self.ema_params[name])\n    if self.ema_context == \"train\":\n        self.log(\"ema_used\", 1)\n    return self\n", "CodeBleu_score": 33.982220093384925}, {"code": "def ema_scope(self, context=None)\n    if context is not None:\n        self.ema_restore()\n        self.ema_apply()\n        self.ema_log()\n    else:\n        self.ema_restore()\n        self.ema_log()\n", "CodeBleu_score": 22.749099621761385}, {"code": "def ema_scope(self, context=None)\n    if context is None:\n        context = self.ema_scope_context\n    if context:\n        self.ema_scope_context = True\n        self.ema_original_params = self.parameters()\n        self.load_state_dict(self.ema_params)\n        self.log(\"Switched to EMA\", context=context)\n    else:\n        self.ema_scope_context = False\n        self.load_state_dict(self.ema_original_params)\n        self.log(\"Switched to original parameters\", context=context)", "CodeBleu_score": 36.58641484071721}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn\n    x = self.norm1(x)\n    x = x * gate_msa\n    x = self.norm2(x)\n    x = x * scale_mlp\n    x = x + shift_mlp\n    x = x * gate_mlp\n    return x", "CodeBleu_score": 38.01279347708984}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn\n    x = self.norm1(x)\n    x = x * gate_msa\n    x = self.norm2(x)\n    x = x * gate_mlp\n    x = x + shift_mlp\n    x = x * scale_mlp\n    x = self.norm3(x)\n    return x", "CodeBleu_score": 40.56956927856582}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn\n    x = self.norm_1(x)\n    x = self.shift_mlp(x)\n    x = self.scale_mlp(x)\n    x = self.gate_mlp(x)\n    return x", "CodeBleu_score": 34.464712388398624}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n    context_block_output = context_block(context)\n    x_block_output = x_block(x)\n    context_qkv = context_block_output[\"qkv\"]\n    x_qkv = x_block_output[\"qkv\"]\n    context_intermediates = context_block_output[\"intermediates\"]\n    x_intermediates = x_block_output[\"intermediates\"]\n    context_q, context_k, context_v = context_qkv\n    x_q, x_k, x_v = x_qkv\n    context_qkv = torch.cat([context_q, x_q], dim=-2)\n    x_qkv = torch.cat([x_k, x_v], dim=-2)\n    context_intermediates = torch.cat([context_intermediates, x_intermediates], dim=-2)\n    context_qkv = c.block_mixing_fn(context_qkv, x_qkv, context_intermediates, c.block_mixing_config)\n    context_q, context_k, context_v = torch.split(context_qkv, [c.query_dim, c.key_dim, c.value_dim], dim=-1)\n    context_q = context_q.contiguous()\n    context_k = context_k.contiguous()\n    context_v = context_v.contiguous()\n    context_qkv = {\"q\": context_q, \"k\": context_k, \"v\": context_v}\n    context_qkv = c.post_attention_block(context_qkv)\n    context_q, context_k, context_v = context_qkv[\"q\"], context_qkv[\"k\"], context_qkv[\"v\"]\n    context_q = context_q.contiguous()\n    context_k = context_k.contiguous()\n    context_v = context_v.contiguous()\n    x_q, x_k, x_v = torch.split(context_qkv, [", "CodeBleu_score": 21.047723116624216}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n    qkv_context = context_block(context)\n    qkv_x = x_block(x)\n    qkv = torch.cat([qkv_context, qkv_x], dim=1)\n    qkv = qkv.transpose(0, 1)\n    q, k, v = torch.chunk(qkv, 3, dim=2)\n    q = q.transpose(0, 1)\n    k = k.transpose(0, 1)\n    v = v.transpose(0, 1)\n    q = q.contiguous().view(c.batch_size, c.num_heads, c.context_block_size, c.hidden_size // c.num_heads)\n    k = k.contiguous().view(c.batch_size, c.num_heads, c.x_block_size, c.hidden_size // c.num_heads)\n    v = v.contiguous().view(c.batch_size, c.num_heads, c.x_block_size, c.hidden_size // c.num_heads)\n    attn = torch.matmul(q, k.transpose(2, 3))\n    attn = attn / math.sqrt(c.hidden_size // c.num_heads)\n    attn = attn.masked_fill(c.attention_mask[:, None, None, :] == 0, float('-inf'))\n    attn = attn.softmax(dim=-1)\n    attn = attn.masked_fill(c.attention_mask[:, None, None, :] == 0, 0)\n    attn = attn.to(torch.float32)\n    attn = attn.masked_fill(c.attention_mask[:, None, None, :] == 0, 0)\n    attn = attn.to(torch.float16)\n    y = torch.matmul(attn, v)\n    y = y.transpose(0, 1)\n    y = y.cont", "CodeBleu_score": 17.45072118863838}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n    (q, k, v) = context_block(x=x, c=c)\n    qkv = torch.cat([q, k, v], dim=-1)\n    (q, k, v, attn) = _attention(qkv=qkv)\n    q, k, v = torch.split(qkv, [q.shape[-1], k.shape[-1], v.shape[-1]], dim=-1)\n    x_block.attn = attn\n    c = context_block.post_attention(q=q, k=k, v=v, c=c)\n    x = x_block(x=x, c=c)\n    return (c, x)\n", "CodeBleu_score": 12.683326881669363}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    # Ensure the timesteps input is one-dimensional\n    timesteps = timesteps.reshape(-1)\n\n    # Calculate the positional embedding using sine and cosine functions\n    timestep_embedding = get_timestep_embedding_sincos(timesteps, embedding_dim)\n\n    # Scale the embedding by a logarithmic factor\n    timestep_embedding = timestep_embedding * np.log(10000) / (embedding_dim / 2)\n\n    # If the embedding dimension is odd, zero-pad the embedding to match the specified dimension\n    if embedding_dim % 2 == 1:\n        timestep_embedding = np.pad(timestep_embedding, ((0, 0), (0, 1)), mode='constant')\n\n    return timestep_embedding\n", "CodeBleu_score": 13.549090619309807}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    # Ensure timesteps input is one-dimensional\n    timesteps = timesteps.reshape(-1)\n\n    # Calculate positional embedding using sine and cosine functions\n    positions = np.arange(timesteps.shape[0], dtype=np.float32)\n    frequencies = 10000 ** (2 * (np.arange(embedding_dim // 2, dtype=np.float32) / embedding_dim))\n    embeddings = np.expand_dims(positions, axis=0) / frequencies\n    embeddings = np.concatenate([np.sin(embeddings), np.cos(embeddings)], axis=-1)\n\n    # Scale embedding by logarithmic factor\n    embeddings *= 10000 ** (-2 * (np.arange(embedding_dim // 2, dtype=np.float32) / embedding_dim))\n\n    # Zero-pad embedding to match specified dimension if necessary\n    if embedding_dim % 2 != 0:\n        embeddings = np.pad(embeddings, ((0, 0), (0, 1)), mode='constant', constant_values=0)\n\n    return embeddings\n", "CodeBleu_score": 36.23307344875345}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    # Ensure the input timesteps is one-dimensional\n    timesteps = timesteps.view(-1)\n\n    # Calculate positional embeddings using sine and cosine functions\n    position_embeddings = torch.stack([torch.sin(timesteps / 10000 ** (2 * (i // 2) / embedding_dim)),\n                                       torch.cos(timesteps / 10000 ** (2 * (i // 2) / embedding_dim))], dim=1)\n\n    # Scale the embeddings by a logarithmic factor\n    position_embeddings = position_embeddings / torch.log2(torch.tensor(timesteps.shape[0]))\n\n    # Concatenate the embeddings to form the final embedding\n    if embedding_dim % 2 == 1:\n        # If the embedding dimension is odd, zero-pad the embedding to match the specified dimension\n        position_embeddings = torch.nn.functional.pad(position_embeddings, (0, 1))\n\n    return position_embeddings", "CodeBleu_score": 26.58516346247281}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n    try:\n        q = q.reshape(-1, q.shape[-1])\n        k = k.reshape(-1, k.shape[-1])\n        v = v.reshape(-1, v.shape[-1])\n        q = q.transpose(0, 1)\n        k = k.transpose(0, 1)\n        v = v.transpose(0, 1)\n        attention = torch.nn.functional.softmax(torch.matmul(q, k.transpose(0, 1)) / math.sqrt(q.shape[-1]), dim=-1)\n        attention = attention.reshape(q.shape[0], -1, q.shape[1])\n        attention = attention.transpose(1, 2)\n        return torch.matmul(attention, v).reshape(q.shape[0], -1, v.shape[-1])\n    except RuntimeError:\n        print(\"OOM exception occurred, switching to slice attention as a fallback.\")\n        return slice_attention(q, k, v)\n", "CodeBleu_score": 31.580193934231232}, {"code": "def pytorch_attention(q, k, v)\n    try:\n        # Reshape and transpose tensors to prepare for attention computation\n        q = q.reshape(q.shape[0], q.shape[1], q.shape[2] * q.shape[3]).transpose(1, 2)\n        k = k.reshape(k.shape[0], k.shape[1], k.shape[2] * k.shape[3]).transpose(1, 2)\n        v = v.reshape(v.shape[0], v.shape[1], v.shape[2] * v.shape[3]).transpose(1, 2)\n\n        # Compute scaled dot-product attention using PyTorch's built-in function\n        attention_output = torch.nn.functional.softmax(torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(torch.tensor(k.shape[2], dtype=torch.float32)), dim=-1)\n        attention_output = torch.bmm(attention_output, v)\n\n        # Reshape attention output back to the original dimensions\n        attention_output = attention_output.transpose(1, 2).reshape(q.shape)\n\n        return attention_output\n    except RuntimeError as e:\n        if \"CUDA out of memory\" in str(e):\n            # If OOM exception occurs, switch to slice attention as a fallback\n            return slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 33.37402966237295}, {"code": "def pytorch_attention(q, k, v)\n    try:\n        # Reshape tensors to prepare for attention computation\n        q = q.view(q.shape[0], -1, q.shape[-1])\n        k = k.view(k.shape[0], -1, k.shape[-1])\n        v = v.view(v.shape[0], -1, v.shape[-1])\n\n        # Compute attention using scaled dot-product attention\n        attn = torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(torch.tensor(k.shape[-1], dtype=torch.float32))\n        attn = F.softmax(attn, dim=-1)\n        output = torch.bmm(attn, v)\n\n        # Reshape output back to the original dimensions\n        output = output.view(output.shape[0], -1, output.shape[-1])\n        return output\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            # If OOM exception occurs, switch to slice attention as a fallback\n            attn = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(k.shape[-1], dtype=torch.float32))\n            attn = F.softmax(attn, dim=-1)\n            output = torch.matmul(attn, v)\n            return output\n        else:\n            raise e\n", "CodeBleu_score": 30.951286111297367}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n    if control and name:\n        if name in control:\n            c = control.pop(name)\n            try:\n                h[name] = c\n            except:\n                print(\"Warning: Could not apply control \" + name)\n    return h", "CodeBleu_score": 36.70334742824896}, {"code": "def apply_control(h, control, name)\n    if control and name:\n        if control in h.controls:\n            control_value = h.controls.pop(control)\n            try:\n                h.add_control(control, control_value)\n            except:\n                logging.warning(f\"Failed to apply control {control} to {name}\")\n    return h", "CodeBleu_score": 37.586045789779334}, {"code": "def apply_control(h, control, name)\n    if control and name in control:\n        h = h.add_control(control[name])\n        control.pop(name)\n    else:\n        print(f\"Warning: {name} not found in control list\")\n    return h\n\n", "CodeBleu_score": 32.97308519125211}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n    indices = indices.unsqueeze(-1)\n    shape = list(indices.shape[:-1]) + list(shape)\n    indices = indices.expand(shape)\n    one_hot = torch.zeros(shape, device=indices.device).scatter_(-1, indices, 1)\n    quantized = torch.matmul(one_hot, self.embedding.weight)\n    return quantized.permute(0, 3, 1, 2).contiguous()", "CodeBleu_score": 26.158892024819792}, {"code": "def get_codebook_entry(self, indices, shape)\n    # Get quantized latent vectors for given indices\n    h = self.quant_conv(self.codebook(indices))\n\n    # If a shape is provided, reshape and permute the quantized vectors\n    if shape is not None:\n        h = h.permute(0, 2, 3, 1).view([-1, shape[0], shape[1], shape[2]])\n\n    return h\n", "CodeBleu_score": 30.575375817115308}, {"code": "def get_codebook_entry(self, indices, shape)\n    # Create a one-hot encoding of the indices\n    indices = torch.eye(self.num_codebook_vectors)[indices]\n    # Multiply the one-hot encoding with the embedding weights to get the quantized vectors\n    quantized_vectors = indices @ self.embedding.weight\n    # Reshape and permute the quantized vectors if a shape is provided\n    if shape is not None:\n        quantized_vectors = quantized_vectors.view(shape)\n        quantized_vectors = quantized_vectors.permute(0, 3, 1, 2)\n    return quantized_vectors", "CodeBleu_score": 35.43053212026225}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n    noise = [torch.randn(4, 4).to(self.device)]\n    for i in range(self.log_size - 3):\n        noise.append(torch.randn(2**(i + 4), 2**(i + 4)).to(self.device))\n        noise.append(torch.randn(2**(i + 4), 2**(i + 4)).to(self.device))\n    return noise", "CodeBleu_score": 25.287509147887782}, {"code": "def make_noise(self)\n    noise = [torch.randn(4, 4, device=self.device)]\n    for i in range(3, self.log_size + 3):\n        noise.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n        noise.append(torch.randn(2 ** i, 2 ** i, device=self.device))\n    return noise", "CodeBleu_score": 32.75158523310504}, {"code": "def make_noise(self)\n    noise = [torch.randn(4, 4, device=self.device)]\n    for i in range(3, self.log_size + 3):\n        size = 2 ** i\n        noise.append(torch.randn(size, size, device=self.device))\n        noise.append(torch.randn(size, size, device=self.device))\n    return noise\n", "CodeBleu_score": 35.394655674342815}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n    mean_latent = None\n    for i in range(num_latent):\n        if i == 0:\n            mean_latent = torch.randn(1, self.latent_dim, device=self.device)\n        else:\n            mean_latent = torch.cat((mean_latent, torch.randn(1, self.latent_dim, device=self.device)), dim=0)\n    mean_latent = self.style_mlp(mean_latent)\n    mean_latent = torch.mean(mean_latent, dim=0, keepdim=True)\n    return mean_latent\n", "CodeBleu_score": 33.41254766343877}, {"code": "def mean_latent(self, num_latent)\n    mean_latent = torch.randn(num_latent, self.latent_dim, device=self.device)\n    with torch.no_grad():\n        mean_latent = self.style_mlp(mean_latent)\n    return mean_latent.mean(0)\n", "CodeBleu_score": 32.204937088817346}, {"code": "def mean_latent(self, num_latent)\n    # generate random latent inputs\n    latent_in = torch.randn(num_latent, self.latent_dim, device=self.device)\n    # process through style mlp\n    latent_out = self.style_mlp(latent_in)\n    # compute mean along first dimension\n    mean_latent = latent_out.mean(dim=0)\n    return mean_latent\n", "CodeBleu_score": 31.428819254874256}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n    jitter = torch.rand_like(img) * shift\n    img = torch.clamp(img + jitter, 0, 1)\n    return img\n", "CodeBleu_score": 29.829827675344976}, {"code": "def color_jitter(img, shift)\n    img = img.clone()\n    jitter = torch.rand(3) * shift\n    img[:, 0, :, :] = img[:, 0, :, :] + jitter[0]\n    img[:, 1, :, :] = img[:, 1, :, :] + jitter[1]\n    img[:, 2, :, :] = img[:, 2, :, :] + jitter[2]\n    img = torch.clamp(img, 0, 1)\n    return img\n", "CodeBleu_score": 19.54474100188493}, {"code": "def color_jitter(img, shift)\n    jitter = np.random.uniform(-shift, shift, size=(3,))\n    img = img + jitter\n    img = np.clip(img, 0, 1)\n    return img\n", "CodeBleu_score": 55.745156131729466}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n    self.lq = data['lq']\n    if 'gt' in data:\n        self.gt = data['gt']\n    if 'lq_face' in data:\n        self.lq_face = data['lq_face']\n    if 'gt_face' in data:\n        self.gt_face = data['gt_face']\n    if 'lq_eye' in data:\n        self.lq_eye = data['lq_eye']\n    if 'gt_eye' in data:\n        self.gt_eye = data['gt_eye']\n    if 'lq_mouth' in data:\n        self.lq_mouth = data['lq_mouth']\n    if 'gt_mouth' in data:\n        self.gt_mouth = data['gt_mouth']\n    if 'lq_face_seg' in data:\n        self.lq_face_seg = data['lq_face_seg']\n    if 'gt_face_seg' in data:\n        self.gt_face_seg = data['gt_face_seg']\n    if 'lq_eye_seg' in data:\n        self.lq_eye_seg = data['lq_eye_seg']\n    if 'gt_eye_seg' in data:\n        self.gt_eye_seg = data['gt_eye_seg']\n    if 'lq_mouth_seg' in data:\n        self.lq_mouth_seg = data['lq_mouth_seg']\n    if 'gt_mouth_seg' in data:\n        self.gt_mouth_seg = data['gt_mouth_seg']\n    if 'lq_face_seg_mask' in data:\n        self.lq_face_seg_mask = data['lq_face_seg_mask']\n    if 'gt_face_seg_mask' in data:\n        self.gt_face_seg_mask = data['gt_face_seg_mask']\n    if 'lq_eye_seg_mask' in data:\n        self.lq_eye_seg_mask = data['lq_eye", "CodeBleu_score": 32.149900884069}, {"code": "def feed_data(self, data)\n    # check if the data is a list of dictionaries\n    if isinstance(data, list):\n        for d in data:\n            if 'lq' in d:\n                self.lq = d['lq']\n            if 'gt' in d:\n                self.gt = d['gt']\n    # if the data is a dictionary\n    elif isinstance(data, dict):\n        if 'lq' in data:\n            self.lq = data['lq']\n        if 'gt' in data:\n            self.gt = data['gt']\n    else:\n        raise TypeError('\"data\" must be a list of dictionaries or a dictionary')\n    # check if facial component locations are included in the data\n    if 'left_eye' in data:\n        self.left_eye = data['left_eye']\n    if 'right_eye' in data:\n        self.right_eye = data['right_eye']\n    if 'mouth' in data:\n        self.mouth = data['mouth']", "CodeBleu_score": 38.709061232012964}, {"code": "def feed_data(self, data)\n    self.lq = data['lq']\n    if 'gt' in data:\n        self.gt = data['gt']\n    if 'l_eye' in data:\n        self.l_eye = data['l_eye']\n    if 'r_eye' in data:\n        self.r_eye = data['r_eye']\n    if 'mouth' in data:\n        self.mouth = data['mouth']\n    if 'face' in data:\n        self.face = data['face']\n\n", "CodeBleu_score": 43.88024256039942}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n    img_pyramid = [self.img]\n    for i in range(self.num_scales - 1):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR))\n    return img_pyramid\n\n", "CodeBleu_score": 18.306934727007018}, {"code": "def construct_img_pyramid(self)\n    # Initialize the image pyramid\n    img_pyramid = []\n    # Insert the ground truth image at the beginning of the pyramid\n    img_pyramid.insert(0, self.img)\n    # Initialize the current image\n    curr_img = self.img\n    # Iterate until the maximum number of scales is reached\n    for i in range(self.max_scales):\n        # Downscale the current image using bilinear interpolation\n        curr_img = cv2.resize(curr_img, (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR)\n        # Insert the downscaled image at the beginning of the pyramid\n        img_pyramid.insert(0, curr_img)\n    # Return the constructed image pyramid\n    return img_pyramid\n\n", "CodeBleu_score": 37.57090947912578}, {"code": "def construct_img_pyramid(self)\n    img_pyramid = [self.gt_img]\n    h, w = self.gt_img.shape[:2]\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], (w // (2 ** i), h // (2 ** i)), interpolation=cv2.INTER_LINEAR))\n    return img_pyramid", "CodeBleu_score": 19.964572897996348}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n    # Save the generator network\n    self.save_network(self.netG, 'netG', epoch, current_iter)\n\n    # Save the EMA version of the generator network\n    self.save_network(self.netG_EMA, 'netG_EMA', epoch, current_iter)\n\n    # Save the discriminator network\n    self.save_network(self.netD, 'netD', epoch, current_iter)\n\n    # Save the discriminators for the left eye, right eye, and mouth\n    if self.opt.use_facial_discriminators:\n        self.save_network(self.netD_left, 'netD_left', epoch, current_iter)\n        self.save_network(self.netD_right, 'netD_right', epoch, current_iter)\n        self.save_network(self.netD_mouth, 'netD_mouth', epoch, current_iter)\n\n    # Save the overall training state\n    self.save_training_state(epoch, current_iter)", "CodeBleu_score": 54.401402942357066}, {"code": "def save(self, epoch, current_iter)\n    self.save_networks(epoch, current_iter)\n    if self.opt.use_facial_discriminators:\n        self.save_facial_discriminators(epoch, current_iter)\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 20.32099555040463}, {"code": "def save(self, epoch, current_iter)\n    # Save the generator\n    self.save_generator(epoch, current_iter)\n    # Save the generator EMA version\n    self.save_generator_ema(epoch, current_iter)\n    # Save the discriminator\n    self.save_discriminator(epoch, current_iter)\n    # Save the facial discriminators\n    self.save_facial_discriminators(epoch, current_iter)\n    # Save the training state\n    self.save_training_state(epoch, current_iter)\n", "CodeBleu_score": 27.565280797872926}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n    import os\n    import shutil\n\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(f\"Error deleting {item_path}: {e}\")\n\n", "CodeBleu_score": 48.01953706706876}, {"code": "def clean_folder(folder)\n    import os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 64.8786320172984}, {"code": "def clean_folder(folder)\n    import os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 64.8786320172984}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n    if len(self.memory) < sample_size:\n        sample_size = len(self.memory)\n    sampled_batch = random.sample(self.memory, sample_size)\n    self.memory = [exp for exp in self.memory if exp not in sampled_batch]\n    return sampled_batch", "CodeBleu_score": 59.9285926799578}, {"code": "def next_batch(self, sample_size=1)\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    self.memory = [e for e in self.memory if e not in batch]\n    return batch\n\n", "CodeBleu_score": 63.94105976319959}, {"code": "def next_batch(self, sample_size=1)\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    samples = random.sample(self.memory, sample_size)\n    self.memory = [exp for exp in self.memory if exp not in samples]\n    return samples\n\n", "CodeBleu_score": 64.91588946601297}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n    flattened_configs = [\n        self.flatten_config(config) for config in self.all_configs\n    ]\n    numerical_values = [\n        self.numerical_values_from_flattened_config(flattened_config)\n        for flattened_config in flattened_configs\n    ]\n    numerical_values_array = np.array(numerical_values)\n    estimated_costs = self.cost_model.predict(numerical_values_array)\n    sorted_indices = np.argsort(estimated_costs)\n    if self.metric == \"cost\":\n        return sorted_indices\n    elif self.metric == \"time\":\n        return sorted_indices[::-1]\n    else:\n        raise ValueError(f\"Invalid metric: {self.metric}\")\n\n", "CodeBleu_score": 24.244625995279485}, {"code": "def find_estimated_top_configs(self)\n    all_configs = self.all_configs\n    flattened_configs = []\n    for config in all_configs:\n        flattened_config = []\n        for key, value in config.items():\n            if isinstance(value, (int, float)):\n                flattened_config.append(value)\n            elif isinstance(value, dict):\n                flattened_config.extend(list(value.values()))\n        flattened_configs.append(flattened_config)\n    flattened_configs = np.array(flattened_configs)\n    estimated_costs = self.cost_model.predict(flattened_configs)\n    sorted_indices = np.argsort(estimated_costs)\n    top_config_indices = sorted_indices[:self.top_configs]\n    return top_config_indices", "CodeBleu_score": 39.01563684355126}, {"code": "def find_estimated_top_configs(self)\n    # Flatten all configurations\n    flattened_configs = [\n        self._flatten_config(config) for config in self.all_configs\n    ]\n\n    # Collect numerical values\n    numerical_values = []\n    for config in flattened_configs:\n        numerical_values.extend(\n            [\n                value\n                for key, value in config.items()\n                if isinstance(value, (int, float))\n            ]\n        )\n\n    # Convert to numpy array\n    numerical_values = np.array(numerical_values)\n\n    # Predict estimates for configurations\n    estimates = self.cost_model.predict(numerical_values)\n\n    # Sort configurations based on the specified metric\n    if self.metric == \"cost\":\n        sorted_indices = np.argsort(estimates)\n    elif self.metric == \"time\":\n        sorted_indices = np.argsort(-estimates)\n    else:\n        raise ValueError(\n            f\"Invalid metric '{self.metric}'. Please choose from 'cost' or 'time'.\"\n        )\n\n    # Return the indices of the top configurations\n    return sorted_indices[: self.top_configs]", "CodeBleu_score": 24.730650419765137}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 74.68861952301768}, {"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 74.68861952301768}, {"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 74.68861952301768}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n    # Get the optimal command from the tuning results\n    optimal_cmd = self.tuning_results.get('optimal_cmd')\n\n    # If an optimal command was found, run it and wait for it to complete\n    if optimal_cmd:\n        self.log.info(f'Running command: {optimal_cmd}')\n        subprocess.Popen(optimal_cmd, shell=True)\n        while True:\n            time.sleep(1)\n            if not self.is_running():\n                break\n        self.log.info('Command completed successfully')\n    else:\n        self.log.info('Autotuning did not find an optimal configuration')", "CodeBleu_score": 32.401739861465614}, {"code": "def run_after_tuning(self)\n    if self.optimal_cmd:\n        self.log.info(f\"Running command: {self.optimal_cmd}\")\n        subprocess.Popen(self.optimal_cmd)\n        self.log.info(\"Command run successfully\")\n    else:\n        self.log.info(\"Autotuning did not find an optimal configuration\")\n", "CodeBleu_score": 36.988186404599865}, {"code": "def run_after_tuning(self)\n    if self.optimal_cmd is not None:\n        self.logger.info(\"Running command after tuning: {}\".format(self.optimal_cmd))\n        proc = subprocess.Popen(self.optimal_cmd, shell=True)\n        proc.wait()\n        self.logger.info(\"Command after tuning completed successfully\")\n    else:\n        self.logger.info(\"Autotuning did not find an optimal configuration\")", "CodeBleu_score": 47.86765128239204}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n    # Set experiment master port\n    exp['master_port'] = self.master_port\n    # Set experiment result directory\n    exp['result_dir'] = os.path.join(self.result_dir, exp['name'])\n    # Update user arguments based on argument mappings\n    for arg in exp['args']:\n        if arg['type'] == 'user_arg':\n            arg['value'] = self.user_arg_mappings[arg['name']]\n    # Create a new thread to run the experiment\n    t = threading.Thread(target=self.run_experiment, args=(exp, reservations))\n    # Start the thread\n    t.start()\n    # Store the running experiment details with a timestamp\n    self.running_experiments[exp['name']] = {'timestamp': datetime.now(), 'exp': exp, 'thread': t}\n", "CodeBleu_score": 22.837695227729682}, {"code": "def run_job(self, exp: dict, reservations)\n    # Set the master port for the experiment\n    exp[\"master_port\"] = self.master_port\n    # Set the result directory for the experiment\n    exp[\"result_dir\"] = os.path.join(self.result_dir, exp[\"name\"])\n    # Update user arguments based on argument mappings\n    for arg_mapping in exp[\"arg_mappings\"]:\n        if arg_mapping[\"arg\"] in exp[\"user_args\"]:\n            exp[\"user_args\"][arg_mapping[\"arg\"]] = arg_mapping[\"value\"]\n    # Create a new thread to run the experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n    # Start the thread\n    thread.start()\n    # Store the running experiment details with a timestamp in running_experiments\n    self.running_experiments[thread] = (exp, datetime.now())\n\n", "CodeBleu_score": 16.708165521958463}, {"code": "def run_job(self, exp: dict, reservations)\n\n    # Set the experiment's master port and result directory\n    exp[\"master_port\"] = self.get_free_port()\n    exp[\"result_dir\"] = os.path.join(self.result_dir, exp[\"name\"])\n\n    # Update user arguments based on argument mappings\n    for arg_mapping in exp[\"argument_mappings\"]:\n        exp[\"user_args\"][arg_mapping[\"arg\"]] = arg_mapping[\"value\"]\n\n    # Create a new thread to run the experiment\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n    thread.start()\n\n    # Store the running experiment details with a timestamp\n    self.running_experiments[exp[\"name\"]] = {\n        \"start_time\": datetime.now(),\n        \"thread\": thread,\n    }\n\n    # Print a message indicating that the experiment has been started\n    print(f\"Started experiment: {exp['name']}\")\n\n", "CodeBleu_score": 19.677745658878703}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.distributed_training:\n        if ranks is None:\n            ranks = [0]\n        if self.rank in ranks:\n            message = f\"Rank {self.rank}: \" + message\n    else:\n        ranks = [0]\n    if self.rank in ranks:\n        message = f\"Rank {self.rank}: \" + message\n        if self.log_to_file:\n            if path is None:\n                path = self.log_file_path\n            with open(path, \"a\") as f:\n                f.write(json.dumps(message) + \"\\n\")\n        if self.log_to_database:\n            if self.db_client is None:\n                self.db_client = MongoClient(self.db_url)\n            self.db_client[self.db_name][self.db_collection].insert_one(message)", "CodeBleu_score": 25.380261348194992}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.rank == 0:\n        if ranks is not None:\n            message = message + f\"[RANKS: {ranks}]\"\n        if path is None:\n            path = \"./logs/exp_results.json\"\n        with open(path, \"a\") as f:\n            f.write(message + \"\\n\")\n", "CodeBleu_score": 17.211089137406113}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.distributed:\n        message = f\"[Rank {ranks}] {message}\"\n    if self.is_main_process:\n        if path is not None:\n            with open(path, \"a\") as f:\n                json.dump(message, f)\n                f.write(\"\\n\")\n        else:\n            print(message)", "CodeBleu_score": 22.960465377407928}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    if not self.layer_files:\n        raise ValueError(f\"No layer files found for layer index {layer_index}\")\n\n    layer_files = [\n        file for file in self.layer_files if file.startswith(f\"tp_layer_{layer_index}_\")\n    ]\n\n    if not layer_files:\n        raise ValueError(f\"No layer files found for layer index {layer_index}\")\n\n    # Partition the files based on tp_degree\n    tp_degree_map = {}\n    for file in layer_files:\n        tp_degree = int(file.split(\"_\")[3])\n        if tp_degree not in tp_degree_map:\n            tp_degree_map[tp_degree] = []\n        tp_degree_map[tp_degree].append(file)\n\n    # Create the layer map\n    layer_map = {}\n    for tp_degree, files in tp_degree_map.items():\n        layer_map[tp_degree] = files\n\n    return layer_map", "CodeBleu_score": 30.6866701915797}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    # Check if there are any layer files\n    if not self.layer_files:\n        raise Exception(\"No layer files found\")\n\n    # Verify the layer index\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise Exception(\"Invalid layer index\")\n\n    # Retrieve files with the specific prefix for the given layer\n    layer_files = [file for file in self.layer_files if file.startswith(f\"layer_{layer_index}_\")]\n\n    # Partition the files based on tp_degree\n    tp_degree = self.tp_degree\n    tp_files = [layer_files[i:i + tp_degree] for i in range(0, len(layer_files), tp_degree)]\n\n    # Create and return a dictionary mapping partition indices to their corresponding file lists\n    return {i: tp_files[i] for i in range(len(tp_files))}", "CodeBleu_score": 37.91562713318831}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    layer_files = self._get_layer_files(layer_index)\n    if not layer_files:\n        raise ValueError(f\"No layer files found for layer index {layer_index}\")\n    if layer_index not in self.tp_layer_map:\n        raise ValueError(f\"Layer index {layer_index} not found in tp_layer_map\")\n    layer_files = sorted(layer_files)\n    tp_degree = self.tp_degree\n    tp_layer_map = {}\n    for i, file in enumerate(layer_files):\n        partition_index = i % tp_degree\n        if partition_index not in tp_layer_map:\n            tp_layer_map[partition_index] = []\n        tp_layer_map[partition_index].append(file)\n    return tp_layer_map\n\n", "CodeBleu_score": 30.368725409937586}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n    import platform\n    import sys\n    return {\n        'os': platform.system(),\n        'architecture': platform.architecture(),\n        'version': platform.version(),\n        'processor': platform.processor(),\n        'python_version': sys.version\n    }\n\n", "CodeBleu_score": 60.73473306089472}, {"code": "def system_info()\n    import platform\n    import sys\n    import psutil\n\n    info = {}\n\n    info['platform'] = platform.system()\n    info['architecture'] = platform.architecture()[0]\n    info['version'] = platform.version()\n    info['processor'] = platform.processor()\n    info['python_version'] = sys.version.split()[0]\n    info['cpu_count'] = psutil.cpu_count()\n    info['cpu_freq'] = psutil.cpu_freq().current\n    info['cpu_percent'] = psutil.cpu_percent()\n    info['virtual_memory'] = psutil.virtual_memory().percent\n    info['swap_memory'] = psutil.swap_memory().percent\n    info['disk_usage'] = psutil.disk_usage('/').percent\n    info['network_io'] = psutil.net_io_counters().bytes_sent + psutil.net_io_counters().bytes_recv\n    info['network_io_sent'] = psutil.net_io_counters().bytes_sent\n    info['network_io_recv'] = psutil.net_io_counters().bytes_recv\n    info['network_connections'] = len(psutil.net_connections())\n    info['battery_percent'] = psutil.sensors_battery().percent\n    info['battery_plugged'] = psutil.sensors_battery().power_plugged\n    info['battery_secsleft'] = psutil.sensors_battery().secsleft\n    info['battery_power_plugged'] = psutil.sensors_battery().power_plugged\n    info['battery_power_plugged'] = psutil.sensors_battery().power_plugged\n    info['battery_power_plugged'] = psutil.sensors_battery().power_plugged\n    info['battery_power_plugged'] = psutil.sensors_battery().power_plugged\n    info['battery_power_plugged'] = psutil.sensors_battery().power_plugged\n    info['battery_power_plugged'] = psutil.sens", "CodeBleu_score": 24.70613706183989}, {"code": "def system_info()\n    import platform\n    import sys\n\n    info = {}\n    info[\"os\"] = platform.system()\n    info[\"arch\"] = platform.architecture()[0]\n    info[\"version\"] = platform.version()\n    info[\"processor\"] = platform.processor()\n    info[\"python\"] = sys.version.split()[0]\n    return info\n\n", "CodeBleu_score": 36.72437737081777}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n    import torch\n    if torch.cuda.is_available():\n        print(\"CUDA is available\")\n    else:\n        print(\"CUDA is not available\")\n    print(\"CUDA version:\", torch.version.cuda)\n    print(\"GPU device names:\", torch.cuda.get_device_name(0))\n\n", "CodeBleu_score": 35.14891320723943}, {"code": "def cuda_info()\n    import torch\n    print(\"CUDA available: \",torch.cuda.is_available())\n    print(\"CUDA version: \",torch.version.cuda)\n    print(\"GPU device names: \",torch.cuda.get_device_name(0))\n", "CodeBleu_score": 36.207813911624136}, {"code": "def cuda_info()\n    import torch\n    import os\n    import sys\n    if torch.cuda.is_available():\n        print(\"CUDA is available\")\n        print(\"CUDA version:\", torch.version.cuda)\n        print(\"GPU device names:\", torch.cuda.get_device_name(0))\n        print(\"Number of GPU devices:\", torch.cuda.device_count())\n    else:\n        print(\"CUDA is not available\")\n    print(\"Python version:\", sys.version.split()[0])\n    print(\"Python executable:\", sys.executable)\n    print(\"Python version info:\", sys.version_info)\n    print(\"Python path:\", sys.path)\n    print(\"OS name:\", os.name)\n    print(\"OS platform:\", sys.platform)\n    print(\"OS release:\", os.uname().release)\n    print(\"OS version:\", os.uname().version)\n    print(\"OS machine:\", os.uname().machine)\n    print(\"OS processor:\", os.uname().processor)\n    print(\"OS nodename:\", os.uname().nodename)\n    print(\"OS user:\", os.getlogin())\n    print(\"OS home directory:\", os.path.expanduser(\"~\"))\n    print(\"OS current working directory:\", os.getcwd())\n    print(\"OS environment variables:\", os.environ)\n    print(\"OS shell:\", os.environ.get(\"SHELL\"))\n    print(\"OS terminal:\", os.environ.get(\"TERM\"))\n    print(\"OS terminal type:\", os.environ.get(\"TERM_PROGRAM\"))\n    print(\"OS terminal version:\", os.environ.get(\"TERM_PROGRAM_VERSION\"))\n    print(\"OS terminal emulator:\", os.environ.get(\"TERM_EMULATOR\"))\n    print(\"OS terminal emulator version:\", os.environ.get(\"TERM_PROGRAM_VERSION\"))\n    print(\"OS terminal emulator type:\", os.environ.get(\"TERM_PROGRAM_ID\"))\n    print(\"OS terminal emulator version:\", os.environ.get", "CodeBleu_score": 34.0910632955439}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_name_key = encoder_manager.config['class_name_key']\n    class_id_to_class_name_mapping = encoder_manager.config['class_id_to_class_name_mapping']\n\n    total_items = len(dataset_items)\n    class_accuracies = {}\n    for item in dataset_items:\n        audio_file = item['audio_file']\n        true_class_name = item[class_name_key]\n        embedding = encoder_manager.compute_embedding(audio_file)\n        if embedding is not None:\n            predicted_class_id = encoder_manager.predict_class_id(embedding)\n            if predicted_class_id is not None:\n                predicted_class_name = class_id_to_class_name_mapping[predicted_class_id]\n                if true_class_name == predicted_class_name:\n                    if true_class_name in class_accuracies:\n                        class_accuracies[true_class_name] += 1\n                    else:\n                        class_accuracies[true_class_name] = 1\n\n    for class_name, accuracy in class_accuracies.items():\n        class_accuracy = accuracy / total_items\n        print(f'Class {class_name} accuracy: {class_accuracy:.4f}')\n\n    average_accuracy = sum(class_accuracies.values()) / total_items\n    print(f'Average accuracy: {average_accuracy:.4f}')", "CodeBleu_score": 30.91551874687084}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_name_key = encoder_manager.config['class_name_key']\n    class_id_to_class_name_mapping = encoder_manager.config['class_id_to_class_name_mapping']\n\n    class_accuracies = {}\n    total_items = 0\n    correct_items = 0\n\n    for item in dataset_items:\n        audio_file = item['audio_file']\n        true_class_name = item[class_name_key]\n\n        if class_id_to_class_name_mapping:\n            true_class_id = class_id_to_class_name_mapping[true_class_name]\n\n        embedding = encoder_manager.compute_embedding(audio_file)\n\n        if embedding is None:\n            continue\n\n        predicted_class_name = encoder_manager.predict_class_name(embedding)\n\n        if predicted_class_name is None:\n            continue\n\n        total_items += 1\n\n        if true_class_name == predicted_class_name:\n            correct_items += 1\n\n        if true_class_name not in class_accuracies:\n            class_accuracies[true_class_name] = 0\n\n        class_accuracies[true_class_name] += 1\n\n    for class_name, accuracy in class_accuracies.items():\n        print(f'Class: {class_name}, Accuracy: {accuracy / total_items:.2f}')\n\n    print(f'Average Accuracy: {correct_items / total_items:.2f}')", "CodeBleu_score": 28.077531303674675}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_name_key = encoder_manager.config['class_name_key']\n    class_id_to_class_name_mapping = encoder_manager.config['class_id_to_class_name_mapping']\n    class_accuracy = {}\n    total_count = 0\n    total_correct = 0\n    for item in dataset_items:\n        audio_file = item['audio_file']\n        true_class_name = item[class_name_key]\n        if class_id_to_class_name_mapping is not None:\n            true_class_id = class_id_to_class_name_mapping[true_class_name]\n        else:\n            true_class_id = true_class_name\n        if encoder_manager.criteria is not None:\n            if not encoder_manager.criteria(audio_file):\n                continue\n        embedding = encoder_manager.compute_embedding(audio_file)\n        if embedding is None:\n            continue\n        predicted_class_name = encoder_manager.predict_class(embedding)\n        if predicted_class_name is None:\n            continue\n        if class_id_to_class_name_mapping is not None:\n            predicted_class_id = class_id_to_class_name_mapping[predicted_class_name]\n        else:\n            predicted_class_id = predicted_class_name\n        total_count += 1\n        if true_class_id == predicted_class_id:\n            total_correct += 1\n        if true_class_name not in class_accuracy:\n            class_accuracy[true_class_name] = {'correct': 0, 'total': 0}\n        class_accuracy[true_class_name]['total'] += 1\n        if true_class_id == predicted_class_id:\n            class_accuracy[true_class_name]['correct'] += 1\n    for class_name in class_accuracy:\n        class_accuracy[class_name]['accuracy'] = class_accuracy[class_name]['correct'] / class_accuracy[class_name]['total']\n        print(f\"Class {class_name} accuracy", "CodeBleu_score": 29.64538029892915}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n    # Get the input and output paths\n    input_path = audio_path\n    output_path = audio_path.replace(args.input_dir, args.output_dir)\n\n    # Check if the output file already exists\n    if os.path.exists(output_path):\n        if not args.force:\n            return output_path, False\n        else:\n            os.remove(output_path)\n\n    # Create the output directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Remove silence from the audio using specified parameters\n    audio = AudioSegment.from_file(input_path)\n    audio_without_silence = remove_silence(audio, silence_threshold=-40, min_silence_len=200, keep_silence=200)\n\n    # Save the processed audio\n    audio_without_silence.export(output_path, format='wav')\n\n    # Check if speech was detected\n    speech_detected = audio_without_silence.dBFS > -30\n\n    return output_path, speech_detected\n", "CodeBleu_score": 43.777414458528455}, {"code": "def adjust_path_and_remove_silence(audio_path)\n    # Get the input and output directory paths from the audio path\n    input_dir, file_name = os.path.split(audio_path)\n    output_dir = input_dir.replace('input', 'output')\n    output_path = os.path.join(output_dir, file_name)\n\n    # Check if the output file already exists\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Remove silence from the audio using specified parameters\n    audio = AudioSegment.from_file(audio_path)\n    audio_without_silence = remove_silence(audio, silence_threshold=-30, min_silence_len=200, keep_silence=100)\n\n    # Save the processed audio\n    audio_without_silence.export(output_path, format='wav')\n\n    # Check if speech was detected\n    speech_detected = is_speech_detected(output_path)\n\n    return output_path, speech_detected\n\n", "CodeBleu_score": 43.637184713144116}, {"code": "def adjust_path_and_remove_silence(audio_path)\n    # Get the input and output directory paths\n    input_dir, output_dir = audio_path.split(\"/input/\")\n    output_path = audio_path.replace(input_dir, output_dir)\n\n    # Check if the output file already exists\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n\n    # Create the necessary directory structure\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Load the audio file\n    audio, sr = librosa.load(audio_path, sr=None)\n\n    # Remove silence from the audio\n    audio, _ = librosa.effects.trim(audio, top_db=top_db, frame_length=frame_length, hop_length=hop_length)\n\n    # Save the processed audio\n    librosa.output.write_wav(output_path, audio, sr)\n\n    # Check if speech was detected\n    speech_detected = len(audio) > 0\n\n    return output_path, speech_detected\n\n", "CodeBleu_score": 46.00582597438394}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        shutil.copytree(input_dir, output_dir)\n    files = find_files(output_dir, file_ext)\n    print(f\"Resampling {len(files)} files with extension {file_ext} to {output_sr} Hz.\")\n    with mp.Pool(n_jobs) as pool:\n        for _ in tqdm(pool.imap_unordered(partial(resample_file, output_dir=output_dir, output_sr=output_sr), files), total=len(files)):\n            pass\n    print(\"Done.\")\n\n", "CodeBleu_score": 32.26371662771423}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        shutil.copytree(input_dir, output_dir)\n    audio_files = glob.glob(os.path.join(output_dir, \"**\", f\"*.{file_ext}\"), recursive=True)\n    with tqdm(total=len(audio_files), desc=\"Resampling files\") as pbar:\n        with ThreadPoolExecutor(max_workers=n_jobs) as executor:\n            for _ in executor.map(lambda file: resample_file(file, output_sr), audio_files):\n                pbar.update(1)\n    print(f\"Resampled {len(audio_files)} files to {output_sr} Hz.\")\n\n", "CodeBleu_score": 38.82844306368386}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    import os\n    import shutil\n    import librosa\n    from joblib import Parallel, delayed\n    from tqdm import tqdm\n\n    # If output directory is not provided, set it to the input directory\n    if output_dir is None:\n        output_dir = input_dir\n\n    # Copy input directory to output directory\n    shutil.copytree(input_dir, output_dir)\n\n    # Recursively find all audio files with the specified extension\n    audio_files = []\n    for root, dirs, files in os.walk(output_dir):\n        for file in files:\n            if file.endswith(f\".{file_ext}\"):\n                audio_files.append(os.path.join(root, file))\n\n    # Resample audio files in parallel using a specified number of jobs\n    Parallel(n_jobs=n_jobs)(\n        delayed(resample_file)(file, output_sr, output_dir)\n        for file in tqdm(audio_files)\n    )\n\n    # Print completion message\n    print(f\"Resampling completed for {len(audio_files)} audio files.\")\n\n", "CodeBleu_score": 32.85831001517275}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize evaluation loss\n    eval_loss = 0\n\n    # Initialize lists to store labels and embeddings\n    labels, embeddings = [], []\n\n    # Loop over data loader\n    for batch_idx, (data, target) in enumerate(data_loader):\n        # Reshape inputs and labels to group samples of each class\n        data = data.view(-1, 28 * 28)\n        target = target.view(-1)\n\n        # Move data and labels to GPU if available\n        if torch.cuda.is_available():\n            data, target = data.cuda(), target.cuda()\n\n        # Compute embeddings\n        with torch.no_grad():\n            embeddings_batch = model(data)\n\n        # Compute loss\n        loss = criterion(embeddings_batch, target)\n\n        # Update evaluation loss\n        eval_loss += loss.item()\n\n        # Append labels and embeddings to lists\n        labels.append(target.cpu().numpy())\n        embeddings.append(embeddings_batch.cpu().numpy())\n\n    # Concatenate labels and embeddings\n    labels = np.concatenate(labels)\n    embeddings = np.concatenate(embeddings)\n\n    # Calculate average evaluation loss\n    eval_loss /= len(data_loader)\n\n    # Log evaluation statistics\n    wandb.log({\"eval_loss\": eval_loss}, step=global_step)\n\n    # Generate UMAP plot\n    fig = plot_umap(labels, embeddings)\n\n    # Log UMAP plot\n    wandb.log({\"eval_umap\": fig}, step=global_step)\n\n    # Set model back to training mode\n    model.train()\n\n    # Return average evaluation loss\n    return eval_loss", "CodeBleu_score": 30.137365807005455}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\n    model.eval()\n    avg_loss = 0\n    with torch.no_grad():\n        for batch_idx, (data, labels) in enumerate(data_loader):\n            data = data.to(device)\n            labels = labels.to(device)\n            data = data.view(data.size(0), -1)\n            labels = labels.view(-1)\n            output = model(data)\n            loss = criterion(output, labels)\n            avg_loss += loss.item()\n\n            if batch_idx == len(data_loader) - 1:\n                # Log embeddings and UMAP plot\n                embeddings = output.cpu().detach().numpy()\n                labels = labels.cpu().detach().numpy()\n                plot_umap(embeddings, labels, global_step)\n\n    avg_loss /= len(data_loader)\n    print(f\"Evaluation loss at step {global_step}: {avg_loss:.4f}\")\n    return avg_loss", "CodeBleu_score": 27.86000308376172}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n    model.eval()\n    with torch.no_grad():\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            inputs = inputs.reshape(-1, 28, 28)\n            labels = labels.reshape(-1)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            if i == len(data_loader) - 1:\n                embeddings = outputs.cpu().numpy()\n                labels = labels.cpu().numpy()\n                plot_umap(embeddings, labels, global_step)\n        avg_loss = running_loss / len(data_loader)\n        logger.info(f\"Evaluation Loss: {avg_loss:.4f}\")\n    return avg_loss\n\n", "CodeBleu_score": 28.937392221456825}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n    config_class = model_name + \"Config\"\n    try:\n        if model_name == \"xtts\":\n            from . import xtts_config\n\n            return getattr(xtts_config, config_class)\n        else:\n            from . import config\n\n            return getattr(config, config_class)\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            f\"Could not find configuration class for model '{model_name}'.\"\n        )\n\n", "CodeBleu_score": 30.94011682357907}, {"code": "def register_config(model_name: str) -> Coqpit\n    config_name = f\"{model_name}_config\"\n    config_class = None\n    for path in CONFIG_PATHS:\n        try:\n            config_module = importlib.import_module(f\"{path}.{config_name}\")\n            config_class = getattr(config_module, config_name)\n            break\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        raise ModuleNotFoundError(f\"Could not find configuration class for model '{model_name}'.\")\n    return config_class\n", "CodeBleu_score": 41.009742071515134}, {"code": "def register_config(model_name: str) -> Coqpit\n    config_name = model_name + \"_config\"\n    config_class = None\n    if model_name == \"xtts\":\n        from espnet2.bin.tts_train.config import XTTSConfig\n\n        config_class = XTTSConfig\n    else:\n        for path in [\n            \"espnet2/bin/tts_train/config.py\",\n            \"espnet2/bin/tts_train/config_vits.py\",\n            \"espnet2/bin/tts_train/config_fastspeech.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2_mol.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2_mol_hifigan.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2_mol_hifigan_vits.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2_mol_hifigan_tts.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2_mol_hifigan_tts_vits.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2_mol_hifigan_tts_vits_vits.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2_mol_hifigan_tts_vits_vits_vits.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2_mol_hifigan_tts_vits_vits_vits_vits.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2_mol_hifigan_tts_vits_vits_vits_vits_vits.py\",\n            \"espnet2/bin/tts_train/config_fastspeech2_mol_", "CodeBleu_score": 25.73044393849363}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n    # Determine the file extension\n    file_extension = os.path.splitext(config_path)[1].lower()\n\n    # Read the configuration file\n    with open(config_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    # Get the model name from the configuration\n    model_name = config_dict.get(\"model\", \"default\")\n\n    # Register the configuration class for the model\n    register_config(model_name)\n\n    # Create an instance of the configuration class\n    config_class = get_config_class(model_name)\n    config = config_class()\n\n    # Populate the configuration object with the loaded data\n    config.from_dict(config_dict)\n\n    return config", "CodeBleu_score": 31.728632893905374}, {"code": "def load_config(config_path: str) -> Coqpit\n    config_path = Path(config_path)\n    config_type = config_path.suffix.replace(\".\", \"\")\n\n    if config_type == \"yaml\":\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    elif config_type == \"json\":\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown config type: {config_type}\")\n\n    model_name = config_dict.get(\"model_name\", \"\")\n    config_class = get_config_class(model_name)\n    config = config_class()\n    config.update(config_dict)\n\n    return config", "CodeBleu_score": 39.17302612339114}, {"code": "def load_config(config_path: str) -> Coqpit\n    with open(config_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n    model_name = config_dict[\"model\"]\n    config_class = CONFIG_REGISTRY[model_name]\n    config = config_class()\n    config.update_from_dict(config_dict)\n    return config", "CodeBleu_score": 22.774996313647407}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        for filename in filenames:\n            if contains is None or filename.find(contains) != -1:\n                ext = filename[filename.rfind(\".\"):].lower()\n                if validExts is None or ext.endswith(validExts):\n                    yield os.path.join(rootDir, filename)\n\n", "CodeBleu_score": 59.43444717718015}, {"code": "def list_files(basePath, validExts=None, contains=None)\n\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # construct the full path to the file and\n            # check if it's valid\n            filePath = os.path.sep.join([rootDir, filename])\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            if validExts is not None:\n                ext = filePath[filePath.rfind(\".\"):].lower()\n                if ext.endswith(tuple(validExts)):\n                    yield filePath\n\n", "CodeBleu_score": 63.51439368428969}, {"code": "def list_files(basePath, validExts=None, contains=None)\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # construct the full path to the file and then initialize\n            # if the contains string is not none, then ignore the\n            # file if it does not contain the supplied string\n            if contains is not None and filename.find(contains) == -1:\n                continue\n            # construct the full path to the file, then check if\n            # it has a valid extension\n            filePath = os.path.join(rootDir, filename)\n            ext = filePath[filePath.rfind(\".\"):].lower()\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # yield the image path\n                yield filePath\n\n", "CodeBleu_score": 72.94140599432153}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    import os\n    import torch\n    import sys\n    from transformers import XTTSProcessor, XTTSForConditionalGeneration\n\n    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        raise ValueError(\"Please provide the XTTS checkpoint, config, and vocab paths.\")\n\n    print(\"Loading XTTS model...\")\n\n    processor = XTTSProcessor.from_pretrained(xtts_config)\n    model = XTTSForConditionalGeneration.from_pretrained(xtts_checkpoint)\n    model.load_state_dict(torch.load(xtts_checkpoint)[\"state_dict\"])\n    model.eval()\n    model.to(\"cpu\")\n\n    print(\"XTTS model loaded successfully.\")\n\n    return model, processor\n\n", "CodeBleu_score": 36.03728683905758}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    import torch\n    from transformers import XTTSConfig, XTTSForConditionalGeneration, XTTSTokenizer\n    import gc\n\n    if xtts_checkpoint == None:\n        return \"Error: No checkpoint provided\"\n    if xtts_config == None:\n        return \"Error: No config provided\"\n    if xtts_vocab == None:\n        return \"Error: No vocab provided\"\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    config = XTTSConfig.from_pretrained(xtts_config)\n    model = XTTSForConditionalGeneration(config)\n    tokenizer = XTTSTokenizer.from_pretrained(xtts_vocab)\n\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=torch.device('cpu')))\n    model.eval()\n\n    if torch.cuda.is_available():\n        model.to(torch.device('cuda'))\n\n    print(\"Model loaded successfully\")\n    return \"Model loaded successfully\"\n", "CodeBleu_score": 44.98984898983613}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    clear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: Please provide paths to the XTTS checkpoint, config, and vocab files.\"\n    config = XTTSConfig.from_json_file(xtts_config)\n    model = XTTSModel(config)\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"state_dict\"])\n    model.eval()\n    if torch.cuda.is_available():\n        model.cuda()\n    print(\"Model loaded successfully.\")\n    return \"Model loaded successfully.\"\n", "CodeBleu_score": 41.078411003619905}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    # Adjust the number of frames based on the hop length\n    if self.torch:\n        num_frames = int(num_frames * self.hop_length / self.sr)\n    # Ensure the number of frames does not exceed the input length\n    if num_frames > x.shape[0]:\n        num_frames = x.shape[0]\n    # Calculate offset positions for evaluation\n    offsets = np.linspace(0, x.shape[0] - num_frames, num=num_eval, dtype=int)\n    # Extract frames at these offsets, concatenate them into a batch, and perform inference to obtain embeddings\n    embeddings = []\n    for offset in offsets:\n        frames = x[offset:offset + num_frames]\n        frames = torch.from_numpy(frames).unsqueeze(0)\n        if self.torch:\n            frames = frames.to(self.device)\n        with torch.no_grad():\n            embedding = self.model(frames)\n        if self.torch:\n            embedding = embedding.cpu().numpy()\n        embeddings.append(embedding)\n    # Compute the mean of the embeddings and return the result, optionally normalizing them with L2 norm\n    embeddings = np.concatenate(embeddings, axis=0)\n    if return_mean:\n        embeddings = np.mean(embeddings, axis=0)\n    if l2_norm:\n        embeddings = embeddings / np.linalg.norm(embeddings)\n    return embeddings", "CodeBleu_score": 43.6811387900026}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    # Adjust number of frames based on the hop length if using torch specifications\n    if self.torch_specs:\n        num_frames = int(num_frames / self.hop_length * self.sr)\n    # Ensure the number of frames does not exceed the input length\n    if num_frames > len(x):\n        num_frames = len(x)\n    # Calculate offset positions for evaluation\n    offsets = np.arange(0, len(x) - num_frames + 1, num_frames)\n    # Extract frames at these offsets\n    frames = [x[offset:offset + num_frames] for offset in offsets]\n    # Concatenate frames into a batch\n    frames = np.stack(frames)\n    # Perform inference to obtain embeddings\n    embeddings = self.model.embed(frames)\n    # Compute the mean of the embeddings and return the result\n    if return_mean:\n        embeddings = np.mean(embeddings, axis=0)\n    # Optionally normalize the embeddings with L2 norm\n    if l2_norm:\n        embeddings = embeddings / np.linalg.norm(embeddings)\n    return embeddings\n", "CodeBleu_score": 38.58024155392016}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    # Adjust number of frames if using torch specifications\n    if self.torch_spec:\n        num_frames = self.adjust_num_frames(num_frames)\n\n    # Calculate number of frames and offsets\n    num_frames, offsets = self.get_num_frames_and_offsets(x, num_frames, num_eval)\n\n    # Extract frames at offsets\n    x = self.extract_frames(x, offsets)\n\n    # Compute embeddings\n    embeddings = self.compute_embeddings(x)\n\n    # Compute mean of embeddings\n    if return_mean:\n        embeddings = embeddings.mean(dim=1)\n\n    # Normalize embeddings with L2 norm\n    if l2_norm:\n        embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)\n\n    return embeddings\n\n", "CodeBleu_score": 27.571723048543674}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 85.18641475692186}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from sklearn.manifold import TSNE\n    from umap import UMAP\n    from sklearn.preprocessing import StandardScaler\n\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.bincount(embeddings[\"labels\"])\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.bincount(embeddings[\"labels\"])\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.bincount(embeddings[\"labels\"])\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.bincount(embeddings[\"labels\"])\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.bincount(embeddings[\"labels\"])\n\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_utterances_per_class = num_", "CodeBleu_score": 22.118637743268117}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    import umap\n    import matplotlib.pyplot as plt\n\n    # Calculate the number of utterances per class and limit the number of classes to 10 if necessary\n    num_utterances_per_class = [len(embeddings[embeddings[:, -1] == i]) for i in range(num_classes_in_batch)]\n    num_classes_to_plot = min(10, num_classes_in_batch)\n\n    # Select the embeddings for the classes to plot\n    embeddings_to_plot = embeddings[embeddings[:, -1] < num_classes_to_plot]\n\n    # UMAP transforms the embeddings\n    reducer = umap.UMAP()\n    embeddings_umap = reducer.fit_transform(embeddings_to_plot[:, :-1])\n\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots()\n    for i in range(num_classes_to_plot):\n        ax.scatter(embeddings_umap[embeddings_to_plot[:, -1] == i, 0], embeddings_umap[embeddings_to_plot[:, -1] == i, 1], label=f'Class {i}')\n    ax.legend()\n\n    # Adjust for equal aspect ratio and save the plot\n    ax.set_aspect('equal', 'box')\n    plt.title(\"UMAP projection\")\n    plt.savefig(\"umap.png\")\n\n    # Return the figure\n    return fig", "CodeBleu_score": 36.72695911675268}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from sklearn.manifold import TSNE\n    from umap import UMAP\n\n    # Calculate number of utterances per class\n    num_utterances_per_class = np.unique(num_classes_in_batch, return_counts=True)[1]\n    # Limit number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        num_classes_in_batch = num_classes_in_batch[:10]\n\n    # UMAP transforms the embeddings\n    umap_transform = UMAP(n_components=2, random_state=0)\n    embeddings = umap_transform.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots()\n    for i in range(len(num_utterances_per_class)):\n        ax.scatter(embeddings[num_classes_in_batch == i, 0], embeddings[num_classes_in_batch == i, 1], label=f\"Class {i}\")\n    ax.legend()\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    fig.tight_layout()\n    plt.savefig(\"umap.png\")\n    return fig", "CodeBleu_score": 37.46502438766771}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n    cosine_sim_matrix = []\n    for i in range(len(dvecs)):\n        # Calculate the new centroids excluding the current utterance\n        new_centroids = np.delete(centroids, i, axis=0)\n        # Calculate the cosine similarity with the new centroids\n        cosine_sim = np.dot(dvecs[i], new_centroids.T) / (np.linalg.norm(dvecs[i]) * np.linalg.norm(new_centroids, axis=1))\n        # Clamp the similarity values to avoid small values\n        cosine_sim = np.clip(cosine_sim, 0, 1)\n        # Concatenate the similarity values for the current utterance\n        cosine_sim_matrix.append(cosine_sim)\n    # Stack the similarity values to form the final cosine similarity matrix\n    cosine_sim_matrix = np.vstack(cosine_sim_matrix)\n    return cosine_sim_matrix", "CodeBleu_score": 26.143862661277538}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n    cos_sim_matrix = []\n    for speaker, utterances in dvecs.items():\n        cos_sim_list = []\n        for i, utterance in enumerate(utterances):\n            # Compute new centroids excluding the current utterance\n            new_centroids = np.delete(centroids, i, axis=0)\n\n            # Calculate cosine similarity with new centroids\n            cos_sim = np.dot(utterance, new_centroids.T) / (np.linalg.norm(utterance) * np.linalg.norm(new_centroids, axis=1))\n\n            # Clamp values to avoid small values\n            cos_sim = np.clip(cos_sim, 0, 1)\n\n            # Concatenate and stack cosine similarity values\n            cos_sim_list.append(cos_sim)\n\n        # Stack cosine similarity values for each speaker\n        cos_sim_matrix.append(np.hstack(cos_sim_list))\n\n    # Stack cosine similarity values for all speakers\n    cos_sim_matrix = np.vstack(cos_sim_matrix)\n\n    return cos_sim_matrix\n\n", "CodeBleu_score": 34.33352135897983}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n    cos_sim_mat = []\n    for i, dvec in enumerate(dvecs):\n        cos_sim = []\n        for j, centroid in enumerate(centroids):\n            if i == j:\n                cos_sim.append(0)\n            else:\n                cos_sim.append(\n                    np.clip(\n                        np.dot(dvec, centroid)\n                        / (np.linalg.norm(dvec) * np.linalg.norm(centroid)),\n                        -1,\n                        1,\n                    )\n                )\n        cos_sim_mat.append(cos_sim)\n    return np.array(cos_sim_mat)\n\n", "CodeBleu_score": 36.250953124694334}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n    # The function calculates the embedding loss using softmax.\n    # It iterates over the embeddings, computing the negative log softmax of the cosine similarity matrix for each embedding.\n    # The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\n    # The input parameters are:\n    # dvecs: a tensor of shape (num_embeddings, embedding_dim)\n    # cos_sim_matrix: a tensor of shape (num_embeddings, num_embeddings)\n    # The output is a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a tensor of shape (num_embeddings,)\n    # The function returns a", "CodeBleu_score": 2.26010101010101}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n    losses = []\n    for dvec in dvecs:\n        dvec = torch.from_numpy(dvec).to(self.device)\n        loss = -torch.log(torch.softmax(cos_sim_matrix @ dvec, dim=-1))\n        losses.append(loss)\n    return torch.stack(losses)\n", "CodeBleu_score": 29.332528762656796}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n    batch_size = dvecs.shape[0]\n    losses = []\n    for i in range(batch_size):\n        loss = -torch.log(torch.nn.functional.softmax(cos_sim_matrix[i], dim=0))\n        losses.append(loss)\n    losses = torch.stack(losses, dim=0)\n    return losses\n\n", "CodeBleu_score": 27.170526328628696}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    contrast_losses = []\n    for i, dvec in enumerate(dvecs):\n        # Exclude the current centroid\n        cos_sim_matrix_excl_centroid = cos_sim_matrix[i, :].clone()\n        cos_sim_matrix_excl_centroid[i] = -1\n        # Compute the sigmoid of the cosine similarity matrix for the current embedding\n        sigmoid_cos_sim_matrix = torch.sigmoid(cos_sim_matrix_excl_centroid)\n        # Compute the contrast loss\n        contrast_loss = torch.mean(torch.log(1 + torch.sum(sigmoid_cos_sim_matrix)))\n        contrast_losses.append(contrast_loss)\n    # Stack the contrast losses into a tensor\n    contrast_losses = torch.stack(contrast_losses)\n    return contrast_losses\n", "CodeBleu_score": 29.2810490383753}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    # Compute sigmoid of cosine similarity matrix for each embedding\n    sigmoid_matrix = torch.sigmoid(cos_sim_matrix)\n    # Initialize an empty list to store the loss row for each embedding\n    loss_row = []\n    # Iterate over the embeddings\n    for i in range(dvecs.shape[0]):\n        # Exclude the current centroid\n        exclude_centroid = torch.cat([dvecs[:i], dvecs[i+1:]])\n        # Compute the contrast loss for the current embedding\n        contrast_loss = torch.mean(torch.log(1 + torch.sum(sigmoid_matrix[i] * exclude_centroid, dim=1)))\n        # Append the contrast loss to the loss row\n        loss_row.append(contrast_loss)\n    # Stack the loss row into a tensor\n    loss_row = torch.stack(loss_row)\n    # Return the final stacked tensor of losses\n    return loss_row\n", "CodeBleu_score": 24.93559897782138}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    cos_sim_matrix = torch.sigmoid(cos_sim_matrix)\n    loss_row = torch.zeros(dvecs.shape[0], dtype=torch.float32, device=self.device)\n    for i in range(dvecs.shape[0]):\n        cos_sim_row = cos_sim_matrix[i]\n        cos_sim_row[i] = 0\n        contrast_loss = -torch.log(cos_sim_row.max())\n        loss_row[i] = contrast_loss\n    return torch.stack(loss_row)", "CodeBleu_score": 23.99386267094126}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n    # x: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # _label: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n    # loss: (num_speakers, num_utts_per_speaker)\n", "CodeBleu_score": 3.6585365853658542}, {"code": "def forward(self, x, _label=None)\n    if x.shape[1] < 2:\n        raise ValueError(\"Input data must have at least two utterances per speaker.\")\n    centroids = x.mean(dim=1)\n    sim_matrix = torch.matmul(x, centroids.transpose(0, 1))\n    sim_matrix = sim_matrix / torch.norm(x, dim=2)[:, :, None]\n    sim_matrix = sim_matrix / torch.norm(centroids, dim=1)[:, None]\n    sim_matrix = sim_matrix.clamp(min=1e-6)\n    sim_matrix = torch.log(sim_matrix)\n    loss = F.nll_loss(sim_matrix, torch.arange(x.shape[0]).to(x.device))\n    return loss", "CodeBleu_score": 30.849945162480285}, {"code": "def forward(self, x, _label=None)\n    # Ensure the input has at least two utterances per speaker\n    assert x.size(1) >= 2\n\n    # Compute centroids by averaging the embeddings\n    centroids = x.mean(dim=1)\n\n    # Calculate a cosine similarity matrix between the embeddings and centroids\n    sim_matrix = torch.matmul(x, centroids.t())\n\n    # Scale and shift the similarity matrix\n    sim_matrix = sim_matrix / self.scale\n    sim_matrix = sim_matrix + self.shift\n\n    # Compute the embedding loss\n    loss = F.cross_entropy(sim_matrix, torch.arange(sim_matrix.size(0), device=sim_matrix.device))\n\n    return loss\n\n", "CodeBleu_score": 39.46408081278351}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n    voices = {}\n    for voice_dir in extra_voice_dirs:\n        if not os.path.isdir(voice_dir):\n            print(f\"Warning: {voice_dir} is not a directory. Skipping...\")\n            continue\n        for root, dirs, files in os.walk(voice_dir):\n            if not files:\n                continue\n            subdir_name = os.path.basename(root)\n            voices[subdir_name] = []\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voices[subdir_name].append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voices[subdir_name].append(os.path.join(root, file))\n    return voices", "CodeBleu_score": 29.25726074270444}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n    voices = {}\n    for voice_dir in extra_voice_dirs + [get_data_dir()]:\n        for dirpath, dirnames, filenames in os.walk(voice_dir):\n            if len(filenames) > 0:\n                if dirpath not in voices:\n                    voices[dirpath] = []\n                for filename in filenames:\n                    if filename.endswith(\".npz\"):\n                        voices[dirpath].append(os.path.join(dirpath, filename))\n                    elif filename.endswith(\".wav\"):\n                        voices[dirpath].append(os.path.join(dirpath, filename))\n                    elif filename.endswith(\".mp3\"):\n                        voices[dirpath].append(os.path.join(dirpath, filename))\n    return voices\n", "CodeBleu_score": 26.248133044567872}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n    # Get the list of directories to search\n    dirs = [os.path.join(os.path.dirname(__file__), 'voices')] + extra_voice_dirs\n\n    # Create an empty dictionary to store the file paths\n    voices = {}\n\n    # Iterate through each directory and subdirectory\n    for dir in dirs:\n        for root, dirs, files in os.walk(dir):\n            # Get the subdirectory name\n            subdir = os.path.relpath(root, dir)\n\n            # Initialize the list of file paths for this subdirectory\n            voices[subdir] = []\n\n            # Iterate through each file in the subdirectory\n            for file in files:\n                # Get the file path\n                file_path = os.path.join(root, file)\n\n                # Check if the file is a .npz file\n                if file.endswith('.npz'):\n                    # Add the file path to the list of file paths for this subdirectory\n                    voices[subdir].append(file_path)\n                elif file.endswith('.wav') or file.endswith('.mp3'):\n                    # Add the file path to the list of file paths for this subdirectory\n                    voices[subdir].append(file_path)\n\n    # Return the dictionary of file paths\n    return voices", "CodeBleu_score": 31.585521629198933}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n    if voice == \"random\":\n        return None, None, None, None\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for {voice}\")\n    if len(voice_paths) == 0:\n        raise ValueError(f\"No voice found for {voice}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        with np.load(voice_path) as data:\n            return data[\"wave\"], data[\"pitch\"], data[\"energy\"], data[\"duration\"]\n    else:\n        voice_path = voice_path.replace(\".wav\", \".npz\")\n        if os.path.exists(voice_path):\n            with np.load(voice_path) as data:\n                return data[\"wave\"], data[\"pitch\"], data[\"energy\"], data[\"duration\"]\n        else:\n            wave, pitch, energy, duration = load_audio(voice_path)\n            np.savez(voice_path, wave=wave, pitch=pitch, energy=energy, duration=duration)\n            return wave, pitch, energy, duration", "CodeBleu_score": 45.992723847954174}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if voice_paths:\n        if len(voice_paths) > 1:\n            raise ValueError(f\"Multiple voices found for {voice}: {voice_paths}\")\n        voice_path = voice_paths[0]\n        if voice_path.endswith(\".npz\"):\n            return np.load(voice_path)\n        else:\n            npz_path = voice_path.replace(\".wav\", \".npz\")\n            if not os.path.exists(npz_path):\n                convert_audio_to_npz(voice_path, npz_path)\n            return np.load(npz_path)\n    else:\n        return None\n\n", "CodeBleu_score": 36.66931237271288}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if voice == \"random\":\n        return None, None, None, None\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices found for model {model} and voice {voice}\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        return voice_data[\"wav\"], voice_data[\"mel\"], voice_data[\"embedding\"], voice_data[\"embedding_path\"]\n    else:\n        npz_path = f\"{voice_path}.npz\"\n        if os.path.exists(npz_path):\n            voice_data = np.load(npz_path)\n            return voice_data[\"wav\"], voice_data[\"mel\"], voice_data[\"embedding\"], voice_data[\"embedding_path\"]\n        else:\n            wav, mel, embedding = get_wav_mel_embedding(model, voice_path)\n            np.savez(npz_path, wav=wav, mel=mel, embedding=embedding, embedding_path=voice_path)\n            return wav, mel, embedding, voice_path", "CodeBleu_score": 45.1124252475507}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    batch_size, seq_length = pos_score.size()\n    zeros = pos_score.new_zeros((batch_size, 1))\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, seq_length + 1, -1)\n    pos_score = padded_pos_score[:, :seq_length, :seq_length]\n    return pos_score\n", "CodeBleu_score": 38.13709398099094}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    batch_size, seq_len, _ = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, seq_len, 1)\n    padded = torch.cat([zeros, pos_score], dim=-1)\n    padded = padded.view(batch_size, seq_len + 1, -1)\n    pos_score = padded[:, :seq_len, :seq_len]\n    return pos_score\n\n", "CodeBleu_score": 30.353924831107626}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    batch_size, seq_len, _ = pos_score.size()\n    zeros = pos_score.new_zeros(batch_size, seq_len, 1)\n    padded = torch.cat([zeros, pos_score], dim=-1)\n    padded = padded.view(batch_size, seq_len + 1, -1)\n    pos_score = padded[:, :seq_len, :seq_len]\n    return pos_score\n\n", "CodeBleu_score": 30.353924831107626}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n    # Extract the context up to the specified length\n    context = context[:, :, :lens]\n\n    # Apply the convolutional layers with ReLU activation and dropout\n    for conv in self.conv_layers:\n        context = conv(context)\n        context = F.relu(context)\n        context = F.dropout(context, p=self.dropout, training=self.training)\n\n    # Pad the processed contexts to create a consistent batch size\n    context = nn.utils.rnn.pad_sequence(context, batch_first=True)\n\n    return context\n", "CodeBleu_score": 27.897648670843044}, {"code": "def run_padded_sequence(self, context, lens)\n    batch_size = context.size(0)\n    seq_len = context.size(1)\n    padded_seq = []\n    for i in range(batch_size):\n        start = max(0, seq_len - lens[i])\n        end = seq_len\n        context_i = context[i, start:end]\n        for conv in self.conv_layers:\n            context_i = F.relu(conv(context_i))\n            context_i = F.dropout(context_i, p=self.dropout, training=self.training)\n        padded_seq.append(context_i)\n    padded_seq = torch.stack(padded_seq)\n    return padded_seq", "CodeBleu_score": 32.51507687042301}, {"code": "def run_padded_sequence(self, context, lens)\n    batch_size = context.size(0)\n    max_len = context.size(1)\n    if lens is None:\n        lens = torch.tensor([max_len] * batch_size, dtype=torch.long, device=context.device)\n    output_list = []\n    for i in range(len(self.conv_layers)):\n        context = self.conv_layers[i](context)\n        output_list.append(context)\n        context = nn.utils.rnn.pack_padded_sequence(context, lens, batch_first=True, enforce_sorted=False)\n        context, _ = self.rnn_layers[i](context)\n        context, lens = nn.utils.rnn.pad_packed_sequence(context, batch_first=True)\n    output = torch.cat(output_list, dim=-1)\n    output = nn.functional.relu(output)\n    output = self.dropout(output)\n    return output", "CodeBleu_score": 30.298941311186084}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n    lens = lens.tolist()\n    lens_sorted, lens_idx = torch.sort(lens, descending=True)\n    lens_sorted = lens_sorted.tolist()\n    lens_idx = lens_idx.tolist()\n    context = [context[idx] for idx in lens_idx]\n    context = self.pack(context)\n    context = fn(context)\n    context = self.unpack(context)\n    context = [context[idx] for idx in lens_idx]\n    return context", "CodeBleu_score": 16.081441083787404}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n    lens_sorted, lens_idx = torch.sort(lens, descending=True)\n    lens_idx_sorted = lens_idx.to(lens.device)\n    context_sorted = torch.index_select(context, 0, lens_idx_sorted)\n    context_sorted_packed = nn.utils.rnn.pack_padded_sequence(context_sorted, lens_sorted, batch_first=True, enforce_sorted=False)\n    context_sorted_packed_out = fn(context_sorted_packed)\n    context_sorted_packed_out, _ = nn.utils.rnn.pad_packed_sequence(context_sorted_packed_out, batch_first=True)\n    context_sorted_out = torch.index_select(context_sorted_packed_out, 0, lens_idx)\n    return context_sorted_out", "CodeBleu_score": 21.05138493095452}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n    lens = np.array(lens)\n    lens_sorted, lens_indices = np.unique(lens, return_index=True)\n    lens_indices = lens_indices[np.argsort(lens_sorted)]\n    lens_sorted = lens_sorted[np.argsort(lens_sorted)]\n    context_sorted = context[lens_indices]\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_sorted)\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_sorted)\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_sorted)\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_sorted)\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_sorted)\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_sorted)\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_sorted)\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_sorted)\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_sorted)\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_sorted)\n    context_sorted = [c for c in context_sorted if c is not None]\n    context_sorted = np.array(context_", "CodeBleu_score": 13.084701881841474}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n    # Remove weight normalization from the kernel predictor\n    self.kernel_predictor.remove_weight_norm()\n    # Remove parameterizations from the convt_pre layer\n    self.convt_pre.remove_weight_norm()\n    # Iterate through the conv_blocks to remove parameterizations from each block's weight\n    for block in self.conv_blocks:\n        block.remove_weight_norm()", "CodeBleu_score": 33.51046275391997}, {"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n", "CodeBleu_score": 33.51046275391997}, {"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()", "CodeBleu_score": 33.51046275391997}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', '')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', '')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 46.28513747825952}, {"code": "def map_mx_to_torch_model(mx_name)\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name[6:]\n    elif mx_name == 'inceptionv3':\n        mx_name = 'inception_v3'\n    return 'gluon_' + mx_name\n", "CodeBleu_score": 31.567854720386546}, {"code": "def map_mx_to_torch_model(mx_name)\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name[3:]\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name[6:]\n    elif mx_name == 'inceptionv3':\n        mx_name = 'inception_v3'\n    return 'gluon_' + mx_name", "CodeBleu_score": 31.567854720386546}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n    if self.repeats > 1:\n        self.num_samples = int(math.ceil(self.num_samples / self.repeats))\n    if self.num_workers > 0:\n        self.num_samples = int(math.ceil(self.num_samples / self.num_workers))\n    if self.batch_size > 0:\n        self.num_samples = int(math.ceil(self.num_samples / self.batch_size)) * self.batch_size\n    return self.num_samples", "CodeBleu_score": 34.00634181725545}, {"code": "def _num_samples_per_worker(self)\n    if self.repeats > 1:\n        num_samples = (self.repeats * self.num_samples) // self.num_replicas\n        num_samples = (num_samples + self.num_replicas - 1) // self.num_replicas\n    else:\n        num_samples = self.num_samples\n\n    if self.batch_size is not None:\n        num_samples = (num_samples + self.batch_size - 1) // self.batch_size\n        num_samples *= self.batch_size\n\n    return num_samples", "CodeBleu_score": 33.606164058302504}, {"code": "def _num_samples_per_worker(self)\n    num_samples_per_worker = self.num_samples // self.num_workers\n    if self.num_workers == 0:\n        num_samples_per_worker = self.num_samples\n    if self.num_repeats > 1:\n        num_samples_per_worker = math.ceil(num_samples_per_worker / self.num_repeats)\n    if self.batch_size is not None:\n        num_samples_per_worker = (num_samples_per_worker // self.batch_size) * self.batch_size\n    return num_samples_per_worker", "CodeBleu_score": 40.23955239834682}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n    if class_to_idx is None:\n        class_to_idx = {}\n        classes = set()\n        for root, _, files in os.walk(folder, topdown=False):\n            dirname = os.path.basename(root)\n            if dirname not in classes:\n                classes.add(dirname)\n        classes_list = list(classes)\n        if sort:\n            classes_list.sort()\n        for i, class_name in enumerate(classes_list):\n            class_to_idx[class_name] = i\n\n    images_paths = []\n    targets = []\n    for root, _, files in os.walk(folder, topdown=False):\n        dirname = os.path.basename(root)\n        if dirname in class_to_idx:\n            target = class_to_idx[dirname]\n            for file in files:\n                if leaf_name_only:\n                    file = os.path.basename(file)\n                if types is None or file.lower().endswith(types):\n                    path = os.path.join(root, file)\n                    item = (path, target)\n                    images_paths.append(item)\n                    targets.append(target)\n\n    return images_paths, targets, class_to_idx\n\n", "CodeBleu_score": 37.53043749278068}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n    if class_to_idx is None:\n        unique_labels = set()\n        unique_labels.update(\n            *(d.rglob(p) for d in [Path(folder)] for p in ['*'])\n        )\n        unique_labels = {\n            tuple(x.parts)\n            for x in unique_labels\n            if x.is_dir()\n        }\n        class_to_idx = {\n            cls_name: i\n            for i, cls_name in enumerate(sorted(unique_labels))\n        }\n    elif not isinstance(class_to_idx, dict):\n        raise TypeError('class_to_idx must be a dictionary')\n\n    images_paths = []\n    images_targets = []\n    for target_class in sorted(class_to_idx.keys()):\n        class_index = class_to_idx[target_class]\n        target_folder = Path(folder) / Path(target_class)\n        if not target_folder.is_dir():\n            continue\n        for img_path in target_folder.rglob('*'):\n            if img_path.is_dir():\n                continue\n            if types is not None:\n                ext = img_path.suffix[1:].lower()\n                if types is str:\n                    types = [types]\n                if ext not in types:\n                    continue\n            if leaf_name_only:\n                img_path = img_path.name\n            images_paths.append(img_path)\n            images_targets.append(class_index)\n\n    if sort:\n        images_paths, images_targets = zip(\n            *sorted(zip(images_paths, images_targets))\n        )\n\n    return images_paths, images_targets, class_to_idx", "CodeBleu_score": 34.387081012664794}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n    if class_to_idx is None:\n        unique_labels = set()\n        # walk through the folder tree\n        for root, _, fnames in os.walk(folder, topdown=True):\n            for fname in fnames:\n                # get the class name for each image\n                label = os.path.relpath(root, folder)\n                # skip any files that do not match the desired types\n                if types is not None and not fname.lower().endswith(types):\n                    continue\n                # add the class name to the set of unique labels\n                unique_labels.add(label)\n        # create a dictionary that maps each class name to an index\n        class_to_idx = {cls_name: idx\n                        for idx, cls_name in enumerate(sorted(unique_labels))}\n    # create a list of image-path and class-index pairs\n    instances = []\n    # walk through the folder tree\n    for root, _, fnames in sorted(os.walk(folder, topdown=True)):\n        for fname in sorted(fnames):\n            # get the class name for each image\n            label = os.path.relpath(root, folder)\n            # skip any files that do not match the desired types\n            if types is not None and not fname.lower().endswith(types):\n                continue\n            # get the full path to the image file\n            path = os.path.join(root, fname)\n            # get the index of the class name\n            idx = class_to_idx[label]\n            # add the image-path and class-index pair to the list\n            instances.append((path, idx))\n    # sort the list of image-path and class-index pairs\n    if sort:\n        instances.sort(key=lambda x: x[0])\n    # return the list of image-path and class-index pairs along with the class-to-index mapping\n    return instances, class_to_idx\n\n", "CodeBleu_score": 38.74813956543552}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n    import torch.distributed as dist\n    import torch.utils.data\n    if dist.is_available() and dist.is_initialized():\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is not None:\n            seed = worker_info.seed\n        else:\n            seed = torch.initial_seed()\n    else:\n        seed = torch.initial_seed()\n    return seed\n", "CodeBleu_score": 42.083130649425016}, {"code": "def pytorch_worker_seed()\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed\n    else:\n        seed = (\n            int(os.environ.get(\"WANDB_WDS_RANK\", 0))\n            + int(os.environ.get(\"WANDB_WDS_EPOCH\", 0))\n            + int(os.environ.get(\"WANDB_WDS_ITERATION\", 0))\n        )\n    return seed\n\n", "CodeBleu_score": 43.308373578617854}, {"code": "def pytorch_worker_seed()\n    import torch\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed\n    else:\n        seed = torch.initial_seed()\n    return seed\n", "CodeBleu_score": 40.80818539336148}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n    import cv2\n    if img.mode in [\"L\", \"RGB\"]:\n        lut = [min(i + add, 255) for i in range(256)]\n        return img.point(lut)\n    return img", "CodeBleu_score": 26.987323730404896}, {"code": "def solarize_add(img, add, thresh=128, **__)\n    import numpy as np\n    import cv2\n    if img.mode in [\"L\", \"RGB\"]:\n        lut = np.arange(0, 256)\n        lut[lut < thresh] += add\n        lut[lut > 255] = 255\n        lut[lut < 0] = 0\n        return img.point(lut)\n    else:\n        return img\n\n", "CodeBleu_score": 30.619616718034802}, {"code": "def solarize_add(img, add, thresh=128, **__)\n    import numpy as np\n    if img.mode == \"L\":\n        lut = np.arange(256, dtype=np.uint8)\n        lut[lut < thresh] += add\n        return img.point(lut)\n    elif img.mode == \"RGB\":\n        lut = np.arange(256, dtype=np.uint8)\n        lut[lut < thresh] += add\n        return img.point(lut, \"RGB\")\n    else:\n        return img\n\n", "CodeBleu_score": 33.672620789583}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n    transforms_list = []\n    transforms_probabilities = []\n    for transform, probability in transforms.items():\n        transforms_list.append(transform)\n        transforms_probabilities.append(probability)\n    transforms_probabilities = np.array(transforms_probabilities) / sum(transforms_probabilities)\n    return transforms_list, transforms_probabilities\n", "CodeBleu_score": 29.78695834041246}, {"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names, transform_probs = zip(*transforms.items())\n    transform_probs = np.array(transform_probs)\n    transform_probs = transform_probs / transform_probs.sum()\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 25.580629254557564}, {"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names = []\n    transform_weights = []\n    for transform_name, transform_weight in transforms.items():\n        transform_names.append(transform_name)\n        transform_weights.append(transform_weight)\n    transform_weights = np.array(transform_weights)\n    transform_weights = transform_weights / transform_weights.sum()\n    return transform_names, transform_weights\n\n", "CodeBleu_score": 30.746544993213927}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed_img = np.zeros_like(img)\n    for i, weight in enumerate(mixing_weights):\n        mixed_img += weight * self._apply_operations(img, m)\n    mixed_img = np.clip(mixed_img, 0, 1)\n    mixed_img = Image.fromarray((mixed_img * 255).astype(np.uint8))\n    return Image.blend(img, mixed_img, m)\n\n", "CodeBleu_score": 21.65760876581916}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed_img = np.zeros(img.shape)\n    for i, w in enumerate(mixing_weights):\n        mixed_img += w * self._apply_random_sequence(img)\n    mixed_img = np.clip(mixed_img, 0, 255)\n    mixed_img = Image.fromarray(mixed_img.astype(np.uint8))\n    return Image.blend(img, mixed_img, m)\n", "CodeBleu_score": 23.832276626377347}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed_img = np.zeros_like(img)\n    for i in range(m):\n        # Apply a random sequence of operations to the image\n        for op in self.random_ops:\n            img = op(img)\n\n        # Accumulate the result\n        mixed_img += img\n\n        # Reset the image to its original state\n        img = self.original_img.copy()\n\n    # Clip the mixed image to valid pixel values\n    mixed_img = np.clip(mixed_img, 0, 255)\n\n    # Convert the mixed image back to an image\n    mixed_img = Image.fromarray(mixed_img.astype(np.uint8))\n\n    # Blend the mixed image with the original image\n    blended_img = Image.blend(self.original_img, mixed_img, m)\n\n    return blended_img", "CodeBleu_score": 29.16969693904299}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n    if not isinstance(x, (tuple, list)):\n        x = (x, ) * n\n    elif len(x) != n:\n        x = tuple(x)\n        x = (sum(x) / len(x), ) * n\n        print(f'Warning: input x has length {len(x)}, but n is {n}. The mean of x has been used to adapt x to n.')\n    else:\n        x = tuple(x)\n    assert len(x) == n, f'Error: input x has length {len(x)}, but n is {n}.'\n    return x\n", "CodeBleu_score": 44.222696252009776}, {"code": "def adapt_to_chs(x, n)\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            return x\n        elif len(x) == 1:\n            return tuple(x[0] for _ in range(n))\n        else:\n            logging.warning(f\"Input x has length {len(x)}, but n is {n}. The mean of x will be repeated {n} times.\")\n            return tuple(np.mean(x) for _ in range(n))\n    else:\n        return tuple(x for _ in range(n))\n", "CodeBleu_score": 26.83322582059527}, {"code": "def adapt_to_chs(x, n)\n    if not isinstance(x, (tuple, list)):\n        x = (x, ) * n\n    elif len(x) != n:\n        import logging\n        logging.warning(f\"Input x has length {len(x)}, but n is {n}. Using mean of x instead.\")\n        x = (sum(x) / len(x), ) * n\n    assert len(x) == n\n    return x\n", "CodeBleu_score": 41.04851624344098}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    worker_id = worker_info.id\n    if callable(worker_seeding):\n        seed = worker_seeding(worker_id)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    elif worker_seeding == 'all':\n        seed = worker_info.seed\n        np.random.seed(seed)\n    else:\n        raise ValueError(f'Invalid worker_seeding: {worker_seeding}')\n\n", "CodeBleu_score": 47.41983111177144}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_id is None:\n        worker_id = worker_info.id\n    if worker_seeding == 'all':\n        seed = worker_info.seed % 2**32\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        np.random.seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_id)\n    else:\n        np.random.seed(worker_seeding)", "CodeBleu_score": 51.764296016438884}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        return\n    if worker_id is not None:\n        assert worker_id == worker_info.id\n    if worker_seeding is not None:\n        if callable(worker_seeding):\n            seed = worker_seeding()\n        elif worker_seeding == 'all':\n            seed = worker_info.seed\n        else:\n            raise ValueError(f'Invalid worker_seeding {worker_seeding}')\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n\n", "CodeBleu_score": 45.76897519295052}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + PADDING)) * tf.cast(tf.shape(image)[:2], tf.float32)),\n        tf.int32,\n    )\n    offset_height = ((tf.shape(image)[0] - padded_center_crop_size) + 1) // 2\n    offset_width = ((tf.shape(image)[1] - padded_center_crop_size) + 1) // 2\n    cropped_image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size, padded_center_crop_size\n    )\n    return resize_method(cropped_image, [image_size, image_size])\n\n", "CodeBleu_score": 36.647490335818894}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    shape = tf.shape(image)\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = tf.cast(\n        tf.reduce_min(\n            [image_height, image_width]\n        ) * (PADDED_CENTER_CROP_SIZE / min(image_height, image_width)), tf.int32\n    )\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    image = tf.image.crop_to_bounding_box(\n        image, offset_height, offset_width, padded_center_crop_size, padded_center_crop_size\n    )\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 44.18208206149457}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    padded_center_crop_size = tf.cast(\n        tf.cast(tf.shape(image)[:2], tf.float32) * (4 / 3),\n        tf.int32\n    )\n    image = tf.image.resize_with_crop_or_pad(\n        image,\n        padded_center_crop_size[0] + (PADDING_CONSTANT * 2),\n        padded_center_crop_size[1] + (PADDING_CONSTANT * 2)\n    )\n    offset_height = ((tf.shape(image)[0] - image_size[0]) + 1) // 2\n    offset_width = ((tf.shape(image)[1] - image_size[1]) + 1) // 2\n    image = tf.image.crop_to_bounding_box(\n        image,\n        offset_height,\n        offset_width,\n        image_size[0],\n        image_size[1]\n    )\n    image = tf.image.resize(image, image_size, method=resize_method)\n    return image\n\n", "CodeBleu_score": 32.484859206882206}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image = tf.image.random_flip_left_right(image)\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n", "CodeBleu_score": 31.97621968132143}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.resize(image, (image_size, image_size), method=interpolation)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n", "CodeBleu_score": 39.718006260166014}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    # Decode the image bytes\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    # Randomly crop the image\n    image = tf.image.resize_with_crop_or_pad(image, image_size, image_size)\n    # Flip the image\n    image = tf.image.random_flip_left_right(image)\n    # Reshape the image to the target size\n    image = tf.reshape(image, (image_size, image_size, 3))\n    # Convert the image to the specified dtype\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n", "CodeBleu_score": 34.84898989656661}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image = tf.image.central_crop(image, central_fraction=0.875)\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 33.633331437092615}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, image_size, method=interpolation)\n    image = tf.image.central_crop(image, 0.875)\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    return image", "CodeBleu_score": 35.14443403027441}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    image = center_crop(image, image_size, image_size)\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    return image\n", "CodeBleu_score": 42.474993380741694}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Run prediction loop\n    predictions = []\n    for batch in eval_dataloader:\n        input_ids = batch['input_ids'].to(self.device)\n        attention_mask = batch['attention_mask'].to(self.device)\n        labels = batch['labels'].to(self.device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        predictions.extend(outputs.logits.argmax(dim=1).tolist())\n\n    # Save predictions to file\n    with open(f\"{output_file}.txt\", \"w\") as f:\n        for prediction in predictions:\n            f.write(str(prediction) + \"\\n\")\n\n    # Compute evaluation metrics\n    with open(f\"{output_file}.txt\", \"r\") as f:\n        gold_labels = [int(line.strip()) for line in f]\n    metrics = self.compute_metrics(gold_labels, predictions)\n\n    return metrics", "CodeBleu_score": 42.207682066812275}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Initialize a list to store the predictions\n    predictions = []\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Disable gradient computation\n    with torch.no_grad():\n        # Loop through the dataloader\n        for batch in eval_dataloader:\n            # Move the batch to the appropriate device\n            batch = {k: v.to(self.device) for k, v in batch.items()}\n\n            # Run the model on the batch\n            outputs = model(**batch)\n\n            # Get the predicted class probabilities\n            probs = F.softmax(outputs.logits, dim=-1)\n\n            # Get the predicted class labels\n            preds = torch.argmax(probs, dim=-1)\n\n            # Append the predictions to the list\n            predictions.extend(preds.tolist())\n\n    # Convert the predictions to a DataFrame\n    predictions_df = pd.DataFrame({\"predictions\": predictions})\n\n    # Save the predictions to a file\n    predictions_df.to_csv(f\"{output_file}.csv\", index=False)\n\n    # Compute evaluation metrics\n    accuracy = accuracy_score(self.eval_df[\"label\"], predictions_df[\"predictions\"])\n    f1 = f1_score(self.eval_df[\"label\"], predictions_df[\"predictions\"], average=\"macro\")\n    precision = precision_score(self.eval_df[\"label\"], predictions_df[\"predictions\"], average=\"macro\")\n    recall = recall_score(self.eval_df[\"label\"], predictions_df[\"predictions\"], average=\"macro\")\n\n    # Print the evaluation metrics\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n\n    # Return the evaluation metrics\n", "CodeBleu_score": 36.86453753627062}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    # Set model to evaluation mode\n    model.eval()\n\n    # Get the evaluation dataloader\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    # Run the prediction loop\n    outputs = []\n    for batch in tqdm(eval_dataloader):\n        # Move the batch to the device\n        batch = {k: v.to(self.device) for k, v in batch.items()}\n\n        # Run the model\n        with torch.no_grad():\n            output = model(**batch)\n\n        # Save the output\n        outputs.append(output)\n\n    # Concatenate the outputs\n    outputs = torch.cat(outputs)\n\n    # Save the outputs to a file\n    torch.save(outputs, f\"{output_file}.pt\")\n\n    # Compute evaluation metrics\n    metrics = self.compute_metrics(outputs)\n\n    return metrics\n\n", "CodeBleu_score": 48.68354213492317}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('rank1: {:.4f}'.format(rank1))\n    print('rank5: {:.4f}'.format(rank5))\n    print('miou: {:.4f}'.format(miou))\n\n  def print_computed_metrics_for_each_class(self, metrics):\n    \"\"\"The function prints the computed metrics for each class, including the class name, rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    for class_name, class_metrics in metrics.items():\n      rank1 = class_metrics['rank1']\n      rank5 = class_metrics['rank5']\n      miou = class_metrics['miou']\n      print('{}: rank1: {:.4f} rank5: {:.4f} miou: {:.4f}'.format(class_name, rank1, rank5, miou))", "CodeBleu_score": 35.47501871178153}, {"code": " def print_computed_metrics(self, metrics)\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    print(\"rank1: {:.4f}\".format(rank1))\n    print(\"rank5: {:.4f}\".format(rank5))\n    print(\"miou: {:.4f}\".format(miou))", "CodeBleu_score": 37.18411150562123}, {"code": " def print_computed_metrics(self, metrics)\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('rank1: {:.4f}, rank5: {:.4f}, miou: {:.4f}'.format(rank1, rank5, miou))\n\n\n", "CodeBleu_score": 28.019739745089772}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n    rank1 = 0\n    rank5 = 0\n    mean_iou = 0\n    for segment in segments:\n        iou_list = []\n        for data_item in data:\n            iou = self._get_iou(segment, data_item)\n            iou_list.append(iou)\n        iou_list.sort(reverse=True)\n        rank1 += iou_list[0]\n        rank5 += sum(iou_list[:5])\n        mean_iou += sum(iou_list) / len(iou_list)\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    mean_iou /= len(segments)\n    return rank1, rank5, mean_iou\n", "CodeBleu_score": 28.359753500863476}, {"code": "def _eval_predictions(self, segments, data)\n    # Initialize variables\n    rank1 = 0\n    rank5 = 0\n    iou_sum = 0\n    # Iterate over segments and data\n    for segment, data_segment in zip(segments, data):\n        # Compute IoU between segment and data segment\n        iou = self._compute_iou(segment, data_segment)\n        # Add IoU to sum\n        iou_sum += iou\n        # If IoU is greater than or equal to 1, increment rank1\n        if iou >= 1:\n            rank1 += 1\n        # If IoU is greater than or equal to 0.5, increment rank5\n        if iou >= 0.5:\n            rank5 += 1\n    # Compute mean IoU\n    mean_iou = iou_sum / len(segments)\n    # Return rank1, rank5, and mean IoU\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 21.299074812880267}, {"code": "def _eval_predictions(self, segments, data)\n    # Calculate IoUs for all segments\n    ious = []\n    for segment in segments:\n        iou = self._eval_segment(segment, data)\n        ious.append(iou)\n\n    # Calculate average IoU\n    mean_iou = sum(ious) / len(ious)\n\n    # Calculate rank for each segment\n    ranks = []\n    for i, segment in enumerate(segments):\n        rank = self._eval_rank(segment, data, ious)\n        ranks.append(rank)\n\n    # Calculate rank1 and rank5\n    rank1 = sum([1 for rank in ranks if rank == 1]) / len(ranks)\n    rank5 = sum([1 for rank in ranks if rank <= 5]) / len(ranks)\n\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 29.27605648624893}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    scores = []\n    model.eval()\n    model.to(self.device)\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            batch = tuple(t.to(self.device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n            logits = model(input_ids, segment_ids, input_mask, labels=None)\n            scores.extend(logits.cpu().numpy())\n            if output_file is not None:\n                with open(output_file, 'w') as f:\n                    json.dump(scores, f)\n    return scores\n", "CodeBleu_score": 33.846829977341955}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    scores = []\n    model.eval()\n    model.to(self.device)\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            data = batch[0].to(self.device)\n            mask = batch[1].to(self.device)\n            output = model(data, mask)\n            scores.append(output.detach().cpu().numpy())\n    scores = np.concatenate(scores, axis=0)\n    if output_file is not None:\n        np.save(output_file, scores)\n    return scores", "CodeBleu_score": 38.79162700522031}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    scores = []\n    model.eval()\n    model = model.to(self.device)\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader):\n            input_ids = batch[\"input_ids\"].to(self.device)\n            attention_mask = batch[\"attention_mask\"].to(self.device)\n            token_type_ids = batch[\"token_type_ids\"].to(self.device)\n            output = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n            )\n            scores.extend(output.logits.detach().cpu().numpy())\n    if output_file is not None:\n        with open(output_file, \"w\") as f:\n            for score in scores:\n                f.write(str(score) + \"\\n\")\n    return scores", "CodeBleu_score": 35.37792076320086}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2\n    video_scores = scores[0]\n    text_scores = scores[1]\n    full_scores = torch.cat([video_scores, text_scores], dim=0)\n    self.full_scores = None\n    return torch.mm(video_scores, text_scores.t())", "CodeBleu_score": 39.23436136160882}, {"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2\n    text_scores = scores[0]\n    video_scores = scores[1]\n    assert text_scores.shape[0] == video_scores.shape[0]\n    full_scores = torch.cat((text_scores, video_scores), dim=0)\n    self.full_scores = full_scores\n    return torch.mm(text_scores, video_scores.t())\n", "CodeBleu_score": 36.64073601956432}, {"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2\n    text_scores = torch.cat(scores[0], dim=0)\n    video_scores = torch.cat(scores[1], dim=0)\n    self.full_scores = None\n    return torch.matmul(text_scores, video_scores.T)", "CodeBleu_score": 60.00837001042325}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n\n    # Check for prediction errors\n    errors = np.where(Y_pred != Y_true)\n    if len(errors[0]) > 0:\n        print(\"Prediction errors:\")\n        for i in range(len(errors[0])):\n            print(f\"Sample {errors[0][i]}: Predicted {Y_pred[errors[0][i]]}, True {Y_true[errors[0][i]]}\")\n\n    # Save predictions and true labels as a pickle file\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 37.035437188049656}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    Y_pred = np.concatenate(Y_pred)\n    Y_true = np.concatenate(Y_true)\n    error_indices = np.where(Y_pred != Y_true)[0]\n    if len(error_indices) > 0:\n        print(f\"{len(error_indices)} errors found.\")\n        print(f\"First 10 errors: {error_indices[:10]}\")\n        print(f\"Last 10 errors: {error_indices[-10:]}\")\n    if output_file is not None:\n        pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, open(output_file, \"wb\"))\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 29.807662878270268}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    if len(Y_pred) != len(Y_true):\n        raise ValueError(\"Number of predictions and true labels do not match.\")\n    if output_file:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n    errors = []\n    for i in range(len(Y_pred)):\n        if Y_pred[i] != Y_true[i]:\n            errors.append(i)\n    if errors:\n        print(\"Found {} errors in predictions:\".format(len(errors)))\n        for i in errors[:10]:\n            print(\"Prediction: {}, True label: {}\".format(Y_pred[i], Y_true[i]))\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 41.20264083002476}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get('loss', 0) for log in logging_outputs)\n    sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n    loss_sum /= sample_size\n    metrics.log_scalar('loss', loss_sum, sample_size, round=3)\n\n", "CodeBleu_score": 51.12247454877268}, {"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    loss_sum /= sample_size\n    metrics.log_scalar(\"loss\", loss_sum, sample_size, round=3)\n\n    if len(logging_outputs) > 0 and \"ntokens\" in logging_outputs[0]:\n        ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n        metrics.log_scalar(\"nll_loss\", loss_sum / math.log(2), ntokens, round=3)\n\n    if sample_size != ntokens:\n        metrics.log_scalar(\n            \"nll_loss\", loss_sum / math.log(2) * sample_size / ntokens, ntokens, round=3\n        )\n\n    if sample_size != ntokens:\n        metrics.log_scalar(\n            \"learning_rate\",\n            (2 ** self.cfg.optimization.lr_scheduler.get(\"exponent\", -1))\n            * self.cfg.optimization.lr_scheduler.get(\"base_lr\", 1e-5)\n            * self.cfg.optimization.lr_scheduler.get(\"total_num_update\", 1)\n            / sample_size\n            / ntokens,\n            ntokens,\n            round=3,\n        )\n    else:\n        metrics.log_scalar(\n            \"learning_rate\",\n            (2 ** self.cfg.optimization.lr_scheduler.get(\"exponent\", -1))\n            * self.cfg.optimization.lr_scheduler.get(\"base_lr\", 1e-5)\n            * self.cfg.optimization.lr_scheduler.get(\"total_num_update\", 1)\n            / sample_size,\n            sample_size,\n            round=3,\n        )\n\n    if sample_size != ntokens:\n        metrics.log_", "CodeBleu_score": 48.702662923314634}, {"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3)\n\n", "CodeBleu_score": 60.837287134385654}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=1)\n    token_type_ids = torch.cat([torch.zeros_like(vmasks), torch.ones_like(cmasks)], dim=1)\n    return attention_mask, token_type_ids", "CodeBleu_score": 25.434930551458244}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = torch.cat([cmasks, vmasks], dim=-1)\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=-1)\n    return attention_mask, token_type_ids", "CodeBleu_score": 25.47717600231397}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    # Create attention mask\n    if attention_mask is None:\n        attention_mask = torch.zeros_like(cmasks)\n        attention_mask = attention_mask.masked_fill(vmasks == 0, -100)\n        attention_mask = attention_mask.masked_fill(cmasks == 0, -100)\n    else:\n        attention_mask = attention_mask.masked_fill(vmasks == 0, -100)\n        attention_mask = attention_mask.masked_fill(cmasks == 0, -100)\n    # Create token type IDs\n    token_type_ids = torch.zeros_like(cmasks)\n    token_type_ids = token_type_ids.masked_fill(vmasks == 0, 1)\n    token_type_ids = token_type_ids.masked_fill(cmasks == 0, 0)\n    return attention_mask, token_type_ids", "CodeBleu_score": 30.187517165464516}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n\n    input_ids = input_ids.to(self.device)\n    input_video_embeds = input_video_embeds.to(self.device)\n    attention_mask = attention_mask.to(self.device)\n    token_type_ids = token_type_ids.to(self.device)\n\n    # Calculate the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.shape[1] + input_video_embeds.shape[1]\n\n    # Adjust the sequence length to match the combined length\n    input_ids = input_ids[:, -combined_length:]\n    input_video_embeds = input_video_embeds[:, -combined_length:]\n    attention_mask = attention_mask[:, -combined_length:]\n    token_type_ids = token_type_ids[:, -combined_length:]\n\n    # Modify the attention_mask and token_type_ids to fit the new sequence length\n    attention_mask = torch.cat([torch.ones_like(input_ids), torch.zeros_like(input_video_embeds)], dim=1)\n    token_type_ids = torch.cat([torch.zeros_like(input_ids), torch.ones_like(input_video_embeds)], dim=1)\n\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }", "CodeBleu_score": 44.36219000385401}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    # Adjust sequence length to match the combined length of input_ids and input_video_embeds\n    combined_length = len(input_ids) + len(input_video_embeds)\n    input_ids = input_ids + [self.tokenizer.pad_token_id] * (combined_length - len(input_ids))\n    input_video_embeds = input_video_embeds + [0.0] * (combined_length - len(input_video_embeds))\n\n    # Modify attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = attention_mask + [0] * (combined_length - len(attention_mask))\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids + [0] * (combined_length - len(token_type_ids))\n\n    # Return the updated inputs as a dictionary\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }", "CodeBleu_score": 45.65315556127079}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\n    # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.shape[1] + input_video_embeds.shape[1]\n    input_ids = input_ids[:, -combined_length:]\n    input_video_embeds = input_video_embeds[:, -combined_length:]\n\n    # Modify the attention_mask and token_type_ids to fit the new sequence length\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, -combined_length:]\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids[:, -combined_length:]\n\n    # Create a dictionary with the updated input_ids, input_video_embeds, attention_mask, and token_type_ids\n    inputs = {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n    # Update the dictionary with any additional model_kwargs\n    inputs.update(model_kwargs)\n\n    return inputs", "CodeBleu_score": 47.93079623324267}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n    pipeline = cfg.data.test.pipeline\n    if \"img_id\" in pipeline[0].meta_keys:\n        pipeline[0].meta_keys.remove(\"img_id\")\n    load_img_idx = next(\n        (\n            i\n            for i, transform in enumerate(pipeline)\n            if transform.type == \"LoadImageFromFile\"\n        ),\n        None,\n    )\n    if load_img_idx is None:\n        raise ValueError(\"LoadImageFromFile transform not found in pipeline\")\n    pipeline[load_img_idx].type = \"mmdet.InferencerLoader\"\n    return Compose(pipeline)\n\n", "CodeBleu_score": 33.70595234205122}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n    pipeline = copy.deepcopy(cfg.pipeline)\n    if 'img_id' in pipeline[0].meta_keys:\n        pipeline[0].meta_keys.remove('img_id')\n    load_image_from_file_idx = next(\n        (i for i, t in enumerate(pipeline) if t.type == 'LoadImageFromFile'),\n        None)\n    if load_image_from_file_idx is None:\n        raise ValueError(\n            'LoadImageFromFile is not found in the pipeline, please check the '\n            'pipeline config')\n    pipeline[load_image_from_file_idx].type = 'mmdet.InferencerLoader'\n    return Compose(pipeline)", "CodeBleu_score": 38.41204119666454}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n    pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    if \"img_id\" in cfg.data.test.meta_keys:\n        pipeline[0][\"meta_keys\"].remove(\"img_id\")\n    loader_idx = pipeline[0][\"type\"].find(\"Loader\")\n    if loader_idx == -1:\n        raise ValueError(\n            f\"Can not find Loader in {pipeline[0]['type']}\")\n    pipeline[0][\"type\"] = \"mmdet.InferencerLoader\"\n    return Compose(pipeline)", "CodeBleu_score": 30.500095451737298}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, (str, Path)):\n        if os.path.isdir(inputs):\n            inputs = list(Path(inputs).glob(\"*.*\"))\n        else:\n            inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 39.33148364987933}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, (str, Path)):\n        if is_dir(inputs):\n            inputs = list_images(inputs)\n        else:\n            inputs = [inputs]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 36.244613977720455}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if f.endswith('.jpg')]\n    if isinstance(inputs, (list, tuple)):\n        inputs = list(inputs)\n    else:\n        inputs = [inputs]\n    return inputs", "CodeBleu_score": 43.41305643373581}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    chunk = []\n    for i, input in enumerate(inputs):\n        if i % chunk_size == 0 and i > 0:\n            yield chunk\n            chunk = []\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            chunk.append(self.pipeline(input))\n        else:\n            chunk.append(input)\n    if len(chunk) > 0:\n        yield chunk\n", "CodeBleu_score": 26.780963406480357}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            chunk.append(input)\n        else:\n            if len(chunk) > 0:\n                yield chunk\n            chunk = []\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if len(chunk) > 0:\n        yield chunk", "CodeBleu_score": 24.38327188981147}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    chunk = []\n    for i, input in enumerate(inputs):\n        if i % chunk_size == 0 and i > 0:\n            yield chunk\n            chunk = []\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            chunk.append(self.pipeline(input))\n        else:\n            chunk.append(input)\n    if chunk:\n        yield chunk\n\n", "CodeBleu_score": 26.984748574810514}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if pred_out_dir and return_datasamples:\n        logger.warning('Saving datasamples is not supported when return_datasamples is True.')\n    if not no_save_pred:\n        if not os.path.exists(pred_out_dir):\n            os.makedirs(pred_out_dir)\n    results = dict()\n    if return_datasamples:\n        results['datasamples'] = []\n    if not no_save_pred:\n        results['predictions'] = []\n    if visualization is not None:\n        results['visualization'] = []\n    if isinstance(preds, list):\n        for pred in preds:\n            if isinstance(pred, list):\n                for p in pred:\n                    if isinstance(p, dict):\n                        self.postprocess_single(p, results, visualization, return_datasamples, print_result, pred_out_dir, **kwargs)\n                    else:\n                        self.postprocess_single(p, results, visualization, return_datasamples, print_result, pred_out_dir, **kwargs)\n            elif isinstance(pred, dict):\n                self.postprocess_single(pred, results, visualization, return_datasamples, print_result, pred_out_dir, **kwargs)\n            else:\n                raise TypeError('preds must be a list of dict or list of list of dict.')\n    elif isinstance(preds, dict):\n        self.postprocess_single(preds, results, visualization, return_datasamples, print_result, pred_out_dir, **kwargs)\n    else:\n        raise TypeError('preds must be a list of dict or dict.')\n    if print_result:\n        self.print_result(results)\n    return results", "CodeBleu_score": 38.032934277975194}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if pred_out_dir == '' and not no_save_pred:\n        raise ValueError('pred_out_dir must be provided when no_save_pred is False')\n    if pred_out_dir != '' and return_datasamples:\n        warnings.warn('Saving datasamples is not supported when return_datasamples is True')\n\n    results = {}\n    if return_datasamples:\n        results['datasamples'] = preds\n    else:\n        results['predictions'] = self.pred2dict(preds)\n\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    if print_result:\n        print(results)\n\n    if pred_out_dir != '' and not no_save_pred:\n        self.save_pred(preds, pred_out_dir)\n\n    return results", "CodeBleu_score": 47.884794652125265}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if pred_out_dir and return_datasamples:\n        logger.warning(\"Saving datasamples is not supported when return_datasamples=True\")\n\n    if no_save_pred and return_datasamples:\n        logger.warning(\"Saving datasamples is not supported when no_save_pred=True\")\n\n    if no_save_pred and pred_out_dir:\n        logger.warning(\"Saving predictions is not supported when no_save_pred=True\")\n\n    if not pred_out_dir and not return_datasamples:\n        logger.warning(\"Saving predictions is not supported when pred_out_dir=''\")\n\n    result = {}\n    if return_datasamples:\n        result[\"datasamples\"] = []\n\n    if preds is not None:\n        if not isinstance(preds, list):\n            preds = [preds]\n        if not isinstance(visualization, list):\n            visualization = [visualization]\n\n        for i, pred in enumerate(preds):\n            pred_dict = self.postprocess_single(pred, **kwargs)\n            if return_datasamples:\n                result[\"datasamples\"].append(pred_dict)\n            else:\n                result.update(pred_dict)\n\n            if visualization[i] is not None:\n                result[\"visualization\"] = visualization[i]\n\n        if print_result:\n            self.print_result(result)\n\n        if not no_save_pred:\n            if pred_out_dir:\n                self.save_pred(pred_out_dir, result)\n            else:\n                self.save_pred(self.work_dir, result)\n\n    return result", "CodeBleu_score": 37.97711948180229}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    test_pipeline = copy.deepcopy(cfg.data.test.pipeline)\n    test_pipeline[0] = dict(type=\"Resize\", **test_pipeline[0])\n    test_pipeline[-1] = dict(type=\"Collect\", **test_pipeline[-1])\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline", "CodeBleu_score": 20.60638915413633}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    dataset_pipeline = cfg.dataset.pipeline\n    test_pipeline = deepcopy(dataset_pipeline)\n    test_pipeline.transforms = [\n        test_pipeline.transforms[0],\n        Compose([test_pipeline.transforms[-1]]),\n    ]\n    test_pipeline = Compose(test_pipeline.transforms)\n    return test_pipeline\n\n", "CodeBleu_score": 19.139980034524218}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    test_pipeline = deepcopy(cfg.data.test.pipeline)\n    test_pipeline[0][\"type\"] = \"Resize\"\n    test_pipeline.insert(0, test_pipeline.pop(-1))\n    test_pipeline = Compose(test_pipeline)\n    return test_pipeline", "CodeBleu_score": 17.39217694746623}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    img = img.astype(np.float32) / 255.0\n    img = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).contiguous()\n    data = {\n        \"img\": img,\n        \"frame_id\": torch.tensor([frame_id], dtype=torch.int64),\n        \"ori_shape\": torch.tensor(img.shape[2:]),\n        \"img_id\": torch.tensor([0]),\n        \"video_len\": torch.tensor([video_len]),\n    }\n    if not model.cfg.model.get(\"_is_on_cpu\", False):\n        model.cfg.model.backbone.update(\n            _remove_roi_align=True, _remove_rpn_head=True\n        )\n    data = model.test_pipeline(data)\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result", "CodeBleu_score": 35.33713782099046}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    if isinstance(img, np.ndarray):\n        img = torch.from_numpy(img)\n    img = img.unsqueeze(0)\n    data = dict(img=img, frame_id=frame_id,\n                img_shape=img.shape[2:],\n                ori_shape=img.shape[2:],\n                pad_shape=img.shape[2:],\n                img_id=frame_id,\n                video_len=video_len)\n    data = model.test_pipeline(data)\n    data = model.forward(return_loss=False, rescale=True, **data)\n    return data", "CodeBleu_score": 35.82635895160568}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    img_info = dict(\n        filename=\"\",\n        frame_id=frame_id,\n        original_shape=img.shape,\n        img_id=0,\n        video_len=video_len,\n    )\n    data = dict(img=img, img_info=img_info)\n    data = model.test_step(data)\n    return data[\"results\"]", "CodeBleu_score": 15.377600978305233}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.2280525202322}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n    return tokens_a, tokens_b\n\n", "CodeBleu_score": 78.24694332028959}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop(rng.randrange(len(tokens_a)))\n        else:\n            tokens_b.pop(rng.randrange(len(tokens_b)))\n\n", "CodeBleu_score": 48.191004799529225}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n    if type(activation_string) != str:\n        return activation_string\n    if activation_string == \"\":\n        return None\n    if activation_string == \"linear\":\n        return lambda x: x\n    if activation_string == \"sigmoid\":\n        return lambda x: 1 / (1 + np.exp(-x))\n    if activation_string == \"relu\":\n        return lambda x: np.maximum(0, x)\n    if activation_string == \"leaky_relu\":\n        return lambda x: np.maximum(0.01 * x, x)\n    if activation_string == \"softmax\":\n        return lambda x: np.exp(x) / np.sum(np.exp(x))\n    raise ValueError(\"Unsupported activation function: \" + activation_string)\n\n", "CodeBleu_score": 41.08191931487429}, {"code": "def get_activation(activation_string)\n    if not isinstance(activation_string, str):\n        return activation_string\n    if activation_string == \"\":\n        return None\n    activation_string = activation_string.lower()\n    if activation_string == \"relu\":\n        return relu\n    elif activation_string == \"sigmoid\":\n        return sigmoid\n    elif activation_string == \"softmax\":\n        return softmax\n    elif activation_string == \"tanh\":\n        return tanh\n    elif activation_string == \"linear\":\n        return linear\n    else:\n        raise ValueError(f\"Activation function {activation_string} not supported.\")\n\n", "CodeBleu_score": 42.69198504526152}, {"code": "def get_activation(activation_string)\n    if not isinstance(activation_string, str):\n        return activation_string\n    if activation_string == \"\":\n        return None\n    if activation_string == \"linear\":\n        return lambda x: x\n    if activation_string == \"relu\":\n        return lambda x: x * (x > 0)\n    if activation_string == \"sigmoid\":\n        return lambda x: 1 / (1 + np.exp(-x))\n    if activation_string == \"tanh\":\n        return lambda x: np.tanh(x)\n    if activation_string == \"softmax\":\n        return lambda x: np.exp(x) / np.sum(np.exp(x))\n    raise ValueError(\"Unsupported activation function: {}\".format(activation_string))\n\n", "CodeBleu_score": 41.74037724208054}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)", "CodeBleu_score": 53.03103381915149}, {"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is not None and dropout_prob > 0.0:\n        return tf.nn.dropout(input_tensor, dropout_prob)\n    else:\n        return input_tensor", "CodeBleu_score": 40.65254582443107}, {"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n", "CodeBleu_score": 53.03103381915149}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = from_tensor.shape\n    projection_dim = from_shape[-1]\n    to_shape = to_mask.shape\n\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length, projection_dim]),\n        tf.float32,\n    )\n    from_tensor = tf.ones(\n        (batch_size, from_seq_length, 1, projection_dim), dtype=tf.float32\n    )\n    attention_mask = tf.matmul(from_tensor, to_mask)\n\n    return attention_mask\n\n", "CodeBleu_score": 37.36390462743719}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_tensor = tf.cast(from_tensor, dtype=tf.float32)\n    to_mask = tf.cast(to_mask, dtype=tf.float32)\n    to_mask = tf.reshape(to_mask, [-1, 1, tf.shape(from_tensor)[-1]])\n    return tf.minimum(from_tensor, to_mask)\n\n", "CodeBleu_score": 13.81148940376889}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = from_tensor.shape\n    proj_shape = [-1, from_shape[1]]\n    batch_size = proj_shape[0]\n    seq_length = proj_shape[1]\n    ones = tf.ones\n    to_mask = tf.reshape(to_mask, [-1])\n    to_mask = tf.cast(to_mask, tf.float32)\n    to_mask = tf.reshape(to_mask, [batch_size, 1, seq_length])\n    to_mask = tf.transpose(to_mask, [2, 0, 1])\n    to_mask = tf.tile(to_mask, [1, tf.shape(from_tensor)[1], 1])\n    return to_mask\n", "CodeBleu_score": 23.155731543535634}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    input_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    input_tensor = tf.transpose(input_tensor, [0, 2, 1, 3])\n    return input_tensor", "CodeBleu_score": 42.61845112285781}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = input_tensor.reshape([batch_size, seq_length, num_attention_heads, width])\n    output_tensor = output_tensor.transpose([0, 2, 1, 3])\n    return output_tensor\n", "CodeBleu_score": 44.24622760766943}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = input_tensor.reshape(batch_size, seq_length, num_attention_heads, width)\n    output_tensor = output_tensor.transpose(0, 2, 1, 3)\n    return output_tensor", "CodeBleu_score": 34.365746274706325}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n    input_tensor_shape = input_tensor.get_shape().as_list()\n    if len(input_tensor_shape) < 2:\n        raise ValueError(\"Input tensor must have rank >= 2. Current rank: {}\".format(len(input_tensor_shape)))\n    if len(input_tensor_shape) == 2:\n        return input_tensor\n    else:\n        width = input_tensor_shape[-1]\n        return tf.reshape(input_tensor, [-1, width])", "CodeBleu_score": 40.62296586238139}, {"code": "def reshape_to_matrix(input_tensor)\n    if len(input_tensor.shape) < 2:\n        raise ValueError(f\"Input tensor must have at least 2 dimensions, but got {len(input_tensor.shape)}\")\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        width = input_tensor.shape[-1]\n        return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 45.53014836484566}, {"code": "def reshape_to_matrix(input_tensor)\n    tensor_rank = input_tensor.get_shape().ndims\n    if tensor_rank is None:\n        raise ValueError(\"The input tensor must have a known rank.\")\n    elif tensor_rank < 2:\n        raise ValueError(\"The input tensor must have rank at least 2.\")\n    elif tensor_rank == 2:\n        return input_tensor\n    else:\n        width = input_tensor.get_shape()[-1]\n        reshaped_tensor = tf.reshape(input_tensor, [-1, width])\n        return reshaped_tensor\n\n", "CodeBleu_score": 50.560962773212935}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\", encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        next(reader)\n        for line in reader:\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(\n                InputExample(guid=None, text_a=text_a, text_b=text_b, label=label)\n            )\n    return train_examples\n\n", "CodeBleu_score": 29.819482756278028}, {"code": "def get_train_examples(self, data_dir)\n    examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\", encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        next(reader)\n        for line in reader:\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(\n                InputExample(guid=None, text_a=text_a, text_b=text_b, label=label)\n            )\n    return examples\n\n", "CodeBleu_score": 29.96435940106894}, {"code": "def get_train_examples(self, data_dir)\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), \"r\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        next(reader)\n        for line in reader:\n            text_a = line[1]\n            label = line[0]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(InputExample(guid=None, text_a=text_a, text_b=None, label=label))\n    return train_examples\n", "CodeBleu_score": 27.64165980091009}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    predictions = logits.argmax(dim=-1)\n    correct_predictions = torch.sum(predictions == label_ids)\n    total_predictions = torch.sum(is_real_example)\n    accuracy = correct_predictions / total_predictions\n    mean_loss = torch.sum(per_example_loss * is_real_example) / total_predictions\n    return {\"accuracy\": accuracy, \"loss\": mean_loss}", "CodeBleu_score": 24.380275979901928}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    import tensorflow as tf\n    import numpy as np\n\n    predictions = tf.argmax(logits, axis=-1)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions = tf.cast(predictions, tf.int32)\n    predictions", "CodeBleu_score": 19.101345798569323}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = (predictions == label_ids).mean()\n    loss = per_example_loss.mean()\n    return {\n        \"accuracy\": accuracy,\n        \"loss\": loss,\n    }", "CodeBleu_score": 22.07275893189095}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 77.88705310134927}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        features.append(convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer))\n    return features\n\n", "CodeBleu_score": 63.721067084057005}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 77.88705310134927}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n    next_sentence_output_weights = tf.get_variable(\n        \"next_sentence_output_weights\",\n        shape=[bert_config.hidden_size, 2],\n        initializer=tf.truncated_normal_initializer(stddev=0.02))\n    next_sentence_output_bias = tf.get_variable(\n        \"next_sentence_output_bias\",\n        shape=[2],\n        initializer=tf.zeros_initializer())\n    logits = tf.matmul(input_tensor, next_sentence_output_weights)\n    logits = tf.nn.bias_add(logits, next_sentence_output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    mean_loss = tf.reduce_mean(per_example_loss)\n    return (mean_loss, per_example_loss, log_probs)", "CodeBleu_score": 44.38526674797703}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_bias = tf.get_variable(\n            \"output_bias\",\n            shape=[bert_config.num_labels],\n            initializer=tf.zeros_initializer(),\n        )\n\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[bert_config.hidden_size, bert_config.num_labels],\n            initializer=tf.truncated_normal_initializer(stddev=0.02),\n        )\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 77.43319299552323}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n    output_weights = tf.get_variable(\n        \"output_weights\", [bert_config.hidden_size, 2],\n        initializer=tf.truncated_normal_initializer(stddev=0.02))\n    output_bias = tf.get_variable(\n        \"output_bias\", [2], initializer=tf.zeros_initializer())\n\n    hidden_size = bert_config.hidden_size\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    mean_loss = tf.reduce_mean(per_example_loss)\n    return (mean_loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 47.681101301203945}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n    sequence_length = sequence_tensor.shape[1]\n    batch_size = sequence_tensor.shape[0]\n    flat_offsets = torch.arange(batch_size) * sequence_length\n    flat_positions = flat_offsets.view(-1, 1) + positions.view(-1, 1)\n    flat_sequence_tensor = sequence_tensor.view(-1, sequence_tensor.shape[-1])\n    output_tensor = flat_sequence_tensor[flat_positions]\n    return output_tensor\n\n", "CodeBleu_score": 25.743010322596433}, {"code": "def gather_indexes(sequence_tensor, positions)\n    # gather from sequence tensor\n    shape = sequence_tensor.shape\n    flat_offsets = torch.arange(0, shape[0] * shape[1], shape[1])\n    flat_offsets = flat_offsets.type_as(positions)\n    flat_positions = (positions + flat_offsets[:, None]).view(-1)\n    output_tensor = sequence_tensor.flatten(0, 1)[flat_positions]\n    return output_tensor.view(shape[0], -1, shape[2])\n\n", "CodeBleu_score": 24.561137740812075}, {"code": "def gather_indexes(sequence_tensor, positions)\n    sequence_length = sequence_tensor.shape[1]\n    batch_size = sequence_tensor.shape[0]\n    flat_offsets = torch.arange(batch_size, dtype=torch.long) * sequence_length\n    flat_positions = positions + flat_offsets[:, None]\n    flat_sequence_tensor = sequence_tensor.reshape(-1, sequence_tensor.shape[-1])\n    output_tensor = flat_sequence_tensor[flat_positions]\n    return output_tensor\n\n", "CodeBleu_score": 27.601488134659064}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n", "CodeBleu_score": 60.19566542534298}, {"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 55.17342716427083}, {"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 60.19566542534298}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n\n    def _improve_answer_span_single(input_start, input_end, orig_answer_text):\n        \"\"\"This function improves the answer span by tokenizing the original answer text and then iterating through possible new start and end positions within the input span.\\nFor each possible span, it checks if the text matches the tokenized answer text.\\nIf a match is found, it returns the new start and end positions.\\nIf no match is found, it returns the original input start and end positions.\"\"\"\n\n        def _check_answer(start, end, orig_answer_text):\n            \"\"\"This function checks if the text between the start and end positions matches the tokenized answer text.\\nIf a match is found, it returns True.\\nIf no match is found, it returns False.\"\"\"\n            answer_text = \" \".join(doc_tokens[start:(end + 1)])\n            start_char = doc_tokens[start][0]\n            end_char = doc_tokens[end][1]\n            tokenized_answer_text = tokenizer.tokenize(answer_text)\n\n            if len(tokenized_answer_text) != len(orig_answer_text):\n                return False\n\n            for i, pair in enumerate(zip(tokenized_answer_text, orig_answer_text)):\n                if pair[0].lower() != pair[1].lower():\n                    return False\n                if orig_answer_text[i][0] != start_char + len(\n                        \" \".join(tokenized_answer_text[:i])):\n                    return False\n            return True\n\n        final_start = input_start\n        final_end = input_end\n        for new_start in range(input_start, input_end + 1):\n            for new_end in range(input_end, new_start - 1, -1):\n                if _check_answer(new_start, new_end, orig_answer_text):\n                    final_start = new_start\n                    final_end = new_end\n                    break\n        return final_start, final_end\n\n    start, end = _improve_answer_span_single(input_start, input_end, orig_answer_text)\n   ", "CodeBleu_score": 46.35772212376139}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    # Tokenize the original answer text\n    orig_answer_tokens = tokenizer.tokenize(orig_answer_text)\n\n    # Check if the original answer text is a subset of the input span\n    if orig_answer_tokens == doc_tokens[input_start:input_end + 1]:\n        return (input_start, input_end)\n\n    # Iterate through possible new start and end positions within the input span\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(new_start, input_end + 1):\n            # Check if the new span matches the tokenized answer text\n            if doc_tokens[new_start:new_end + 1] == orig_answer_tokens:\n                return (new_start, new_end)\n\n    # If no match is found, return the original input start and end positions\n    return (input_start, input_end)\n\n", "CodeBleu_score": 42.77129523634271}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    start_position = input_start\n    end_position = input_end\n    start_position = _find_start_position(doc_tokens, start_position, orig_answer_text, tokenizer)\n    end_position = _find_end_position(doc_tokens, end_position, orig_answer_text, tokenizer)\n    return start_position, end_position\n\n", "CodeBleu_score": 17.397282284785934}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = -1000000\n    best_span_index = -1\n    for (span_index, span) in enumerate(doc_spans):\n        end = span.start + span.length - 1\n        if position < span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - span.start\n        num_right_context = end - position\n        score = (num_left_context + num_right_context) / span.length\n        if score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n", "CodeBleu_score": 63.7079123823843}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = -1000000\n    best_span_index = -1\n    for (span_index, span) in enumerate(doc_spans):\n        end = span.start + span.length - 1\n        if position < span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - span.start\n        num_right_context = end - position\n        score = ((-1 * num_left_context) ** 2) + ((-1 * num_right_context) ** 2)\n        if score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 63.322297964677254}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = -1000000\n    best_span_index = -1\n    for (span_index, span) in enumerate(doc_spans):\n        end = span.start + span.length - 1\n        if position < span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - span.start\n        num_right_context = end - position\n        score = ((-num_left_context) ** 0.2) + ((-num_right_context) ** 0.2)\n        if score > best_score:\n            best_score = score\n            best_span_index = span_index\n    return cur_span_index == best_span_index\n", "CodeBleu_score": 62.045347166923825}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, \"rb\") as f:\n        while True:\n            token = f.readline()\n            if not token:\n                break\n            token = token.decode(\"utf-8\").strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 59.9093665578504}, {"code": "def load_vocab(vocab_file)\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, mode=\"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n", "CodeBleu_score": 69.18062742411362}, {"code": "def load_vocab(vocab_file)\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.gfile.GFile(vocab_file, mode=\"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 69.18062742411362}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n    return [vocab[item] for item in items]\n\n", "CodeBleu_score": 18.3448522644182}, {"code": "def convert_by_vocab(vocab, items)\n    result = []\n    for item in items:\n        result.append(vocab[item])\n    return result\n\n", "CodeBleu_score": 64.86892302612384}, {"code": "def convert_by_vocab(vocab, items)\n    return [vocab[item] for item in items]\n", "CodeBleu_score": 18.3448522644182}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n    tokens = self.basic_tokenizer.tokenize(text)\n    for i in range(len(tokens)):\n        tokens[i] = self.wordpiece_tokenizer.tokenize(tokens[i])\n    return tokens\n", "CodeBleu_score": 32.87409355029261}, {"code": "def tokenize(self, text)\n    tokens = []\n    for token in text.split():\n        tokens.extend(self.wordpiece_tokenizer.tokenize(token))\n    return tokens\n", "CodeBleu_score": 38.702046959027655}, {"code": "def tokenize(self, text)\n    text = text.lower()\n    text = text.replace(\"'s\", \" 's\")\n    text = text.replace(\"'ve\", \" 've\")\n    text = text.replace(\"'re\", \" 're\")\n    text = text.replace(\"'d\", \" 'd\")\n    text = text.replace(\"'ll\", \" 'll\")\n    text = text.replace(\"n't\", \" n't\")\n    text = text.replace(\"'m\", \" 'm\")\n    text = text.replace(\"'t\", \" 't\")\n    text = text.replace(\"'em\", \" 'em\")\n    text = text.replace(\"'\", \"\")\n    text = text.replace(\"`\", \"\")\n    text = text.replace(\"\u2019\", \"\")\n    text = text.replace(\"\u2018\", \"\")\n    text = text.replace(\"\u201c\", \"\")\n    text = text.replace(\"\u201d\", \"\")\n    text = text.replace(\"\u2026\", \"\")\n    text = text.replace(\"\u2014\", \"\")\n    text = text.replace(\"\u2013\", \"\")\n    text = text.replace(\"\u2022\", \"\")\n    text = text.replace(\"\u2044\", \"\")\n    text = text.replace(\"\u2705\", \"\")\n    text = text.replace(\"\u274c\", \"\")\n    text = text.replace(\"\u2714\", \"\")\n    text = text.replace(\"\u2716\", \"\")\n    text = text.replace(\"\u2714\ufe0f\", \"\")\n    text = text.replace(\"\u2716\ufe0f\", \"\")\n    text = text.replace(\"\u2705\ufe0f\", \"\")\n    text = text.replace(\"\u274c\ufe0f\", \"\")\n    text = text.replace(\"\u2714\ufe0f\", \"\")\n    text = text.replace(\"\u2716\ufe0f\", \"\")\n    text = text.replace(\"\u2705\ufe0f\", \"\")\n    text = text.replace(\"\u274c\ufe0f\", \"\")\n    text = text.replace(\"\u2714\ufe0f\", \"\")\n    text = text.replace(\"\u2716\ufe0f\", \"\")\n    text = text.replace", "CodeBleu_score": 16.951579637299442}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        audio_data = load_audio(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        print(f\"Error loading audio from {path} at {offset} for {duration} seconds: {e}\")\n        return -1.0, True\n", "CodeBleu_score": 20.21169626283276}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        audio_data = load(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        print(f\"Error loading audio data from {path}: {e}\")\n        return -1.0, True\n", "CodeBleu_score": 18.47362905537141}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        audio_data = load_audio(path, offset, duration, sample_rate, dtype)\n        return audio_data, False\n    except Exception as e:\n        logging.error(f\"Error loading audio from {path}: {e}\")\n        return -1.0, True\n", "CodeBleu_score": 22.985730944116657}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    waveform = tf.cast(waveform, tf.float32)\n    if waveform.shape[-1] == n_channels:\n        return waveform\n    elif waveform.shape[-1] > n_channels:\n        return waveform[:, :n_channels]\n    else:\n        return tf.tile(waveform, [1, 1, n_channels // waveform.shape[-1] + 1])[:, :, :n_channels]\n", "CodeBleu_score": 49.99423567197688}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    shape = waveform.shape\n    if len(shape) == 1:\n        shape = [1, shape[0]]\n    if shape[1] == n_channels:\n        return waveform\n    elif shape[1] > n_channels:\n        return waveform[:, :n_channels]\n    else:\n        return tf.tile(waveform, [1, n_channels // shape[1] + 1])[:, :n_channels]\n\n", "CodeBleu_score": 42.60568244355521}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    waveform = tf.squeeze(waveform)\n    waveform_channels = waveform.shape[-1]\n    if waveform_channels >= n_channels:\n        waveform = waveform[..., :n_channels]\n    else:\n        waveform = tf.tile(waveform, [1] * (len(waveform.shape) - 1) + [n_channels // waveform_channels + 1])\n        waveform = waveform[..., :n_channels]\n    return waveform\n\n", "CodeBleu_score": 43.69917537307315}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram)\n    spectrogram = tf.clip_by_value(spectrogram, -db_range, max_db)\n    spectrogram = tf.cast(spectrogram, tf.uint8)\n    return spectrogram", "CodeBleu_score": 41.75517606487893}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    # Convert the spectrogram to dB\n    spectrogram_db = gain_to_db(spectrogram, **kwargs)\n\n    # Find the maximum dB value\n    max_db = tf.reduce_max(spectrogram_db)\n\n    # Clip the dB spectrogram to the specified range below the maximum dB value\n    spectrogram_db_clipped = tf.clip_by_value(spectrogram_db, -db_range, max_db)\n\n    # Convert the clipped dB spectrogram from float32 to uint8 format\n    spectrogram_uint = tf.cast(\n        tf.round((spectrogram_db_clipped + db_range) / (2 * db_range) * 255), tf.uint8\n    )\n\n    return spectrogram_uint\n\n", "CodeBleu_score": 43.274086324842735}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    spectrogram = gain_to_db(spectrogram, **kwargs)\n    max_db = tf.reduce_max(spectrogram)\n    spectrogram = tf.clip_by_value(spectrogram, -db_range, max_db)\n    spectrogram = tf.cast(\n        tf.math.round((spectrogram + db_range) * 255 / (2 * db_range)), tf.uint8\n    )\n    return spectrogram\n\n", "CodeBleu_score": 45.716049639860856}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\n    # Define the Hann window function\n    window = tf.signal.hann_window(\n        frame_length,\n        periodic=True,\n        dtype=tf.float32,\n    )\n    window = tf.pow(window, window_exponent)\n\n    # Perform the STFT\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        window_fn=lambda x: window,\n        pad_end=True,\n    )\n\n    # Compute the spectrogram\n    spectrogram = tf.abs(stft)\n\n    # Transpose the spectrogram\n    spectrogram = tf.transpose(spectrogram)\n\n    # Raise the absolute value to a specified exponent\n    spectrogram = tf.pow(spectrogram, spec_exponent)\n\n    return spectrogram\n\n", "CodeBleu_score": 40.514864534069055}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    # Compute spectrogram using STFT\n    spectrogram = tf.abs(\n        tf.signal.stft(\n            waveform,\n            frame_length=frame_length,\n            frame_step=frame_step,\n            window_fn=lambda frame_length: tf.math.pow(\n                tf.signal.hann_window(frame_length, periodic=True), window_exponent\n            ),\n        )\n    )\n\n    # Transpose spectrogram\n    spectrogram = tf.transpose(spectrogram, perm=[1, 0, 2])\n\n    # Raise spectrogram to a specified exponent\n    spectrogram = tf.math.pow(spectrogram, spec_exponent)\n\n    return spectrogram", "CodeBleu_score": 39.472300630620126}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    frame_length = int(frame_length)\n    frame_step = int(frame_step)\n\n    # Hann window function\n    window = tf.signal.hann_window(frame_length, dtype=tf.float32)\n    window = tf.pow(window, window_exponent)\n\n    # STFT\n    stfts = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        window_fn=lambda x: window,\n    )\n\n    # Absolute value\n    spectrogram = tf.abs(stfts)\n\n    # Raise to the power of the spec_exponent\n    spectrogram = tf.pow(spectrogram, spec_exponent)\n\n    # Transpose\n    spectrogram = tf.transpose(spectrogram, perm=[1, 0, 2])\n\n    return spectrogram", "CodeBleu_score": 38.83545786872622}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Get the original spectrogram dimensions\n    original_height, original_width = spectrogram.shape\n\n    # Calculate the new time dimension\n    new_height = int(original_height * factor)\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=(new_height, original_width),\n        method=method,\n        antialias=True,\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    if new_height < original_height:\n        # Crop the spectrogram\n        crop_height = original_height - new_height\n        crop_top = crop_height // 2\n        crop_bottom = crop_top + new_height\n        resized_spectrogram = resized_spectrogram[crop_top:crop_bottom, :]\n    else:\n        # Pad the spectrogram\n        pad_height = new_height - original_height\n        pad_top = pad_height // 2\n        pad_bottom = pad_top + pad_height % 2\n        resized_spectrogram = tf.pad(\n            resized_spectrogram,\n            paddings=[[pad_top, pad_bottom], [0, 0]],\n            mode=\"CONSTANT\",\n        )\n\n    return resized_spectrogram", "CodeBleu_score": 34.38898882927278}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(tf.shape(spectrogram)[1], tf.float32) * factor\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [tf.cast(tf.shape(spectrogram)[0], tf.int32), tf.cast(new_time_dim, tf.int32)],\n        method=method,\n    )\n    # Adjust the resized spectrogram to the original dimensions\n    if new_time_dim > tf.shape(spectrogram)[1]:\n        # Pad the resized spectrogram with zeros\n        padding = tf.zeros([tf.shape(spectrogram)[0], tf.cast(new_time_dim, tf.int32) - tf.shape(spectrogram)[1]], dtype=tf.float32)\n        resized_spectrogram = tf.concat([resized_spectrogram, padding], axis=1)\n    else:\n        # Crop the resized spectrogram\n        resized_spectrogram = resized_spectrogram[:, :tf.shape(spectrogram)[1]]\n    return resized_spectrogram", "CodeBleu_score": 44.34535642338743}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the new time dimension\n    new_time = int(spectrogram.shape[1] * factor)\n\n    # Resize the spectrogram\n    resized = tf.image.resize(\n        spectrogram, (spectrogram.shape[0], new_time), method=method\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    if new_time > spectrogram.shape[1]:\n        # Crop the spectrogram to the original dimensions\n        start = (new_time - spectrogram.shape[1]) // 2\n        end = start + spectrogram.shape[1]\n        resized = resized[:, start:end, :]\n    else:\n        # Pad the spectrogram to the original dimensions\n        start = (spectrogram.shape[1] - new_time) // 2\n        end = start + new_time\n        resized = tf.pad(resized, [[0, 0], [start, end - start], [0, 0]])\n\n    return resized", "CodeBleu_score": 37.731480180427866}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        (tf.shape(spectrogram)[0], tf.cast(tf.shape(spectrogram)[1] * resizing_factor, tf.int32)),\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, tf.shape(spectrogram)[1] - tf.shape(resized_spectrogram)[1]]],\n        mode=\"CONSTANT\",\n    )\n\n    return padded_spectrogram", "CodeBleu_score": 34.74901746354257}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [tf.shape(spectrogram)[0], tf.shape(spectrogram)[1] * resizing_factor],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        [[0, 0], [0, tf.shape(spectrogram)[1] - tf.shape(resized_spectrogram)[1]]],\n    )\n\n    return padded_spectrogram", "CodeBleu_score": 33.94655430510781}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the resizing factor based on the semitone shift\n    resizing_factor = tf.math.exp(semitone_shift / 12 * tf.math.log(2.0))\n    # Resize the spectrogram along the frequency axis using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=[spectrogram.shape[0], tf.cast(spectrogram.shape[1] * resizing_factor, tf.int32)],\n        method=method,\n    )\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=tf.constant([[0, 0], [0, spectrogram.shape[1] - resized_spectrogram.shape[1]]]),\n        mode=\"CONSTANT\",\n    )\n    return padded_spectrogram", "CodeBleu_score": 28.256350158571014}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\n            f\"Invalid activation function: {params['conv_activation']}. Please choose one of ['relu', 'elu', 'leaky_relu'].\"\n        )\n\n", "CodeBleu_score": 29.33102167877849}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\n            f\"Invalid value for conv_activation: {params['conv_activation']}. Expected one of ['relu', 'elu', 'leaky_relu'].\"\n        )\n", "CodeBleu_score": 29.325975679812434}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    activation = params.get(\"conv_activation\", \"relu\")\n    if activation == \"relu\":\n        return nn.ReLU()\n    elif activation == \"elu\":\n        return nn.ELU()\n    elif activation == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Invalid activation: {activation}\")\n", "CodeBleu_score": 31.612618158408008}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n    logit_masks = []\n    for instrument in instruments:\n        logit_mask = unet(input_tensor, params)\n        logit_masks.append(logit_mask)\n    logit_masks = tf.stack(logit_masks)\n    softmax_masks = tf.nn.softmax(logit_masks, axis=0)\n    spectrograms = {}\n    for i, instrument in enumerate(instruments):\n        spectrograms[instrument] = input_tensor * softmax_masks[i]\n    return spectrograms", "CodeBleu_score": 30.579587699583154}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n    # Define the U-Net model for each instrument\n    unet_models = {\n        instrument: unet_model(input_tensor.shape[1:]) for instrument in instruments\n    }\n\n    # Apply the U-Net model to the input tensor for each instrument\n    logit_masks = {\n        instrument: unet_models[instrument](input_tensor) for instrument in instruments\n    }\n\n    # Combine the logit masks using a softmax function along the time axis\n    combined_logit_masks = tf.reduce_sum(\n        [\n            tf.nn.softmax(logit_mask, axis=-1)\n            for logit_mask in logit_masks.values()\n        ],\n        axis=0,\n    )\n\n    # Create an output dictionary where each entry is a spectrogram for an instrument\n    spectrograms = {\n        instrument: tf.reduce_sum(\n            [\n                input_tensor * tf.nn.softmax(logit_mask, axis=-1)\n                for instrument, logit_mask in logit_masks.items()\n            ],\n            axis=0,\n        )\n        for instrument in instruments\n    }\n\n    return spectrograms", "CodeBleu_score": 29.275715754483755}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n    logit_masks = {}\n    for instrument in instruments:\n        logit_masks[instrument] = unet(input_tensor, params)\n    logit_masks = tf.stack(list(logit_masks.values()))\n    logit_masks = tf.nn.softmax(logit_masks, axis=0)\n    output = {}\n    for i, instrument in enumerate(instruments):\n        output[instrument] = tf.reduce_sum(\n            input_tensor * logit_masks[i : i + 1], axis=0\n        )\n    return output", "CodeBleu_score": 41.11213182186843}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n    import hashlib\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 51.074864338551386}, {"code": "def compute_file_checksum(path)\n    import hashlib\n    sha256_hash = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for byte_block in iter(lambda: f.read(4096), b''):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 46.3641309375688}, {"code": "def compute_file_checksum(path)\n    import hashlib\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n", "CodeBleu_score": 37.024896758346344}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        index = response.json()\n        if name in index:\n            return index[name]\n        else:\n            raise ValueError(f\"Model '{name}' not found in index\")\n    else:\n        raise ValueError(f\"Failed to retrieve checksum index from {url}\")", "CodeBleu_score": 39.40170889933198}, {"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{name}.json\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        index = response.json()\n        if name in index:\n            return index[name]\n        else:\n            raise ValueError(f\"Model '{name}' not found in index.\")\n    else:\n        raise ValueError(f\"Failed to retrieve index from URL: {url}\")", "CodeBleu_score": 38.898820868070494}, {"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to retrieve checksum index from {url}\")\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in index\")\n    return index[name]", "CodeBleu_score": 41.53335537553192}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n    url = self.get_url(name)\n    self.log.info(f\"Downloading: {url}\")\n    with self.http_session.get(url, stream=True) as response:\n        with open(path, \"wb\") as file:\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    file.write(chunk)\n    self.validate_checksum(path, name)\n    self.extract_archive(path)\n    os.remove(path)\n    self.log.info(f\"Downloaded and extracted: {name}\")\n", "CodeBleu_score": 22.95110253717339}, {"code": "def download(self, name: str, path: str) -> None:\n    url = self._get_url(name)\n    self._log_download_start(url)\n    with self._get_http_client().stream(url) as response:\n        self._log_download_finish(url, response)\n        self._extract_archive(response, path)\n", "CodeBleu_score": 13.995673419364557}, {"code": "def download(self, name: str, path: str) -> None:\n    url = f\"{self.model_url}/{name}\"\n    self.logger.info(f\"Downloading {name} from {url}\")\n    self.logger.info(f\"Downloading {name} from {url}\")\n    with open(path, \"wb\") as f:\n        for chunk in self.session.get(url, stream=True).iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n    self.logger.info(f\"Downloaded {name} to {path}\")\n    self.logger.info(f\"Downloaded {name} to {path}\")\n    self.logger.info(f\"Validating checksum for {name}\")\n    self.logger.info(f\"Validating checksum for {name}\")\n    checksum = self.checksum(path)\n    if checksum != self.checksums[name]:\n        self.logger.error(f\"Checksum for {name} is invalid. Expected {self.checksums[name]}, got {checksum}\")\n        self.logger.error(f\"Checksum for {name} is invalid. Expected {self.checksums[name]}, got {checksum}\")\n        raise ValueError(f\"Checksum for {name} is invalid. Expected {self.checksums[name]}, got {checksum}\")\n    self.logger.info(f\"Checksum for {name} is valid. Extracting to {path}\")\n    self.logger.info(f\"Checksum for {name} is valid. Extracting to {path}\")\n    with tarfile.open(path, \"r:gz\") as tar:\n        tar.extractall(path=path)\n    os.remove(path)\n    self.logger.info(f\"Extracted {name} to {path}\")\n    self.logger.info(f\"Extracted {name} to {path}\")\n", "CodeBleu_score": 33.149959321942774}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n    # Load the configuration from the embedded resource\n    if descriptor.startswith('embedded:'):\n        name = descriptor.split(':')[1]\n        resource = pkg_resources.resource_string(__name__, f'config/{name}.json')\n        return json.loads(resource)\n\n    # Load the configuration from the file\n    if os.path.isfile(descriptor):\n        with open(descriptor, 'r') as file:\n            return json.load(file)\n\n    raise ValueError(f'Configuration not found: {descriptor}')", "CodeBleu_score": 39.81241495409926}, {"code": "def load_configuration(descriptor: str) -> Dict\n    if descriptor.startswith(\"embedded:\"):\n        config_name = descriptor.split(\":\")[1]\n        config_resource = f\"{config_name}.json\"\n        config_data = pkg_resources.read_text(f\"{__package__}.resources\", config_resource)\n        return json.loads(config_data)\n    else:\n        if not os.path.isfile(descriptor):\n            raise FileNotFoundError(f\"Configuration file not found: {descriptor}\")\n        with open(descriptor, \"r\") as config_file:\n            return json.load(config_file)", "CodeBleu_score": 44.01516590009135}, {"code": "def load_configuration(descriptor: str) -> Dict\n    # Check if the descriptor is an embedded configuration\n    if descriptor.startswith(\"embedded://\"):\n        # Extract the configuration name from the descriptor\n        config_name = descriptor.split(\"embedded://\")[1]\n        # Get the resource path for the configuration\n        resource_path = f\"embedded://{config_name}\"\n        # Load the configuration from the resource\n        with open(resource_path, \"r\") as f:\n            config = json.load(f)\n    # If the descriptor is not an embedded configuration, it is a file path\n    else:\n        # Check if the file exists\n        if not os.path.exists(descriptor):\n            raise FileNotFoundError(f\"Configuration file not found: {descriptor}\")\n        # Load the configuration from the file\n        with open(descriptor, \"r\") as f:\n            config = json.load(f)\n    # Return the loaded configuration\n    return config\n", "CodeBleu_score": 43.17075307113472}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    if concat_axis not in (0, 1):\n        raise ValueError(\n            f\"Invalid concatenation axis {concat_axis}. Must be either 0 or 1.\"\n        )\n    if concat_axis == 0:\n        tensors = [tensor_dict[key] for key in sorted(tensor_dict.keys())]\n        concatenated = tf.concat(tensors, axis=concat_axis)\n        processed = func(concatenated)\n        split_indices = [\n            tf.shape(tensor_dict[key])[0] for key in sorted(tensor_dict.keys())\n        ]\n        split_tensors = tf.split(processed, split_indices, axis=concat_axis)\n        return {\n            key: split_tensors[i]\n            for i, key in enumerate(sorted(tensor_dict.keys()))\n        }\n    else:\n        tensors = [tensor_dict[key] for key in sorted(tensor_dict.keys())]\n        concatenated = tf.concat(tensors, axis=concat_axis)\n        processed = func(concatenated)\n        split_indices = [\n            tf.shape(tensor_dict[key])[1] for key in sorted(tensor_dict.keys())\n        ]\n        split_tensors = tf.split(processed, split_indices, axis=concat_axis)\n        return {\n            key: split_tensors[i]\n            for i, key in enumerate(sorted(tensor_dict.keys()))\n        }", "CodeBleu_score": 40.65251711508937}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            \"concat_axis must be either 0 or 1, but got {}\".format(concat_axis)\n        )\n    if concat_axis == 0:\n        concat_dim = [0]\n    else:\n        concat_dim = [1]\n    return {\n        key: tf.split(func(tf.concat(list(tensor_dict.values()), concat_dim)), list(tensor_dict.keys()))\n        for key in tensor_dict.keys()\n    }", "CodeBleu_score": 29.289024352289616}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    # Check if the concatenation axis is either 0 or 1\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            \"concat_axis must be either 0 or 1, but got {}\".format(concat_axis)\n        )\n\n    # Concatenate the tensors along the specified axis\n    tensors = [tensor_dict[key] for key in sorted(tensor_dict.keys())]\n    concat_tensors = tf.concat(tensors, axis=concat_axis)\n\n    # Apply the function to the concatenated tensor\n    processed_tensor = func(concat_tensors)\n\n    # Split the processed tensor back into the original dictionary format\n    if concat_axis == 0:\n        processed_tensor_dict = {\n            key: processed_tensor[: tensor_dict[key].shape[0]]\n            for key in sorted(tensor_dict.keys())\n        }\n    else:\n        processed_tensor_dict = {\n            key: processed_tensor[:, : tensor_dict[key].shape[1]]\n            for key in sorted(tensor_dict.keys())\n        }\n\n    return processed_tensor_dict\n\n", "CodeBleu_score": 45.6931515197737}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255.0, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 47.202142731066075}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 47.09845284075762}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    tensor = tf.cast(tensor, tf.float32)\n    tensor = (tensor - min_value) / (max_value - min_value)\n    tensor = tf.cast(tensor * 255, tf.uint8)\n    return {tensor_key: tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 49.21164030304975}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    padding = segment_len - tf.shape(tensor)[0] % segment_len\n    padded_tensor = tf.pad(tensor, [[0, padding]])\n    return tf.reshape(padded_tensor, [-1, segment_len])\n", "CodeBleu_score": 18.888558041492175}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    tensor_len = tf.shape(tensor)[0]\n    padding_len = segment_len - (tensor_len % segment_len)\n    padded_tensor = tf.pad(tensor, [[0, padding_len]])\n    return tf.reshape(padded_tensor, [-1, segment_len])", "CodeBleu_score": 20.096699789488003}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    padding = segment_len - tf.math.mod(tf.shape(tensor)[0], segment_len)\n    padded_tensor = tf.pad(tensor, [[0, padding]])\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 21.390905768179834}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = frame_length // 2\n    # Create an extension of zeros with the correct number of extra rows and the same number of columns as the input spectrogram\n    extension = tf.zeros([num_extra_rows, instr_spec.shape[1]], dtype=tf.float32)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    extended_spec = tf.concat([extension, instr_spec, extension], axis=0)\n    # Reshape the extended spectrogram by merging the first two dimensions and returning the processed spectrogram\n    return tf.reshape(extended_spec, [1, -1, F])\n\n", "CodeBleu_score": 13.418677345916649}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = frame_length - (instr_spec.shape[0] % frame_length)\n    # Create an extension of zeros\n    padding = torch.zeros((num_extra_rows, instr_spec.shape[1], instr_spec.shape[2]), dtype=torch.float32)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = torch.cat((instr_spec, padding), dim=0)\n    # Reshape the extended spectrogram\n    instr_spec = instr_spec.reshape(instr_spec.shape[0] // frame_length, frame_length, instr_spec.shape[1] * instr_spec.shape[2])\n    return instr_spec", "CodeBleu_score": 17.31577940761994}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    # Calculate the number of extra rows needed for padding\n    num_extra_rows = frame_length - (instr_spec.shape[0] % frame_length)\n\n    # Create an extension of zeros\n    zeros_extension = np.zeros((num_extra_rows, instr_spec.shape[1]), dtype=np.float32)\n\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    extended_spec = np.concatenate((instr_spec, zeros_extension), axis=0)\n\n    # Reshape the extended spectrogram by merging the first two dimensions\n    processed_spec = extended_spec.reshape((-1, F))\n\n    return processed_spec", "CodeBleu_score": 16.915337306308455}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    import pandas as pd\n    import tensorflow as tf\n    import tensorflow_datasets as tfds\n    import numpy as np\n\n    df = pd.read_csv(csv_path, **kwargs)\n    data = df.values\n    dataset = tf.data.Dataset.from_tensor_slices(data)\n    return dataset\n\n", "CodeBleu_score": 28.425288606294824}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    df = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    return dataset", "CodeBleu_score": 37.4879361692088}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    df = pd.read_csv(csv_path)\n    ds = tf.data.Dataset.from_tensor_slices((df.values[:, :-1], df.values[:, -1]))\n    return ds\n", "CodeBleu_score": 27.21292934798193}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i, length in enumerate(target_shape):\n        if length is not None:\n            result = result and (tensor_tf.shape[i] == length)\n    return result\n\n", "CodeBleu_score": 32.842957208595855}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for dim, length in enumerate(target_shape):\n        result = result and tensor_tf.shape[dim] == length\n    return result\n\n", "CodeBleu_score": 26.933445921410883}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i, dim in enumerate(target_shape):\n        if dim is not None:\n            result = result and (tensor_tf.shape[i] == dim)\n    return result\n\n", "CodeBleu_score": 31.182997668408653}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset = builder.build(\n        train_csv_path=audio_params[\"train_csv_path\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        data_augmentation=audio_params[\"data_augmentation\"],\n        cache=audio_params[\"cache\"],\n    )\n    return dataset", "CodeBleu_score": 40.382917668038246}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        chunk_duration=10,\n        random_seed=42,\n    )\n    return builder.build(\n        train_csv_path=os.path.join(audio_path, \"train.csv\"),\n        cache_directory=os.path.join(audio_path, \"cache\"),\n        batch_size=16,\n        num_chunks_per_song=2,\n        data_augmentation_options={\n            \"time_shift\": 0.2,\n            \"gain_change\": 0.1,\n            \"pitch_shift\": 0.2,\n            \"speed_change\": 0.2,\n        },\n        cache_data=True,\n    )", "CodeBleu_score": 39.45349948105055}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    train_dataset = builder.build(\n        train_csv_path=audio_params[\"train_csv_path\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        data_augmentation_options=audio_params[\"data_augmentation_options\"],\n        cache_options=audio_params[\"cache_options\"],\n    )\n    return train_dataset\n\n", "CodeBleu_score": 34.10484402671802}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n    if self._instruments is None:\n        self._instruments = [InstrumentDatasetBuilder(instrument) for instrument in self._instrument_list]\n    for instrument in self._instruments:\n        yield instrument", "CodeBleu_score": 33.36738472885903}, {"code": "def instruments(self) -> Any\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self.instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(instrument, self.data_root)\n            )\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 66.05938900308236}, {"code": "def instruments(self) -> Any\n    if not self.instruments:\n        for instrument in self.instruments_list:\n            self.instruments[instrument] = InstrumentDatasetBuilder(instrument)\n    for instrument in self.instruments_list:\n        yield self.instruments[instrument]\n\n", "CodeBleu_score": 34.515549325334014}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is None:\n        return dataset\n\n    cache_path = Path(cache)\n    cache_path.mkdir(exist_ok=True, parents=True)\n\n    if wait:\n        while not (cache_path / \"index.json\").exists():\n            logger.info(\"Waiting for cache index...\")\n            time.sleep(1)\n\n    dataset.cache(cache_path)\n\n    return dataset\n", "CodeBleu_score": 44.04196549734319}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is None:\n        return dataset\n    if not os.path.exists(cache):\n        os.makedirs(cache)\n    cache_index = os.path.join(cache, \"index.json\")\n    if not os.path.exists(cache_index):\n        with open(cache_index, \"w\") as f:\n            json.dump({}, f)\n    if not wait:\n        return dataset\n    while not os.path.exists(cache_index):\n        logger.info(f\"Waiting for cache index file at {cache_index}\")\n        time.sleep(1)\n    return dataset\n", "CodeBleu_score": 47.25299050801641}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is None:\n        return dataset\n\n    cache_dir = os.path.dirname(cache)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    if os.path.exists(cache):\n        return dataset\n\n    if wait:\n        while True:\n            if os.path.exists(cache):\n                return dataset\n            else:\n                print(f\"Waiting for {cache} to be created...\")\n                time.sleep(1)\n\n    dataset.save_to_disk(cache)\n    return dataset\n\n", "CodeBleu_score": 42.48515464637161}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_dir = ModelProvider.get_model_dir(params['model_name'], params['model_version'], MWF)\n    params['model_dir'] = model_dir\n    params['train_batch_size'] = params['train_batch_size'] * params['num_gpus']\n    params['eval_batch_size'] = params['eval_batch_size'] * params['num_gpus']\n    params['predict_batch_size'] = params['predict_batch_size'] * params['num_gpus']\n    session_config = tf.ConfigProto(allow_soft_placement=True)\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.visible_device_list = str(params['gpu_ids'])\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params, config=run_config)\n    return estimator\n\n", "CodeBleu_score": 33.66215143249756}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_dir = ModelProvider.get_model_dir(params)\n    params.update({\"model_dir\": model_dir})\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(model_fn=model_fn, params=params, model_dir=model_dir, config=run_config)\n    return estimator\n\n", "CodeBleu_score": 35.54522881486245}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_dir = ModelProvider.get_model_dir(params[\"model_name\"])\n    params[\"model_dir\"] = model_dir\n    if MWF:\n        params[\"model_dir\"] = ModelProvider.get_model_dir(params[\"model_name\"])\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.3\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=params[\"model_dir\"], params=params, config=run_config)\n    return estimator", "CodeBleu_score": 59.759347059954706}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    waveform = self._ensure_stereo(waveform)\n    prediction_generator = self._get_prediction_generator()\n    data_generator = self._get_data_generator(waveform, audio_descriptor)\n    prediction = next(prediction_generator(data_generator))\n    del prediction[\"audio_id\"]\n    return prediction", "CodeBleu_score": 39.96274027961675}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    if waveform.shape[1] != 2:\n        raise ValueError(\"Waveform must be stereo\")\n    prediction_generator = self.model.predict(\n        data_generator=self.data_generator,\n        waveform=waveform,\n        audio_descriptor=audio_descriptor,\n    )\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n", "CodeBleu_score": 59.87615132932479}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    # Ensure the waveform is stereo\n    if waveform.shape[1] != 2:\n        raise ValueError(\n            \"The waveform must be stereo for TensorFlow separation. \"\n            \"Please use the 'separate_audio_sources' function instead.\"\n        )\n    # Obtain a prediction generator\n    prediction_generator = self.get_prediction_generator()\n    # Update the data generator with the waveform and audio descriptor\n    prediction_generator.update_data_generator(waveform, audio_descriptor)\n    # Generate a prediction\n    prediction = prediction_generator.generate_prediction()\n    # Remove the \"audio_id\" key from the prediction\n    prediction.pop(\"audio_id\")\n    # Return the prediction\n    return prediction", "CodeBleu_score": 44.83072748536567}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"Backbone {} not supported.\".format(config.BACKBONE)\n        return {\n            \"resnet50\": compute_backbone_shapes_resnet50(image_shape),\n            \"resnet101\": compute_backbone_shapes_resnet101(image_shape),\n        }[config.BACKBONE]\n\n", "CodeBleu_score": 47.06612294591675}, {"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    elif config.BACKBONE == \"resnet50\":\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])\n    elif config.BACKBONE == \"resnet101\":\n        return np.array(\n            [[int(math.ceil(image_shape[0] / stride)),\n              int(math.ceil(image_shape[1] / stride))]\n             for stride in config.BACKBONE_STRIDES])\n    else:\n        raise ValueError(\"Backbone '{}' not supported.\".format(config.BACKBONE))", "CodeBleu_score": 70.44876991052067}, {"code": "def compute_backbone_shapes(config, image_shape)\n    backbone = config.BACKBONE\n    if callable(backbone):\n        return backbone.compute_backbone_shapes(image_shape, config.BACKBONE_STRIDES)\n    else:\n        assert backbone in {\"resnet50\", \"resnet101\"}, \"Backbone '{}' not supported.\".format(backbone)\n        return {\n            \"resnet50\": [\n                (32, 32),\n                (16, 16),\n                (8, 8),\n                (4, 4),\n                (2, 2),\n            ],\n            \"resnet101\": [\n                (64, 64),\n                (32, 32),\n                (16, 16),\n                (8, 8),\n                (4, 4),\n                (2, 2),\n            ],\n        }[backbone][: len(config.BACKBONE_STRIDES)]\n\n", "CodeBleu_score": 25.098092948367025}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n    # Convert to 2d tensor\n    boxes = tf.reshape(boxes, [-1, 4])\n    window = tf.reshape(window, [1, 4])\n    # Split\n    b = tf.split(boxes, 2, axis=1)\n    w = tf.split(window, 2, axis=1)\n    # Clip\n    b0 = tf.maximum(tf.minimum(b[0], w[1]), w[0])\n    b1 = tf.maximum(tf.minimum(b[1], w[3]), w[2])\n    clipped_boxes = tf.concat([b0, b1], axis=1)\n    clipped_boxes = tf.reshape(clipped_boxes, [tf.shape(boxes)[0], 4])\n    return clipped_boxes\n", "CodeBleu_score": 22.902705307392036}, {"code": "def clip_boxes_graph(boxes, window)\n    # Split the window into (y_min, x_min, y_max, x_max) tensors\n    y_min, x_min, y_max, x_max = tf.split(window, 4)\n    # Split the boxes into (y_min, x_min, y_max, x_max) tensors\n    ymin, xmin, ymax, xmax = tf.split(boxes, 4, axis=1)\n    # Clip the boxes using TensorFlow operations\n    ymin = tf.maximum(tf.minimum(ymin, y_max), y_min)\n    xmin = tf.maximum(tf.minimum(xmin, x_max), x_min)\n    ymax = tf.minimum(tf.maximum(ymax, y_min), y_max)\n    xmax = tf.minimum(tf.maximum(xmax, x_min), x_max)\n    # Concatenate the clipped coordinates back into a single tensor\n    clipped_boxes = tf.concat([ymin, xmin, ymax, xmax], axis=1)\n    # Reshape the clipped boxes to match the original shape of the input boxes\n    clipped_boxes = tf.reshape(clipped_boxes, tf.shape(boxes))\n    return clipped_boxes\n\n", "CodeBleu_score": 22.068314948631016}, {"code": "def clip_boxes_graph(boxes, window)\n    box_window_x1_y1 = tf.maximum(boxes[..., :2], window[0])\n    box_window_x2_y2 = tf.minimum(boxes[..., 2:], window[1])\n    clipped_boxes = tf.concat([box_window_x1_y1, box_window_x2_y2], axis=-1)\n    clipped_boxes.set_shape((None, 4))\n    return clipped_boxes\n\n", "CodeBleu_score": 15.3967255410301}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    # Shared convolutional layer for class and bounding box predictions\n    shared = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu', name='rpn_conv_shared')(feature_map)\n\n    # Class scores\n    class_logits = tf.keras.layers.Conv2D(anchors_per_location * 2, (1, 1), activation='linear', name='rpn_class_raw')(shared)\n    class_logits = tf.keras.layers.Reshape((-1, 2), name='rpn_class_logits')(class_logits)\n    class_probs = tf.keras.layers.Activation('softmax', name='rpn_class_prob')(class_logits)\n\n    # Bounding box predictions\n    bbox_deltas = tf.keras.layers.Conv2D(anchors_per_location * 4, (1, 1), activation='linear', name='rpn_bbox_pred')(shared)\n    bbox_deltas = tf.keras.layers.Reshape((-1, 4), name='rpn_bbox_deltas')(bbox_deltas)\n\n    # Convert class logits to class probabilities\n    class_probs = tf.keras.layers.Activation('softmax', name='rpn_class_prob')(class_logits)\n\n    # Convert bounding box deltas to bounding box predictions\n    bbox_deltas = tf.keras.layers.Activation('linear', name='rpn_bbox_deltas')(bbox_deltas)\n\n    # Reshape class logits and bounding box deltas\n    class_logits = tf.keras.layers.Reshape((-1, 2), name='rpn_class_logits')(class_logits)\n    bbox_deltas = tf.keras.layers.Reshape((-1, 4", "CodeBleu_score": 37.63550044002575}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    # Define the shared convolutional layer for the RPN\n    rpn_conv = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv')(feature_map)\n\n    # Define the class score layer\n    class_logits = Conv2D(anchors_per_location * 2, (1, 1), activation='linear', kernel_initializer='zeros', name='class_logits')(rpn_conv)\n    class_logits = Reshape((-1, 2), name='class_logits_reshape')(class_logits)\n    class_logits = Activation('softmax', name='class_logits_softmax')(class_logits)\n\n    # Define the bounding box prediction layer\n    bbox_deltas = Conv2D(anchors_per_location * 4, (1, 1), activation='linear', kernel_initializer='zeros', name='bbox_deltas')(rpn_conv)\n    bbox_deltas = Reshape((-1, 4), name='bbox_deltas_reshape')(bbox_deltas)\n\n    # Return the class logits, class probabilities, and bounding box predictions\n    return class_logits, class_logits, bbox_deltas", "CodeBleu_score": 33.93123902450576}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    # Apply a shared convolutional layer to the feature map\n    shared_conv = tf.keras.layers.Conv2D(512, kernel_size=3, strides=anchor_stride, padding='same', activation='relu', kernel_initializer='glorot_uniform', name='shared_conv')(feature_map)\n\n    # Apply a convolutional layer for class scores\n    class_conv = tf.keras.layers.Conv2D(anchors_per_location * 2, kernel_size=1, strides=1, padding='same', activation=None, kernel_initializer='glorot_uniform', name='class_conv')(shared_conv)\n\n    # Apply a convolutional layer for bounding box predictions\n    bbox_conv = tf.keras.layers.Conv2D(anchors_per_location * 4, kernel_size=1, strides=1, padding='same', activation=None, kernel_initializer='glorot_uniform', name='bbox_conv')(shared_conv)\n\n    # Reshape the class scores\n    class_logits = tf.keras.layers.Reshape((-1, 2), name='class_logits')(class_conv)\n    class_probs = tf.keras.layers.Activation('softmax', name='class_probs')(class_logits)\n\n    # Reshape the bounding box predictions\n    bbox_preds = tf.keras.layers.Reshape((-1, 4), name='bbox_preds')(bbox_conv)\n\n    return class_logits, class_probs, bbox_preds", "CodeBleu_score": 34.96621893648101}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n    absolute_difference = tf.abs(y_true - y_pred)\n    small_mask = tf.cast(tf.less(absolute_difference, 1.0), tf.float32)\n    large_mask = tf.cast(tf.greater_equal(absolute_difference, 1.0), tf.float32)\n    squared_difference = tf.square(absolute_difference)\n    return tf.reduce_sum(tf.multiply(small_mask, 0.5 * squared_difference) + tf.multiply(large_mask, absolute_difference - 0.5))\n\n", "CodeBleu_score": 31.440239517151237}, {"code": "def smooth_l1_loss(y_true, y_pred)\n    abs_diff = abs(y_true - y_pred)\n    mask = abs_diff < 1\n    loss = (1 - 0.5 * mask) * abs_diff ** 2 + 0.5 * mask * abs_diff\n    return loss.mean()\n", "CodeBleu_score": 22.08625373705324}, {"code": "def smooth_l1_loss(y_true, y_pred)\n    # Calculate the absolute difference between the true and predicted values\n    diff = K.abs(y_true - y_pred)\n\n    # Create a mask for values less than one\n    mask = K.cast(K.less(diff, 1.0), 'float32')\n\n    # Compute the Smooth L1 loss\n    loss = (0.5 * K.square(diff) * mask) + (diff - 0.5) * (1.0 - mask)\n\n    # Return the mean loss\n    return K.mean(loss)", "CodeBleu_score": 32.9932459571521}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(64, activation='relu'))\n    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n", "CodeBleu_score": 23.60696097267331}, {"code": "def build_model(x_train, num_classes)\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(128, activation='relu'))\n    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n", "CodeBleu_score": 26.571151701685586}, {"code": "def build_model(x_train, num_classes)\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(64, activation='relu'))\n    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n", "CodeBleu_score": 23.60696097267331}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        horizontal_index = np.any(mask[:, :, i].cpu(), axis=0)\n        vertical_index = np.any(mask[:, :, i].cpu(), axis=1)\n        horizontal_length, vertical_length = np.sum(horizontal_index), np.sum(vertical_index)\n        if horizontal_length > 0 and vertical_length > 0:\n            horizontal_start, horizontal_end = np.where(horizontal_index)[0][[0, -1]]\n            vertical_start, vertical_end = np.where(vertical_index)[0][[0, -1]]\n            boxes[i] = np.array([vertical_start, horizontal_start, vertical_end, horizontal_end])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 39.70601099039515}, {"code": "def extract_bboxes(mask)\n    bboxes = []\n    for i in range(mask.shape[0]):\n        h_indices = np.where(mask[i, :, :] > 0)[0]\n        v_indices = np.where(mask[i, :, :] > 0)[1]\n        if h_indices.size == 0 or v_indices.size == 0:\n            bbox = np.zeros((4,), dtype=np.int32)\n        else:\n            bbox = np.zeros((4,), dtype=np.int32)\n            bbox[0] = np.min(h_indices)\n            bbox[1] = np.min(v_indices)\n            bbox[2] = np.max(h_indices)\n            bbox[3] = np.max(v_indices)\n        bboxes.append(bbox)\n    return np.array(bboxes, dtype=np.int32)\n\n", "CodeBleu_score": 27.685825425479564}, {"code": "def extract_bboxes(mask)\n    bboxes = []\n    for mask_layer in mask:\n        # Find the indices where the mask is present\n        h_indices = np.where(mask_layer > 0)[0]\n        v_indices = np.where(mask_layer > 0)[1]\n        # If no mask is present, set the bounding box to zeros\n        if len(h_indices) == 0:\n            bbox = np.zeros((4,), dtype=np.int32)\n        else:\n            # Determine the coordinates of the bounding box\n            bbox = np.array([\n                np.min(h_indices),\n                np.min(v_indices),\n                np.max(h_indices),\n                np.max(v_indices)\n            ], dtype=np.int32)\n        bboxes.append(bbox)\n    return bboxes\n\n", "CodeBleu_score": 24.589352165760374}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # Calculate the intersection areas between the box and each box in the array\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # Calculate the union areas using the provided box areas and the intersection areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Compute the IoU values by dividing the intersection areas by the union areas\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # calculate union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # compute IoU\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 58.663259083089194}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # Calculate the intersection areas\n    iw = (torch.min(box[2], boxes[:, 2]) - torch.max(box[0], boxes[:, 0]) + 1).clamp(min=0)\n    ih = (torch.min(box[3], boxes[:, 3]) - torch.max(box[1], boxes[:, 1]) + 1).clamp(min=0)\n    inters = iw * ih\n\n    # Calculate the union areas\n    uni = (box_area + boxes_area[:] - inters)\n\n    # Compute the IoU values\n    iou = inters / uni\n\n    return iou\n\n", "CodeBleu_score": 27.948556950158622}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n    # If there are no boxes in either set of boxes, return an empty result.\n    if len(boxes1) == 0 or len(boxes2) == 0:\n        return np.zeros((0, len(boxes2)), dtype=np.float32)\n    # Initialize an array to store the overlap values for each pair of boxes.\n    overlaps = np.zeros((len(boxes1), len(boxes2)), dtype=np.float32)\n    # Calculate the area of each box in both sets.\n    area1 = np.prod(boxes1[:, 2:] - boxes1[:, :2], axis=1)\n    area2 = np.prod(boxes2[:, 2:] - boxes2[:, :2], axis=1)\n    # Iterate through each box in the second set.\n    for i, box2 in enumerate(boxes2):\n        # Calculate the intersection of the box with each box in the first set.\n        x1 = np.maximum(box2[0], boxes1[:, 0])\n        y1 = np.maximum(box2[1], boxes1[:, 1])\n        x2 = np.minimum(box2[2], boxes1[:, 2])\n        y2 = np.minimum(box2[3], boxes1[:, 3])\n        # Calculate the area of the intersection.\n        intersection = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n        # Calculate the Intersection over Union (IoU) value.\n        union = area1 + area2[i] - intersection\n        overlaps[:, i] = intersection / union\n    # Return the matrix of overlaps.\n    return overlaps\n", "CodeBleu_score": 32.34436742481973}, {"code": "def compute_overlaps(boxes1, boxes2)\n    # If there are no boxes in either set of boxes, return an empty result\n    if len(boxes1) == 0 or len(boxes2) == 0:\n        return np.zeros((0, len(boxes2)), dtype=np.float32)\n    # Initialize an array to store the overlaps between the two sets of boxes\n    overlaps = np.zeros((len(boxes1), len(boxes2)), dtype=np.float32)\n    # Compute the area of each box in both sets\n    area1 = np.prod(boxes1[:, 2:] - boxes1[:, :2], axis=1)\n    area2 = np.prod(boxes2[:, 2:] - boxes2[:, :2], axis=1)\n    # Iterate through each box in the second set\n    for i in range(boxes2.shape[0]):\n        # Compute the intersection over union (IoU) between the current box and all boxes in the first set\n        ixmin = np.maximum(boxes2[i, 0], boxes1[:, 0])\n        iymin = np.maximum(boxes2[i, 1], boxes1[:, 1])\n        ixmax = np.minimum(boxes2[i, 2], boxes1[:, 2])\n        iymax = np.minimum(boxes2[i, 3], boxes1[:, 3])\n        iw = np.maximum(ixmax - ixmin, 0)\n        ih = np.maximum(iymax - iymin, 0)\n        inters = iw * ih\n        uni = area1 + area2[i] - inters\n        overlaps[:, i] = inters / uni\n    # Return the matrix of overlaps\n    return overlaps", "CodeBleu_score": 34.7305981855506}, {"code": "def compute_overlaps(boxes1, boxes2)\n    N = len(boxes1)\n    M = len(boxes2)\n    overlaps = np.zeros((N, M))\n    for i in range(N):\n        for j in range(M):\n            overlaps[i, j] = compute_iou(boxes1[i], boxes2[j])\n    return overlaps\n\n", "CodeBleu_score": 13.957063111134687}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n    # Load image\n    image = Image.open(self.image_info[image_id]['path'])\n    # Convert image from BGR to RGB\n    image = image.convert('RGB')\n    # Remove the alpha channel\n    image = image[:, :, :3]\n    return image\n", "CodeBleu_score": 39.75985364659621}, {"code": "def load_image(self, image_id):\n    image_path = self.image_info[image_id]['path']\n    image = skimage.io.imread(image_path)\n    image = skimage.color.rgb2rgb(image)\n    if image.shape[2] == 4:\n        image = image[:, :, :3]\n    return image\n", "CodeBleu_score": 58.19200862052685}, {"code": "def load_image(self, image_id):\n    image = Image.open(self.image_info[image_id]['path'])\n    image = image.convert('RGB')\n    if image.mode == \"RGBA\":\n        image = image.convert(\"RGB\")\n    return image\n", "CodeBleu_score": 39.133793111726355}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title)\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.axis('off')\n        i += 1\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 55.300029799490304}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title if title else f'Image {i}')\n        plt.imshow(image.astype('uint8'), cmap=cmap, norm=norm,\n                   interpolation=interpolation)\n        plt.axis('off')\n        i += 1\n    plt.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 58.67503663593682}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    assert ((titles is None) or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1, n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images / float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()\n", "CodeBleu_score": 36.76492283051003}]}
