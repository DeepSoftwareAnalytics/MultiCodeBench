{"instance_id": "58936b2a621b", "generation_result": [{"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model", "CodeBleu_score": 83.87646776544821}, {"code": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model\n\n", "CodeBleu_score": 83.87646776544821}]}
{"instance_id": "83b142d9d251", "generation_result": [{"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    from sklearn.model_selection import train_test_split\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, test_size=1 - support_ratio, random_state=42, stratify=label\n    )\n\n    return x_train, x_test, y_train, y_test\n\n", "CodeBleu_score": 63.50399664351647}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    from sklearn.model_selection import train_test_split\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, test_size=1 - support_ratio, random_state=42, stratify=label\n    )\n    return x_train, x_test, y_train, y_test", "CodeBleu_score": 63.50399664351647}, {"code": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]\n    from sklearn.model_selection import train_test_split\n\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=1 - support_ratio,\n        random_state=42,\n        stratify=label,\n    )\n\n    return x_train, x_test, y_train, y_test", "CodeBleu_score": 61.72997477037986}]}
{"instance_id": "5fd02b540f61", "generation_result": [{"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    from sklearn.datasets import make_regression\n    from sklearn.model_selection import train_test_split\n    X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=1)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n    return X_train, y_train, X_test, y_test\n\n", "CodeBleu_score": 35.62631506853199}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    X, y = make_regression(\n        n_samples=1000,\n        n_features=3,\n        n_informative=3,\n        noise=10,\n        random_state=42,\n        bias=20,\n    )\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.33, random_state=42\n    )\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 35.10043383638951}, {"code": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)\n    # Import necessary modules\n    from sklearn.datasets import make_regression\n    from sklearn.model_selection import train_test_split\n\n    # Create regression dataset\n    X, y = make_regression(\n        n_samples=100, n_features=3, noise=10, random_state=1\n    )\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42\n    )\n\n    return X_train, X_test, y_train, y_test\n\n", "CodeBleu_score": 35.26105026987902}]}
{"instance_id": "ad958cb31a57", "generation_result": [{"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    gmm_param = GMMParameters()\n    gmm_param.label = ndarrays[0]\n    gmm_param.means = ndarrays[1]\n    gmm_param.weights = ndarrays[2]\n    gmm_param.covariances = ndarrays[3]\n    gmm_param.num_samples = ndarrays[4]\n    return gmm_param\n\n", "CodeBleu_score": 44.18137784622395}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n\n    # Initialize the GMMParameters object\n    gmm_param = GMMParameters()\n\n    # Map the elements of the ndarray to the corresponding attributes of the GMMParameters object\n    gmm_param.label = ndarrays[0]\n    gmm_param.means = ndarrays[1]\n    gmm_param.weights = ndarrays[2]\n    gmm_param.covariances = ndarrays[3]\n    gmm_param.num_samples = ndarrays[4]\n\n    # Return the GMMParameters object\n    return gmm_param", "CodeBleu_score": 44.18137784622395}, {"code": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters\n    gmm_param = GMMParameters()\n    gmm_param.label = ndarrays[0]\n    gmm_param.means = ndarrays[1]\n    gmm_param.weights = ndarrays[2]\n    gmm_param.covariances = ndarrays[3]\n    gmm_param.num_samples = ndarrays[4]\n    return gmm_param\n\n", "CodeBleu_score": 44.18137784622395}]}
{"instance_id": "061d5a5b9d56", "generation_result": [{"code": "def datafiles_fusion(data_paths):\n    # Load the first data file\n    X, Y = load_svmlight_file(data_paths[0])\n    # Convert X to a dense array\n    X = X.toarray()\n    # Initialize the list of feature arrays\n    features = [X]\n    # Initialize the list of label arrays\n    labels = [Y]\n    # Loop over the remaining data files\n    for data_path in data_paths[1:]:\n        # Load the data file\n        X, Y = load_svmlight_file(data_path)\n        # Convert X to a dense array\n        X = X.toarray()\n        # Check that the number of features is the same as the first file\n        assert X.shape[1] == features[0].shape[1]\n        # Append the new features and labels to the list\n        features.append(X)\n        labels.append(Y)\n    # Combine the features and labels into a single array\n    X = np.concatenate(features)\n    Y = np.concatenate(labels)\n    # Return the combined features and labels\n    return X, Y\n\n", "CodeBleu_score": 34.86268649173379}, {"code": "def datafiles_fusion(data_paths):\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.todense()\n    for data_path in data_paths[1:]:\n        X_temp, Y_temp = load_svmlight_file(data_path)\n        assert X_temp.shape[1] == X.shape[1], \"All data files must have the same number of features.\"\n        X = np.concatenate((X, X_temp.todense()), axis=0)\n        Y = np.concatenate((Y, Y_temp), axis=0)\n    return X, Y\n", "CodeBleu_score": 37.97246055486346}, {"code": "def datafiles_fusion(data_paths):\n\n    from sklearn.datasets import load_svmlight_file\n    import numpy as np\n\n    X, Y = load_svmlight_file(data_paths[0])\n    X = X.toarray()\n    for data_path in data_paths[1:]:\n        X_temp, Y_temp = load_svmlight_file(data_path)\n        if X_temp.shape[1] != X.shape[1]:\n            raise Exception(\"Number of features in data file \" + data_path + \" does not match previous files.\")\n        X = np.concatenate((X, X_temp.toarray()))\n        Y = np.concatenate((Y, Y_temp))\n    return X, Y\n\n", "CodeBleu_score": 30.71341943263653}]}
{"instance_id": "977d3bd5d5a3", "generation_result": [{"code": "def eval_learning(y_test, y_pred)\n\n    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"micro\")\n    precision = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n\n    return accuracy, recall, precision, f1", "CodeBleu_score": 61.39499773490273}, {"code": "def eval_learning(y_test, y_pred)\n    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n    return accuracy, recall, precision, f1\n", "CodeBleu_score": 54.6058043693212}, {"code": "def eval_learning(y_test, y_pred)\n    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1 = f1_score(y_test, y_pred, average='micro')\n\n    return accuracy, recall, precision, f1\n\n", "CodeBleu_score": 54.6058043693212}]}
{"instance_id": "f4c838f1f092", "generation_result": [{"code": "def train(model, train_loader, num_epochs=1)\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch in train_loader:\n            optimizer.zero_grad()\n            inputs, labels = batch\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n", "CodeBleu_score": 58.97995178495567}, {"code": "def train(model, train_loader, num_epochs=1)\n    model.train()\n    for epoch in range(num_epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            output = model(data)\n            loss = F.binary_cross_entropy(output, target)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        print(f\"Epoch: {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n\n", "CodeBleu_score": 45.370471122166}, {"code": "def train(model, train_loader, num_epochs=1)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = torch.nn.BCELoss()\n    for epoch in range(num_epochs):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n", "CodeBleu_score": 48.528595170322106}]}
{"instance_id": "35068c779c02", "generation_result": [{"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    if cfg.dataset.type != \"LIBSVM\":\n        raise RuntimeError(f\"Invalid dataset type: {cfg.dataset.type}\")\n\n    dataset_path = cfg.dataset.path\n    dataset_name = cfg.dataset.name\n\n    data, labels = load_svmlight_file(f\"{dataset_path}/{dataset_name}\")\n    data = data.todense()\n    data = data.astype(np.float32)\n    labels = labels.astype(np.float32)\n\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Shape of features: {data.shape}\")\n\n    if dataset_name == \"MUSHROOMS\":\n        labels[labels == 1] = 0\n        labels[labels != 0] = 1\n\n    return TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 38.09056833257839}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    if cfg.dataset.type != \"LIBSVM\":\n        raise RuntimeError(f\"Unsupported dataset type: {cfg.dataset.type}\")\n    dataset_path = Path(cfg.dataset.path)\n    dataset_name = dataset_path.name\n    if dataset_name == \"MUSHROOMS\":\n        data, labels = load_svmlight_file(dataset_path, zero_based=False)\n        labels = labels == 1\n    else:\n        raise RuntimeError(f\"Unsupported dataset name: {dataset_name}\")\n    data = data.toarray().astype(np.float32)\n    print(f\"Dataset {dataset_name} loaded:\")\n    print(f\"Unique labels: {np.unique(labels, return_counts=True)}\")\n    print(f\"Features shape: {data.shape}\")\n    return TensorDataset(torch.from_numpy(data), torch.from_numpy(labels))\n\n", "CodeBleu_score": 36.62499015736586}, {"code": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset\n    if cfg.dataset.type != \"LIBSVM\":\n        raise RuntimeError(\"Dataset type must be LIBSVM.\")\n\n    dataset_path = cfg.dataset.path\n    dataset_name = cfg.dataset.name\n\n    X, y = load_svmlight_file(os.path.join(dataset_path, dataset_name))\n\n    print(f\"Unique labels: {np.unique(y, return_counts=True)}\")\n    print(f\"Shape of features: {X.shape}\")\n\n    if dataset_name == \"MUSHROOMS\":\n        y = y.astype(int)\n        y[y == 1] = 0\n        y[y != 0] = 1\n\n    X = X.toarray().astype(np.float32)\n    y = y.astype(np.float32)\n\n    return TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n\n", "CodeBleu_score": 40.54641847124295}]}
{"instance_id": "d602a96e571e", "generation_result": [{"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, 'fit_transform'):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        transformer.fit(X, y, **fit_params)\n        res = transformer.transform(X)\n    if weight is not None:\n        res *= weight\n    return res, transformer\n\n", "CodeBleu_score": 64.2294818656124}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n\n    if hasattr(transformer, \"fit_transform\"):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        transformer.fit(X, y, **fit_params)\n        res = transformer.transform(X)\n\n    if weight is not None:\n        res = weight * res\n\n    return res, transformer\n\n", "CodeBleu_score": 61.76547992055998}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\n    if hasattr(transformer, \"fit_transform\"):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        transformer.fit(X, y, **fit_params)\n        res = transformer.transform(X)\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n", "CodeBleu_score": 74.40634089534446}]}
{"instance_id": "fcf88a773fd8", "generation_result": [{"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    if not isinstance(X, (np.ndarray, sparse.spmatrix)):\n        raise TypeError(\"X must be an array or sparse matrix\")\n    if not isinstance(selected, (list, tuple, np.ndarray, sparse.spmatrix)):\n        raise TypeError(\"selected must be an array or sparse matrix\")\n    if not isinstance(copy, bool):\n        raise TypeError(\"copy must be a boolean\")\n    if sparse.issparse(X):\n        if X.getformat() != \"csc\":\n            raise ValueError(\"sparse X must be in csc format\")\n        if sparse.issparse(selected):\n            if selected.getformat() != \"csc\":\n                raise ValueError(\"sparse selected must be in csc format\")\n    if sparse.issparse(selected):\n        if not sparse.issparse(X):\n            raise ValueError(\"X must be sparse if selected is sparse\")\n    if isinstance(selected, (list, tuple)):\n        selected = np.array(selected)\n    if isinstance(selected, np.ndarray):\n        if selected.ndim != 1:\n            raise ValueError(\"selected must be a 1-d array\")\n    if sparse.issparse(X):\n        if sparse.issparse(selected):\n            selected = selected.toarray().ravel()\n        else:\n            selected = np.asarray(selected)\n    if selected.size == 0:\n        return X\n    if selected.size == X.shape[1]:\n        return transform(X)\n    if selected.size == 0:\n        return X\n    if sparse.issparse(X):\n        if copy:\n            X = X.copy()\n        X[:, selected] = transform(X[:, selected])\n        return X\n    else:\n        Xt = transform(X[:, selected])\n        if copy:\n            X = X.copy()\n        X[:, selected] = Xt\n        return X\n\n", "CodeBleu_score": 35.38134180181494}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n    from numpy import array, asarray\n    from numpy import ndarray\n    from numpy import hstack as hstack_\n    from numpy import vstack as vstack_\n    from scipy.sparse import issparse\n    from scipy.sparse import hstack as hstack_sparse\n    from scipy.sparse import vstack as vstack_sparse\n    from scipy.sparse import csc_matrix\n    from sklearn.utils.validation import check_array\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.base import TransformerMixin\n    from sklearn.base import BaseEstimator\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.validation import check_array\n    from sklearn.utils.validation import FLOAT_DTYPES\n    from sklearn.utils.sparsefuncs import mean_variance_axis\n    from sklearn.utils.sparsefuncs_fast import inplace_csr_column_scale\n    from sklearn.utils.sparsefuncs_fast import inplace_csr_row_scale\n    from sklearn.utils.sparsefuncs import inplace_column_scale\n    from sklearn.utils.sparsefuncs import inplace_row_scale\n    from sklearn.utils.sparsefuncs import min_max_axis\n    from sklearn.preprocessing._data import _handle_zeros_in_scale\n    from sklearn.preprocessing._data import _mean_and_std\n    from sklearn.preprocessing._data import _handle_zeros_in_scale\n    from sklearn.preprocessing._data import _mean_and_std\n    from sklearn.preprocessing._data import _handle_zeros_in_scale\n    from sklearn.preprocessing._data import _mean_and_std\n    from sklearn.preprocessing._data import _handle_zeros_in_scale\n    from sklearn.preprocessing._data import _mean_and_std\n    from sklearn.preprocessing._data import _handle_zeros_in_scale\n    from sklearn.preprocessing._data import _mean_and_std\n    from sklearn.", "CodeBleu_score": 11.090093215804998}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\n\n    if selected == \"all\":\n        return transform(X)\n\n    X = check_array(X, accept_sparse=\"csc\", copy=copy)\n    n_samples, n_features = X.shape\n\n    if isinstance(selected, int):\n        selected = [selected]\n    elif not isinstance(selected, (list, tuple)):\n        raise ValueError(\n            \"Parameter selected should be either a list or tuple of integers or string 'all'\"\n        )\n\n    if len(selected) == 0:\n        return X\n\n    mask = np.ones(n_features, dtype=bool)\n    mask[np.asarray(selected)] = False\n\n    if len(selected) == n_features:\n        return transform(X)\n    else:\n        X_transformed = transform(X[:, selected])\n        if sparse.issparse(X):\n            X_transformed = sparse.hstack(\n                (X[:, mask], X_transformed), format=\"csc\"\n            ).tocsc()\n        else:\n            X_transformed = np.hstack((X[:, mask], X_transformed))\n        return X_transformed", "CodeBleu_score": 49.33947570732429}]}
{"instance_id": "c9dfb00047d1", "generation_result": [{"code": "def _transform_new(self, X)\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    if not isinstance(X, np.ndarray):\n        raise TypeError(\"X must be an array\")\n    if X.dtype.type is np.str_:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.object_:\n        raise TypeError(\"X must not contain strings\")\n    if X.dtype.type is np.float64:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.float32:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.int64:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.uint32:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.uint64:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.bool_:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.int16:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.uint16:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.uint8:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.int8:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.float16:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.complex64:\n        X = X.astype(np.int32)\n    if X.dtype.type is np.complex128:\n        X = X.astype(np.int32)\n    if X.d", "CodeBleu_score": 16.271990910522426}, {"code": "def _transform_new(self, X)\n\n    # Check the input array and adjust its data type if necessary\n    X = check_array(X, accept_sparse='csc', dtype=None, force_all_finite=False)\n\n    # Retrieve the number of samples and features in X\n    n_samples, n_features = X.shape\n\n    # Transform the input array into integer-encoded values and generate a mask for valid entries\n    X, mask = self._transform(X)\n\n    # Calculate the number of categories for each feature\n    n_values = np.bincount(X.ravel())\n    n_values[n_values == 0] = 1  # Avoid zero divisions\n    n_values = np.maximum(n_values, 2)  # At least 2 categories needed for one-hot encoding\n\n    # Compute feature indices\n    indices = np.cumsum([0] + list(n_values))\n\n    # Construct indices and indptr arrays for creating a sparse matrix\n    indptr = np.repeat(indices, n_samples)\n    indptr[1:] = indptr[:-1]\n    indptr = indptr.astype('int64')\n    indptr = np.cumsum(indptr)\n\n    # Generate the data array\n    data = np.ones(n_samples * n_features)\n    data[~mask] = 0\n\n    # Construct a sparse matrix\n    X_transformed = csr_matrix((data, X.ravel(), indptr),\n                               shape=(n_samples, indices[-1]),\n                               dtype=self.dtype)\n\n    # Return the transformed data\n    return X_transformed\n\n", "CodeBleu_score": 33.24827968202616}, {"code": "def _transform_new(self, X)\n\n    # Check input array\n    X = check_array(X, accept_sparse=True, force_all_finite=False, dtype=None)\n\n    # Retrieve number of samples and features\n    n_samples, n_features = X.shape\n\n    # Transform input array into integer-encoded values\n    X, mask = self._transform_helper(X)\n\n    # Calculate number of categories for each feature\n    n_values = np.max(X, axis=0) + 1\n\n    # Compute feature indices\n    ind = np.array([np.arange(n_samples), X[:, self.feature_indices_].flatten('F')]).T\n\n    # Construct sparse matrix\n    spmat = csr_matrix((mask.ravel(), ind.T),\n                       shape=(n_samples, np.sum(self.n_values_)))\n\n    # Convert to dense array if sparse=False\n    if self.sparse is False:\n        spmat = spmat.toarray()\n\n    # Return transformed data\n    return spmat", "CodeBleu_score": 17.96593202644965}]}
{"instance_id": "fcde391a8972", "generation_result": [{"code": "def convert_to_sklearn(self)\n    from scipy.stats import uniform, loguniform\n    if self.log:\n        return loguniform(self.low, self.high)\n    else:\n        return uniform(self.low, self.high)", "CodeBleu_score": 31.55322422683748}, {"code": "def convert_to_sklearn(self)\n    from scipy.stats import uniform, loguniform\n    if self.log:\n        return loguniform(self.lower, self.upper)\n    else:\n        return uniform(self.lower, self.upper)", "CodeBleu_score": 34.62120361072451}, {"code": "def convert_to_sklearn(self)\n\n    from scipy.stats import uniform, loguniform\n\n    if self.log:\n        return loguniform(self.lower, self.upper)\n    else:\n        return uniform(self.lower, self.upper)\n\n", "CodeBleu_score": 34.62120361072451}]}
{"instance_id": "5d5413adcca6", "generation_result": [{"code": "def _render(self, state: AnalysisState) -> None\n    additional_args = self._get_additional_args(state)\n    figure_args = self._get_figure_args(state)\n    features = self._get_features(state)\n\n    if self.headers:\n        self._render_header(state)\n\n    fig, axes = plt.subplots(**figure_args)\n    kwargs = {**self.kw_args, **additional_args}\n    if self.two_way_interaction:\n        kwargs[\"data\"] = state.data.dropna()\n    display = PartialDependenceDisplay.from_estimator(\n        state.model,\n        state.data,\n        features,\n        **kwargs,\n    )\n    display.plot(ax=axes)\n    fig.tight_layout()\n    plt.show()\n\n", "CodeBleu_score": 42.026292449612775}, {"code": "def _render(self, state: AnalysisState) -> None\n    ...\n\n", "CodeBleu_score": 2.3634453781512605}, {"code": "def _render(self, state: AnalysisState) -> None\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.inspection import PartialDependenceDisplay\n    from sklearn.preprocessing import LabelEncoder\n\n    additional_arguments = self._additional_arguments(state)\n    figure_arguments = self._figure_arguments(state)\n    features = self._features(state)\n\n    if self._header:\n        self._render_header(state)\n\n    # Create a figure and axes using plt.subplots with the specified figure arguments\n    fig, ax = plt.subplots(**figure_arguments)\n\n    # Prepare keyword arguments by merging additional arguments and predefined keyword arguments\n    kwargs = {\n        **additional_arguments,\n        \"ax\": ax,\n        \"features\": features,\n    }\n\n    # Filter the data for non-null values if two-way interaction plots are enabled\n    if self._two_way_interaction:\n        data = state.data.dropna()\n    else:\n        data = state.data\n\n    # Create PDPs from the model and data, plotting them on the specified axes\n    PartialDependenceDisplay.from_estimator(\n        state.estimator,\n        data,\n        **kwargs,\n    )\n\n    # Adjust the layout with tight_layout for better spacing\n    fig.tight_layout()\n\n    # Display the plots using plt.show()\n    plt.show()\n\n", "CodeBleu_score": 37.76986812604983}]}
{"instance_id": "db8aac29180b", "generation_result": [{"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_generator is None:\n        self.feature_generator = LabelEncoderFeatureGenerator(verbose=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.features:\n        X_copy = X.copy()\n        self.feature_generator.transform(X_copy)\n        X = X_copy\n    X.fillna(0, inplace=True)\n    return X.to_numpy(dtype='float32')", "CodeBleu_score": 42.287456505692994}, {"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_generator is None:\n        from sklearn.preprocessing import LabelEncoder\n        self.feature_generator = LabelEncoderFeatureGenerator(verbose=0)\n        self.feature_generator.fit(X)\n    if self.feature_generator.n_features_ > 0:\n        X = X.copy()\n        self.feature_generator.transform(X)\n    X = X.fillna(0)\n    return X.to_numpy(dtype=np.float32)", "CodeBleu_score": 46.11441854112478}, {"code": "def _preprocess(self, X, **kwargs)\n    X = super()._preprocess(X, **kwargs)\n    if self.feature_gen is None:\n        from sklearn.preprocessing import LabelEncoder\n        self.feature_gen = LabelEncoderFeatureGenerator(verbose=0)\n        self.feature_gen.fit(X)\n    if self.feature_gen.features:\n        X = X.copy()\n        for feature in self.feature_gen.features:\n            X[feature] = self.feature_gen.transform(X[feature])\n    X.fillna(0, inplace=True)\n    return X.to_numpy(dtype=np.float32)", "CodeBleu_score": 43.322568834268296}]}
{"instance_id": "d4eb45e6d013", "generation_result": [{"code": "def _estimate_memory_usage(self, X, **kwargs)\n    # Retrieve model parameters\n    model_params = self.get_params()\n    # Get the number of estimators\n    n_estimators = model_params[\"n_estimators\"]\n    # If a search space is defined, use the minimum number of estimators\n    if self.search_space:\n        n_estimators = model_params[\"n_estimators\"][0]\n    # If the number of estimators is less than 40, use 40\n    if n_estimators < 40:\n        n_estimators = 40\n    # Get the number of trees per estimator\n    n_trees = model_params[\"n_estimators\"]\n    # Get the size of the input data\n    size = X.memory_usage().sum()\n    # Calculate the bytes used per estimator\n    bytes_per_estimator = size * 0.01 * n_trees\n    # Calculate the expected minimum memory usage\n    expected_min_memory_usage = bytes_per_estimator * n_estimators\n    # Return the expected minimum memory usage\n    return expected_min_memory_usage\n\n", "CodeBleu_score": 40.37552423701275}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n    # Get model parameters\n    model_params = self.get_params()\n    # Set the final number of estimators\n    if model_params['n_estimators'] == 'auto':\n        if 'n_estimators_range' in kwargs:\n            model_params['n_estimators'] = kwargs['n_estimators_range'][0]\n        else:\n            model_params['n_estimators'] = 100\n    elif isinstance(model_params['n_estimators'], int):\n        if model_params['n_estimators'] < 40:\n            model_params['n_estimators'] = 40\n    elif isinstance(model_params['n_estimators'], list):\n        if model_params['n_estimators'][0] < 40:\n            model_params['n_estimators'][0] = 40\n    else:\n        raise ValueError('n_estimators must be \"auto\", an integer, or a list of integers')\n    # Get the number of trees per estimator\n    if model_params['n_estimators'] == 'auto':\n        n_trees = 1\n    else:\n        n_trees = int(model_params['n_estimators'])\n    # Get the number of features\n    n_features = X.shape[1]\n    # Get the number of classes\n    if hasattr(self, 'n_classes_'):\n        n_classes = self.n_classes_\n    else:\n        n_classes = 2\n    # Get the number of nodes per tree\n    if hasattr(self, 'max_leaf_nodes'):\n        n_nodes = self.max_leaf_nodes\n    else:\n        n_nodes = 1\n    # Calculate the bytes per estimator\n    bytes_per_estimator = n_trees * n_features * n_nodes * n_classes * 8\n    # Calculate the expected minimum memory usage\n    min_memory_usage = bytes_per_estimator * model_params['n_estimators']\n    return min_memory_usage", "CodeBleu_score": 32.54090387487559}, {"code": "def _estimate_memory_usage(self, X, **kwargs)\n    import numpy as np\n    import pandas as pd\n    import sys\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.model_selection import GridSearchCV\n\n    # Retrieve the model parameters\n    n_estimators = self.get_params()[\"n_estimators\"]\n    max_depth = self.get_params()[\"max_depth\"]\n    min_samples_split = self.get_params()[\"min_samples_split\"]\n    min_samples_leaf = self.get_params()[\"min_samples_leaf\"]\n    max_features = self.get_params()[\"max_features\"]\n    max_leaf_nodes = self.get_params()[\"max_leaf_nodes\"]\n    bootstrap = self.get_params()[\"bootstrap\"]\n    oob_score = self.get_params()[\"oob_score\"]\n    n_jobs = self.get_params()[\"n_jobs\"]\n    random_state = self.get_params()[\"random_state\"]\n    verbose = self.get_params()[\"verbose\"]\n    warm_start = self.get_params()[\"warm_start\"]\n    class_weight = self.get_params()[\"class_weight\"]\n    ccp_alpha = self.get_params()[\"ccp_alpha\"]\n    max_samples = self.get_params()[\"max_samples\"]\n\n    # Determine the final number of estimators\n    if n_estimators == 0:\n        n_estimators = 100\n    elif n_estimators < 40:\n        n_estimators = 40\n    elif \"n_estimators\" in kwargs:\n        n_estimators = kwargs[\"n_estimators\"]\n\n    # Create the model\n    model = RandomForestRegressor(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        max_features=max_features,\n        max_leaf_nodes=max_leaf_nodes,", "CodeBleu_score": 27.49096170484735}]}
{"instance_id": "6923cb007f4c", "generation_result": [{"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n    # create a Counter object from the n-gram frequency data\n    counter = Counter(ngram_freq)\n    # identify the top n most common n-grams according to the specified vocabulary size\n    top_n = counter.most_common(vocab_size)\n    # create a list of the n-grams\n    ngram_list = [ngram[0] for ngram in top_n]\n    # sort the n-grams\n    ngram_list.sort()\n    # create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {ngram: index for index, ngram in enumerate(ngram_list, 1)}\n    # update the vectorizer's vocabulary\n    vectorizer.vocabulary_ = new_vocab\n\n", "CodeBleu_score": 33.28827687457376}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\n    # Create a Counter object from the n-gram frequency data\n    counter = Counter(ngram_freq)\n\n    # Identify the top n most common n-grams according to the specified vocabulary size\n    ngram_freq_top_n = counter.most_common(vocab_size)\n\n    # Create a list of the names of the top n n-grams\n    ngram_freq_top_n_names = [ngram[0] for ngram in ngram_freq_top_n]\n\n    # Sort the list of the names of the top n n-grams\n    ngram_freq_top_n_names.sort()\n\n    # Create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {ngram: i for i, ngram in enumerate(ngram_freq_top_n_names)}\n\n    # Update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n\n    return vectorizer", "CodeBleu_score": 30.849763184996814}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\n\n    # create a counter object from the n-gram frequency data\n    counter = Counter(ngram_freq)\n\n    # identify the top n most common n-grams according to the specified vocabulary size\n    top_ngrams = counter.most_common(vocab_size)\n\n    # create a list of the top n most common n-grams\n    top_ngrams_list = [ngram[0] for ngram in top_ngrams]\n\n    # sort the list of top n most common n-grams\n    top_ngrams_list.sort()\n\n    # create a new vocabulary dictionary, mapping each n-gram to a unique index\n    new_vocab = {ngram: idx for idx, ngram in enumerate(top_ngrams_list, 1)}\n\n    # update the vectorizer's vocabulary with the new, downsized vocabulary\n    vectorizer.vocabulary_ = new_vocab\n\n    return vectorizer", "CodeBleu_score": 26.740128451051593}]}
{"instance_id": "0e837b2d8088", "generation_result": [{"code": " def _select_continuous(self, df, features)    skewness_threshold = self.parameters['skewness_threshold']\n    skewed_features = {}\n    skewed_features['continuous'] = []\n    skewed_features['skewed'] = []\n    skewness = df[features].skew()\n    for feature in skewness.index:\n      if skewness[feature] > skewness_threshold:\n        skewed_features['skewed'].append(feature)\n      else:\n        skewed_features['continuous'].append(feature)\n    return skewed_features\n\n\n  def _select_categorical(self, df, features):\n    \"\"\"# This function categorizes features in a DataFrame as either categorical or continuous based on their cardinality.\\nIt initializes a dictionary to store these feature types and retrieves the cardinality threshold from the parameters.\\nFor each feature, it calculates the cardinality and compares it to the threshold.\\nFeatures with cardinality exceeding the threshold are classified as \"categorical\", while others are classified as \"continuous\".\\nThe function returns a dictionary with lists of categorical and continuous features.\"\"\"\n    cardinality_threshold = self.parameters['cardinality_threshold']\n    categorical_features = {}\n    categorical_features['categorical'] = []\n    categorical_features['continuous'] = []\n    cardinality = df[features].nunique()\n    for feature in cardinality.index:\n      if cardinality[feature] > cardinality_threshold:\n        categorical_features['categorical'].append(feature)\n      else:\n        categorical_features['continuous'].append(feature)\n    return categorical_features\n\n\n  def _select_features(self, df):\n    \"\"\"# This function selects features for the model based on their type and the model type.\\nIt initializes a dictionary to store the selected features and retrieves the model type from the parameters.\\nIt then calls the appropriate feature selection function based on the model type.\\nThe function returns a dictionary with lists of selected features.\"\"\"\n    model_type = self.parameters['model_type']\n    selected_features = {}\n    selected_features['selected'] = []\n    if model_type ==", "CodeBleu_score": 34.43301211612288}, {"code": " def _select_continuous(self, df, features)    skewed_features = {}\n    skewness_threshold = self.parameters['skewness_threshold']\n    for col in features:\n        if df[col].skew() > skewness_threshold:\n            skewed_features[col] = \"skewed\"\n        else:\n            skewed_features[col] = \"continuous\"\n    return skewed_features", "CodeBleu_score": 33.04193803081715}, {"code": " def _select_continuous(self, df, features)    skewness_threshold = self.parameters['skewness_threshold']\n    skewed_features = []\n    continuous_features = []\n    skewness = df[features].skew()\n    for feature in skewness.index:\n        if skewness[feature] > skewness_threshold:\n            skewed_features.append(feature)\n        else:\n            continuous_features.append(feature)\n    return {'skewed': skewed_features, 'continuous': continuous_features}", "CodeBleu_score": 30.979858264163813}]}
{"instance_id": "bb569c10f975", "generation_result": [{"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        if dist.dtype.kind == \"f\":\n            # mask out any inf distances\n            mask = np.isinf(dist)\n            if np.any(mask):\n                dist = dist.copy()\n                dist[mask] = 0\n            # adjust distances so that no infinities are returned\n            dist += np.finfo(dist.dtype).eps\n            return 1.0 / dist\n        else:\n            return 1.0 / dist\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights not recognized: should be 'uniform', 'distance', or a callable function\")\n\n", "CodeBleu_score": 57.87854422966514}, {"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        if dist.dtype.kind == \"f\":\n            dist = np.where(np.isfinite(dist), dist, 0)\n            dist = np.where(dist == 0, np.finfo(float).eps, dist)\n        return 1.0 / dist\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\n            \"weights not recognized: should be 'uniform', \"\n            \"'distance', or a callable function\"\n        )\n\n", "CodeBleu_score": 54.94455680210859}, {"code": "def _get_weights(dist, weights):\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        w = 1.0 / dist\n        w[w == inf] = 0\n        w = w / w.sum(axis=1)[:, np.newaxis]\n        return w\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\n            \"weights not recognized: should be 'uniform', 'distance', or a callable function\"\n        )\n\n", "CodeBleu_score": 56.00068125966848}]}
{"instance_id": "12aaa9c78402", "generation_result": [{"code": "def predict_loo(self)\n\n    # Compute distances and indices of the k-nearest neighbors\n    dist, ind = self.kneighbors(self._X)\n\n    # Get weights for the neighbors\n    weights = self._get_weights(dist)\n\n    # If the target variable _y is one-dimensional, reshape it to a two-dimensional array\n    if self._y.ndim == 1:\n        self._y = self._y.reshape(self._y.shape[0], 1)\n\n    # Initialize a zero array to store the leave-one-out predictions\n    loo_pred = np.zeros(self._y.shape)\n\n    # Loop through the observations\n    for i in range(self._X.shape[0]):\n        # Get the indices of the k-nearest neighbors for the ith observation\n        ind_i = ind[i]\n\n        # Get the weights for the k-nearest neighbors for the ith observation\n        weights_i = weights[i]\n\n        # Get the target values of the k-nearest neighbors for the ith observation\n        y_i = self._y[ind_i]\n\n        # Calculate the leave-one-out prediction for the ith observation\n        if self.weights == \"uniform\":\n            loo_pred[i] = np.mean(y_i, axis=0)\n        else:\n            loo_pred[i] = np.sum(y_i * weights_i, axis=0) / np.sum(weights_i)\n\n    # If the original target variable _y was one-dimensional, flatten the predictions back to a one-dimensional array\n    if self._y.shape[1] == 1:\n        loo_pred = loo_pred.flatten()\n\n    return loo_pred", "CodeBleu_score": 36.069823614439365}, {"code": "def predict_loo(self)\n    from sklearn.neighbors import KNeighborsRegressor\n    from sklearn.neighbors import KNeighborsClassifier\n    import numpy as np\n    import pandas as pd\n\n    X = self._X\n    y = self._y\n    weights = self.weights\n    n_neighbors = self.n_neighbors\n    algorithm = self.algorithm\n    p = self.p\n    metric = self.metric\n\n    if self.is_fitted:\n        if self.is_classifier:\n            knn = KNeighborsClassifier(\n                n_neighbors=n_neighbors,\n                algorithm=algorithm,\n                p=p,\n                metric=metric,\n                weights=weights,\n            )\n        else:\n            knn = KNeighborsRegressor(\n                n_neighbors=n_neighbors,\n                algorithm=algorithm,\n                p=p,\n                metric=metric,\n                weights=weights,\n            )\n\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n\n        loo_predictions = np.zeros(y.shape)\n        for i in range(X.shape[0]):\n            neighbors = knn.kneighbors(X[i].reshape(1, -1), n_neighbors=n_neighbors)\n            indices = neighbors[1][0]\n            distances = neighbors[0][0]\n            weights = self._get_weights(distances)\n            if self.is_classifier:\n                loo_predictions[i] = np.argmax(\n                    np.sum(np.multiply(y[indices], weights), axis=0)\n                )\n            else:\n                loo_predictions[i] = np.sum(np.multiply(y[indices], weights), axis=0) / np.sum(\n                    weights\n                )\n\n        if y.ndim == 1:\n            loo_predictions = loo_predictions.flatten()", "CodeBleu_score": 29.985486953272744}, {"code": "def predict_loo(self)\n    import numpy as np\n    from sklearn.neighbors import KNeighborsRegressor\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import r2_score\n    from sklearn.model_selection import LeaveOneOut\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n\n    # Create a new KNeighborsRegressor object with the specified parameters\n    knn = KNeighborsRegressor(n_neighbors=self.n_neighbors,\n                              weights=self.weights,\n                              algorithm=self.algorithm,\n                              leaf_size=self.leaf_size,\n                              p=self.p,\n                              metric=self.metric,\n                              metric_params=self.metric_params,\n                              n_jobs=self.n_jobs)\n\n    # Fit the model to the training data\n    knn.fit(self.X_train, self.y_train)\n\n    # Get the distances and indices of the k-nearest neighbors\n    distances, indices = knn.kneighbors(self.X_train)\n\n    # Get the weights for the neighbors\n    weights = self._get_weights(distances)\n\n    # Get the k-nearest neighbor target values\n    neighbor_targets = self.y_train[indices]\n\n    # If the target variable is one-dimensional, reshape it to a two-dimensional array\n    if len(self.y_train.shape) == 1:\n        self.y_train = self.y_train.reshape(-1, 1)\n\n    # Compute the weighted predictions\n    weighted_predictions = np.sum(neighbor_targets * weights, axis=1) / np.sum(weights, axis=1)\n\n    # Compute the unweighted predictions\n    unweighted_predictions = np.mean(neighbor_targets, axis=1)\n\n    # If the original target variable was one-dimensional, flatten the predictions\n    if len(self", "CodeBleu_score": 29.739010789036058}]}
{"instance_id": "b316687d9e9b", "generation_result": [{"code": "def isotonic(input_data, quantile_list)\n    import numpy as np\n    from sklearn.isotonic import IsotonicRegression\n\n    quantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = len(input_data)\n    output_data = []\n\n    for i in range(batch_size):\n        data = input_data[i]\n        data = data.reshape(-1)\n        iso = IsotonicRegression(y_min=min(quantile_list), y_max=max(quantile_list))\n        iso.fit(np.arange(len(data)), data)\n        output_data.append(iso.transform(np.arange(len(data))))\n\n    output_data = np.stack(output_data)\n    return output_data", "CodeBleu_score": 51.40672330438536}, {"code": "def isotonic(input_data, quantile_list)\n    quantile_list = quantile_list.flatten()\n    batch_size = input_data.shape[0]\n    transformed_data = np.empty((batch_size, len(quantile_list)))\n    for i in range(batch_size):\n        iso = IsotonicRegression(quantile=quantile_list)\n        iso.fit(input_data[i, :], input_data[i, :])\n        transformed_data[i, :] = iso.transform(input_data[i, :])\n    return transformed_data\n\n", "CodeBleu_score": 37.66742939464142}, {"code": "def isotonic(input_data, quantile_list)\n\n    # Reshape quantile list into 1D array\n    quantile_list = quantile_list.reshape(len(quantile_list))\n\n    # Determine batch size\n    batch_size = input_data.shape[0]\n\n    # Initialize transformed data array\n    transformed_data = np.empty((batch_size, len(quantile_list)))\n\n    # For each row in the input data\n    for i in range(batch_size):\n        # Fit and transform the data using IsotonicRegression\n        isotonic_regressor = IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')\n        isotonic_regressor.fit(quantile_list, input_data[i])\n        transformed_data[i] = isotonic_regressor.transform(quantile_list)\n\n    # Return transformed data\n    return transformed_data", "CodeBleu_score": 38.54653162570207}]}
{"instance_id": "17566e088b4e", "generation_result": [{"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    matrix = vectorizer.transform(text_data)\n    return vectorizer, matrix\n\n", "CodeBleu_score": 45.42697112561172}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, vectorizer.transform(text_data)\n\n", "CodeBleu_score": 40.6826747095547}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\n    vectorizer.fit(text_data)\n    vectorizer.stop_words_ = None\n    return vectorizer, vectorizer.transform(text_data)\n\n", "CodeBleu_score": 40.6826747095547}]}
{"instance_id": "be23c0afc2d6", "generation_result": [{"code": "def preprocess(df, test_size, shuffle)\n    # Drop rows with missing entries\n    df = df.dropna()\n    # Drop columns with unique values less than or equal to one or containing \"id\" in their name\n    df = df.drop(df.columns[df.nunique() <= 1], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains(\"id\")], axis=1)\n    # Drop shortcut columns like \"days_to_death\" and \"year_of_death\"\n    df = df.drop(df.columns[df.columns.str.contains(\"days_to_\")], axis=1)\n    df = df.drop(df.columns[df.columns.str.contains(\"year_of_\")], axis=1)\n    # Split into training and testing sets\n    train, test = train_test_split(df, test_size=test_size, shuffle=shuffle)\n    return train, test\n\n", "CodeBleu_score": 26.179483155899387}, {"code": "def preprocess(df, test_size, shuffle)\n    df = df.dropna()\n    df = df.drop(columns=['id', 'days_to_death', 'year_of_death'])\n    df = df.loc[:, (df != 1).any(axis=0)]\n    df = df.loc[:, (df != '--').any(axis=0)]\n    df = df.loc[:, ~df.columns.str.contains('id')]\n    df = df.loc[:, ~df.columns.str.contains('days_to_death')]\n    df = df.loc[:, ~df.columns.str.contains('year_of_death')]\n    from sklearn.model_selection import train_test_split\n    train, test = train_test_split(df, test_size=test_size, shuffle=shuffle)\n    return train, test", "CodeBleu_score": 34.53223906678531}, {"code": "def preprocess(df, test_size, shuffle)\n\n    # Drop rows with missing entries\n    df = df.dropna()\n\n    # Drop columns with unique values less than or equal to one or containing \"id\" in their name\n    df = df.loc[:, (df.nunique() > 1) & (~df.columns.str.contains(\"id\"))]\n\n    # Drop shortcut columns\n    df = df.drop([\"days_to_death\", \"year_of_death\"], axis=1)\n\n    # Split into training and testing sets\n    X_train, X_test = train_test_split(df, test_size=test_size, shuffle=shuffle)\n\n    return X_train, X_test", "CodeBleu_score": 20.59321892935757}]}
{"instance_id": "6a82997a8c59", "generation_result": [{"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n\n    # Load the dataset\n    X_train, X_test, y_train, y_test = _load_dataset(dataset, sparse)\n\n    # Retrieve the classifier's hyperparameter search space and default configuration\n    search_space = classifier.get_hyperparameter_search_space()\n    default = search_space.get_default_configuration()\n\n    # Initialize the classifier with the default configuration and random state\n    clf = classifier(random_state=1, **{hp_name: default[hp_name] for hp_name in default})\n\n    # Fit the classifier iteratively to the training data for two iterations, with refitting enabled\n    clf.fit_transformer(X_train, y_train, refit=True, n_iter=2)\n\n    # Fit the classifier iteratively to the training data until it is fully fitted\n    while not clf.configuration_fully_fitted():\n        clf.fit_transformer(X_train, y_train, refit=False, n_iter=clf.iterations * 2)\n\n    # Make predictions on the test data\n    predictions = clf.predict(X_test)\n\n    return predictions, y_test, clf", "CodeBleu_score": 25.938093405703466}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n\n    # Load data\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n\n    if sparse:\n        X_train = X_train.toarray()\n        X_test = X_test.toarray()\n\n    # Get hyperparameter search space and default configuration\n    cs = classifier.get_hyperparameter_search_space()\n    default = cs.get_default_configuration()\n\n    # Initialize classifier with default configuration and random state\n    clf = classifier(random_state=1, **{hp_name: default[hp_name] for hp_name in default})\n\n    # Fit classifier iteratively\n    clf.iterative_fit(X_train, y_train, n_iter=2, refit=True)\n    n_iter = 2\n    while not clf.configuration_fully_fitted():\n        clf.iterative_fit(X_train, y_train, n_iter=n_iter)\n        n_iter *= 2\n\n    # Make predictions\n    predictions = clf.predict(X_test)\n\n    return predictions, y_test, clf\n\n", "CodeBleu_score": 45.04421803776765}, {"code": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)\n    from sklearn.datasets import load_iris\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n    from skopt.space import Real, Integer\n    from skopt.utils import use_named_args\n    from hyperactive import Hyperactive\n\n    data = load_iris()\n    X, y = data.data, data.target\n\n    if sparse:\n        X = X.toarray()\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n\n    search_space = classifier.get_hyperparameter_search_space()\n    default = classifier.get_default_parameters()\n    default = {str(k): v for k, v in default.items()}\n\n    @use_named_args(search_space)\n    def objective(**params):\n        params = {str(k): v for k, v in params.items()}\n        params = {**default, **params}\n        params[\"random_state\"] = 0\n        model = classifier(**params)\n        model.fit(X_train, y_train)\n        return -accuracy_score(y_test, model.predict(X_test))\n\n    hyper = Hyperactive()\n    hyper.add_search(objective, search_space, n_iter=1)\n    hyper.run()\n\n    best_para = hyper.best_para(objective)\n    best_para = {str(k): v for k, v in best_para.items()}\n    best_para = {**default, **best_para}\n    best_para[\"random_state\"] = 0\n\n    model = classifier(**best_para)\n    model.fit(X_train, y_train)\n\n    for i in range(2, 10):\n        model.iterative_fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n\n    return predictions, y_test, model", "CodeBleu_score": 31.555638707788763}]}
{"instance_id": "d0d8450c4c17", "generation_result": [{"code": "def _load_models(self)\n    if self.ensemble_class is not None:\n        self.ensemble_ = self.backend_.load_ensemble(self.seed)\n        if self.ensemble_ is not None:\n            self.identifiers_ = self.backend_.load_ensemble_identifiers(self.seed)\n            self.models_ = self.backend_.load_models_by_identifiers(self.identifiers_)\n            self.cv_models_ = self.backend_.load_cv_models_by_identifiers(self.identifiers_)\n        else:\n            self.models_ = []\n            self.cv_models_ = []\n    elif self.ensemble_class is None and self.resampling_strategy in ['holdout', 'holdout-iterative-fit',\n                                                                      'cv', 'cv-iterative-fit', 'partial-cv',\n                                                                      'partial-cv-iterative-fit']:\n        self.ensemble_ = self.backend_.load_ensemble(self.seed)\n        if self.ensemble_ is not None:\n            self.identifiers_ = self.backend_.load_ensemble_identifiers(self.seed)\n            self.models_ = self.backend_.load_models_by_identifiers(self.identifiers_)\n            self.cv_models_ = self.backend_.load_cv_models_by_identifiers(self.identifiers_)\n        else:\n            self.models_ = []\n            self.cv_models_ = []\n    else:\n        self.models_ = []\n        self.cv_models_ = []\n\n", "CodeBleu_score": 42.15470116421778}, {"code": "def _load_models(self)\n    if self.ensemble_class is not None:\n        self.ensemble = self.ensemble_class(\n            backend=self.backend,\n            seed=self.seed,\n        )\n        if self.ensemble.weights is None:\n            self.ensemble = None\n            self.ensemble_class = None\n            self.resampling_strategy = None\n            self.resampling_strategy_arguments = None\n        else:\n            self.model_identifiers = self.ensemble.get_model_identifiers()\n            self.models = [\n                self.backend.load_model(model_identifier)\n                for model_identifier in self.model_identifiers\n            ]\n            self.cv_models = [\n                self.backend.load_cv_model(model_identifier)\n                for model_identifier in self.model_identifiers\n            ]\n    else:\n        if (\n            self.resampling_strategy in [\"holdout\", \"holdout-iterative-fit\"]\n            and self.num_run == 1\n            and self.num_run_per_config == 1\n            and self.ensemble_size == 1\n            and self.ensemble_nbest == 1\n        ):\n            self.model_identifiers = [self.model_identifier]\n            self.models = [self.backend.load_model(self.model_identifier)]\n            self.cv_models = [self.backend.load_cv_model(self.model_identifier)]\n        else:\n            self.model_identifiers = []\n            self.models = []\n            self.cv_models = []\n\n", "CodeBleu_score": 37.65014945559021}, {"code": "def _load_models(self)\n    # Load ensemble\n    if self.ensemble_class is not None:\n        self.ensemble = self.ensemble_class(\n            backend=self.backend,\n            random_seed=self.seed,\n            model_class=self.model_class,\n            model_kwargs=self.model_kwargs,\n        )\n        self.ensemble.load(self.ensemble_path)\n    else:\n        self.ensemble = None\n    # Load best individual model\n    if self.ensemble is None and self.evaluator.output_condition == \"individual\":\n        self.ensemble = self.model_class(**self.model_kwargs)\n        self.ensemble.load(self.ensemble_path)\n    # Load models\n    if self.ensemble is not None:\n        self.model_identifiers = self.ensemble.get_model_identifiers()\n        self.models = [\n            self.model_class(**self.model_kwargs)\n            for _ in range(len(self.model_identifiers))\n        ]\n        for model, identifier in zip(self.models, self.model_identifiers):\n            model.load(self.backend.get_model_path(identifier))\n    else:\n        self.models = []\n    # Load cross-validated models\n    if self.ensemble is not None and self.resampling_strategy in [\n        \"holdout\",\n        \"holdout-iterative-fit\",\n        \"cv-iterative-fit\",\n    ]:\n        self.cv_models = [\n            self.model_class(**self.model_kwargs)\n            for _ in range(len(self.model_identifiers))\n        ]\n        for cv_model, identifier in zip(self.cv_models, self.model_identifiers):\n            cv_model.load(self.backend.get_cv_model_path(identifier))\n    else:\n        self.cv_models = []\n    # Initialize empty lists\n    if self.ensemble is None:\n        self.models = []\n        self.cv_models = []\n\n", "CodeBleu_score": 39.200094509349285}]}
{"instance_id": "ef71b1cddb50", "generation_result": [{"code": "def load_task(task_id)\n    import numpy as np\n    import openml\n    from sklearn.preprocessing import LabelEncoder\n\n    # Load the task\n    task = openml.tasks.get_task(task_id)\n\n    # Extract features and labels\n    X, y, categorical_indicator, _ = task.get_X_and_y()\n\n    # Obtain training-testing split indices\n    train_indices, test_indices = task.get_train_test_split_indices()\n\n    # Create training and testing features\n    X_train = X[train_indices]\n    X_test = X[test_indices]\n\n    # Create training and testing labels\n    y_train = y[train_indices]\n    y_test = y[test_indices]\n\n    # Obtain the dataset associated with the task\n    dataset = openml.datasets.get_dataset(task.dataset_id)\n\n    # Determine categorical or numerical nature of each feature\n    categorical = dataset.attributes[dataset.default_target_attribute]['nominal']\n    categorical = np.append(categorical, False)\n\n    # Map unique label values to integer indices\n    encoder = LabelEncoder()\n    y_train = encoder.fit_transform(y_train)\n    y_test = encoder.transform(y_test)\n\n    # Return training features, training labels, testing features, testing labels, and categorical feature indicator\n    return X_train, y_train, X_test, y_test, categorical\n", "CodeBleu_score": 49.6644709636348}, {"code": "def load_task(task_id)\n    from openml import datasets, tasks\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelEncoder\n    import numpy as np\n    # Load the task\n    task = tasks.get_task(task_id)\n    # Retrieve features and labels\n    X, y, categorical_indicator, _ = task.get_X_and_y()\n    # Get train-test split indices\n    train_indices, test_indices = task.get_train_test_split_indices(repeat=0, fold=0)\n    # Split features and labels\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    # Get dataset\n    dataset = datasets.get_dataset(task.dataset_id)\n    # Get feature types\n    feature_types = dataset.features\n    # Get list of categorical features\n    categorical_features = [i for i, x in enumerate(feature_types) if x == 'nominal']\n    # Encode labels\n    label_encoder = LabelEncoder()\n    y_train = label_encoder.fit_transform(y_train)\n    y_test = label_encoder.transform(y_test)\n    # Convert to numpy arrays\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    y_test = np.array(y_test)\n    # Return results\n    return X_train, y_train, X_test, y_test, categorical_features", "CodeBleu_score": 47.08367180416326}, {"code": "def load_task(task_id)\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelEncoder\n    from openml import datasets, tasks\n    from openml.exceptions import OpenMLServerException\n    from openml.datasets.functions import _get_dataset_by_id\n    from openml.tasks import Task\n    from openml.datasets import get_dataset\n    import numpy as np\n    import pandas as pd\n\n    # Load the task\n    task = tasks.get_task(task_id)\n\n    # Load the dataset\n    dataset = _get_dataset_by_id(task.dataset_id)\n\n    # Get the features and labels\n    X, y, categorical_indicator, _ = dataset.get_data(\n        dataset_format=\"array\", target=dataset.default_target_attribute\n    )\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=task.get_split_value(fold=0)\n    )\n\n    # Map the labels to integers\n    label_encoder = LabelEncoder()\n    y_train = label_encoder.fit_transform(y_train)\n    y_test = label_encoder.transform(y_test)\n\n    # Determine which features are categorical\n    categorical = [True if i == \"categorical\" else False for i in categorical_indicator]\n\n    return X_train, y_train, X_test, y_test, categorical\n\n", "CodeBleu_score": 31.14524691248367}]}
{"instance_id": "90bd6edbcc9e", "generation_result": [{"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if issparse(y):\n        y = y.toarray()\n        if y.shape[1] == 1:\n            y = y.flatten()\n    return y\n\n", "CodeBleu_score": 55.4541965764666}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n    if issparse(y):\n        y = y.toarray()\n        if len(y.shape) == 1:\n            y = y.flatten()\n    return y", "CodeBleu_score": 52.714949459114436}, {"code": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]\n\n    if issparse(y):\n        y = y.toarray()\n        if y.ndim == 1:\n            y = y.flatten()\n        if isinstance(y, np.ndarray):\n            y = y.astype(np.float32)\n    return y\n\n", "CodeBleu_score": 56.79655236670087}]}
{"instance_id": "5728e3d4f914", "generation_result": [{"code": "def _calculate_weights(self) -> None\n    # Count the occurrences of each ensemble member index\n    occurrences = np.bincount(self.ensemble_preds.flatten())\n    # Initialize weights array with zeros\n    weights = np.zeros(self.ensemble_preds.shape[1])\n    # Calculate the weight for each ensemble member\n    for i in range(len(weights)):\n        weights[i] = occurrences[i] / len(self.ensemble_preds)\n    # If the sum of weights is less than 1, normalize the weights\n    if np.sum(weights) < 1:\n        weights = weights / np.sum(weights)\n    # Store the calculated weights in the instance variable self.weights_\n    self.weights_ = weights", "CodeBleu_score": 41.917105382379994}, {"code": "def _calculate_weights(self) -> None\n    # Initialize an array with zeros\n    weights = np.zeros(self.n_models)\n    # Count the occurrences of each ensemble member index\n    for i in self.ensemble_:\n        weights[i] += 1\n    # Calculate the weight for each ensemble member\n    for i in range(self.n_models):\n        weights[i] /= self.n_models\n    # If the sum of weights is less than 1, normalize the weights to ensure their sum equals 1\n    if np.sum(weights) < 1:\n        weights /= np.sum(weights)\n    # Store the calculated weights in the instance variable self.weights_\n    self.weights_ = weights\n    return None", "CodeBleu_score": 31.113159108546007}, {"code": "def _calculate_weights(self) -> None\n\n    # Count the occurrences of each ensemble member index\n    occurrences = np.bincount(self.ensemble_members_indexes)\n\n    # Initialize a weights array with zeros\n    weights = np.zeros(len(self.input_models))\n\n    # For each ensemble member, calculate the weight as the frequency divided by the ensemble size\n    for i in range(len(self.input_models)):\n        weights[i] = occurrences[i] / len(self.ensemble_members_indexes)\n\n    # If the sum of weights is less than 1, normalize the weights to ensure their sum equals 1\n    if sum(weights) < 1:\n        weights = weights / sum(weights)\n\n    # Store the calculated weights in the instance variable self.weights_\n    self.weights_ = weights\n\n    return None", "CodeBleu_score": 34.52472192601404}]}
{"instance_id": "4f9c4799fd8f", "generation_result": [{"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    probabilities = super().predict_proba(X, batch_size, n_jobs)\n    if self.target_type != \"multilabel-indicator\":\n        assert np.allclose(probabilities.sum(axis=1), 1)\n    assert np.all(probabilities >= 0) and np.all(probabilities <= 1)\n    return probabilities", "CodeBleu_score": 28.766007360115808}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    probas = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    if self.target_type != \"multilabel-indicator\":\n        assert np.allclose(np.sum(probas, axis=1), np.ones(len(probas))), \"The sum of probabilities for each instance must be 1.\"\n    assert np.all(np.min(probas, axis=1) >= 0), \"All probabilities must be greater than or equal to 0.\"\n    assert np.all(np.max(probas, axis=1) <= 1), \"All probabilities must be less than or equal to 1.\"\n    return probas", "CodeBleu_score": 37.956488190982604}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1)\n    # Get the predicted probabilities from the superclass\n    proba = super().predict_proba(X, batch_size, n_jobs)\n\n    # If the target type is not \"multilabel-indicator\", assert that the sum of probabilities for each instance is 1\n    if self.target_type != \"multilabel-indicator\":\n        assert np.allclose(proba.sum(axis=1), 1), \"The sum of probabilities for each instance must be 1\"\n\n    # Assert that all probability values lie between 0 and 1\n    assert (proba >= 0).all() and (proba <= 1).all(), \"All probability values must lie between 0 and 1\"\n\n    return proba", "CodeBleu_score": 37.70325832584186}]}
{"instance_id": "a9a13150aebe", "generation_result": [{"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    if Ybin.ndim == 1:\n        return np.array([int(x) for x in Ybin])\n    else:\n        return np.dot(Ybin, np.arange(Ybin.shape[1]))\n", "CodeBleu_score": 31.846831260379467}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    if Ybin.ndim == 1:\n        return Ybin\n    else:\n        return np.dot(Ybin, range(Ybin.shape[1]))\n\n", "CodeBleu_score": 32.043052488809614}, {"code": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    if Ybin.ndim == 1:\n        return np.dot(Ybin, np.arange(Ybin.shape[0]))\n    else:\n        return np.dot(Ybin, np.arange(Ybin.shape[1]))\n\n", "CodeBleu_score": 30.073244157935125}]}
{"instance_id": "e97d38e62f81", "generation_result": [{"code": "def fit_transformer(self, X, y, fit_params=None)\n    if fit_params is None:\n        fit_params = {}\n    self.config.update_initialization_params(self.config.initialization_params)\n    if self.config.balancing_strategy == \"weighting\":\n        self.balancing = Balancing(self.config.balancing_strategy, self.config.classification_type, self.config.scoring_metric)\n        self.balancing.get_weights(X, y)\n        self.config.update_hyperparams(self.config.initialization_params, self.balancing.classifier_weights, self.balancing.preprocessor_weights)\n        self.config.update_hyperparams(self.config.initialization_params, self.config.classifier_params, self.config.preprocessor_params)\n    fit_params.update(self.config.fit_params)\n    X_transformed = super().fit_transformer(X, y, fit_params)\n    return X_transformed, fit_params\n\n", "CodeBleu_score": 44.42543759162275}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n    if fit_params is None:\n        fit_params = {}\n    if self.balancing_strategy == \"weighting\":\n        balancing = Balancing(self.config)\n        fit_params.update(balancing.fit(X, y))\n    self.update_params()\n    self.set_hyperparams(self.config, self.init_params)\n    fit_params.update(self.fit_params)\n    X_transformed = super().fit_transformer(X, y, fit_params)\n    return X_transformed, fit_params\n", "CodeBleu_score": 37.613871638272016}, {"code": "def fit_transformer(self, X, y, fit_params=None)\n\n    if fit_params is None:\n        fit_params = {}\n\n    if self.balancing_strategy == \"weighting\":\n        balancing = Balancing(self.config, self.init_params)\n        self.weights = balancing.get_weights(X, y)\n\n    self.init_params.update(self.config)\n    self.set_hyperparameters(self.init_params)\n    fit_params.update(self.fit_params)\n\n    X_transformed = super().fit_transformer(X, y, fit_params)\n\n    return X_transformed, fit_params", "CodeBleu_score": 40.63544693938218}]}
{"instance_id": "b1aaad80a9c4", "generation_result": [{"code": "def test_glm_identity_link_func()\n    import pandas as pd\n    from sklearn.datasets import load_boston\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import r2_score\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import median_absolute_error\n    from sklearn.metrics import explained_variance_score\n    from sklearn.metrics import mean_squared_log_error\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import recall_score\n    from sklearn.metrics import f1_score\n    from sklearn.metrics import roc_auc_score\n    from sklearn.metrics import roc_curve\n    from sklearn.metrics import auc\n    from sklearn.metrics import precision_recall_curve\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import multilabel_confusion_matrix\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import median_absolute_error\n    from sklearn.metrics import explained_variance_score\n    from sklearn.metrics import mean_squared_log_error\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import recall_score\n    from sklearn.metrics import f1_score\n    from sklearn.metrics import roc_auc_score\n    from sklearn.metrics import roc_curve\n    from sklearn.metrics import auc\n    from sklearn.metrics import precision_recall_curve\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import multilabel_confusion_matrix\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import mean", "CodeBleu_score": 7.159264399686499}, {"code": "def test_glm_identity_link_func()\n\n    import pandas as pd\n    import numpy as np\n    from sklearn.datasets import load_boston\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import median_absolute_error\n    from sklearn.metrics import r2_score\n    from sklearn.metrics import explained_variance_score\n    from sklearn.metrics import max_error\n    from sklearn.metrics import mean_squared_log_error\n    from sklearn.metrics import mean_poisson_deviance\n    from sklearn.metrics import mean_gamma_deviance\n    from sklearn.metrics import mean_tweedie_deviance\n    from sklearn.metrics import mean_pinball_loss\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import median_absolute_error\n    from sklearn.metrics import r2_score\n    from sklearn.metrics import explained_variance_score\n    from sklearn.metrics import max_error\n    from sklearn.metrics import mean_squared_log_error\n    from sklearn.metrics import mean_poisson_deviance\n    from sklearn.metrics import mean_gamma_deviance\n    from sklearn.metrics import mean_tweedie_deviance\n    from sklearn.metrics import mean_pinball_loss\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import median_absolute_error\n    from sklearn.metrics import r2_score\n    from sklearn.metrics import explained_variance_score\n    from sklearn.metrics import max_error\n    from sklearn.metrics import mean_squared_log_error\n    from sklearn.metrics import mean_poisson_deviance\n    from sklearn.metrics import mean_gamma_deviance\n    from sklearn.metrics import mean_t", "CodeBleu_score": 6.0621288251910554}, {"code": "def test_glm_identity_link_func()\n    import pandas as pd\n    import numpy as np\n    import pyspark.sql.functions as F\n    from pyspark.sql.types import *\n    from pyspark.ml import Pipeline\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.linalg import Vectors\n    from pyspark.sql import Row\n    from pyspark.ml.feature import OneHotEncoder, StringIndexer\n    from pyspark.sql.functions import col, udf\n    from pyspark.sql.types import IntegerType, FloatType\n    from pyspark.ml.linalg import DenseVector\n    from pyspark.ml.feature import OneHotEncoderEstimator\n    from pyspark.ml.feature import VectorAssembler\n    from pyspark.ml.regression import GeneralizedLinearRegression\n    from pyspark.ml.evaluation import RegressionEvaluator\n    from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n    from pyspark.ml.feature import MinMaxScaler\n    from pyspark.ml.feature import StandardScaler\n    from pyspark.ml.feature import RobustScaler\n    from pyspark.ml.feature import QuantileDiscretizer\n    from pyspark.ml.feature import Bucketizer\n    from pyspark.ml.feature import ElementwiseProduct\n    from pyspark.ml.feature import SQLTransformer\n    from pyspark.ml.feature import VectorIndexer\n    from pyspark.ml.feature import RFormula\n    from pyspark.ml.feature import ChiSqSelector\n    from pyspark.ml.feature import VectorSlicer\n    from pyspark.ml.wrapper import JavaModel\n    from pyspark.ml.classification import LogisticRegression\n    from pyspark.ml.feature import PCA\n    from pyspark.ml.feature import PolynomialExpansion\n    from pyspark.ml.feature import BucketedRandomProjectionLSH\n    from pyspark.ml.feature import ElementwiseProduct\n    from pyspark.", "CodeBleu_score": 7.129726511728399}]}
{"instance_id": "bcab63e592d0", "generation_result": [{"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n\n    # If the aggregation is provided as a tuple, extract the name and function; otherwise, determine the name from the string or function name.\n    if isinstance(agg, tuple):\n        name, func = agg\n    else:\n        func = agg\n        if isinstance(agg, str):\n            name = agg\n        else:\n            name = func.__name__\n\n    # Construct a new column name based on the original column name and the aggregation name.\n    new_col_name = f\"{col.name}_{name}\"\n\n    # Apply the aggregation to the grouped DataFrame, with the new column name.\n    return self.agg({new_col_name: func})[new_col_name]\n\n", "CodeBleu_score": 57.37899685684237}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n    if isinstance(agg, str):\n        agg_name = agg\n        agg_func = None\n    elif isinstance(agg, Callable):\n        agg_name = agg.__name__\n        agg_func = agg\n    elif isinstance(agg, tuple):\n        agg_name = agg[0]\n        agg_func = agg[1]\n    else:\n        raise ValueError(\n            f\"Invalid aggregation {agg}.\\nAggregation must be a string, function, or tuple.\"\n        )\n    if agg_func is None:\n        agg_func = getattr(pd.core.groupby.DataFrameGroupBy, agg_name)\n    if agg_name in self.columns:\n        agg_name = f\"{col}__{agg_name}\"\n    return agg_func(self[col])\n\n", "CodeBleu_score": 40.023775808844846}, {"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\n    if isinstance(agg, tuple):\n        agg_name, agg_func = agg\n    elif isinstance(agg, str):\n        agg_name = agg\n        agg_func = agg\n    else:\n        agg_name = agg.__name__\n        agg_func = agg\n\n    if isinstance(agg_func, str):\n        agg_func = getattr(pd.Series, agg_func)\n\n    col_name = col.name\n    new_col_name = f\"{col_name}_{agg_name}\"\n    return self.agg({col_name: agg_func}).rename(columns={col_name: new_col_name})[new_col_name]\n\n", "CodeBleu_score": 46.359926946303055}]}
{"instance_id": "114d623dae7b", "generation_result": [{"code": "def _corrfunc(self, x, y, **kws)\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.font_manager import FontProperties\n    from matplotlib.offsetbox import AnchoredText\n    from scipy.stats import pearsonr\n    from itertools import cycle\n    # Handle optional hue groups\n    if 'hue' in kws:\n        hue = kws['hue']\n        hue_kws = {'hue':hue}\n    else:\n        hue = None\n        hue_kws = {}\n    # Handle optional size groups\n    if 'size' in kws:\n        size = kws['size']\n        size_kws = {'size':size}\n    else:\n        size = None\n        size_kws = {}\n    # Get the data, excluding rows where x or y is NaN\n    data = pd.concat([x,y], axis=1).dropna()\n    # Compute the correlation coefficient\n    r, pval = pearsonr(data[x.name], data[y.name])\n    # Get the axes\n    if 'ax' in kws:\n        ax = kws['ax']\n    else:\n        ax = plt.gca()\n    # Get the figure\n    fig = ax.figure\n    # Get the figure width and height\n    fig_width = fig.get_figwidth()\n    fig_height = fig.get_figheight()\n    # Get the number of hue groups\n    if hue is None:\n        num_hue_groups = 1\n    else:\n        num_hue_groups = len(data[hue].unique())\n    # Get the number of size groups\n    if size is None:\n        num_size_groups = 1\n    else:\n        num_size_groups = len(data[size].unique())\n    # Set the font size based on the number of hue groups and the correlation value\n    if num_hue_groups == 1:\n        if r > 0.8:\n            fontsize = 20\n        elif r > 0.5:\n            fontsize = 16\n        elif r > 0.3:", "CodeBleu_score": 36.97453390929499}, {"code": "def _corrfunc(self, x, y, **kws)\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from matplotlib.font_manager import FontProperties\n    from matplotlib.text import Text\n    from matplotlib.transforms import Bbox\n    from matplotlib.patheffects import Stroke\n    from matplotlib.patches import Rectangle\n    from scipy.stats import pearsonr\n    from statsmodels.stats.multitest import multipletests\n    from statsmodels.stats.weightstats import DescrStatsW\n    from statsmodels.stats.multitest import multipletests\n    from scipy.stats import norm\n    from statsmodels.stats.multitest import multipletests\n\n    # Extract the hue group index and set the default font size\n    hue_index = kws.get(\"hue_index\", 0)\n    default_fontsize = 12\n\n    # Extract the hue group name and set the default font size\n    hue_name = kws.get(\"hue_name\", None)\n    default_fontsize = 12\n\n    # Extract the hue group name and set the default font size\n    hue_name = kws.get(\"hue_name\", None)\n    default_fontsize = 12\n\n    # Extract the hue group name and set the default font size\n    hue_name = kws.get(\"hue_name\", None)\n    default_fontsize = 12\n\n    # Extract the hue group name and set the default font size\n    hue_name = kws.get(\"hue_name\", None)\n    default_fontsize = 12\n\n    # Extract the hue group name and set the default font size\n    hue_name = kws.get(\"hue_name\", None)\n    default_fontsize = 12\n\n    # Extract the hue group name and set the default font size\n    hue_name = kws.get(\"hue_name\", None)\n    default_fontsize = 12\n\n    # Extract the hue group name and set the default font size\n    hue_name = kws.get(\"hue_name\", None)\n    default", "CodeBleu_score": 14.974419688546186}, {"code": "def _corrfunc(self, x, y, **kws)\n    import numpy as np\n    import pandas as pd\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import matplotlib.patheffects as path_effects\n\n    # Handle optional hue groups\n    if \"hue\" in kws:\n        hue = kws[\"hue\"]\n        hue_groups = np.unique(hue)\n        hue_group_index = hue_groups.tolist().index(hue)\n    else:\n        hue_groups = [None]\n        hue_group_index = 0\n\n    # Handle optional weights\n    if \"weights\" in kws:\n        weights = kws[\"weights\"]\n    else:\n        weights = None\n\n    # Filter out rows where x or y is NaN\n    x = x[pd.notnull(x)]\n    y = y[pd.notnull(y)]\n    if \"hue\" in kws:\n        hue = hue[pd.notnull(hue)]\n\n    # Calculate the correlation coefficient\n    r = np.corrcoef(x, y)[0, 1]\n\n    # Set the font size based on the number of hue groups and the correlation value\n    fontsize = 12 + 2 * hue_group_index + (r ** 2) * 10\n\n    # Retrieve the appropriate axis\n    if \"ax\" in kws:\n        ax = kws[\"ax\"]\n    else:\n        ax = plt.gca()\n\n    # Annotate the axis\n    ax.annotate(\n        f\"r = {r:.2f}\",\n        xy=(0.1, 0.9 - 0.05 * hue_group_index),\n        xycoords=ax.transAxes,\n        fontsize=fontsize,\n        path_effects=[path_effects.withStroke(linewidth=3, foreground=\"w\")],\n    )\n\n    # Return the correlation coefficient\n    return r\n\n", "CodeBleu_score": 37.4970692017851}]}
{"instance_id": "4c1ade308953", "generation_result": [{"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return src\n    elif method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        import decimal\n        return round(decimal.Decimal(src).quantize(decimal.Decimal('1E-{}'.format(rounddigit)), rounding=decimal.ROUND_HALF_UP), rounddigit)\n    elif method == 'format':\n        return float(format(src, '.{}g'.format(rounddigit)))\n    else:\n        raise ValueError('Method must be decimal, sig, or format.')", "CodeBleu_score": 56.922318247273054}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        import decimal\n        return decimal.Decimal(src).quantize(decimal.Decimal('1E-{}'.format(rounddigit)))\n    elif method == 'format':\n        return float(format(src, '.{}f'.format(rounddigit)))\n    else:\n        raise ValueError(\"Invalid method: {}\".format(method))\n\n", "CodeBleu_score": 60.72918580537592}, {"code": "def _round_digits(src: float, rounddigit: int = None, method='decimal')\n    if rounddigit is None:\n        return src\n    elif method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        import decimal\n        with decimal.localcontext() as ctx:\n            ctx.prec = rounddigit\n            return ctx.create_decimal(src)\n    elif method == 'format':\n        return float(format(src, '.%sf' % rounddigit))\n    else:\n        raise ValueError(\"The method must be one of 'decimal', 'sig', or 'format'\")\n\n", "CodeBleu_score": 73.05815590526652}]}
{"instance_id": "a9923e678320", "generation_result": [{"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n    newdict = {}\n    for key, value in srcdict.items():\n        if isinstance(value, float):\n            newdict[key] = cls._round_digits(value, rounddigit, method)\n        else:\n            newdict[key] = value\n    return newdict\n\n", "CodeBleu_score": 43.6021040010129}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n    newdict = {}\n    for key, value in srcdict.items():\n        if isinstance(value, float):\n            if rounddigit is None:\n                newdict[key] = value\n            else:\n                if method == 'decimal':\n                    newdict[key] = cls._round_digits(value, rounddigit)\n                elif method == 'sig':\n                    newdict[key] = cls._round_sig(value, rounddigit)\n                elif method == 'format':\n                    newdict[key] = cls._round_format(value, rounddigit)\n                else:\n                    raise ValueError(f'Method must be decimal, sig, or format.')\n        else:\n            newdict[key] = value\n    return newdict\n\n", "CodeBleu_score": 36.411112589360364}, {"code": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')\n    if not isinstance(srcdict, dict):\n        raise TypeError('The input must be a dictionary')\n    if rounddigit is None:\n        return srcdict\n    if not isinstance(rounddigit, int):\n        raise TypeError('The number of digits to round to must be an integer')\n    if method not in ['decimal', 'sig', 'format']:\n        raise ValueError(\"The method must be 'decimal', 'sig', or 'format'\")\n    newdict = {}\n    for key, val in srcdict.items():\n        if isinstance(val, float):\n            if method == 'decimal':\n                newdict[key] = _round_digits(val, rounddigit, method=method)\n            elif method == 'sig':\n                newdict[key] = _round_sigdigits(val, rounddigit)\n            elif method == 'format':\n                newdict[key] = _round_formatdigits(val, rounddigit)\n        else:\n            newdict[key] = val\n    return newdict\n\n", "CodeBleu_score": 37.40344402950323}]}
{"instance_id": "b6106e079dc0", "generation_result": [{"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    import pandas as pd\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    df = pd.DataFrame({x_name: x, y_name: y})\n    if hue_data is not None:\n        df[hue_name] = hue_data\n        if 'title' not in legend_kws:\n            legend_kws['title'] = hue_name\n    sns.scatterplot(x=x_name, y=y_name, hue=hue_name, data=df, ax=ax, **scatter_kws)\n    ax.legend(**legend_kws)\n    return ax\n", "CodeBleu_score": 42.9826423707932}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n    # Combine x, y, and hue data into a DataFrame\n    df = pd.DataFrame({x_name: x, y_name: y})\n    if hue_data is not None:\n        df[hue_name] = hue_data\n\n    # Plot the scatter plot\n    cls.scatterplot(data=df, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n\n    # Set the legend title\n    if ax.legend_ is not None:\n        if ax.legend_.get_title() == \"\":\n            ax.legend_.set_title(hue_name)\n\n    # Return the axis object\n    return ax\n\n", "CodeBleu_score": 31.251979017234397}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\n\n    import pandas as pd\n    import seaborn as sns\n\n    # Combine x and y into a DataFrame\n    df = pd.DataFrame({x_name: x, y_name: y})\n\n    # Add hue data to the DataFrame if provided\n    if hue_data is not None:\n        df[hue_name] = hue_data\n\n    # Plot the scatter plot using Seaborn\n    sns.scatterplot(data=df, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n\n    # Set the legend title if not specified\n    if legend_kws.get(\"title\") is None:\n        legend_kws[\"title\"] = hue_name\n\n    # Set the legend\n    ax.legend(**legend_kws)\n\n    return ax\n\n", "CodeBleu_score": 39.74721999588653}]}
{"instance_id": "257e0aa5dd4d", "generation_result": [{"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    # Retrieve the main time series data\n    ts_data = self.data.ts_data\n    # Check for any exogenous variables\n    exogenous = self.data.exogenous\n    if exogenous is not None:\n        exogenous = exogenous.to_numpy()\n    # Suppress warnings related to user actions and convergence\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        # Fit the model\n        self.model.fit(ts_data, exogenous=exogenous, **fit_args)\n    return self", "CodeBleu_score": 29.836051997759117}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    # Retrieve the main time series data\n    ts_data = self.get_ts_data()\n\n    # Check for any exogenous variables\n    exog_vars = self.get_exog_vars()\n    if exog_vars is not None:\n        ts_data = ts_data.merge(exog_vars, how=\"left\", left_index=True, right_index=True)\n\n    # Fit the model\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        model = self.model_class(**fit_args).fit(ts_data)\n\n    return model", "CodeBleu_score": 35.88062991922877}, {"code": "def fit(self, **fit_args) -> \"TimeSeriesModel\"\n    if self.exogenous_variables is not None:\n        exogenous_data = self.exogenous_variables.get_data(\n            self.data_context, self.data_context.current_period\n        )\n        return self.model.fit(\n            self.get_data(self.data_context, self.data_context.current_period),\n            exogenous_data,\n            **fit_args,\n        )\n    else:\n        return self.model.fit(\n            self.get_data(self.data_context, self.data_context.current_period),\n            **fit_args,\n        )\n\n", "CodeBleu_score": 26.341375153100245}]}
{"instance_id": "6446de2223e8", "generation_result": [{"code": "def create_model(self, **kwargs)\n    if self.model_type == 'holt_winters':\n        # Create an ExponentialSmoothing model\n        model = ExponentialSmoothing(self.transformed_data.values, **kwargs)\n    elif self.model_type == 'auto_arima':\n        # Create an AutoARIMA model\n        try:\n            from pmdarima import AutoARIMA\n            model = AutoARIMA(**kwargs)\n        except ModuleNotFoundError:\n            self.logger.error('pmdarima is not installed. Please install pmdarima to use auto_arima.')\n            raise\n    elif self.model_type == 'sarimax':\n        # Create a SARIMAX model\n        model = SARIMAX(self.transformed_data.values, **kwargs)\n    else:\n        # Raise an exception if the model type is not recognized\n        raise Exception('Model type not recognized.')\n\n    return model", "CodeBleu_score": 33.13391412972286}, {"code": "def create_model(self, **kwargs)\n    if self.model_type == 'holt_winters':\n        data = self.transform_data(**kwargs)\n        model = ExponentialSmoothing(data['transformed_data'], seasonal_periods=self.seasonal_periods, trend=self.trend, seasonal=self.seasonal)\n    elif self.model_type == 'auto_arima':\n        try:\n            from pmdarima import AutoARIMA\n            model = AutoARIMA(**kwargs)\n        except ModuleNotFoundError:\n            self.logger.error('pmdarima is not installed. Please install it and try again.')\n            sys.exit()\n    elif self.model_type == 'sarimax':\n        data = self.transform_data(**kwargs)\n        exogenous = self.get_exogenous()\n        if exogenous:\n            model = SARIMAX(data['transformed_data'], exogenous=exogenous, order=self.order, seasonal_order=self.seasonal_order)\n        else:\n            model = SARIMAX(data['transformed_data'], order=self.order, seasonal_order=self.seasonal_order)\n    else:\n        raise Exception(f'The model type {self.model_type} is not recognized.')\n    return model", "CodeBleu_score": 46.76136082732061}, {"code": "def create_model(self, **kwargs)\n    if self.model_type == 'holt_winters':\n        # Retrieve transformed data\n        data = self.transformed_data\n        # Create model\n        model = ExponentialSmoothing(data, trend=self.trend, seasonal=self.seasonal, seasonal_periods=self.seasonal_periods)\n    elif self.model_type == 'auto_arima':\n        # Try to import AutoARIMA\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:\n            # Log error and exit\n            print('Error: pmdarima is not installed. Run pip install pmdarima to install it.')\n            sys.exit()\n        # Retrieve transformed data\n        data = self.transformed_data\n        # Create model\n        model = AutoARIMA(data, start_p=self.start_p, d=self.d, start_q=self.start_q, max_p=self.max_p, max_d=self.max_d, max_q=self.max_q, start_P=self.start_P, D=self.D, start_Q=self.start_Q, max_P=self.max_P, max_D=self.max_D, max_Q=self.max_Q, max_order=self.max_order, m=self.m, seasonal=self.seasonal, stationary=self.stationary, information_criterion=self.information_criterion, alpha=self.alpha, test=self.test, seasonal_test=self.seasonal_test, stepwise=self.stepwise, n_jobs=self.n_jobs, start_params=self.start_params, trend=self.trend, method=self.method, maxiter=self.maxiter, offset_test_args=self.offset_test_args, seasonal_test_args=self.seasonal_test_args, suppress_warnings=self.suppress_warnings, error_action=self.error_action, trace=self.trace, random_state=self.random_state, n_fits=self.n_", "CodeBleu_score": 28.15085485487363}]}
{"instance_id": "b2f67820e27f", "generation_result": [{"code": "def _to_chx(array)\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    else:\n        return array\n", "CodeBleu_score": 75.22514168335545}, {"code": "def _to_chx(array)\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array\n\n", "CodeBleu_score": 75.96453859444033}, {"code": "def _to_chx(array)\n    import chainerx\n    import numpy\n    if isinstance(array, chainerx.ndarray):\n        return array\n    if isinstance(array, numpy.ndarray):\n        return chainerx.array(array)\n    try:\n        import cupy\n        if isinstance(array, cupy.ndarray):\n            return chainerx.array(array)\n    except ImportError:\n        pass\n    return array\n\n", "CodeBleu_score": 52.483246405600624}]}
{"instance_id": "b96f7d9bcc95", "generation_result": [{"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    if len(ys) == 0:\n        return ys\n    if axis is None:\n        axis = 0\n    if len(x.shape) != len(ys[0].shape):\n        x_split = np.split(np.empty(x.shape), indices_or_sections, axis)\n        x_split_sizes = [x_s.shape[axis] for x_s in x_split]\n        for i, y in enumerate(ys):\n            if y.shape[axis] != x_split_sizes[i]:\n                ys[i] = y.reshape(tuple([x_split_sizes[i] if d == axis else s for d, s in enumerate(y.shape)]))\n    return ys\n\n", "CodeBleu_score": 39.56766619780455}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    import numpy as np\n\n    if len(ys) > 0 and ys[0].ndim != x.ndim:\n        # Calculate the expected shapes for the split segments\n        tmp_split = np.split(np.empty(x.shape), indices_or_sections, axis)\n        shapes = [s.shape for s in tmp_split]\n        # Reshape each segment if the dimensions do not match\n        for i in range(len(ys)):\n            if ys[i].shape != shapes[i]:\n                ys[i] = ys[i].reshape(shapes[i])\n    return ys\n\n", "CodeBleu_score": 35.56242541355885}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\n    if axis >= 0:\n        axis = axis - len(x.shape)\n    if axis < 0:\n        axis = axis + len(x.shape)\n    if axis < 0:\n        axis = axis + len(x.shape)\n    if axis < 0:\n        raise ValueError(\"axis \" + str(axis) + \" is out of bounds for array of dimension \" + str(len(x.shape)))\n    if axis >= len(x.shape):\n        raise ValueError(\"axis \" + str(axis) + \" is out of bounds for array of dimension \" + str(len(x.shape)))\n    if len(ys) == 0:\n        return ys\n    if len(ys) == 1:\n        return ys\n    if len(ys) == 2:\n        return ys\n    if len(ys) == 3:\n        return ys\n    if len(ys) == 4:\n        return ys\n    if len(ys) == 5:\n        return ys\n    if len(ys) == 6:\n        return ys\n    if len(ys) == 7:\n        return ys\n    if len(ys) == 8:\n        return ys\n    if len(ys) == 9:\n        return ys\n    if len(ys) == 10:\n        return ys\n    if len(ys) == 11:\n        return ys\n    if len(ys) == 12:\n        return ys\n    if len(ys) == 13:\n        return ys\n    if len(ys) == 14:\n        return ys\n    if len(ys) == 15:\n        return ys\n    if len(ys) == 16:\n        return ys\n    if len(ys) == 17:\n        return ys\n    if len(ys) == 18:\n        return ys\n    if len(ys) == 19:\n        return ys\n    if len(ys) == 20:\n        return ys\n    if len(ys) == 21:\n        return ys\n    if", "CodeBleu_score": 19.279232193865578}]}
{"instance_id": "cf39fb123842", "generation_result": [{"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    elif device < 0:\n        return [to_device(device, x) for x in batch]\n    else:\n        xp = cuda.cupy.get_array_module(batch[0])\n        concat = xp.concatenate(batch, axis=0)\n        sections = np.cumsum([len(x) for x in batch[:-1]])\n        device_concat = cuda.to_gpu(concat, device)\n        return cuda.cupy.split(device_concat, sections)", "CodeBleu_score": 62.25581828933562}, {"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    if device < 0:\n        return [to_device(device, x) for x in batch]\n    xp = cuda.cupy if device >= 0 else np\n    concat = xp.concatenate(batch, axis=0)\n    sections = np.cumsum([len(x) for x in batch[:-1]], dtype=np.int32)\n    sections = xp.asarray(sections)\n    split = xp.split(concat, sections)\n    return split\n\n", "CodeBleu_score": 57.40916010556325}, {"code": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    elif device < 0:\n        return [to_device(device, x) for x in batch]\n    else:\n        xp = cuda.cupy.get_array_module(batch[0])\n        batch = xp.concatenate(batch, axis=0)\n        sections = numpy.cumsum([len(x) for x in batch])\n        sections = numpy.roll(sections, 1)\n        sections[0] = 0\n        batch = cuda.to_gpu(batch, device)\n        batch = cuda.cupy.split(batch, sections)\n        return batch", "CodeBleu_score": 63.86290463956777}]}
{"instance_id": "845c744d897c", "generation_result": [{"code": "def make_image(trainer)\n\n    # Set a random seed\n    seed = 1\n    torch.manual_seed(seed)\n\n    # Determine the number of images to generate\n    num_images = 64\n\n    # Create a latent vector z\n    z = torch.randn(num_images, trainer.model.z_dim)\n\n    # Generate images\n    images = trainer.model.generate(z, mode='test')\n\n    # Move to CPU, scale to [0, 255] range and convert to uint8\n    images = images.permute(0, 2, 3, 1).cpu()\n    images = (images * 0.5 + 0.5) * 255\n    images = images.type(torch.uint8)\n\n    # Create a grid of images\n    grid = torchvision.utils.make_grid(images, nrow=8)\n\n    # Create a directory for saving the preview image if it doesn't exist\n    os.makedirs('./previews', exist_ok=True)\n\n    # Save the generated image as a PNG file\n    file_name = f'./previews/iteration-{trainer.iteration}.png'\n    torchvision.utils.save_image(grid, file_name)\n\n    print(f'Saved image to {file_name}')", "CodeBleu_score": 26.989780875338116}, {"code": "def make_image(trainer)\n\n    # Set a random seed\n    seed = 1\n    np.random.seed(seed)\n\n    # Determine the number of images to generate\n    num_images = 10\n\n    # Create a latent vector z\n    z = np.random.randn(num_images, trainer.z_dim)\n\n    # Generate images\n    gen_images = trainer.generate(z, mode='test')\n\n    # Move to CPU, scale to [0, 255], and convert to uint8\n    gen_images = gen_images.permute(0, 2, 3, 1).cpu()\n    gen_images = (0.5 + 0.5 * gen_images) * 255.0\n    gen_images = gen_images.numpy().astype(np.uint8)\n\n    # Reshape and transpose to form a grid of images\n    gen_images = gen_images.reshape((10, 10, 64, 64, 3))\n    gen_images = gen_images.transpose(0, 2, 1, 3, 4)\n    gen_images = gen_images.reshape((10 * 64, 10 * 64, 3))\n\n    # Create a directory for saving the preview image if it doesn't exist\n    os.makedirs('preview', exist_ok=True)\n\n    # Save the generated image as a PNG file named according to the current training iteration\n    filename = f'preview/gen_images_iter_{trainer.iter}.png'\n    imageio.imwrite(filename, gen_images)\n\n", "CodeBleu_score": 31.217062350664072}, {"code": "def make_image(trainer)\n\n    import os\n    import numpy as np\n    import torch\n    import torchvision.utils as vutils\n    from torchvision.utils import save_image\n    import torchvision.transforms as transforms\n    import torchvision.transforms.functional as TF\n\n    # set a random seed\n    seed = 42\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    # determine the number of images to generate\n    num_images = 10\n\n    # create a latent variable z\n    z = torch.randn(num_images, 128, 1, 1)\n\n    # generate images\n    with torch.no_grad():\n        generated_images = trainer.generator(z, False)\n\n    # move the generated images to the CPU\n    generated_images = generated_images.cpu()\n\n    # scale the images to a [0, 255] range\n    generated_images = (generated_images + 1) / 2\n\n    # convert the images to uint8 type\n    generated_images = generated_images.type(torch.uint8)\n\n    # reshape the images\n    generated_images = generated_images.reshape(generated_images.shape[0], 3, 64, 64)\n\n    # transpose the images\n    generated_images = generated_images.permute(0, 2, 3, 1)\n\n    # create a directory for saving the preview image if it doesn't exist\n    if not os.path.exists('preview'):\n        os.makedirs('preview')\n\n    # save the generated image as a PNG file\n    save_image(generated_images, f'preview/generated_image_{trainer.iteration}.png')\n\n    # create a directory for saving the preview image if it doesn't exist\n    if not os.path.exists('preview'):\n        os.makedirs('preview')\n\n    # save the generated image as a PNG file\n    save_image(generated_images, f'preview/generated_image_{trainer.iteration}.png')\n\n    # create a directory", "CodeBleu_score": 29.555938453083197}]}
{"instance_id": "fc2162d43aee", "generation_result": [{"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    from_dtype = np.dtype(from_dtype)\n    to_dtype = np.dtype(to_dtype)\n    _typechars = 'fFbBhHiIlLqQpP'\n    from_type = _typechars.find(from_dtype.char)\n    to_type = _typechars.find(to_dtype.char)\n    if from_type < to_type:\n        return True\n    else:\n        return False\n\n", "CodeBleu_score": 43.32181492038873}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    from_dtype = np.dtype(from_dtype).type\n    to_dtype = np.dtype(to_dtype).type\n\n    _typechars = '?bhilqBHILQefdFD'\n    return _typechars.index(from_dtype.char) <= _typechars.index(to_dtype.char)\n\n", "CodeBleu_score": 47.22013182128796}, {"code": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool\n    _typechars = '?bhilqBHILQefdFD'\n    _from_type = np.dtype(from_dtype).char\n    _to_type = np.dtype(to_dtype).char\n    return _typechars.find(_from_type) <= _typechars.find(_to_type)\n\n", "CodeBleu_score": 42.05947630008362}]}
{"instance_id": "5b8e2d5e6e12", "generation_result": [{"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Patch\n\n    # Obtain the matplotlib Axes object from the input\n    if isinstance(g, Visualizer):\n        ax = g.ax\n    elif g is None:\n        ax = plt.gca()\n    elif isinstance(g, plt.Axes):\n        ax = g\n    else:\n        raise ValueError(\n            \"The input g must be a Visualizer, None, or a matplotlib Axes object.\"\n        )\n\n    # Check that the lengths of the labels and colors lists match\n    if len(labels) != len(colors):\n        raise ValueError(\n            \"The lengths of the labels and colors lists must match.\"\n        )\n\n    # Create legend handles by pairing each label with its corresponding color using patches.Patch\n    handles = [Patch(color=c, label=l) for c, l in zip(colors, labels)]\n\n    # Return the legend object created with the specified legend handles and additional keyword arguments\n    return ax.legend(handles=handles, **legend_kwargs)", "CodeBleu_score": 59.4626643099756}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n    if g is None:\n        ax = plt.gca()\n    elif isinstance(g, plt.Axes):\n        ax = g\n    else:\n        ax = g.ax\n    if len(labels) != len(colors):\n        raise ValueError(\"The number of labels must equal the number of colors.\")\n    legend_handles = [patches.Patch(color=color, label=label) for color, label in zip(colors, labels)]\n    return ax.legend(handles=legend_handles, **legend_kwargs)", "CodeBleu_score": 62.79161678791188}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\n    from matplotlib.patches import Patch\n    from matplotlib.pyplot import gca\n\n    if g is None:\n        g = gca()\n    elif isinstance(g, Visualizer):\n        g = g.ax\n\n    if len(labels) != len(colors):\n        raise ValueError('Number of labels and colors must be equal.')\n\n    handles = [Patch(color=c, label=l) for c, l in zip(colors, labels)]\n    legend = g.legend(handles=handles, **legend_kwargs)\n\n    return legend\n\n", "CodeBleu_score": 58.22311340152033}]}
{"instance_id": "494e7f719eb6", "generation_result": [{"code": "def pcoords_time(X, y, fast=True)\n\n    import matplotlib.pyplot as plt\n    import time\n    from yellowbrick.features import ParallelCoordinates\n\n    # Create a figure and axis object\n    fig, ax = plt.subplots()\n\n    # Initialize the visualizer\n    if fast:\n        visualizer = ParallelCoordinates(ax=ax, fast=True)\n    else:\n        visualizer = ParallelCoordinates(ax=ax)\n\n    # Fit and transform the data\n    start = time.time()\n    visualizer.fit_transform(X, y)\n    end = time.time()\n\n    # Compute the elapsed time\n    elapsed = end - start\n\n    # Clear and close the figure to free up resources\n    plt.clf()\n    plt.close(fig)\n\n    return elapsed\n\n", "CodeBleu_score": 50.535000931132416}, {"code": "def pcoords_time(X, y, fast=True)\n    import time\n    from yellowbrick.features.pcoords import ParallelCoordinates\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    start = time.time()\n    visualizer = ParallelCoordinates(ax, fast=fast)\n    visualizer.fit_transform_poof(X, y)\n    end = time.time()\n    plt.close()\n    return end - start\n\n", "CodeBleu_score": 43.94334548454799}, {"code": "def pcoords_time(X, y, fast=True)\n    import matplotlib.pyplot as plt\n    import time\n    from yellowbrick.features.pcoords import ParallelCoordinates\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    if fast:\n        oz = ParallelCoordinates(fast=True, ax=ax)\n    else:\n        oz = ParallelCoordinates(fast=False, ax=ax)\n\n    start = time.time()\n    oz.fit_transform(X, y)\n    end = time.time()\n\n    plt.close()\n    return (end - start)\n\n", "CodeBleu_score": 49.67852122571135}]}
{"instance_id": "db4143bc029f", "generation_result": [{"code": "def draw_instances(self, X, y, **kwargs):\n\n    # Retrieve the alpha transparency value from the object's attributes or defaults to 0.25\n    alpha = getattr(self, 'alpha', 0.25)\n\n    # Extract the feature vector and corresponding label for each instance\n    for x, label in zip(X, y):\n        # Determine the color based on the label\n        color = 'red' if label == 0 else 'blue'\n        # Plot the feature vector against a predefined increment sequence on the axis\n        self.ax.plot(self.x, x, color=color, alpha=alpha, **kwargs)\n\n    # Return the axis object with the plotted instances\n    return self.ax", "CodeBleu_score": 37.64536841825857}, {"code": "def draw_instances(self, X, y, **kwargs):\n    alpha = kwargs.pop(\"alpha\", self.alpha)\n    ax = kwargs.pop(\"ax\", plt.gca())\n    colors = kwargs.pop(\"colors\", self.colors)\n    x_min, x_max = kwargs.pop(\"x_range\", (0, 1))\n    y_min, y_max = kwargs.pop(\"y_range\", (0, 1))\n    x_inc = kwargs.pop(\"x_inc\", 0.1)\n    y_inc = kwargs.pop(\"y_inc\", 0.1)\n    x_seq = np.arange(x_min, x_max, x_inc)\n    y_seq = np.arange(y_min, y_max, y_inc)\n    for i, (x, y) in enumerate(zip(X, y)):\n        color = colors[y]\n        ax.scatter(x[0], x[1], alpha=alpha, color=color)\n    return ax\n", "CodeBleu_score": 28.955638351379765}, {"code": "def draw_instances(self, X, y, **kwargs):\n    alpha = kwargs.get(\"alpha\", self.alpha)\n    # Create the plot\n    fig, ax = plt.subplots()\n    # Plot each instance\n    for i in range(len(X)):\n        # Get the feature vector and label\n        x = X[i]\n        y_i = y[i]\n        # Determine the color\n        color = self.colors[y_i]\n        # Plot the feature vector\n        ax.plot(x, self.incr, color=color, alpha=alpha)\n    return ax", "CodeBleu_score": 41.907400479372015}]}
{"instance_id": "676433312ffe", "generation_result": [{"code": "def draw_classes(self, X, y, **kwargs):\n    # Set alpha transparency value\n    alpha = kwargs.get('alpha', self.alpha)\n    # Add column of ones to X to separate instances visually\n    X = np.insert(X, 0, 1, axis=1)\n    # Append None to increments list to create breaks between instances\n    increments = kwargs.get('increments', self.increments)\n    increments.append(None)\n    # Identify unique class labels\n    unique_labels = np.unique(y)\n    # Iterate over unique class labels\n    for label in unique_labels:\n        # Determine color\n        color = kwargs.get('color', self.color_dict[label])\n        # Select instances of X belonging to class label\n        X_class = X[y == label]\n        # Plot instances as a single line plot with the specified color and alpha transparency\n        self.ax.plot(X_class, color=color, alpha=alpha)\n    return self.ax", "CodeBleu_score": 31.299319691684985}, {"code": "def draw_classes(self, X, y, **kwargs):\n\n    # Set alpha transparency from object's attributes or default to 0.5\n    if not hasattr(self, 'alpha'):\n        alpha = 0.5\n    else:\n        alpha = self.alpha\n\n    # Add column of ones to X to separate instances visually\n    X = np.append(X, np.ones((X.shape[0], 1)), axis=1)\n\n    # Append None to increments list to create breaks between instances\n    self.increments.append(None)\n\n    # Identify unique class labels in y\n    unique_labels = np.unique(y)\n\n    # Iterate over unique class labels\n    for label in unique_labels:\n\n        # Determine color for each class\n        color = self.colors[label]\n\n        # Select instances of X belonging to each class\n        X_class = X[y == label]\n\n        # Plot instances as a single line plot with specified color and alpha transparency\n        self.ax.plot(X_class[:, 0], X_class[:, 1], color=color, alpha=alpha)\n\n    # Return axis object with plotted classes\n    return self.ax", "CodeBleu_score": 33.26523565242955}, {"code": "def draw_classes(self, X, y, **kwargs):\n\n    # Set alpha transparency value\n    alpha = self.alpha if hasattr(self, 'alpha') else 0.5\n\n    # Add a column of ones to X to separate the instances visually\n    X = np.c_[X, np.ones(X.shape[0])]\n\n    # Append None to the increments list to create breaks between instances\n    self.increments.append(None)\n\n    # Identify unique class labels in y\n    unique_labels = np.unique(y)\n\n    # Iterate over unique class labels\n    for label in unique_labels:\n        # Determine the color for the current class\n        color = self.colors[label]\n\n        # Select the instances of X belonging to the current class\n        instances = X[y == label]\n\n        # Plot the instances as a single line plot with the specified color and alpha transparency\n        self.ax.plot(instances[:, :-1], instances[:, -1], color=color, alpha=alpha, **kwargs)\n\n    # Return the axis object with the plotted classes\n    return self.ax\n\n", "CodeBleu_score": 29.538714587272676}]}
{"instance_id": "9d297b12a673", "generation_result": [{"code": "def feature_analysis(fname=\"feature_analysis.png\")\n\n    # Import libraries\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import seaborn as sns\n    from sklearn.cluster import KMeans\n    from sklearn.decomposition import PCA\n    from sklearn.preprocessing import StandardScaler\n\n    # Load data\n    data = pd.read_csv(\"../datasets/occupancy_data.csv\")\n    data = data.drop(\"Date\", axis=1)\n\n    # Create a side-by-side axes grid\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Perform RadViz analysis on the left subplot\n    # Set up the visualization\n    viz = sns.radviz(\n        data=data,\n        x=\"Temperature\",\n        y=\"Humidity\",\n        z=\"Light\",\n        hue=\"Occupancy\",\n        ax=ax1,\n    )\n\n    # Fit and finalize the visualization\n    viz.set_xticklabels(viz.get_xticklabels(), rotation=45)\n    viz.set_xlabel(\"Temperature\")\n    viz.set_ylabel(\"Humidity\")\n    viz.set_zlabel(\"Light\")\n    viz.set_title(\"RadViz Visualization\")\n\n    # Perform Rank2D analysis on the right subplot\n    # Set up the visualization\n    viz = sns.rankdata(data=data, method=\"dense\", axis=1)\n    viz = pd.DataFrame(viz, columns=data.columns)\n\n    # Fit the visualization\n    scaler = StandardScaler()\n    pca = PCA(n_components=2)\n    kmeans = KMeans(n_clusters=2)\n\n    # Transform the visualization\n    viz = scaler.fit_transform(viz)\n    viz = pca.fit_transform(viz)\n    viz = kmeans.fit_predict(viz)\n    viz = pd.DataFrame(viz, columns=[\"Cluster\"])\n\n    # Finalize the visualization\n    viz", "CodeBleu_score": 31.0910439987378}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.decomposition import PCA\n    from sklearn.cluster import KMeans\n    from sklearn.metrics import silhouette_score\n    from sklearn.metrics import davies_bouldin_score\n    from sklearn.metrics import calinski_harabasz_score\n    from sklearn.metrics import adjusted_rand_score\n    from sklearn.metrics import adjusted_mutual_info_score\n    from sklearn.metrics import normalized_mutual_info_score\n    from sklearn.metrics import homogeneity_score\n    from sklearn.metrics import completeness_score\n    from sklearn.metrics import v_measure_score\n    from sklearn.metrics import fowlkes_mallows_score\n    from sklearn.metrics import mutual_info_score\n    from sklearn.metrics import adjusted_rand_score\n    from sklearn.metrics import homogeneity_completeness_v_measure\n    from sklearn.metrics import homogeneity_score\n    from sklearn.metrics import completeness_score\n    from sklearn.metrics import v_measure_score\n    from sklearn.metrics import fowlkes_mallows_score\n    from sklearn.metrics import mutual_info_score\n    from sklearn.metrics import adjusted_rand_score\n    from sklearn.metrics import silhouette_score\n    from sklearn.metrics import calinski_harabasz_score\n    from sklearn.metrics import davies_bouldin_score\n    from sklearn.metrics import adjusted_mutual_info_score\n    from sklearn.metrics import normalized_mutual_info_score\n    from sklearn.metrics import adjusted_rand_score\n    from sklearn.metrics import homogeneity_completeness_v_measure\n    from sklearn.metrics import homogeneity_score\n    from sklearn.metrics import completeness_score\n    from sklearn.metrics import v_measure_score\n    from sklearn.", "CodeBleu_score": 7.715370545360009}, {"code": "def feature_analysis(fname=\"feature_analysis.png\")\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler\n    from yellowbrick.features import RadViz, Rank2D\n\n    # Load the data\n    data = pd.read_csv(\"../datasets/occupancy_data.csv\")\n\n    # Extract the features of interest and target\n    X = data.loc[:, [\"temperature\", \"relative_humidity\", \"light\", \"C02\", \"humidity\"]]\n    y = data.loc[:, \"occupancy\"]\n\n    # Create the figure\n    fig = plt.figure(figsize=(12, 6))\n\n    # Create the axes\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n\n    # Perform RadViz analysis\n    visualizer = RadViz(ax=ax1)\n    visualizer.fit(X, y)\n    visualizer.transform(X)\n    visualizer.poof(outpath=fname)\n\n    # Perform Rank2D analysis\n    visualizer = Rank2D(ax=ax2)\n    visualizer.fit(X, y)\n    visualizer.transform(X)\n    visualizer.poof(outpath=fname)\n\n    # Save the figure\n    fig.savefig(fname)", "CodeBleu_score": 32.60022433424445}]}
{"instance_id": "fd1b4b9cd7ef", "generation_result": [{"code": "def regression(fname=\"regression.png\")\n\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.linear_model import RidgeCV, LassoCV\n    from sklearn.model_selection import train_test_split\n\n    # Load the data\n    df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/concrete.csv')\n    df = df.drop('f.c.', axis=1)\n    X = df.iloc[:, :-1].values\n    y = df.iloc[:, -1].values\n\n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n    # Define the alphas\n    alphas = np.logspace(-5, 0, 10)\n\n    # Create the figure\n    fig = plt.figure(figsize=(18, 6))\n\n    # Create the subplots\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n\n    # Create the LassoCV model\n    lasso = LassoCV(alphas=alphas, random_state=0).fit(X_train, y_train)\n\n    # Create the RidgeCV model\n    ridge = RidgeCV(alphas=alphas, random_state=0).fit(X_train, y_train)\n\n    # Plot the prediction errors\n    ax1.plot(alphas, lasso.cv_errors_, label='LassoCV')\n    ax1.plot(alphas, ridge.cv_errors_, label='RidgeCV')\n\n    # Plot the residuals\n    ax2.plot(alphas, lasso.cv_errors_, label='LassoCV')\n    ax2.plot(alphas, ridge.cv_errors_, label='RidgeCV')\n\n    # Adjust the layout\n    ax1.set_xscale('log')\n    ax1.set_xlabel('alpha')\n    ax1.set_ylabel", "CodeBleu_score": 29.98235159026521}, {"code": "def regression(fname=\"regression.png\")\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    import seaborn as sns\n    from sklearn.linear_model import LassoCV, RidgeCV\n\n    # Set up the figure\n    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\n    # Load the data\n    X = pd.read_csv(\"concrete.csv\", index_col=0)\n    X = X.drop(columns=[\"cement\", \"water\", \"superplasticizer\"])\n    y = pd.read_csv(\"concrete.csv\", index_col=0)[\"cement\"]\n\n    # Split the data into training and testing sets\n    from sklearn.model_selection import train_test_split\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    # Define the alphas\n    alphas = np.logspace(-5, 2, 100)\n\n    # Fit the model and score it on the testing set\n    model = LassoCV(alphas=alphas, cv=5, n_jobs=-1)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    score = model.score(X_test, y_test)\n\n    # Plot the prediction error\n    axes[0].plot(alphas, model.mse_path_, label=\"MSE\")\n    axes[0].plot(alphas, model.alphas_, label=\"Alpha\")\n    axes[0].set_xscale(\"log\")\n    axes[0].set_xlabel(\"Alpha\")\n    axes[0].set_ylabel(\"Error\")\n    axes[0].legend()\n    axes[0].set_title(f\"MSE vs. Alpha (R2 = {score:.3f})\")\n\n    # Fit the model and score it on the testing set\n    model = RidgeCV(alphas=alphas, cv=5, n_jobs=-1)\n    model.fit(X_train, y_train)", "CodeBleu_score": 32.86801977972139}, {"code": "def regression(fname=\"regression.png\")\n\n    # Import libraries\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    from sklearn.linear_model import LassoCV, RidgeCV\n    from sklearn.model_selection import train_test_split\n\n    # Load the data\n    data = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/concrete.csv\")\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(data.drop(\"CompressiveStrength\", axis=1), data[\"CompressiveStrength\"], test_size=0.2, random_state=0)\n\n    # Define the alphas for regularization\n    alphas = np.logspace(-10, -2, 50)\n\n    # Create a figure with 2 subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n\n    # Plot the prediction error for LassoCV\n    for alpha in alphas:\n        model = LassoCV(alphas=[alpha], random_state=0)\n        model.fit(X_train, y_train)\n        ax1.plot(X_test, model.predict(X_test), label=r\"$\\alpha = {}$\".format(alpha))\n    ax1.legend(title=r\"$\\alpha$\", loc=\"best\")\n    ax1.set_title(\"Prediction Error\")\n    ax1.set_xlabel(\"Actual strength\")\n    ax1.set_ylabel(\"Predicted strength\")\n\n    # Plot the residuals for RidgeCV\n    for alpha in alphas:\n        model = RidgeCV(alphas=[alpha], random_state=0)\n        model.fit(X_train, y_train)\n        ax2.plot(model.predict(X_test) - y_test, label=r\"$\\alpha = {}$\".format(alpha))\n    ax2.legend(title=r\"$\\alpha$\", loc=\"best\")\n    ax2.set_title(\"Residuals\")", "CodeBleu_score": 27.624831994645348}]}
{"instance_id": "627126918f88", "generation_result": [{"code": "def classification(fname=\"classification.png\")\n\n    # Import libraries\n    import matplotlib.pyplot as plt\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report, plot_confusion_matrix, plot_roc_curve\n    from sklearn.datasets import fetch_20newsgroups\n    from sklearn.feature_extraction.text import CountVectorizer\n\n    # Load spam dataset\n    dataset = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med'], shuffle=True, random_state=1)\n\n    # Split into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.25, random_state=1)\n\n    # Create CountVectorizer\n    vectorizer = CountVectorizer()\n\n    # Fit and transform training data\n    X_train_feats = vectorizer.fit_transform(X_train)\n\n    # Fit and transform test data\n    X_test_feats = vectorizer.transform(X_test)\n\n    # Create MultinomialNB classifier\n    model = MultinomialNB()\n\n    # Fit model to training data\n    model.fit(X_train_feats, y_train)\n\n    # Predict test data\n    y_pred = model.predict(X_test_feats)\n\n    # Create ClassificationReport\n    report = classification_report(y_test, y_pred, output_dict=True)\n\n    # Create grid of two axes\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Create ClassificationReport plot on left axis\n    axs[0] = plot_confusion_matrix(model, X_test_feats, y_test, cmap='Blues', ax=axs[0])\n\n    # Create DiscriminationThreshold", "CodeBleu_score": 32.82494616140417}, {"code": "def classification(fname=\"classification.png\")\n\n    # Import libraries\n    import matplotlib.pyplot as plt\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import plot_confusion_matrix\n    from sklearn.metrics import plot_roc_curve\n    from sklearn.metrics import plot_precision_recall_curve\n    from sklearn.metrics import ConfusionMatrixDisplay\n    from sklearn.metrics import RocCurveDisplay\n    from sklearn.metrics import PrecisionRecallDisplay\n    from sklearn.datasets import fetch_20newsgroups\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.pipeline import make_pipeline\n    from sklearn.compose import ColumnTransformer\n    from sklearn.preprocessing import LabelEncoder\n    import pandas as pd\n\n    # Load the spam dataset\n    data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n\n    # Split the dataset into a training and test set\n    X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=42)\n\n    # Create a pipeline\n    pipe = make_pipeline(\n        ColumnTransformer([\n            ('tfidf', TfidfVectorizer(), 'message')\n        ]),\n        MultinomialNB()\n    )\n\n    # Fit the pipeline to the training set\n    pipe.fit(X_train, y_train)\n\n    # Create a classification report\n    report = classification_report(y_test, pipe.predict(X_test), output_dict=True)\n    report = pd.DataFrame(report).transpose()\n\n    # Create a confusion matrix\n    cm = plot_confusion_matrix(pipe, X_test, y_test)\n\n    # Create a ROC curve\n    roc = plot_roc_curve(", "CodeBleu_score": 30.144031880868248}, {"code": "def classification(fname=\"classification.png\")\n\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report, plot_confusion_matrix, plot_roc_curve, ConfusionMatrixDisplay\n    from sklearn.datasets import fetch_20newsgroups\n\n    # Load dataset\n    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n    newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n\n    # Split dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(newsgroups_train.data, newsgroups_train.target, test_size=0.2, random_state=1)\n\n    # Create a grid of two axes\n    fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n\n    # Create a Multinomial Naive Bayes classifier and fit it to the training data\n    clf = MultinomialNB().fit(X_train, y_train)\n\n    # Create a ClassificationReport and display it\n    report = classification_report(y_test, clf.predict(X_test), output_dict=True)\n    ax[0].text(0.01, 0.99, str(report), transform=ax[0].transAxes, fontsize=14, verticalalignment='top')\n\n    # Create a ConfusionMatrix and display it\n    disp = ConfusionMatrixDisplay(confusion_matrix=clf.predict(X_test), display_labels=newsgroups_train.target_names).plot(ax=ax[0])\n\n    # Create a Logistic Regression classifier and fit it to the entire dataset\n    clf = LogisticRegression().fit(newsgroups_train.data, newsgroups_train.target)\n\n    # Create a DiscriminationThreshold and display", "CodeBleu_score": 33.00661357911763}]}
{"instance_id": "318d3a674694", "generation_result": [{"code": "def get_color_cycle()\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import sys\n    try:\n        if int(mpl.__version__[0]) < 1:\n            raise ValueError\n        if int(mpl.__version__[2]) < 5:\n            raise ValueError\n        color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n    except (ValueError, KeyError):\n        color_cycle = plt.rcParams['axes.color_cycle']\n    return color_cycle\n\n", "CodeBleu_score": 39.27691372508016}, {"code": "def get_color_cycle()\n    try:\n        import matplotlib as mpl\n        import matplotlib.pyplot as plt\n        import cycler\n\n        if mpl.__version__ >= \"1.5.0\":\n            try:\n                colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n                return colors\n            except KeyError:\n                pass\n        colors = plt.rcParams['axes.color_cycle']\n        return colors\n    except:\n        return ['b', 'g', 'r', 'c', 'm', 'y', 'k']", "CodeBleu_score": 44.94362862206431}, {"code": "def get_color_cycle()\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\n    if mpl.__version__ >= \"1.5.0\":\n        try:\n            return plt.rcParams['axes.prop_cycle'].by_key()['color']\n        except KeyError:\n            pass\n    return plt.rcParams['axes.color_cycle']\n\n", "CodeBleu_score": 39.35183276685164}]}
{"instance_id": "f3fac557334c", "generation_result": [{"code": "def plot_knee_normalized(\n        self,\n    )\n\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(self.normalized_x, self.normalized_y)\n    ax.plot(self.distance_x, self.distance_y, color=\"red\")\n    ax.set_xlabel(\"Normalized x\")\n    ax.set_ylabel(\"Normalized y\")\n    ax.set_xlim(min(self.normalized_x), max(self.normalized_x))\n    ax.set_ylim(min(self.normalized_y), max(self.normalized_y))\n    ax.set_xticks(np.arange(min(self.normalized_x), max(self.normalized_x), 0.1))\n    ax.set_yticks(np.arange(min(self.normalized_y), max(self.normalized_y), 0.1))\n    if self.knee_exists:\n        ax.axvline(x=self.knee_x, color=\"blue\")\n    plt.show()\n\n", "CodeBleu_score": 34.768284227304875}, {"code": "def plot_knee_normalized(\n        self,\n    )\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(8,8))\n    plt.plot(self.normalized_x,self.normalized_y,'b')\n    plt.plot(self.normalized_x,self.distance_curve,'r')\n    plt.xlabel('Normalized x')\n    plt.ylabel('Normalized y')\n    plt.xticks(np.arange(min(self.normalized_x),max(self.normalized_x)+0.1,0.1))\n    plt.yticks(np.arange(min(self.normalized_y),max(self.normalized_y)+0.1,0.1))\n    if self.knee_point:\n        plt.axvline(self.knee_point,color='k',linestyle='--')\n    plt.show()\n\n", "CodeBleu_score": 43.766464993038255}, {"code": "def plot_knee_normalized(\n        self,\n    )\n        import matplotlib.pyplot as plt\n        fig = plt.figure(figsize=(8,8))\n        ax = fig.add_subplot(111)\n        ax.plot(self.x_norm,self.y_norm,'b',label='Normalized Curve')\n        ax.plot(self.x_norm,self.distance,'r',label='Distance Curve')\n        ax.set_xlabel('Normalized X-axis')\n        ax.set_ylabel('Normalized Y-axis')\n        ax.set_xlim(min(self.x_norm),max(self.x_norm))\n        ax.set_ylim(min(self.y_norm),max(self.y_norm))\n        ax.set_xticks(np.arange(min(self.x_norm),max(self.x_norm),0.1))\n        ax.set_yticks(np.arange(min(self.y_norm),max(self.y_norm),0.1))\n        ax.legend(loc='best')\n        if self.knee_x_norm:\n            ax.axvline(x=self.knee_x_norm,color='k',linestyle='--')\n        plt.show()", "CodeBleu_score": 33.4227155822838}]}
{"instance_id": "b2d48aae4fce", "generation_result": [{"code": "def plot_knee(\n        self,\n    )\n\n        # Import libraries\n        import matplotlib.pyplot as plt\n\n        # Create plot\n        plt.figure(figsize=(8, 8))\n        plt.plot(self.x, self.y)\n        plt.vlines(self.knee, plt.ylim()[0], plt.ylim()[1], color=\"red\")\n        plt.show()\n\n        # Print knee point\n        print(f\"Knee point: {self.knee}\")\n\n        # Print knee value\n        print(f\"Knee value: {self.y[self.knee]}\")", "CodeBleu_score": 61.18740010587532}, {"code": "def plot_knee(\n        self,\n    )\n        import matplotlib.pyplot as plt\n        fig = plt.figure(figsize=(8, 8))\n        ax = fig.add_subplot(1, 1, 1)\n        ax.plot(self.x, self.y, 'b-')\n        ax.set_xlabel('x')\n        ax.set_ylabel('y')\n        ax.set_title('Curve and knee point')\n        ax.set_xlim([0, 1])\n        ax.set_ylim([0, 1])\n        if self.knee_x is not None:\n            ax.axvline(self.knee_x, color='k')\n        plt.show()", "CodeBleu_score": 36.44271545213951}, {"code": "def plot_knee(\n        self,\n    )\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.plot(self.x, self.y, 'b')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    if self.knee_x is not None:\n        ax.axvline(self.knee_x, color='r')\n    plt.show()", "CodeBleu_score": 41.471831071717006}]}
{"instance_id": "4450ba2384c9", "generation_result": [{"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    from IPython.display import display\n    import ipywidgets as widgets\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    transformed_data = pca.transform(data)\n\n    def show_decomp(idx):\n        \"\"\"# This function plots the original data and its PCA components for a selected index.\"\"\"\n        import matplotlib.pyplot as plt\n        fig, axs = plt.subplots(n_components + 1, 1, figsize=(10, 10))\n        axs[0].scatter(data[idx, 0], data[idx, 1])\n        axs[0].set_title(f\"Original data (instance {idx})\")\n        for i in range(n_components):\n            axs[i + 1].scatter(transformed_data[idx, i], transformed_data[idx, i + 1])\n            axs[i + 1].set_title(f\"PCA component {i + 1}\")\n        plt.show()\n\n    w = widgets.interact(show_decomp, idx=widgets.IntSlider(min=0, max=data.shape[0] - 1, step=1, value=0))\n    display(w)", "CodeBleu_score": 49.37909307220798}, {"code": "def plot_pca_interactive(data, n_components=6)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from sklearn.decomposition import PCA\n    from IPython.html.widgets import interact\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    transformed_data = pca.transform(data)\n    components = pca.components_\n\n    def show_decomp(index):\n        plt.figure(figsize=(12, 10))\n        plt.subplot(2, 2, 1)\n        plt.scatter(data[:, 0], data[:, 1])\n        plt.scatter(transformed_data[index, 0], transformed_data[index, 1], c='r', s=50)\n        plt.title('Original data')\n        plt.xlabel('First principal component')\n        plt.ylabel('Second principal component')\n\n        plt.subplot(2, 2, 2)\n        plt.scatter(transformed_data[:, 0], transformed_data[:, 1])\n        plt.scatter(transformed_data[index, 0], transformed_data[index, 1], c='r', s=50)\n        plt.title('Transformed data')\n        plt.xlabel('First principal component')\n        plt.ylabel('Second principal component')\n\n        plt.subplot(2, 2, 3)\n        plt.scatter(data[:, 0], data[:, 1])\n        plt.scatter(transformed_data[index, 0] * components[0, 0] + transformed_data[index, 1] * components[1, 0],\n                    transformed_data[index, 0] * components[0, 1] + transformed_data[index, 1] * components[1, 1],\n                    c='r', s=50)\n        plt.title('Original data')\n        plt.xlabel('First principal component')\n        plt.ylabel('Second principal component')\n\n        plt.subplot(2, 2, 4)\n        plt.scatter(transformed_data[:, 0],", "CodeBleu_score": 50.11852796369388}, {"code": "def plot_pca_interactive(data, n_components=6)\n    from sklearn.decomposition import PCA\n    import numpy as np\n    from IPython.html.widgets import interact, IntSliderWidget\n    from IPython.display import display\n\n    pca = PCA(n_components=n_components)\n    pca.fit(data)\n    transformed = pca.transform(data)\n    data = np.array(data)\n\n    def show_decomp(i):\n        \"\"\"# This function plots the original data and its PCA components for a selected index.\\nThe function takes the selected index and plots the original data instance and its PCA components.\"\"\"\n        import matplotlib.pyplot as plt\n        plt.figure(figsize=(10, 6))\n        plt.subplot(1, 2, 1)\n        plt.title(\"Original data\")\n        plt.plot(data[i, :])\n        plt.subplot(1, 2, 2)\n        plt.title(\"PCA components\")\n        plt.plot(transformed[i, :])\n        plt.show()\n\n    # Create an interactive widget to select the index and update the plot\n    interact(show_decomp, i=IntSliderWidget(min=0, max=len(data)-1, step=1, value=0))\n\n", "CodeBleu_score": 45.774869351787814}]}
{"instance_id": "b09a163f3ff1", "generation_result": [{"code": "def plot_iris_knn()\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.datasets import load_iris\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n\n    # Load the Iris dataset\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    # Extract the first two features and the target labels\n    X = X[:, :2]\n\n    # Split the data into a training set and a test set\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    # Create a kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n\n    # Train the classifier on the training data\n    knn.fit(X_train, y_train)\n\n    # Predict the class labels of the test set\n    y_pred = knn.predict(X_test)\n\n    # Compute the accuracy of the classifier\n    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\n    # Define a mesh grid over the feature space\n    h = .02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Predict the class labels for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the predicted class labels to the original mesh grid shape\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision", "CodeBleu_score": 48.81787503538453}, {"code": "def plot_iris_knn()\n\n    # Load the dataset\n    from sklearn import datasets\n    iris = datasets.load_iris()\n\n    # Extract the first two features and the target labels\n    X = iris.data[:, :2]\n    y = iris.target\n\n    # Train a kNN classifier with 5 neighbors\n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X, y)\n\n    # Define a mesh grid over the feature space\n    from matplotlib.colors import ListedColormap\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                         np.arange(y_min, y_max, 0.01))\n\n    # Predict the class for each point in the grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Reshape the predictions to a 2D grid\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundary and the training points\n    from matplotlib.colors import ListedColormap\n    cmap = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, edgecolor='k', s=20)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.", "CodeBleu_score": 62.6724742653305}, {"code": "def plot_iris_knn()\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn import datasets\n    from sklearn.neighbors import KNeighborsClassifier\n\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n\n    # Extract the first two features and the target labels\n    features = iris.data[:, :2]\n    labels = iris.target\n\n    # Create a kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n\n    # Train the classifier on the features and labels\n    knn.fit(features, labels)\n\n    # Create a mesh grid over the feature space\n    x_min, x_max = features[:, 0].min() - 1, features[:, 0].max() + 1\n    y_min, y_max = features[:, 1].min() - 1, features[:, 1].max() + 1\n    x_grid, y_grid = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                                 np.arange(y_min, y_max, 0.01))\n\n    # Predict the class for each point in the grid\n    grid_predictions = knn.predict(np.c_[x_grid.ravel(), y_grid.ravel()])\n\n    # Reshape the predictions back to a grid\n    grid_predictions = grid_predictions.reshape(x_grid.shape)\n\n    # Plot the predictions on a color plot\n    plt.figure(figsize=(12, 8))\n    plt.pcolormesh(x_grid, y_grid, grid_predictions, cmap=plt.cm.terrain)\n\n    # Plot the training points on top of the grid\n    plt.scatter(features[:, 0], features[:, 1], c=labels, cmap=plt.cm.terrain)\n\n    # Add the labels for the axes\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')", "CodeBleu_score": 53.64641761783411}]}
{"instance_id": "8eaee3207c09", "generation_result": [{"code": "def plot_sgd_separator()\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.datasets import make_blobs\n    from matplotlib import pyplot as plt\n    from matplotlib.colors import ListedColormap\n    import numpy as np\n\n    # Create 50 separable points\n    X, y = make_blobs(n_samples=50, centers=2, random_state=1, cluster_std=2.5)\n\n    # Instantiate SGD classifier\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n\n    # Fit SGD classifier\n    clf.fit(X, y)\n\n    # Create a meshgrid to plot in 2D\n    h = .01\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Evaluate decision function at all points in meshgrid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put Z back in meshgrid form\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundary\n    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=[\"--\", \"-\", \"--\"], colors=[\"red\", \"green\", \"red\"])\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap([\"red\", \"green\"]))\n\n    # Set axis limits\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(),", "CodeBleu_score": 40.27945390029015}, {"code": "def plot_sgd_separator()\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.datasets import make_blobs\n    from matplotlib import pyplot as plt\n\n    # Create data\n    X, y = make_blobs(n_samples=50, centers=2, random_state=42)\n\n    # Create and fit model\n    model = SGDClassifier(loss='hinge', alpha=0.01, max_iter=200, fit_intercept=True)\n    model.fit(X, y)\n\n    # Create a mesh to plot in\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    h = 0.01\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Plot the decision boundary\n    plt.contour(xx, yy, model.decision_function(np.c_[xx.ravel(), yy.ravel()]),\n                levels=[-1, 0, 1], colors=['black', 'black', 'black'],\n                linestyles=['--', '-', '--'],\n                )\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.show()\n\n", "CodeBleu_score": 35.29987132176764}, {"code": "def plot_sgd_separator()\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.datasets import make_blobs\n    from sklearn.metrics import plot_decision_boundary\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Create data\n    X, y = make_blobs(n_samples=50, centers=2, random_state=42)\n\n    # Fit SGD classifier\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n    clf.fit(X, y)\n\n    # Plot decision boundary\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    fig, ax = plt.subplots()\n    ax.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=[\"--\", \"-\", \"--\"], colors=[\"red\", \"green\", \"red\"])\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Paired)\n    plt.axis(\"tight\")\n    plt.show()\n\n", "CodeBleu_score": 41.12629109200138}]}
{"instance_id": "8b54f999b4c0", "generation_result": [{"code": "def dump_svmlight_file(self, file)\n    import numpy as np\n    import svmlight\n    from sklearn.utils.validation import check_array\n    X = check_array(self.data, accept_sparse=True)\n    y = self.target\n    svmlight.dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 25.884330837560587}, {"code": "def dump_svmlight_file(self, file)\n    import numpy as np\n    import svmlight\n\n    # Convert data to a NumPy array\n    data = np.array(self.data)\n\n    # Separate features and labels\n    X = data[:, 0:2]\n    y = data[:, 2]\n\n    # Dump the features and labels to the specified file\n    svmlight.dump_svmlight_file(X, y, file)\n\n    return", "CodeBleu_score": 68.25029029949803}, {"code": "def dump_svmlight_file(self, file)\n    import numpy as np\n    from sklearn.datasets import dump_svmlight_file\n    X = np.array(self.data)[:, 0:2]\n    y = np.array(self.data)[:, 2]\n    dump_svmlight_file(X, y, file)\n\n", "CodeBleu_score": 47.54512326604157}]}
{"instance_id": "2419013adc6e", "generation_result": [{"code": "def fit(self)\n    print(\"Fitting model...\")\n    X, y = self.data.get_train_data()\n    if len(set(y)) == 1:\n        self.classifier = OneClassSVM(nu=0.1, kernel=self.kernel, gamma=self.gamma, coef0=self.coef0, degree=self.degree)\n    else:\n        self.classifier = SVC(kernel=self.kernel, gamma=self.gamma, coef0=self.coef0, degree=self.degree)\n    self.classifier.fit(X, y)\n    if hasattr(self.classifier, \"score\"):\n        print(\"Model accuracy: {:.2f}%\".format(self.classifier.score(X, y) * 100))\n    self.decision_surface = self.classifier.decision_function(X)\n    self.surface_type = \"decision\"\n    self.fitted = True\n    self.surface_changed.emit()\n", "CodeBleu_score": 28.048421854705285}, {"code": "def fit(self)\n    print(\"Fitting model...\")\n    X, y = self.get_data()\n    X, y = self.split_data(X, y)\n    C = self.complexity\n    gamma = self.gamma\n    coef0 = self.coef0\n    degree = self.degree\n    kernel = self.kernel\n    if len(np.unique(y)) == 1:\n        clf = OneClassSVM(nu=C, gamma=gamma, coef0=coef0, degree=degree, kernel=kernel)\n    else:\n        clf = SVC(C=C, gamma=gamma, coef0=coef0, degree=degree, kernel=kernel)\n    clf.fit(X, y)\n    if hasattr(clf, \"score\"):\n        print(\"Model accuracy: \" + str(clf.score(X, y)))\n    self.model = clf\n    self.decision_surface = self.calculate_surface()\n    self.surface_type = \"decision\"\n    self.fitted = True\n    self.surface_changed.emit()\n\n", "CodeBleu_score": 37.10358419241865}, {"code": "def fit(self)\n\n    print('Fitting model...')\n\n    X = self.data.get_features()\n    y = self.data.get_labels()\n\n    C = self.complexity\n    gamma = self.gamma\n    coef0 = self.coef0\n    degree = self.degree\n    kernel = self.kernel\n\n    if all(y == y[0]):\n        clf = OneClassSVM(kernel=kernel, gamma=gamma, coef0=coef0, degree=degree)\n    else:\n        clf = SVC(kernel=kernel, gamma=gamma, coef0=coef0, degree=degree, C=C)\n\n    clf.fit(X, y)\n\n    if hasattr(clf, 'score'):\n        print('Accuracy: ' + str(clf.score(X, y)))\n\n    self.decision_surface = self.calculate_decision_surface(clf)\n    self.surface_type = 'decision'\n    self.fitted = True\n    self.surface_changed.emit()\n\n", "CodeBleu_score": 35.07950418310013}]}
{"instance_id": "e887fd2a8513", "generation_result": [{"code": "def source_decode(sourcecode, verbose=0)\n    import importlib\n    import re\n    import sys\n\n    import_str = \"\"\n    object_str = \"\"\n    obj = None\n\n    # split the source code into module and object components\n    # if the source code starts with \"tpot.\", import the object from the corresponding module\n    # otherwise, import the object from the module directly\n    if sourcecode.startswith(\"tpot.\"):\n        import_str = sourcecode.split(\".\")[1]\n        object_str = \".\".join(sourcecode.split(\".\")[2:])\n    else:\n        import_str = sourcecode.split(\".\")[0]\n        object_str = \".\".join(sourcecode.split(\".\")[1:])\n\n    # try to import the object\n    try:\n        obj = getattr(importlib.import_module(import_str), object_str)\n    except ImportError as e:\n        if verbose > 2:\n            raise e\n        else:\n            sys.stderr.write(\n                \"Warning: Could not import %s.%s: %s\\n\" % (import_str, object_str, e)\n            )\n\n    return import_str, object_str, obj\n\n", "CodeBleu_score": 43.96895229745324}, {"code": "def source_decode(sourcecode, verbose=0)\n    import re\n    import sys\n    import importlib\n    from tpot import TPOTClassifier, TPOTRegressor\n    importlib.reload(TPOTClassifier)\n    importlib.reload(TPOTRegressor)\n    if sourcecode.startswith(\"tpot.\"):\n        # source code is a tpot object\n        object_string = sourcecode.split(\".\")[-1]\n        module_string = \".\".join(sourcecode.split(\".\")[:-1])\n        try:\n            module = importlib.import_module(module_string)\n            obj = getattr(module, object_string)\n        except ImportError as e:\n            if verbose > 2:\n                raise e\n            else:\n                print(\"WARNING: Failed to import object %s from module %s\" % (object_string, module_string))\n                obj = None\n    else:\n        # source code is a module\n        try:\n            module = importlib.import_module(sourcecode)\n            obj = module\n        except ImportError as e:\n            if verbose > 2:\n                raise e\n            else:\n                print(\"WARNING: Failed to import module %s\" % sourcecode)\n                obj = None\n    return sourcecode, object_string, obj\n\n", "CodeBleu_score": 40.34853773492374}, {"code": "def source_decode(sourcecode, verbose=0)\n    import re\n    import importlib\n    import warnings\n    import sys\n    import traceback\n\n    # remove comments\n    sourcecode = re.sub(r'#.*', '', sourcecode)\n\n    # remove whitespace\n    sourcecode = re.sub(r'\\s+', '', sourcecode)\n\n    # remove line breaks\n    sourcecode = re.sub(r'[\\r\\n]+', '', sourcecode)\n\n    # remove trailing parentheses\n    sourcecode = re.sub(r'[\\(\\)]+$', '', sourcecode)\n\n    # remove trailing commas\n    sourcecode = re.sub(r',$', '', sourcecode)\n\n    # remove trailing colons\n    sourcecode = re.sub(r':$', '', sourcecode)\n\n    # remove trailing semicolons\n    sourcecode = re.sub(r';$', '', sourcecode)\n\n    # remove trailing periods\n    sourcecode = re.sub(r'\\.$', '', sourcecode)\n\n    # remove trailing underscores\n    sourcecode = re.sub(r'_+$', '', sourcecode)\n\n    # remove leading parentheses\n    sourcecode = re.sub(r'^\\(', '', sourcecode)\n\n    # remove leading colons\n    sourcecode = re.sub(r'^:', '', sourcecode)\n\n    # remove leading semicolons\n    sourcecode = re.sub(r'^;', '', sourcecode)\n\n    # remove leading periods\n    sourcecode = re.sub(r'^\\.', '', sourcecode)\n\n    # remove leading underscores\n    sourcecode = re.sub(r'^_+', '', sourcecode)\n\n    # remove leading and trailing parentheses\n    sourcecode = re.sub(r'^\\(|\\)$', '', sourcecode)\n\n    # remove leading and trailing colons\n    sourcecode = re.sub(r'^:|:$', '', sourcecode)\n\n    # remove leading and trailing semicolons\n    sourcecode = re.sub(r'^;|;$', '', sourcecode)\n\n    # remove leading and trailing periods\n    sourcecode = re.sub(r'^\\.|\\.$', '', sourcecode)\n\n    # remove leading and trailing underscores", "CodeBleu_score": 20.70312237901902}]}
{"instance_id": "5d6f629c3363", "generation_result": [{"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\n    # initialize an empty dictionary for sample weights\n    sample_weight_dict = {}\n\n    # check if sample_weight is not None\n    if sample_weight is not None:\n\n        # loop over the steps in the pipeline\n        for step in pipeline_steps:\n\n            # get the fit method for each step\n            fit_method = step.get_params()['fit']\n\n            # check if fit method accepts sample_weight argument\n            if 'sample_weight' in fit_method.__code__.co_varnames:\n\n                # construct a key by combining the step name and \"sample_weight\"\n                key = step.name + \"_sample_weight\"\n\n                # add key to the dictionary\n                sample_weight_dict[key] = sample_weight\n\n    # return sample weight dictionary if it is not empty, otherwise, return None\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None", "CodeBleu_score": 47.07747673331955}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n    if sample_weight is None:\n        return None\n    sample_weight_dict = {}\n    for step in pipeline_steps:\n        step_name = step[0]\n        step_obj = step[1]\n        step_params = step[2]\n        if \"fit\" in dir(step_obj):\n            fit_method = step_obj.fit\n            if \"sample_weight\" in fit_method.__code__.co_varnames:\n                sample_weight_dict[step_name + \"_sample_weight\"] = sample_weight\n    if len(sample_weight_dict) == 0:\n        return None\n    else:\n        return sample_weight_dict", "CodeBleu_score": 31.610952439548296}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None)\n\n    # Initialize an empty dictionary for sample weights\n    sample_weights = {}\n\n    # Check if sample_weight is not None\n    if sample_weight is not None:\n\n        # Iterate through each step in the pipeline\n        for step in pipeline_steps:\n\n            # Inspect the fit method to see if it accepts a \"sample_weight\" argument\n            fit_signature = inspect.signature(step.fit)\n            if \"sample_weight\" in fit_signature.parameters:\n\n                # Construct a key by combining the step name and \"sample_weight\"\n                sample_weights[step.name + \"_sample_weight\"] = sample_weight\n\n    # Return the sample weight dictionary if it is not empty, otherwise, return None\n    if len(sample_weights) > 0:\n        return sample_weights\n    else:\n        return None\n\n", "CodeBleu_score": 36.41704309093571}]}
{"instance_id": "cae17d519818", "generation_result": [{"code": "def _summary_of_best_pipeline(self, features, target):\n    if not hasattr(self, 'fitted_pipeline_'):\n        raise RuntimeError(\"No pipeline was fitted yet. Run tpot.fit(X_train, y_train) first.\")\n\n    self.fitted_pipeline_ = self.fitted_pipeline_.fit(features, target)\n    self.fitted_pipelines_ = [pipeline.fit(features, target) for pipeline in self.fitted_pipelines_]\n\n    if self.verbosity > 0:\n        print(self.fitted_pipeline_.__str__())\n        if self.verbosity > 1:\n            print()\n\n    return self\n", "CodeBleu_score": 20.82109494465767}, {"code": "def _summary_of_best_pipeline(self, features, target):\n    if self.fitted_pipeline_ is None:\n        raise RuntimeError(\"No pipeline was fitted yet. Call the fit method first.\")\n    self.fitted_pipeline_ = self.fitted_pipeline_.fit(features, target)\n    if self.verbosity > 0:\n        print(\"Best pipeline:\")\n        print(self.fitted_pipeline_.get_formatted_pipeline())\n    self.fitted_pipelines_ = [p.fit(features, target) for p in self.pareto_front_fitted_]\n    return self\n\n", "CodeBleu_score": 18.51429218685606}, {"code": "def _summary_of_best_pipeline(self, features, target):\n    if not hasattr(self, \"fitted_pipeline_\"):\n        raise RuntimeError(\"No fitted pipeline available. TPOT must be fit to a training set first.\")\n\n    if self.verbosity > 0:\n        print(\"\\nBest pipeline:\")\n        print(self.fitted_pipeline_.exported_pipeline_)\n\n    self.fitted_pipeline_.fit(features, target)\n\n    if self.verbosity > 0:\n        print(\"\\nFitting entire Pareto front of pipelines...\")\n\n    self.fitted_pipelines_ = [p.fit(features, target) for p in self.optimized_pipelines_]\n\n    if self.verbosity > 0:\n        print(\"Done.\")\n\n    return self", "CodeBleu_score": 21.05769286840759}]}
{"instance_id": "638683d78167", "generation_result": [{"code": "def score(self, testing_features, testing_target)\n    if not self.is_optimized:\n        raise NotImplementedError(\n            \"The pipeline has not been optimized yet. Please run the optimize() method first.\"\n        )\n    testing_features, testing_target = self._validate_inputs(\n        testing_features, testing_target\n    )\n    if isinstance(self.scoring, str):\n        scorer = SCORERS[self.scoring]\n    else:\n        scorer = self.scoring\n    score = scorer(self, testing_features, testing_target)\n    return score", "CodeBleu_score": 38.29089926952833}, {"code": "def score(self, testing_features, testing_target)\n\n    # Check if pipeline has been optimized\n    if not self.is_fitted:\n        raise NotFittedError(\n            \"This %s instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\"\n            % type(self).__name__\n        )\n\n    # Validate the testing data\n    testing_features, testing_target = self._validate_data(\n        testing_features, testing_target\n    )\n\n    # Check if the scorer is a predefined scorer\n    if self.scoring in self.SCORERS:\n        scorer = self.SCORERS[self.scoring]\n    # Otherwise, check if it is a callable scorer\n    elif callable(self.scoring):\n        scorer = self.scoring\n    # Otherwise, raise an error\n    else:\n        raise ValueError(\n            \"The provided scoring function must be either a string or a callable.\"\n        )\n\n    # Calculate and return the score\n    return scorer(self, testing_features, testing_target)", "CodeBleu_score": 41.974361623190546}, {"code": "def score(self, testing_features, testing_target)\n    if not self.optimized:\n        raise ValueError(\"This pipeline has not been optimized yet. Please run the optimize() method first.\")\n    testing_features, testing_target = self._validate_data(testing_features, testing_target)\n    if callable(self.scoring):\n        scorer = self.scoring\n    elif self.scoring in self.SCORERS:\n        scorer = self.SCORERS[self.scoring]\n    else:\n        raise ValueError(f\"{self.scoring} is not a valid scoring function. Please use a predefined scorer or provide a callable scorer.\")\n    return scorer(self.pipeline, testing_features, testing_target)", "CodeBleu_score": 37.48174530342315}]}
{"instance_id": "a04382560333", "generation_result": [{"code": " def _impute_values(self, features)    if self.verbosity > 1:\n      print(\"Imputing missing values...\")\n    if self.imputer is None:\n      self.imputer = SimpleImputer(strategy=\"median\")\n      self.imputer.fit(features)\n    features = self.imputer.transform(features)\n    return features", "CodeBleu_score": 73.77771761030314}, {"code": " def _impute_values(self, features)    if self.verbosity > 1:\n        print(\"Imputing missing values...\")\n\n    if self.imputer is None:\n        self.imputer = SimpleImputer(strategy=\"median\")\n        self.imputer.fit(features)\n\n    return self.imputer.transform(features)", "CodeBleu_score": 73.82620092731361}, {"code": " def _impute_values(self, features)    if self.verbose > 1:\n      print(\"Imputing missing values...\")\n    if not self.fitted:\n      self.imputer = SimpleImputer(strategy=\"median\")\n      self.imputer.fit(features)\n      self.fitted = True\n    return self.imputer.transform(features)\n\n  def _scale_values(self, features):\n    \"\"\"# This function scales a feature set.\\nIf verbosity is greater than 1, it prints a message indicating the scaling process.\\nIf the scaler has not been fitted yet, it creates a StandardScaler and fits it to the features.\\nThe function then transforms the features using the fitted scaler and returns the scaled feature set.\"\"\"\n    if self.verbose > 1:\n      print(\"Scaling values...\")\n    if not self.fitted:\n      self.scaler = StandardScaler()\n      self.scaler.fit(features)\n      self.fitted = True\n    return self.scaler.transform(features)\n\n  def fit(self, features, labels):\n    \"\"\"# This function fits the model to the features and labels.\\nIf verbosity is greater than 1, it prints a message indicating the fitting process.\\nIf the model has not been fitted yet, it creates a LogisticRegression model with the \"lbfgs\" solver and fits it to the features and labels.\\nThe function then returns the fitted model.\"\"\"\n    if self.verbose > 1:\n      print(\"Fitting model...\")\n    if not self.fitted:\n      self.model = LogisticRegression(solver=\"lbfgs\")\n      self.model.fit(features, labels)\n      self.fitted = True\n    return self.model\n\n  def predict(self, features):\n    \"\"\"# This function predicts the labels for a feature set.\\nIf verbosity is greater than 1, it prints a message indicating the prediction process.\\nThe function returns the predicted labels.\"\"\"\n    if self.verbose > 1:\n      print(\"Predicting labels...\")\n    return self.model.predict(features)\n\n  def score(self, features, labels):\n    \"\"\"#", "CodeBleu_score": 47.9485270440984}]}
{"instance_id": "0043d411c332", "generation_result": [{"code": "def _compile_to_sklearn(self, expr)\n\n    import ast\n    import inspect\n    import pkgutil\n    import re\n\n    from sklearn.pipeline import Pipeline\n\n    # Get the context of the current module\n    context = inspect.getmodule(inspect.currentframe()).__dict__\n\n    # Get the code string from the expression\n    code = self.compile_to_string(expr)\n\n    # Evaluate the code string within the context\n    pipeline = eval(code, context)\n\n    # Set the memory attribute of the pipeline\n    pipeline.set_params(memory=self.memory)\n\n    # If a random state is specified, recursively set the random_state parameter for each step in the pipeline\n    if self.random_state is not None:\n        for step in pipeline.steps:\n            step[1].set_params(random_state=self.random_state)\n\n    return pipeline\n\n", "CodeBleu_score": 42.93874459958201}, {"code": "def _compile_to_sklearn(self, expr)\n\n    # Generate the pipeline code string from the expression and a predefined set of operators\n    pipeline_code = self._generate_pipeline_code(expr)\n\n    # Evaluate the pipeline code string within a specific context to obtain the scikit-learn pipeline object\n    pipeline = eval(pipeline_code, self.context)\n\n    # Set the pipeline's memory attribute\n    pipeline.memory = self.memory\n\n    # If a random state is specified, recursively set the random_state parameter for each step in the pipeline\n    if self.random_state is not None:\n        self._set_random_state(pipeline)\n\n    return pipeline\n\n", "CodeBleu_score": 19.09795077822446}, {"code": "def _compile_to_sklearn(self, expr)\n    import sklearn.pipeline\n    import sklearn.compose\n    import sklearn.preprocessing\n    import sklearn.decomposition\n    import sklearn.feature_selection\n    import sklearn.model_selection\n    import sklearn.linear_model\n    import sklearn.neural_network\n    import sklearn.tree\n    import sklearn.ensemble\n    import sklearn.neighbors\n    import sklearn.svm\n    import sklearn.gaussian_process\n    import sklearn.gaussian_process.kernels\n    import sklearn.naive_bayes\n    import sklearn.discriminant_analysis\n    import sklearn.semi_supervised\n    import sklearn.metrics\n    import sklearn.cluster\n    import sklearn.covariance\n    import sklearn.manifold\n    import sklearn.isotonic\n    import sklearn.kernel_ridge\n    import sklearn.multioutput\n    import sklearn.multioutput\n    import sklearn.multiclass\n    import sklearn.calibration\n    import sklearn.dummy\n    import sklearn.impute\n    import sklearn.random_projection\n    import sklearn.decomposition\n    import sklearn.kernel_approximation\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import sklearn.feature_extraction\n    import", "CodeBleu_score": 7.1283554049910896}]}
{"instance_id": "ca0ce462aa76", "generation_result": [{"code": "def pick_two_individuals_eligible_for_crossover(population)\n    # Create sets of primitive nodes for each individual\n    primitive_node_sets = []\n    for individual in population:\n        primitive_node_set = set()\n        for node in individual:\n            if node.type == \"primitive\":\n                primitive_node_set.add(node.name)\n        primitive_node_sets.append(primitive_node_set)\n\n    # Convert each individual to a string representation\n    string_representations = []\n    for individual in population:\n        string_representation = \"\"\n        for node in individual:\n            string_representation += node.name\n        string_representations.append(string_representation)\n\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i in range(len(population)):\n        for j in range(i + 1, len(population)):\n            if len(primitive_node_sets[i].intersection(primitive_node_sets[j])) > 0 and string_representations[i] != string_representations[j]:\n                eligible_pairs.append((i, j))\n\n    # If no eligible pairs are found, return None for both individuals\n    if len(eligible_pairs) == 0:\n        return None, None\n\n    # Otherwise, randomly select one of the eligible pairs and return the corresponding individuals from the population\n    else:\n        pair = eligible_pairs[random.randint(0, len(eligible_pairs) - 1)]\n        return population[pair[0]], population[pair[1]]", "CodeBleu_score": 28.315473158464787}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\n    # Create sets of primitive nodes for each individual and convert each individual to a string representation\n    primitive_sets = []\n    string_representations = []\n    for individual in population:\n        primitive_sets.append(individual.get_primitive_nodes())\n        string_representations.append(individual.to_string())\n\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    eligible_pairs = []\n    for i in range(len(population)):\n        for j in range(i + 1, len(population)):\n            if primitive_sets[i].intersection(primitive_sets[j]) and string_representations[i] != string_representations[j]:\n                eligible_pairs.append((i, j))\n\n    # If no eligible pairs are found, the function returns None for both individuals\n    if len(eligible_pairs) == 0:\n        return None, None\n\n    # Otherwise, it randomly selects one of the eligible pairs and returns the corresponding individuals from the population\n    else:\n        i, j = eligible_pairs[np.random.randint(len(eligible_pairs))]\n        return population[i], population[j]\n\n", "CodeBleu_score": 27.51958491205409}, {"code": "def pick_two_individuals_eligible_for_crossover(population)\n\n    # Create sets of primitive nodes for each individual\n    primitive_nodes = []\n    for individual in population:\n        primitive_nodes.append(set())\n        for node in individual:\n            if node.type == 'primitive':\n                primitive_nodes[-1].add(node.name)\n\n    # Convert each individual to a string representation\n    strings = []\n    for individual in population:\n        strings.append(str(individual))\n\n    # Identify pairs of individuals that share at least one primitive and are not identical\n    pairs = []\n    for i in range(len(population)):\n        for j in range(i + 1, len(population)):\n            if len(primitive_nodes[i].intersection(primitive_nodes[j])) > 0 and strings[i] != strings[j]:\n                pairs.append((i, j))\n\n    # If no eligible pairs are found, the function returns None for both individuals\n    if len(pairs) == 0:\n        return None, None\n\n    # Otherwise, it randomly selects one of the eligible pairs and returns the corresponding individuals from the population\n    else:\n        i, j = pairs[np.random.randint(0, len(pairs))]\n        return population[i], population[j]\n\n", "CodeBleu_score": 30.47755892703253}]}
{"instance_id": "4b03c843f506", "generation_result": [{"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    # Process operators to create steps of pipeline\n    steps = []\n    for operator in operators:\n        # Get name and parameters\n        name = operator[\"name\"]\n        params = operator[\"parameters\"]\n        # Create step\n        step = name + \"(**\" + str(params) + \")\"\n        steps.append(step)\n    # Determine number of steps\n    num_steps = len(steps)\n    # If there are more than one step, format steps into make_pipeline call with indentation\n    if num_steps > 1:\n        # Format steps into make_pipeline call\n        steps_str = \"make_pipeline(\\n    \" + \",\\n    \".join(steps) + \"\\n)\"\n    # If there is only one step, format it directly without make_pipeline\n    elif num_steps == 1:\n        steps_str = steps[0]\n    # Return code as string\n    return steps_str\n\n", "CodeBleu_score": 33.14191266980855}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n\n    # The code_steps variable is a list of strings that will be used to store the code for each step in the pipeline.\n    code_steps = []\n\n    # The steps variable is a list of dictionaries that will be used to store the step name and the code for each step in the pipeline.\n    steps = []\n\n    # The pipeline_steps variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable is a list that will be used to store the step names for the pipeline.\n    pipeline_steps_code = []\n\n    # The pipeline_steps_code variable", "CodeBleu_score": 11.674885595790618}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators)\n    # Process the operators to create the steps of the pipeline\n    steps = []\n    for operator in operators:\n        # Add the step to the list of steps\n        steps.append(operator.export())\n    # Determine the number of steps\n    num_steps = len(steps)\n    # If there is more than one step, format the steps into a make_pipeline call with indentation\n    if num_steps > 1:\n        # Format the steps into a make_pipeline call with indentation\n        steps_indented = (\"\\n\" + \" \" * 4).join(steps)\n        # Format the pipeline code\n        pipeline_code = f\"make_pipeline(\\n    {steps_indented}\\n)\"\n    # If there is only one step, format it directly without make_pipeline\n    if num_steps == 1:\n        # Format the pipeline code\n        pipeline_code = steps[0]\n    # Return the resulting code as a string\n    return pipeline_code", "CodeBleu_score": 33.06154303357858}]}
{"instance_id": "e9aec2ee39a4", "generation_result": [{"code": "def _process_operator(operator, operators, depth=0)\n\n    # Check if the operator is a \"CombineDFs\" operation.\n    if operator[\"name\"] == \"CombineDFs\":\n        # Append the result of combining dataframes.\n        steps = [(\"combine_dfs\", operator[\"primitive\"][\"produce\"])]\n    else:\n        # Retrieve the corresponding TPOT operator.\n        tpot_operator = operators[operator[\"primitive\"][\"python_path\"]]\n        # Process any input operators recursively.\n        input_steps = []\n        for input_operator in operator[\"inputs\"]:\n            if input_operator[\"id\"] != \"0\":\n                input_steps += _process_operator(operators[input_operator[\"id\"]], operators, depth + 1)\n        # If the operator is a root operator and the depth is greater than zero, wrap the operator in a StackingEstimator.\n        if depth > 0:\n            tpot_operator = StackingEstimator(tpot_operator)\n        # Append the processed operator.\n        steps = [(operator[\"name\"], tpot_operator)] + input_steps\n    return steps\n\n", "CodeBleu_score": 33.45709341525605}, {"code": "def _process_operator(operator, operators, depth=0)\n\n    # Identify operator name\n    name = operator[\"name\"]\n\n    # Check if operator is CombineDFs\n    if name == \"CombineDFs\":\n        # Retrieve operator inputs\n        operator_inputs = operator[\"inputs\"]\n\n        # Retrieve input operators\n        operator_inputs_0 = operator_inputs[0]\n        operator_inputs_1 = operator_inputs[1]\n\n        # Retrieve input operator names\n        operator_inputs_0_name = operator_inputs_0[\"name\"]\n        operator_inputs_1_name = operator_inputs_1[\"name\"]\n\n        # Check if input operators are initial input matrix\n        if operator_inputs_0_name == \"X_train\" and operator_inputs_1_name == \"X_train\":\n            # Return combine dataframes step\n            return [(\"CombineDFs\", \"passthrough\", [(\"X_train\", \"X_train\")])]\n\n        # Process input operators recursively\n        steps_0 = _process_operator(operator_inputs_0, operators, depth + 1)\n        steps_1 = _process_operator(operator_inputs_1, operators, depth + 1)\n\n        # Return combine dataframes step\n        return [(\"CombineDFs\", \"passthrough\", steps_0 + steps_1)]\n\n    # Retrieve operator type\n    operator_type = operator[\"type\"]\n\n    # Check if operator is a root operator\n    if depth == 0:\n        # Retrieve operator inputs\n        operator_inputs = operator[\"inputs\"]\n\n        # Retrieve input operator names\n        operator_inputs_0_name = operator_inputs[0][\"name\"]\n        operator_inputs_1_name = operator_inputs[1][\"name\"]\n\n        # Check if input operators are initial input matrix\n        if operator_inputs_0_name == \"X_train\" and operator_inputs_1_name == \"X_train\":\n            # Retrieve operator parameters\n            operator_params = operator[\"params\"]\n\n            # Check if operator is a regressor\n            if operator_", "CodeBleu_score": 31.707298468159994}, {"code": "def _process_operator(operator, operators, depth=0)\n    if operator[\"primitive\"][\"python_path\"] == \"d3m.primitives.data_transformation.construct_predictions.Common\":\n        # Retrieve the input dataframe\n        input_dataframe = operator[\"inputs\"][0][\"dataframe\"]\n        # Retrieve the input dataframe's name\n        input_dataframe_name = input_dataframe[\"metadata\"][\"name\"]\n        # Retrieve the input dataframe's index\n        input_dataframe_index = input_dataframe[\"metadata\"][\"index\"]\n        # Retrieve the input dataframe's semantic types\n        input_dataframe_semantic_types = input_dataframe[\"metadata\"][\"semantic_types\"]\n        # Retrieve the input dataframe's length\n        input_dataframe_length = input_dataframe[\"metadata\"][\"length\"]\n        # Retrieve the input dataframe's resource ID\n        input_dataframe_resource_id = input_dataframe[\"resource_id\"]\n        # Retrieve the input dataframe's resource ID\n        input_dataframe_columns = input_dataframe[\"columns\"]\n        # Retrieve the input dataframe's resource ID\n        input_dataframe_columns_length = len(input_dataframe_columns)\n        # Retrieve the input dataframe's resource ID\n        input_dataframe_columns_metadata = input_dataframe[\"columns_metadata\"]\n        # Retrieve the input dataframe's resource ID\n        input_dataframe_columns_metadata_length = len(input_dataframe_columns_metadata)\n        # Retrieve the input dataframe's resource ID\n        input_dataframe_columns_metadata_index = input_dataframe_columns_metadata[0][\"index\"]\n        # Retrieve the input dataframe's resource ID\n        input_dataframe_columns_metadata_index_length = len(input_dataframe_columns_metadata_index)\n        # Retrieve the input dataframe's resource ID\n        input_dataframe_columns_metadata_index_value = input_dataframe_columns_metadata_index[0][\"value\"]\n        # Retrieve the input dataframe's resource ID\n        input_dataframe_columns_metadata_index_value_", "CodeBleu_score": 20.779629973656746}]}
{"instance_id": "0513bf969510", "generation_result": [{"code": "def transform(self, X)\n    # check if the input is a dataframe\n    if not isinstance(X, pd.DataFrame):\n        raise TypeError(\"X needs to be a pandas dataframe.\")\n\n    # check if the threshold is valid\n    if self.threshold is None or not isinstance(self.threshold, int):\n        raise TypeError(\"The threshold needs to be a positive integer.\")\n\n    # check if the threshold is valid\n    if self.threshold < 1:\n        raise ValueError(\"The threshold needs to be a positive integer.\")\n\n    # find categorical features\n    categorical_features = X.nunique()[X.nunique() < self.threshold].index\n\n    # check if there are categorical features\n    if len(categorical_features) == 0:\n        raise ValueError(\"No categorical features found.\")\n\n    # one-hot encode the categorical features\n    X_transformed = pd.get_dummies(X, columns=categorical_features)\n\n    # return the transformed data\n    return X_transformed", "CodeBleu_score": 34.35000919913272}, {"code": "def transform(self, X)\n    # Check that X is a DataFrame\n    if not isinstance(X, pd.DataFrame):\n        raise TypeError(\"X needs to be a DataFrame\")\n    # Check that the threshold is a float\n    if not isinstance(self.threshold, float):\n        raise TypeError(\"threshold needs to be a float\")\n    # Check that the threshold is between 0 and 1\n    if self.threshold < 0 or self.threshold > 1:\n        raise ValueError(\"threshold needs to be between 0 and 1\")\n    # Check that the threshold is not 1\n    if self.threshold == 1:\n        raise ValueError(\"threshold needs to be different than 1\")\n    # Check that the threshold is not 0\n    if self.threshold == 0:\n        raise ValueError(\"threshold needs to be different than 0\")\n    # Select categorical features\n    categorical_features = X.select_dtypes(include=[\"object\"]).columns\n    # Check that there are categorical features\n    if len(categorical_features) == 0:\n        raise ValueError(\"No categorical features found\")\n    # Select categorical features with cardinality above the threshold\n    categorical_features_to_encode = X[categorical_features].nunique()[X[categorical_features].nunique() > self.threshold * len(X)].index\n    # Check that there are categorical features to encode\n    if len(categorical_features_to_encode) == 0:\n        raise ValueError(\"No categorical features to encode\")\n    # Apply one-hot encoding to the selected categorical features\n    X_encoded = pd.get_dummies(X, columns=categorical_features_to_encode)\n    # Return the encoded data\n    return X_encoded", "CodeBleu_score": 32.05924568614205}, {"code": "def transform(self, X)\n    categorical_features = self.select_features(X)\n    if categorical_features.shape[1] == 0:\n        raise ValueError(\"No categorical features found.\")\n    else:\n        return self.encoder.transform(categorical_features)", "CodeBleu_score": 16.670520266916917}]}
{"instance_id": "5b1bcf42ca9e", "generation_result": [{"code": "def balanced_accuracy(y_true, y_pred)\n\n    # Get unique classes\n    unique_classes = np.unique(y_true)\n\n    # Initialize balanced accuracy\n    balanced_accuracy = 0\n\n    # Iterate through classes\n    for i in range(len(unique_classes)):\n\n        # Get class\n        c = unique_classes[i]\n\n        # Get true positives\n        tp = np.sum((y_true == c) & (y_pred == c))\n\n        # Get false positives\n        fp = np.sum((y_true != c) & (y_pred == c))\n\n        # Get false negatives\n        fn = np.sum((y_true == c) & (y_pred != c))\n\n        # Get true negatives\n        tn = np.sum((y_true != c) & (y_pred != c))\n\n        # Calculate sensitivity\n        sensitivity = tp / (tp + fn)\n\n        # Calculate specificity\n        specificity = tn / (tn + fp)\n\n        # Calculate class accuracy\n        class_accuracy = (sensitivity + specificity) / 2\n\n        # Update balanced accuracy\n        balanced_accuracy += class_accuracy\n\n    # Calculate mean balanced accuracy\n    balanced_accuracy = balanced_accuracy / len(unique_classes)\n\n    return balanced_accuracy", "CodeBleu_score": 22.840298339850253}, {"code": "def balanced_accuracy(y_true, y_pred)\n    classes = np.unique(np.concatenate([y_true, y_pred]))\n    accuracy = 0\n    for c in classes:\n        # Sensitivity\n        tp = np.sum((y_true == c) & (y_pred == c))\n        fp = np.sum((y_pred == c) & (y_true != c))\n        fn = np.sum((y_true == c) & (y_pred != c))\n        sensitivity = tp / (tp + fn)\n        # Specificity\n        tn = np.sum((y_true != c) & (y_pred != c))\n        fp = np.sum((y_pred == c) & (y_true != c))\n        specificity = tn / (tn + fp)\n        # Class accuracy\n        accuracy += (sensitivity + specificity) / 2\n    return accuracy / len(classes)\n\n", "CodeBleu_score": 20.760912149988357}, {"code": "def balanced_accuracy(y_true, y_pred)\n    classes = np.unique(y_true)\n    balanced_accuracies = []\n    for c in classes:\n        tp = np.sum((y_true == c) & (y_pred == c))\n        tn = np.sum((y_true != c) & (y_pred != c))\n        fp = np.sum((y_true != c) & (y_pred == c))\n        fn = np.sum((y_true == c) & (y_pred != c))\n        sensitivity = tp / (tp + fn)\n        specificity = tn / (tn + fp)\n        balanced_accuracies.append((sensitivity + specificity) / 2)\n    return np.mean(balanced_accuracies)", "CodeBleu_score": 19.955483620704666}]}
{"instance_id": "19a9ad39de0a", "generation_result": [{"code": "def transform(self, X, y=None)\n    X = check_array(X)\n    nonzero_count = np.count_nonzero(X, axis=1)\n    zero_count = X.shape[1] - nonzero_count\n    X_transformed = np.c_[zero_count, nonzero_count, X]\n    return X_transformed", "CodeBleu_score": 11.825486962950572}, {"code": "def transform(self, X, y=None)\n\n    X = check_array(X)\n\n    nonzero_counts = np.count_nonzero(X, axis=1)\n    zero_counts = np.count_zero(X, axis=1)\n\n    X_transformed = np.hstack((nonzero_counts.reshape(-1, 1), zero_counts.reshape(-1, 1), X))\n\n    return X_transformed\n\n", "CodeBleu_score": 18.477640827818764}, {"code": "def transform(self, X, y=None)\n    X = check_array(X)\n    nonzero_count = np.count_nonzero(X, axis=1)\n    zero_count = np.count_nonzero(X == 0, axis=1)\n    return np.c_[nonzero_count, zero_count, X]", "CodeBleu_score": 8.480097949659992}]}
{"instance_id": "5e8db0b5d447", "generation_result": [{"code": "def _init_pretest(self, features, target)\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n\n    self.pretest_X, self.pretest_y = train_test_split(\n        features, target, test_size=self.pretest_size, random_state=self.random_state\n    )\n    self.num_classes = len(np.unique(self.pretest_y))\n\n    if self.pretest_X.shape[0] < self.num_classes:\n        self.pretest_X = np.concatenate(\n            (self.pretest_X, self.pretest_X[: self.num_classes - self.pretest_X.shape[0]])\n        )\n        self.pretest_y = np.concatenate(\n            (self.pretest_y, self.pretest_y[: self.num_classes - self.pretest_y.shape[0]])\n        )\n\n    if len(np.unique(self.pretest_y)) != self.num_classes:\n        self.pretest_y = self.pretest_y[: self.num_classes]\n        self.pretest_X = self.pretest_X[: self.num_classes]\n\n", "CodeBleu_score": 30.992658329508867}, {"code": "def _init_pretest(self, features, target)\n    self.n_classes = len(np.unique(target))\n    if self.n_classes > features.shape[0]:\n        self.pretest_X = features\n        self.pretest_y = target\n    else:\n        self.pretest_X, self.pretest_y = train_test_split(\n            features,\n            target,\n            train_size=self.n_classes,\n            random_state=self.random_state,\n        )\n    self.pretest_y = self.pretest_y.astype(int)\n    if len(np.unique(self.pretest_y)) < self.n_classes:\n        for i in range(self.n_classes):\n            if i not in self.pretest_y:\n                self.pretest_y = np.append(self.pretest_y, i)\n    return self\n\n", "CodeBleu_score": 27.764374205018537}, {"code": "def _init_pretest(self, features, target)\n    self.n_classes = len(np.unique(target))\n    self.n_train = max(self.n_classes, self.n_train)\n    self.pretest_X, self.pretest_y = train_test_split(features, target, train_size=self.n_train, random_state=self.random_state)\n    if len(np.unique(self.pretest_y)) < self.n_classes:\n        self.pretest_y = np.array([self.pretest_y[np.where(target == unique_target)[0][0]] for unique_target in np.unique(target)])\n    return self\n", "CodeBleu_score": 27.09601091708802}]}
{"instance_id": "5e3d48667396", "generation_result": [{"code": "def _to_numpy_type(dtype)\n    if dtype is None:\n        return None\n    if isinstance(dtype, str):\n        dtype = tf.DType(dtype)\n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    return dtype\n", "CodeBleu_score": 72.32566468631128}, {"code": "def _to_numpy_type(dtype)\n    if isinstance(dtype, str):\n        return _to_numpy_type(getattr(tf, dtype))\n    elif isinstance(dtype, tf.DType):\n        return getattr(np, dtype.name.capitalize())\n    elif isinstance(dtype, np.dtype):\n        return dtype\n    else:\n        raise TypeError(\"Unsupported data type: %s\" % dtype)\n\n", "CodeBleu_score": 46.7031363200777}, {"code": "def _to_numpy_type(dtype)\n    import numpy as np\n    if dtype is None:\n        return None\n    if isinstance(dtype, np.dtype):\n        return dtype\n    if dtype.is_floating:\n        return np.float32\n    elif dtype.is_integer:\n        return np.int32\n    elif dtype.is_unsigned:\n        return np.uint8\n    else:\n        raise TypeError('The given data type is not supported.')\n", "CodeBleu_score": 45.746804285927276}]}
{"instance_id": "8f1120fdd463", "generation_result": [{"code": "def _dataset_as_numpy(ds, batch_size=None)\n    try:\n        if batch_size is None:\n            batch_size = ds.cardinality().numpy()\n        ds = ds.batch(batch_size)\n    except:\n        pass\n    return [x.numpy() for x in ds]\n\n", "CodeBleu_score": 16.263359842226368}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n    try:\n        # Try to use the batch function\n        if batch_size is not None:\n            ds = ds.batch(batch_size)\n        return [x.numpy() for x in ds]\n    except:\n        # If batching is not supported, fall back to a default method\n        return [x.numpy() for x in ds]\n", "CodeBleu_score": 13.168818717667216}, {"code": "def _dataset_as_numpy(ds, batch_size=None)\n\n    if batch_size is not None:\n        try:\n            ds = ds.batch(batch_size)\n        except:\n            ds = ds.apply(tf.data.experimental.dense_to_sparse_batch(batch_size))\n\n    return [x.numpy() for x in ds]", "CodeBleu_score": 20.674333625800706}]}
{"instance_id": "282271955871", "generation_result": [{"code": "def get_prng(seed)\n    from jax import random\n    import numpy as np\n\n    if not isinstance(seed, int):\n        raise TypeError(\"The seed must be an integer.\")\n\n    # Split seed into two 32-bit unsigned integers\n    seed_1 = seed // 2**32\n    seed_2 = seed % 2**32\n\n    # Convert to NumPy array\n    seed_np = np.array([seed_1, seed_2], dtype=np.uint32)\n\n    # Generate PRNG key\n    prng_key = random.PRNGKey(seed_np)\n\n    return prng_key", "CodeBleu_score": 30.812804864545324}, {"code": "def get_prng(seed)\n    import jax.numpy as jnp\n    import jax.random as jrandom\n\n    if isinstance(seed, jnp.ndarray):\n        assert seed.shape == (1,)\n        seed = int(seed)\n\n    seed_key = jrandom.PRNGKey(seed)\n    return seed_key\n\n", "CodeBleu_score": 17.707096135913257}, {"code": "def get_prng(seed)\n    import jax\n    import numpy as np\n\n    # Ensure seed is a scalar\n    seed = np.array(seed)\n    assert seed.shape == (), \"seed must be a scalar\"\n\n    # Split seed into two 32-bit unsigned integers\n    seed_uint32 = seed.astype(np.uint32)\n    seed_uint32_0 = seed_uint32[0]\n    seed_uint32_1 = seed_uint32[1]\n\n    # Concatenate to form PRNG key\n    prng_key = jax.random.PRNGKey(seed_uint32_0, seed_uint32_1)\n\n    return prng_key\n\n", "CodeBleu_score": 29.43105167048099}]}
{"instance_id": "42234dccce9a", "generation_result": [{"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif isinstance(dtype, pd.DatetimeIndex):\n        return s.dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.Timestamp):\n        return s.dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.NumericDtype):\n        return pd.to_datetime(s, unit=dtype.unit).dt.tz_localize(pandas_type.tz)\n    elif isinstance(dtype, pd.DatetimeTZDtype):\n        return pd.to_datetime(s).dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.DatetimeIndex):\n        return pd.to_datetime(s).dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.Timestamp):\n        return pd.to_datetime(s).dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.NumericDtype):\n        return pd.to_datetime(s, unit=dtype.unit).dt.tz_localize(pandas_type.tz)\n    elif isinstance(dtype, pd.DatetimeTZDtype):\n        return pd.to_datetime(s).dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.DatetimeIndex):\n        return pd.to_datetime(s).dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.Timestamp):\n        return pd.to_datetime(s).dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, pd.NumericDtype):\n        return pd.to_datetime(s, unit=dtype.unit).dt.tz_localize(pandas_type.tz)\n    elif isinstance(dtype, pd.DatetimeTZDtype):\n        return pd.to_datetime(s).dt.tz_localize(dtype.tz)\n    elif is", "CodeBleu_score": 44.45531880713111}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif isinstance(dtype, np.dtype) and dtype.kind in (\"M\", \"m\"):\n        return s.dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, np.dtype) and dtype.kind in (\"i\", \"f\"):\n        return pd.to_datetime(s, unit=dtype.str[1:]).dt.tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s, utc=True).dt.tz_localize(dtype.tz)\n\n", "CodeBleu_score": 60.61890778826111}, {"code": "def convert_Timestamp(cls, s, dtype, pandas_type)\n    if isinstance(dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.tz)\n    elif isinstance(dtype, np.dtype) and dtype.kind == 'M':\n        return s.dt.tz_localize(dtype.tz)\n    elif isinstance(dtype, np.dtype) and dtype.kind in ['i', 'f']:\n        return pd.to_datetime(s, utc=True).dt.tz_localize(dtype.tz)\n    else:\n        return pd.to_datetime(s, utc=True).dt.tz_localize(dtype.tz)\n\n", "CodeBleu_score": 61.020296361790486}]}
{"instance_id": "9cee7388879b", "generation_result": [{"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if not isinstance(arg, dd.Series):\n        raise ValueError(\n            \"The argument of a time-based operation must be a Dask Series.\"\n        )\n    if not isinstance(arg.dtype, pd.DatetimeTZDtype):\n        raise ValueError(\n            \"The argument of a time-based operation must be a time-zone aware column.\"\n        )\n    if not arg.dtype.tz.zone == \"UTC\":\n        arg = arg.dt.tz_convert(\"UTC\")\n    indexer = (arg >= lower_bound) & (arg <= upper_bound)\n    return dd.from_array(indexer)", "CodeBleu_score": 30.354871596351618}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if not isinstance(arg, dd.Series):\n        raise ValueError(\n            \"BetweenTime operation only supported on Dask Series, got %s\" % type(arg)\n        )\n    if not isinstance(lower_bound, datetime.time) or not isinstance(\n        upper_bound, datetime.time\n    ):\n        raise ValueError(\n            \"BetweenTime operation only supported on time objects, got %s and %s\"\n            % (type(lower_bound), type(upper_bound))\n        )\n\n    if not arg.dtype.tz:\n        arg = arg.dt.tz_localize(\"UTC\")\n\n    lower_bound = pd.Timestamp(lower_bound, tz=\"UTC\")\n    upper_bound = pd.Timestamp(upper_bound, tz=\"UTC\")\n\n    indexer = (arg.dt.time >= lower_bound) & (arg.dt.time <= upper_bound)\n    return dd.from_array(indexer, chunksize=arg.npartitions)", "CodeBleu_score": 29.515606127353045}, {"code": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)\n    if not isinstance(arg, dd.Series):\n        raise ValueError(\"Operation not supported on type %s\" % type(arg))\n\n    if not isinstance(arg.dtype, pd.DatetimeTZDtype):\n        raise ValueError(\"Operation not supported on type %s\" % type(arg))\n\n    # Convert timezone-naive data to UTC\n    if arg.dtype.tzinfo is None or arg.dtype.tzinfo.utcoffset(arg.dtype.tzinfo) is None:\n        arg = arg.dt.tz_localize(\"UTC\")\n\n    # Convert the time range to UTC\n    lower_bound = lower_bound.tz_convert(\"UTC\")\n    upper_bound = upper_bound.tz_convert(\"UTC\")\n\n    # Create a boolean indexer\n    indexer = (arg >= lower_bound) & (arg <= upper_bound)\n\n    # Convert the indexer to a Dask DataFrame\n    return dd.from_array(indexer, arg.npartitions)\n\n", "CodeBleu_score": 32.549125216849404}]}
{"instance_id": "0ab1cc8ccf71", "generation_result": [{"code": "def asseries(cls, value, like=None)\n\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.Scalar):\n        return dd.from_array(value.compute())\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, (list, tuple, dict)):\n        return dd.from_pandas(pd.Series(value), npartitions=1)\n    elif like is not None:\n        if isinstance(like, dd.Series):\n            return dd.from_pandas(pd.Series(value), npartitions=like.npartitions)\n        elif isinstance(like, dd.DataFrame):\n            return dd.from_pandas(pd.Series(value), npartitions=like.npartitions)\n        elif isinstance(like, pd.Series):\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n        elif isinstance(like, pd.DataFrame):\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n        else:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n    else:\n        return dd.from_pandas(pd.Series(value), npartitions=1)", "CodeBleu_score": 39.19351989627495}, {"code": "def asseries(cls, value, like=None)\n    import pandas as pd\n    import dask.dataframe as dd\n    import dask.array as da\n\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, da.Array):\n        return dd.from_array(value)\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif isinstance(value, (list, tuple)):\n        if like is not None:\n            return dd.from_pandas(pd.Series(value, index=like.index), npartitions=1)\n        else:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n    elif isinstance(value, dict):\n        if like is not None:\n            return dd.from_pandas(pd.Series(value, index=like.index), npartitions=1)\n        else:\n            return dd.from_pandas(pd.Series(value), npartitions=1)\n    else:\n        return dd.from_pandas(pd.Series([value]), npartitions=1)", "CodeBleu_score": 41.729696890391345}, {"code": "def asseries(cls, value, like=None)\n    if isinstance(value, dict):\n        if like is not None:\n            like = asseries(cls, like)\n            return cls.from_delayed([dask.delayed(value.get)(k) for k in like.index], index=like.index)\n        else:\n            return cls.from_delayed([dask.delayed(value.get)(k) for k in value.keys()], index=value.keys())\n    elif isinstance(value, list):\n        if like is not None:\n            like = asseries(cls, like)\n            return cls.from_delayed([dask.delayed(value.__getitem__)(i) for i in range(len(like))], index=like.index)\n        else:\n            return cls.from_delayed([dask.delayed(value.__getitem__)(i) for i in range(len(value))])\n    elif isinstance(value, tuple):\n        if like is not None:\n            like = asseries(cls, like)\n            return cls.from_delayed([dask.delayed(value.__getitem__)(i) for i in range(len(like))], index=like.index)\n        else:\n            return cls.from_delayed([dask.delayed(value.__getitem__)(i) for i in range(len(value))])\n    elif isinstance(value, (int, float)):\n        if like is not None:\n            like = asseries(cls, like)\n            return cls.from_delayed([dask.delayed(lambda: value)() for i in range(len(like))], index=like.index)\n        else:\n            return cls.from_delayed([dask.delayed(lambda: value)()])\n    elif isinstance(value, cls):\n        return value\n    elif isinstance(value, dask.dataframe.Series):\n        return cls.from_delayed([dask.delayed(value.__getitem__)(i) for i in range(len(value))], index=value.index)\n    elif isinstance(value, dask.array.Array):\n        if like", "CodeBleu_score": 33.07680361838926}]}
{"instance_id": "926220a06a0c", "generation_result": [{"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n\n    if dictionary is None:\n        dictionary = {}\n\n    for key, value in dictionary.items():\n        if not isinstance(value, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(f\"{key} is not a valid DataFrame\")\n\n    super().connect(dictionary)\n\n", "CodeBleu_score": 60.4701911700833}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    for key, value in dictionary.items():\n        if not isinstance(value, (pd.DataFrame, dd.DataFrame)):\n            raise TypeError(\n                f\"{key} is not a valid DataFrame. Please check your dictionary.\"\n            )\n    super().do_connect(dictionary)", "CodeBleu_score": 56.70351151019023}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None\n    if dictionary is None:\n        dictionary = {}\n    for key, value in dictionary.items():\n        if not isinstance(value, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"The value for key {key} is not a valid DataFrame. Please provide a valid DataFrame.\"\n            )\n    super().do_connect(dictionary)\n\n", "CodeBleu_score": 67.44122740097126}]}
{"instance_id": "95afc38c9125", "generation_result": [{"code": "def get_estimator(library_name: str, estimator_name: str)\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"{estimator_name} could not be found in {library_name}\")\n    estimator_class = classes[estimator_name]\n    if len(classes) > 1:\n        logger.debug(f\"Multiple classes found in {library_name}: {list(classes.keys())}\")\n        logger.warning(f\"Multiple classes found in {library_name}, using {estimator_name}\")\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(f\"{estimator_name} is not a subclass of sklearn's BaseEstimator\")\n    return estimator_class\n\n", "CodeBleu_score": 33.03902654425922}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f'{estimator_name} could not be found in {library_name}')\n    estimator_class = classes[estimator_name]\n    matching_classes = [k for k, v in classes.items() if v == estimator_class]\n    if len(matching_classes) > 1:\n        logger.debug(f'Multiple estimators found: {matching_classes}')\n        logger.warning(f'Multiple estimators found. Using {estimator_name}')\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(f'{estimator_name} is not a subclass of BaseEstimator')\n    return estimator_class\n\n", "CodeBleu_score": 39.493373805267815}, {"code": "def get_estimator(library_name: str, estimator_name: str)\n    classes = get_module_members(library_name)\n    if estimator_name not in classes:\n        raise ValueError(f\"Could not find {estimator_name} in {library_name}.\")\n    if len(classes[estimator_name]) > 1:\n        logger.debug(f\"Found multiple classes matching {estimator_name}: {classes[estimator_name]}.\")\n        logger.warning(f\"Multiple classes found for {estimator_name} in {library_name}. Using first class found.\")\n    estimator_class = classes[estimator_name][0]\n    if not issubclass(estimator_class, BaseEstimator):\n        logger.info(f\"{estimator_class} is not a subclass of BaseEstimator.\")\n    return estimator_class", "CodeBleu_score": 39.47539839813757}]}
{"instance_id": "9562a9cc043b", "generation_result": [{"code": "def estimator_to_task(estimator_name: str) -> str\n    import json\n    import os\n\n    with open(os.path.join(os.path.dirname(__file__), \"estimator_task_mapping.json\"), \"r\") as f:\n        mapping = json.load(f)\n\n    for postfix, task in mapping.items():\n        if estimator_name.endswith(postfix):\n            return task\n\n    return \"unknown\"", "CodeBleu_score": 44.39885489605647}, {"code": "def estimator_to_task(estimator_name: str) -> str\n    import json\n    import os\n    from pathlib import Path\n    from typing import Any, Dict\n\n    # load mapping\n    mapping_path = Path(os.path.dirname(__file__)) / \"mapping.json\"\n    with open(mapping_path, \"r\") as mapping_file:\n        mapping: Dict[str, Any] = json.load(mapping_file)\n\n    # get task\n    task = \"unknown\"\n    for key in mapping:\n        if estimator_name.endswith(key):\n            task = mapping[key]\n            break\n\n    return task", "CodeBleu_score": 39.35044643431477}, {"code": "def estimator_to_task(estimator_name: str) -> str\n    import json\n\n    with open(\"task_mapping.json\", \"r\") as f:\n        task_mapping = json.load(f)\n\n    for key in task_mapping:\n        if estimator_name.endswith(key):\n            return task_mapping[key]\n\n    return \"unknown\"", "CodeBleu_score": 27.971177695379374}]}
{"instance_id": "2f50195d043a", "generation_result": [{"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n    import os\n    import json\n    from .load_data_from_file import load_data_from_file\n\n    data_dict = {}\n    for file in os.listdir(data_cache):\n        if file.startswith(data_name) and file.endswith(\".json\"):\n            continue\n        component = file.split(\".\")[0]\n        file_extension = file.split(\".\")[1]\n        data_dict[component] = load_data_from_file(\n            os.path.join(data_cache, file), file_extension\n        )\n    return data_dict", "CodeBleu_score": 40.61703557698128}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n    import os\n    import json\n    from .load_data_from_file import load_data_from_file\n\n    data_cache = os.path.abspath(data_cache)\n    data = {}\n    for filename in os.listdir(data_cache):\n        if filename.startswith(data_name) and filename.endswith(\".json\"):\n            continue\n        component = filename.split(\".\")[0].split(\"_\")[1]\n        file_extension = filename.split(\".\")[1]\n        data[component] = load_data_from_file(\n            os.path.join(data_cache, filename), file_extension\n        )\n    return data", "CodeBleu_score": 41.07892645553635}, {"code": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict\n    import os\n    import json\n    from .load_data_from_file import load_data_from_file\n\n    data_dict = {}\n    for file_name in os.listdir(data_cache):\n        if file_name.startswith(data_name):\n            if file_name.endswith(\".json\"):\n                with open(os.path.join(data_cache, file_name), \"r\") as f:\n                    data_dict[file_name.split(\".\")[0]] = json.load(f)\n            else:\n                component = file_name.split(\".\")[0].split(\"_\")[1]\n                file_extension = file_name.split(\".\")[1]\n                data_dict[component] = load_data_from_file(\n                    os.path.join(data_cache, file_name), file_extension\n                )\n    return data_dict", "CodeBleu_score": 36.90491196654911}]}
{"instance_id": "e0281206029e", "generation_result": [{"code": "def retrieve(url: str, filename: str) -> None\n    import os\n    import requests\n\n    if os.path.isfile(filename):\n        return\n\n    if url.startswith(\"http\"):\n        response = requests.get(url)\n        assert response.status_code == 200, f\"Download failed with status code {response.status_code}\"\n\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024\n        written_size = 0\n\n        with open(filename, \"wb\") as f:\n            for data in response.iter_content(block_size):\n                written_size += len(data)\n                f.write(data)\n\n        assert written_size == total_size, f\"Written data size {written_size} does not match expected content length {total_size}\"\n\n", "CodeBleu_score": 53.00443468200914}, {"code": "def retrieve(url: str, filename: str) -> None\n    import os\n    import requests\n    import sys\n\n    if os.path.isfile(filename):\n        return\n\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        assert response.status_code == 200, \"Download failed: \" + str(response.status_code)\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024\n        wrote = 0\n        with open(filename, \"wb\") as file:\n            for data in response.iter_content(block_size):\n                wrote = wrote + len(data)\n                file.write(data)\n        assert wrote == total_size, \"Download size mismatch\"\n    else:\n        sys.exit(\"URL must start with http\")", "CodeBleu_score": 54.9451125386685}, {"code": "def retrieve(url: str, filename: str) -> None\n    import os\n    import requests\n    if os.path.isfile(filename):\n        return\n    if not url.startswith(\"http\"):\n        raise ValueError(\"URL must start with 'http'\")\n    response = requests.get(url)\n    assert response.status_code == 200, \"Request failed, status code: \" + str(response.status_code)\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024\n    wrote = 0\n    with open(filename, \"wb\") as f:\n        for data in response.iter_content(block_size):\n            wrote = wrote + len(data)\n            f.write(data)\n    assert total_size == wrote, \"Total size does not match the downloaded size\"", "CodeBleu_score": 50.21709777671881}]}
{"instance_id": "a69a71cc2fe8", "generation_result": [{"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\n    import os\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelEncoder\n\n    # Load the dataset.\n    data_path = os.path.join(raw_data_cache, data_name)\n    data_df = pd.read_csv(data_path, header=None)\n\n    # Split the dataset into features and labels.\n    features = data_df.iloc[:, :-1].values\n    labels = data_df.iloc[:, -1].values\n\n    # Adjust the labels to be zero-indexed.\n    le = LabelEncoder()\n    labels = le.fit_transform(labels)\n\n    # Convert the labels to binary if the parameter is set.\n    if dataset_params[\"binary\"]:\n        labels = (labels > 0).astype(int)\n\n    # Split the dataset into training and testing sets.\n    features_train, features_test, labels_train, labels_test = train_test_split(\n        features, labels, test_size=dataset_params[\"test_size\"], random_state=0\n    )\n\n    # Create a description of the dataset.\n    data_description = {\n        \"n_classes\": len(le.classes_),\n        \"train_samples\": features_train.shape[0],\n        \"test_samples\": features_test.shape[0],\n    }\n\n    # Create a dictionary of the data.\n    data = {\n        \"train_features\": features_train,\n        \"test_features\": features_test,\n        \"train_labels\": labels_train,\n        \"test_labels\": labels_test,\n    }\n\n    return data, data_description", "CodeBleu_score": 36.899306977137826}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\n    # Load the dataset\n    dataset = pd.read_csv(\n        os.path.join(raw_data_cache, data_name, \"covtype.data\"),\n        header=None,\n        names=[\n            \"Elevation\",\n            \"Aspect\",\n            \"Slope\",\n            \"Horizontal_Distance_To_Hydrology\",\n            \"Vertical_Distance_To_Hydrology\",\n            \"Horizontal_Distance_To_Roadways\",\n            \"Hillshade_9am\",\n            \"Hillshade_Noon\",\n            \"Hillshade_3pm\",\n            \"Horizontal_Distance_To_Fire_Points\",\n            \"Wilderness_Area\",\n            \"Soil_Type\",\n            \"Cover_Type\",\n        ],\n    )\n\n    # Process the target labels\n    if dataset_params[\"binary\"]:\n        dataset[\"Cover_Type\"] = dataset[\"Cover_Type\"].apply(\n            lambda x: 1 if x in [2, 3, 4, 5, 6, 7] else 0\n        )\n    dataset[\"Cover_Type\"] = dataset[\"Cover_Type\"].astype(int)\n    dataset[\"Cover_Type\"] -= 1\n\n    # Create a description dictionary\n    desc = dict()\n    desc[\"n_classes\"] = dataset[\"Cover_Type\"].nunique()\n    desc[\"split_params\"] = {\n        \"split_perc\": [0.6, 0.2, 0.2],\n        \"split_method\": [\"random\", \"random\", \"random\"],\n        \"split_seed\": 42,\n    }\n\n    # Save the dataset\n    dataset.to_csv(os.path.join(data_cache, data_name, \"covtype.data\"), index=False)\n\n    return dataset, desc\n\n", "CodeBleu_score": 31.07687399179658}, {"code": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]\n\n    import os\n    import numpy as np\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    # load dataset\n    data_path = os.path.join(raw_data_cache, data_name + \".data\")\n    data = pd.read_csv(data_path, header=None)\n\n    # process labels\n    labels = data[540].values\n    labels = np.where(labels == 2, 1, labels)\n    labels = np.where(labels == 4, 2, labels)\n    labels = np.where(labels == 5, 3, labels)\n    labels = np.where(labels == 6, 4, labels)\n    labels = np.where(labels == 7, 5, labels)\n    labels = np.where(labels == 8, 6, labels)\n    labels = np.where(labels == 9, 7, labels)\n    labels = np.where(labels == 10, 8, labels)\n    labels = np.where(labels == 11, 9, labels)\n    labels = np.where(labels == 12, 10, labels)\n    labels = np.where(labels == 13, 11, labels)\n    labels = np.where(labels == 14, 12, labels)\n    labels = np.where(labels == 15, 13, labels)\n    labels = np.where(labels == 16, 14, labels)\n    labels = np.where(labels == 17, 15, labels)\n    labels = np.where(labels == 18, 16, labels)\n    labels = np.where(labels == 19, 17, labels)\n    labels = np.where(labels == 20, 18, labels)\n    labels = np.where(labels == 21, 19, labels)\n    labels = np.where(labels == 22, 20, labels)\n    labels = np.where(labels == 23, 21, labels)\n    labels = np.where(labels ==", "CodeBleu_score": 18.87340940263715}]}
{"instance_id": "129cbd2529ef", "generation_result": [{"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    output_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            output_list += flatten_list(element, ensure_type_homogeneity)\n        else:\n            output_list.append(element)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 44.06448208015372}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    output_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            output_list += flatten_list(element, ensure_type_homogeneity)\n        else:\n            output_list.append(element)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n\n", "CodeBleu_score": 44.06448208015372}, {"code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List\n    output_list = []\n    for element in input_list:\n        if isinstance(element, list):\n            output_list.extend(flatten_list(element, ensure_type_homogeneity))\n        else:\n            output_list.append(element)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list\n", "CodeBleu_score": 41.79911451455421}]}
{"instance_id": "2d38c2c60b59", "generation_result": [{"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    if eps is None:\n        if np.any(np.logical_or(x < 0, x > 1)):\n            x = np.where(np.logical_or(x < 0, x > 1), np.nan, x)\n    else:\n        x = np.clip(x, eps, 1 - eps)\n    if complex_mode == \"split\":\n        if out is None:\n            out = np.empty(x.shape, dtype=np.float64)\n        np.log(x, out)\n        np.log(1 - x, out)\n        out -= np.log(1 + x, out)\n        return out\n    elif complex_mode == \"magnitude\":\n        return np.log(x / (1 - x))\n    elif complex_mode == \"jax\":\n        return np.log(x / (1 - x))\n    else:\n        raise ValueError(\n            \"complex_mode must be one of 'split', 'magnitude' or 'jax'.\"\n        )", "CodeBleu_score": 55.176925733158996}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    if out is None:\n        out = np.empty_like(x)\n    if eps is None:\n        out = np.log(x / (1 - x))\n    else:\n        out = np.log((x / (1 - x)) / (1 - 2 * eps))\n    return out", "CodeBleu_score": 39.91002367990477}, {"code": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)\n    return _mx_nd_np.logit(x, eps, complex_mode, out)\n\n", "CodeBleu_score": 17.36383642161105}]}
{"instance_id": "205e296f8621", "generation_result": [{"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    ...\n\n", "CodeBleu_score": 18.229166671873013}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n\n    if out is None:\n        out = np.zeros_like(x)\n    else:\n        if out.dtype != x.dtype:\n            raise TypeError(\n                \"Input and output arrays must have the same data type.\"\n            )\n        if out.shape != x.shape:\n            raise ValueError(\n                \"Input and output arrays must have the same shape.\"\n            )\n\n    if lambd < 0:\n        raise ValueError(\n            \"Threshold value must be greater than or equal to zero.\"\n        )\n\n    out[:] = x\n    out[np.abs(x) <= lambd] = 0\n\n    return out", "CodeBleu_score": 30.471435655200974}, {"code": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if not isinstance(x, np.ndarray):\n        raise TypeError(\n            \"expected x to be of type numpy.ndarray, but got \" + type(x).__name__\n        )\n    if not isinstance(lambd, (float, int)):\n        raise TypeError(\n            \"expected lambd to be of type float or int, but got \" + type(lambd).__name__\n        )\n    if not isinstance(out, (np.ndarray, type(None))):\n        raise TypeError(\n            \"expected out to be of type numpy.ndarray or None, but got \"\n            + type(out).__name__\n        )\n    if out is not None and not isinstance(out, np.ndarray):\n        raise TypeError(\n            \"expected out to be of type numpy.ndarray or None, but got \"\n            + type(out).__name__\n        )\n    if not isinstance(x.dtype, np.inexact):\n        raise TypeError(\n            \"expected x to be of floating-point data type, but got \" + str(x.dtype)\n        )\n    if not isinstance(out.dtype, np.inexact):\n        raise TypeError(\n            \"expected out to be of floating-point data type, but got \" + str(out.dtype)\n        )\n    if x.ndim <= 0:\n        raise ValueError(\"expected x to have a non-zero number of dimensions\")\n    if out is not None and x.shape != out.shape:\n        raise ValueError(\"expected x and out to have the same shape\")\n    if out is None:\n        out = np.copy(x)\n    else:\n        out = out.reshape(x.shape)\n    out[np.abs(x) <= lambd] = 0.0\n    return out", "CodeBleu_score": 38.291637370079144}]}
{"instance_id": "5b133f6ebe2d", "generation_result": [{"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        out = out.astype(dtype)\n\n    for i in range(window_length):\n        out[i] = (1 - np.cos(2 * np.pi * i / window_length)) ** 0.5\n\n    return out", "CodeBleu_score": 38.5550159823003}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        assert out.shape[0] == window_length\n        assert out.dtype == dtype\n    window = []\n    for i in range(window_length):\n        window.append(\n            np.round(\n                np.sin(np.pi * np.sin(np.pi * 2 * i / window_length) / 2)\n                * np.sin(np.pi * 2 * i / window_length)\n            )\n        )\n    return np.array(window, dtype=dtype)", "CodeBleu_score": 38.09500576133478}, {"code": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n    else:\n        assert out.dtype == dtype\n        assert out.shape[0] == window_length\n    for i in range(window_length):\n        out[i] = np.sin(np.pi * np.sin(np.pi * i / window_length) / 2)\n    return out", "CodeBleu_score": 37.67301234202428}]}
{"instance_id": "473296f771a9", "generation_result": [{"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    # Set the precision mode to double\n    np.seterr(all='raise')\n    # Promote the data types of the coefficients and inputs to a common type\n    promoted_type = np.promote_types(coeffs.dtype, x.dtype)\n    # Cast the coefficients and inputs to the promoted data type\n    coeffs = coeffs.astype(promoted_type)\n    x = x.astype(promoted_type)\n    # Compute the polynomial values\n    result = np.polyval(coeffs, x)\n    # Cast the result to the promoted data type\n    result = result.astype(promoted_type)\n    # Return the result\n    return result", "CodeBleu_score": 40.289505459904085}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    # Set the precision mode to 100 digits\n    np.seterr(all=\"raise\")\n\n    # Promote the data types of the coefficients and inputs to a common type\n    promoted_type = np.result_type(coeffs, x)\n    coeffs = coeffs.astype(promoted_type)\n    x = x.astype(promoted_type)\n\n    # Compute the polynomial values using numpy's polyval\n    y = np.polyval(coeffs, x)\n\n    # Convert the result to an array with the promoted data type\n    y = np.array(y, dtype=promoted_type)\n\n    return y", "CodeBleu_score": 33.021365868257014}, {"code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray\n    # Set precision mode\n    np.seterr(all=\"raise\")\n\n    # Promote data types of coefficients and inputs\n    promoted_type = np.promote_types(coeffs.dtype, x.dtype)\n    coeffs = coeffs.astype(promoted_type)\n    x = x.astype(promoted_type)\n\n    # Compute polynomial values\n    result = np.polyval(coeffs, x)\n\n    # Return result\n    return result", "CodeBleu_score": 37.35914545784672}]}
{"instance_id": "b8455221cb2a", "generation_result": [{"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    if axis is None:\n        axis = tuple(range(x.ndim))\n    if out is None:\n        out = np.amax(x, axis=axis, keepdims=keepdims)\n    if out.shape == ():\n        out = np.array(out)\n    return out\n\n", "CodeBleu_score": 57.13456310367398}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\n    if axis is not None:\n        axis = tuple(axis) if isinstance(axis, list) else axis\n    if out is not None:\n        out = np.amax(x, axis=axis, keepdims=keepdims, out=out)\n    else:\n        out = np.amax(x, axis=axis, keepdims=keepdims)\n    return out if out.shape != () else np.array(out)", "CodeBleu_score": 63.93906166088068}, {"code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    ...\n\n", "CodeBleu_score": 20.821678330262134}]}
{"instance_id": "f72489f434a3", "generation_result": [{"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return np.copysign(\n        np.array(x1, dtype=np.promote_types(type(x1), type(x2))),\n        np.array(x2, dtype=np.promote_types(type(x1), type(x2))),\n        out=out,\n    )\n\n", "CodeBleu_score": 32.72517342865376}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    ...\n\n", "CodeBleu_score": 16.203703703704903}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    ...\n\n", "CodeBleu_score": 16.203703703704903}]}
{"instance_id": "55c0d77e4dc2", "generation_result": [{"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n\n    # convert axis to tuple\n    if axis is not None:\n        axis = tuple(axis)\n\n    # count non-zero elements\n    count = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n\n    # handle scalar results\n    if count.shape == ():\n        if dtype is None:\n            dtype = np.int64\n        count = np.array(count, dtype=dtype)\n\n    return count", "CodeBleu_score": 60.52734983950523}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if isinstance(a, (list, tuple)):\n        a = np.array(a)\n    if dtype is None:\n        dtype = np.int64\n    if out is None:\n        out = np.empty(shape=a.shape, dtype=dtype)\n    if isinstance(axis, int):\n        axis = (axis,)\n    if axis is None:\n        out = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype)\n        if isinstance(out, int):\n            out = np.array(out, dtype=dtype)\n        return out\n    else:\n        out = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype)\n        if isinstance(out, int):\n            out = np.array(out, dtype=dtype)\n        return out", "CodeBleu_score": 51.51195251552981}, {"code": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if axis is not None:\n        axis = (axis,) if isinstance(axis, int) else tuple(axis)\n    if dtype is not None:\n        dtype = np.dtype(dtype)\n    if out is not None:\n        out = np.array(out)\n    result = np.count_nonzero(a, axis=axis, keepdims=keepdims, dtype=dtype, out=out)\n    if isinstance(result, np.ndarray) and result.ndim == 0:\n        result = np.array(result.item(), dtype=dtype)\n    return result\n\n", "CodeBleu_score": 54.64020683316541}]}
{"instance_id": "784bf7d24fc2", "generation_result": [{"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n\n    if isinstance(spacing, int):\n        return np.gradient(x, spacing, axis, edge_order)\n    else:\n        return [\n            np.gradient(x, spacing[i], axis, edge_order) for i in range(len(spacing))\n        ]", "CodeBleu_score": 54.38556139662206}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    if isinstance(spacing, (list, tuple)):\n        return [\n            gradient(x, spacing=sp, axis=ax, edge_order=edge_order)\n            for sp, ax in zip(spacing, axis)\n        ]\n    return np.gradient(x, spacing, axis, edge_order)\n\n", "CodeBleu_score": 57.85960745665941}, {"code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]\n    if isinstance(spacing, int):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    else:\n        return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)", "CodeBleu_score": 71.0574149881516}]}
{"instance_id": "efa0a51af6bc", "generation_result": [{"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if x.size == 0:\n        return x\n    if x.size != y.size:\n        raise ValueError(\"x and y must have the same size\")\n    x, y = promote_types_of_inputs(x, y)\n    if out is not None:\n        if out.size != x.size:\n            raise ValueError(\"out must have the same size as x and y\")\n        if out.dtype != x.dtype:\n            raise ValueError(\"out must have the same dtype as x and y\")\n    if x.dtype == np.float32:\n        return _xlogy_float32(x, y, out=out)\n    elif x.dtype == np.float64:\n        return _xlogy_float64(x, y, out=out)\n    else:\n        raise TypeError(\"x and y must have dtype float32 or float64\")\n\n", "CodeBleu_score": 48.099587202626495}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    x, y = np.broadcast_arrays(x, y)\n    if out is None:\n        out = np.empty_like(x)\n    if x.dtype.kind == \"f\" or y.dtype.kind == \"f\":\n        np.multiply(np.log(y, out), x, out)\n    else:\n        np.multiply(np.log(y, out), x, out, where=y != 0)\n    return out", "CodeBleu_score": 44.086210216179836}, {"code": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray\n    if out is None:\n        out = np.empty_like(x)\n    if x.dtype.kind in \"iu\":\n        x = x.astype(np.float64)\n    if y.dtype.kind in \"iu\":\n        y = y.astype(np.float64)\n    if x.dtype.kind in \"f\" and y.dtype.kind in \"f\":\n        if x.dtype.itemsize > y.dtype.itemsize:\n            y = y.astype(x.dtype)\n        elif x.dtype.itemsize < y.dtype.itemsize:\n            x = x.astype(y.dtype)\n        x, y = np.broadcast_arrays(x, y)\n        out = x * np.log(y, out=out)\n        out[np.isnan(out)] = 0.0\n        return out\n    else:\n        raise TypeError(\"xlogy() only supports floating point inputs\")\n\n", "CodeBleu_score": 40.28910095787302}]}
{"instance_id": "ac1645284260", "generation_result": [{"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\n    if data_format == \"channels_first\":\n        x = tf.transpose(x, [0, 2, 3, 1])\n        data_format = \"channels_last\"\n\n    kernel, strides, need_depth_pooling = _determine_depth_pooling_size(\n        x.shape, kernel, strides, dims\n    )\n\n    if need_depth_pooling:\n        x = tf.transpose(x, [0, 3, 1, 2])\n\n    return x, kernel, strides, need_depth_pooling\n\n", "CodeBleu_score": 42.453752556921806}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n    # get the updated kernel size, strides, and flag indicating if depth pooling is needed\n    kernel, strides, depth_pooling = _get_updated_pooling_params(x, kernel, strides, dims, data_format)\n    # transpose the tensor if depth pooling is needed\n    if depth_pooling:\n        x = tf.transpose(x, [0, 3, 1, 2])\n    return x, kernel, strides, depth_pooling\n", "CodeBleu_score": 36.09887499683981}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")\n\n    # Get the updated kernel size, strides, and depth pooling flag\n    kernel_size, strides, depth_pooling = _determine_depth_pooling_size(\n        kernel, strides, dims, data_format\n    )\n\n    # If depth pooling is required, transpose the input tensor\n    if depth_pooling:\n        x = tf.transpose(x, [0, 3, 1, 2])\n\n    return x, kernel_size, strides, depth_pooling\n\n", "CodeBleu_score": 35.118264658640705}]}
{"instance_id": "18ff0a4f7550", "generation_result": [{"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if not training:\n        return x\n    if data_format == \"NCW\":\n        x = x.transpose(0, 2, 1)\n    if out is None:\n        out = np.empty_like(x)\n    if x.ndim == 2:\n        n, w = x.shape\n        mask = np.random.binomial(1, prob, (n, w))\n    elif x.ndim == 3:\n        n, c, w = x.shape\n        mask = np.random.binomial(1, prob, (n, c, w))\n    else:\n        raise ValueError(\"x must be a 2D or 3D array\")\n    out = x * mask / (1 - prob)\n    if data_format == \"NCW\":\n        out = out.transpose(0, 2, 1)\n    return out\n\n", "CodeBleu_score": 46.63353720310828}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    ...\n\n", "CodeBleu_score": 13.144132653061224}, {"code": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    ...\n\n", "CodeBleu_score": 13.144132653061224}]}
{"instance_id": "0ce45eb7cbd9", "generation_result": [{"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    ...\n\n", "CodeBleu_score": 14.79044834307992}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n\n    x = np.asarray(x)\n    if axis is None:\n        axis = 0\n    if axis < 0:\n        axis = x.ndim + axis\n\n    if largest:\n        order = np.argpartition(x, -k, axis=axis)[..., -k:]\n        indices = np.take_along_axis(np.argsort(-x, axis=axis), order, axis=axis)\n        values = np.take_along_axis(x, order, axis=axis)\n    else:\n        order = np.argpartition(x, k, axis=axis)[..., :k]\n        indices = np.take_along_axis(np.argsort(x, axis=axis), order, axis=axis)\n        values = np.take_along_axis(x, order, axis=axis)\n\n    if sorted:\n        indices = np.take_along_axis(indices, np.argsort(values, axis=axis), axis=axis)\n        values = np.take_along_axis(values, np.argsort(values, axis=axis), axis=axis)\n\n    if out is None:\n        return values, indices\n    else:\n        out[0][...] = values\n        out[1][...] = indices\n        return out\n\n", "CodeBleu_score": 43.83119996421799}, {"code": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]\n    if largest:\n        return np.argpartition(x, -k, axis=axis)[..., -k:], np.partition(\n            x, -k, axis=axis\n        )[..., -k:]\n    else:\n        return np.argpartition(x, k, axis=axis)[..., :k], np.partition(\n            x, k, axis=axis\n        )[..., :k]", "CodeBleu_score": 26.170608617806813}]}
{"instance_id": "1424f9541998", "generation_result": [{"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if axis is None:\n        norm = np.linalg.norm(x.reshape(-1), 1)\n        denormalization = np.ones_like(x) * norm\n    else:\n        norm = np.linalg.norm(x, 1, axis=axis)\n        denormalization = np.ones_like(x)\n        denormalization[axis] = norm\n    if out is None:\n        return np.divide(x, denormalization + 1e-8, out=out)\n    else:\n        np.divide(x, denormalization + 1e-8, out=out)\n        return out\n\n", "CodeBleu_score": 46.75144402636387}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if axis is None:\n        norm = np.sum(np.abs(x.reshape(-1)))\n        denorm = np.full(x.shape, norm)\n    else:\n        norm = np.sum(np.abs(x), axis=axis, keepdims=True)\n        denorm = np.abs(x) + np.finfo(x.dtype).eps\n    if out is None:\n        return x / denorm\n    else:\n        np.divide(x, denorm, out=out)\n        return out\n\n", "CodeBleu_score": 62.198310892969346}, {"code": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if axis is None:\n        norm = np.linalg.norm(x.reshape(-1), ord=1)\n        denorm = np.full(x.shape, norm)\n    else:\n        norm = np.linalg.norm(x, ord=1, axis=axis)\n        denorm = np.divide(norm, np.abs(x) + 1e-10)\n    if out is not None:\n        out[:] = np.divide(x, denorm)\n        return out\n    return np.divide(x, denorm)\n\n", "CodeBleu_score": 60.780588127151944}]}
{"instance_id": "4c3a386a1f05", "generation_result": [{"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    return ivy.array(\n        ivy.functional.backends.numpy.random.beta(\n            alpha,\n            beta,\n            shape=shape,\n            device=device,\n            dtype=dtype,\n            seed=seed,\n            out=out,\n        )\n    )\n\n", "CodeBleu_score": 46.22900775983829}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if ivy.exists(out):\n        raise Exception(\n            \"beta does not support out argument yet, please \"\n            \"consider using backend.random.beta directly\"\n        )\n    return ivy.backend_handler.handle_random_backend(\n        \"beta\",\n        alpha,\n        beta,\n        shape=shape,\n        device=device,\n        dtype=dtype,\n        seed=seed,\n        out=out,\n    )\n\n", "CodeBleu_score": 44.97539531203298}, {"code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\n    if seed is None:\n        seed = np.random.randint(0, 2 ** 32 - 1)\n    np.random.seed(seed)\n    if shape is None:\n        shape = (1,)\n    if dtype is None:\n        dtype = np.float32\n    if out is None:\n        out = np.empty(shape, dtype=dtype)\n    if device is None:\n        device = _dev_callable(None)\n    if ivy.is_native_array(alpha):\n        alpha = np.array(alpha)\n    if ivy.is_native_array(beta):\n        beta = np.array(beta)\n    out = np.random.beta(alpha, beta, size=shape)\n    out = ivy.array(out, device=device, dtype=dtype)\n    return out\n\n", "CodeBleu_score": 50.09668781171164}]}
{"instance_id": "271200a8b261", "generation_result": [{"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    if not isinstance(x, (np.ndarray, list, tuple)):\n        raise TypeError(\n            f\"x must be either a numpy array, a list, or a tuple, not {type(x)}.\"\n        )\n    if not isinstance(x, np.ndarray):\n        x = np.array(x)\n    sorted_indices = np.argsort(x)\n    inverse = np.zeros(sorted_indices.shape)\n    inverse[sorted_indices] = np.arange(sorted_indices.shape[0])\n    return np.sort(inverse)\n\n", "CodeBleu_score": 55.46464977941595}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    x = np.array(x)\n    sorted_indices = np.argsort(x)\n    inverse = np.zeros_like(sorted_indices)\n    inverse[sorted_indices] = np.arange(len(x))\n    return inverse.astype(int)", "CodeBleu_score": 57.929435223363}, {"code": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray\n    if not isinstance(x, (np.ndarray, list, tuple)):\n        raise TypeError(\"x must be an array-like object\")\n    x = np.array(x)\n    if not x.ndim == 1:\n        raise ValueError(\"x must be one-dimensional\")\n    if not x.size:\n        raise ValueError(\"x cannot be empty\")\n    if not np.issubdtype(x.dtype, np.integer):\n        raise TypeError(\"x must contain integers\")\n    if not np.all(np.diff(x) >= 0):\n        raise ValueError(\"x must be sorted in ascending order\")\n    inverse = np.zeros(x.shape, dtype=np.int64)\n    inverse[x] = np.arange(x.size, dtype=np.int64)\n    return np.sort(inverse)\n\n", "CodeBleu_score": 42.758549366783235}]}
{"instance_id": "040d73b37229", "generation_result": [{"code": "def plot_medical_charts() -> None\n\n    # Import packages\n    from matplotlib import pyplot as plt\n    from matplotlib.ticker import FuncFormatter\n    import pandas as pd\n    import numpy as np\n    import os\n\n    # Set the style of the plotting environment\n    plt.style.use(\"seaborn-whitegrid\")\n\n    # Create a figure with subplots\n    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n\n    # Import the data\n    data = pd.read_csv(\n        os.path.join(\"data\", \"medical_data.csv\"),\n        parse_dates=[\"Date of Vomit\", \"Date of Daycare\", \"Date of Doctor Visit\"],\n    )\n\n    # Create a function to format the x-axis tick labels\n    def date_formatter(x, pos):\n        return pd.to_datetime(x).strftime(\"%b %Y\")\n\n    # Create a function to format the y-axis tick labels\n    def number_formatter(x, pos):\n        return f\"{x:,.0f}\"\n\n    # Plot the total vomit per month\n    axs[0, 0].plot(data[\"Date of Vomit\"], data[\"Vomit Count\"])\n    axs[0, 0].set_xlabel(\"Date\")\n    axs[0, 0].set_ylabel(\"Vomit Count\")\n    axs[0, 0].set_title(\"Total Vomit per Month\")\n    axs[0, 0].xaxis.set_major_formatter(FuncFormatter(date_formatter))\n    axs[0, 0].yaxis.set_major_formatter(FuncFormatter(number_formatter))\n\n    # Plot the days between vomit\n    axs[0, 1].plot(data[\"Date of Vomit\"], data[\"Days Between Vomit\"])\n    axs[0, 1].set_xlabel(\"Date\")\n    axs[0, 1].set_ylabel(\"Days Between Vomit\")\n    axs[0, 1].set_title(\"Days Between Vomit\")", "CodeBleu_score": 24.25338173557122}, {"code": "def plot_medical_charts() -> None\n\n    # Import packages\n    import matplotlib.pyplot as plt\n    import matplotlib.dates as mdates\n    import pandas as pd\n    import numpy as np\n    import os\n\n    # Set plotting style\n    plt.style.use(\"seaborn-white\")\n\n    # Create figure and axes\n    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n\n    # Import data\n    data = pd.read_csv(\"data/medical_data.csv\")\n\n    # Calculate number of days in daycare\n    data[\"Daycare\"] = data[\"Daycare\"].astype(str)\n    data[\"Daycare\"] = data[\"Daycare\"].str.replace(\"-\", \"\")\n    data[\"Daycare\"] = data[\"Daycare\"].astype(int)\n    data[\"Daycare\"] = data[\"Daycare\"].fillna(0)\n\n    # Calculate number of days between vomit\n    data[\"Vomit\"] = pd.to_datetime(data[\"Vomit\"], format=\"%Y-%m-%d\")\n    data[\"Vomit\"] = data[\"Vomit\"].dt.strftime(\"%Y-%m-%d\")\n    data[\"Vomit\"] = data[\"Vomit\"].astype(str)\n    data[\"Vomit\"] = data[\"Vomit\"].str.replace(\"-\", \"\")\n    data[\"Vomit\"] = data[\"Vomit\"].astype(int)\n    data[\"Vomit\"] = data[\"Vomit\"].fillna(0)\n    data[\"Vomit\"] = data[\"Vomit\"].astype(str)\n    data[\"Vomit\"] = data[\"Vomit\"].str.replace(\"-\", \"\")\n    data[\"Vomit\"] = data[\"Vomit\"].astype(int)\n    data[\"Vomit\"] = data[\"Vomit\"].fillna(0)\n    data[\"Vomit\"] = data[\"Vomit\"].astype(str)\n    data[\"Vomit\"] = data[\"Vomit\"].str.replace(\"-\", \"\")\n    data[\"Vomit\"] = data[\"Vomit\"].astype(int)\n    data[\"V", "CodeBleu_score": 25.750137656822215}, {"code": "def plot_medical_charts() -> None\n    import matplotlib.pyplot as plt\n    import pandas as pd\n\n    # Set style\n    plt.style.use(\"seaborn-whitegrid\")\n\n    # Create figure and subplots\n    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n\n    # Plot total vomit per month\n    axs[0, 0].plot(data[\"month\"], data[\"total_vomit\"], marker=\"o\", color=\"orange\")\n    axs[0, 0].set_title(\"Total Vomit Per Month\")\n    axs[0, 0].set_xlabel(\"Month\")\n    axs[0, 0].set_ylabel(\"Total Vomit\")\n\n    # Plot days between vomit\n    axs[0, 1].plot(data[\"days_between_vomit\"], marker=\"o\", color=\"green\")\n    axs[0, 1].set_title(\"Days Between Vomit\")\n    axs[0, 1].set_xlabel(\"Month\")\n    axs[0, 1].set_ylabel(\"Days Between Vomit\")\n\n    # Plot days in daycare\n    axs[1, 0].plot(data[\"days_in_daycare\"], marker=\"o\", color=\"purple\")\n    axs[1, 0].set_title(\"Days in Daycare\")\n    axs[1, 0].set_xlabel(\"Month\")\n    axs[1, 0].set_ylabel(\"Days in Daycare\")\n\n    # Plot doctor visits\n    axs[1, 1].plot(data[\"doctor_visits\"], marker=\"o\", color=\"blue\")\n    axs[1, 1].set_title(\"Doctor Visits\")\n    axs[1, 1].set_xlabel(\"Month\")\n    axs[1, 1].set_ylabel(\"Doctor Visits\")\n\n    # Adjust layout\n    fig.tight_layout()\n\n    # Export figure\n    plt.savefig(\"./output/medical_charts.png\")\n\n", "CodeBleu_score": 28.625354436792534}]}
{"instance_id": "7061f4d0c3fc", "generation_result": [{"code": "def plot_feeding_24h_viz() -> None\n    import pandas as pd\n    import numpy as np\n    from datetime import datetime, timedelta\n    import matplotlib.pyplot as plt\n    from parse_raw_data import parse_raw_data\n    from export_figure import export_figure\n\n    # Import and extract feeding data\n    feeding_data = parse_raw_data()\n\n    # Set up plot\n    plt.style.use('dark_background')\n    fig, ax = plt.subplots()\n    ax.grid(True, linestyle='-', which='both', color='lightgrey', alpha=0.5)\n\n    # Compute offset from birthday\n    offset = feeding_data['solid_feeding_date'].max() - feeding_data['birthday']\n\n    # Create scatter plot of feeding times\n    ax.scatter(\n        feeding_data['bottle_feeding_time'] - offset,\n        feeding_data['bottle_feeding_date'] - feeding_data['birthday'],\n        s=10,\n        c='red',\n        alpha=0.75,\n        label='Bottle feeding'\n    )\n    ax.scatter(\n        feeding_data['solid_feeding_time'] - offset,\n        feeding_data['solid_feeding_date'] - feeding_data['birthday'],\n        s=10,\n        c='blue',\n        alpha=0.75,\n        label='Solid feeding'\n    )\n    ax.legend()\n\n    # Calculate end date for plot\n    end_date = feeding_data['bottle_feeding_date'].max() + timedelta(days=7)\n\n    # Format plot to display feeding data over a week in a 24-hour format\n    ax.set_xlim(0, 168)\n    ax.set_ylim(0, end_date - feeding_data['birthday'])\n    ax.set_xlabel('Hours since birthday')\n    ax.set_ylabel('Days since birthday')\n    ax.set_title('24-hour feeding data", "CodeBleu_score": 27.045873938139998}, {"code": "def plot_feeding_24h_viz() -> None\n\n    # Import and extract data\n    import pandas as pd\n    from parse_raw_data import parse_raw_data\n    from datetime import timedelta\n    from plot_dark_grid import plot_dark_grid\n    from export_figure import export_figure\n\n    # Set up plot\n    plot_dark_grid()\n\n    # Extract data\n    data = parse_raw_data()\n    birthday = data.loc[data[\"Type\"] == \"Birthday\", \"Date\"].iloc[0]\n    last_solid_feeding_date = data.loc[data[\"Type\"] == \"Solid\", \"Date\"].iloc[-1]\n    offset = last_solid_feeding_date - birthday\n    end_date = last_solid_feeding_date + timedelta(days=7)\n\n    # Plot data\n    data.loc[data[\"Type\"] == \"Bottle\", \"Date\"].plot(kind=\"scatter\", x=\"Date\", y=\"Time\", color=\"red\")\n    data.loc[data[\"Type\"] == \"Solid\", \"Date\"].plot(kind=\"scatter\", x=\"Date\", y=\"Time\", color=\"blue\")\n\n    # Format plot\n    plt.xlim(birthday, end_date)\n    plt.ylim(0, 24)\n    plt.xticks(pd.date_range(birthday, end_date, freq=\"D\"))\n    plt.yticks(range(25))\n    plt.legend([\"Bottle\", \"Solid\"])\n\n    # Export figure\n    export_figure(\"24h_feeding_viz.png\")\n\n", "CodeBleu_score": 24.984196429686307}, {"code": "def plot_feeding_24h_viz() -> None\n    from parse_raw_data import parse_raw_data\n    from datetime import datetime\n    from datetime import timedelta\n    from plotly.graph_objs import Layout\n    from plotly.graph_objs import Scatter\n    from plotly.offline import plot\n    from plotly.offline import iplot\n    from plotly.offline import init_notebook_mode\n    import plotly.graph_objs as go\n    import pandas as pd\n    import numpy as np\n    import os\n    import sys\n    import csv\n    import matplotlib.pyplot as plt\n    import matplotlib.dates as mdates\n    import seaborn as sns\n    import re\n    import plotly.express as px\n\n    # Set up the plot with a dark grid style\n    layout = Layout(\n        title=\"24-Hour Feeding Data\",\n        xaxis=dict(\n            title=\"Time of Day\",\n            tickmode='array',\n            tickvals=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n            ticktext=['00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00', '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'],\n            tickfont=dict(size=14),\n            titlefont=dict(size=16),\n        ),", "CodeBleu_score": 19.125067987429084}]}
{"instance_id": "6c1ffa63d122", "generation_result": [{"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    if task == ClassificationTask.Result:\n        target_counts = self.get_target_counts()\n    elif task == ClassificationTask.Prediction:\n        target_counts = self.get_prediction_counts()\n    else:\n        raise ValueError(f\"Unknown classification task: {task}\")\n\n    ax.bar(target_counts.index, target_counts.values)\n    ax.set_xlabel(task.value)\n    ax.set_ylabel(\"Count\")\n    ax.set_title(f\"Target Counts for {task.value}\")\n    ax.set_xticklabels(target_counts.index, rotation=45)\n\n    return ax", "CodeBleu_score": 34.156730457170745}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    if task == ClassificationTask.Result:\n        target_counts = self.get_target_counts()\n    elif task == ClassificationTask.Prediction:\n        target_counts = self.get_prediction_counts()\n    else:\n        raise ValueError(f\"{task} is not a valid classification task\")\n    ax = sns.barplot(x=target_counts.index, y=target_counts, ax=ax, **kwargs)\n    ax.set_xlabel(\"Targets\")\n    ax.set_ylabel(\"Counts\")\n    return ax\n\n", "CodeBleu_score": 41.3067849781784}, {"code": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    target_counts = self.get_target_counts(task)\n    sns.barplot(x=target_counts.index, y=target_counts.values, ax=ax)\n    ax.set_title(f\"Target Counts for {task.name} Task\")\n    ax.set_xlabel(\"Target\")\n    ax.set_ylabel(\"Count\")\n    return ax", "CodeBleu_score": 38.44783562465294}]}
{"instance_id": "2f81a906bad5", "generation_result": [{"code": "def render_to_rgb(figure)\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.backends.backend_agg as plt_backend_agg\n    from PIL import Image\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            canvas = plt_backend_agg.FigureCanvasAgg(fig)\n            canvas.draw()\n            image_from_plot = np.array(canvas.buffer_rgba())\n            image_from_plot = image_from_plot[:, :, 0:3]\n            image_from_plot = np.moveaxis(image_from_plot, 2, 0)\n            images.append(image_from_plot)\n        images = np.array(images)\n        return images\n    else:\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        image_from_plot = np.array(canvas.buffer_rgba())\n        image_from_plot = image_from_plot[:, :, 0:3]\n        image_from_plot = np.moveaxis(image_from_plot, 2, 0)\n        return image_from_plot\n\n", "CodeBleu_score": 34.163811625166}, {"code": "def render_to_rgb(figure)\n    import matplotlib\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from PIL import Image\n    from io import BytesIO\n\n    def _render_to_rgb(figure):\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        # Get RGBA buffer from figure\n        w, h = figure.canvas.get_width_height()\n        buf = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n        buf.shape = (h, w, 4)\n        # Convert to RGB\n        buf = buf[:, :, :3]\n        # Flip Y axis to match image orientation\n        buf = np.flipud(buf)\n        # Convert to CHW\n        buf = np.transpose(buf, (2, 0, 1))\n        return buf\n\n    plt_backend_agg = getattr(matplotlib.backends, \"backend_agg\")\n\n    if isinstance(figure, list):\n        images = [_render_to_rgb(fig) for fig in figure]\n        images = np.stack(images)\n        if close:\n            plt.close(\"all\")\n        return images\n    else:\n        image = _render_to_rgb(figure)\n        if close:\n            plt.close(figure)\n        return image\n\n", "CodeBleu_score": 53.44885355073158}, {"code": "def render_to_rgb(figure)\n    import numpy as np\n    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n    from matplotlib.figure import Figure\n    import matplotlib.pyplot as plt\n\n    if isinstance(figure, Figure):\n        figure = [figure]\n\n    images = []\n    for fig in figure:\n        canvas = FigureCanvas(fig)\n        canvas.draw()\n        data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n        w, h = fig.canvas.get_width_height()\n        image = data.reshape(h, w, 4)[:, :, :3]\n        images.append(image)\n        plt.close(fig)\n    if len(images) == 1:\n        return images[0]\n    else:\n        return np.stack(images)", "CodeBleu_score": 44.60386320338572}]}
{"instance_id": "c3037b01569c", "generation_result": [{"code": "def beholder_pytorch()\n    from beholder.beholder import Beholder\n    import time\n    import torch\n    import random\n\n    log_dir = \"./beholder_pytorch_example\"\n    beholder = Beholder(log_dir)\n\n    for i in range(1000):\n        # Generate random tensors\n        tensors = [torch.randn(random.randint(1, 10), random.randint(1, 10), random.randint(1, 10)) for i in range(random.randint(1, 10))]\n        names = [f\"tensor_{i}\" for i in range(len(tensors))]\n        # Update Beholder\n        beholder.update(tensors, names, frame=i)\n        # Sleep briefly\n        time.sleep(0.01)\n        # Print current iteration\n        print(i)\n\n    return None", "CodeBleu_score": 39.33652237455215}, {"code": "def beholder_pytorch()\n    import beholder\n    import time\n    import torch\n\n    for i in range(1000):\n        tensors = [\n            torch.rand(2, 3, 4, 5),\n            torch.rand(2, 3, 4, 5),\n            torch.rand(2, 3, 4, 5),\n            torch.rand(2, 3, 4, 5),\n        ]\n        names = [\"tensor1\", \"tensor2\", \"tensor3\", \"tensor4\"]\n        b = beholder.Beholder(log_dir=\"./logs\")\n        b.update(tensors, names, frame=i)\n        time.sleep(0.1)\n        print(i)\n\n", "CodeBleu_score": 27.312750903165483}, {"code": "def beholder_pytorch()\n    import beholder\n    import numpy as np\n    import torch\n    import time\n\n    for i in range(1000):\n        tensors = [torch.rand(10, 10), torch.rand(10, 10)]\n        names = ['tensor1', 'tensor2']\n        b = beholder.Beholder(log_dir='./logs')\n        b.update(tensors, names, frame=np.random.randint(10000))\n        time.sleep(0.01)\n        print(i)\n\n", "CodeBleu_score": 24.761182488515818}]}
{"instance_id": "cd57b5de8a88", "generation_result": [{"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    tensor_data = summary.value[0].tensor.float_val\n    tensor_data = np.array(tensor_data).reshape(num_thresholds, 4)\n    thresholds = np.linspace(0, 1, num_thresholds)\n    TP = tensor_data[:, 0].tolist()\n    FP = tensor_data[:, 1].tolist()\n    TN = tensor_data[:, 2].tolist()\n    FN = tensor_data[:, 3].tolist()\n    precision = tensor_data[:, 0]/(tensor_data[:, 0] + tensor_data[:, 1])\n    recall = tensor_data[:, 0]/(tensor_data[:, 0] + tensor_data[:, 3])\n    pr_data = {\n        \"thresholds\": thresholds.tolist(),\n        \"TP\": TP,\n        \"FP\": FP,\n        \"TN\": TN,\n        \"FN\": FN,\n        \"precision\": precision.tolist(),\n        \"recall\": recall.tolist()\n    }\n    self.log_asset_data(tag, pr_data, step=step)\n\n", "CodeBleu_score": 28.10932718443997}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    tensor_data = summary.tensor_data\n    tensor_data = np.array(tensor_data).reshape(num_thresholds, 6)\n    thresholds = [i / num_thresholds for i in range(num_thresholds)]\n    TP = tensor_data[:, 0].tolist()\n    FP = tensor_data[:, 1].tolist()\n    TN = tensor_data[:, 2].tolist()\n    FN = tensor_data[:, 3].tolist()\n    precision = tensor_data[:, 4].tolist()\n    recall = tensor_data[:, 5].tolist()\n    pr_data = {\n        \"thresholds\": thresholds,\n        \"TP\": TP,\n        \"FP\": FP,\n        \"TN\": TN,\n        \"FN\": FN,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n    self.log_asset_data(tag, pr_data, step=step)\n\n", "CodeBleu_score": 31.440791943936354}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None)\n    pr_data = {}\n    tensor_data = summary.tensor_data\n    tensor_shape = summary.tensor_shape\n    tensor_shape = [int(dim.size) for dim in tensor_shape.dim]\n    tensor_data = np.reshape(tensor_data, tensor_shape)\n    thresholds = [i / num_thresholds for i in range(0, num_thresholds)]\n    tp = tensor_data[0::5].flatten().tolist()\n    fp = tensor_data[1::5].flatten().tolist()\n    tn = tensor_data[2::5].flatten().tolist()\n    fn = tensor_data[3::5].flatten().tolist()\n    precision = tensor_data[4::5].flatten().tolist()\n    recall = tensor_data[5::5].flatten().tolist()\n    pr_data = {\n        \"thresholds\": thresholds,\n        \"tp\": tp,\n        \"fp\": fp,\n        \"tn\": tn,\n        \"fn\": fn,\n        \"precision\": precision,\n        \"recall\": recall,\n    }\n    self.log_asset_data(tag, pr_data, step=step)\n\n", "CodeBleu_score": 39.076215111398824}]}
{"instance_id": "7c4ea9beecc9", "generation_result": [{"code": "def load_mnist(images, labels, num)\n    images = images.reshape(-1, 28 * 28)\n    labels = labels.reshape(-1, 1)\n    return images[:num], labels[:num]\n\n", "CodeBleu_score": 6.457116363155315}, {"code": "def load_mnist(images, labels, num)\n    data = np.empty((num, 784))\n    target = np.empty((num,), dtype=np.uint8)\n    with gzip.open(images, 'rb') as f:\n        f.read(16)\n        for i in range(num):\n            data[i] = np.frombuffer(f.read(784), dtype=np.uint8, offset=0)\n    with gzip.open(labels, 'rb') as f:\n        f.read(8)\n        for i in range(num):\n            target[i] = np.frombuffer(f.read(1), dtype=np.uint8, offset=0)\n    return data, target\n\n", "CodeBleu_score": 42.276031004933984}, {"code": "def load_mnist(images, labels, num)\n\n    # Initialize the data and target arrays\n    data = np.empty((num, 784), dtype=np.uint8)\n    target = np.empty((num, 1), dtype=np.uint8)\n\n    # Open the images and labels with gzip\n    with gzip.open(images, 'rb') as f_images, gzip.open(labels, 'rb') as f_labels:\n\n        # Skip the headers\n        f_images.read(16)\n        f_labels.read(8)\n\n        # Read the data and labels\n        for i in range(num):\n            data[i] = np.frombuffer(f_images.read(784), dtype=np.uint8)\n            target[i] = np.frombuffer(f_labels.read(1), dtype=np.uint8)\n\n    # Reshape the data and target arrays\n    data = data.reshape(num, 28, 28)\n    target = target.reshape(num)\n\n    return data, target\n\n", "CodeBleu_score": 47.006771164795516}]}
{"instance_id": "bf4f1c828b25", "generation_result": [{"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.unique_values = {}\n    self.cardinality = {}\n    self.min_max = {}\n    self.data_type = {}\n    self.data_type_ratio = {}\n    self.data_model_lookup = {}\n    self.length = len(ldf)\n    self.pre_aggregated = {}\n    self.name = ldf.name\n    self.executor = ldf.executor\n    self.pandas_df = ldf.pandas_df\n    self.pandas_executor = ldf.pandas_executor\n    self.unique_values_pandas = {}\n    self.cardinality_pandas = {}\n    self.min_max_pandas = {}\n    self.data_type_pandas = {}\n    self.data_type_ratio_pandas = {}\n    self.data_model_lookup_pandas = {}\n    self.pre_aggregated_pandas = {}\n    self.executor_name = ldf.executor.name\n\n    if self.executor_name == \"Pandas\":\n        for attr in ldf.columns:\n            self.data_type_pandas[attr] = type(ldf.pandas_df[attr][0])\n            if self.data_type_pandas[attr] == str:\n                self.unique_values_pandas[attr] = ldf.pandas_df[attr].unique()\n                self.cardinality_pandas[attr] = len(self.unique_values_pandas[attr])\n            elif self.data_type_pandas[attr] == bool:\n                self.unique_values_pandas[attr] = ldf.pandas_df[attr].unique()\n                self.cardinality_pandas[attr] = len(self.unique_values_pandas[attr])\n            elif self.data_type_pandas[attr] == object:\n                self.unique_values_pandas[attr] = ldf.pandas_df[attr].unique()\n                self.cardinality_pandas[attr] = len(self.unique_values_pandas[attr])\n            elif self.data_type_pandas[attr] in [int, float]:\n                self.min_max_pandas[attr] = (", "CodeBleu_score": 27.230553360225855}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.unique_values = {}\n    self.cardinality = {}\n    self.min_max = {}\n    self.length = len(self)\n    for attribute in self.columns:\n        if self.data_type_lookup[attribute] == \"temporal\":\n            self.unique_values[attribute] = self.unique_values_temporal(attribute)\n            self.cardinality[attribute] = len(self.unique_values[attribute])\n        elif self.data_type_lookup[attribute] == \"nominal\":\n            self.unique_values[attribute] = self.unique_values_nominal(attribute)\n            self.cardinality[attribute] = len(self.unique_values[attribute])\n        elif self.data_type_lookup[attribute] == \"quantitative\":\n            self.min_max[attribute] = self.min_max_value(attribute)\n            self.cardinality[attribute] = self.cardinality_quantitative(attribute)\n    if not is_numeric_dtype(self.index):\n        self.unique_values[self.index.name] = self.unique_values_nominal(self.index.name)\n        self.cardinality[self.index.name] = len(self.unique_values[self.index.name])\n    self.pre_aggregated = False\n\n", "CodeBleu_score": 32.84011104596913}, {"code": "def compute_stats(self, ldf: LuxDataFrame)\n    self.unique_values = {}\n    self.cardinality = {}\n    self.min_max = {}\n    self.data_type = {}\n    self.data_length = len(ldf)\n    self.data_type = {\n        col: ldf[col].dtype for col in ldf.columns\n    }  # compute data types\n    for attr in ldf.columns:\n        if self.data_type[attr] == \"datetime64[ns]\":\n            # if timestamp, use string representation as key\n            self.unique_values[attr] = ldf[attr].apply(str).unique()\n            self.cardinality[attr] = len(self.unique_values[attr])\n        else:\n            self.unique_values[attr] = ldf[attr].unique()\n            self.cardinality[attr] = len(self.unique_values[attr])\n        if self.data_type[attr] in [\"int64\", \"float64\"]:\n            self.min_max[attr] = (ldf[attr].min(), ldf[attr].max())\n    if ldf.index.dtype != \"int64\":\n        self.unique_values[ldf.index.name] = ldf.index.unique()\n        self.cardinality[ldf.index.name] = len(self.unique_values[ldf.index.name])\n        self.min_max[ldf.index.name] = (ldf.index.min(), ldf.index.max())\n\n", "CodeBleu_score": 37.593549509381404}]}
{"instance_id": "3e6fc3d2f47c", "generation_result": [{"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    import pandas as pd\n    import psycopg2\n    import psycopg2.extras\n    from psycopg2.extensions import register_adapter, AsIs\n    import numpy as np\n\n    # registering numpy data types with psycopg2\n    def addapt_numpy_int64(numpy_int64):\n        return AsIs(numpy_int64)\n    def addapt_numpy_float64(numpy_float64):\n        return AsIs(numpy_float64)\n    psycopg2.extensions.register_adapter(np.int64, addapt_numpy_int64)\n    psycopg2.extensions.register_adapter(np.float64, addapt_numpy_float64)\n\n    # execute the query\n    query = f\"SELECT * FROM {tbl.table_name} LIMIT {preview_size};\"\n    conn = psycopg2.connect(tbl.conn_string)\n    df = pd.read_sql_query(query, conn)\n    return df", "CodeBleu_score": 35.09345119072804}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    import pandas as pd\n    from lux.utils import utils\n    from lux.utils.date_utils import compute_date_granularity\n    from lux.utils.utils import get_agg_title\n    import psycopg2\n    import os\n    from lux.utils.sql_utils import get_preview_query\n\n    if tbl.table_type == \"postgres\":\n        preview_query = get_preview_query(tbl.table_name, preview_size)\n        # print(preview_query)\n        # print(tbl.source)\n        # print(tbl.source.user)\n        # print(tbl.source.password)\n        # print(tbl.source.host)\n        # print(tbl.source.port)\n        # print(tbl.source.dbname)\n        try:\n            conn = psycopg2.connect(\n                user=tbl.source.user,\n                password=tbl.source.password,\n                host=tbl.source.host,\n                port=tbl.source.port,\n                dbname=tbl.source.dbname,\n            )\n            df = pd.read_sql(preview_query, conn)\n            conn.close()\n        except Exception as e:\n            utils.display_error(e, tbl)\n            return pd.DataFrame()\n\n        # Compute the granularity of time series data\n        time_cols = utils.get_time_cols(tbl)\n        for timeCol in time_cols:\n            time_granularity = compute_date_granularity(df[timeCol])\n            tbl.time_granularity[timeCol] = time_granularity\n\n        # Rename columns with duplicate column names\n        duplicate_columns = [\n            item for item, count in list(tbl.duplicate_columns.items()) if count > 1\n        ]\n        for column in duplicate_columns:\n            tbl.columns[tbl.columns.index(column)] = column + \"_\" + tbl.name\n\n        # Rename columns with reserved keywords\n        for column in tbl.columns:\n            if column in utils.SQL_RESERVED_WORDS:\n                tbl.columns[tbl", "CodeBleu_score": 36.69040884479964}, {"code": "def execute_preview(tbl: LuxSQLTable, preview_size=5)\n    import pandas as pd\n    import psycopg2\n    import psycopg2.extras\n\n    # Connect to PostgreSQL database\n    conn = psycopg2.connect(\n        host=\"localhost\",\n        database=\"lux\",\n        user=\"postgres\",\n        password=\"lux\",\n        port=5432,\n    )\n    # Create a cursor to perform database operations\n    cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n    # Execute a preview query\n    query = f\"SELECT * FROM {tbl.name} LIMIT {preview_size}\"\n    cur.execute(query)\n    # Retrieve the data\n    data = cur.fetchall()\n    # Close the cursor and the connection\n    cur.close()\n    conn.close()\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n    # Return the DataFrame\n    return df", "CodeBleu_score": 37.45776884580341}]}
{"instance_id": "b3dee87be4b8", "generation_result": [{"code": "def execute_sampling(tbl: LuxSQLTable)\n    import pandas as pd\n    import psycopg2\n    from lux.utils.utils import get_connection_string\n\n    conn = psycopg2.connect(get_connection_string())\n    cur = conn.cursor()\n    tbl_name = tbl.table_name\n    tbl_schema = tbl.table_schema\n    tbl_type = tbl.table_type\n\n    if tbl_type == \"view\":\n        tbl_name = f\"({tbl_name})\"\n        tbl_schema = \"main\"\n\n    if tbl_schema != \"main\":\n        tbl_name = f\"{tbl_schema}.{tbl_name}\"\n\n    tbl_len = pd.read_sql(f\"SELECT COUNT(*) FROM {tbl_name}\", conn)\n    tbl_len = tbl_len.values[0][0]\n    tbl_len = int(tbl_len)\n    if tbl_len < 10000:\n        tbl._sampled = pd.read_sql(f\"SELECT * FROM {tbl_name}\", conn)\n    else:\n        sample_frac = lux.config.sampling_cap / tbl_len\n        if sample_frac < 1:\n            tbl._sampled = pd.read_sql(\n                f\"SELECT * FROM {tbl_name} TABLESAMPLE BERNOULLI({sample_frac})\", conn\n            )\n        else:\n            tbl._sampled = pd.read_sql(f\"SELECT * FROM {tbl_name}\", conn)\n    conn.close()\n\n", "CodeBleu_score": 26.73921843721734}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n    import pandas as pd\n    import numpy as np\n    import random\n    import lux\n\n    if lux.config.sampling:\n        if lux.config.SAMPLE_FLAG:\n            tbl._sampled = pd.read_sql_query(\n                f\"SELECT * FROM {tbl.table_name} LIMIT {lux.config.SAMPLE_CAP}\",\n                tbl.engine,\n            )\n        else:\n            tbl._sampled = pd.read_sql_query(\n                f\"SELECT * FROM {tbl.table_name}\", tbl.engine\n            )\n    else:\n        tbl._sampled = pd.read_sql_query(f\"SELECT * FROM {tbl.table_name}\", tbl.engine)", "CodeBleu_score": 21.49864043582594}, {"code": "def execute_sampling(tbl: LuxSQLTable)\n    import pandas as pd\n    from lux.utils.utils import get_filter_specs\n    from lux.utils.date_utils import compute_date_granularity\n    from lux.utils.utils import compute_data_type\n    import lux\n    import warnings\n    import numpy as np\n    import pandas as pd\n    import psycopg2\n    import sqlalchemy as sa\n    from sqlalchemy.sql import text\n    from sqlalchemy.sql.expression import literal_column\n    from sqlalchemy.sql.expression import TextAsFrom\n    import re\n\n    if lux.config.sampling:\n        sampling_flag = lux.config.sampling\n    else:\n        sampling_flag = False\n\n    if sampling_flag:\n        sample_start = lux.config.sampling_start\n        sample_cap = lux.config.sampling_cap\n        sample_frac = lux.config.sampling_fraction\n        if sample_frac > 1 or sample_frac < 0:\n            warnings.warn(\n                \"Sampling fraction must be between 0 and 1. Setting sampling fraction to 1.\"\n            )\n            sample_frac = 1\n\n        # Get the table length\n        if tbl.table_type == \"SQLTable\":\n            # TODO: check if table is empty\n            table_length = tbl.nrows\n        elif tbl.table_type == \"pandas_df\":\n            table_length = len(tbl.pandas_df)\n        elif tbl.table_type == \"pandas_series\":\n            table_length = len(tbl.pandas_df)\n\n        # Compute the sample limit\n        limit = int(table_length * sample_frac)\n        if limit < sample_start:\n            warnings.warn(\n                \"Sampling start must be less than the sampling limit. Setting sampling start to 0.\"\n            )\n            sample_start = 0\n        if limit > sample_cap:\n            warnings.warn(\n                \"Sampling limit must be less than the sampling cap. Setting sampling limit to the cap.\"\n            )\n            limit = sample_cap\n\n        # Get the sample query\n        if tbl.table_type == \"SQLTable\":", "CodeBleu_score": 25.183953255369616}]}
{"instance_id": "485253b1088a", "generation_result": [{"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\n    # Override default text settings\n    text_settings = params['text_settings'].copy()\n    text_settings['text'] = params['text']\n    text_settings['x'] = params['x']\n    text_settings['y'] = params['y']\n\n    # Extract text, x, and y values\n    text = text_settings.pop('text')\n    x = text_settings.pop('x')\n    y = text_settings.pop('y')\n\n    # Extract background bounds\n    x_min, x_max = background.get_x_bounds()\n    y_min, y_max = background.get_y_bounds()\n\n    # Interpolate x and y values\n    x = interpolate_x(x, x_min, x_max)\n    y = interpolate_y(y, y_min, y_max)\n\n    # Draw text\n    plt.text(x, y, text, **text_settings)\n\n", "CodeBleu_score": 18.574074187708568}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n\n    # Override default text settings with provided parameters\n    text_params = {\n        \"x\": 0,\n        \"y\": 0,\n        \"text\": \"\",\n        \"fontsize\": 12,\n        \"color\": \"black\",\n        \"alpha\": 1,\n        \"horizontalalignment\": \"center\",\n        \"verticalalignment\": \"center\",\n        \"rotation\": 0,\n        \"fontweight\": \"normal\",\n        \"fontstyle\": \"normal\",\n        \"fontfamily\": \"sans-serif\",\n        \"bbox\": None,\n        \"zorder\": 10,\n    }\n    text_params.update(params)\n\n    # Extract text, x, and y values\n    text = text_params[\"text\"]\n    x = text_params[\"x\"]\n    y = text_params[\"y\"]\n\n    # Get background bounds\n    x_min, y_min, x_max, y_max = background.bounds\n\n    # Interpolate x and y values to fit within background bounds\n    x = x_min + (x_max - x_min) * x\n    y = y_min + (y_max - y_min) * y\n\n    # Draw text with remaining parameters\n    plt.text(\n        x,\n        y,\n        text,\n        fontsize=text_params[\"fontsize\"],\n        color=text_params[\"color\"],\n        alpha=text_params[\"alpha\"],\n        horizontalalignment=text_params[\"horizontalalignment\"],\n        verticalalignment=text_params[\"verticalalignment\"],\n        rotation=text_params[\"rotation\"],\n        fontweight=text_params[\"fontweight\"],\n        fontstyle=text_params[\"fontstyle\"],\n        fontfamily=text_params[\"fontfamily\"],\n        bbox=text_params[\"bbox\"],\n        zorder=text_params[\"zorder\"],\n    )", "CodeBleu_score": 22.31498733520189}, {"code": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None\n    text_params = params['text']\n    text = text_params['text']\n    x = text_params['x']\n    y = text_params['y']\n    x_bounds = background.bounds[0]\n    y_bounds = background.bounds[1]\n    x_new = (x - x_bounds[0]) / (x_bounds[1] - x_bounds[0])\n    y_new = (y - y_bounds[0]) / (y_bounds[1] - y_bounds[0])\n    plt.text(x_new, y_new, text, **{k: v for k, v in text_params.items() if k != 'text'})", "CodeBleu_score": 15.51570482609909}]}
{"instance_id": "5f70a88b6b72", "generation_result": [{"code": "def _determine_format_string( dates, datetime_format=None )\n\n    import numpy as np\n    import pandas as pd\n\n    # If the datetime_format is provided, return this format\n    if datetime_format is not None:\n        return datetime_format\n\n    # Calculate the average number of days between consecutive dates\n    date_diffs = np.diff(pd.to_datetime(dates))\n    avg_days = np.mean(date_diffs)\n\n    # If the data is intraday, return the appropriate format string\n    if avg_days < 0.33:\n        if (date_diffs.max().days > 1):\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n\n    # If the data is daily or less frequent, return the appropriate format string\n    if (date_diffs.max().days < 365):\n        return '%b %d'\n    else:\n        return '%Y-%b-%d'\n", "CodeBleu_score": 36.1367258221213}, {"code": "def _determine_format_string( dates, datetime_format=None )\n    import numpy as np\n    from datetime import datetime\n\n    if datetime_format is not None:\n        return datetime_format\n\n    # Calculate the average number of days between consecutive points\n    days_between_points = np.mean( np.diff( dates ) )\n\n    if days_between_points < 0.33:\n        # If the data is intraday, check if it spans multiple days\n        if np.max( dates ) - np.min( dates ) > 86400:\n            # If it spans multiple days, use the format string '%b %d, %H:%M'\n            datetime_format = '%b %d, %H:%M'\n        else:\n            # If it spans less than one day, use the format string '%H:%M'\n            datetime_format = '%H:%M'\n    else:\n        # If the data is daily or less frequent, check if it spans multiple years\n        if np.max( dates ) - np.min( dates ) > 365 * 86400:\n            # If it spans multiple years, use the format string '%Y-%b-%d'\n            datetime_format = '%Y-%b-%d'\n        else:\n            # If it spans less than one year, use the format string '%b %d'\n            datetime_format = '%b %d'\n\n    return datetime_format\n", "CodeBleu_score": 42.25314123705611}, {"code": "def _determine_format_string( dates, datetime_format=None )\n\n    import numpy as np\n\n    if datetime_format:\n        return datetime_format\n\n    dates = np.array( dates )\n    delta = np.diff( dates )\n    avg_days = np.mean( delta )\n\n    if avg_days < 0.33:\n        if ( dates[-1] - dates[0] ) > 1:\n            return '%b %d, %H:%M'\n        else:\n            return '%H:%M'\n\n    if ( dates[-1] - dates[0] ) > 365:\n        return '%Y-%b-%d'\n    else:\n        return '%b %d'\n\n", "CodeBleu_score": 30.528633021570794}]}
{"instance_id": "f9589bf4666d", "generation_result": [{"code": "def _mpf_to_rgba(c, alpha=None)\n    # Check if the color input is in uint8 RGB or RGBA format\n    if type(c) == tuple and len(c) == 3:\n        # If so, check if any RGB components are greater than 1\n        if any(x > 1 for x in c):\n            # If so, normalize the RGB components to the range [0, 1]\n            c = tuple(x / 255 for x in c)\n    # Convert the processed color input to an RGBA tuple using a color conversion utility\n    rgba = mpl.colors.to_rgba(c)\n    # Optionally apply a specified alpha value\n    if alpha is not None:\n        rgba = rgba[0], rgba[1], rgba[2], alpha\n    return rgba\n", "CodeBleu_score": 34.84838418680794}, {"code": "def _mpf_to_rgba(c, alpha=None)\n    if isinstance(c, tuple):\n        if len(c) == 3:\n            c = list(c)\n            c.append(1)\n            c = tuple(c)\n        elif len(c) == 4:\n            c = tuple(c)\n        else:\n            raise ValueError(\"Color input must be RGB or RGBA.\")\n    elif isinstance(c, list):\n        if len(c) == 3:\n            c.append(1)\n            c = tuple(c)\n        elif len(c) == 4:\n            c = tuple(c)\n        else:\n            raise ValueError(\"Color input must be RGB or RGBA.\")\n    elif isinstance(c, str):\n        if c.startswith(\"#\"):\n            c = c[1:]\n        if len(c) == 3:\n            c = tuple(int(c[i] * 2, 16) for i in (0, 1, 2))\n            c = (c[0], c[1], c[2], 1)\n        elif len(c) == 4:\n            c = tuple(int(c[i] * 2, 16) for i in (0, 1, 2, 3))\n        elif len(c) == 6:\n            c = tuple(int(c[i : i + 2], 16) for i in (0, 2, 4))\n            c = (c[0], c[1], c[2], 1)\n        elif len(c) == 8:\n            c = tuple(int(c[i : i + 2], 16) for i in (0, 2, 4, 6))\n        else:\n            raise ValueError(\"Color input must be RGB or RGBA.\")\n    else:\n        raise ValueError(\"Color input must be RGB or RGBA.\")\n    if any(i > 1 for i in c[:3]):\n        c = tuple(i / 255 for i in c[:3])\n    if alpha is not None:\n        c = c[:3] + (alpha,)\n    return", "CodeBleu_score": 33.84654810812741}, {"code": "def _mpf_to_rgba(c, alpha=None)\n\n    if isinstance(c, tuple):\n        if len(c) == 3:\n            c = (c[0], c[1], c[2], 1.0)\n        elif len(c) == 4:\n            c = (c[0], c[1], c[2], c[3])\n        else:\n            raise ValueError(\"Invalid RGB or RGBA tuple\")\n    elif isinstance(c, list):\n        if len(c) == 3:\n            c = (c[0], c[1], c[2], 1.0)\n        elif len(c) == 4:\n            c = (c[0], c[1], c[2], c[3])\n        else:\n            raise ValueError(\"Invalid RGB or RGBA list\")\n    elif isinstance(c, str):\n        c = mplc.to_rgba(c)\n    elif isinstance(c, np.ndarray):\n        if len(c) == 3:\n            c = (c[0], c[1], c[2], 1.0)\n        elif len(c) == 4:\n            c = (c[0], c[1], c[2], c[3])\n        else:\n            raise ValueError(\"Invalid RGB or RGBA array\")\n    else:\n        raise ValueError(\"Invalid RGB or RGBA color input\")\n\n    if alpha is not None:\n        c = (c[0], c[1], c[2], alpha)\n\n    return c\n\n", "CodeBleu_score": 28.670214025208995}]}
{"instance_id": "4202e378ab0b", "generation_result": [{"code": "def subplots(self,*args,**kwargs)\n    if 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(self,kwargs.get('style',None))\n    else:\n        style = self.mpfstyle\n    subplots = super().subplots(*args,**kwargs)\n    if isinstance(subplots,matplotlib.axes.Axes):\n        subplots.mpfstyle = style\n    elif isinstance(subplots,np.ndarray):\n        for subplot in subplots:\n            subplot.mpfstyle = style\n    elif isinstance(subplots,tuple):\n        for subplot in subplots:\n            subplot.mpfstyle = style\n    else:\n        raise TypeError(\"Unexpected subplots type: {}\".format(type(subplots)))\n    return subplots\n", "CodeBleu_score": 52.48264202028858}, {"code": "def subplots(self,*args,**kwargs)\n    if 'style' in kwargs:\n        self._check_for_and_apply_style(kwargs['style'])\n        self.mpfstyle = kwargs['style']\n        del kwargs['style']\n    else:\n        if not hasattr(self,'mpfstyle'):\n            raise AttributeError('No style has been applied to the figure.')\n        self._check_for_and_apply_style(self.mpfstyle)\n    subplots = self.figure.subplots(*args,**kwargs)\n    if isinstance(subplots, matplotlib.axes.Axes):\n        subplots.mpfstyle = self.mpfstyle\n    elif isinstance(subplots, numpy.ndarray):\n        for subplot in subplots.flat:\n            subplot.mpfstyle = self.mpfstyle\n    else:\n        raise TypeError('Unexpected type returned by subplots.')\n    return subplots", "CodeBleu_score": 35.83958823558445}, {"code": "def subplots(self,*args,**kwargs)\n    if 'style' in kwargs:\n        self._check_for_and_apply_style(kwargs['style'])\n        self.mpfstyle = kwargs['style']\n        del kwargs['style']\n    elif not hasattr(self,'mpfstyle'):\n        self._check_for_and_apply_style('default')\n        self.mpfstyle = 'default'\n    else:\n        self._check_for_and_apply_style(self.mpfstyle)\n    result = super().subplots(*args,**kwargs)\n    if isinstance(result,matplotlib.axes.Axes):\n        result.mpfstyle = self.mpfstyle\n    elif isinstance(result,np.ndarray):\n        for axes in result:\n            axes.mpfstyle = self.mpfstyle\n    else:\n        raise TypeError(f'subplots returned unexpected type: {type(result)}')\n    return result\n", "CodeBleu_score": 36.732893308501936}]}
{"instance_id": "3e2b6dfe4753", "generation_result": [{"code": "def _adjust_figsize(fig,config)\n    if not fig:\n        return\n    if 'figsize' not in config:\n        if 'figratio' in config:\n            figratio = config['figratio']\n        else:\n            figratio = 16/9\n        if 'figwidth' in config:\n            figwidth = config['figwidth']\n        else:\n            figwidth = 10\n        if 'figheight' in config:\n            figheight = config['figheight']\n        else:\n            figheight = 10\n        if figratio > 1:\n            figwidth = figwidth * figratio\n        elif figratio < 1:\n            figheight = figheight / figratio\n        if figwidth > 100:\n            figwidth = 100\n            figheight = figheight / figratio\n        elif figheight > 100:\n            figheight = 100\n            figwidth = figwidth * figratio\n        if figwidth < 0.1:\n            figwidth = 0.1\n            figheight = figheight / figratio\n        elif figheight < 0.1:\n            figheight = 0.1\n            figwidth = figwidth * figratio\n        fig.set_size_inches(figwidth,figheight)\n    else:\n        fig.set_size_inches(config['figsize'][0],config['figsize'][1])\n", "CodeBleu_score": 22.41293642438418}, {"code": "def _adjust_figsize(fig,config)\n    if fig is None:\n        return\n    if config is None:\n        return\n    if config.get(\"figsize\") is not None:\n        fig.set_size_inches(config[\"figsize\"])\n        return\n    if config.get(\"figratio\") is None:\n        return\n    figratio = config[\"figratio\"]\n    xlim = fig.axes[0].get_xlim()\n    ylim = fig.axes[0].get_ylim()\n    xrange = xlim[1] - xlim[0]\n    yrange = ylim[1] - ylim[0]\n    xfigsize = xrange / figratio[0]\n    yfigsize = yrange / figratio[1]\n    if xfigsize < yfigsize:\n        fig.set_size_inches([xfigsize, xfigsize])\n    else:\n        fig.set_size_inches([yfigsize, yfigsize])\n    return\n", "CodeBleu_score": 33.10290678295041}, {"code": "def _adjust_figsize(fig,config)\n    if fig is None:\n        return\n    if 'figure_size' in config:\n        fig.set_size_inches(config['figure_size'])\n    elif 'figure_ratio' in config:\n        fig.set_size_inches(config['figure_ratio']*fig.get_size_inches())\n        if fig.get_size_inches()[0]/fig.get_size_inches()[1] > 1.5:\n            fig.set_size_inches(fig.get_size_inches()[1]*1.5,fig.get_size_inches()[1])\n        elif fig.get_size_inches()[1]/fig.get_size_inches()[0] > 1.5:\n            fig.set_size_inches(fig.get_size_inches()[0],fig.get_size_inches()[0]*1.5)\n    else:\n        fig.set_size_inches(fig.get_size_inches()[0]*1.5,fig.get_size_inches()[0]*1.5)\n", "CodeBleu_score": 20.72664261012531}]}
{"instance_id": "986e5804b941", "generation_result": [{"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    if apmav is None:\n        mav = config['mav']\n    else:\n        mav = apmav\n    if apwidth is None:\n        width = config['width']\n    else:\n        width = apwidth\n    mav_prices = []\n    if isinstance(mav,dict):\n        if 'shift' in mav:\n            shifts = mav['shift']\n        else:\n            shifts = [0]\n        if 'period' in mav:\n            periods = mav['period']\n        else:\n            periods = [20]\n    else:\n        shifts = [0]\n        periods = [mav]\n    for shift in shifts:\n        for period in periods:\n            if len(mav_prices) < 7:\n                mav_prices.append(prices.shift(shift).rolling(period).mean().dropna())\n    for i,mav_price in enumerate(mav_prices):\n        ax.plot(xdates,mav_price,linewidth=width,color=next(ax._get_lines.prop_cycler)['color'])\n    return mav_prices\n", "CodeBleu_score": 37.45728059067309}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    # Determine the moving averages to plot\n    if apmav is None:\n        mav = config.get('mav',None)\n    else:\n        mav = apmav\n    # If moving averages are specified as a dictionary, extract the shift values and periods\n    if mav is not None:\n        if isinstance(mav,dict):\n            mavshift = list(mav.keys())\n            mavperiod = list(mav.values())\n        else:\n            mavshift = [0]\n            mavperiod = [mav]\n        # Ensure at most 7 moving averages are plotted\n        if len(mavshift) > 7:\n            mavshift = mavshift[:7]\n            mavperiod = mavperiod[:7]\n        # Calculate the moving averages\n        mavprices = []\n        for i in range(len(mavshift)):\n            mavprices.append(prices.rolling(mavperiod[i]).mean().shift(mavshift[i]))\n        # Plot the moving averages\n        for i in range(len(mavprices)):\n            if apwidth is None:\n                ax.plot(xdates,mavprices[i],color=next(ax._get_lines.prop_cycler)['color'],linewidth=config.get('mavwidth',1.5))\n            else:\n                ax.plot(xdates,mavprices[i],color=next(ax._get_lines.prop_cycler)['color'],linewidth=apwidth)\n    # Return the calculated moving averages\n    return mavprices\n", "CodeBleu_score": 36.30104972867806}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\n    if apmav is None:\n        mavs = config['mav']\n    else:\n        mavs = apmav\n    if apwidth is None:\n        width = config['width']\n    else:\n        width = apwidth\n    if isinstance(mavs,dict):\n        mavs = list(mavs.keys())\n    mavs = mavs[:7]\n    mavprices = []\n    for mav in mavs:\n        if isinstance(mav,int):\n            shift = 0\n            period = mav\n        elif isinstance(mav,tuple):\n            shift = mav[0]\n            period = mav[1]\n        else:\n            raise TypeError('Moving average must be an integer or a tuple.')\n        mavprice = prices.rolling(period).mean().shift(shift)\n        mavprices.append(mavprice)\n        ax.plot(xdates,mavprice,linewidth=width,color=next(ax._get_lines.prop_cycler)['color'])\n    return mavprices\n", "CodeBleu_score": 35.839112499404095}]}
{"instance_id": "5308e20f3c90", "generation_result": [{"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n    # Extract the kernel height and width from the input tensor's shape\n    kh = opnode.kernel_shape[0]\n    kw = opnode.kernel_shape[1]\n\n    # Determine the number of input channels\n    in_channels = inputs[0].shape[1]\n\n    # Calculate the product of the output tensor's dimensions\n    out_size = 1\n    for dim in outputs[0].shape:\n        out_size *= dim\n\n    # Check if the operation includes a bias term\n    if len(inputs) == 3:\n        # Compute the FLOPs by multiplying the total number of output elements by the number of input channels and the kernel size, adding the bias if present\n        return out_size * in_channels * kh * kw + out_size\n    else:\n        # Compute the FLOPs by multiplying the total number of output elements by the number of input channels and the kernel size\n        return out_size * in_channels * kh * kw\n\n", "CodeBleu_score": 19.897902807195113}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\n    # Extract kernel height and width from the input tensor's shape\n    kh = opnode.parameters.get(\"kernel_height\")\n    kw = opnode.parameters.get(\"kernel_width\")\n\n    # Determine the number of input channels\n    num_in_channels = inputs[0].shape[1]\n\n    # Calculate the product of the output tensor's dimensions\n    num_out_elements = np.product(outputs[0].shape)\n\n    # Check if the operation includes a bias term\n    if opnode.parameters.get(\"bias_term\"):\n        # Compute the FLOPs by multiplying the total number of output elements by the number of input channels and the kernel size, adding the bias\n        return num_out_elements * num_in_channels * (kh * kw + 1)\n    else:\n        # Compute the FLOPs by multiplying the total number of output elements by the number of input channels and the kernel size\n        return num_out_elements * num_in_channels * (kh * kw)\n\n", "CodeBleu_score": 31.323421984224918}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)\n\n    # Extract the kernel height and width from the input tensor's shape\n    kernel_height = inputs[0].shape[2]\n    kernel_width = inputs[0].shape[3]\n\n    # Determine the number of input channels\n    num_input_channels = inputs[0].shape[1]\n\n    # Calculate the product of the output tensor's dimensions\n    output_dims = np.prod(outputs[0].shape)\n\n    # Check if the operation includes a bias term\n    if len(inputs) > 1:\n        # Compute the FLOPs by multiplying the total number of output elements by the number of input channels and the kernel size, adding the bias if present\n        flops = num_input_channels * (kernel_height * kernel_width) * output_dims + output_dims\n    else:\n        # Compute the FLOPs by multiplying the total number of output elements by the number of input channels and the kernel size\n        flops = num_input_channels * (kernel_height * kernel_width) * output_dims\n\n    return flops\n\n", "CodeBleu_score": 19.77537334240601}]}
{"instance_id": "70811d3cf578", "generation_result": [{"code": "def mge_dtype_to_ir_type(mge_dtype)\n    import numpy as np\n    import tvm\n    from tvm import relay\n\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype)\n    return _dtype_to_ir_type[mge_dtype](relay)\n\n", "CodeBleu_score": 41.76688435971976}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n    from numpy import dtype\n    import numpy as np\n    mge_dtype = dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype), \"Expected a NumPy data type, got {}\".format(type(mge_dtype))\n    return _dtype_to_ir_type[mge_dtype](mge_dtype)\n\n", "CodeBleu_score": 35.051321908344704}, {"code": "def mge_dtype_to_ir_type(mge_dtype)\n    import numpy as np\n    import tvm\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(mge_dtype, np.dtype), \"mge_dtype must be a NumPy data type\"\n    ir_type = _dtype_to_ir_type[mge_dtype](mge_dtype)\n    return ir_type\n", "CodeBleu_score": 53.354858643447976}]}
{"instance_id": "a108c9ba2c22", "generation_result": [{"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = np.asarray(x)\n    dtype = x.dtype\n    if dtype == np.bool_:\n        x = x.astype(np.uint8)\n        dtype = x.dtype\n    elif dtype == np.bfloat16:\n        x = x.astype(np.float32)\n        dtype = x.dtype\n    elif dtype.hasobject or np.issubdtype(dtype, np.str_):\n        raise NotImplementedError(\n            \"NumPy array constant of {} is not supported.\".format(dtype)\n        )\n    x = np.ascontiguousarray(x)\n    elem_type = ir.Type.parse(np_dtype_to_mlir_type(dtype))\n    return [\n        ir.ConstantOp(\n            ir.RankedTensorType.get(x.shape, elem_type),\n            ir.DenseElementsAttr.get(x.reshape(-1)),\n        )\n    ]\n\n", "CodeBleu_score": 41.433751572319785}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = np.ascontiguousarray(x, x.dtype)\n    else:\n        x = np.ascontiguousarray(x)\n    elem_type = _dtype_to_mlir_type(x.dtype)\n    if elem_type == ir.IntegerType.get_signless(1):\n        # Convert boolean arrays to integer arrays.\n        x = x.astype(np.int8)\n        elem_type = ir.IntegerType.get_signless(8)\n    elif elem_type == ir.BF16Type.get():\n        # Convert bfloat16 arrays to float32 arrays.\n        x = x.astype(np.float32)\n        elem_type = ir.F32Type.get()\n    # TODO: Consider adding a check that the array is actually C-contiguous.\n    # TODO: Consider adding a check that the array has no padding.\n    return [\n        hlo.ConstOp(\n            ir.RankedTensorType.get(x.shape, elem_type),\n            ir.DenseElementsAttr.get(x.reshape(-1)),\n        )\n    ]\n\n", "CodeBleu_score": 40.38719551861444}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\n    if canonicalize_types:\n        x = np.ascontiguousarray(x)\n        x = np.array(x, dtype=np.float32) if x.dtype == np.float16 else x\n        x = np.array(x, dtype=np.float32) if x.dtype == np.bfloat16 else x\n        x = np.array(x, dtype=np.int32) if x.dtype == np.int64 else x\n        x = np.array(x, dtype=np.uint32) if x.dtype == np.uint64 else x\n    if x.dtype == np.bool_:\n        x = x.astype(np.uint8)\n    if x.dtype == np.bfloat16:\n        x = x.view(np.uint16)\n    return [\n        hlo.ConstOp(\n            ir.RankedTensorType.get(x.shape, _type_to_np_type[x.dtype.type]),\n            ir.DenseElementsAttr.get(x.reshape(-1)),\n        )\n    ]\n\n", "CodeBleu_score": 45.778746346114204}]}
{"instance_id": "3345fe4d008b", "generation_result": [{"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if not val.size:\n        # Empty arrays are not supported.\n        raise NotImplementedError(\n            \"ndarray_constant_handler: empty arrays are not supported\"\n        )\n    if not np.any(val.strides):\n        # All-zero strides are not supported.\n        raise NotImplementedError(\n            \"ndarray_constant_handler: all-zero strides are not supported\"\n        )\n    if np.all(val.strides):\n        # No zero strides.\n        return _numpy_array_constant(val, canonicalize_types)\n    # Zero strides.\n    zero_stride_axes = np.where(val.strides == 0)[0]\n    non_zero_stride_axes = np.where(val.strides != 0)[0]\n    # Collapse the array along the zero-stride axes.\n    collapsed_val = val.reshape(\n        np.array(val.shape)[non_zero_stride_axes].tolist() + [-1]\n    )\n    collapsed_val = collapsed_val.reshape(\n        np.delete(collapsed_val.shape, -1)\n    )\n    # Create a broadcast operation for the collapsed array.\n    collapsed_val = _numpy_array_constant(collapsed_val, canonicalize_types)\n    result = _op.broadcast_to(\n        collapsed_val,\n        _op.const(val.shape, dtype=\"int64\"),\n        non_zero_stride_axes,\n    )\n    return [result]\n\n", "CodeBleu_score": 40.253855771539484}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if np.ndim(val) == 0:\n        return _numpy_array_constant(val, canonicalize_types)\n    if not canonicalize_types:\n        return _numpy_array_constant(val, canonicalize_types)\n    # We need to handle the case where the array has zero strides.\n    # In this case, we need to collapse the array along the zero-strided\n    # dimensions and then broadcast it back to the original shape.\n    # We can't just use np.ndim(val) because that counts the zero-strided\n    # dimensions as well.\n    # TODO: We should probably handle the case where the array has zero-strides\n    # but no zero-stride dimensions.\n    num_zero_strides = len(np.argwhere(val.strides == 0))\n    if num_zero_strides == 0:\n        return _numpy_array_constant(val, canonicalize_types)\n    zero_strides = np.argwhere(val.strides == 0).flatten()\n    non_zero_strides = np.argwhere(val.strides != 0).flatten()\n    # Collapse the array along the zero-strided dimensions.\n    collapsed_val = np.array(val)\n    for axis in zero_strides:\n        collapsed_val = np.squeeze(collapsed_val, axis=axis)\n    # Create a broadcast operation for the collapsed array.\n    # The broadcast operation will match the original shape and non-zero-strided\n    # axes.\n    broadcast_shape = np.ones(len(val.shape), dtype=np.int64)\n    broadcast_shape[zero_strides] = 1\n    broadcast_shape[non_zero_strides] = val.shape[non_zero_strides]\n    broadcast_axes = np.zeros(len(val.shape), dtype=np.int64)\n    broadcast_axes[non_zero_strides] = np.arange(len(non_zero_strides))\n    broadcast_axes[zero_strides] = -1\n    return _numpy_array_constant(", "CodeBleu_score": 37.07146219555531}, {"code": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]\n    if not val.shape:\n        return [\n            ir.Constant(\n                ir.LiteralType.get(val.dtype),\n                np.array(val, dtype=val.dtype),\n            )\n        ]\n    # Check for zero strides.\n    zero_stride_axes = []\n    nonzero_stride_axes = []\n    for axis, stride in enumerate(val.strides):\n        if stride == 0:\n            zero_stride_axes.append(axis)\n        else:\n            nonzero_stride_axes.append(axis)\n    # Collapse the array along the zero-stride axes.\n    collapsed_val = np.ascontiguousarray(val)\n    for axis in reversed(zero_stride_axes):\n        collapsed_val = np.squeeze(collapsed_val, axis)\n    # If canonicalize_types is True, canonicalize the data type.\n    if canonicalize_types:\n        collapsed_val = np.array(collapsed_val, dtype=np.float32)\n    # Create a broadcast operation for the collapsed array.\n    broadcast_op = ir.RankedTensorType(\n        [\n            ir.IndexAttr.get() if axis in zero_stride_axes else val.shape[axis]\n            for axis in range(len(val.shape))\n        ],\n        collapsed_val.dtype,\n    )\n    return [\n        ir.Constant(\n            broadcast_op,\n            collapsed_val,\n        )\n    ]\n\n", "CodeBleu_score": 40.456999302868454}]}
{"instance_id": "91b298678351", "generation_result": [{"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    return _op.dense_bool_elements(xs)\n\n", "CodeBleu_score": 8.485937340348356}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    return ir.DenseElementsAttr(\n        ir.DenseIntElementsAttr(\n            ir.IntegerType.get_signless(1),\n            [int(x) for x in xs],\n        )\n    )\n\n", "CodeBleu_score": 22.1557787373266}, {"code": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr\n    ...\n\n", "CodeBleu_score": 7.211538461539189}]}
{"instance_id": "06ebabf1e0ff", "generation_result": [{"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    assert len(args) == len(ctx.vars_in) == len(ctx.vars_out) == 1, \"create_tensor_lower: One input and one output variable required.\"\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n    if in_var.bound:\n        in_var.data = np.array(in_var.data, dtype=in_var.dtype)\n        out_var.data = np.array(out_var.data, dtype=out_var.dtype)\n        out_var.data[:] = in_var.data\n    elif out_var.bound:\n        out_var.data = np.array(out_var.data, dtype=out_var.dtype)\n        in_var.data = np.array(in_var.data, dtype=in_var.dtype)\n        in_var.data[:] = out_var.data\n    else:\n        assert False, \"create_tensor_lower: Only tensor creation from constant is supported.\"\n    assert in_var.shape == out_var.shape, \"create_tensor_lower: Shapes of input and output variables must be the same.\"\n    return HLOTensor(in_var.data, in_var.shape, in_var.dtype, in_var.name)\n\n", "CodeBleu_score": 44.74850912624183}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    # TODO: check if this is the correct way to handle multiple inputs\n    if len(args) == 1 and isinstance(args[0], Sequence):\n        args = args[0]\n    assert len(args) == 1 and len(ctx.vars_in) == 1 and len(ctx.vars_out) == 1, \\\n        \"Expected one input and one output variable\"\n    in_var = ctx.vars_in[0]\n    out_var = ctx.vars_out[0]\n    assert in_var.shape == out_var.shape, \\\n        \"Input and output variables must have the same shape\"\n    if in_var.bound:\n        ctx.set_data(out_var, in_var.data)\n        return HLOTensor(out_var.data, out_var.shape, out_var.dtype)\n    elif out_var.bound:\n        ctx.set_data(in_var, out_var.data)\n        return HLOTensor(in_var.data, in_var.shape, in_var.dtype)\n    else:\n        raise AssertionError(\"Only tensor creation from constant is supported\")\n\n", "CodeBleu_score": 35.04447815464044}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\n    if len(args) != 1:\n        raise ValueError(\"Expected one argument\")\n    arg = args[0]\n    if not isinstance(arg, HLOTensor):\n        raise TypeError(\"Expected a HLOTensor\")\n    if len(ctx.vars_in) != 1 or len(ctx.vars_out) != 1:\n        raise ValueError(\"Expected one input and one output variable\")\n    var_in = ctx.vars_in[0]\n    var_out = ctx.vars_out[0]\n    if var_in.has_bound_data():\n        var_out.set_data(var_in.get_data())\n    elif var_out.has_bound_data():\n        var_in.set_data(var_out.get_data())\n    else:\n        raise AssertionError(\"Only tensor creation from constant is supported\")\n    assert var_in.shape == var_out.shape\n    data = var_out.get_data()\n    data = np.array(data, dtype=var_out.dtype)\n    return HLOTensor(data)\n\n", "CodeBleu_score": 44.73444790837999}]}
{"instance_id": "6cd750fa0c8d", "generation_result": [{"code": "def _device_put_scalar(x, device)\n    import numpy as np\n    from ._device_put import device_put\n    if isinstance(x, np.ndarray):\n        raise TypeError('The provided value is already a numpy array.')\n    else:\n        x = np.array(x)\n    x = device_put(x, device)\n    return x\n\n", "CodeBleu_score": 30.561437874917342}, {"code": "def _device_put_scalar(x, device)\n    if not isinstance(x, (np.ndarray, np.generic)):\n        x = np.array(x)\n    return _device_put_array(x, device)\n\n", "CodeBleu_score": 21.553711782805834}, {"code": "def _device_put_scalar(x, device)\n    import numpy as np\n    from ._device_put import device_put\n\n    if not isinstance(x, np.ndarray):\n        x = np.array(x)\n\n    x = device_put(x, device)\n\n    return x", "CodeBleu_score": 27.415238398711484}]}
{"instance_id": "140de7c2b6a8", "generation_result": [{"code": "def _shape_equal(lhs_shape, rhs_shape)\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n    assert isinstance(lhs_shape, tuple) or isinstance(lhs_shape, list)\n    assert isinstance(rhs_shape, tuple) or isinstance(rhs_shape, list)\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if not isinstance(lhs_shape[0], int) or not isinstance(rhs_shape[0], int):\n        return False\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i] != rhs_shape[i]:\n            return False\n    return True\n\n", "CodeBleu_score": 48.970641288324614}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n    # Convert shapes to lists if they are NumPy arrays\n    if type(lhs_shape) is np.ndarray:\n        lhs_shape = lhs_shape.tolist()\n    if type(rhs_shape) is np.ndarray:\n        rhs_shape = rhs_shape.tolist()\n    # Assert that both shapes are either tuples or lists\n    assert type(lhs_shape) in [tuple, list], \"lhs_shape must be a tuple or a list\"\n    assert type(rhs_shape) in [tuple, list], \"rhs_shape must be a tuple or a list\"\n    # If both shapes are empty, return True\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    # Verify that the first elements of the shapes are integers\n    if len(lhs_shape) > 0:\n        assert type(lhs_shape[0]) is int, \"lhs_shape must be a list of integers\"\n    if len(rhs_shape) > 0:\n        assert type(rhs_shape[0]) is int, \"rhs_shape must be a list of integers\"\n    # If the shapes have different lengths, return False\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    # Iterate through the elements of both shapes, and if any corresponding elements are not equal, return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i] != rhs_shape[i]:\n            return False\n    # If all checks pass, return True\n    return True\n\n", "CodeBleu_score": 47.55594295600425}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\n    if type(lhs_shape) == type(np.array([])):\n        lhs_shape = lhs_shape.tolist()\n    if type(rhs_shape) == type(np.array([])):\n        rhs_shape = rhs_shape.tolist()\n    assert type(lhs_shape) == type(rhs_shape) and type(lhs_shape) in [tuple, list], \"Shapes must be tuples or lists\"\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    assert type(lhs_shape[0]) == int and type(rhs_shape[0]) == int, \"Shapes must be tuples or lists of integers\"\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n    for i in range(len(lhs_shape)):\n        if lhs_shape[i] != rhs_shape[i]:\n            return False\n    return True\n", "CodeBleu_score": 41.611754093892316}]}
{"instance_id": "55d0c71a259b", "generation_result": [{"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n    import pandas as pd\n\n    training_set = pd.read_csv(\n        training_set_filename, dtype=dtypes, parse_dates=[\"date\"]\n    )\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes, parse_dates=[\"date\"])\n    training_set_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes, parse_dates=[\"date\"]\n    )\n    meta_dtypes[\"target\"] = \"float64\"\n    test_set_metadata = pd.read_csv(\n        test_set_metadata_filename, dtype=meta_dtypes, parse_dates=[\"date\"]\n    )\n    return training_set, test_set, training_set_metadata, test_set_metadata\n\n", "CodeBleu_score": 28.341259606797443}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\n    # Read in training data\n    training_set = pd.read_csv(\n        training_set_filename, dtype=dtypes, parse_dates=[\"date\"]\n    )\n    # Read in test data\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes, parse_dates=[\"date\"])\n    # Read in training metadata\n    training_set_metadata = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    # Read in test metadata\n    # Remove the target column type from the metadata dictionary\n    meta_dtypes.pop(\"target\")\n    test_set_metadata = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    # Restore the target column type in the metadata dictionary\n    meta_dtypes[\"target\"] = \"object\"\n\n    return training_set, test_set, training_set_metadata, test_set_metadata\n\n", "CodeBleu_score": 47.01586107678151}, {"code": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)\n\n    import pandas as pd\n\n    # Read training data and metadata\n    training_set = pd.read_csv(\n        training_set_filename,\n        dtype=dtypes,\n        parse_dates=[\"date\"],\n        infer_datetime_format=True,\n    )\n    training_set_metadata = pd.read_csv(\n        training_set_metadata_filename, dtype=meta_dtypes\n    )\n\n    # Read test data and metadata\n    test_set = pd.read_csv(\n        test_set_filename,\n        dtype=dtypes,\n        parse_dates=[\"date\"],\n        infer_datetime_format=True,\n    )\n    meta_dtypes.pop(\"target\")\n    test_set_metadata = pd.read_csv(\n        test_set_metadata_filename, dtype=meta_dtypes\n    )\n    meta_dtypes[\"target\"] = \"int64\"\n\n    return training_set, test_set, training_set_metadata, test_set_metadata\n\n", "CodeBleu_score": 40.1470973148243}]}
{"instance_id": "9e5e02cd73e0", "generation_result": [{"code": "def _show_cmap(cmap)\n    from rcmod import axes_style\n    axes_style.use('white')\n    fig, ax = plt.subplots(1, 1, figsize=(10, 2))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    x = np.linspace(0, 1, 256)\n    x = np.repeat(x[np.newaxis, :], 256, axis=0)\n    ax.pcolormesh(x, cmap=cmap)\n    plt.show()\n\n", "CodeBleu_score": 52.016785976277916}, {"code": "def _show_cmap(cmap)\n    from .rcmod import axes_style\n    import matplotlib.pyplot as plt\n    from matplotlib import cm\n    from numpy import linspace\n    with axes_style(\"white\"):\n        fig, ax = plt.subplots(figsize=(5, 2))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        x = linspace(0, 1, 100)\n        y = linspace(0, 1, 100)\n        X, Y = meshgrid(x, y)\n        ax.pcolormesh(X, Y, X, cmap=cmap)\n        plt.show()\n", "CodeBleu_score": 46.746373958154294}, {"code": "def _show_cmap(cmap)\n    from rcmod import axes_style\n    axes_style.dark_background()\n    fig, ax = plt.subplots(figsize=(10, 2))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    x = np.linspace(0, 1, 256)\n    y = np.linspace(0, 1, 256)\n    X, Y = np.meshgrid(x, y)\n    ax.pcolormesh(X, Y, X, cmap=cmap)\n    plt.show()\n\n", "CodeBleu_score": 48.12243759024978}]}
{"instance_id": "ad0781b5cc38", "generation_result": [{"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    axes = obj.axes\n    if axes is None:\n        return xdata, ydata\n    transData = axes.transData\n    if obj.get_transform() != transData:\n        trans = obj.get_transform() - transData\n        xdata, ydata = trans.transform(np.array([xdata, ydata]).T).T\n    return xdata, ydata\n", "CodeBleu_score": 49.53809741345967}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n\n    # Get the axes for the object\n    ax = obj.axes\n\n    # If the axes are not defined, return the original coordinates\n    if ax is None:\n        return xdata, ydata\n\n    # Get the object's transform\n    obj_transform = obj.get_transform()\n\n    # Get the axes transform\n    ax_transform = ax.transData\n\n    # If the object's transform is equal to the axes transform, return the original coordinates\n    if obj_transform == ax_transform:\n        return xdata, ydata\n\n    # Get the object's coordinates in data coordinates\n    xy_data = obj_transform.transform(ax.transData.transform((xdata, ydata)))\n\n    # Return the data coordinates\n    return xy_data", "CodeBleu_score": 46.74765148984649}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata)\n    # Check whether the axes are defined\n    if obj.axes is None:\n        return xdata, ydata\n    # Get the data transform\n    data_transform = obj.axes.transData\n    # Get the object's transform\n    obj_transform = obj.get_transform()\n    # Check whether the transforms are the same\n    if obj_transform == data_transform:\n        return xdata, ydata\n    # Check whether the transforms are invertible\n    if not obj_transform.is_affine or not data_transform.is_affine:\n        raise ValueError('Cannot transform points from object to data coordinates.')\n    # Combine the transforms\n    composite_transform = obj_transform + data_transform.inverted()\n    # Transform the coordinates\n    xdata, ydata = composite_transform.transform(np.vstack([xdata, ydata]).T).T\n    return xdata, ydata\n\n", "CodeBleu_score": 47.69884856485931}]}
{"instance_id": "bf776cadef8d", "generation_result": [{"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    if data is None:\n        data = self.data\n    if data2 is not None:\n        if not isinstance(data2, list):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)", "CodeBleu_score": 58.7249737889016}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data, 'data')\n    if data2 is not None and not isinstance(data2, list):\n        data2 = self._df[data2]\n    return self._module.kdeplot(data, data2, *args, **kwargs)", "CodeBleu_score": 54.69763518945714}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs)\n    data = self._maybe_target_series(data, \"data\")\n    if data2 is not None:\n        data2 = self._df.get(data2)\n    return self._module.kdeplot(data, data2, *args, **kwargs)", "CodeBleu_score": 39.456607317382726}]}
{"instance_id": "fb44964174d9", "generation_result": [{"code": "def plot_most_used_words(df)\n    df['token'] = df['token'].str.replace('programa', 'programar')\n    df = df[(df['token'].str.isalpha()) & (df['token'].str.len() > 1) & (~df['token'].isin(stop_words))]\n    df = df.groupby('token').size().reset_index(name='counts').sort_values(by='counts', ascending=False).head(20)\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x='token', y='counts', data=df)\n    plt.xlabel('Most Frequent Words')\n    plt.ylabel('Frequency')\n    plt.title('Most Frequent Words in the Dataframe')\n    plt.savefig('most_used_words.png')\n", "CodeBleu_score": 35.94470160681068}, {"code": "def plot_most_used_words(df)\n    \n    # Correcting the word \"programa\" to \"programar\"\n    df[\"token\"] = df[\"token\"].replace(\"programa\", \"programar\")\n    \n    # Filtering the dataframe to include only alphabetic tokens that are longer than one character and are not stop words\n    df = df[df[\"token\"].str.isalpha() & (df[\"token\"].str.len() > 1) & (~df[\"token\"].isin(stopwords.words(\"spanish\")))]\n    \n    # Counting the occurrences of these words\n    count_words = df[\"token\"].value_counts()\n    \n    # Selecting the top 20\n    top_20 = count_words.head(20)\n    \n    # Creating a bar plot showing their frequency\n    top_20.plot.bar(color=\"red\", title=\"Most Used Words\", figsize=(15, 10))\n    \n    # Labelling and saving as an image file\n    plt.xlabel(\"Words\")\n    plt.ylabel(\"Frequency\")\n    plt.savefig(\"most_used_words.png\")", "CodeBleu_score": 38.301598478198464}, {"code": "def plot_most_used_words(df)\n    df[\"token\"] = df[\"token\"].str.replace(\"programa\", \"programar\")\n    df = df[df[\"token\"].str.isalpha() & (df[\"token\"].str.len() > 1) & (~df[\"token\"].isin(stopwords.words(\"portuguese\")))]\n    df = df[\"token\"].value_counts().sort_values(ascending=False).head(20)\n    df.plot.bar(figsize=(10, 5), color=\"green\", edgecolor=\"black\")\n    plt.title(\"Most used words\")\n    plt.xlabel(\"Words\")\n    plt.ylabel(\"Frequency\")\n    plt.savefig(\"most_used_words.png\")\n\n", "CodeBleu_score": 38.69036101532121}]}
{"instance_id": "13414a7eb4e3", "generation_result": [{"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    import numpy as np\n    import bottleneck as bn\n\n    if a.dtype == np.float16:\n        a = a.astype(np.float32)\n    elif a.dtype == np.float32:\n        a = a.astype(np.float64)\n    elif a.dtype == np.int8:\n        a = a.astype(np.int16)\n    elif a.dtype == np.uint8:\n        a = a.astype(np.uint16)\n    elif a.dtype == np.int16:\n        a = a.astype(np.int32)\n    elif a.dtype == np.uint16:\n        a = a.astype(np.uint32)\n    elif a.dtype == np.int32:\n        a = a.astype(np.int64)\n    elif a.dtype == np.uint32:\n        a = a.astype(np.uint64)\n\n    if min_count is None:\n        min_count = window\n\n    depth = int(window / 2)\n    boundary = {\n        0: {\"x\": (depth, depth), \"y\": (0, 0)},\n        1: {\"x\": (0, 0), \"y\": (depth, depth)},\n        -1: {\"x\": (0, 0), \"y\": (depth, depth)},\n    }\n    ag = da.overlap.overlap(a, depth, boundary=boundary, boundary_as_array=True)\n    ag = da.map_blocks(moving_func, ag, window, axis, min_count)\n    ag = da.overlap.trim_internal(ag, depth)\n\n    return ag", "CodeBleu_score": 35.24874488350903}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    import numpy as np\n\n    # Promote the data type of the input array a to handle edge cases\n    if np.issubdtype(a.dtype, np.integer):\n        a = a.astype(np.float64)\n    elif np.issubdtype(a.dtype, np.bool_):\n        a = a.astype(np.float64)\n        # Set the fill value to 0.0\n        fill_value = 0.0\n    else:\n        fill_value = np.nan\n\n    # Calculate the depth of overlap\n    depth = (window - 1) // 2\n\n    # Set the boundary fill values\n    boundary = [(0, 0)] * a.ndim\n    boundary[axis] = (depth, depth)\n\n    # Create an overlapped array ag\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary, trim=False)\n\n    # Apply the moving_func to the overlapped array ag\n    agg = da.map_blocks(moving_func, ag, window, axis=axis, min_count=min_count)\n\n    # Trim the overlapped regions\n    result = da.overlap.trim_internal(agg)\n\n    return result", "CodeBleu_score": 44.21648143892142}, {"code": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)\n    import dask.array as da\n    import numpy as np\n\n    # Promote data type to handle edge cases\n    if np.issubdtype(a.dtype, np.integer):\n        a = a.astype(float)\n    elif np.issubdtype(a.dtype, np.bool_):\n        a = a.astype(np.uint8)\n    if np.issubdtype(a.dtype, np.floating):\n        fill_value = np.nan\n    elif np.issubdtype(a.dtype, np.integer) or np.issubdtype(a.dtype, np.uint8):\n        fill_value = 0\n    elif np.issubdtype(a.dtype, np.bool_):\n        fill_value = False\n\n    # Calculate depth of overlap\n    depth = (window - 1) // 2\n\n    # Set boundary fill values\n    boundary = []\n    for i in range(a.ndim):\n        if i == axis:\n            boundary.append((depth, depth))\n        else:\n            boundary.append(None)\n\n    # Create overlapped array\n    ag = da.overlap.overlap(a, depth, boundary, boundary, boundary)\n\n    # Apply moving_func to overlapped array\n    out = da.map_blocks(\n        moving_func,\n        ag,\n        window,\n        min_count=min_count,\n        axis=axis,\n        dtype=a.dtype,\n        chunks=a.chunks,\n    )\n\n    # Trim overlapped regions\n    out = da.overlap.trim_internal(out)\n\n    return out", "CodeBleu_score": 45.39630555345825}]}
{"instance_id": "68ce25d325d4", "generation_result": [{"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    import numpy as np\n    import dask.array as da\n    from scipy.linalg import lstsq\n\n    # Convert inputs to Dask arrays\n    lhs = da.from_array(lhs, chunks=lhs.shape)\n    rhs = da.from_array(rhs, chunks=rhs.shape)\n\n    # Check if inputs are 1-D or 2-D\n    if len(lhs.shape) == 1:\n        lhs = lhs.reshape(1, -1)\n        rhs = rhs.reshape(1, -1)\n\n    # Check if inputs are empty\n    if lhs.shape[1] == 0 or rhs.shape[1] == 0:\n        raise ValueError(\"Inputs must not be empty\")\n\n    # Check if inputs are compatible\n    if lhs.shape[0] != rhs.shape[0]:\n        raise ValueError(\"Inputs must have the same number of rows\")\n\n    # Check if inputs have the same number of columns\n    if lhs.shape[1] != rhs.shape[1]:\n        raise ValueError(\"Inputs must have the same number of columns\")\n\n    # Check if skipna is True\n    if skipna:\n        # Check if inputs have missing values\n        if np.isnan(lhs).any() or np.isnan(rhs).any():\n            # Check if inputs have more than one column\n            if lhs.shape[1] > 1 or rhs.shape[1] > 1:\n                # Check if inputs have the same number of columns\n                if lhs.shape[1] != rhs.shape[1]:\n                    raise ValueError(\n                        \"Inputs must have the same number of columns\"\n                    )\n                # Reshape inputs to have one column\n                lhs = lhs.reshape(-1, 1)\n                rhs = rhs.reshape(-1, 1)\n            # Check if inputs have more than one row\n            if lhs.shape[0] > 1 or rhs.shape[0] > 1:\n                # Apply polynomial fit function\n                lhs = da.apply_along_axis(", "CodeBleu_score": 29.93911595445728}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    import numpy as np\n    import dask.array as da\n    from scipy.linalg import lstsq\n    import warnings\n    from dask_ml.utils import handle_zeros_in_scale\n\n    if not isinstance(lhs, da.Array):\n        lhs = da.from_array(lhs, chunks=\"auto\")\n    if not isinstance(rhs, da.Array):\n        rhs = da.from_array(rhs, chunks=\"auto\")\n\n    if skipna:\n        if lhs.ndim == 1:\n            lhs = lhs.reshape((lhs.shape[0], 1))\n        if rhs.ndim == 1:\n            rhs = rhs.reshape((rhs.shape[0], 1))\n        lhs, rhs = da.polynomial.polyfit(lhs, rhs, deg=1, rcond=rcond, full=True)\n        coef = lhs[0]\n        residuals = rhs[0]\n    else:\n        if rcond is None:\n            rcond = np.finfo(lhs.dtype).eps * max(lhs.shape)\n        if lhs.ndim == 1:\n            lhs = lhs.reshape((lhs.shape[0], 1))\n        if rhs.ndim == 1:\n            rhs = rhs.reshape((rhs.shape[0], 1))\n        if lhs.shape[0] < lhs.shape[1]:\n            warnings.warn(\n                \"The least-squares solution could not be computed\", RuntimeWarning\n            )\n            coef = da.full((lhs.shape[1], rhs.shape[1]), np.nan)\n            residuals = da.full(rhs.shape, np.nan)\n        else:\n            coef, residuals, _, _ = lstsq(lhs, rhs, rcond)\n            coef, residuals = handle_zeros_in_scale(coef, residuals)\n    return coef, residuals", "CodeBleu_score": 41.259526944250524}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\n    import numpy as np\n    import dask.array as da\n    from dask.array import polyfit\n\n    if not isinstance(lhs, da.Array):\n        lhs = da.from_array(lhs, chunks=lhs.shape)\n    if not isinstance(rhs, da.Array):\n        rhs = da.from_array(rhs, chunks=rhs.shape)\n\n    if skipna:\n        if len(rhs.shape) > 1:\n            rhs = rhs.reshape(rhs.shape[0], -1)\n        if len(lhs.shape) > 1:\n            lhs = lhs.reshape(lhs.shape[0], -1)\n\n        if rcond is not None:\n            coeffs = polyfit(lhs, rhs, 1, rcond=rcond)\n        else:\n            coeffs = polyfit(lhs, rhs, 1)\n\n        residuals = rhs - np.dot(lhs, coeffs[0])\n        return coeffs[0], residuals\n    else:\n        if rcond is not None:\n            coeffs = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n        else:\n            coeffs = da.linalg.lstsq(lhs, rhs)\n\n        residuals = rhs - np.dot(lhs, coeffs[0])\n        return coeffs[0], residuals", "CodeBleu_score": 39.630840615520704}]}
{"instance_id": "c0c2c53b76e0", "generation_result": [{"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs.setdefault(\"meta\", np.ndarray)\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 56.7921307517903}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 70.27898286466454}, {"code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any\n    import dask.array as da\n\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs[\"meta\"] = np.ndarray\n\n    return da.from_array(data, chunks, **kwargs)\n\n", "CodeBleu_score": 70.27898286466454}]}
{"instance_id": "6559019d09a0", "generation_result": [{"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n    if isinstance(self, da.Array):\n        func, args = self.__dask_postpersist__()\n        return (finalize_dask_array, (func, args))\n    else:\n        raise AttributeError(\"Data is not a dask array.\")\n\n", "CodeBleu_score": 41.45838027252458}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n    if isinstance(self, dask.array.Array):\n        func, args = self.__dask_postpersist__()\n        return (\n            finalize_dask_array,\n            (func, args),\n        )\n    raise AttributeError(\n        \"The data must be a dask array to be persisted.\"\n    )\n\n", "CodeBleu_score": 47.187569677821706}, {"code": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]\n    if isinstance(self, da.Array):\n        func, args = self.__dask_postpersist__()\n        return (\n            finalize_array,\n            (func, args, self.shape, self.dtype),\n        )\n    else:\n        raise AttributeError(\n            \"Can only persist dask arrays. Got \"\n            f\"{type(self).__name__} instead.\",\n        )\n\n", "CodeBleu_score": 46.845896299440305}]}
{"instance_id": "4a2801bb3851", "generation_result": [{"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\n    def f(*args, **kwargs):\n        if is_duck_dask_array(args[0]):\n            if isinstance(dask_module, str):\n                dask_module = importlib.import_module(dask_module)\n            func = getattr(dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 52.518438200335225}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\n    def f(*args, **kwargs):\n        \"\"\"\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        if is_duck_dask_array(*args, **kwargs):\n            if isinstance(dask_module, str):\n                dask_module = importlib.import_module(dask_module)\n            func = getattr(dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 52.718099467876044}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\n\n    def f(*args, **kwargs):\n        if is_duck_dask_array(*args):\n            if isinstance(dask_module, str):\n                dask_module = importlib.import_module(dask_module)\n            func = getattr(dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n        return func(*args, **kwargs)\n\n    return f\n\n", "CodeBleu_score": 52.547898023931964}]}
{"instance_id": "eef8d746d01e", "generation_result": [{"code": "def lazy_array_equiv(arr1, arr2)\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    try:\n        import dask.array as da\n    except ImportError:\n        return None\n    if isinstance(arr1, da.Array) and isinstance(arr2, da.Array):\n        return da.tokenize(arr1) == da.tokenize(arr2)\n    return None\n\n", "CodeBleu_score": 62.742526002668406}, {"code": "def lazy_array_equiv(arr1, arr2)\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    try:\n        from dask.base import tokenize\n    except ImportError:\n        return None\n    if not is_dask_array(arr1) or not is_dask_array(arr2):\n        return None\n    return tokenize(arr1) == tokenize(arr2)", "CodeBleu_score": 78.84660836924105}, {"code": "def lazy_array_equiv(arr1, arr2)\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if has_dask:\n        from dask.base import tokenize\n        arr1 = tokenize(arr1)\n        arr2 = tokenize(arr2)\n        if arr1 == arr2:\n            return True\n        else:\n            return None\n    else:\n        return None", "CodeBleu_score": 79.11718350971042}]}
{"instance_id": "15e830ff5fcb", "generation_result": [{"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    import numpy as np\n    import dask.array as da\n    import random\n    random.seed(seed)\n    if chunks is None:\n        return np.random.randn(*shape) * np.sqrt(frac_nan)\n    else:\n        x = da.random.RandomState(seed).normal(size=shape, chunks=chunks)\n        if frac_nan is not None:\n            x = x.map_blocks(lambda x: np.where(np.random.random(size=x.shape) < frac_nan, np.nan, x))\n        return x", "CodeBleu_score": 50.13827689049661}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n    import numpy as np\n    import dask.array as da\n    if chunks is None:\n        np.random.seed(seed)\n        if frac_nan is None:\n            return np.random.randn(*shape)\n        else:\n            x = np.random.randn(*shape)\n            x[np.random.rand(*shape) < frac_nan] = np.nan\n            return x\n    else:\n        np.random.seed(seed)\n        if frac_nan is None:\n            return da.random.randn(shape, chunks=chunks)\n        else:\n            x = da.random.randn(shape, chunks=chunks)\n            x[da.random.rand(*shape, chunks=chunks) < frac_nan] = np.nan\n            return x", "CodeBleu_score": 40.20835001951952}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\n\n    import numpy as np\n    import dask.array as da\n    import math\n\n    if chunks is None:\n        np.random.seed(seed)\n        return np.random.randn(*shape) * math.sqrt(frac_nan)\n    else:\n        np.random.seed(seed)\n        x = da.random.RandomState(seed).random_normal(shape, chunks=chunks)\n        if frac_nan is not None:\n            x = da.random.RandomState(seed).random(shape, chunks=chunks)\n            x = da.where(x < frac_nan, np.nan, x)\n        return x", "CodeBleu_score": 50.01947678609437}]}
{"instance_id": "5b57abbbe38b", "generation_result": [{"code": "def analyst_price_target(self) -> pd.DataFrame\n\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n\n    return self._analyst_price_target\n\n", "CodeBleu_score": 81.06081450739325}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target\n\n", "CodeBleu_score": 81.06081450739325}, {"code": "def analyst_price_target(self) -> pd.DataFrame\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError(\"analyst_price_target\")\n    return self._analyst_price_target\n\n", "CodeBleu_score": 81.06081450739325}]}
{"instance_id": "592e414671e9", "generation_result": [{"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n\n    # validate the name\n    if name not in self.allowed_financials:\n        raise ValueError(\n            \"{} is not a valid financial statement. Please choose from the following: {}\".format(\n                name, self.allowed_financials\n            )\n        )\n\n    # validate the timescale\n    if timescale not in self.allowed_timescales:\n        raise ValueError(\n            \"{} is not a valid timescale. Please choose from the following: {}\".format(\n                timescale, self.allowed_timescales\n            )\n        )\n\n    # create a financials table\n    try:\n        financials_table = self._create_financials_table(\n            name, timescale, proxy=proxy\n        )\n    except Exception as e:\n        logging.error(\n            \"An error occurred while creating a financials table: {}\".format(e)\n        )\n        return pd.DataFrame()\n\n    return financials_table", "CodeBleu_score": 35.041485581597016}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    # Validate the name\n    if name not in self.allowed_statements:\n        raise ValueError(f\"{name} is not an allowed financial statement. Please choose from {self.allowed_statements}\")\n\n    # Validate the timescale\n    if timescale not in self.allowed_periods:\n        raise ValueError(f\"{timescale} is not an allowed period. Please choose from {self.allowed_periods}\")\n\n    # Attempt to create a financials table\n    try:\n        table = self._create_financials_table(name, timescale, proxy)\n    except Exception as e:\n        logging.error(f\"An error occurred while creating the financials table: {e}\")\n        table = pd.DataFrame()\n\n    return table\n", "CodeBleu_score": 40.94956455765367}, {"code": "def _fetch_time_series(self, name, timescale, proxy=None)\n    import pandas as pd\n    import requests\n    import logging\n    from datetime import datetime\n    from . import _financial_statements\n    from . import _timescale\n    from . import _validate\n\n    # Validate name and timescale\n    _validate.validate_name(name)\n    _validate.validate_timescale(timescale)\n\n    # Create a dictionary of allowed financial statements and periods\n    statements = _financial_statements.financial_statements()\n    periods = _timescale.timescale()\n\n    # Create a dictionary of financial statements and periods\n    statement_dict = {\n        \"income_statement\": \"income-statement\",\n        \"balance_sheet\": \"balance-sheet\",\n        \"cash_flow\": \"cash-flow\",\n    }\n    period_dict = {\n        \"annual\": \"annual\",\n        \"quarterly\": \"quarter\",\n    }\n\n    # Create a dictionary of financial statements and periods\n    statement = statement_dict[name]\n    period = period_dict[timescale]\n\n    # Create a dictionary of financial statements and periods\n    statement_period_dict = {\n        \"income_statement\": {\n            \"annual\": \"income-statement\",\n            \"quarterly\": \"income-statement/quarterly\",\n        },\n        \"balance_sheet\": {\n            \"annual\": \"balance-sheet\",\n            \"quarterly\": \"balance-sheet/quarterly\",\n        },\n        \"cash_flow\": {\n            \"annual\": \"cash-flow\",\n            \"quarterly\": \"cash-flow/quarterly\",\n        },\n    }\n\n    # Create a dictionary of financial statements and periods\n    statement_period = statement_period_dict[name][timescale]\n\n    # Create a dictionary of financial statements and periods\n    statement_period_dict = {\n        \"income_statement\": {\n            \"annual\": \"income-statement\",\n            \"quarterly\": \"income-statement/quarterly\",\n        },\n        \"balance_sheet\": {\n            \"annual", "CodeBleu_score": 24.911128681927806}]}
{"instance_id": "af01d9bae091", "generation_result": [{"code": "def _parse_fund_ownership(self, data)\n    self._mutualfund = pd.DataFrame()\n    if 'ownershipList' in data:\n        for owner in data['ownershipList']:\n            del owner['maxAge']\n            self._mutualfund = self._mutualfund.append(owner, ignore_index=True)\n    if not self._mutualfund.empty:\n        self._mutualfund['reportDate'] = pd.to_datetime(self._mutualfund['reportDate'])\n        self._mutualfund.rename(columns={'reportDate': 'Date', 'reportedHolding': 'Shares', 'reportedTitle': 'Name', 'reportedOwner': 'Owner'}, inplace=True)\n\n", "CodeBleu_score": 33.19952285393409}, {"code": "def _parse_fund_ownership(self, data)\n    self._mutualfund = pd.DataFrame()\n    if data.get('fundOwnershipList'):\n        ownership_list = data['fundOwnershipList']\n        for owner in ownership_list:\n            owner_data = owner['ownershipList']\n            owner_data.pop('maxAge')\n            self._mutualfund = self._mutualfund.append(owner_data, ignore_index=True)\n        if not self._mutualfund.empty:\n            self._mutualfund['reportDate'] = pd.to_datetime(self._mutualfund['reportDate'])\n            self._mutualfund.rename(columns={'reportDate': 'Date', 'positionDirect': 'Direct', 'positionIndirect': 'Indirect', 'positionSummary': 'Total', 'positionDirectCurrency': 'Direct (Currency)', 'positionIndirectCurrency': 'Indirect (Currency)', 'positionSummaryCurrency': 'Total (Currency)'}, inplace=True)\n\n", "CodeBleu_score": 30.999801275883787}, {"code": "def _parse_fund_ownership(self, data)\n    self._mutualfund = pd.DataFrame()\n    if 'ownershipList' in data:\n        for owner in data['ownershipList']:\n            del owner['maxAge']\n            owner['reportDate'] = pd.to_datetime(owner['reportDate'])\n            self._mutualfund = self._mutualfund.append(owner, ignore_index=True)\n        if not self._mutualfund.empty:\n            self._mutualfund.rename(columns={'reportDate': 'Date', 'reportedOwner': 'Owner', 'reportedOwnerShares': 'Shares', 'reportedOwnerSharesPercent': 'Percentage'}, inplace=True)\n\n", "CodeBleu_score": 33.9499731743841}]}
{"instance_id": "2b5c6f1253db", "generation_result": [{"code": "def currency(self)\n    if self.currency:\n        return self.currency\n    else:\n        metadata = self.get_metadata()\n        self.currency = metadata['currency']\n        return self.currency\n\n", "CodeBleu_score": 42.237334789172124}, {"code": "def currency(self)\n    if self.currency:\n        return self.currency\n    else:\n        metadata = self.get_metadata()\n        currency = metadata[\"currency\"]\n        self.currency = currency\n        return currency\n\n", "CodeBleu_score": 38.69579263725126}, {"code": "def currency(self)\n    if self.currency:\n        return self.currency\n    else:\n        metadata = self.get_metadata()\n        self.currency = metadata[\"currency\"]\n        return self.currency\n\n", "CodeBleu_score": 42.842326422300495}]}
{"instance_id": "16b80be60e3b", "generation_result": [{"code": "def recommendations(self) -> pd.DataFrame\n\n    if self._recommendations is None:\n        self._recommendations = recommendationTrend(self.ticker)\n        if self._recommendations is None:\n            self._recommendations = pd.DataFrame()\n        else:\n            try:\n                self._recommendations = self._recommendations[\"trend\"]\n            except KeyError:\n                raise YFDataException(\"Error parsing recommendation data.\")\n\n    return self._recommendations\n\n", "CodeBleu_score": 40.079261631660415}, {"code": "def recommendations(self) -> pd.DataFrame\n    if self._recommendations is None:\n        self._recommendations = recommendationTrend(self.ticker, self.proxy)\n        if self._recommendations is None:\n            self._recommendations = pd.DataFrame()\n        else:\n            try:\n                self._recommendations = self._recommendations[\"trend\"][\"trend\"]\n            except KeyError:\n                raise YFDataException(\"Error parsing recommendation trend data\")\n    return self._recommendations", "CodeBleu_score": 43.66960895075638}, {"code": "def recommendations(self) -> pd.DataFrame\n    if self._recommendations is None:\n        self._recommendations = recommendationTrend(self.ticker)\n        if self._recommendations is None:\n            self._recommendations = pd.DataFrame()\n        else:\n            try:\n                self._recommendations = self._recommendations['trend']['history']['trend']\n            except Exception as e:\n                raise YFDataException(\"Error parsing recommendation data: \" + str(e))\n    return self._recommendations", "CodeBleu_score": 42.613076749660436}]}
{"instance_id": "abaf8c2a3ecc", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from time import time\n\n    if not hasattr(clf, 'n_clusters'):\n        raise AttributeError(\"The classifier must have the attribute 'n_clusters'\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(2, 11)\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.get_figure()\n\n    if n_jobs > 1:\n        from joblib import Parallel, delayed\n        n_jobs = min(n_jobs, len(cluster_ranges))\n        cluster_scores = Parallel(n_jobs=n_jobs)(delayed(score_cluster)(clf, X, n_clusters, show_cluster_time) for n_clusters in cluster_ranges)\n    else:\n        cluster_scores = [score_cluster(clf, X, n_clusters, show_cluster_time) for n_clusters in cluster_ranges]\n\n    cluster_scores = np.array(cluster_scores)\n    ax.plot(cluster_ranges, cluster_scores[:,0], 'o-')\n    ax.set_xlabel('Number of Clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of Squared Errors', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, cluster_scores[:,1], 'o-')\n        ax2.set_ylabel('Clustering Time (s)', fontsize=text_fontsize)\n\n    return ax\n", "CodeBleu_score": 54.1756203493386}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n\n    if not hasattr(clf, \"n_clusters\"):\n        raise AttributeError(\"Classifier must have the attribute 'n_clusters'\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    if show_cluster_time:\n        ax2 = ax.twinx()\n\n    scores = []\n    times = []\n    for n_clusters in cluster_ranges:\n        clf.set_params(n_clusters=n_clusters)\n        t0 = time.time()\n        clf.fit(X)\n        t1 = time.time()\n        scores.append(clf.inertia_)\n        times.append(t1 - t0)\n\n    ax.plot(cluster_ranges, scores, 'bx-')\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('SSE', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n\n    if show_cluster_time:\n        ax2.plot(cluster_ranges, times, 'r.-')\n        ax2.set_ylabel('Clustering Time (s)', fontsize=text_fontsize)\n\n    return ax", "CodeBleu_score": 46.08642597554996}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.metrics import silhouette_score\n    from sklearn.metrics import calinski_harabasz_score\n    from sklearn.metrics import davies_bouldin_score\n    from sklearn.metrics import adjusted_rand_score\n    from sklearn.metrics import adjusted_mutual_info_score\n    from sklearn.metrics import homogeneity_score\n    from sklearn.metrics import completeness_score\n    from sklearn.metrics import v_measure_score\n    from sklearn.metrics import fowlkes_mallows_score\n    from sklearn.metrics import mutual_info_score\n    from sklearn.metrics import normalized_mutual_info_score\n    from sklearn.metrics import adjusted_mutual_info_score\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import roc_auc_score\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import recall_score\n    from sklearn.metrics import f1_score\n    from sklearn.metrics import cohen_kappa_score\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import plot_confusion_matrix\n    from sklearn.metrics import roc_curve\n    from sklearn.metrics import roc_auc_score\n    from sklearn.metrics import precision_recall_curve\n    from sklearn.metrics import average_precision_score\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import recall_score\n    from sklearn.metrics import f1_score\n    from sklearn.metrics import log_loss\n    from sklearn.metrics import hinge_loss\n    from sklearn.metrics import brier_score_loss\n    from sklearn.metrics import multilabel_confusion_matrix\n    from sklearn.metrics import r2_score\n    from sklearn.metrics import mean_squ", "CodeBleu_score": 9.833514738025183}]}
{"instance_id": "8a90815c4ee1", "generation_result": [{"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    start_time = time.time()\n    clf_clone = clone(clf)\n    clf_clone.set_params(n_clusters=n_clusters)\n    clf_clone.fit(X)\n    end_time = time.time()\n    return clf_clone.score(X), end_time - start_time\n\n", "CodeBleu_score": 31.29761189566691}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    start_time = time.time()\n    clf_clone = clone(clf)\n    clf_clone.set_params(n_clusters=n_clusters)\n    clf_clone.fit(X)\n    end_time = time.time()\n    return clf_clone.score(X), end_time - start_time\n\n", "CodeBleu_score": 31.29761189566691}, {"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\n    t0 = time.time()\n    clf_clone = clone(clf)\n    clf_clone.set_params(n_clusters=n_clusters)\n    clf_clone.fit(X)\n    t1 = time.time()\n    return clf_clone.score(X), t1 - t0\n\n", "CodeBleu_score": 32.329035590036185}]}
{"instance_id": "da40bd5dd81a", "generation_result": [{"code": "def clustering_factory(clf)\n    # Check if the clustering object has the required methods\n    if not hasattr(clf, 'fit') or not hasattr(clf, 'fit_predict'):\n        raise TypeError(f\"{clf} is not a valid clusterer instance!\")\n\n    # Define additional methods\n    def plot_silhouette(self, X, y, **kwargs):\n        \"\"\"The method plot_silhouette takes the following arguments:\\nX: A 2-dimensional array-like object containing the features of the data points.\\ny: A 1-dimensional array-like object containing the labels of the data points.\\nkwargs: Additional keyword arguments passed to the plot function.\"\"\"\n        # Import libraries\n        import matplotlib.pyplot as plt\n        from sklearn.metrics import silhouette_score, silhouette_samples\n        from sklearn.preprocessing import StandardScaler\n\n        # Scale the features\n        X_scaled = StandardScaler().fit_transform(X)\n\n        # Compute the silhouette score\n        self.silhouette_score_ = silhouette_score(X_scaled, self.fit_predict(X_scaled))\n\n        # Compute the silhouette samples\n        self.silhouette_samples_ = silhouette_samples(X_scaled, self.fit_predict(X_scaled))\n\n        # Create a figure\n        fig = plt.figure(figsize=(10, 6))\n\n        # Create the plot\n        plt.plot(self.silhouette_samples_, 'o', color='#666666', alpha=0.5)\n        plt.axvline(self.silhouette_score_, color='#111111', linestyle='--', label='Silhouette Score')\n        plt.axhline(0, color='#aaaaaa', linestyle='--', alpha=0.5)\n        plt.axhline(1, color='#aaaaaa', linestyle='--', alpha=0.5)\n        plt.ylim([-0.1, 1.1])\n        plt.xlabel('Silhouette Samples')\n        plt.ylabel('Silhouette Score')", "CodeBleu_score": 25.053528404980725}, {"code": "def clustering_factory(clf)\n\n    # Check if the clustering object has the required methods\n    if not hasattr(clf, 'fit'):\n        raise TypeError(\"The clustering object should have a fit method\")\n    if not hasattr(clf, 'fit_predict'):\n        raise TypeError(\"The clustering object should have a fit_predict method\")\n\n    # Define additional methods\n    def plot_silhouette(self, X, labels, ax=None, **kwargs):\n        \"\"\"Plot the silhouette plot for the clustering object.\"\"\"\n        from sklearn.metrics import silhouette_samples, silhouette_score\n        import matplotlib.pyplot as plt\n        from matplotlib import cm\n\n        # Create a subplot with 1 row and 2 columns\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n\n        # The 1st subplot is the silhouette plot\n        # The silhouette coefficient can range from -1, 1 but in this example all\n        # lie within [-0.1, 1]\n        ax1.set_xlim([-0.1, 1])\n        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n        # plots of individual clusters, to demarcate them clearly.\n        ax1.set_ylim([0, len(X) + (self.n_clusters + 1) * 10])\n\n        # The silhouette_score gives the average value for all the samples.\n        # The silhouette_samples function gives the value for each sample.\n        sample_silhouette_values = silhouette_samples(X, labels)\n        silhouette_avg = silhouette_score(X, labels)\n\n        # Compute the silhouette scores for each sample\n        y_lower = 10\n        for i in range(self.n_clusters):\n            # Aggregate the silhouette scores for samples belonging to\n            # cluster i, and sort them\n            ith_cluster_silhouette_values = \\\n                sample_silhouette_values[labels == i]\n\n            ith_cluster_silhouette_values.", "CodeBleu_score": 27.128814694536445}, {"code": "def clustering_factory(clf)\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.metrics import silhouette_score\n    from sklearn.metrics import silhouette_samples\n    from sklearn.metrics import calinski_harabasz_score\n    from sklearn.metrics import davies_bouldin_score\n    from sklearn.metrics import adjusted_rand_score\n    from sklearn.metrics import adjusted_mutual_info_score\n    from sklearn.metrics import completeness_score\n    from sklearn.metrics import homogeneity_score\n    from sklearn.metrics import v_measure_score\n    from sklearn.metrics import mutual_info_score\n    from sklearn.metrics import normalized_mutual_info_score\n    from sklearn.metrics import fowlkes_mallows_score\n    from sklearn.metrics import contingency_matrix\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import recall_score\n    from sklearn.metrics import f1_score\n    from sklearn.metrics import roc_auc_score\n    from sklearn.metrics import roc_curve\n    from sklearn.metrics import precision_recall_curve\n    from sklearn.metrics import auc\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import cohen_kappa_score\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import median_absolute_error\n    from sklearn.metrics import r2_score\n    from sklearn.metrics import mean_squared_log_error\n    from sklearn.metrics import explained_variance_score\n    from sklearn.metrics import max_error\n    from sklearn.metrics import mean_poisson_deviance\n    from sklearn.metrics import mean_gamma_deviance\n    from sklearn.metrics import mean_absolute_percentage_error", "CodeBleu_score": 4.9807893734029856}]}
{"instance_id": "b8d8ba3c0b1b", "generation_result": [{"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    from sklearn.model_selection import learning_curve\n    from sklearn.model_selection import ShuffleSplit\n    import matplotlib.pyplot as plt\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    if cv is None:\n        cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n\n    if train_sizes is None:\n        train_sizes = np.linspace(0.1, 1.0, 5)\n\n    train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=cv,\n                                                            n_jobs=n_jobs,\n                                                            train_sizes=train_sizes,\n                                                            scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.legend(loc=\"best\")\n    ax.set_xlabel(\"Training examples\", fontsize=text_", "CodeBleu_score": 67.21127060549443}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import learning_curve\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, scoring=scoring, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n\n    ax.legend(loc=\"best\")\n\n    return ax\n\n", "CodeBleu_score": 74.90183120474605}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\n    from sklearn.model_selection import learning_curve\n    from matplotlib import pyplot as plt\n    import numpy as np\n\n    if ax is None:\n        _, ax = plt.subplots(figsize=figsize)\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training Size\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, train_sizes=train_sizes, n_jobs=n_jobs, scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.grid()\n\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n\n    ax.legend(loc=\"best\")\n\n    return ax\n\n", "CodeBleu_score": 74.17541957840729}]}
{"instance_id": "e6e76eb35945", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    if not hasattr(clf, \"n_clusters\"):\n        raise TypeError(\"The classifier must have an attribute n_clusters.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n    cluster_nums = []\n    scores = []\n    for n_clusters in cluster_ranges:\n        clf.n_clusters = n_clusters\n        clf.fit(X)\n        cluster_nums.append(n_clusters)\n        scores.append(abs(clf.score(X)))\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(cluster_nums, scores)\n    ax.grid(True)\n    ax.set_xlabel(\"Number of Clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    return ax\n\n", "CodeBleu_score": 52.53759519024184}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier clf must have an attribute n_clusters.\")\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n    clfs = [clf.__class__(n_clusters=n) for n in cluster_ranges]\n    scores = [c.fit(X).score(X) for c in clfs]\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    ax.plot(cluster_ranges, np.abs(scores))\n    ax.set_xlabel(\"Number of Clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.grid()\n    return ax", "CodeBleu_score": 50.06372507062038}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier must have an attribute n_clusters.\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n\n    classifiers = [clf.__class__(n_clusters=i) for i in cluster_ranges]\n\n    scores = [clf.fit(X).score(X) for clf in classifiers]\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    ax.plot(cluster_ranges, np.abs(scores), marker='o')\n    ax.set_xlabel(\"Number of Clusters\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.grid(True)\n\n    return ax", "CodeBleu_score": 51.65772934560386}]}
{"instance_id": "6f4530c8a20f", "generation_result": [{"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    from sklearn.decomposition import PCA\n    import matplotlib.pyplot as plt\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.pipeline import make_pipeline\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.pipeline import make_pipeline\n    import numpy as np\n\n    # Apply PCA transformation to the data\n    pipeline = make_pipeline(StandardScaler(), PCA(2))\n    X_r2 = pipeline.fit_transform(X)\n\n    # Set up the plot\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    # Plot the projection\n    colors = ['#1f77b4', '#ff7f0e']\n    markers = ['o', '^']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(X_r2[y == l, 0], X_r2[y == l, 1],\n                    c=c, label=l, marker=m)\n\n    # Label the axes and add a legend\n    plt.xlabel('PC 1', fontsize=text_fontsize)\n    plt.ylabel('PC 2', fontsize=text_fontsize)\n    plt.legend(title='Class', loc='upper left', fontsize=text_fontsize)\n    plt.title(title, fontsize=title_fontsize)\n\n    return ax", "CodeBleu_score": 50.84905348804569}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    from sklearn.decomposition import PCA\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Create a PCA instance: model\n    model = PCA()\n\n    # Apply the fit_transform method of model to the scaled data: pca_features\n    pca_features = model.fit_transform(X)\n\n    # Assign the 0th and 1st principal components to the x and y columns of a DataFrame: xs, ys\n    xs = pca_features[:, 0]\n    ys = pca_features[:, 1]\n\n    # Set up the figure and axes\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    # Make a scatter plot of the PCA features\n    scatter = ax.scatter(xs, ys, c=y, cmap=cmap)\n\n    # Set the scatter plot title\n    ax.set_title(title, fontsize=title_fontsize)\n\n    # Specify axis labels\n    ax.set_xlabel('Principal Component 1', fontsize=text_fontsize)\n    ax.set_ylabel('Principal Component 2', fontsize=text_fontsize)\n\n    # Specify axis limits\n    ax.set_xlim(-15, 15)\n    ax.set_ylim(-15, 15)\n\n    # Make a legend and show the plot\n    ax.legend(handles=scatter.legend_elements()[0], labels=clf.classes_,\n              loc='upper left', title=\"Classes\")\n\n    # Return the Axes object with the scatter plot\n    return ax", "CodeBleu_score": 44.88218514140434}, {"code": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    X_transformed = clf.transform(X)\n    x_labels = ['PC {}'.format(i) for i in range(1, X_transformed.shape[1] + 1)]\n\n    ax.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y, cmap=cmap,\n               edgecolor='k')\n    ax.set_xlabel(x_labels[0], fontsize=text_fontsize)\n    ax.set_ylabel(x_labels[1], fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.legend(handles=ax.legend_.legendHandles, labels=np.unique(y),\n              loc='upper left', title='Class', fontsize=text_fontsize)\n\n    return ax\n\n", "CodeBleu_score": 42.75867312493726}]}
{"instance_id": "e7c2775d024c", "generation_result": [{"code": "def barplot(ax)\n    # Create clipping paths\n    clip_path1 = clip_path(ax, -1, 16, 0, 1)\n    clip_path2 = clip_path(ax, -1, 16, 1, 2)\n\n    # Create data for 16 bars\n    Y1 = np.random.randint(0, 100, 16)\n    Y2 = np.random.randint(0, 100, 16)\n    X = np.arange(16)\n\n    # Plot four sets of bars\n    ax.bar(X, Y1, color='lightgray', edgecolor='gray', width=0.75,\n           clip_path=clip_path1)\n    ax.bar(X, Y2, color='gray', edgecolor='gray', width=0.75,\n           clip_path=clip_path2)\n    ax.bar(X, Y1, color='red', edgecolor='red', width=0.75,\n           clip_path=clip_path1)\n    ax.bar(X, Y2, color='darkred', edgecolor='darkred', width=0.75,\n           clip_path=clip_path2)\n\n    # Set x-axis limits\n    ax.set_xlim(-1, 16)\n\n    # Add text labels\n    ax.text(0, -1, 'Bar plot', color='black', ha='center')\n    ax.text(0, -1.5, 'ax.bar()', color='blue', fontname='monospace',\n            ha='center')\n\n", "CodeBleu_score": 37.898768517548206}, {"code": "def barplot(ax)\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib.patches import Polygon\n\n    # Generate clipping paths\n    clip_path1 = clip_path(ax, -1, 1, -1, 1, 0.5, 0.5)\n    clip_path2 = clip_path(ax, -1, 1, -1, 1, -0.5, -0.5)\n\n    # Generate data for 16 bars\n    y1 = np.random.uniform(0.25, 1.0, 16)\n    y2 = np.random.uniform(0.25, 1.0, 16)\n    x = np.arange(16)\n\n    # Plot four sets of bars on the provided axis\n    ax.bar(x, y1, width=0.35, color='lightgray', edgecolor='black', linewidth=1.2, clip_path=clip_path1)\n    ax.bar(x, y2, width=0.35, color='gray', edgecolor='black', linewidth=1.2, clip_path=clip_path2)\n    ax.bar(x, y1, width=0.35, color='red', edgecolor='black', linewidth=1.2, clip_path=clip_path1)\n    ax.bar(x, y2, width=0.35, color='darkred', edgecolor='black', linewidth=1.2, clip_path=clip_path2)\n\n    # Set x-axis limits\n    ax.set_xlim(-1, 16)\n\n    # Add text labels\n    ax.text(0, -1.25, 'Bar plot', color='black', fontsize=14, ha='center')\n    ax.text(0, -1.45, r'$\\mathit{ax.bar()}$', color='blue', fontsize=14, fontname='monospace', ha='center')\n", "CodeBleu_score": 40.37174029199418}, {"code": "def barplot(ax)\n    from matplotlib import pyplot as plt\n    from matplotlib.patches import Rectangle\n    import numpy as np\n\n    # generate two clipping paths\n    clip_path1 = clip_path(ax, 0.5, 1.5)\n    clip_path2 = clip_path(ax, 1.5, 2.5)\n\n    # generate data for 16 bars\n    y1 = np.random.randint(1, 100, 16)\n    y2 = np.random.randint(1, 100, 16)\n\n    # plot the bars\n    ax.bar(np.arange(16), y1, width=1, color='lightgray', edgecolor='gray',\n           clip_path=clip_path1)\n    ax.bar(np.arange(16), y2, width=1, color='darkgray', edgecolor='gray',\n           clip_path=clip_path2)\n    ax.bar(np.arange(16), y1, width=1, color='red', edgecolor='red',\n           clip_path=clip_path1)\n    ax.bar(np.arange(16), y2, width=1, color='orangered', edgecolor='red',\n           clip_path=clip_path2)\n\n    # set the x-axis limits\n    ax.set_xlim(-1, 16)\n\n    # add text labels\n    ax.text(0.5, -0.25, 'Bar plot', color='black')\n    ax.text(0.5, -0.4, r'$\\mathtt{ax.bar()}$', color='blue')\n\n", "CodeBleu_score": 34.2694744657222}]}
{"instance_id": "0ec1d146a32b", "generation_result": [{"code": "def split(n_segment)\n    import numpy as np\n    import math\n    import matplotlib.pyplot as plt\n\n    # Calculate the width of each segment\n    segment_width = 1 / n_segment\n    # Calculate the padding between segments\n    padding = segment_width / 2\n    # Determine the starting and ending positions of each segment\n    x0 = np.linspace(0, 1 - segment_width, n_segment)\n    x1 = np.linspace(segment_width, 1, n_segment)\n    # Plot the segments\n    plt.figure(figsize=(10, 5))\n    for i in range(n_segment):\n        plt.plot([x0[i], x1[i]], [0, 0], 'k-', linewidth=1)\n    plt.plot([0, 1], [0, 0], 'k--', linewidth=1)\n    plt.xlim(0, 1)\n    plt.ylim(-0.1, 0.1)\n    plt.xlabel('Position')\n    plt.ylabel('Depth')\n    plt.title('Segmented Wellbore')\n    plt.show()\n    return x0, x1", "CodeBleu_score": 22.058806618535577}, {"code": "def split(n_segment)\n\n    # Import the required libraries\n    import numpy as np\n\n    # Compute the width of each segment\n    width = 1 / n_segment\n\n    # Compute the padding between segments\n    padding = width / 2\n\n    # Create an array of starting positions\n    x0 = np.arange(0, 1, width)\n\n    # Create an array of ending positions\n    x1 = np.arange(width, 1 + width, width)\n\n    # Return the starting and ending positions\n    return x0, x1\n\n", "CodeBleu_score": 21.403764754432572}, {"code": "def split(n_segment)\n    import numpy as np\n    import math\n    import matplotlib.pyplot as plt\n\n    # Calculate the width of each segment\n    width = 1 / n_segment\n\n    # Calculate the padding between segments\n    padding = width / 2\n\n    # Calculate the starting and ending positions of each segment\n    x0 = np.arange(0, 1, width)\n    x1 = x0 + width\n\n    # Plot the segments\n    plt.plot(x0, np.zeros(x0.shape), 'ro')\n    plt.plot(x1, np.zeros(x1.shape), 'bo')\n    plt.show()\n\n    return x0, x1", "CodeBleu_score": 23.90574826682753}]}
{"instance_id": "604a3e93ac0d", "generation_result": [{"code": "def pixelated_text(dpi=100)\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    fig = plt.figure(figsize=(1, 1), dpi=dpi)\n    canvas = fig.canvas\n    plt.axis('off')\n    plt.text(0.5, 0.5, 'a', fontsize=75, color='black')\n    canvas.draw()\n    image = np.array(canvas.renderer.buffer_rgba())\n    image = image.reshape((dpi, dpi, 4))\n    image = np.rollaxis(image, 2)\n    return image\n\n", "CodeBleu_score": 48.14940192812319}, {"code": "def pixelated_text(dpi=100)\n\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    fig = plt.figure(figsize=(1, 1), dpi=dpi)\n    canvas = fig.canvas\n    fig.text(0.5, 0.5, 'a', fontsize=75)\n    ax = plt.gca()\n    ax.set_axis_off()\n    canvas.draw()\n    image = np.frombuffer(canvas.tostring_argb(), dtype=np.uint8)\n    image = image.reshape(dpi, dpi, 4)\n    image = np.roll(image, 3, axis=2)\n    return image", "CodeBleu_score": 64.07658018879965}, {"code": "def pixelated_text(dpi=100)\n    import numpy as np\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    canvas = fig.canvas\n    fig.text(0.5, 0.5, 'a', fontsize=75)\n    fig.axes[0].set_axis_off()\n    canvas.draw()\n    image = np.array(canvas.renderer._renderer)\n    image = image.reshape(dpi, dpi, 4)\n    image = np.rollaxis(image, 2, 0)\n    return image\n\n", "CodeBleu_score": 44.41859901822089}]}
{"instance_id": "2a7fc7d560ea", "generation_result": [{"code": "def curve()\n    import numpy as np\n    import random\n    import matplotlib.pyplot as plt\n    import scipy.stats as stats\n    import time\n    from scipy.optimize import curve_fit\n\n    # Randomly determine the number of components\n    num_comp = random.randint(2, 10)\n\n    # Randomly determine the centers of the components\n    centers = np.random.uniform(0, 100, num_comp)\n\n    # Randomly determine the widths of the components\n    widths = np.random.uniform(0.1, 1, num_comp)\n\n    # Randomly determine the scales of the components\n    scales = np.random.uniform(0.1, 1, num_comp)\n\n    # Normalize the widths and scales so they sum to specific values\n    widths = widths / np.sum(widths)\n    scales = scales / np.sum(scales)\n\n    # Initialize an array of zeros\n    curve = np.zeros(100)\n\n    # Iteratively add scaled Gaussian components based on the calculated parameters\n    for i in range(num_comp):\n        curve += scales[i] * stats.norm.pdf(np.linspace(0, 100, 100), centers[i], widths[i])\n\n    # Return the resulting composite curve\n    return curve\n\n", "CodeBleu_score": 30.633713580195188}, {"code": "def curve()\n    # import numpy as np\n    # import random\n    # import matplotlib.pyplot as plt\n    # from scipy.stats import norm\n    #\n    # # Determine the number of Gaussian components\n    # num_components = random.randint(3, 10)\n    #\n    # # Determine the centers of the Gaussian components\n    # centers = np.zeros(num_components)\n    # for i in range(num_components):\n    #     centers[i] = random.uniform(-10, 10)\n    #\n    # # Determine the widths of the Gaussian components\n    # widths = np.zeros(num_components)\n    # for i in range(num_components):\n    #     widths[i] = random.uniform(0.1, 1)\n    #\n    # # Determine the scales of the Gaussian components\n    # scales = np.zeros(num_components)\n    # for i in range(num_components):\n    #     scales[i] = random.uniform(0.1, 1)\n    #\n    # # Normalize the widths and scales so they sum to 1\n    # widths = widths / np.sum(widths)\n    # scales = scales / np.sum(scales)\n    #\n    # # Define the x-axis values\n    # x = np.linspace(-10, 10, 1000)\n    #\n    # # Initialize an array of zeros\n    # curve = np.zeros(x.shape)\n    #\n    # # Iteratively add scaled Gaussian components\n    # for i in range(num_components):\n    #     curve += scales[i] * norm.pdf(x, centers[i], widths[i])\n    #\n    # # Plot the composite curve\n    # plt.plot(x, curve)\n    # plt.show()\n    #\n    # # Return the composite curve\n    # return curve\n    pass", "CodeBleu_score": 0.3424657534246575}, {"code": "def curve()\n    import numpy as np\n    from scipy.stats import norm\n    from random import randint, random\n\n    # initialize composite curve\n    curve = np.zeros(1000)\n\n    # determine number of components\n    num_components = randint(1, 5)\n\n    # determine component centers\n    centers = np.random.randint(0, 1000, num_components)\n\n    # determine component widths\n    widths = np.random.randint(10, 100, num_components)\n    widths = widths / np.sum(widths)\n\n    # determine component scales\n    scales = np.random.randint(10, 100, num_components)\n    scales = scales / np.sum(scales)\n\n    # add components to curve\n    for i in range(num_components):\n        curve += scales[i] * norm.pdf(np.arange(1000), centers[i], widths[i])\n\n    return curve\n\n", "CodeBleu_score": 31.971733423281474}]}
{"instance_id": "be694335ebfc", "generation_result": [{"code": "def difference(derivative, accuracy=1)\n    derivative += 1\n    radius = derivative / accuracy\n    x = np.arange(-radius, radius + 1)\n    V = np.vander(x, N=derivative)\n    D = np.linalg.inv(V)\n    D = D[derivative - 1]\n    D = D / np.math.factorial(derivative - 1)\n    return D, x\n\n", "CodeBleu_score": 46.66697867649163}, {"code": "def difference(derivative, accuracy=1)\n    derivative += 1\n    radius = accuracy * derivative\n    x = np.arange(-radius, radius + 1)\n    V = np.vander(x, N=derivative)\n    Vinv = np.linalg.inv(V)\n    return Vinv[derivative - 1] / np.math.factorial(derivative - 1), x\n\n", "CodeBleu_score": 48.70413153861992}, {"code": "def difference(derivative, accuracy=1)\n    # Increment derivative order by one\n    derivative += 1\n    # Calculate radius based on accuracy and derivative order\n    radius = (derivative + accuracy) / 2\n    # Define range of points centered around zero\n    points = [i * radius for i in range(-derivative, derivative + 1)]\n    # Calculate inverse of Vandermonde matrix for points\n    V = vandermonde(points)\n    Vinv = inv(V)\n    # Return coefficients scaled by factorial of one less than derivative order\n    return Vinv[derivative - 1] * factorial(derivative - 1), points\n\n", "CodeBleu_score": 46.55545531895895}]}
{"instance_id": "fa3fb47f33fa", "generation_result": [{"code": "def interpolate(X, Y, T)\n    # Calculate the distances between consecutive points\n    dist = [0]\n    for i in range(1, len(X)):\n        dist.append(dist[i-1] + ((X[i] - X[i-1])**2 + (Y[i] - Y[i-1])**2)**0.5)\n\n    # Calculate the cumulative distances\n    cum_dist = [0]\n    for i in range(1, len(X)):\n        cum_dist.append(cum_dist[i-1] + dist[i])\n\n    # Calculate the interpolated coordinates\n    X_int = []\n    Y_int = []\n    for i in T:\n        for j in range(len(cum_dist)):\n            if i <= cum_dist[j]:\n                if j == 0:\n                    X_int.append(X[0])\n                    Y_int.append(Y[0])\n                else:\n                    X_int.append(X[j-1] + ((X[j] - X[j-1]) / dist[j-1]) * (i - cum_dist[j-1]))\n                    Y_int.append(Y[j-1] + ((Y[j] - Y[j-1]) / dist[j-1]) * (i - cum_dist[j-1]))\n                break\n\n    # Calculate the total distance covered by the original coordinates\n    total_dist = dist[-1]\n\n    return X_int, Y_int, total_dist\n\n", "CodeBleu_score": 23.271293048437812}, {"code": "def interpolate(X, Y, T)\n    # Compute the distances between consecutive points\n    D = np.sqrt(np.sum(np.diff(X, axis=0)**2 + np.diff(Y, axis=0)**2, axis=1))\n    # Compute the cumulative distances\n    C = np.cumsum(np.insert(D, 0, 0))\n    # Compute the total distance\n    total_distance = C[-1]\n    # Find the indices of the two points that are just below and above each target position\n    indices = np.searchsorted(C, T)\n    # Linearly interpolate the X and Y coordinates at the target positions\n    X_interp = X[indices] + (T - C[indices - 1]) / (C[indices] - C[indices - 1]) * (X[indices + 1] - X[indices])\n    Y_interp = Y[indices] + (T - C[indices - 1]) / (C[indices] - C[indices - 1]) * (Y[indices + 1] - Y[indices])\n    return X_interp, Y_interp, total_distance\n\n", "CodeBleu_score": 20.67289444984402}, {"code": "def interpolate(X, Y, T)\n    # Calculate the distances between consecutive points\n    dist = [0]\n    for i in range(1, len(X)):\n        dist.append(dist[i - 1] + (X[i] - X[i - 1]) ** 2 + (Y[i] - Y[i - 1]) ** 2)\n    dist = np.sqrt(dist)\n    # Calculate the cumulative distance\n    cum_dist = np.cumsum(dist)\n    # Interpolate the coordinates\n    X_interp = np.interp(T, cum_dist, X)\n    Y_interp = np.interp(T, cum_dist, Y)\n    # Return the interpolated coordinates and the total distance\n    return X_interp, Y_interp, cum_dist[-1]\n\n", "CodeBleu_score": 35.77070446710465}]}
{"instance_id": "870a2dad3c66", "generation_result": [{"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\n    import numpy as np\n\n    # Create a grid of complex numbers\n    x = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    c = x + 1j * y[:, None]\n\n    # Initialize iteration count array and complex array\n    N = np.zeros(c.shape, dtype=int)\n    Z = np.zeros(c.shape, dtype=np.complex64)\n\n    # Iteratively compute the Mandelbrot sequence\n    for n in range(maxiter):\n        N[Z.real * Z.real + Z.imag * Z.imag < horizon * horizon] = n  # Update iteration count where magnitude is within the horizon\n        Z[Z.real * Z.real + Z.imag * Z.imag < horizon * horizon] = Z[Z.real * Z.real + Z.imag * Z.imag < horizon * horizon] ** 2 + c[Z.real * Z.real + Z.imag * Z.imag < horizon * horizon]  # Update Z where magnitude is within the horizon\n\n    # Set values where magnitude exceeds the horizon to 0\n    N[Z.real * Z.real + Z.imag * Z.imag > horizon * horizon] = 0\n\n    return Z, N\n\n", "CodeBleu_score": 32.3879633894287}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.cm as cm\n\n    # Create a grid of complex numbers x+iy\n    x = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    c = x + 1j*y\n\n    # Create a grid for iteration counts and initialize to 0\n    N = np.zeros(c.shape, dtype=int)\n\n    # Create a grid for the results and initialize to 0+0j\n    Z = np.zeros(c.shape, np.complex64)\n\n    # Iterate for each value of c\n    for j in range(c.shape[0]):\n        for k in range(c.shape[1]):\n            z = c[j, k]\n            for i in range(maxiter):\n                z = z**2 + c[j, k]\n                if (z.real*z.real + z.imag*z.imag) >= horizon*horizon:\n                    N[j, k] = i\n                    break\n            Z[j, k] = z\n\n    return Z, N\n\n", "CodeBleu_score": 42.773182974405074}, {"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\n\n    # Create a grid of complex numbers C from the specified ranges\n    C = np.linspace(xmin, xmax, xn) + 1j * np.linspace(ymin, ymax, yn)\n\n    # Create arrays to store the iteration counts and complex values\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, dtype=complex)\n\n    # Iteratively compute the Mandelbrot sequence, updating N with the iteration count where the magnitude of Z exceeds the horizon\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n\n    # Points that remain bounded after maxiter iterations are set to 0 in N\n    N[N==maxiter] = 0\n\n    return Z, N\n\n", "CodeBleu_score": 55.36438568184061}]}
{"instance_id": "e1a908945acf", "generation_result": [{"code": "def smooth2d(A, sigma=3)\n    window_length = int(2 * np.ceil(sigma) + 1)\n    window = np.exp(-(np.arange(window_length) - window_length // 2) ** 2 / (2 * sigma ** 2))\n    window = window / np.sum(window)\n    A = np.apply_along_axis(lambda x: np.convolve(x, window, mode='same'), 0, A)\n    A = np.apply_along_axis(lambda x: np.convolve(x, window, mode='same'), 1, A.T).T\n    return A\n\n", "CodeBleu_score": 31.076524056670674}, {"code": "def smooth2d(A, sigma=3)\n    # Calculate the window length based on sigma\n    window_length = int(2 * sigma + 1)\n    if window_length % 2 == 0:\n        window_length += 1\n\n    # Create the 1D smoothing kernel\n    window = np.exp(-(np.arange(window_length) - (window_length - 1) / 2) ** 2 / (2 * sigma ** 2))\n    window /= window.sum()\n\n    # Smooth each row of the array\n    smoothed_A = np.apply_along_axis(lambda x: np.convolve(x, window, mode='same'), 1, A)\n\n    # Transpose the array\n    smoothed_A = smoothed_A.T\n\n    # Smooth each row of the array\n    smoothed_A = np.apply_along_axis(lambda x: np.convolve(x, window, mode='same'), 1, smoothed_A)\n\n    # Transpose the array again\n    smoothed_A = smoothed_A.T\n\n    return smoothed_A\n\n", "CodeBleu_score": 30.44340526981947}, {"code": "def smooth2d(A, sigma=3)\n    import numpy as np\n    from scipy import signal\n    window_length = int(2 * np.ceil(sigma * 3) + 1)\n    window_length = window_length if window_length % 2 == 1 else window_length + 1\n    window = signal.gaussian(window_length, sigma)\n    window = window / np.sum(window)\n    A = signal.convolve2d(A, window, mode='same', boundary='symm')\n    A = signal.convolve2d(A.T, window, mode='same', boundary='symm')\n    A = signal.convolve2d(A.T, window, mode='same', boundary='symm')\n    return A\n", "CodeBleu_score": 28.680115039189825}]}
{"instance_id": "94563c54b483", "generation_result": [{"code": "def process_image(self, padded_src, dpi)\n    # Retrieve the offsets ox and oy from the instance's attributes.\n    ox, oy = self.ox, self.oy\n    # Convert the offsets from millimeters to pixels.\n    ox_px = int(ox * dpi / 25.4)\n    oy_px = int(oy * dpi / 25.4)\n    # Shift the image horizontally by ox converted to pixels and vertically by oy converted to pixels using numpy's roll function.\n    processed_image = np.roll(padded_src, shift=(oy_px, ox_px), axis=(0, 1))\n    # Return the processed image.\n    return processed_image", "CodeBleu_score": 18.748919653710484}, {"code": "def process_image(self, padded_src, dpi)\n    # Retrieve the offsets\n    ox = self.ox\n    oy = self.oy\n\n    # Convert the offsets to pixels\n    ox_pix = ox * dpi\n    oy_pix = oy * dpi\n\n    # Shift the image horizontally and vertically\n    padded_src = np.roll(padded_src, int(ox_pix), axis=1)\n    padded_src = np.roll(padded_src, int(oy_pix), axis=0)\n\n    # Return the processed image\n    return padded_src\n\n", "CodeBleu_score": 32.54639226825834}, {"code": "def process_image(self, padded_src, dpi)\n    # Retrieve offsets from instance's attributes\n    ox = self.ox\n    oy = self.oy\n    # Convert offsets to pixels\n    ox_pixels = int(ox * dpi)\n    oy_pixels = int(oy * dpi)\n    # Shift image horizontally and vertically\n    padded_src = np.roll(padded_src, ox_pixels, axis=1)\n    padded_src = np.roll(padded_src, oy_pixels, axis=0)\n    # Return processed image\n    return padded_src\n\n", "CodeBleu_score": 21.60798018399231}]}
{"instance_id": "d71d232872fe", "generation_result": [{"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\n    if not isinstance(shape, tuple):\n        shape = (shape, shape)\n    if not isinstance(width, tuple):\n        width = (width, width)\n    if not isinstance(center, tuple):\n        center = (center, center)\n\n    grid = np.meshgrid(np.linspace(-1, 1, shape[0]), np.linspace(-1, 1, shape[1]))\n    grid = np.stack(grid, axis=-1)\n    dist = np.linalg.norm(grid - center, axis=-1)\n    gaussian = np.exp(-0.5 * np.square(dist / width))\n\n    return gaussian\n\n", "CodeBleu_score": 28.774557107641197}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n\n    if type(shape) == int:\n        shape = (shape, shape)\n    if type(width) == int or type(width) == float:\n        width = (width, width)\n    if type(center) == int or type(center) == float:\n        center = (center, center)\n\n    grid = np.mgrid[0:shape[0], 0:shape[1]]\n    grid = (grid[0] - center[0]) ** 2 + (grid[1] - center[1]) ** 2\n    grid = np.exp(-0.5 * grid / width[0] / width[1])\n\n    return grid\n\n", "CodeBleu_score": 32.112108804532916}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\n    shape = tuple(shape)\n    width = tuple(width)\n    center = tuple(center)\n    grid = np.mgrid[tuple(slice(-1, 1 + 1 / s, 1 / s) for s in shape)]\n    grid = 1.0 * (grid - center) / width\n    gauss = np.exp(-0.5 * np.sum(grid**2, 0))\n    return gauss\n\n", "CodeBleu_score": 15.063181826899145}]}
{"instance_id": "7bbd08c987bc", "generation_result": [{"code": "def stimulus(position, size, intensity)\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib import cm\n    from matplotlib.ticker import LinearLocator, FormatStrFormatter\n    from mpl_toolkits.mplot3d import Axes3D\n    from matplotlib import animation\n    from scipy.spatial import distance\n\n    # Create a grid of x, y points\n    x = np.linspace(-1, 1, 50)\n    y = np.linspace(-1, 1, 50)\n    X, Y = np.meshgrid(x, y)\n\n    # Convert polar coordinates to Cartesian coordinates\n    r = distance.euclidean((0, 0), position)\n    theta = np.arctan2(position[1], position[0])\n    x_c = r * np.cos(theta)\n    y_c = r * np.sin(theta)\n\n    # Normalize and center the stimulus\n    x_c = (x_c + 1) / 2\n    y_c = (y_c + 1) / 2\n\n    # Compute squared Euclidean distance from each point in the grid to the stimulus center\n    R = (X - x_c) ** 2 + (Y - y_c) ** 2\n\n    # Compute the stimulus\n    stimulus = intensity * np.exp(-R / (2 * size ** 2))\n\n    return stimulus\n\n", "CodeBleu_score": 32.54643241316905}, {"code": "def stimulus(position, size, intensity)\n    # Convert polar coordinates to Cartesian coordinates\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n\n    # Normalize coordinates\n    x /= 10\n    y /= 10\n\n    # Create a mesh grid representing the spatial domain, normalized and centered\n    x_grid, y_grid = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n\n    # Compute the squared Euclidean distance (R) from each point in the grid to the stimulus center\n    R = (x_grid - x) ** 2 + (y_grid - y) ** 2\n\n    # Return an exponential decay function of these distances, scaled by the size parameter\n    return np.exp(-R / (2 * size ** 2)) * intensity\n\n", "CodeBleu_score": 20.17033696057719}, {"code": "def stimulus(position, size, intensity)\n    import numpy as np\n\n    # Convert polar coordinates to Cartesian coordinates\n    x = position[0] * np.cos(position[1])\n    y = position[0] * np.sin(position[1])\n\n    # Normalize coordinates\n    x = (x + 1) / 2\n    y = (y + 1) / 2\n\n    # Create a mesh grid representing the spatial domain, normalized and centered\n    x_grid, y_grid = np.meshgrid(np.linspace(0, 1, size), np.linspace(0, 1, size))\n\n    # Compute the squared Euclidean distance (R) from each point in the grid to the stimulus center\n    R = (x_grid - x) ** 2 + (y_grid - y) ** 2\n\n    # Return an exponential decay function of these distances, scaled by the size parameter\n    return np.exp(-R / (0.01 * size)) * intensity\n\n", "CodeBleu_score": 19.413303744661253}]}
{"instance_id": "2b5806238d5d", "generation_result": [{"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 0.25, 0.5, 0.75, 1])\n    ax.set_yticks([0, 0.25, 0.5, 0.75, 1])\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.text(0.5, 0.5, text, fontdict={\"size\": 30, \"color\": \"black\", \"ha\": \"center\", \"va\": \"center\"})\n    ax.set_title(\"Title\", fontdict={\"family\": \"serif\", \"weight\": \"bold\"})\n\n", "CodeBleu_score": 40.35889738162225}, {"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 1])\n    ax.set_yticks([0, 1])\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.text(0.5, 0.5, text, horizontalalignment='center', fontsize=15, fontfamily='monospace', fontweight='bold')\n    ax.set_title('Title', fontfamily='serif', fontweight='bold')", "CodeBleu_score": 45.275917880038904}, {"code": "def plot(ax, text)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xticks([0, 0.25, 0.5, 0.75, 1])\n    ax.set_yticks([0, 0.25, 0.5, 0.75, 1])\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.text(0.5, 0.5, text, ha='center', va='center', fontsize=30)\n    ax.set_title('The title', fontfamily='serif', fontweight='bold')\n\n", "CodeBleu_score": 39.151113780657035}]}
{"instance_id": "0b42aac61e21", "generation_result": [{"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    X = self.transform(X, **kwargs)\n    return self.steps[-1][1].predict_cumulative_hazard_function(X, **kwargs)\n\n", "CodeBleu_score": 19.827186685347865}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    # Check if X is a dataframe\n    if not isinstance(X, pd.DataFrame):\n        raise TypeError(\"X must be a pandas dataframe\")\n\n    # Check if X has the same number of columns as the training set\n    if X.shape[1] != self.n_features_:\n        raise ValueError(\"X has a different number of columns than the training set\")\n\n    # Check if X has the same column names as the training set\n    if not X.columns.equals(self.feature_names_):\n        raise ValueError(\"X has different column names than the training set\")\n\n    # Check if X has the same target as the training set\n    if self.target_ != \"cumulative_hazard_function\":\n        raise ValueError(\"X has a different target than the training set\")\n\n    # Check if X has the same number of rows as the training set\n    if self.n_samples_ == 1 and X.shape[0] != self.n_samples_:\n        raise ValueError(\"X must have the same number of rows as the training set\")\n\n    # Check if the pipeline has been fitted\n    if not self.fitted_:\n        raise ValueError(\"This %s instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\" % type(self).__name__)\n\n    # Apply the transformations to X\n    X_transformed = self.transform(X, **kwargs)\n\n    # Use the final estimator in the pipeline to predict the cumulative hazard function based on the transformed data\n    return self.steps_[-1][-1].predict_cumulative_hazard_function(X_transformed)", "CodeBleu_score": 27.540103561234712}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs)\n    X = self.transform(X, **kwargs)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(X)\n\n", "CodeBleu_score": 19.308447193902406}]}
{"instance_id": "c8439e4918ad", "generation_result": [{"code": "def predict_survival_function(self, X, **kwargs)\n    Xt = self.transform(X)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 20.721062438910216}, {"code": "def predict_survival_function(self, X, **kwargs)\n    Xt = self.transform(X)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)\n\n", "CodeBleu_score": 20.721062438910216}, {"code": "def predict_survival_function(self, X, **kwargs)\n    X = self.transform(X)\n    return self.steps[-1][-1].predict_survival_function(X, **kwargs)\n\n", "CodeBleu_score": 18.47747032711214}]}
{"instance_id": "216662ca7db4", "generation_result": [{"code": "def _check_success(self, results)\n    if results.get('solver').get('termination_condition') == 'optimal':\n        pass\n    elif results.get('solver').get('termination_condition') == 'max_iterations':\n        warnings.warn('Maximum iterations reached.')\n    elif results.get('solver').get('termination_condition') == 'primal_infeasible':\n        raise RuntimeError('Primal infeasible.')\n    elif results.get('solver').get('termination_condition') == 'dual_infeasible':\n        raise RuntimeError('Dual infeasible.')\n    else:\n        raise RuntimeError('Unknown termination condition: ' + results.get('solver').get('termination_condition'))\n\n", "CodeBleu_score": 24.29260619819852}, {"code": "def _check_success(self, results)\n    if results[\"success\"]:\n        pass\n    elif results[\"status\"] == 0:\n        warnings.warn(\"Convergence not achieved. Increase maxiter to improve solution.\")\n    elif results[\"status\"] == 1:\n        raise RuntimeError(\"Primal infeasible.\")\n    elif results[\"status\"] == 2:\n        raise RuntimeError(\"Dual infeasible.\")\n    else:\n        raise RuntimeError(\"Unknown exit flag: \" + str(results[\"status\"]))\n\n", "CodeBleu_score": 17.613737172427385}, {"code": "def _check_success(self, results)\n    if results.get('status') == 'optimal':\n        return\n    elif results.get('status') == 'max_iter':\n        warnings.warn('Maximum iterations reached.', ConvergenceWarning)\n    elif results.get('status') == 'primal_infeasible':\n        raise RuntimeError('Primal infeasible.')\n    elif results.get('status') == 'dual_infeasible':\n        raise RuntimeError('Dual infeasible.')\n    else:\n        raise RuntimeError(f'Unknown exit status: {results.get(\"status\")}.')\n\n", "CodeBleu_score": 25.2241687168202}]}
{"instance_id": "1fe2e379fca0", "generation_result": [{"code": "def _decompose(self, P)\n    # Get eigenvalues and eigenvectors\n    eigenvals, eigenvecs = np.linalg.eigh(P)\n    # Get condition number\n    cond_num = np.abs(eigenvals[0]) / np.finfo(float).eps\n    # Check if all eigenvalues are above the negative cutoff\n    if np.any(eigenvals < -self.cutoff):\n        raise ValueError(\n            \"Negative eigenvalue detected. Matrix P is not positive definite.\"\n        )\n    # Filter out eigenvalues below the positive cutoff\n    eigenvals = eigenvals[eigenvals > self.cutoff]\n    # Normalize eigenvalues\n    eigenvals = eigenvals / eigenvals.max()\n    # Get largest eigenvalue\n    largest_eigenval = eigenvals.max()\n    # Get decomposed matrix\n    decomposed_matrix = np.dot(eigenvecs[:, eigenvals > self.cutoff],\n                               np.diag(eigenvals[eigenvals > self.cutoff]))\n    # Return decomposed matrix and condition number\n    return np.transpose(decomposed_matrix), cond_num, largest_eigenval\n\n", "CodeBleu_score": 29.317219710504578}, {"code": "def _decompose(self, P)\n    import numpy as np\n    from scipy.linalg import eigh\n    from scipy.sparse import spdiags\n    from scipy.sparse.linalg import spsolve\n    from scipy.sparse import csr_matrix\n\n    # Get eigenvalues and eigenvectors\n    w, v = eigh(P)\n\n    # Get condition number\n    cond = np.abs(w[0] / w[-1])\n\n    # Get cutoff\n    cutoff = np.sqrt(np.finfo(float).eps) * cond * np.max(P.shape)\n\n    # Filter out negative eigenvalues\n    w = w[w > -cutoff]\n\n    # Normalize eigenvalues\n    w = w / np.max(w)\n\n    # Get sparse diagonal matrix\n    D = spdiags(w, 0, len(w), len(w))\n\n    # Get decomposed matrix\n    P_decomp = csr_matrix(v) * D * csr_matrix(v).transpose()\n\n    return P_decomp.transpose(), np.max(w)", "CodeBleu_score": 35.327406981180644}, {"code": "def _decompose(self, P)\n    # get eigenvalues and eigenvectors of P\n    eigvals, eigvecs = np.linalg.eigh(P)\n    # get the maximum eigenvalue\n    max_eigval = np.max(eigvals)\n    # get the condition number\n    cond_num = max_eigval / np.finfo(float).eps\n    # get the shape of P\n    n_rows, n_cols = P.shape\n    # get the cutoffs\n    pos_cutoff = 10 ** (-10) * cond_num * max(n_rows, n_cols)\n    neg_cutoff = -10 ** (-10) * cond_num * max(n_rows, n_cols)\n    # filter out eigenvalues below the positive cutoff\n    eigvals = eigvals[eigvals > pos_cutoff]\n    # filter out eigenvalues above the negative cutoff\n    eigvals = eigvals[eigvals < neg_cutoff]\n    # normalize eigenvalues\n    eigvals = eigvals / max_eigval\n    # return the decomposed matrix and the largest eigenvalue\n    return eigvecs.T @ np.diag(eigvals) @ eigvecs, max_eigval", "CodeBleu_score": 22.59447566748824}]}
{"instance_id": "20c3f5d05d79", "generation_result": [{"code": "def _update_coef(self, coef, D)\n    # thresholding\n    coef[coef < self.threshold] = 0\n    # scaling\n    coef = coef * D\n    # storing the updated coefficients\n    self.coef = coef", "CodeBleu_score": 20.19534724737336}, {"code": "def _update_coef(self, coef, D)\n    if self.sparse:\n        self.coef_ = coef\n        self.coef_ = self.coef_[self.coef_ > self.threshold]\n        self.coef_ = self.coef_ * (D[self.coef_ > self.threshold])\n    else:\n        self.coef_ = coef\n        self.coef_ = self.coef_ * (D[self.coef_ > self.threshold])\n\n", "CodeBleu_score": 18.667073024555755}, {"code": "def _update_coef(self, coef, D)\n    # Select the coefficients that are greater than a small threshold\n    coef_sel = coef[coef > self.coef_thresh]\n    # Scale the selected coefficients by corresponding elements in D\n    coef_sel = coef_sel * D[coef_sel.index]\n    # Update the model's coefficients\n    self.coef = coef_sel", "CodeBleu_score": 17.818468959016414}]}
{"instance_id": "37c347d00d35", "generation_result": [{"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\n    # get event data\n    event = y_true[:, 0]\n    time = y_true[:, 1]\n\n    # get prediction data\n    prediction = raw_prediction[:, 0]\n\n    # initialize gradient\n    gradient = np.zeros(raw_prediction.shape)\n\n    # compute gradient\n    gradient[:, 0] = np.exp(prediction)\n\n    # apply sample weights\n    if sample_weight is not None:\n        gradient *= sample_weight\n\n    return gradient", "CodeBleu_score": 52.43407696565112}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n    if sample_weight is None:\n        sample_weight = np.ones(len(y_true))\n    # Compute the negative gradient of the Cox PH model\n    gradient = -np.sum(y_true * raw_prediction, axis=0)\n    # Apply sample weights to the gradient\n    gradient = gradient * sample_weight\n    return gradient", "CodeBleu_score": 38.078896087194025}, {"code": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)\n\n    # Extract the event type and time from the true event data.\n    event = y_true[:, 0]\n    time = y_true[:, 1]\n\n    # Calculate the gradient of the Cox PH model.\n    gradient = (event - np.exp(raw_prediction)) / (1 + np.exp(raw_prediction))\n\n    # Apply sample weights to the gradient, if provided.\n    if sample_weight is not None:\n        gradient *= sample_weight\n\n    # Return the gradient.\n    return gradient", "CodeBleu_score": 48.284481869831886}]}
{"instance_id": "6be432d27e52", "generation_result": [{"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n\n    if estimator_name is None:\n        estimator_name = self.estimator_name\n\n    if not self.supports_missing_values:\n        if not np.isfinite(X).all():\n            raise ValueError(\n                \"{} does not support missing values in X.\".format(estimator_name)\n            )\n        return None\n\n    # We use the sum to check for the presence of NaNs\n    try:\n        X_sum = X.sum(axis=0)\n    except ValueError:\n        # Avoid failing on object dtype\n        X_sum = np.zeros(X.shape[1], dtype=X.dtype)\n        for i in range(X.shape[1]):\n            X_sum[i] = X[:, i].sum()\n\n    # Check for NaNs\n    if np.isnan(X_sum).any():\n        return np.isnan(X_sum)\n    else:\n        return None", "CodeBleu_score": 38.67285710078997}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    import numpy as np\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.exceptions import NotFittedError\n\n    # Check if the estimator is fitted\n    try:\n        check_is_fitted(self, \"n_features_in_\")\n    except NotFittedError:\n        pass\n\n    # Set default parameters\n    if estimator_name is None:\n        estimator_name = self.__class__.__name__\n\n    # Check if the dataset supports missing values\n    if not self.missing_values:\n        if not np.isfinite(X).all():\n            raise ValueError(\n                f\"{estimator_name} does not support infinite values in X\"\n            )\n        return None\n\n    # Calculate the overall sum of X while ignoring overflow errors\n    with np.errstate(over=\"ignore\"):\n        X_sum = X.sum(axis=0)\n\n    # If the sum is not finite, raise a ValueError for any infinite elements\n    if not np.isfinite(X_sum).all():\n        raise ValueError(\n            f\"{estimator_name} does not support infinite values in X\"\n        )\n\n    # If the sum is finite and not NaN, return None, indicating no missing values\n    if np.isnan(X_sum).all():\n        return None\n\n    # If there are NaN values, return a mask indicating which features have missing values\n    return np.isnan(X_sum)", "CodeBleu_score": 38.284667816726}, {"code": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)\n    if estimator_name is None:\n        estimator_name = \"estimator\"\n\n    if not self.supports_missing_values:\n        if not np.isfinite(X).all():\n            raise ValueError(\n                \"{} does not support missing values in X\".format(estimator_name)\n            )\n        return None\n\n    with np.errstate(over=\"ignore\"):\n        if np.isfinite(X).all():\n            return None\n\n    if np.isnan(X).any():\n        return ~np.isfinite(X)\n    else:\n        raise ValueError(\n            \"{} only supports missing values represented by NaNs\".format(estimator_name)\n        )\n\n", "CodeBleu_score": 34.85526380422668}]}
{"instance_id": "dc5bda895a07", "generation_result": [{"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    # Check the model's memory and verify that the model is fitted\n    check_is_fitted(self, \"is_fitted_\")\n    # Validate the input data and possibly convert to a sparse matrix\n    X = check_array(X, accept_sparse=\"csr\", force_all_finite=False, dtype=\"float64\")\n    # Make predictions\n    predictions = self.decision_tree_.predict(X, check_input=check_input)\n    # Extract the survival function from the predictions\n    survival_function = predictions[:, 1]\n    # If return_array is True, return the array of predictions\n    if return_array:\n        return survival_function\n    # Otherwise, convert the array to a step function and return the step function\n    else:\n        return self.convert_array_to_step_function(survival_function)", "CodeBleu_score": 47.511780923290736}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    self._check_memory()\n    self._check_fitted()\n    X = self._validate_input(X, check_input=check_input)\n    predictions = self.decision_tree.predict(X, check_input=False)\n    predictions = predictions[:, 1]\n    if return_array:\n        return predictions\n    else:\n        return self._predictions_to_step_function(predictions)", "CodeBleu_score": 26.449544828221192}, {"code": "def predict_survival_function(self, X, check_input=True, return_array=False)\n    # Check if the model is fitted\n    if not self.fitted:\n        raise Exception('Model not fitted yet.')\n\n    # Check the input data\n    if check_input:\n        X = self._check_input(X)\n\n    # Make predictions\n    predictions = self.decision_tree.predict(X)\n\n    # Return the predictions\n    if return_array:\n        return predictions\n    else:\n        return self._predictions_to_step_function(predictions)", "CodeBleu_score": 36.43341425716036}]}
{"instance_id": "84ae915b5933", "generation_result": [{"code": "def _pre_fit(self, X, y)\n    X = check_array(X, accept_sparse=['csr', 'csc'])\n    y = check_array(y, ensure_2d=False)\n    if y.ndim == 2 and y.shape[1] == 1:\n        warnings.warn(\"A column-vector y was passed when a 1d array was\"\n                      \" expected. Please change the shape of y to \"\n                      \"(n_samples,), for example using ravel().\",\n                      DataConversionWarning, stacklevel=2)\n        y = np.ravel(y)\n    check_consistent_length(X, y)\n    if y.ndim == 1 or y.ndim == 2 and y.shape[1] == 1:\n        # If y is a 1d array, then we reshape it to (n_samples, 1)\n        y = y.reshape((-1, 1))\n    if y.shape[1] != 2:\n        raise ValueError(\"Survival data must be of shape (n_samples, 2).\")\n    if np.any(y[:, 1] < 0):\n        raise ValueError(\"Survival data contains negative survival times.\")\n    if np.any(y[:, 0] < 0) or np.any(y[:, 0] > 1):\n        raise ValueError(\"Survival data contains non-binary event values.\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples.\")\n    if X.shape[0] < 2:\n        raise ValueError(\"X must have at least 2 samples.\")\n    if np.all(y[:, 0] == 1):\n        raise ValueError(\"At least one event must occur.\")\n    if np.all(y[:, 0] == 0):\n        raise ValueError(\"At least one non-event must occur.\")\n    if self.fit_intercept:\n        X = np.c_[X, np.ones(X.shape[0])]\n    if self.scale is not None:\n        X, self.scales_ =", "CodeBleu_score": 18.613958956637195}, {"code": "def _pre_fit(self, X, y)\n\n    # Check the input data\n    X, y = check_X_y(X, y, accept_sparse=[\"csr\", \"csc\", \"coo\"])\n\n    # Check the number of samples\n    if X.shape[0] < 2:\n        raise ValueError(\"X must contain at least 2 samples.\")\n\n    # Convert the data type to float64\n    X = X.astype(np.float64)\n    y = y.astype(np.float64)\n\n    # Separate the event numbers and survival times\n    events = y[:, 0]\n    times = y[:, 1]\n\n    # Center the feature matrix\n    self.means_ = np.mean(X, axis=0)\n    X -= self.means_\n\n    # Normalize the data\n    if self.normalize:\n        self.scales_ = np.std(X, axis=0)\n        self.scales_[self.scales_ == 0] = 1\n        X /= self.scales_\n\n    # Sort the data in descending order based on survival time\n    sort_idx = np.argsort(times)[::-1]\n    X = X[sort_idx]\n    times = times[sort_idx]\n    events = events[sort_idx]\n\n    # Convert the sorted data to Fortran-contiguous arrays\n    X = np.asfortranarray(X)\n    times = np.asfortranarray(times)\n    events = np.asfortranarray(events)\n\n    # Return the processed feature matrix, event numbers, survival times, feature means, and scales\n    return X, events, times, self.means_, self.scales_\n\n", "CodeBleu_score": 33.71827367615265}, {"code": "def _pre_fit(self, X, y)\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        raise TypeError(\"X and y must be numpy arrays\")\n    if X.shape[0] < 2:\n        raise ValueError(\"X must contain at least 2 samples\")\n    if y.shape[0] != X.shape[0]:\n        raise ValueError(\"X and y must contain the same number of samples\")\n    if y.ndim != 1:\n        raise ValueError(\"y must be a 1-dimensional array\")\n    if y.shape[0] != np.unique(y).shape[0]:\n        raise ValueError(\"y must contain unique samples\")\n    if y.dtype != np.int:\n        raise TypeError(\"y must be of type int\")\n    if np.min(y) < 0:\n        raise ValueError(\"y must contain only positive integers\")\n    if np.max(y) == 0:\n        raise ValueError(\"y must contain at least one event\")\n    if np.max(y) > X.shape[0]:\n        raise ValueError(\"y must contain at most X.shape[0] events\")\n    if np.sum(y) == 0:\n        raise ValueError(\"y must contain at least one event\")\n    if np.sum(y) == X.shape[0]:\n        raise ValueError(\"y must contain at least one censored sample\")\n    if np.min(y) == 1:\n        raise ValueError(\"y must contain samples with at least 1 event\")\n    if np.max(y) == X.shape[0]:\n        raise ValueError(\"y must contain samples with at most X.shape[0] events\")\n    if np.sum(y) == np.sum(y.astype(bool)):\n        raise ValueError(\"y must contain samples with at least 1 censored sample\")\n    if np.sum(y.astype(bool)) == np.sum(y):\n        raise ValueError(\"y must contain samples with at most X.shape[0] censored samples\")\n    if np.sum(y) == np.sum(y.astype(bool)) + 1:\n        raise ValueError(\"", "CodeBleu_score": 12.929345460610364}]}
{"instance_id": "c70bb013f85f", "generation_result": [{"code": "def _check_alphas(self)\n    # Check if a new path is required\n    new_path = False\n    if self.alphas is None:\n        new_path = True\n        # Check that the number of alphas is a positive integer\n        if not isinstance(self.n_alphas, int):\n            raise TypeError(\"The number of alphas must be an integer.\")\n        if self.n_alphas < 1:\n            raise ValueError(\"The number of alphas must be a positive integer.\")\n        # Initialize the array of alpha values\n        self.alphas = np.linspace(0, 1, self.n_alphas)\n    else:\n        # Check that the alpha values are in a 1D array\n        if self.alphas.ndim != 1:\n            raise ValueError(\"The alphas must be in a 1D array.\")\n        # Check that the alpha values are finite and non-negative\n        if np.any(~np.isfinite(self.alphas)):\n            raise ValueError(\"The alphas must be finite.\")\n        if np.any(self.alphas < 0):\n            raise ValueError(\"The alphas must be non-negative.\")\n    return self.alphas, new_path\n\n", "CodeBleu_score": 32.68098201342939}, {"code": "def _check_alphas(self)\n    if self.alphas is None:\n        self.alphas = np.arange(1, self.n_alphas + 1)\n        return self.alphas, True\n    else:\n        self.alphas = np.array(self.alphas)\n        if self.alphas.ndim != 1:\n            raise ValueError(\"The alpha values must be in a 1D array.\")\n        if (self.alphas < 0).any():\n            raise ValueError(\"The alpha values must be non-negative.\")\n        if not np.isfinite(self.alphas).all():\n            raise ValueError(\"The alpha values must be finite.\")\n        return self.alphas, False\n\n", "CodeBleu_score": 31.898367676769624}, {"code": "def _check_alphas(self)\n    if self.alphas is None:\n        self.alphas = np.ones(self.n_alphas)\n        self.alphas_path = True\n    else:\n        self.alphas = np.array(self.alphas)\n        if self.alphas.ndim == 0:\n            self.alphas = np.array([self.alphas])\n        if self.alphas.ndim != 1:\n            raise ValueError(\"Alpha values must be in a 1D array.\")\n        if len(self.alphas) == 0:\n            raise ValueError(\"Alpha values must be in a 1D array.\")\n        if self.alphas.dtype != np.float64:\n            self.alphas = self.alphas.astype(np.float64)\n        if not np.isfinite(self.alphas).all():\n            raise ValueError(\"Alpha values must be finite.\")\n        if (self.alphas < 0).any():\n            raise ValueError(\"Alpha values must be non-negative.\")\n        self.alphas_path = False\n    return self.alphas, self.alphas_path", "CodeBleu_score": 34.52907488577377}]}
{"instance_id": "7b54a0554ba4", "generation_result": [{"code": "def nelson_aalen_estimator(event, time)\n    import numpy as np\n    import pandas as pd\n\n    # Validate event and time inputs\n    if not isinstance(event, list):\n        raise TypeError('event must be a list')\n    if not isinstance(time, list):\n        raise TypeError('time must be a list')\n    if len(event) != len(time):\n        raise ValueError('event and time must be the same length')\n    if len(event) == 0:\n        raise ValueError('event and time must have at least one element')\n\n    # Ensure event and time are consistent lengths\n    event = event[:]\n    time = time[:]\n\n    # Create a dataframe with event and time values\n    df = pd.DataFrame({'event': event, 'time': time})\n\n    # Sort the dataframe by time\n    df = df.sort_values(by=['time'])\n\n    # Create a column of unique times\n    df['unique_time'] = df['time'].unique()\n\n    # Create a column of the number of events\n    df['num_events'] = df.groupby('unique_time')['event'].transform('sum')\n\n    # Create a column of the number of subjects at risk\n    df['num_at_risk'] = df.groupby('unique_time')['event'].transform('count')\n\n    # Create a column of the cumulative hazard\n    df['cumulative_hazard'] = df.groupby('unique_time')['num_events'].transform(lambda x: x/x.iloc[0])\n\n    # Return the unique times and the cumulative hazard values\n    return df['unique_time'].values, df['cumulative_hazard'].values\n", "CodeBleu_score": 21.44229435511871}, {"code": "def nelson_aalen_estimator(event, time)\n    import numpy as np\n    import pandas as pd\n    import sys\n    from time_transform import time_transform\n    from check_length import check_length\n    from check_values import check_values\n    from check_type import check_type\n\n    # Ensure event is a numpy array\n    event = np.array(event)\n\n    # Ensure time is a numpy array\n    time = np.array(time)\n\n    # Ensure event is a numpy array of type boolean\n    event = check_type(event, \"boolean\")\n\n    # Ensure time is a numpy array of type float\n    time = check_type(time, \"float\")\n\n    # Ensure time and event have the same length\n    check_length(time, event)\n\n    # Ensure all values of time are positive\n    check_values(time, \"positive\")\n\n    # Transform time from days to months\n    time = time_transform(time, \"months\")\n\n    # Ensure event is a numpy array of type boolean\n    event = check_type(event, \"boolean\")\n\n    # Ensure time is a numpy array of type float\n    time = check_type(time, \"float\")\n\n    # Ensure time and event have the same length\n    check_length(time, event)\n\n    # Ensure all values of time are positive\n    check_values(time, \"positive\")\n\n    # Transform time from days to months\n    time = time_transform(time, \"months\")\n\n    # Create a data frame of the event and time data\n    data = pd.DataFrame({\"event\": event, \"time\": time})\n\n    # Create a data frame of unique time values\n    unique_time = pd.DataFrame({\"time\": np.unique(time)})\n\n    # Add a column of 0's to the data frame of unique time values\n    unique_time[\"events\"] = 0\n\n    # Add a column of 0's to the data frame of unique time values\n    unique_time[\"at_risk\"] = 0\n\n    # Create a data frame of the number of events at each time point\n    events = data.groupby([\"time\"])[\"event\"].sum().reset", "CodeBleu_score": 25.37397638108107}, {"code": "def nelson_aalen_estimator(event, time)\n\n    import numpy as np\n    import pandas as pd\n    import math\n\n    # Check that event and time are the same length\n    if len(event) != len(time):\n        raise ValueError(\"event and time must be the same length\")\n\n    # Check that event is binary\n    if len(np.unique(event)) != 2:\n        raise ValueError(\"event must be binary\")\n\n    # Check that time is non-negative\n    if np.any(time < 0):\n        raise ValueError(\"time must be non-negative\")\n\n    # Check that there are no duplicate time values\n    if len(np.unique(time)) != len(time):\n        raise ValueError(\"time must not contain duplicate values\")\n\n    # Sort event and time by time\n    time, event = zip(*sorted(zip(time, event)))\n\n    # Convert event and time to numpy arrays\n    event = np.array(event)\n    time = np.array(time)\n\n    # Create a dataframe with event and time\n    df = pd.DataFrame({\"event\": event, \"time\": time})\n\n    # Create a dataframe with unique times\n    unique_times = pd.DataFrame({\"time\": np.unique(time)})\n\n    # Add number of events and number of subjects at risk to unique_times\n    unique_times[\"num_events\"] = 0\n    unique_times[\"num_subjects\"] = 0\n\n    # Add number of events and number of subjects at risk to unique_times\n    for i in range(len(unique_times)):\n        # Add number of events\n        unique_times.loc[i, \"num_events\"] = len(df[df[\"time\"] == unique_times.loc[i, \"time\"]][\"event\"])\n        # Add number of subjects at risk\n        if i == 0:\n            unique_times.loc[i, \"num_subjects\"] = len(df[df[\"time\"] <= unique_times.loc[i, \"time\"]][\"event\"])\n        else:\n            unique_times.loc[i, \"num_subjects\"] = unique_times.loc[i - 1, \"num_subjects\"] - unique_times.loc[i -", "CodeBleu_score": 26.186537102199313}]}
{"instance_id": "35af806300cc", "generation_result": [{"code": "def ipc_weights(event, time)\n    if np.all(event):\n        return np.ones(len(event))\n    else:\n        time_unique = np.unique(time)\n        time_unique = time_unique[::-1]\n        survival = np.ones(len(time_unique))\n        for i in range(1, len(time_unique)):\n            survival[i] = (survival[i - 1] * (1 - (np.sum(event[(time == time_unique[i]) & (event == 1)]) / np.sum(event[time == time_unique[i]]))))\n        survival = survival[::-1]\n        survival[survival < 0] = 0\n        weights = np.ones(len(event))\n        for i in range(0, len(time_unique)):\n            weights[(time == time_unique[i]) & (event == 1)] = 1 / survival[i]\n        return weights\n\n", "CodeBleu_score": 29.94085487647087}, {"code": "def ipc_weights(event, time)\n\n    # Check if all events have occurred\n    if (event == 1).all():\n        return np.ones(len(event))\n\n    # Otherwise, calculate Kaplan-Meier estimator for event and time data in reverse\n    else:\n        time_rev = time[::-1]\n        event_rev = event[::-1]\n        km_est = np.cumsum(event_rev) / np.arange(1, len(event_rev) + 1)\n\n    # Find index positions of event times within unique time values\n    time_unique = np.unique(time)\n    time_index = [np.where(time_unique == i)[0][0] for i in time_rev]\n\n    # Retrieve corresponding survival probabilities\n    km_est_rev = km_est[time_index]\n\n    # Ensure all retrieved probabilities are positive\n    km_est_rev[km_est_rev < 0] = 0\n\n    # Create weights array, assigning inverse probabilities to events\n    weights = 1 / km_est_rev\n\n    return weights\n\n", "CodeBleu_score": 32.424942033505296}, {"code": "def ipc_weights(event, time)\n    if np.all(event):\n        return np.ones(len(event))\n    else:\n        km_est = KaplanMeierEstimator().fit(time, event)\n        time_u = np.unique(time)\n        time_u_i = np.searchsorted(time_u, time)\n        surv_prob = km_est.survival_function_.loc[time_u].values\n        surv_prob[surv_prob < 0] = 0\n        weights = 1 / surv_prob[time_u_i]\n        return weights\n\n", "CodeBleu_score": 35.179664680949344}]}
{"instance_id": "72b19985ea11", "generation_result": [{"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if not isinstance(dfs, list):\n        dfs = [dfs]\n    connectors = []\n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            connectors.append(PandasConnector(df))\n        elif isinstance(df, BaseConnector):\n            connectors.append(df)\n        elif isinstance(df, list):\n            connectors.append(ListConnector(df))\n        elif isinstance(df, dict):\n            connectors.append(DictConnector(df))\n        else:\n            raise ValueError(\n                f\"The provided data type is not supported. Please provide a pandas, modin, polars, or custom dataframe or a connector object.\"\n            )\n    return connectors", "CodeBleu_score": 38.582564485576064}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, (pd.DataFrame, BaseConnector)):\n        dfs = [dfs]\n    elif isinstance(dfs, list):\n        if not dfs:\n            raise ValueError(\"Empty list of dataframes\")\n        for df in dfs:\n            if not isinstance(df, (pd.DataFrame, BaseConnector)):\n                raise ValueError(\"Invalid type in list of dataframes\")\n    else:\n        raise ValueError(\"Invalid type of dataframes\")\n\n    dfs = [\n        self._get_df_connector(df) if isinstance(df, pd.DataFrame) else df\n        for df in dfs\n    ]\n    return dfs\n\n", "CodeBleu_score": 25.330052424644506}, {"code": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n    if isinstance(dfs, BaseConnector):\n        dfs = [dfs]\n    if isinstance(dfs, list):\n        if len(dfs) == 0:\n            raise ValueError(\"Empty list of dataframes provided\")\n        dfs_list = []\n        for df in dfs:\n            if isinstance(df, pd.DataFrame):\n                dfs_list.append(PandasConnector(df))\n            elif isinstance(df, BaseConnector):\n                dfs_list.append(df)\n            elif isinstance(df, list):\n                dfs_list.append(PandasConnector(pd.DataFrame(df)))\n            else:\n                raise ValueError(f\"Invalid type of dataframe: {type(df)}\")\n        return dfs_list\n    else:\n        raise ValueError(f\"Invalid type of dataframe: {type(dfs)}\")", "CodeBleu_score": 37.10588306539165}]}
{"instance_id": "cf22fb696e90", "generation_result": [{"code": "def execute(self) -> pd.DataFrame\n    if self.data is None:\n        self.data = self.fetch()\n    return self.data\n\n", "CodeBleu_score": 24.106109798927253}, {"code": "def execute(self) -> pd.DataFrame\n\n    # Check if there is a cached version of the data\n    if self.cached:\n        # If there is a cached version of the data, read it and return it as a DataFrame\n        return pd.read_csv(self.cached, index_col=0)\n    else:\n        # If there is no cached version of the data, fetch the data, assign it to the instance variable, and return the fetched data as a DataFrame\n        self.data = self.fetch()\n        return self.data", "CodeBleu_score": 30.508820986127027}, {"code": "def execute(self) -> pd.DataFrame\n    if self.data is not None:\n        if isinstance(self.data, pd.DataFrame):\n            return self.data\n        else:\n            raise TypeError(\"The data variable is not a DataFrame.\")\n    else:\n        if self.data_path is not None:\n            self.data = pd.read_csv(self.data_path, sep=\";\")\n        else:\n            self.data = self.fetch_data()\n        return self.data", "CodeBleu_score": 47.868358202792045}]}
{"instance_id": "21df5ecea95f", "generation_result": [{"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.to_frame()\n    return hashlib.sha256(\n        \"|\".join(self.columns).encode(\"utf-8\") + formula.encode(\"utf-8\")\n    ).hexdigest()\n\n", "CodeBleu_score": 32.847848574741285}, {"code": "def column_hash(self)\n    import hashlib\n    import pandas as pd\n    if not isinstance(self, pd.DataFrame):\n        self = self.to_df()\n    return hashlib.sha256((\"|\".join(self.columns) + self.formula).encode()).hexdigest()", "CodeBleu_score": 31.405820854002624}, {"code": "def column_hash(self)\n    if not isinstance(self, pd.DataFrame):\n        self = self.to_frame()\n    column_names = \"|\".join(self.columns)\n    formula = \"|\".join(self.columns)\n    return hashlib.sha256(column_names.encode() + formula.encode()).hexdigest()", "CodeBleu_score": 39.86298712471433}]}
{"instance_id": "27ee5f415000", "generation_result": [{"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\n    if isinstance(df, pd.DataFrame):\n        self.df = df\n    elif isinstance(df, pd.Series):\n        self.df = df.to_frame()\n    elif isinstance(df, str):\n        if os.path.isfile(df):\n            self.df = pd.read_csv(df)\n        else:\n            raise ValueError(\n                \"The input string does not represent a file path. Please provide a file path or a DataFrame.\"\n            )\n    elif isinstance(df, list):\n        self.df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        self.df = pd.DataFrame(df)\n    else:\n        raise ValueError(\n            \"The input type is not supported. Please provide a DataFrame, Series, file path, list, or dictionary.\"\n        )\n\n    return self", "CodeBleu_score": 51.984583061997036}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n\n    if isinstance(df, pd.DataFrame):\n        self.df = df\n    elif isinstance(df, pd.Series):\n        self.df = df.to_frame()\n    elif isinstance(df, str):\n        try:\n            self.df = pd.read_csv(df)\n        except Exception:\n            raise ValueError(\n                \"The input is a string, but it does not represent a file path.\"\n            )\n    elif isinstance(df, list):\n        try:\n            self.df = pd.DataFrame(df)\n        except Exception:\n            raise ValueError(\"The input is a list, but it cannot be converted.\")\n    elif isinstance(df, dict):\n        try:\n            self.df = pd.DataFrame(df)\n        except Exception:\n            raise ValueError(\"The input is a dict, but it cannot be converted.\")\n    else:\n        raise ValueError(\"The input is of an unsupported type.\")\n\n", "CodeBleu_score": 51.42606345596692}, {"code": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n    elif isinstance(df, pd.DataFrame):\n        pass\n    elif isinstance(df, list):\n        df = pd.DataFrame(df)\n    elif isinstance(df, dict):\n        df = pd.DataFrame(df)\n    elif isinstance(df, str):\n        df = pd.read_csv(df)\n    else:\n        raise ValueError(\"Input is not a supported type.\")\n    return df\n\n", "CodeBleu_score": 42.86269163886502}]}
{"instance_id": "38f48a358606", "generation_result": [{"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    if n > self.shape[0]:\n        return self.sample(frac=1).reset_index(drop=True)\n    else:\n        sampled_df = pd.DataFrame()\n        for column in self.columns:\n            if self[column].dtype == object:\n                sampled_df[column] = self[column].sample(n)\n            else:\n                sampled_df[column] = self[column].sample(n).round(2)\n        sampled_df = sampled_df.reset_index(drop=True)\n        return sampled_df\n\n", "CodeBleu_score": 36.45107479460298}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n\n    # Check if the DataFrame has fewer rows than the specified number\n    if self.shape[0] <= n:\n        # If so, shuffle the DataFrame and return it\n        return self.sample(frac=1).reset_index(drop=True)\n\n    # Otherwise, sample each column individually\n    sampled_columns = [column.sample(n) for column in self.columns]\n\n    # Combine the sampled columns into a new DataFrame\n    sampled_df = pd.concat(sampled_columns, axis=1)\n\n    # Anonymize the first few rows of the sampled DataFrame\n    sampled_df.iloc[:2, :] = \"Anonymized\"\n\n    # Return the sampled DataFrame\n    return sampled_df", "CodeBleu_score": 33.08327958474193}, {"code": "def sample(self, n: int = 3) -> pd.DataFrame\n    # If the DataFrame has fewer rows than the specified number, return a shuffled version of the entire DataFrame\n    if len(self) < n:\n        return self.sample(frac=1).reset_index(drop=True)\n    # If the DataFrame has more rows than the specified number, sample each column individually and combine the samples into a new DataFrame\n    else:\n        # Sample each column individually\n        sampled_columns = []\n        for column in self.columns:\n            # If the column is numeric, sample the column using the custom sampling method\n            if self[column].dtype in [\"int64\", \"float64\"]:\n                sampled_columns.append(self[column].sample(n, replace=True))\n            # If the column is categorical, sample the column using the custom sampling method\n            elif self[column].dtype == \"object\":\n                sampled_columns.append(self[column].sample(n, replace=True))\n            # If the column is datetime, sample the column using the custom sampling method\n            elif self[column].dtype == \"datetime64[ns]\":\n                sampled_columns.append(self[column].sample(n, replace=True))\n            # If the column is boolean, sample the column using the custom sampling method\n            elif self[column].dtype == \"bool\":\n                sampled_columns.append(self[column].sample(n, replace=True))\n            # If the column is a list, sample the column using the custom sampling method\n            elif self[column].dtype == \"list\":\n                sampled_columns.append(self[column].sample(n, replace=True))\n            # If the column is a dictionary, sample the column using the custom sampling method\n            elif self[column].dtype == \"dict\":\n                sampled_columns.append(self[column].sample(n, replace=True))\n            # If the column is a tuple, sample the column using the custom sampling method\n            elif self[column].dtype == \"tuple\":\n                sampled_columns.append(self[column].sample(n, replace=True))\n            # If the column is a set, sample the column using the custom sampling method\n            elif self[column].dtype", "CodeBleu_score": 19.33350336838756}]}
{"instance_id": "8bddad2e31ca", "generation_result": [{"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n\n    # Check if the DataFrame has a name or description\n    if df.name is None:\n        df_name = \"DataFrame\"\n    else:\n        df_name = df.name\n    if df.description is None:\n        df_description = \"\"\n    else:\n        df_description = df.description\n\n    # Construct the opening tag\n    opening_tag = f\"<{df_name} name='{df_name}' description='{df_description}'\"\n\n    # Add details about the DataFrame's dimensions and content\n    opening_tag += f\" shape='{df.shape}'\"\n    opening_tag += f\" dtypes='{df.dtypes.to_dict()}'\"\n    opening_tag += f\" content='{df.to_csv(index=False)}'\"\n\n    # Add details about the DataFrame's extras\n    for key, value in extras.items():\n        opening_tag += f\" {key}='{value}'\"\n\n    # Close the opening tag\n    opening_tag += \">\"\n\n    # Return the opening tag\n    return opening_tag\n\n", "CodeBleu_score": 41.04900094630535}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    csv_string = ''\n    # Add opening tag with name and description\n    csv_string += f'<{df.name}'\n    if df.description:\n        csv_string += f' description=\"{df.description}\"'\n    csv_string += '>\\n'\n    # Add details about dimensions and content\n    csv_string += f'<dimensions rows=\"{df.shape[0]}\" columns=\"{df.shape[1]}\"/>\\n'\n    csv_string += f'<content>\\n{df.to_csv(index=False, header=False)}\\n</content>\\n'\n    # Add closing tag\n    csv_string += f'</{df.name}>\\n'\n    return csv_string\n\n", "CodeBleu_score": 40.40104282267851}, {"code": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str\n    # Check if the DataFrame has a name\n    if df.name is None:\n        # If not, use the default name\n        name = \"DataFrame\"\n    else:\n        # If so, use the DataFrame's name\n        name = df.name\n    # Check if the DataFrame has a description\n    if df.description is None:\n        # If not, use the default description\n        description = \"A DataFrame\"\n    else:\n        # If so, use the DataFrame's description\n        description = df.description\n    # Check if the DataFrame has a shape\n    if df.shape is None:\n        # If not, use the default shape\n        shape = \"(0, 0)\"\n    else:\n        # If so, use the DataFrame's shape\n        shape = str(df.shape)\n    # Check if the DataFrame has a head\n    if df.head is None:\n        # If not, use the default head\n        head = \"No head available\"\n    else:\n        # If so, use the DataFrame's head\n        head = str(df.head())\n    # Check if the DataFrame has a tail\n    if df.tail is None:\n        # If not, use the default tail\n        tail = \"No tail available\"\n    else:\n        # If so, use the DataFrame's tail\n        tail = str(df.tail())\n    # Check if the DataFrame has a dtypes\n    if df.dtypes is None:\n        # If not, use the default dtypes\n        dtypes = \"No dtypes available\"\n    else:\n        # If so, use the DataFrame's dtypes\n        dtypes = str(df.dtypes)\n    # Check if the DataFrame has a columns\n    if df.columns is None:\n        # If not, use the default columns\n        columns = \"No columns available\"\n    else:\n        # If so, use the DataFrame's columns\n        columns = str(df.columns)\n    # Check if the DataFrame has a index\n    if df.index is None:\n        # If not, use the default index\n        index = \"No index available\"\n    else:\n        # If so,", "CodeBleu_score": 30.9764358237826}]}
{"instance_id": "5380bb60fd39", "generation_result": [{"code": "def convert_df_sql_connector_to_str\n    import pandas as pd\n    from pandas.io.sql import SQLTable\n    from pandas.io.sql import SQLDatabase\n    from pandas.io.sql import SQLQuery\n    from pandas.io.sql import read_sql_table\n    from pandas.io.sql import read_sql_query\n    from pandas.io.sql import to_sql\n    from pandas.io.sql import SQLDatabase\n    from pandas.io.sql import SQLAlchemyEngine\n    from pandas.io.sql import SQLAlchemyDialect\n    from pandas.io.sql import SQLAlchemyConnection\n    from pandas.io.sql import SQLAlchemyCompiler\n    from pandas.io.sql import PandasSQLAlchemy\n    from pandas.io.sql import PandasSQLDatabase\n    from pandas.io.sql import PandasSQLTable\n    from pandas.io.sql import PandasSQLQuery\n    from pandas.io.sql import PandasSQLiteDatabase\n    from pandas.io.sql import PandasSQLEngine\n    from pandas.io.sql import PandasSQLiteConnection\n    from pandas.io.sql import PandasSQLiteCompiler\n    from pandas.io.sql import PandasSQLiteDialect\n    from pandas.io.sql import PandasSQLiteQuery\n    from pandas.io.sql import PandasSQLiteTable\n    from pandas.io.sql import PandasSQLiteAlchemy\n    from pandas.io.sql import PandasSQLiteAlchemyConnection\n    from pandas.io.sql import PandasSQLiteAlchemyCompiler\n    from pandas.io.sql import PandasSQLiteAlchemyDialect\n    from pandas.io.sql import PandasSQLiteAlchemyQuery\n    from pandas.io.sql import PandasSQLiteAlchemyTable\n    from pandas.io.sql import PandasSQLiteAlchemyEngine\n    from pandas.io.sql import PandasSQLiteAlchemyDatabase\n    from pandas.io.sql import PandasSQLiteAlchemyDialect\n    from pandas.io.sql import PandasSQLiteAlchemyQuery\n    from pandas.io.sql import PandasSQLiteAlchemyTable\n    from pandas.io.sql import PandasSQLiteAlchemyEngine", "CodeBleu_score": 14.28809783267922}, {"code": "def convert_df_sql_connector_to_str\n    df_name = df.metadata.get(\"name\")\n    df_desc = df.metadata.get(\"description\")\n    if df_desc is None:\n        df_desc = \"\"\n    df_html = f\"<table id='{df_name}'>\\n<caption>{df_desc}</caption>\\n\"\n    df_html += df.head().to_csv(index=False).replace(\"\\n\", \"\\n\")\n    df_html += \"\\n</table>\"\n    return df_html", "CodeBleu_score": 38.56083636742035}, {"code": "def convert_df_sql_connector_to_str\n    import pandas as pd\n    import numpy as np\n    import json\n    from pandas.io.json import json_normalize\n    import datetime\n    import pytz\n    import os\n    import re\n    import sys\n    import traceback\n    import inspect\n    from IPython.display import display, HTML\n    import ipywidgets as widgets\n    import ipywidgets as widgets\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from IPython.display import display, HTML\n    from", "CodeBleu_score": 12.579702248893742}]}
{"instance_id": "0afd995716c5", "generation_result": [{"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    # Convert the DataFrame to JSON format\n    json_str = self.convert_df_to_json(df)\n    # Convert the JSON to a YAML string\n    yaml_str = yaml.dump(json.loads(json_str), sort_keys=False)\n    # If the is_direct_sql key in the extras dictionary is True, wrap the YAML string in HTML table tags\n    if extras.get(\"is_direct_sql\") == True:\n        yaml_str = \"<table>\" + yaml_str + \"</table>\"\n    return yaml_str\n\n", "CodeBleu_score": 48.2314251228952}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n\n    json_str = self.convert_df_to_json(df)\n    yml_str = json2yaml(json_str)\n\n    if extras[\"is_direct_sql\"]:\n        yml_str = f\"<table><tbody>{yml_str}</tbody></table>\"\n\n    return yml_str", "CodeBleu_score": 42.15943096911688}, {"code": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    json_str = convert_df_to_json(df)\n    yaml_str = json2yaml(json_str)\n    if \"is_direct_sql\" in extras.keys() and extras[\"is_direct_sql\"]:\n        yaml_str = f\"<table>{yaml_str}</table>\"\n    return yaml_str", "CodeBleu_score": 43.99967492442507}]}
{"instance_id": "e2b90ebd4e5d", "generation_result": [{"code": "def validate_value(self, expected_type: str) -> bool\n    if expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        if isinstance(self.value, str):\n            return True\n        elif isinstance(self.value, dict):\n            return True\n        else:\n            return False\n    else:\n        raise ValueError(f\"expected_type must be one of '', 'number', 'string', 'dataframe', or 'plot'. Got {expected_type}.\")", "CodeBleu_score": 58.47383756244312}, {"code": "def validate_value(self, expected_type: str) -> bool\n    if expected_type == \"number\":\n        if isinstance(self.value, int) or isinstance(self.value, float):\n            return True\n        else:\n            return False\n    elif expected_type == \"string\":\n        if isinstance(self.value, str):\n            return True\n        else:\n            return False\n    elif expected_type == \"dataframe\":\n        if isinstance(self.value, pd.DataFrame) or isinstance(self.value, pd.Series):\n            return True\n        else:\n            return False\n    elif expected_type == \"plot\":\n        if isinstance(self.value, str) and self.value.endswith((\".png\", \".jpg\", \".jpeg\", \".pdf\")):\n            return True\n        elif isinstance(self.value, dict):\n            return True\n        else:\n            return False\n    elif expected_type == \"\":\n        return True\n    else:\n        raise ValueError(f\"Invalid expected type: {expected_type}\")", "CodeBleu_score": 48.23649830372287}, {"code": "def validate_value(self, expected_type: str) -> bool\n    if expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        return isinstance(self, (str, dict))\n    else:\n        raise ValueError(f\"Unknown expected_type: {expected_type}\")", "CodeBleu_score": 62.45423118871284}]}
{"instance_id": "d95828fdcbcb", "generation_result": [{"code": " def _distance(self, X, X2=None)    if isinstance(self.distance_params, dict):\n        if len(self.distance_params) == 0:\n            self.distance_params = {}\n    elif isinstance(self.distance_params, list):\n        self.distance_params = self.distance_params[0]\n    elif isinstance(self.distance_params, tuple):\n        self.distance_params = self.distance_params[0]\n\n    if isinstance(self.distance, str):\n        return pairwise_distance(X, X2, metric=self.distance, **self.distance_params)\n    else:\n        if X2 is None:\n            if len(signature(self.distance).parameters) == 1:\n                return self.distance(X, **self.distance_params)\n            else:\n                raise ValueError(\"Distance function must have one parameter if X2 is None\")\n        else:\n            return self.distance(X, X2, **self.distance_params)", "CodeBleu_score": 33.50803006008775}, {"code": " def _distance(self, X, X2=None)    # initialize distance parameters\n    if self.distance_params is None:\n        distance_params = {}\n    else:\n        distance_params = self.distance_params\n\n    # compute distance\n    if isinstance(self.distance, str):\n        return pairwise_distance(X, X2, metric=self.distance, **distance_params)\n    else:\n        if X2 is None:\n            if \"X2\" in signature(self.distance).parameters:\n                return self.distance(X, **distance_params)\n            else:\n                raise ValueError(\n                    \"The distance function must support a single dataset as input.\"\n                )\n        else:\n            return self.distance(X, X2, **distance_params)\n\n", "CodeBleu_score": 40.54534123851117}, {"code": " def _distance(self, X, X2=None)    if isinstance(self.distance_params, str):\n        self.distance_params = {}\n    if isinstance(self.distance, str):\n        distance = pairwise_distance(X, X2, metric=self.distance, **self.distance_params)\n    else:\n        distance = self.distance(X, X2, **self.distance_params)\n    if X2 is None:\n        if len(distance.shape) == 1:\n            distance = distance.reshape(1, -1)\n        else:\n            distance = distance.T\n    return distance", "CodeBleu_score": 34.8663148100047}]}
{"instance_id": "497d73b298ee", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n    import pandas as pd\n    import numpy as np\n    if isinstance(X, pd.DataFrame):\n        if len(X.columns) == 1:\n            X = pd.DataFrame(X.iloc[:, 0].values.reshape(-1, 1),\n                             columns=X.columns)\n        else:\n            X = pd.DataFrame(X.iloc[:, 0].values.reshape(-1, 1),\n                             columns=X.columns)\n            for i in range(1, len(X.columns)):\n                X[i] = X[i].values.reshape(-1, 1)\n            X = X.pivot_table(index=X.index,\n                              columns=X.columns)\n            X = X.fillna(0)\n            X = X.to_numpy()\n            X = np.hstack([np.array([X.shape[1]] * X.shape[0]).reshape(-1, 1),\n                           X])\n        return X\n    elif isinstance(X, np.ndarray):\n        if len(X.shape) == 1:\n            X = X.reshape(-1, 1)\n        return X\n    elif isinstance(X, list):\n        if len(X[0]) == 1:\n            X = np.array(X).reshape(-1, 1)\n        else:\n            X = pd.DataFrame(X)\n            X = pd.DataFrame(X.iloc[:, 0].values.reshape(-1, 1),\n                             columns=X.columns)\n            for i in range(1, len(X.columns)):\n                X[i] = X[i].values.reshape(-1, 1)\n            X = X.pivot_table(index=X.index,\n                              columns=X.columns)\n            X = X.fillna(0)\n            X = X.to_numpy()\n            X = np.hstack([np.array([X.shape[1]] * X.shape[0]).reshape(-1, 1),\n                           X])\n        return X\n    else:\n        raise TypeError('X must be", "CodeBleu_score": 32.00377881534563}, {"code": "def _convert_X_to_sklearn(self, X)\n    if not isinstance(X, pd.DataFrame):\n        X = pd.DataFrame(X)\n    if len(X.shape) == 1:\n        X = pd.DataFrame(X)\n    elif len(X.shape) > 2:\n        raise ValueError(\"X must be a 1D or 2D data structure\")\n    if len(X.columns) == 1:\n        X = pd.DataFrame(X.iloc[:, 0])\n    if len(X.columns) > 1:\n        X = X.reset_index(drop=True)\n        X = pd.DataFrame(X.T)\n        X = X.fillna(0)\n        X = X.to_numpy()\n        X_lengths = np.array(X.shape[1] * np.ones(X.shape[0])).reshape(-1, 1)\n        X = np.concatenate((X_lengths, X), axis=1)\n    else:\n        if isinstance(X.iloc[0, 0], str):\n            X = np.array(X.iloc[:, 0].str.split(\" \").tolist())\n            X = X.reshape(X.shape[0], X.shape[1])\n        elif isinstance(X.iloc[0, 0], list):\n            X = np.array(X.iloc[:, 0].tolist())\n            X = X.reshape(X.shape[0], X.shape[1])\n        elif isinstance(X.iloc[0, 0], np.ndarray):\n            X = np.array(X.iloc[:, 0].tolist())\n            X = X.reshape(X.shape[0], X.shape[1])\n        else:\n            X = X.to_numpy()\n    return X\n\n", "CodeBleu_score": 36.80328704088146}, {"code": "def _convert_X_to_sklearn(self, X)\n    import pandas as pd\n    import numpy as np\n    import copy\n\n    X_copy = copy.deepcopy(X)\n\n    # If X is a pandas DataFrame\n    if isinstance(X_copy, pd.DataFrame):\n        X_copy = X_copy.reset_index(drop=True)\n        X_copy = X_copy.pivot(columns=X_copy.columns[0])\n        X_copy = X_copy.fillna(0)\n        X_copy = X_copy.values\n        X_copy = X_copy.astype(np.float32)\n        X_copy = np.hstack((np.array([X_copy.shape[1]]).reshape(-1, 1), X_copy))\n\n    # If X is a numpy array\n    elif isinstance(X_copy, np.ndarray):\n        X_copy = X_copy.astype(np.float32)\n        X_copy = np.hstack((np.array([X_copy.shape[1]]).reshape(-1, 1), X_copy))\n\n    # If X is a list\n    elif isinstance(X_copy, list):\n        X_copy = np.array(X_copy)\n        X_copy = X_copy.astype(np.float32)\n        X_copy = np.hstack((np.array([X_copy.shape[1]]).reshape(-1, 1), X_copy))\n\n    return X_copy", "CodeBleu_score": 35.386418439201016}]}
{"instance_id": "8798260a6f12", "generation_result": [{"code": "def is_sklearn_estimator(obj)\n    if not isinstance(obj, type):\n        raise ValueError(\n            \"The object must be a class, not an instance. \"\n            \"Use type(obj) to get a class object.\"\n        )\n    if issubclass(obj, SklearnBaseEstimator) and not issubclass(obj, BaseObject):\n        return True\n    else:\n        return False\n\n", "CodeBleu_score": 23.708540264214502}, {"code": "def is_sklearn_estimator(obj)\n\n    import inspect\n    from sktime.base import BaseObject\n    from sktime.classification.base import BaseClassifier\n    from sktime.clustering.base import BaseClusterer\n    from sktime.forecasting.base import BaseForecaster\n    from sktime.regression.base import BaseRegressor\n    from sktime.transformations.base import BaseTransformer\n    from sktime.transformations.panel.base import BasePanelTransformer\n    from sklearn.base import BaseEstimator\n\n    if not inspect.isclass(obj):\n        return False\n    if not issubclass(obj, BaseEstimator):\n        return False\n    if issubclass(obj, BaseObject):\n        return False\n    if issubclass(obj, BaseTransformer):\n        return False\n    if issubclass(obj, BasePanelTransformer):\n        return False\n    if issubclass(obj, BaseForecaster):\n        return False\n    if issubclass(obj, BaseRegressor):\n        return False\n    if issubclass(obj, BaseClassifier):\n        return False\n    if issubclass(obj, BaseClusterer):\n        return False\n    return True", "CodeBleu_score": 18.301406763615876}, {"code": "def is_sklearn_estimator(obj)\n    return (\n        isinstance(obj, type)\n        and issubclass(obj, sklearn.base.BaseEstimator)\n        and not issubclass(obj, sktime.base.BaseObject)\n    )\n\n", "CodeBleu_score": 19.287890362346793}]}
{"instance_id": "2ab2596d8520", "generation_result": [{"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return SklearnRegressorPipeline(self.regressor, TransformerPipeline(other, self.transformer_pipeline))\n    else:\n        return NotImplemented", "CodeBleu_score": 20.394339502171547}, {"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return SklearnRegressorPipeline(self.regressor, TransformerPipeline(self.transformers, other))\n    else:\n        return NotImplemented", "CodeBleu_score": 19.635211311513075}, {"code": "def __rmul__(self, other)\n    if isinstance(other, BaseTransformer):\n        return SklearnRegressorPipeline(\n            regressor=self.regressor,\n            transformer_pipeline=TransformerPipeline(\n                transformers=[*self.transformer_pipeline.transformers, other]\n            ),\n        )\n    else:\n        return NotImplemented\n\n", "CodeBleu_score": 31.79471518880118}]}
{"instance_id": "5c1c6d14ca93", "generation_result": [{"code": "def prep_skl_df(df, copy_df=False)\n    if copy_df:\n        df = df.copy()\n    if not all(isinstance(col, str) for col in df.columns):\n        df.columns = [str(col) for col in df.columns]\n    return df", "CodeBleu_score": 48.47336941335975}, {"code": "def prep_skl_df(df, copy_df=False)\n    if copy_df:\n        df = df.copy()\n    if not all(isinstance(col, str) for col in df.columns):\n        df.columns = [str(col) for col in df.columns]\n    return df\n\n", "CodeBleu_score": 48.47336941335975}, {"code": "def prep_skl_df(df, copy_df=False)\n\n    # If the copy option is True, create a copy of the DataFrame.\n    if copy_df:\n        df = df.copy()\n\n    # Convert the column names to strings.\n    df.columns = df.columns.astype(str)\n\n    # Return the DataFrame.\n    return df", "CodeBleu_score": 41.41179666430295}]}
{"instance_id": "d5e567c20361", "generation_result": [{"code": "def predict(self, X)\n    if self.random_state:\n        np.random.seed(self.random_state)\n    y_pred = np.zeros(X.shape[0])\n    for i in range(X.shape[0]):\n        y_pred[i] = np.random.choice(np.argwhere(self.predict_proba(X[i,:]) == np.amax(self.predict_proba(X[i,:]))).flatten())\n    return y_pred", "CodeBleu_score": 40.989239980634416}, {"code": "def predict(self, X)\n    rng = check_random_state(self.random_state)\n    y_pred = np.zeros(X.shape[0])\n    for i, x in enumerate(X):\n        probs = self.predict_proba(x)\n        idx = np.where(probs == probs.max())[0]\n        y_pred[i] = rng.choice(idx)\n    return y_pred", "CodeBleu_score": 48.30256716818101}, {"code": "def predict(self, X)\n\n    # Initialize a random state\n    random_state = check_random_state(self.random_state)\n\n    # Get the predicted probabilities\n    probabilities = self.predict_proba(X)\n\n    # Select the indices of the maximum probability values\n    indices = np.argmax(probabilities, axis=1)\n\n    # Randomly choose one among the indices\n    indices = random_state.choice(indices)\n\n    # Map the chosen indices to the corresponding class labels\n    return self.classes_[indices]", "CodeBleu_score": 24.083221102122934}]}
{"instance_id": "210b60dffd55", "generation_result": [{"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    import numpy as np\n    import pandas as pd\n    import sklearn.preprocessing as preprocessing\n    from sklearn.utils import check_array\n\n    X = check_array(X, accept_sparse=True)\n    X_transformed = np.concatenate(\n        [\n            pca.transform(X[:, group])\n            for pca, group in zip(pcas, groups)\n        ],\n        axis=1,\n    )\n    X_transformed = np.nan_to_num(X_transformed)\n    probas = clf.predict_proba(X_transformed)\n    if probas.shape[1] < self.n_classes_:\n        # If the number of predicted classes is less than the expected number of classes, adjust the probabilities\n        # to match the expected class structure\n        n_missing_classes = self.n_classes_ - probas.shape[1]\n        probas = np.pad(probas, [(0, 0), (0, n_missing_classes)], \"constant\")\n        probas[:, -n_missing_classes:] = 1 / n_missing_classes\n    return probas\n\n", "CodeBleu_score": 39.66234994479429}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n\n    # Apply PCA to each group of features\n    X_transformed = []\n    for group in pcas:\n        X_transformed.append(pcas[group].transform(X[group].values))\n    X_transformed = np.concatenate(X_transformed, axis=1)\n\n    # Replace missing values with zeros\n    X_transformed = np.nan_to_num(X_transformed, copy=False)\n\n    # Predict class probabilities\n    probas = clf.predict_proba(X_transformed)\n\n    # Adjust probabilities to match the expected class structure\n    if len(probas[0]) != len(groups):\n        probas = probas[:, :len(groups)]\n        probas = np.concatenate((probas, 1 - probas.sum(axis=1, keepdims=True)), axis=1)\n    return probas\n\n", "CodeBleu_score": 36.189805642006924}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\n    # Apply PCA to each group and concatenate results\n    X_pca = np.concatenate([pcas[i].transform(X[:, groups[i]]) for i in range(len(groups))], axis=1)\n    # Replace missing values with zeros\n    X_pca = np.nan_to_num(X_pca)\n    # Compute class probabilities\n    y_proba = clf.predict_proba(X_pca)\n    # Check if the number of predicted classes matches the expected number of classes\n    if y_proba.shape[1] != self.n_classes_:\n        # If not, adjust the probabilities to match the expected class structure\n        y_proba = self._adjust_proba(y_proba)\n    return y_proba\n\n", "CodeBleu_score": 15.651186410291205}]}
{"instance_id": "9436ee0860bb", "generation_result": [{"code": "def predict_proba(self, X)\n\n    if not self.fitted:\n        raise Exception(\"Model not fitted yet.\")\n\n    if not isinstance(X, np.ndarray):\n        raise TypeError(\"X must be a numpy array.\")\n\n    if len(X.shape) == 1:\n        X = X.reshape(1, -1)\n    elif len(X.shape) == 2:\n        X = X.reshape(X.shape[0], -1)\n    elif len(X.shape) == 3:\n        if X.shape[2] != 1:\n            raise ValueError(\"Invalid input shape.\")\n        X = X.reshape(X.shape[0], -1)\n    else:\n        raise ValueError(\"Invalid input shape.\")\n\n    if not np.issubdtype(X.dtype, np.number):\n        raise TypeError(\"X must contain numeric values.\")\n\n    if np.any(np.isnan(X)):\n        raise ValueError(\"X must not contain NaN values.\")\n\n    if np.any(np.isinf(X)):\n        raise ValueError(\"X must not contain infinite values.\")\n\n    if np.any(X < 0):\n        raise ValueError(\"X must not contain negative values.\")\n\n    if np.any(X > 1):\n        raise ValueError(\"X must not contain values greater than 1.\")\n\n    if np.any(np.isnan(self.root.distribution)):\n        raise ValueError(\"Root node distribution must not contain NaN values.\")\n\n    if np.any(np.isinf(self.root.distribution)):\n        raise ValueError(\"Root node distribution must not contain infinite values.\")\n\n    if np.any(self.root.distribution < 0):\n        raise ValueError(\"Root node distribution must not contain negative values.\")\n\n    if np.any(self.root.distribution > 1):\n        raise ValueError(\"Root node distribution must not contain values greater than 1.\")\n\n    if np.sum(self.root.distribution) != 1:\n        raise ValueError(\"Root node distribution must sum to 1.\")\n\n    probabilities = np.zeros((X.shape[0], self.n_classes))\n\n    for", "CodeBleu_score": 37.53248300550502}, {"code": "def predict_proba(self, X)\n    if not self.fitted:\n        raise Exception(\"Model must be fitted before it can be used for prediction.\")\n\n    if not isinstance(X, np.ndarray):\n        raise TypeError(\"Invalid input type.\")\n\n    if len(X.shape) == 3:\n        if X.shape[2] != 1:\n            raise ValueError(\"Invalid input shape.\")\n        else:\n            X = np.reshape(X, (X.shape[0], X.shape[1]))\n\n    if len(X.shape) != 2:\n        raise ValueError(\"Invalid input shape.\")\n\n    X = np.array(X, dtype=np.float64)\n    X = np.nan_to_num(X)\n\n    n = X.shape[0]\n    m = self.n_classes\n    probabilities = np.zeros((n, m))\n\n    for i in range(n):\n        probabilities[i, :] = self.root.predict_proba(X[i, :])\n\n    return probabilities", "CodeBleu_score": 39.956479592661886}, {"code": "def predict_proba(self, X)\n    if not self.is_fitted:\n        raise NotFittedError(\"Model is not fitted.\")\n    if not isinstance(X, (np.ndarray, pd.DataFrame)):\n        raise TypeError(\"Invalid input type.\")\n    if isinstance(X, np.ndarray) and X.ndim > 2:\n        raise ValueError(\"Invalid input shape.\")\n    if isinstance(X, np.ndarray) and X.ndim == 2 and X.shape[1] != 1:\n        raise ValueError(\"Invalid input shape.\")\n    X = self._validate_input(X)\n    if X.shape[0] == 0:\n        return np.array([])\n    if self.num_classes == 1:\n        return np.ones((X.shape[0], 1))\n    proba = np.zeros((X.shape[0], self.num_classes))\n    for i in range(X.shape[0]):\n        proba[i] = self.root.predict_proba(X[i])\n    return proba", "CodeBleu_score": 40.64283298873737}]}
{"instance_id": "d7cb1cb66840", "generation_result": [{"code": "def _convert_X_to_sklearn(self, X)\n    if self.transformers_ == \"Primitives\":\n        X = self.convert_to_Primitives(X)\n        return X\n    elif self.transformers_ == \"Series\":\n        X = self.convert_to_Series(X)\n        return X\n    else:\n        raise TypeError(\n            \"The output scitype of the transformers is neither Primitives nor Series. Please specify a valid transformer.\"\n        )", "CodeBleu_score": 24.612454423656366}, {"code": "def _convert_X_to_sklearn(self, X)\n    X_ = X\n    # Convert to 2D numpy array\n    if not isinstance(X_, pd.DataFrame):\n        X_ = pd.DataFrame(X_)\n    X_ = X_.to_numpy()\n    # Convert to 2D numpy array with Table scitype\n    if self.transformers_ == \"Primitives\":\n        X_ = X_.reshape(len(X_), -1)\n    # Convert to flat 2D numpy array with Panel scitype\n    elif self.transformers_ == \"Series\":\n        X_ = X_.reshape(len(X_), -1)\n    # Raise TypeError if scitype is neither Primitives nor Series\n    else:\n        raise TypeError(\n            \"The scitype of the transformed output is not Primitives or Series. Please contact the maintainer.\"\n        )\n    return X_", "CodeBleu_score": 37.82581454223928}, {"code": "def _convert_X_to_sklearn(self, X)\n    from sktime.datatypes._panel._convert import from_2d_array_to_nested\n    from sktime.datatypes._panel._convert import from_nested_to_2d_array\n    from sktime.datatypes._panel._convert import from_long_to_nested\n    from sktime.datatypes._panel._convert import from_nested_to_long\n    from sktime.datatypes._panel._convert import from_3d_numpy_to_2d_numpy\n    from sktime.datatypes._panel._convert import from_2d_numpy_to_3d_numpy\n    from sktime.datatypes._panel._convert import from_dict_to_nested\n    from sktime.datatypes._panel._convert import from_nested_to_dict\n    from sktime.datatypes._panel._convert import from_nested_to_multi_index\n    from sktime.datatypes._panel._convert import from_multi_index_to_nested\n    from sktime.datatypes._panel._convert import from_recarray_to_nested\n    from sktime.datatypes._panel._convert import from_nested_to_recarray\n    from sktime.datatypes._panel._convert import from_nested_to_flat\n    from sktime.datatypes._panel._convert import from_flat_to_nested\n    from sktime.datatypes._panel._convert import from_df_to_nested\n    from sktime.datatypes._panel._convert import from_nested_to_df\n    from sktime.datatypes._panel._convert import from_nested_to_univariate_series\n    from sktime.datatypes._panel._convert import from_univariate_series_to_nested\n    from sktime.datatypes._panel._convert import from_dict_to_df\n    from sktime.datatypes._panel._convert import from_df_to_dict\n    from sktime.datatypes._panel._convert import from_dict_to_series\n    from sktime.datatypes._panel._convert import from_series", "CodeBleu_score": 6.359015217976929}]}
{"instance_id": "497cb75f2b4d", "generation_result": [{"code": "def set_random_state(estimator, random_state=0)\n\n    import numpy as np\n    from sklearn.utils.validation import check_random_state\n\n    random_state = check_random_state(random_state)\n\n    params = {}\n\n    for p in estimator.get_params().keys():\n        if p.endswith('random_state') or p.endswith('RandomState'):\n            params[p] = random_state.randint(np.iinfo(np.int32).max)\n\n    if params:\n        estimator.set_params(**params)\n\n    return estimator", "CodeBleu_score": 50.25117870327854}, {"code": "def set_random_state(estimator, random_state=0)\n    from sklearn.utils.validation import check_random_state\n    from sklearn.base import BaseEstimator\n    from sklearn.utils.estimator_checks import check_estimator\n    from sklearn.utils.validation import check_array\n    from sklearn.utils.validation import check_X_y\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.validation import NotFittedError\n    from sklearn.utils.validation import _check_sample_weight\n    from sklearn.utils.validation import check_consistent_length\n    from sklearn.utils.validation import _num_samples\n    from sklearn.utils.validation import check_array\n    from sklearn.utils.validation import check_random_state\n    from sklearn.utils.validation import _check_sample_weight\n    from sklearn.utils.validation import _deprecate_positional_args\n    from sklearn.utils.validation import check_memory\n    from sklearn.utils.validation import check_non_negative\n    from sklearn.utils.validation import check_scalar\n    from sklearn.utils.validation import check_string\n    from sklearn.utils.validation import check_column_index\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.validation import FLOAT_DTYPES\n    from sklearn.utils.validation import check_array\n    from sklearn.utils.validation import _check_sample_weight\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.validation import check_memory\n    from sklearn.utils.validation import _check_sample_weight\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.validation import check_array\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.validation import check_memory\n    from sklearn.utils.validation import check_array\n    from sklearn.utils.validation import check_is_fitted\n    from sklearn.utils.validation import check_memory\n    from sklearn.utils.validation import check_array\n    from sklearn.utils", "CodeBleu_score": 7.045416878423927}, {"code": "def set_random_state(estimator, random_state=0)\n    from sklearn.utils.validation import check_random_state\n    random_state = check_random_state(random_state)\n    params_to_update = {}\n    for param in estimator.get_params():\n        if param.endswith('random_state') and estimator.get_params()[param] is None:\n            params_to_update[param] = random_state.randint(1000000)\n    if len(params_to_update) > 0:\n        estimator.set_params(**params_to_update)\n    return estimator", "CodeBleu_score": 37.88276768428276}]}
{"instance_id": "59ffd1d596a1", "generation_result": [{"code": "def _transform(X, intervals)\n    n_instances, n_features = X.shape\n    n_intervals = len(intervals)\n    transformed = np.empty((n_features, n_intervals * 3))\n    for i in range(n_intervals):\n        transformed[:, i * 3] = np.mean(X[:, intervals[i]], axis=1)\n        transformed[:, i * 3 + 1] = np.std(X[:, intervals[i]], axis=1)\n        transformed[:, i * 3 + 2] = np.mean(np.diff(X[:, intervals[i]], axis=1))\n    return transformed.T\n\n", "CodeBleu_score": 26.474898643147093}, {"code": "def _transform(X, intervals)\n    # Calculate the number of instances and intervals\n    n_instances, n_features = X.shape\n    n_intervals = len(intervals)\n\n    # Initialize an empty array to store the transformed features\n    transformed = np.zeros((n_features, n_intervals))\n\n    # For each interval, slice the input matrix, compute the mean, standard deviation, and slope of the slice, and store these values in the transformed array\n    for i, interval in enumerate(intervals):\n        transformed[:, i] = np.array([np.mean(X[:, interval]), np.std(X[:, interval]), np.mean(np.diff(X[:, interval]))])\n\n    # Return the transposed transformed array\n    return transformed.T\n\n", "CodeBleu_score": 18.45209934036939}, {"code": "def _transform(X, intervals)\n    # Initialize the number of instances and intervals\n    n_instances, n_intervals = X.shape[0], len(intervals)\n\n    # Initialize an empty array to store the transformed features\n    X_transformed = np.empty((n_instances, 3 * n_intervals))\n\n    # For each interval, slice the input matrix, compute the mean, standard deviation, and slope of the slice, and store these values in the transformed array\n    for i, interval in enumerate(intervals):\n        X_slice = X[:, interval[0]:interval[1]]\n        X_transformed[:, 3 * i] = np.mean(X_slice, axis=1)\n        X_transformed[:, 3 * i + 1] = np.std(X_slice, axis=1)\n        X_transformed[:, 3 * i + 2] = np.mean(np.diff(X_slice, axis=1), axis=1)\n\n    # Return the transposed transformed array\n    return X_transformed.T\n\n", "CodeBleu_score": 33.10312736872775}]}
{"instance_id": "5a01594eebfb", "generation_result": [{"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\n    # initialize annotation and operation parameters if they are not provided\n    if annotation_params is None:\n        annotation_params = dict()\n    if ax_op_before is None:\n        ax_op_before = list()\n    if ax_op_after is None:\n        ax_op_after = list()\n\n    # create plot with given parameters\n    plot_output = getattr(sns, plot)(**plot_params, **kwargs)\n\n    # apply pre-plot axis operations\n    for ax_op in ax_op_before:\n        if isinstance(ax_op, str):\n            getattr(plot_output.axes, ax_op)()\n        elif isinstance(ax_op, list):\n            getattr(plot_output.axes, ax_op[0])(*ax_op[1:])\n        elif isinstance(ax_op, dict):\n            getattr(plot_output.axes, list(ax_op.keys())[0])(\n                **list(ax_op.values())[0])\n\n    # configure and annotate plot based on provided configuration and annotation functions\n    if configuration is not None:\n        plot_output.set(**configuration)\n    if annotation_func is not None:\n        getattr(self, annotation_func)(plot_output, *args, **annotation_params)\n\n    # apply post-plot axis operations\n    for ax_op in ax_op_after:\n        if isinstance(ax_op, str):\n            getattr(plot_output.axes, ax_op)()\n        elif isinstance(ax_op, list):\n            getattr(plot_output.axes, ax_op[0])(*ax_op[1:])\n        elif isinstance(ax_op, dict):\n            getattr(plot_output.axes, list(ax_op.keys())[0])(\n                **list(ax_op.values())[0])\n\n    # return output of plotting process\n    return plot_output\n\n", "CodeBleu_score": 46.198741081412834}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n    if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n    if not isinstance(ax_op_before, list):\n        raise TypeError('ax_op_before must be a list')\n    if not isinstance(ax_op_after, list):\n        raise TypeError('ax_op_after must be a list')\n    if not isinstance(annotation_params, dict):\n        raise TypeError('annotation_params must be a dictionary')\n    if not isinstance(annotate_params, dict):\n        raise TypeError('annotate_params must be a dictionary')\n    if not isinstance(configuration, dict):\n        raise TypeError('configuration must be a dictionary')\n    if not isinstance(plot_params, dict):\n        raise TypeError('plot_params must be a dictionary')\n    if not isinstance(plot, str):\n        raise TypeError('plot must be a string')\n    if not isinstance(annotation_func, str):\n        raise TypeError('annotation_func must be a string')\n    if not isinstance(ax_op_before, list):\n        raise TypeError('ax_op_before must be a list')\n    if not isinstance(ax_op_after, list):\n        raise TypeError('ax_op_after must be a list')\n    if not isinstance(annotation_params, dict):\n        raise TypeError('annotation_params must be a dictionary')\n    if not isinstance(annotate_params, dict):\n        raise TypeError('annotate_params must be a dictionary')\n    if not isinstance(configuration, dict):\n        raise TypeError('configuration must be a dictionary')\n    if not isinstance(plot_params, dict):\n        raise TypeError('plot_params must be a dictionary')\n    if not isinstance(plot, str):\n        raise TypeError('plot must be a string')\n    if not isinstance(annotation_func, str):\n        raise TypeError('annotation_func must be a string')\n    if not isinstance(ax_op_before, list):\n        raise", "CodeBleu_score": 33.867076993735004}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\n\n    # Initialize annotation and operation parameters\n    if annotation_params is None:\n        annotation_params = {}\n    if ax_op_before is None:\n        ax_op_before = []\n    if ax_op_after is None:\n        ax_op_after = []\n\n    # Create the plot\n    plot_func = getattr(sns, plot)\n    plot_func(**plot_params)\n\n    # Apply pre-plot axis operations\n    for op in ax_op_before:\n        if isinstance(op, str):\n            getattr(plt.gca(), op)(**kwargs)\n        elif isinstance(op, list):\n            getattr(plt.gca(), op[0])(*op[1], **kwargs)\n        elif isinstance(op, dict):\n            getattr(plt.gca(), list(op.keys())[0])(**op[list(op.keys())[0]])\n\n    # Configure the plot\n    for key, value in configuration.items():\n        getattr(plt.gca(), key)(value)\n\n    # Annotate the plot\n    annotation_func = getattr(self, annotation_func)\n    annotation_func(*args, **annotation_params)\n\n    # Apply post-plot axis operations\n    for op in ax_op_after:\n        if isinstance(op, str):\n            getattr(plt.gca(), op)(**kwargs)\n        elif isinstance(op, list):\n            getattr(plt.gca(), op[0])(*op[1], **kwargs)\n        elif isinstance(op, dict):\n            getattr(plt.gca(), list(op.keys())[0])(**op[list(op.keys())[0]])\n\n    # Return the output of the plotting process\n    return plt.gca()\n\n", "CodeBleu_score": 44.07905531618525}]}
{"instance_id": "b5b17d312c74", "generation_result": [{"code": "def predict_proba(self, X)\n\n    if not hasattr(self, '_program'):\n        raise RuntimeError('Model has not been fitted yet')\n\n    X = check_array(X, self.dtype)\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError('Number of features of the model must match the input. Model n_features_in_ is {0} and input X.shape[1] is {1}.'.format(self.n_features_in_, X.shape[1]))\n\n    scores = self._program.execute(X)\n    probas = self.out_activation_function(scores)\n    probas = np.vstack([1 - probas, probas]).transpose()\n    return probas", "CodeBleu_score": 57.311628485812015}, {"code": "def predict_proba(self, X)\n    if not hasattr(self, '_program'):\n        raise Exception('The model has not been fitted yet.')\n    X = self._validate_input(X)\n    if X.shape[1] != self._n_features:\n        raise Exception('The number of features in the input data does not match the number of features used to fit the model.')\n    scores = self._program.compute_scores(X)\n    probabilities = self._transform_scores_to_probabilities(scores)\n    return probabilities", "CodeBleu_score": 37.28005785919407}, {"code": "def predict_proba(self, X)\n    if not hasattr(self, '_program'):\n        raise Exception(\"The model has not been fitted yet.\")\n    X = self._validate_data(X)\n    if X.shape[1] != self._n_features:\n        raise Exception(\"The number of features in X does not match the number of features in the model.\")\n    scores = self._program.execute(X)\n    probabilities = self._transform_scores(scores)\n    return probabilities", "CodeBleu_score": 36.49933094624724}]}
{"instance_id": "83f71c959df7", "generation_result": [{"code": "def point_mutation(self, random_state)\n    import random\n    import numpy as np\n    from . import helper\n    from . import constants\n    from . import config\n\n    # Copy the program\n    program = self.copy()\n\n    # Get the node list\n    node_list = program.get_node_list()\n\n    # Get the number of nodes\n    n_nodes = len(node_list)\n\n    # Get the number of features\n    n_features = program.n_features\n\n    # Get the number of constants\n    n_constants = program.n_constants\n\n    # Get the constant range\n    constant_range = program.constant_range\n\n    # Get the function set\n    function_set = program.function_set\n\n    # Get the terminal set\n    terminal_set = program.terminal_set\n\n    # Get the function arity dictionary\n    function_arity_dict = program.function_arity_dict\n\n    # Get the terminal arity dictionary\n    terminal_arity_dict = program.terminal_arity_dict\n\n    # Get the probability threshold\n    p_threshold = program.p_threshold\n\n    # Get the number of nodes to mutate\n    n_mutate = int(np.floor(n_nodes * p_threshold))\n\n    # If the number of nodes to mutate is 0, return the original program\n    if n_mutate == 0:\n        return program, []\n\n    # Create a list of indices to mutate\n    mutate_idx_list = random_state.choice(range(n_nodes), size=n_mutate, replace=False)\n\n    # For each index to mutate\n    for mutate_idx in mutate_idx_list:\n        # Get the node to mutate\n        node = node_list[mutate_idx]\n\n        # If the node is a function\n        if node.type == 'function':\n            # Get the node arity\n            arity = function_arity_dict[node.value]\n\n            # Get a list of valid replacement functions\n            valid_replacement_functions = [f for f in function_set if function_arity_dict[f] == arity]", "CodeBleu_score": 28.11298500733605}, {"code": "def point_mutation(self, random_state)\n\n    import random\n    import numpy as np\n    from tqdm import tqdm\n\n    # Copy program\n    mutated_program = self.copy()\n\n    # Identify nodes to mutate\n    mutation_nodes = []\n    for node_idx in range(len(self)):\n        if random_state.uniform(0, 1) <= self.mutation_probability:\n            mutation_nodes.append(node_idx)\n\n    # Mutate nodes\n    for node_idx in tqdm(mutation_nodes, desc='Mutating program'):\n\n        # If node is a function\n        if self[node_idx].function is not None:\n\n            # Identify valid replacement functions\n            valid_replacement_functions = []\n            for function_idx in range(len(self.function_set)):\n                if self.function_set[function_idx].arity == self[node_idx].arity:\n                    valid_replacement_functions.append(function_idx)\n\n            # Replace node with a valid replacement function\n            mutated_program[node_idx] = self.function_set[random.choice(valid_replacement_functions)]\n\n        # If node is a terminal\n        else:\n\n            # Identify valid replacement terminals\n            valid_replacement_terminals = []\n            if self[node_idx].value_type == 'constant':\n                for constant_idx in range(len(self.constant_set)):\n                    if self.constant_set[constant_idx] >= self.constant_range[0] and self.constant_set[constant_idx] <= self.constant_range[1]:\n                        valid_replacement_terminals.append(constant_idx)\n            elif self[node_idx].value_type == 'variable':\n                for feature_idx in range(self.num_features):\n                    valid_replacement_terminals.append(feature_idx)\n\n            # Replace node with a valid replacement terminal\n            mutated_program[node_idx] = self.terminal_set[random.choice(valid_replacement_terminals)]\n\n    return mutated_program, mutation_nodes", "CodeBleu_score": 30.123378022410996}, {"code": "def point_mutation(self, random_state)\n    import random\n    import copy\n    import numpy as np\n    from functions import get_valid_replacements, get_valid_terminals\n\n    def mutate_node(program, node_idx, random_state):\n        \"\"\"This function mutates a single node of a program.\"\"\"\n        # If the node is a function, find a valid replacement and update the node\n        if program.nodes[node_idx].type == 'func':\n            valid_replacements = get_valid_replacements(program.nodes[node_idx].value, program.nodes[node_idx].arity, program.nodes[node_idx].depth, program.constants, program.num_features)\n            program.nodes[node_idx].value = random.choice(valid_replacements)\n        # If the node is a terminal, replace it with either a constant or a variable, ensuring valid replacement\n        elif program.nodes[node_idx].type == 'term':\n            valid_terminals = get_valid_terminals(program.nodes[node_idx].value, program.nodes[node_idx].depth, program.constants, program.num_features)\n            program.nodes[node_idx].value = random.choice(valid_terminals)\n        return program\n\n    # Create a copy of the program\n    mutated_program = copy.deepcopy(self)\n    # Identify nodes to mutate based on a uniform random distribution and a probability threshold\n    mutated_nodes = np.random.uniform(size=len(mutated_program.nodes)) < random_state.uniform(0, 1)\n    # Mutate each node\n    for node_idx in range(len(mutated_nodes)):\n        if mutated_nodes[node_idx]:\n            mutated_program = mutate_node(mutated_program, node_idx, random_state)\n    return mutated_program, mutated_nodes", "CodeBleu_score": 30.504366422450367}]}
{"instance_id": "f1853251e671", "generation_result": [{"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate Arguments\n    close = verify_series(close)\n    length = int(length) if length and length > 0 else 5\n    mamode = mamode.lower() if mamode and isinstance(mamode, str) else \"ema\"\n    offset = get_offset(offset)\n\n    # Calculate Result\n    ma = moving_average(close=close, length=length, mamode=mamode)\n    bias = close.pct_change() - ma.pct_change()\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n\n    # Name & Category\n    bias.name = f\"BIAS_{length}\"\n    bias.category = \"momentum\"\n\n    return bias\n\n", "CodeBleu_score": 51.03017966397244}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate Arguments\n    close = verify_series(close)\n    length = int(length) if length and length > 0 else 14\n    mamode = mamode.lower() if mamode else \"sma\"\n    offset = get_offset(offset)\n\n    # Calculate Result\n    bias = close.rolling(length).apply(moving_average(mamode), raw=True)\n    bias = bias.fillna(method=\"bfill\")\n    bias = close.pct_change(periods=1) - bias.pct_change(periods=1)\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Name & Category\n    bias.name = f\"BIAS_{length}_{mamode}\"\n    bias.category = \"momentum\"\n\n    return bias\n\n", "CodeBleu_score": 29.998490903623658}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\n    # Validate Arguments\n    close = verify_series(close)\n    length = int(length) if length and length > 0 else 14\n    mamode = mamode.lower() if mamode and isinstance(mamode, str) else \"ema\"\n    min_periods = int(kwargs[\"min_periods\"]) if \"min_periods\" in kwargs and kwargs[\"min_periods\"] is not None else length\n    offset = get_offset(offset)\n\n    # Calculate Result\n    ma = moving_average(close=close, length=length, mamode=mamode, **kwargs)\n    bias = 100 * (close - ma) / ma\n\n    # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Name & Category\n    bias.name = f\"BIAS_{length}\"\n    bias.category = \"momentum\"\n\n    return bias\n\n\n", "CodeBleu_score": 43.29077338876046}]}
{"instance_id": "77825be37c24", "generation_result": [{"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    import pandas as pd\n    import requests\n    from datetime import datetime\n    import traceback\n    try:\n        if broker == 'hb':\n            url = f'https://api.huobi.pro/market/history/kline?period={ktype}&size={size}&symbol={code}'\n            r = requests.get(url)\n            data = r.json()\n            df = pd.DataFrame(data['data'])\n            df['date'] = [datetime.fromtimestamp(x/1000) for x in df['id']]\n            df = df[['date', 'open', 'high', 'low', 'close', 'amount']]\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n        elif broker == 'cb':\n            url = f'https://api.coinbase.com/v2/exchange-rates?currency={code}'\n            r = requests.get(url)\n            data = r.json()\n            df = pd.DataFrame(data['data']['rates'])\n            df['date'] = [datetime.fromtimestamp(x) for x in df.index]\n            df = df[['date', 'amount']]\n            df.columns = ['date', 'close']\n        elif broker == 'bt':\n            url = f'https://api.binance.com/api/v3/klines?symbol={code}&interval={ktype}&limit={size}'\n            r = requests.get(url)\n            data = r.json()\n            df = pd.DataFrame(data)\n            df['date'] = [datetime.fromtimestamp(x/1000) for x in df[0]]\n            df = df[['date', 1, 2, 3, 4, 5]]\n            df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n        else:\n            print('Invalid broker')\n        df['date'] = pd.to_datetime(df['date'])\n        df = df.set_index('date')\n        return df\n    except:\n        traceback.print_exc()", "CodeBleu_score": 34.02680141775994}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    import pandas as pd\n    import traceback\n    import datetime\n\n    try:\n        if broker == 'hb':\n            url = 'https://api.huobi.pro/market/history/kline?period={}&size={}&symbol={}'.format(ktype, size, code)\n            data = pd.DataFrame(pd.read_json(url)['data'])\n            data.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'amount']\n            data['date'] = data['date'].astype(int)\n            data['date'] = pd.to_datetime(data['date'], unit='s')\n        elif broker == 'bt':\n            url = 'https://api.btc.com/v1/kline/period?period={}&size={}&symbol={}'.format(ktype, size, code)\n            data = pd.DataFrame(pd.read_json(url)['data'])\n            data.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'amount']\n            data['date'] = data['date'].astype(int)\n            data['date'] = pd.to_datetime(data['date'], unit='s')\n        elif broker == 'ok':\n            url = 'https://www.okex.com/api/v1/kline.do?symbol={}-usdt&type={}&size={}'.format(code, ktype, size)\n            data = pd.DataFrame(pd.read_json(url)['kline'])\n            data.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'amount']\n            data['date'] = data['date'].astype(int)\n            data['date'] = pd.to_datetime(data['date'], unit='ms')\n        elif broker == 'cb':\n            url = 'https://api.coinbase.com/v2/exchange-rates?currency={}'.format(code)\n            data = pd.DataFrame(pd.read_json(url)['data']['rates'])\n            data.columns = ['date', 'open', 'high', 'low', 'close', 'volume', '", "CodeBleu_score": 28.408489732333177}, {"code": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')\n    import pandas as pd\n    import requests\n    import json\n    import traceback\n    try:\n        if broker == 'hb':\n            url = f'https://api.huobi.pro/market/history/kline?period={ktype}&size={size}&symbol={code}'\n            data = requests.get(url).json()\n            df = pd.DataFrame(data['data'])\n            df = df.rename(columns={'id': 'date', 'vol': 'volume'})\n            df['date'] = pd.to_datetime(df['date'], unit='s')\n            df = df[['date', 'open', 'high', 'low', 'close', 'volume']]\n        elif broker == 'bin':\n            url = f'https://api.binance.com/api/v3/klines?symbol={code}&interval={ktype}&limit={size}'\n            data = requests.get(url).json()\n            df = pd.DataFrame(data)\n            df = df.rename(columns={0: 'date', 1: 'open', 2: 'high', 3: 'low', 4: 'close', 5: 'volume'})\n            df['date'] = pd.to_datetime(df['date'], unit='ms')\n            df = df[['date', 'open', 'high', 'low', 'close', 'volume']]\n        elif broker == 'bittrex':\n            url = f'https://api.bittrex.com/v3/markets/{code}/candles/{ktype}'\n            data = requests.get(url).json()\n            df = pd.DataFrame(data)\n            df = df.rename(columns={'startsAt': 'date', 'open': 'open', 'high': 'high', 'low': 'low', 'close': 'close', 'volume': 'volume'})\n            df['date'] = pd.to_datetime(df['date'], unit='s')\n            df = df[['date', 'open', 'high', 'low', 'close', 'volume']]\n        elif broker == 'coinbase':\n            url = f'https://api.pro.coinbase.com/", "CodeBleu_score": 33.391156777986396}]}
{"instance_id": "c6552851da8a", "generation_result": [{"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    try:\n        # Fetch data\n        data = coins_snapshot_helper(broker, code, size)\n        # Process data\n        if broker == 'hb':\n            # Process data\n            data = data.split('[')[1].split(']')[0].split('},{')\n            data = [x.split(',') for x in data]\n            data = [[x[0].split(':')[1], x[1].split(':')[1], x[2].split(':')[1], x[3].split(':')[1], x[4].split(':')[1]] for x in data]\n            data = pd.DataFrame(data, columns=['ask', 'bid', 'ask_vol', 'bid_vol', 'timestamp'])\n            data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n        elif broker == 'okc':\n            # Process data\n            data = data['ticker']['data']\n            data = [[x['ask'], x['bid'], x['ask_vol'], x['bid_vol'], x['timestamp']] for x in data]\n            data = pd.DataFrame(data, columns=['ask', 'bid', 'ask_vol', 'bid_vol', 'timestamp'])\n            data['timestamp'] = pd.to_datetime(data['timestamp'])\n        # Create dataframe\n        data_ask = pd.DataFrame(data['ask'], columns=['ask'])\n        data_bid = pd.DataFrame(data['bid'], columns=['bid'])\n        # Store dataframes in dictionary\n        data_dict = {'ask': data_ask, 'bid': data_bid}\n        # Create panel\n        panel = pd.Panel(data_dict)\n        # Return panel\n        return panel\n    except:\n        # Print traceback\n        print(traceback.format_exc())\n\n", "CodeBleu_score": 39.32420419552793}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    try:\n        data = broker_snapshot(broker, code, size)\n        if broker == 'hb':\n            time = data['timestamp']\n            time = time.split('T')\n            time = time[1].split(':')\n            time = time[0] + ':' + time[1] + ':' + time[2]\n            ask = pd.DataFrame(data['asks'], columns=['price', 'size'])\n            bid = pd.DataFrame(data['bids'], columns=['price', 'size'])\n        elif broker == 'bt':\n            time = data['timestamp']\n            time = time.split('T')\n            time = time[1].split(':')\n            time = time[0] + ':' + time[1] + ':' + time[2]\n            ask = pd.DataFrame(data['asks'], columns=['price', 'size'])\n            bid = pd.DataFrame(data['bids'], columns=['price', 'size'])\n        elif broker == 'cb':\n            time = data['timestamp']\n            time = time.split('T')\n            time = time[1].split(':')\n            time = time[0] + ':' + time[1] + ':' + time[2]\n            ask = pd.DataFrame(data['asks'], columns=['price', 'size'])\n            bid = pd.DataFrame(data['bids'], columns=['price', 'size'])\n        elif broker == 'bx':\n            time = data['timestamp']\n            time = time.split('T')\n            time = time[1].split(':')\n            time = time[0] + ':' + time[1] + ':' + time[2]\n            ask = pd.DataFrame(data['asks'], columns=['price', 'size'])\n            bid = pd.DataFrame(data['bids'], columns=['price', 'size'])\n        elif broker == 'ex':\n            time = data['timestamp']\n            time = time.split('T')\n            time = time[1].split(':')\n            time = time[0] + ':' + time[1] + ':' + time[2]\n            ask = pd.", "CodeBleu_score": 33.76540768286797}, {"code": "def coins_snapshot(broker='hb', code='btc', size='5')\n    import json\n    import pandas as pd\n    import numpy as np\n    import requests\n    from datetime import datetime\n    from time import sleep\n    from time import time\n    from broker_keys import keys\n\n    # get the broker key\n    broker_key = keys[broker]['key']\n\n    # set the broker url\n    if broker == 'hb':\n        url = 'https://api.huobi.pro/market/depth?symbol=' + code + 'usdt&type=step0'\n    elif broker == 'cb':\n        url = 'https://api.coinbase.com/v2/exchange-rates?currency=' + code\n\n    # get the data\n    while True:\n        try:\n            response = requests.get(url, headers={'Authorization': broker_key})\n            data = json.loads(response.text)\n            break\n        except:\n            print('Error retrieving data. Retrying...')\n            sleep(1)\n\n    # process the data\n    if broker == 'hb':\n        # get the timestamp\n        timestamp = data['ts']\n        # process the ask data\n        ask_data = data['tick']['asks']\n        ask_data = pd.DataFrame(ask_data)\n        ask_data = ask_data.transpose()\n        ask_data.columns = ['ask_price', 'ask_size']\n        ask_data['timestamp'] = timestamp\n        # process the bid data\n        bid_data = data['tick']['bids']\n        bid_data = pd.DataFrame(bid_data)\n        bid_data = bid_data.transpose()\n        bid_data.columns = ['bid_price', 'bid_size']\n        bid_data['timestamp'] = timestamp\n    elif broker == 'cb':\n        # get the timestamp\n        timestamp = datetime.fromtimestamp(int(data['data']['timestamp']))\n        # process the ask data\n        ask_data = data['data']['rates'][code + '-USD']\n        ask_data = pd.DataFrame(ask_data, index=[0])\n        ask_data.columns = ['ask_price']\n        ask_data['ask_size", "CodeBleu_score": 35.390156181112424}]}
{"instance_id": "8b5fee723398", "generation_result": [{"code": "def coins_trade(broker='hb', code='btc')\n    if broker == 'hb':\n        # fetch data\n        data = get_data(f'https://api.huobi.pro/market/history/trade?symbol={code}usdt&size=1000')\n        # process data\n        if data is not None:\n            data = data['tick']['data']\n            data = pd.DataFrame(data)\n            data = data[['id', 'ts', 'price', 'amount', 'direction']]\n            data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            data['datetime'] = pd.to_datetime(data['datetime'], unit='ms')\n            data = data[['tid', 'datetime', 'price', 'amount', 'type']]\n            return data\n    elif broker == 'ok':\n        # fetch data\n        data = get_data(f'https://www.okex.com/api/spot/v3/instruments/{code}-usdt/trades')\n        # process data\n        if data is not None:\n            data = data['data']\n            data = pd.DataFrame(data)\n            data = data[['tid', 'timestamp', 'price', 'amount', 'type']]\n            data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            data['datetime'] = pd.to_datetime(data['datetime'], unit='ms')\n            data = data[['tid', 'datetime', 'price', 'amount', 'type']]\n            return data\n    elif broker == 'chbtc':\n        # fetch data\n        data = get_data(f'https://api.chbtc.com/data/v1/trades?currency={code}_usdt&since=0')\n        # process data\n        if data is not None:\n            data = data['trades']\n            data = pd.DataFrame(data)\n            data = data[['tid', 'date', 'price', 'amount', 'type']]\n            data.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n            data['datetime'] = pd.to_datetime(data['datetime'], unit='ms')\n            data = data[['tid', '", "CodeBleu_score": 41.39072404997981}, {"code": "def coins_trade(broker='hb', code='btc')\n    if broker == 'hb':\n        url = 'http://api.huobi.com/staticmarket/ticker_btc_json.js'\n        data = get_json(url)\n        if data is not None:\n            df = pd.DataFrame(data['ticker']['data'])\n            df.columns = ['tid', 'price', 'amount', 'type']\n            df['datetime'] = pd.to_datetime(df['tid'], unit='s')\n            df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n            df['amount'] = df['amount'].astype(float)\n            df['price'] = df['price'].astype(float)\n            df['amount'] = df['amount'] / 100000000\n            df['price'] = df['price'] / 100000000\n            df['type'] = df['type'].map({'buy': 'buy', 'sell': 'sell'})\n            return df\n    elif broker == 'ok':\n        url = 'https://www.okcoin.cn/api/v1/trades.do?symbol=btc_usd'\n        data = get_json(url)\n        if data is not None:\n            df = pd.DataFrame(data['data'])\n            df.columns = ['tid', 'price', 'amount', 'type']\n            df['datetime'] = pd.to_datetime(df['date'], unit='ms')\n            df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n            df['amount'] = df['amount'].astype(float)\n            df['price'] = df['price'].astype(float)\n            df['amount'] = df['amount'] / 100000000\n            df['price'] = df['price'] / 100000000\n            df['type'] = df['type'].map({'ask': 'sell', 'bid': 'buy'})\n            return df\n    elif broker == 'chbtc':\n        url = 'https://api.chbtc.com/data/v1", "CodeBleu_score": 39.05119527200635}, {"code": "def coins_trade(broker='hb', code='btc')\n    import requests\n    import pandas as pd\n    import numpy as np\n    from datetime import datetime\n    from dateutil.tz import tzutc\n    import time\n\n    def hb_trade(broker='hb', code='btc'):\n        \"\"\"This function retrieves and processes trade data for a specified cryptocurrency from a given broker.\\nIt fetches data from a URL based on the broker and cryptocurrency code.\\nIf the data is not None, it processes the data differently depending on the broker ('hb', 'ok', 'chbtc').\\nFor 'hb', it selects and renames specific columns and converts timestamps.\\nFor 'ok' and 'chbtc', it similarly selects, renames columns, and converts timestamps.\\nFinally, it standardizes the column names to ['tid', 'datetime', 'price', 'amount', 'type'] and returns the processed Data\"\"\"\n        import requests\n        import pandas as pd\n        import numpy as np\n        from datetime import datetime\n        from dateutil.tz import tzutc\n        import time\n\n        url = 'https://api.huobi.pro/market/history/trade?symbol=' + code + 'usdt&size=2000'\n        r = requests.get(url)\n        if r.status_code == 200:\n            data = r.json()\n            data = pd.DataFrame(data['data'])\n            data = data[['id', 'price', 'amount', 'direction']]\n            data.columns = ['tid', 'price', 'amount', 'type']\n            data['datetime'] = data['tid'].apply(lambda x: datetime.fromtimestamp(x / 1000, tz=tzutc()))\n            data['datetime'] = data['datetime'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n            data['amount'] = data['amount'].astype(float)\n            data['price'] = data['price'].astype(float)\n            data['type'] = data['type'].apply(lambda x: 'buy' if x == 'buy-market' else 'sell')\n            data = data[['tid',", "CodeBleu_score": 33.83088502792157}]}
{"instance_id": "25e0a134c45b", "generation_result": [{"code": "def _parse_fund_data(url, fund_type='open')\n    try:\n        print('Requesting URL...')\n        response = requests.get(url)\n        print('Request received.')\n        print('Processing data...')\n        if response.status_code == 200:\n            text = response.text\n            text = text.replace('\"', '')\n            text = text.replace('\\n', '')\n            text = text.replace(' ', '')\n            text = text.replace(':', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',')\n            text = text.replace(',,', ',", "CodeBleu_score": 21.63916997338699}, {"code": "def _parse_fund_data(url, fund_type='open')\n\n    import pandas as pd\n    import requests\n    import json\n\n    try:\n        # Request the URL\n        response = requests.get(url)\n\n        # Handle text encoding\n        response.encoding = 'utf-8'\n\n        # Extract relevant data\n        data = response.text[1:-1]\n\n        # Format the text into valid JSON\n        data = data.replace('\"', '\\\"')\n        data = data.replace('\\\\\\\\n', '')\n        data = data.replace('\\\\\\\\t', '')\n        data = data.replace('\\\\\\\\', '')\n        data = data.replace('\\\\\"', '')\n        data = data.replace('\\\\', '')\n        data = data.replace('\\\\\\\\', '')\n        data = data.replace('\\\\\\\\\\\\\\\\', '')\n        data = data.replace('\\\\\\\\\\\\\\\\\\\\\\\\', '')\n        data = data.replace('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', '')\n        data = data.replace('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', '')\n        data = data.replace('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', '')\n        data = data.replace('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', '')\n        data = data.replace('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', '')\n        data = data.replace('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', '')\n        data = data.replace('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', '')\n        data = data.replace('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\", "CodeBleu_score": 20.834165836604623}, {"code": "def _parse_fund_data(url, fund_type='open')\n    print(f'Requesting {url}...')\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f'Error: {e}')\n        return None\n    else:\n        print('Success!')\n        print('Parsing data...')\n        text = response.text\n        text = text.replace('\\\\n', '')\n        text = text.replace('\\\\', '')\n        text = text.replace('\"{', '{')\n        text = text.replace('}\"', '}')\n        json_data = json.loads(text)\n        if fund_type == 'open':\n            fund_data = pd.DataFrame(json_data['data']['funds'])\n            fund_data = fund_data[['fund_id', 'fund_name', 'fund_type', 'fund_description', 'fund_inception', 'fund_size', 'fund_size_date', 'fund_assets', 'fund_assets_date', 'fund_expense_ratio', 'fund_expense_ratio_date', 'fund_aum', 'fund_aum_date', 'fund_aum_rank', 'fund_aum_rank_date', 'fund_aum_rank_category', 'fund_aum_rank_category_date', 'fund_aum_rank_percentile', 'fund_aum_rank_percentile_date', 'fund_aum_rank_category_percentile', 'fund_aum_rank_category_percentile_date', 'fund_aum_rank_category_percentile_group', 'fund_aum_rank_category_percentile_group_date', 'fund_aum_rank_category_percentile_group_rank', 'fund_aum_rank_category_percentile_group_rank_date', 'fund_aum_rank_category_percentile_group_rank_category', 'fund_aum_rank_category_percentile_group_rank_category_date', 'fund_aum_rank_category_percentile_group_rank_category_percentile', 'fund_aum_rank_category_percentile_group_rank_category_percentile_date', 'fund_aum_rank_category_", "CodeBleu_score": 26.999394201408112}]}
{"instance_id": "401d18e122d8", "generation_result": [{"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    import pandas as pd\n    from datetime import datetime\n    from datetime import timedelta\n    from datetime import date\n    import time\n    import warnings\n    warnings.filterwarnings('ignore')\n    \n    # Determine which function to call based on market\n    if market == 'CFFEX':\n        from cffex_future_daily import cffex_future_daily\n        func = cffex_future_daily\n    else:\n        from cffex_future_daily import cffex_future_daily\n        func = cffex_future_daily\n    \n    # Set default values for start and end dates\n    if start == None:\n        start = date.today()\n    if end == None:\n        end = date.today()\n    \n    # Convert dates to correct format\n    start = datetime.strptime(str(start), '%Y%m%d')\n    end = datetime.strptime(str(end), '%Y%m%d')\n    \n    # Initialize empty list to hold data frames\n    dfs = []\n    \n    # Iterate over each date in range, retrieving daily data and appending to list if data is found\n    delta = end - start\n    for i in range(delta.days + 1):\n        day = start + timedelta(days = i)\n        date = day.strftime('%Y%m%d')\n        try:\n            df = func(date)\n            dfs.append(df)\n        except:\n            pass\n    \n    # Concatenate and return list of data frames if any data was collected\n    if len(dfs) > 0:\n        df = pd.concat(dfs)\n        return df\n    else:\n        return None\n", "CodeBleu_score": 39.65724529862618}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n    import pandas as pd\n    from datetime import datetime\n    from datetime import timedelta\n    from datetime import date\n    from datetime import time\n    from datetime import date\n    from datetime import time\n    from datetime import datetime\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    from pandas.io.json import json_normalize\n    import re\n    from pandas import DataFrame\n    import pandas as pd\n    import numpy as np\n    import pandas as pd\n    import datetime\n    from datetime import datetime\n    from datetime import timedelta\n    from datetime import date\n    from datetime import time\n    from datetime import date\n    from datetime import time\n    from datetime import datetime\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    from pandas.io.json import json_normalize\n    import re\n    from pandas import DataFrame\n    import pandas as pd\n    import numpy as np\n    import pandas as pd\n    import datetime\n    from datetime import datetime\n    from datetime import timedelta\n    from datetime import date\n    from datetime import time\n    from datetime import date\n    from datetime import time\n    from datetime import datetime\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    from pandas.io.json import json_normalize\n    import re\n    from pandas import DataFrame\n    import pandas as pd\n    import numpy as np\n    import pandas as pd\n    import datetime\n    from datetime import datetime\n    from datetime import timedelta\n    from datetime import date\n    from datetime import time\n    from datetime import date\n    from datetime import time\n    from datetime import datetime\n    import time\n    import requests\n    import json\n    import numpy as np\n    import pandas as pd\n    from pandas.io.json import json_normalize\n    import re\n    from pandas import DataFrame\n    import pandas as pd\n    import numpy as np\n    import pandas as pd\n    import datetime\n    from datetime import datetime\n    from datetime import timedelta\n    from datetime import date\n    from datetime import time\n    from datetime import date\n    from datetime import time\n    from datetime import datetime\n    import time", "CodeBleu_score": 12.965859475377146}, {"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\n\n    # Determine the appropriate function to call\n    if market == 'CFFEX':\n        get_future_daily_data = get_cffex_daily\n    elif market == 'SHFE':\n        get_future_daily_data = get_shfe_daily\n    elif market == 'DCE':\n        get_future_daily_data = get_dce_daily\n    elif market == 'CZCE':\n        get_future_daily_data = get_czce_daily\n    elif market == 'INE':\n        get_future_daily_data = get_ine_daily\n    else:\n        get_future_daily_data = get_cffex_daily\n\n    # If no start date is provided, default to today\n    if start is None:\n        start = datetime.date.today()\n\n    # If no end date is provided, default to today\n    if end is None:\n        end = datetime.date.today()\n\n    # Convert the start and end dates to the correct format\n    start = datetime.datetime.strptime(str(start), '%Y-%m-%d').date()\n    end = datetime.datetime.strptime(str(end), '%Y-%m-%d').date()\n\n    # Initialize an empty list to hold data frames\n    data_frames = []\n\n    # Iterate over each date in the range\n    for date in daterange(start, end):\n\n        # Retrieve daily data using the appropriate function\n        df = get_future_daily_data(date)\n\n        # If data was found, append it to the list\n        if not df.empty:\n            data_frames.append(df)\n\n    # Concatenate and return the list of data frames if any data was collected\n    if data_frames:\n        return pd.concat(data_frames)\n    else:\n        return pd.DataFrame()\n\n", "CodeBleu_score": 45.131552704936425}]}
{"instance_id": "697779825988", "generation_result": [{"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n    import requests\n    import time\n    import pandas as pd\n    import json\n    import os\n    import sys\n    from pathlib import Path\n    from dotenv import load_dotenv\n\n    # Load API key from .env file\n    load_dotenv()\n    API_KEY = os.getenv(\"API_KEY\")\n\n    # Specify URL and headers\n    url = \"https://api.boxoffice.kr/api/realtimeBoxOffice\"\n    headers = {\"x-api-key\": API_KEY}\n\n    # Send request to API\n    for i in range(retry_count):\n        try:\n            response = requests.get(url, headers=headers)\n        except Exception as e:\n            print(e)\n            time.sleep(pause)\n            continue\n\n        # If response contains data, parse the JSON and create a DataFrame\n        if response.status_code == 200:\n            try:\n                data = json.loads(response.text)\n                df = pd.DataFrame(data[\"data2\"])\n                df.drop([\"MovieImg\", \"mId\"], axis=1, inplace=True)\n                df[\"timestamp\"] = pd.Timestamp.now()\n                return df\n            except Exception as e:\n                print(e)\n                time.sleep(pause)\n                continue\n\n        # If response does not contain data, retry\n        else:\n            time.sleep(pause)\n            continue\n\n    # If no data is returned after retry_count attempts, raise an exception\n    raise Exception(\"No data returned from API\")\n\n", "CodeBleu_score": 34.0986141713769}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n    import requests\n    import json\n    import pandas as pd\n    import time\n    import logging\n    import sys\n    from datetime import datetime\n\n    # Set up logging\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n    logger = logging.getLogger(__name__)\n\n    # Specify the URL\n    url = \"http://www.cgv.co.kr/common/showtimes/xml.json?key=43053824153666113\"\n\n    # Try up to retry_count times\n    for i in range(retry_count):\n        # Send a request to the URL\n        logger.info(\"Attempt {}\".format(i+1))\n        try:\n            response = requests.get(url)\n        except Exception as e:\n            logger.error(\"Error: {}\".format(e))\n            time.sleep(pause)\n            continue\n\n        # If the response contains data, parse the JSON and create a DataFrame\n        if response.status_code == 200:\n            try:\n                data = json.loads(response.text)\n                df = pd.DataFrame(data['data2'])\n            except Exception as e:\n                logger.error(\"Error: {}\".format(e))\n                time.sleep(pause)\n                continue\n\n            # Drop the 'MovieImg' and 'mId' columns\n            df = df.drop(columns=['MovieImg', 'mId'])\n\n            # Add a timestamp column\n            df['timestamp'] = datetime.now()\n\n            # Return the DataFrame\n            return df\n\n        # If the response does not contain data, log an error and try again\n        else:\n            logger.error(\"No data returned\")\n            time.sleep(pause)\n            continue\n\n    # If all attempts fail, raise an error\n    raise Exception(\"All attempts failed\")", "CodeBleu_score": 31.624101407364215}, {"code": "def realtime_boxoffice(retry_count=3,pause=0.001)\n    import requests\n    import time\n    import json\n    import pandas as pd\n    from pandas.io.json import json_normalize\n\n    #specify the url and headers\n    url = 'http://www.cbooo.cn/BoxOffice/GetRealtimeData'\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36',\n        'Referer': 'http://www.cbooo.cn/',\n        'Host': 'www.cbooo.cn',\n        'Origin': 'http://www.cbooo.cn',\n        'Accept-Encoding': 'gzip, deflate',\n        'Accept-Language': 'en-US,en;q=0.9',\n        'Connection': 'keep-alive',\n        'Accept': 'application/json, text/javascript, */*; q=0.01'\n    }\n\n    #send the request, read the response, and parse the JSON\n    for i in range(retry_count):\n        try:\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()\n            data = json.loads(response.text)\n            break\n        except:\n            if i < retry_count - 1:\n                time.sleep(pause)\n            else:\n                data = None\n                print('An error has occurred.')\n\n    #create a DataFrame from the 'data2' field, drop the 'MovieImg' and 'mId' columns, add a timestamp column, and return the DataFrame\n    if data is not None:\n        df = json_normalize(data['data2'])\n        df = df.drop(['MovieImg', 'mId'], axis=1)\n        df['timestamp'] = pd.to_datetime(df['time'])\n        return df\n    else:\n        return None\n", "CodeBleu_score": 33.712780835599624}]}
