{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n\n    import torch\n    import os\n    import sys\n    import yaml\n    from . import models\n    from . import utils\n\n    # Load configuration from YAML file\n    config_path = os.path.join(self.model_path, 'config.yaml')\n    with open(config_path, 'r') as config_file:\n        config = yaml.load(config_file, Loader=yaml.FullLoader)\n\n    # Load model from cache if available and valid\n    cache_path = os.path.join(self.model_path, 'model.pt')\n    if os.path.exists(cache_path):\n        model = torch.load(cache_path)\n        if model.config == config:\n            print('Loaded model from cache.')\n            return model\n        else:\n            print('Cache is invalid. Loading model from source.')\n\n    # Load model from source\n    if config['model']['type'] == 'safetensors':\n        model_path = os.path.join(self.model_path, 'model.safetensors')\n        model = torch.load(model_path)\n    else:\n        model_path = os.path.join(self.model_path, 'model.pt')\n        model = torch.load(model_path)\n\n    # Configure model\n    model.config = config\n    model.device = self.device\n    model.eval()\n\n    # Move model to device\n    if self.device == 'cuda':\n        model = model.cuda()\n\n    # Optionally use half-precision attention\n    if half_attention:\n        model.half_attention()\n\n    # Optionally use memory format optimization\n    if self.memory_format_opt:\n        model.memory_format_opt()\n\n    # Cache model for future use\n    torch.save(model, cache_path)\n\n    return model\n\n", "CodeBleu_score": 35.197548156920924}, {"code": "def load_model_from_config(self, half_attention)\n\n    # Check if a cached model is available and valid\n    if self.use_cache:\n        cache_path = self.cache_dir + self.model_name + \".pt\"\n        if os.path.exists(cache_path):\n            try:\n                model = torch.load(cache_path)\n                print(\"Loaded model from cache.\")\n                return model\n            except:\n                print(\"Failed to load model from cache.\")\n\n    # Load model from path\n    if self.model_path.endswith(\".safetensors\"):\n        model = torch.load(self.model_path)\n    else:\n        model = torch.load(self.model_path, map_location=torch.device(\"cpu\"))\n\n    # Configure model\n    model.eval()\n    model.to(self.device)\n    model.config.device = self.device\n    model.config.half_attention = half_attention\n    if self.use_cache:\n        torch.save(model, cache_path)\n        print(\"Saved model to cache.\")\n    return model\n\n", "CodeBleu_score": 29.738588207612022}, {"code": "def load_model_from_config(self, half_attention)\n\n    # Load model from cache if available\n    if self.cache_model and self.model_path in self.cached_models:\n        model = self.cached_models[self.model_path]\n        print(\"Loaded model from cache.\")\n\n    # Load model from file\n    else:\n        if self.model_path.endswith(\".safetensors\"):\n            model = self.load_model_from_safetensors(self.model_path)\n        else:\n            model = self.load_model_from_pytorch(self.model_path)\n\n        # Move model to device\n        model = model.to(self.device)\n\n        # Ensure model is in evaluation mode\n        model.eval()\n\n        # Cache model for future use\n        if self.cache_model:\n            self.cached_models[self.model_path] = model\n\n    # Configure model\n    self.configure_model(model, half_attention)\n\n    # Return model\n    return model\n\n", "CodeBleu_score": 24.076076376144197}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\n    sampler = DDIMSampler(model, eta=eta)\n    batch_size = shape[0]\n    print(f\"eta: {eta}\")\n    print(f\"steps: {steps}\")\n\n    if normals_sequence is None:\n        normals_sequence = sampler.sample_normals(steps, batch_size)\n\n    if x0 is None:\n        x0 = sampler.sample_x0(batch_size, shape, cond, x_t=x_t)\n\n    if quantize_x0:\n        x0 = x0.round()\n\n    samples, intermediates = sampler.sample(x0, steps, shape, cond, callback=callback, normals_sequence=normals_sequence,\n                                            mask=mask, temperature=temperature, score_corrector=score_corrector,\n                                            corrector_kwargs=corrector_kwargs)\n\n    return samples, intermediates\n\n", "CodeBleu_score": 47.00041783532854}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\n    if x0 is not None:\n        x0 = x0.to(device=model.device, dtype=model.dtype)\n        if quantize_x0:\n            x0 = x0.round()\n\n    sampler = DDIMSampler(model, eta=eta, score_corrector=score_corrector, **corrector_kwargs)\n    print(f\"sampling with eta={eta} for {steps} steps\")\n    print(f\"sampling with shape={shape}\")\n    if x0 is not None:\n        print(\"using provided initial x0\")\n    if mask is not None:\n        print(\"using provided mask\")\n    if normals_sequence is not None:\n        print(\"using provided normals_sequence\")\n    if x_t is not None:\n        print(\"using provided x_t\")\n\n    samples, intermediates = sampler.sample(\n        steps,\n        shape=shape,\n        conditioning=cond,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        temperature=temperature,\n        x_t=x_t,\n    )\n    return samples, intermediates\n\n", "CodeBleu_score": 47.9983779879647}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    print('eta:', eta, 'steps:', steps)\n    sampler = DDIMSampler(model, eta=eta)\n    batch_size = shape[0]\n    print(batch_size)\n    x = sampler.sample(steps, batch_size, shape, cond, callback=callback, normals_sequence=normals_sequence,\n                       mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                       score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n    return x, sampler.intermediates\n\n", "CodeBleu_score": 27.865968213767587}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if not self.ema_enabled:\n        return\n    if self.ema_weights is None:\n        self.ema_weights = self.get_weights()\n        self.ema_weights = [w.copy() for w in self.ema_weights]\n    if context is not None:\n        print(f\"{context}: Using EMA weights\")\n    self.set_weights(self.ema_weights)\n\n", "CodeBleu_score": 44.40157515097767}, {"code": "def ema_scope(self, context=None)\n    if self.ema_weights is None:\n        return\n    if context is not None:\n        print(f\"{context}: Using EMA weights\")\n    self.model.set_weights(self.ema_weights)\n    yield\n    self.model.set_weights(self.weights)\n    if context is not None:\n        print(f\"{context}: Restored original weights\")", "CodeBleu_score": 54.44448657394065}, {"code": "def ema_scope(self, context=None)\n    if context is not None:\n        print(\"EMA weights are being used.\")\n\n    # Save the original weights\n    original_weights = self.get_weights()\n\n    # Replace the weights with EMA weights\n    self.set_weights(self.get_weights(ema=True))\n\n    yield\n\n    # Restore the original weights\n    self.set_weights(original_weights)\n\n    if context is not None:\n        print(\"Original weights have been restored.\")", "CodeBleu_score": 41.30509710381286}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    if ignore_keys is None:\n        ignore_keys = []\n    state_dict = torch.load(path)\n    for key in ignore_keys:\n        if key in state_dict:\n            del state_dict[key]\n    self.load_state_dict(state_dict, strict=False)\n    self.print_missing_and_unexpected_keys(state_dict)\n\n", "CodeBleu_score": 17.928580501997683}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    import torch\n    import os\n    import logging\n    import re\n\n    logging.info(\"Loading model from checkpoint at '{}'\".format(path))\n    if not os.path.exists(path):\n        raise FileNotFoundError(\"No checkpoint found at '{}'\".format(path))\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    state_dict = checkpoint[\"state_dict\"]\n    if ignore_keys:\n        for key in ignore_keys:\n            if key in state_dict:\n                del state_dict[key]\n    missing_keys, unexpected_keys = self.load_state_dict(state_dict, strict=False)\n    if len(missing_keys) > 0:\n        logging.warning(\"Missing keys in state dict: {}\".format(missing_keys))\n    if len(unexpected_keys) > 0:\n        logging.warning(\"Unexpected keys in state dict: {}\".format(unexpected_keys))\n    return self\n\n", "CodeBleu_score": 35.45438570452264}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    if ignore_keys is None:\n        ignore_keys = []\n    checkpoint = torch.load(path)\n    state_dict = checkpoint['state_dict']\n    for key in ignore_keys:\n        if key in state_dict:\n            del state_dict[key]\n    self.load_state_dict(state_dict, strict=False)\n    print(self.load_state_dict(state_dict, strict=False))\n\n", "CodeBleu_score": 21.69139421171921}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n    # Encode\n    encoded = self.encoder(input)\n    # Decode\n    decoded = self.decoder(encoded)\n    # Difference metric\n    diff = self.diff_metric(input, decoded)\n    # Return\n    if return_pred_indices:\n        _, pred_indices = torch.max(decoded, 1)\n        return decoded, diff, pred_indices\n    else:\n        return decoded, diff", "CodeBleu_score": 22.265059226393603}, {"code": "def forward(self, input, return_pred_indices=False)\n    # Encode\n    encoded = self.encode(input)\n\n    # Decode\n    decoded = self.decode(encoded)\n\n    # Return\n    if return_pred_indices:\n        return decoded, encoded\n    else:\n        return decoded\n\n", "CodeBleu_score": 29.867403053615483}, {"code": "def forward(self, input, return_pred_indices=False)\n    # Encode input\n    encoded_input = self.encoder(input)\n\n    # Decode encoded input\n    decoded_output = self.decoder(encoded_input)\n\n    # Calculate difference metric\n    difference_metric = torch.mean(torch.abs(input - decoded_output))\n\n    if return_pred_indices:\n        # Prediction indices are the indices of the maximum values in the decoded output\n        _, pred_indices = decoded_output.max(dim=1)\n        return decoded_output, difference_metric, pred_indices\n    else:\n        return decoded_output, difference_metric\n\n", "CodeBleu_score": 19.396865034198928}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n    if self.resize_batch:\n        self.batch_size = int(self.batch_size_range[0] + (self.batch_size_range[1] - self.batch_size_range[0]) * self.training_step / self.training_steps)\n        batch = F.interpolate(batch, (self.batch_size, batch.shape[2], batch.shape[3]), mode='bicubic', align_corners=False)\n    x = batch[k].unsqueeze(0).permute(0, 3, 1, 2).contiguous().float()\n    x = x.detach()\n    return x\n\n", "CodeBleu_score": 38.6113941826437}, {"code": "def get_input(self, batch, k)\n    in_tensor = batch[k]\n    if len(in_tensor.shape) == 3:\n        in_tensor = in_tensor[:, None, :, :]\n    in_tensor = in_tensor.permute(0, 3, 1, 2)\n    in_tensor = in_tensor.contiguous().float()\n    if self.resize_batch:\n        in_tensor = self.resize_input(in_tensor)\n    in_tensor = in_tensor.detach()\n    return in_tensor\n\n", "CodeBleu_score": 22.518763899869747}, {"code": "def get_input(self, batch, k)\n    # Select input key.\n    x = batch[k]\n    # Adjust dimensions if necessary.\n    if x.ndim == 2:\n        x = x[:, :, None, None]\n    elif x.ndim == 3:\n        x = x.permute(2, 0, 1)[:, None, :, :]\n    elif x.ndim == 4:\n        x = x.permute(0, 3, 1, 2)\n    else:\n        raise ValueError(f'Input tensor must have 2, 3, or 4 dimensions, but input has {x.ndim} dimensions.')\n    # Resize input if enabled.\n    if self.resize_enabled:\n        # Get current training step.\n        step = self.optimizer.state_dict()['state'][0]['step']\n        # Get range of resizing.\n        resize_range = self.resize_max - self.resize_min\n        # Get interpolation factor.\n        resize_factor = (step / self.num_steps) * resize_range + self.resize_min\n        # Resize input.\n        x = F.interpolate(x, scale_factor=resize_factor, mode='bicubic', align_corners=False)\n    # Convert to contiguous float tensor.\n    x = x.float().contiguous()\n    # Detach from computation graph and return.\n    return x.detach()\n\n", "CodeBleu_score": 41.385437084973454}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    x_hat, y_hat, z, z_hat = self(x)\n\n    if optimizer_idx == 0:\n        loss = self.autoencoder_loss(x, x_hat, y, y_hat)\n        self.log(\"train_autoencoder_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n    elif optimizer_idx == 1:\n        loss = self.discriminator_loss(x, x_hat, z, z_hat)\n        self.log(\"train_discriminator_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n    else:\n        raise ValueError(f\"Invalid optimizer index: {optimizer_idx}\")\n\n    return loss\n", "CodeBleu_score": 18.523620546855145}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    x_hat, z, mu, logvar = self.forward(x)\n    if optimizer_idx == 0:\n        loss = self.autoencoder_loss(x_hat, x)\n        self.log(\"train_autoencoder_loss\", loss, prog_bar=True)\n    elif optimizer_idx == 1:\n        loss = self.discriminator_loss(z, y)\n        self.log(\"train_discriminator_loss\", loss, prog_bar=True)\n    return loss\n\n", "CodeBleu_score": 11.37552338977302}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    x_hat, z, mu, logvar = self.forward(x)\n\n    if optimizer_idx == 0:\n        loss = self.autoencoder_loss(x, x_hat, z, mu, logvar)\n        self.log('train_autoencoder_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    else:\n        loss = self.discriminator_loss(x, x_hat, z, mu, logvar)\n        self.log('train_discriminator_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n    return loss\n\n", "CodeBleu_score": 22.18630467810228}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if only_inputs:\n        if plot_ema:\n            self.log_images_input_ema(batch, **kwargs)\n        else:\n            self.log_images_input(batch, **kwargs)\n    else:\n        if plot_ema:\n            self.log_images_input_recon_ema(batch, **kwargs)\n        else:\n            self.log_images_input_recon(batch, **kwargs)\n\n", "CodeBleu_score": 14.675328261517839}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if not self.log_images:\n        return\n\n    # If EMA is used, get EMA weights\n    if plot_ema:\n        ema_weights = self.model.get_model_ema_weights()\n\n    # Process input data\n    x_recon_log = self.process_data(batch, self.image_shape)\n\n    # If multi-channel, colorize reconstructions\n    if self.model.model_type == \"autoencoder\" and self.model.channels == 1:\n        x_recon_log = tf.tile(x_recon_log, [1, 1, 1, 3])\n\n    # Log inputs\n    if only_inputs:\n        self.logger.experiment.add_images(f\"{self.logger.name}/inputs\", x_recon_log, self.current_epoch)\n\n    # If EMA is used, log reconstructions with EMA weights\n    elif plot_ema:\n        x_recon_log_ema = self.model(batch, training=False, use_ema=True)\n        x_recon_log_ema = self.process_data(x_recon_log_ema, self.image_shape)\n\n        if self.model.model_type == \"autoencoder\" and self.model.channels == 1:\n            x_recon_log_ema = tf.tile(x_recon_log_ema, [1, 1, 1, 3])\n\n        self.logger.experiment.add_images(\n            f\"{self.logger.name}/inputs_vs_reconstructions\",\n            tf.concat([x_recon_log, x_recon_log_ema], axis=2),\n            self.current_epoch,\n        )\n\n    # Log reconstructions\n    else:\n        self.logger.experiment.add_images(\n            f\"{self.logger.name}/reconstructions\", x_recon_log, self.current_epoch\n        )\n\n    # Log samples\n    if self.model.model_type == \"gans\":", "CodeBleu_score": 26.788993737928184}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if only_inputs:\n        # Log input images\n        self.logger.experiment.add_images(\n            \"inputs\",\n            batch[\"images\"],\n            global_step=self.global_step,\n            dataformats=\"NCHW\",\n        )\n    else:\n        # Log input images\n        self.logger.experiment.add_images(\n            \"inputs\",\n            batch[\"images\"],\n            global_step=self.global_step,\n            dataformats=\"NCHW\",\n        )\n        # Log reconstructions\n        if plot_ema:\n            # Log reconstructions using EMA weights\n            self.logger.experiment.add_images(\n                \"reconstructions_ema\",\n                self.model.reconstruct(batch[\"images\"], use_ema=True),\n                global_step=self.global_step,\n                dataformats=\"NCHW\",\n            )\n        # Log reconstructions\n        self.logger.experiment.add_images(\n            \"reconstructions\",\n            self.model.reconstruct(batch[\"images\"]),\n            global_step=self.global_step,\n            dataformats=\"NCHW\",\n        )\n\n", "CodeBleu_score": 21.574338090882048}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = (pred - target)**2\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 44.201165152098476}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError(\"Unknown loss type: {}\".format(self.loss_type))\n    return loss\n\n", "CodeBleu_score": 45.94322775903835}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n    return loss", "CodeBleu_score": 45.846983402382904}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n    if self.cond_type == 'categorical':\n        cond_onehot = F.one_hot(c, num_classes=self.cond_channels)\n        cond_onehot = cond_onehot.float()\n        cond_onehot = cond_onehot.view(cond_onehot.shape[0], -1, cond_onehot.shape[-1])\n        cond_onehot = cond_onehot.permute(0, 2, 1)\n        cond_onehot = cond_onehot.contiguous()\n        cond = cond_onehot\n    elif self.cond_type == 'continuous':\n        cond = c\n    elif self.cond_type == 'none':\n        cond = None\n    else:\n        raise ValueError('Invalid conditional type.')\n    if self.cond_type != 'none':\n        if self.cond_requirement == 'full':\n            cond_size = cond.shape[1]\n            cond_cut = np.random.randint(0, cond_size)\n            cond = cond[:, cond_cut:]\n            x = x[:, cond_cut:]\n        elif self.cond_requirement == 'random':\n            cond_size = cond.shape[1]\n            cond_cut = np.random.randint(0, cond_size)\n            cond = cond[:, cond_cut:]\n            x = x[:, cond_cut:]\n        elif self.cond_requirement == 'none':\n            pass\n        else:\n            raise ValueError('Invalid conditional requirement.')\n    return self.compute_loss(x, cond, *args, **kwargs)", "CodeBleu_score": 37.076239654431966}, {"code": "def forward(self, x, c, *args, **kwargs)\n    # Unpack model\n    model = self.model\n    # Unpack conditioning\n    c = c.to(self.device)\n    # Unpack conditioning requirements\n    cond_requirements = model.cond_requirements\n    # Unpack conditioning\n    if cond_requirements == 'none':\n        c = None\n    elif cond_requirements == 'single':\n        c = c[0]\n    elif cond_requirements == 'multi':\n        c = c\n    else:\n        raise ValueError('Unknown conditioning requirements: %s' % cond_requirements)\n    # Unpack time step information\n    time_steps = self.time_steps\n    # Unpack time step requirements\n    time_step_requirements = model.time_step_requirements\n    # Unpack time steps\n    if time_step_requirements == 'none':\n        time_steps = None\n    elif time_step_requirements == 'single':\n        time_steps = time_steps[0]\n    elif time_step_requirements == 'multi':\n        time_steps = time_steps\n    else:\n        raise ValueError('Unknown time step requirements: %s' % time_step_requirements)\n    # Unpack inputs\n    inputs = x\n    # Unpack input requirements\n    input_requirements = model.input_requirements\n    # Unpack inputs\n    if input_requirements == 'none':\n        inputs = None\n    elif input_requirements == 'single':\n        inputs = inputs[0]\n    elif input_requirements == 'multi':\n        inputs = inputs\n    else:\n        raise ValueError('Unknown input requirements: %s' % input_requirements)\n    # Unpack output requirements\n    output_requirements = model.output_requirements\n    # Unpack outputs\n    if output_requirements == 'none':\n        outputs = None\n    elif output_requirements == 'single':\n        outputs = outputs[0]\n    elif output_requirements == 'multi':\n        outputs = outputs\n    else:\n        raise ValueError('Unknown output requirements: %s' % output_requirements)\n    # Unpack loss requirements\n    loss_requirements = model.loss_requirements", "CodeBleu_score": 20.42134655446893}, {"code": "def forward(self, x, c, *args, **kwargs)\n    # Select time steps\n    if self.time_steps is None:\n      self.time_steps = tf.random.uniform(\n          shape=[self.batch_size, self.num_steps],\n          minval=0,\n          maxval=self.max_time_steps,\n          dtype=tf.int32)\n    # Apply conditioning\n    if self.conditioning_type == 'learned':\n      if self.conditioning_steps is None:\n        self.conditioning_steps = tf.random.uniform(\n            shape=[self.batch_size, self.num_steps],\n            minval=0,\n            maxval=self.max_time_steps,\n            dtype=tf.int32)\n      conditioning = tf.gather(c, self.conditioning_steps, axis=1)\n    elif self.conditioning_type == 'fixed':\n      conditioning = tf.tile(\n          tf.expand_dims(c, 1), [1, self.num_steps, 1])\n    else:\n      conditioning = None\n    # Process inputs\n    if self.input_type == 'fixed':\n      inputs = tf.tile(\n          tf.expand_dims(x, 1), [1, self.num_steps, 1])\n    elif self.input_type == 'learned':\n      inputs = tf.gather(x, self.time_steps, axis=1)\n    elif self.input_type == 'conditioned':\n      inputs = tf.gather(x, self.time_steps, axis=1)\n      inputs = tf.concat([inputs, conditioning], axis=-1)\n    else:\n      raise ValueError('Invalid input type: {}'.format(self.input_type))\n    # Compute loss\n    loss = self.compute_loss(inputs, conditioning)\n    return loss", "CodeBleu_score": 34.42173658198459}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n\n    # if indices are not provided, use the current indices\n    if indices is None:\n        indices = self.indices\n\n    # if shape is not provided, use the current shape\n    if shape is None:\n        shape = self.shape\n\n    # if the indices are not provided, the shape is not provided, and the codebook is not provided, return None\n    if self.codebook is None and indices is None and shape is None:\n        return None\n\n    # if the indices are not provided, the shape is not provided, and the codebook is provided, return the codebook\n    if self.codebook is not None and indices is None and shape is None:\n        return self.codebook\n\n    # if the indices are not provided, the shape is provided, and the codebook is provided, return the codebook reshaped to the provided shape\n    if self.codebook is not None and indices is None and shape is not None:\n        return self.codebook.reshape(shape)\n\n    # if the indices are not provided, the shape is provided, and the codebook is not provided, return None\n    if self.codebook is None and indices is None and shape is not None:\n        return None\n\n    # if the indices are provided, the shape is not provided, and the codebook is provided, return the codebook indexed by the provided indices\n    if self.codebook is not None and indices is not None and shape is None:\n        return self.codebook[indices]\n\n    # if the indices are provided, the shape is provided, and the codebook is provided, return the codebook indexed by the provided indices and reshaped to the provided shape\n    if self.codebook is not None and indices is not None and shape is not None:\n        return self.codebook[indices].reshape(shape)\n\n    # if the indices are provided, the shape is provided, and the codebook is not provided, return None\n    if self.codebook is None and indices is not None and shape is not None:\n        return None\n\n    # if the indices are provided, the shape is not provided, and the codebook is not provided, return None\n    if self.codebook is None and indices is not None and shape is None:\n        return", "CodeBleu_score": 23.878733330709657}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.codebook_indices is not None:\n        indices = self.codebook_indices[indices]\n    return self.codebook.weight[indices].reshape(shape).permute(0, 3, 1, 2)\n", "CodeBleu_score": 20.3594161269673}, {"code": "def get_codebook_entry(self, indices, shape)\n    # Retrieve the latent vectors from the codebook\n    latents = self.codebook[indices]\n\n    # Reshape the latent vectors to match the original input shape\n    latents = latents.reshape(shape)\n\n    # Permute the latent vectors to match the original input dimensions\n    latents = latents.permute(0, 2, 3, 1)\n\n    return latents", "CodeBleu_score": 7.672836147861725}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor >= 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor < 0:\n        m = 1\n        n = dimension\n        while n > 1:\n            while n % m != 0:\n                m += 1\n            n = dimension // m\n        return m, n\n    else:\n        raise ValueError(\"The factor must be positive and divide the dimension.\")", "CodeBleu_score": 33.77125240767403}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0:\n        if dimension % factor == 0:\n            return factor, dimension // factor\n        else:\n            raise ValueError(\"The specified factor does not divide the dimension.\")\n    elif factor < 0:\n        if dimension % -factor == 0:\n            return dimension // -factor, -factor\n        else:\n            m = 1\n            n = dimension\n            while n > 1:\n                m += 1\n                n = dimension // m\n                if n < m:\n                    m -= 1\n                    n = dimension // m\n            return m, n\n    else:\n        raise ValueError(\"The factor must be positive or negative.\")\n", "CodeBleu_score": 38.04436859206074}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0 and dimension % factor == 0:\n        return factor, dimension // factor\n    elif factor < 0:\n        factors = []\n        for i in range(1, dimension + 1):\n            if dimension % i == 0:\n                factors.append(i)\n        if len(factors) == 1:\n            return factors[0], 1\n        else:\n            for i in range(len(factors)):\n                if factors[i] > abs(factor):\n                    return factors[i - 1], dimension // factors[i - 1]\n            return factors[-1], dimension // factors[-1]\n    else:\n        raise ValueError(f\"Factor {factor} does not divide dimension {dimension}\")\n", "CodeBleu_score": 30.023373035907163}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n    # Check if the original weight is a list\n    if isinstance(orig_weight, list):\n        # Check if the original weight is a list of lists\n        if isinstance(orig_weight[0], list):\n            # Check if the original weight is a list of lists of lists\n            if isinstance(orig_weight[0][0], list):\n                # Check if the original weight is a list of lists of lists of lists\n                if isinstance(orig_weight[0][0][0], list):\n                    # Check if the original weight is a list of lists of lists of lists of lists\n                    if isinstance(orig_weight[0][0][0][0], list):\n                        # Check if the original weight is a list of lists of lists of lists of lists of lists\n                        if isinstance(orig_weight[0][0][0][0][0], list):\n                            # Check if the original weight is a list of lists of lists of lists of lists of lists of lists\n                            if isinstance(orig_weight[0][0][0][0][0][0], list):\n                                # Check if the original weight is a list of lists of lists of lists of lists of lists of lists of lists\n                                if isinstance(orig_weight[0][0][0][0][0][0][0], list):\n                                    # Check if the original weight is a list of lists of lists of lists of lists of lists of lists of lists of lists\n                                    if isinstance(orig_weight[0][0][0][0][0][0][0][0], list):\n                                        # Check if the original weight is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists\n                                        if isinstance(orig_weight[0][0][0][0][0][0][0][0][0], list):\n                                            # Check if the original weight is a list of lists of lists of lists of lists of lists of lists of lists of lists of lists of lists\n                                            if isinstance(orig_weight[0][0][0][0][0][0][0][0][0][0], list):\n                                                # Check if the original weight is a list of lists of lists of lists", "CodeBleu_score": 7.440202933298112}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n    # Decompose the original weight\n    weight_decomposed = self.decompose_weight(orig_weight)\n    # Combine the input weight with the decomposed weight\n    weight_combined = self.combine_weights(updown, weight_decomposed)\n    # Normalize the combined weight\n    weight_normalized = self.normalize_weight(weight_combined)\n    # Scale the normalized weight\n    weight_scaled = self.scale_weight(weight_normalized)\n    # Calculate the adjusted weight\n    weight_adjusted = weight_scaled - orig_weight\n    return weight_adjusted\n\n", "CodeBleu_score": 8.183078803862525}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n\n    # Check if the weight is a tensor or a list of tensors\n    if isinstance(orig_weight, list):\n        # If the weight is a list, loop through each tensor and apply the same function to each tensor\n        for i in range(len(orig_weight)):\n            orig_weight[i] = self.apply_weight_decompose(updown, orig_weight[i])\n        return orig_weight\n    else:\n        # If the weight is a tensor, check if the weight is a quantized weight\n        if orig_weight.qscheme() != None:\n            # If the weight is a quantized weight, dequantize the weight\n            orig_weight = orig_weight.dequantize()\n        # Check if the weight is a sparse weight\n        if orig_weight.is_sparse:\n            # If the weight is a sparse weight, convert the weight to a dense weight\n            orig_weight = orig_weight.to_dense()\n        # Check if the weight is a half precision weight\n        if orig_weight.dtype == torch.float16:\n            # If the weight is a half precision weight, convert the weight to a single precision weight\n            orig_weight = orig_weight.float()\n        # Check if the weight is a cuda weight\n        if orig_weight.is_cuda:\n            # If the weight is a cuda weight, move the weight to the cpu\n            orig_weight = orig_weight.cpu()\n        # Check if the input weight is a tensor or a list of tensors\n        if isinstance(updown, list):\n            # If the input weight is a list, loop through each tensor and apply the same function to each tensor\n            for i in range(len(updown)):\n                updown[i] = self.apply_weight_decompose(updown[i], orig_weight)\n            return updown\n        else:\n            # If the input weight is a tensor, check if the input weight is a quantized weight\n            if updown.qscheme() != None:\n                # If the input weight is a quantized weight, dequantize the input weight\n                updown = updown.dequantize()\n            # Check if the input weight is a sparse weight", "CodeBleu_score": 21.719900997056524}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if updown.shape[0] == orig_weight.shape[0]:\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.bias_factor\n    if self.scaling_factor is not None:\n        updown = self.scaling_factor * updown\n    return updown, ex_bias\n\n", "CodeBleu_score": 32.66687396098396}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        updown = updown.reshape(output_shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias.reshape(output_shape)\n        ex_bias = self.weight_decomposition(ex_bias)\n    if self.scaling_factor != 1:\n        updown = self.weight_decomposition(updown)\n    return updown, ex_bias\n\n", "CodeBleu_score": 37.522587969554}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    # Adjusting the bias\n    if ex_bias is not None:\n        ex_bias = ex_bias * updown\n    # Reshaping updown\n    if updown.numel() == orig_weight.numel():\n        updown = updown.view(orig_weight.shape)\n    else:\n        updown = updown.view(output_shape)\n    # Applying the weight decomposition\n    if self.scaling_factor is not None:\n        updown = self.weight_decomp(updown, self.scaling_factor)\n    # Returning the scaled and adjusted updown tensor along with the modified additional bias\n    return updown, ex_bias\n\n", "CodeBleu_score": 39.01602864484516}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n    if self.use_bias:\n        bias = self.bias.to(orig_weight.device)\n        new_weight = self.calc_updown_bias(orig_weight, self.weight, bias)\n        return new_weight, bias\n    else:\n        new_weight = self.calc_updown_bias(orig_weight, self.weight)\n        return new_weight\n\n", "CodeBleu_score": 44.68778381316015}, {"code": "def calc_updown(self, orig_weight)\n    if self.device != orig_weight.device:\n        self.to(orig_weight.device)\n    if self.bias is not None:\n        return self.calc_updown_bias(orig_weight)\n    else:\n        return self.calc_updown_nobias(orig_weight)\n\n", "CodeBleu_score": 27.175177958087144}, {"code": "def calc_updown(self, orig_weight)\n    # if there is no bias, then the updated weight tensor is the same as the current weight tensor\n    if self.bias is None:\n        return self.weight\n\n    # if there is a bias, then the updated weight tensor is the current weight tensor plus the bias\n    else:\n        # transfer the current weight tensor and bias to the device of the original weight tensor\n        weight = self.weight.to(orig_weight.device)\n        bias = self.bias.to(orig_weight.device)\n        return self.calc_updown_bias(orig_weight, weight, bias)\n\n", "CodeBleu_score": 42.51780750777454}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n    from . import memory\n    from . import settings\n    from . import utils\n    from . import logger\n    import gc\n    import os\n    import psutil\n    import sys\n    import time\n    import warnings\n\n    # Get the current memory usage\n    current_memory_usage = psutil.Process(os.getpid()).memory_info().rss\n\n    # Get the maximum memory usage\n    max_memory_usage = settings.MAX_MEMORY_USAGE\n\n    # Get the current network count\n    current_network_count = len(memory.memory)\n\n    # Get the maximum network count\n    max_network_count = settings.MAX_NETWORK_COUNT\n\n    # Check if the memory usage is too high\n    if current_memory_usage > max_memory_usage:\n\n        # Get the current time\n        current_time = time.time()\n\n        # Get the time since the last purge\n        time_since_last_purge = current_time - memory.last_purge_time\n\n        # Check if the time since the last purge is too short\n        if time_since_last_purge < settings.MIN_PURGE_INTERVAL:\n\n            # Get the time until the next purge\n            time_until_next_purge = settings.MIN_PURGE_INTERVAL - time_since_last_purge\n\n            # Raise a warning\n            warnings.warn(f\"The memory usage is too high. The next purge will occur in {round(time_until_next_purge, 2)} seconds.\")\n\n            # Return\n            return\n\n        # Purge networks from memory\n        purge_networks_from_memory()\n\n        # Get the current memory usage\n        current_memory_usage = psutil.Process(os.getpid()).memory_info().rss\n\n        # Check if the memory usage is still too high\n        if current_memory_usage > max_memory_usage:\n\n            # Raise a warning\n            warnings.warn(f\"The memory usage is still too high. The current memory usage is {utils.format_bytes(current_memory_usage)} and the maximum memory usage is {utils.format_bytes", "CodeBleu_score": 32.37121088792551}, {"code": "def purge_networks_from_memory()\n    from . import config\n    from . import network\n    import gc\n    # Get all networks\n    all_networks = network.get_networks()\n    # Get the number of networks\n    num_networks = len(all_networks)\n    # Get the network purge limit\n    purge_limit = config.get_config()[\"network_purge_limit\"]\n    # Purge networks if the number of networks exceeds the limit\n    if num_networks > purge_limit:\n        # Get the number of networks to purge\n        num_to_purge = num_networks - purge_limit\n        # Purge networks\n        for i in range(num_to_purge):\n            network.purge_network(all_networks[i][\"id\"])\n        # Trigger garbage collection\n        gc.collect()\n", "CodeBleu_score": 24.14049848054824}, {"code": "def purge_networks_from_memory()\n    import os\n    import gc\n    import torch\n    import psutil\n    import numpy as np\n    import torch.nn as nn\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory_usage\n    from models.networks import get_network_from_file\n    from utils import get_memory", "CodeBleu_score": 13.326854009222947}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if self.weight.requires_grad:\n        if self.weight.grad is not None:\n            self.weight.grad.data.zero_()\n    if self.bias is not None:\n        if self.bias.requires_grad:\n            if self.bias.grad is not None:\n                self.bias.grad.data.zero_()\n    if self.weight.requires_grad:\n        if self.weight.grad is not None:\n            self.weight.grad.data.zero_()\n    if self.bias is not None:\n        if self.bias.requires_grad:\n            if self.bias.grad is not None:\n                self.bias.grad.data.zero_()\n    if self.weight.requires_grad:\n        if self.weight.grad is not None:\n            self.weight.grad.data.zero_()\n    if self.bias is not None:\n        if self.bias.requires_grad:\n            if self.bias.grad is not None:\n                self.bias.grad.data.zero_()\n    if self.weight.requires_grad:\n        if self.weight.grad is not None:\n            self.weight.grad.data.zero_()\n    if self.bias is not None:\n        if self.bias.requires_grad:\n            if self.bias.grad is not None:\n                self.bias.grad.data.zero_()\n    if self.weight.requires_grad:\n        if self.weight.grad is not None:\n            self.weight.grad.data.zero_()\n    if self.bias is not None:\n        if self.bias.requires_grad:\n            if self.bias.grad is not None:\n                self.bias.grad.data.zero_()\n    if self.weight.requires_grad:\n        if self.weight.grad is not None:\n            self.weight.grad.data.zero_()\n    if self.bias is not None:\n        if self.bias.requires_grad", "CodeBleu_score": 28.16779118645266}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if self.backup_weights is not None:\n        self.weight.data = self.backup_weights.data\n    if self.backup_bias is not None:\n        self.bias.data = self.backup_bias.data\n    else:\n        self.bias = None\n    if type(self) == torch.nn.MultiheadAttention:\n        if self.backup_in_proj_weight is not None:\n            self.in_proj_weight.data = self.backup_in_proj_weight.data\n        if self.backup_in_proj_bias is not None:\n            self.in_proj_bias.data = self.backup_in_proj_bias.data\n        else:\n            self.in_proj_bias = None\n", "CodeBleu_score": 32.552755321150805}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if self.weight.requires_grad:\n        if self.weight.grad is not None:\n            self.weight.grad.data.zero_()\n        if self.weight.data.is_cuda:\n            self.weight.data.copy_(self.weight.data.cpu().detach().clone())\n        else:\n            self.weight.data.copy_(self.weight.data.detach().clone())\n    if self.bias is not None:\n        if self.bias.requires_grad:\n            if self.bias.grad is not None:\n                self.bias.grad.data.zero_()\n            if self.bias.data.is_cuda:\n                self.bias.data.copy_(self.bias.data.cpu().detach().clone())\n            else:\n                self.bias.data.copy_(self.bias.data.detach().clone())\n\n", "CodeBleu_score": 36.58860426321208}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n    if len(loaded_networks) == 0:\n        return original_forward(input)\n    else:\n        input = input.to(device)\n        org_module.load_state_dict(org_module_backup)\n        output = original_forward(input)\n        for network in loaded_networks:\n            if network.layer == layer:\n                output = network.apply(output)\n        return output\n\n", "CodeBleu_score": 30.579274909483594}, {"code": "def network_forward(org_module, input, original_forward)\n\n    if len(org_module.loaded_networks) == 0:\n        return original_forward(org_module, input)\n\n    input = input.to(org_module.device)\n    org_module.restore_weights()\n    org_module.reset_weights()\n\n    output = original_forward(org_module, input)\n\n    for network in org_module.loaded_networks:\n        if network.layer == org_module.layer:\n            output = network.forward(output)\n\n    return output", "CodeBleu_score": 42.23638386613126}, {"code": "def network_forward(org_module, input, original_forward)\n    if len(loaded_networks) == 0:\n        return original_forward(input)\n\n    input = input.to(device)\n    org_module.weight.data = backup_weights.pop(0)\n    org_module.weight.requires_grad = True\n\n    output = original_forward(input)\n\n    for network in loaded_networks:\n        if network.layer == org_module:\n            output = network.forward(output)\n\n    return output\n\n", "CodeBleu_score": 34.52134778957206}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n    trainables = []\n    for layer in self.layers:\n        if isinstance(layer, torch.nn.Linear):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n        elif isinstance(layer, torch.nn.LayerNorm):\n            trainables.append(layer.weight)\n            trainables.append(layer.bias)\n    return trainables", "CodeBleu_score": 36.18876731186704}, {"code": "def trainables(self)\n    trainable_weights = []\n    for layer in self.layers:\n        if isinstance(layer, torch.nn.Linear):\n            trainable_weights.append(layer.weight)\n            trainable_weights.append(layer.bias)\n        elif isinstance(layer, torch.nn.LayerNorm):\n            trainable_weights.append(layer.weight)\n            trainable_weights.append(layer.bias)\n    return trainable_weights", "CodeBleu_score": 35.032227151877024}, {"code": "def trainables(self)\n    trainables = []\n    for name, param in self.named_parameters():\n        if isinstance(param, torch.nn.Parameter):\n            trainables.append(param)\n    return trainables\n", "CodeBleu_score": 28.14373105157958}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n    from .hypernetwork import HyperNetwork\n    from .hypernetworks import loaded_hypernetworks\n\n    # Check for already loaded hypernetworks\n    if loaded_hypernetworks:\n        print(\"WARNING: Already loaded hypernetworks found, clearing list.\")\n        loaded_hypernetworks.clear()\n\n    # Load or retrieve each hypernetwork by name\n    for i, name in enumerate(names):\n        print(\"Loading hypernetwork \" + str(i + 1) + \": \" + name)\n        if name in loaded_hypernetworks:\n            loaded_hypernetworks.append(loaded_hypernetworks[name])\n        else:\n            loaded_hypernetworks.append(HyperNetwork(name))\n        if multipliers:\n            loaded_hypernetworks[loaded_hypernetworks.index(name)].multiplier = multipliers[i]\n\n    # Return the list of loaded hypernetworks\n    return loaded_hypernetworks\n", "CodeBleu_score": 32.18239321375748}, {"code": "def load_hypernetworks(names, multipliers=None)\n    import os\n    import sys\n    import importlib\n    import hypernetworks\n    import hypernetworks.utils\n    from hypernetworks.utils import load_hypernetwork\n    from hypernetworks.utils import load_hypernetwork_from_file\n    from hypernetworks.utils import load_hypernetwork_from_module\n    from hypernetworks.utils import load_hypernetwork_from_path\n    from hypernetworks.utils import load_hypernetwork_from_url\n    from hypernetworks.utils import load_hypernetwork_from_zip\n    from hypernetworks.utils import load_hypernetwork_from_dir\n    from hypernetworks.utils import load_hypernetwork_from_json\n    from hypernetworks.utils import load_hypernetwork_from_yaml\n    from hypernetworks.utils import load_hypernetwork_from_pickle\n    from hypernetworks.utils import load_hypernetwork_from_bytes\n    from hypernetworks.utils import load_hypernetwork_from_text\n    from hypernetworks.utils import load_hypernetwork_from_data\n    from hypernetworks.utils import load_hypernetwork_from_object\n    from hypernetworks.utils import load_hypernetwork_from_dict\n    from hypernetworks.utils import load_hypernetwork_from_kwargs\n    from hypernetworks.utils import load_hypernetwork_from_args\n    from hypernetworks.utils import load_hypernetwork_from_kwargs_or_args\n    from hypernetworks.utils import load_hypernetwork_from_dict_or_kwargs_or_args\n    from hypernetworks.utils import load_hypernetwork_from_json_or_dict_or_kwargs_or_args\n    from hypernetworks.utils import load_hypernetwork_from_yaml_or_dict_or_kwargs_or_args\n    from hypernetworks.utils import load_hypernetwork_from_pickle_or_dict_or_kwargs_or_args\n    from hypernetworks.utils import load_hypernetwork_from_bytes_or_dict_or_kwargs_or_args\n    from hypernetworks.utils import load_hypernetwork_from_text_or_dict_or_kwargs_or_args\n    from hypernetworks.", "CodeBleu_score": 6.681268954000188}, {"code": "def load_hypernetworks(names, multipliers=None)\n    from . import hypernetworks\n    from .hypernetworks import loaded_hypernetworks\n    from .hypernetworks import clear_loaded_hypernetworks\n    from .hypernetworks import load_hypernetwork\n    from .hypernetworks import get_hypernetwork\n    from .hypernetworks import set_multiplier\n    from .hypernetworks import get_multiplier\n    from .hypernetworks import get_hypernetwork_names\n    from .hypernetworks import get_hypernetwork_multipliers\n    from .hypernetworks import get_hypernetwork_by_name\n    from .hypernetworks import get_hypernetwork_multiplier_by_name\n    from .hypernetworks import get_loaded_hypernetworks\n    from .hypernetworks import get_loaded_hypernetwork_names\n    from .hypernetworks import get_loaded_hypernetwork_multipliers\n    from .hypernetworks import get_loaded_hypernetwork_by_name\n    from .hypernetworks import get_loaded_hypernetwork_multiplier_by_name\n    from .hypernetworks import get_loaded_hypernetwork_by_multiplier\n    from .hypernetworks import get_loaded_hypernetwork_multiplier_by_multiplier\n    from .hypernetworks import get_loaded_hypernetwork_by_name_and_multiplier\n    from .hypernetworks import get_loaded_hypernetwork_multiplier_by_name_and_multiplier\n    from .hypernetworks import get_loaded_hypernetwork_by_multiplier_and_name\n    from .hypernetworks import get_loaded_hypernetwork_multiplier_by_multiplier_and_name\n    from .hypernetworks import get_loaded_hypernetwork_by_name_and_multiplier_and_name\n    from .hypernetworks import get_loaded_hypernetwork_multiplier_by_name_and_multiplier_and_name\n    from .hypernetworks import get_loaded_hypernetwork_by_multiplier_and_name_and_multiplier\n    from .hypernetworks import get_loaded_hypernetwork_multiplier_by_multiplier_and_name_and_multiplier\n    from .hypernetworks import get_loaded_hypernetwork_by_name_and_multip", "CodeBleu_score": 7.222906244233429}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 76.93453377925789}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 66.81959752036603}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)    context_k = context\n    context_v = context\n    for i, hypernetwork in enumerate(hypernetworks):\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 55.207333509780575}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    if context is None:\n        raise ValueError(\"`context` must be given as input to `CrossAttention`.\")\n\n    # Project input to query, key, and value\n    query = self.query_proj(x)\n    key = self.key_proj(context)\n    value = self.value_proj(context)\n\n    # Apply hypernetworks to context\n    if self.hyper_key_proj is not None:\n        key = self.hyper_key_proj(key)\n    if self.hyper_value_proj is not None:\n        value = self.hyper_value_proj(value)\n\n    # Reshape for multi-head attention\n    query = self.reshape_query(query)\n    key = self.reshape_key(key)\n    value = self.reshape_value(value)\n\n    # Calculate scaled dot-product attention\n    scale = self.scale or 1.0 / math.sqrt(math.sqrt(key.shape[-1]))\n    attention = torch.matmul(query, key.transpose(-2, -1)) * scale\n\n    # Apply mask\n    if mask is not None:\n        attention = attention.masked_fill(mask == 0, -1e9)\n\n    # Apply softmax\n    attention = torch.softmax(attention, dim=-1)\n\n    # Apply dropout\n    if self.dropout is not None:\n        attention = self.dropout(attention)\n\n    # Calculate final attention output\n    output = torch.matmul(attention, value)\n\n    # Reshape output\n    output = self.reshape_output(output)\n\n    # Pass through final linear layer\n    output = self.output_proj(output)\n\n    return output", "CodeBleu_score": 35.79462790739464}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    x_shape = x.shape\n    x = x.view(-1, x_shape[-1])\n    x_proj_query = self.query(x)\n    x_proj_key = self.key(x)\n    x_proj_value = self.value(x)\n    x_proj_query = x_proj_query.view(x_shape[0], x_shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    x_proj_key = x_proj_key.view(x_shape[0], x_shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    x_proj_value = x_proj_value.view(x_shape[0], x_shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    if context is None:\n        context = x\n    context_shape = context.shape\n    context = context.view(-1, context_shape[-1])\n    context_proj_query = self.context_query(context)\n    context_proj_key = self.context_key(context)\n    context_proj_value = self.context_value(context)\n    context_proj_query = context_proj_query.view(context_shape[0], context_shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    context_proj_key = context_proj_key.view(context_shape[0], context_shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    context_proj_value = context_proj_value.view(context_shape[0], context_shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n    if self.context_hyper_fan_out_dim > 0:\n        context_proj_query = self.context_hyper_query(context_proj_query)\n        context_proj_key = self.context_hyper_key(context_proj_key)\n        context_", "CodeBleu_score": 28.531024168720485}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    # x: (batch_size, seq_len, in_features)\n    # context: (batch_size, seq_len, in_features)\n    # mask: (batch_size, seq_len, seq_len)\n\n    # If context is None, use x as context\n    if context is None:\n        context = x\n\n    # Apply hypernetworks to context\n    if self.hyper_w_0 is not None:\n        context = self.hyper_w_0(context)\n    if self.hyper_w_1 is not None:\n        context = self.hyper_w_1(context)\n    if self.hyper_w_2 is not None:\n        context = self.hyper_w_2(context)\n\n    # Query, key, and value projections\n    query = self.query(x)\n    key = self.key(context)\n    value = self.value(context)\n\n    # Reshape for multi-head attention\n    query = query.view(query.shape[0], -1, self.n_heads, self.d_k).permute(0, 2, 1, 3)\n    key = key.view(key.shape[0], -1, self.n_heads, self.d_k).permute(0, 2, 1, 3)\n    value = value.view(value.shape[0], -1, self.n_heads, self.d_v).permute(0, 2, 1, 3)\n\n    # Scaled dot-product attention\n    attn = torch.matmul(query, key.permute(0, 1, 3, 2)) / (self.d_k ** 0.5)\n\n    # Mask\n    if mask is not None:\n        attn = attn.masked_fill(mask == 0, -1e9)\n\n    # Softmax\n    attn = torch.softmax(attn, dim=-1)\n\n    # Dropout\n    attn = self.dropout(attn)\n\n    # Weighted sum\n    result = torch.matmul(attn", "CodeBleu_score": 35.507515962099234}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    # Backup the original attributes\n    original_attributes = {}\n    for attribute in hypernetwork._hypernetwork_attributes:\n        original_attributes[attribute] = getattr(hypernetwork, attribute)\n    # Update the hypernetwork with the new checkpoint data\n    hypernetwork.update_hypernetwork(checkpoint, hypernetwork_name)\n    # Save the hypernetwork\n    try:\n        hypernetwork.save_hypernetwork(filename)\n    except Exception as e:\n        # Restore the original attributes\n        for attribute in hypernetwork._hypernetwork_attributes:\n            setattr(hypernetwork, attribute, original_attributes[attribute])\n        # Re-raise the exception\n        raise e\n    # Restore the original attributes\n    for attribute in hypernetwork._hypernetwork_attributes:\n        setattr(hypernetwork, attribute, original_attributes[attribute])\n\n", "CodeBleu_score": 27.16868565775331}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    try:\n        hypernetwork.save_hypernetwork(checkpoint, hypernetwork_name, filename)\n    except Exception as e:\n        raise e\n\n", "CodeBleu_score": 11.420256883117172}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    try:\n        hypernetwork.checkpoint = checkpoint\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.save(filename)\n    except Exception as e:\n        raise e\n    finally:\n        hypernetwork.checkpoint = None\n        hypernetwork.name = None\n\n", "CodeBleu_score": 24.394648776501192}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if self.ema:\n        if context:\n            context.log.debug('Using EMA weights.')\n        self.ema.store()\n        self.ema.copy_to()\n    try:\n        yield\n    finally:\n        if self.ema:\n            if context:\n                context.log.debug('Restoring original weights.')\n            self.ema.restore()\n\n", "CodeBleu_score": 44.6889835182779}, {"code": "def ema_scope(self, context=None)\n    if self.ema is not None:\n        self.ema.store(self.model)\n        self.ema.copy_to(self.model)\n        if self.verbose:\n            print(f\"EMA weights are now used for model {self.model_name}.\")\n        try:\n            yield\n        finally:\n            self.ema.restore(self.model)\n            if self.verbose:\n                print(f\"Original weights are now used for model {self.model_name}.\")\n    else:\n        yield\n\n", "CodeBleu_score": 47.55348505540737}, {"code": "def ema_scope(self, context=None)\n    if context is None:\n        context = {}\n    if not hasattr(self, 'ema_weights'):\n        raise Exception(\"EMA weights are not defined.\")\n    if not self.ema_weights:\n        raise Exception(\"EMA weights are disabled.\")\n    if not hasattr(self, 'ema_weights_restore'):\n        raise Exception(\"EMA weights restoration is not defined.\")\n    if not self.ema_weights_restore:\n        raise Exception(\"EMA weights restoration is disabled.\")\n    if not hasattr(self, 'ema_weights_log'):\n        raise Exception(\"EMA weights logging is not defined.\")\n    if not self.ema_weights_log:\n        raise Exception(\"EMA weights logging is disabled.\")\n    if not hasattr(self, 'ema_weights_log_level'):\n        raise Exception(\"EMA weights logging level is not defined.\")\n    if not self.ema_weights_log_level:\n        raise Exception(\"EMA weights logging level is disabled.\")\n    if not hasattr(self, 'ema_weights_log_tag'):\n        raise Exception(\"EMA weights logging tag is not defined.\")\n    if not self.ema_weights_log_tag:\n        raise Exception(\"EMA weights logging tag is disabled.\")\n    if not hasattr(self, 'ema_weights_log_prefix'):\n        raise Exception(\"EMA weights logging prefix is not defined.\")\n    if not self.ema_weights_log_prefix:\n        raise Exception(\"EMA weights logging prefix is disabled.\")\n    if not hasattr(self, 'ema_weights_log_suffix'):\n        raise Exception(\"EMA weights logging suffix is not defined.\")\n    if not self.ema_weights_log_suffix:\n        raise Exception(\"EMA weights logging suffix is disabled.\")\n    if not hasattr(self, 'ema_weights_log_ext'):\n        raise Exception(\"EMA weights logging extension is not defined.\")\n    if not self.ema_weights_log_ext:\n        raise Exception(\"EMA weights logging extension is disabled.\")\n    if not hasattr(self, 'ema_weights_log_path'):\n        raise Exception(\"EMA weights logging path is not defined.\")\n    if not self.ema_weights_log", "CodeBleu_score": 19.068130118845655}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    x_recon = self.decode(self.encode(x, t))\n\n    if clip_denoised:\n        x_recon.clamp_(-1., 1.)\n\n    model_mean, posterior_variance, posterior_log_variance = self.model(x, t, x_recon)\n    return model_mean, posterior_variance, posterior_log_variance\n\n", "CodeBleu_score": 34.771011215020245}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n\n    # get model output\n    model_output = self.model(x, t)\n\n    # reconstruct input\n    if self.parameterization == \"eps\":\n        x_recon = self.model.decode(model_output)\n    elif self.parameterization == \"x0\":\n        x_recon = self.model.decode(model_output, x.to(self.device))\n\n    # clip reconstructed input to range of -1 to 1\n    if clip_denoised:\n        x_recon.clamp_(-1.0, 1.0)\n\n    # calculate mean and variance\n    model_mean, posterior_variance, posterior_log_variance = self.model.get_likelihoods(x, x_recon)\n\n    return model_mean, posterior_variance, posterior_log_variance", "CodeBleu_score": 61.49837477201442}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    # Get the model output for the input and time step\n    x_ = self.model(x, t)\n    # Reconstruct the input either directly or from predicted noise\n    x_recon = self.model.decode(x_, t)\n    # If specified, clip the reconstructed input to a range of -1 to 1\n    if clip_denoised:\n        x_recon.clamp_(-1.0, 1.0)\n    # Calculate and return the model mean, posterior variance, and posterior log variance\n    return x_recon.mean(), x_recon.var(dim=0), x_recon.logvar(dim=0)", "CodeBleu_score": 19.82523332640271}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.mean(torch.pow(pred - target, 2))\n    else:\n        raise ValueError(f\"Unknown loss type {self.loss_type}.\")\n    return loss", "CodeBleu_score": 42.16306569269487}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        return torch.mean((pred - target) ** 2)\n    else:\n        raise ValueError(f'Unknown loss type {self.loss_type}')", "CodeBleu_score": 29.643686843238815}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.mean((pred - target) ** 2)\n        if not mean:\n            loss = torch.sum(loss)\n    else:\n        raise ValueError(f'Unknown loss type: {self.loss_type}')\n    return loss", "CodeBleu_score": 45.61490551314941}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.size(0)\n    t = torch.ones(batch_size, dtype=torch.int64) * (self.num_timesteps - 1)\n    t = t.to(self.device)\n    z_mean, z_logvar = self.model.forward(x_start, t)\n    kl = 0.5 * (z_mean ** 2 + torch.exp(z_logvar) - z_logvar - 1)\n    kl = kl.sum(dim=1)\n    kl = kl.mean(dim=0)\n    return kl / np.log(2)\n\n", "CodeBleu_score": 24.594170170699634}, {"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.ones(batch_size, dtype=torch.long) * (self.num_timesteps - 1)\n    t = t.to(self.device)\n    mu_t, log_var_t = self.model(x_start, t)\n    kl = 0.5 * (mu_t ** 2 + torch.exp(log_var_t) - log_var_t - 1)\n    return torch.mean(kl.sum(dim=1)) / np.log(2)\n\n", "CodeBleu_score": 27.620635793436243}, {"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.ones(batch_size, 1, 1, 1, 1).to(self.device) * (self.n_timesteps - 1)\n    mean, log_var = self.model(x_start, t)\n    kl = 0.5 * (mean**2 + torch.exp(log_var) - log_var - 1)\n    kl = kl.sum()\n    return kl / (np.log(2) * self.n_channels * self.image_size * self.image_size)\n", "CodeBleu_score": 25.85240665748905}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n    import os\n    from diskcache import Cache\n    cache_dir = os.path.join(os.path.dirname(__file__), \"..\", \"cache\")\n    cache = Cache(os.path.join(cache_dir, subsection))\n    cache.max_size = 4 * 1024 ** 3  # 4 GB\n    cache.min_file_size = 256 * 1024  # 256 KB\n    return cache\n\n", "CodeBleu_score": 44.152839935198394}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n    cache_path = os.path.join(CACHE_DIR, subsection)\n    os.makedirs(cache_path, exist_ok=True)\n    return diskcache.Cache(cache_path, size_limit=4 * 1024 ** 3, disk_min_file_size=256 * 1024)\n\n", "CodeBleu_score": 52.207801865545775}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n    # This function initializes a disk-based cache for a specific subsection. It creates a Cache object from the diskcache module with a path based on the provided subsection and a predefined cache directory. The cache has a size limit of 4 GB, and it prioritizes keeping up to 256KB in an SQLite database by setting a minimum file size for disk storage.\n\n    # Import modules ...\n    import diskcache\n    import os\n\n    # Create cache directory if it does not already exist ...\n    if not os.path.exists(os.path.dirname(__file__) + \"/../../.cache\"):\n        os.makedirs(os.path.dirname(__file__) + \"/../../.cache\")\n\n    # Create cache object ...\n    cache = diskcache.Cache(\n        os.path.dirname(__file__) + \"/../../.cache/{}\".format(subsection)\n    )\n\n    # Configure cache object ...\n    cache.max_size = 4 * 1024 * 1024 * 1024                                                   # [B]\n    cache.min_file_size = 256 * 1024                                                          # [B]\n\n    # Return answer ...\n    return cache", "CodeBleu_score": 46.59108763332212}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n    import os\n    import json\n    import pickle\n    import time\n    import threading\n    from . import settings\n\n    if subsection in settings.caches:\n        return settings.caches[subsection]\n\n    settings.cache_lock.acquire()\n    try:\n        if not os.path.exists(settings.cache_directory):\n            os.makedirs(settings.cache_directory)\n        cache_filename = os.path.join(settings.cache_directory, subsection + \".cache\")\n        if not os.path.isfile(cache_filename):\n            convert_old_cached_data()\n        if not os.path.isfile(cache_filename):\n            settings.caches[subsection] = make_cache(subsection)\n        else:\n            with open(cache_filename, \"rb\") as f:\n                settings.caches[subsection] = pickle.load(f)\n    finally:\n        settings.cache_lock.release()\n\n    return settings.caches[subsection]\n\n", "CodeBleu_score": 35.32292011239804}, {"code": "def cache(subsection)\n    from . import caches, cache_lock, convert_old_cached_data, make_cache\n    if subsection in caches:\n        return caches[subsection]\n    else:\n        with cache_lock:\n            if not os.path.isdir(CACHE_DIR):\n                os.mkdir(CACHE_DIR)\n            cache_filename = CACHE_DIR + subsection + \".cache\"\n            if not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            if subsection not in caches:\n                caches[subsection] = make_cache(subsection)\n            return caches[subsection]\n\n", "CodeBleu_score": 45.701442177014165}, {"code": "def cache(subsection)\n    from . import caches\n    from . import cache_lock\n    from . import convert_old_cached_data\n    from . import make_cache\n\n    if subsection in caches:\n        return caches[subsection]\n\n    with cache_lock:\n        if not (cache_dir.exists() and cache_filename.is_file()):\n            convert_old_cached_data()\n        if subsection not in caches:\n            caches[subsection] = make_cache(subsection)\n        return caches[subsection]", "CodeBleu_score": 38.679767738737695}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if cache[title][0] >= mtime:\n            return cache[title][1]\n        else:\n            del cache[title]\n    data = func()\n    cache[title] = [mtime, data]\n    dump_cache(subsection, cache)\n    return data\n", "CodeBleu_score": 19.34811651514986}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    import os\n    mtime = os.path.getmtime(filename)\n    if title in cache and cache[title][0] >= mtime:\n        return cache[title][1]\n    else:\n        data = func()\n        cache[title] = [mtime, data]\n        dump_cache(subsection, cache)\n        return data\n", "CodeBleu_score": 14.25387924843266}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if mtime <= cache[title][1]:\n            return cache[title][0]\n        else:\n            del cache[title]\n    data = func()\n    cache[title] = (data, mtime)\n    dump_cache(subsection, cache)\n    return data\n", "CodeBleu_score": 19.31919698170391}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n    from traceback import print_exc\n    from .record_exception import record_exception\n    record_exception()\n    if exc_info:\n        print_exc()\n    for line in message.splitlines():\n        print(\"***\", line)\n    if exc_info:\n        print(\"---\")", "CodeBleu_score": 51.463882546204545}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n    import sys\n    import traceback\n\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n\n    print(f\"*** {message}\", file=sys.stderr)\n    if exc_info:\n        traceback.print_exception(\n            exc_type, exc_value, exc_traceback, file=sys.stderr\n        )\n        print(\"---\", file=sys.stderr)", "CodeBleu_score": 46.80491975414714}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n    import traceback\n    import sys\n    import logging\n    logging.exception(message)\n    if exc_info:\n        traceback.print_exc(file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n\n", "CodeBleu_score": 55.386781825250075}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        prompt = extract_text(prompt, style.prompt)\n    except ValueError:\n        return False, prompt, negative_prompt\n    try:\n        negative_prompt = extract_text(negative_prompt, style.negative_prompt)\n    except ValueError:\n        return False, prompt, negative_prompt\n    return True, prompt, negative_prompt\n\n", "CodeBleu_score": 51.34841898833174}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == '' and style.negative_prompt == '':\n        return False, prompt, negative_prompt\n    elif style.prompt == '':\n        return False, prompt, negative_prompt\n    elif style.negative_prompt == '':\n        return False, prompt, negative_prompt\n    else:\n        try:\n            prompt = prompt.split(style.prompt)[1]\n            negative_prompt = negative_prompt.split(style.negative_prompt)[1]\n        except:\n            return False, prompt, negative_prompt\n        return True, prompt, negative_prompt\n", "CodeBleu_score": 52.60860745996413}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == '' or style.negative_prompt == '':\n        return False, prompt, negative_prompt\n    positive_prompt = extract_style_text(prompt, style.prompt)\n    if positive_prompt == False:\n        return False, prompt, negative_prompt\n    negative_prompt = extract_style_text(negative_prompt, style.negative_prompt)\n    if negative_prompt == False:\n        return False, prompt, negative_prompt\n    return True, positive_prompt, negative_prompt\n", "CodeBleu_score": 54.000684117936295}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n    if not self.utterances:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 42.93472535513073}, {"code": "def random_partial(self, count, n_frames)\n    if self.utterances is None:\n        self._load_utterances()\n    samples = self.utterance_cycler.sample(count)\n    return [(u, u.random_partial(n_frames)) for u in samples]\n", "CodeBleu_score": 56.19244203117416}, {"code": "def random_partial(self, count, n_frames)\n\n    if not self.utterances_loaded:\n        self._load_utterances()\n\n    utterances = self.utterance_cycler.sample(count)\n\n    return [(u, u.random_partial(n_frames)) for u in utterances]", "CodeBleu_score": 56.01452748329061}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    change_in_dBFS = target_dBFS - wav.dBFS\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav.apply_gain(change_in_dBFS)", "CodeBleu_score": 51.39414237735191}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    change_in_dBFS = target_dBFS - wav.dBFS\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav.apply_gain(change_in_dBFS)\n", "CodeBleu_score": 51.39414237735191}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only are set\")\n    change_in_dBFS = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (change_in_dBFS / 20))\n\n", "CodeBleu_score": 84.35634890872163}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None)\n    from models.speaker_encoder import SpeakerEncoder\n    from utils.checkpoints import load_checkpoint\n    from utils.general import device_from_string\n\n    if device is None:\n        device = device_from_string(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model = SpeakerEncoder(device=device)\n    load_checkpoint(weights_fpath, model, map_location=device)\n    model.to(device).eval()\n    print(\"Loaded pre-trained model from {}\".format(weights_fpath))\n    print(\"Training step: {}\".format(model.step))\n\n    return model\n\n", "CodeBleu_score": 40.1792626900764}, {"code": "def load_model(weights_fpath: Path, device=None)\n\n    # set the device to cuda if available, otherwise cpu\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # initialize the global model\n    _model = SpeakerEncoder(device, pretrained_weights=None).to(device)\n\n    # load the weights\n    checkpoint = torch.load(weights_fpath, map_location=device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n\n    # put the model in eval mode\n    _model.eval()\n\n    # print a confirmation message\n    print(f\"Loaded pre-trained model from {weights_fpath} (step {checkpoint['step']})\")\n\n", "CodeBleu_score": 37.95525395566819}, {"code": "def load_model(weights_fpath: Path, device=None)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n    global _model\n    _model = SpeakerEncoder(device=device)\n    _model.to(device)\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.eval()\n    print(\"Loaded pre-trained model for speaker encoder from {}\".format(weights_fpath))\n    print(\"Training step: {}\".format(_model.step))\n\n", "CodeBleu_score": 38.70590688384016}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n    if not _model:\n        raise Exception(\"You need to load the model first!\")\n\n    # Convert the input frames_batch from a NumPy array to a PyTorch tensor\n    frames_batch_tensor = torch.from_numpy(frames_batch)\n\n    # Transfer the input frames_batch_tensor to the device (_device)\n    frames_batch_tensor = frames_batch_tensor.to(_device)\n\n    # Perform a forward pass through the model to compute the embeddings\n    embeddings = _model(frames_batch_tensor)\n\n    # Detach the embeddings from the computation graph\n    embeddings = embeddings.detach()\n\n    # Move the embeddings to the CPU\n    embeddings = embeddings.to('cpu')\n\n    # Convert the embeddings from a PyTorch tensor to a NumPy array\n    embeddings = embeddings.numpy()\n\n    # Return the embeddings\n    return embeddings\n\n", "CodeBleu_score": 44.93816863017063}, {"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception(\"No model loaded.\")\n\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 60.94369548878844}, {"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception('Model not loaded')\n\n    frames_batch_tensor = torch.from_numpy(frames_batch)\n    frames_batch_tensor = frames_batch_tensor.to(_device)\n\n    with torch.no_grad():\n        embeddings = _model(frames_batch_tensor)\n        embeddings = embeddings.detach()\n        embeddings = embeddings.cpu()\n        embeddings = embeddings.numpy()\n\n    return embeddings\n\n", "CodeBleu_score": 49.995757473264824}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\n    assert 0 <= overlap < 1\n    assert 0 <= min_pad_coverage <= 1\n\n    # Compute the number of frames and the frame step\n    frames = math.ceil(partial_utterance_n_frames / hop_length)\n    frame_step = math.ceil(frames * (1 - overlap))\n\n    # Compute the number of slices\n    n_slices = math.ceil((n_samples - partial_utterance_n_frames) / frame_step + 1)\n\n    # Pad the signal at the beginning and at the end\n    padded_signal_length = (n_slices - 1) * frame_step + partial_utterance_n_frames\n    if padded_signal_length < n_samples:\n        padded_signal_length = ((n_slices + 1) * frame_step + partial_utterance_n_frames)\n    assert padded_signal_length >= n_samples\n\n    # Pad the signal at the beginning and at the end\n    padded_signal = np.pad(wav, (0, padded_signal_length - wav.shape[0]), mode='constant')\n\n    # Compute the wav and mel slices\n    wav_slices, mel_slices = [], []\n    for i in range(n_slices):\n        mel_range = np.array([i * frame_step, i * frame_step + partial_utterance_n_frames])\n        mel_slices.append(mel_range)\n\n        wav_range = np.array([mel_range[0] * hop_length, mel_range[1] * hop_length])\n        wav_slices.append(wav_range)\n\n    coverage = wav_slices[-1][1] / padded_signal_length\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        wav_slices = wav_slices[:-1]\n        mel_slices = mel_slices[:-1]\n\n    return wav_slices, mel_slices", "CodeBleu_score": 49.4561580291992}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0.0 <= overlap < 1.0, \"overlap must be in the range [0.0, 1.0)\"\n    assert 0.0 <= min_pad_coverage <= 1.0, \"min_pad_coverage must be in the range [0.0, 1.0]\"\n    assert partial_utterance_n_frames % 2 == 0, \"partial_utterance_n_frames must be divisible by 2\"\n\n    # Compute number of frames and frame step\n    frame_step = int(partial_utterance_n_frames * (1.0 - overlap))\n    n_frames = int(np.ceil(n_samples / hop_length))\n\n    # Compute wav and mel slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, int(np.rint((n_frames - partial_utterance_n_frames) / frame_step)))\n    for i in range(steps):\n        start = int(i * frame_step)\n        end = int(start + partial_utterance_n_frames)\n        wav_slices.append((start, end))\n        mel_slices.append((start * hop_length, end * hop_length))\n\n    # Evaluate whether extra padding is needed for the last frame\n    coverage = (n_frames - 1) / (len(mel_slices) - 1) if len(mel_slices) > 1 else 0\n    if coverage < min_pad_coverage:\n        mel_slices.pop()\n        wav_slices.pop()\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 38.03667653106857}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0 <= overlap < 1\n    assert 0 <= min_pad_coverage <= 1\n    assert partial_utterance_n_frames % 2 == 0\n\n    # Compute the number of frames and frame step\n    frames = partial_utterance_n_frames\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))\n\n    # Compute the number of partial utterances\n    n_partial_utterances = int(np.ceil(float(n_samples) / frame_step))\n\n    # Compute the wav and mel ranges\n    wav_ranges = np.array([[i * frame_step, i * frame_step + frames]\n                           for i in range(n_partial_utterances)],\n                          dtype=np.int)\n    mel_ranges = np.array([[i * frame_step, i * frame_step + frames]\n                           for i in range(n_partial_utterances)],\n                          dtype=np.int)\n\n    # Evaluate the need for extra padding\n    last_wav_range = wav_ranges[-1]\n    coverage = (n_samples - last_wav_range[0]) / (last_wav_range[1] - last_wav_range[0])\n    if coverage < min_pad_coverage and n_partial_utterances > 1:\n        wav_ranges = wav_ranges[:-1]\n        mel_ranges = mel_ranges[:-1]\n\n    return wav_ranges, mel_ranges\n\n", "CodeBleu_score": 41.81379326469534}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib import cm\n    from matplotlib.colors import Normalize\n\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n    embed = embed.reshape(shape, shape)\n    if ax is None:\n        ax = plt.gca()\n    cmap = cm.get_cmap(\"magma\")\n    norm = Normalize(vmin=color_range[0], vmax=color_range[1])\n    ax.imshow(embed, cmap=cmap, norm=norm)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar()\n    return ax\n\n", "CodeBleu_score": 53.78643858345693}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Check if an Axes object is provided\n    if ax is None:\n        ax = plt.gca()\n\n    # Reshape the embedding if the shape is not provided\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n        embed = embed.reshape(shape, shape)\n\n    # Plot the embedding as a heatmap\n    cmap = plt.cm.get_cmap('RdBu')\n    cmap.set_bad('lightgray')\n    im = ax.imshow(embed, interpolation='nearest', cmap=cmap, vmin=color_range[0], vmax=color_range[1])\n\n    # Add a color bar\n    cbar = ax.figure.colorbar(im, ax=ax)\n    cbar.ax.tick_params(labelsize=14)\n\n    # Remove x and y ticks\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # Set the title\n    ax.set_title(title, fontsize=16)\n\n    return ax\n\n", "CodeBleu_score": 57.50652340480313}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        shape = int(np.sqrt(embed.shape[0]))\n    embed = embed.reshape(shape, shape)\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    sns.heatmap(embed, cmap=cmap, xticklabels=False, yticklabels=False, cbar_kws={'label': 'Embedding value'}, ax=ax, vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n\n", "CodeBleu_score": 48.3366100814265}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n    centroids_incl = torch.zeros(num_speakers, num_utterances, self.embedding_dim, device=self.device)\n    centroids_excl = torch.zeros(num_speakers, num_utterances, self.embedding_dim, device=self.device)\n    for i in range(num_speakers):\n        for j in range(num_utterances):\n            centroids_incl[i, j] = torch.mean(embeds[i, :j + 1], dim=0)\n            centroids_excl[i, j] = torch.mean(embeds[i, :j] + embeds[i, j + 1:], dim=0)\n    sim_matrix = torch.zeros(num_speakers, num_utterances, num_utterances, device=self.device)\n    mask = torch.eye(num_utterances, device=self.device)\n    for i in range(num_speakers):\n        for j in range(num_utterances):\n            sim_matrix[i, j] = torch.mm(centroids_incl[i, j], centroids_excl[i, :].t())\n    sim_matrix = sim_matrix * (1 - mask)\n    sim_matrix = sim_matrix / self.temperature\n    sim_matrix = sim_matrix - torch.max(sim_matrix, dim=2, keepdim=True)[0]\n    return sim_matrix\n\n", "CodeBleu_score": 35.745579811398706}, {"code": "def similarity_matrix(self, embeds)\n\n    # get number of speakers and utterances per speaker\n    num_speakers, num_utterances = embeds.shape[:2]\n\n    # initialize similarity matrix\n    similarity_matrix = torch.zeros(num_speakers, num_speakers, device=self.device)\n\n    # initialize mask matrix\n    mask = torch.ones(num_speakers, num_speakers, device=self.device)\n    mask.fill_diagonal_(0)\n\n    # iterate over all speakers\n    for i in range(num_speakers):\n\n        # get embeddings of current speaker\n        embeds_speaker = embeds[i]\n\n        # calculate centroids\n        centroids_incl = embeds_speaker.mean(dim=0, keepdim=True)\n        centroids_excl = (embeds_speaker.sum(dim=0, keepdim=True) - embeds_speaker) / (num_utterances - 1)\n\n        # calculate similarity scores\n        scores_incl = torch.matmul(embeds_speaker, centroids_incl.t())\n        scores_excl = torch.matmul(embeds_speaker, centroids_excl.t())\n\n        # update similarity matrix\n        similarity_matrix[i] = torch.max(scores_incl, scores_excl)\n\n    # scale and bias similarity matrix\n    similarity_matrix = similarity_matrix * self.scale + self.bias\n\n    # mask similarity matrix\n    similarity_matrix = similarity_matrix * mask\n\n    return similarity_matrix", "CodeBleu_score": 30.445548887154807}, {"code": "def similarity_matrix(self, embeds)\n\n    # get number of speakers and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # initialize similarity matrix\n    device = embeds.device\n    sim_matrix = torch.zeros(num_speakers, num_speakers).to(device)\n\n    # initialize mask matrix\n    mask_matrix = torch.ones(num_speakers, num_speakers).to(device)\n    mask_matrix.fill_diagonal_(0)\n\n    # compute centroids\n    centroids_incl = embeds.mean(dim=1)\n    centroids_excl = (\n        embeds.sum(dim=1) - embeds\n    ) / (num_utterances - 1)\n\n    # compute similarity scores\n    for i in range(num_speakers):\n        for j in range(num_speakers):\n            # compute similarity score\n            sim_score = torch.nn.functional.cosine_similarity(\n                centroids_incl[i], centroids_excl[j], dim=0\n            )\n            # update similarity matrix\n            sim_matrix[i, j] = sim_score\n\n    # scale and bias similarity matrix\n    sim_matrix = sim_matrix * 2 - 1\n    sim_matrix = sim_matrix * mask_matrix\n\n    return sim_matrix", "CodeBleu_score": 24.63777450635139}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n    # Get the number of speakers and utterances per speaker\n    num_speakers, num_utterances = embeds.shape[0], embeds.shape[1]\n\n    # Compute the similarity matrix\n    similarity_matrix = torch.matmul(embeds, torch.transpose(embeds, 1, 0))\n\n    # Reshape the similarity matrix\n    similarity_matrix = similarity_matrix.reshape(num_utterances, num_speakers, num_utterances, num_speakers)\n\n    # Compute the ground truth labels\n    labels = torch.arange(num_utterances).repeat(num_speakers, 1)\n    labels = labels.reshape(num_utterances, num_speakers, 1, 1)\n    labels = labels == labels.transpose(0, 1)\n    labels = labels.float()\n\n    # Compute the loss\n    loss = self.loss_fn(similarity_matrix, labels)\n\n    # Compute the EER\n    with torch.no_grad():\n        # Compute the labels\n        preds = torch.argmax(similarity_matrix, dim=1)\n        preds = preds.reshape(num_utterances, num_speakers)\n        preds = preds == torch.arange(num_utterances).repeat(num_speakers, 1).to(self.device)\n        preds = preds.float()\n\n        # Compute the false positive and false negative rates\n        fpr, tpr, thresholds = roc_curve(labels.flatten().cpu(), preds.flatten().cpu())\n        fnr = 1 - tpr\n\n        # Compute the Equal Error Rate\n        eer = torch.where(fpr - fnr == 0)[0]\n        eer = thresholds[eer]\n\n    return loss, eer\n\n", "CodeBleu_score": 38.523452879344795}, {"code": "def loss(self, embeds)\n    # Get the number of speakers and utterances per speaker from the shape of the input embeddings\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Compute the similarity matrix\n    sim_matrix = self.compute_similarity_matrix(embeds)\n\n    # Reshape the similarity matrix\n    sim_matrix = sim_matrix.reshape(num_speakers, num_utterances, num_utterances)\n\n    # Generate the ground truth labels\n    labels = self.generate_labels(num_speakers, num_utterances)\n\n    # Compute the loss\n    loss = self.compute_loss(sim_matrix, labels)\n\n    # Compute the EER\n    eer = self.compute_eer(sim_matrix, labels)\n\n    return loss, eer", "CodeBleu_score": 8.306014290918954}, {"code": "def loss(self, embeds)\n    # Get the number of speakers and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n\n    # Compute the similarity matrix\n    similarity_matrix = self.compute_similarity_matrix(embeds)\n\n    # Reshape the similarity matrix\n    similarity_matrix = similarity_matrix.reshape(num_speakers * num_utterances, num_speakers * num_utterances)\n\n    # Generate ground truth labels\n    labels = self.generate_labels(num_speakers, num_utterances)\n\n    # Calculate the loss\n    loss = self.calculate_loss(similarity_matrix, labels)\n\n    # Calculate the EER\n    eer = self.calculate_eer(similarity_matrix, labels)\n\n    return loss, eer", "CodeBleu_score": 9.255350600147613}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n    if self.log_params:\n        import pprint\n        import sys\n        from IPython.display import display, HTML\n\n        model_params = self.model.get_params()\n        data_params = self.data.get_params()\n\n        model_params_str = \"Model Parameters: \\n\"\n        data_params_str = \"Data Parameters: \\n\"\n\n        for param, value in model_params.items():\n            model_params_str += f\"{param}: {value} \\n\"\n        for param, value in data_params.items():\n            data_params_str += f\"{param}: {value} \\n\"\n\n        display(HTML(f\"<b>{model_params_str}</b>\"))\n        display(HTML(f\"<b>{data_params_str}</b>\"))\n\n        if self.verbose:\n            print(model_params_str)\n            print(data_params_str)\n            print()\n\n", "CodeBleu_score": 23.0435536185325}, {"code": "def log_params(self)\n    if self.log_params:\n        from .log import log_params\n        log_params(self)\n", "CodeBleu_score": 4.688789693104062}, {"code": "def log_params(self)\n    if not self.disable_logging:\n        import pprint\n        import numpy as np\n        import pandas as pd\n        import matplotlib.pyplot as plt\n        from matplotlib.lines import Line2D\n        from matplotlib.patches import Patch\n        from matplotlib.colors import ListedColormap\n        from matplotlib import cm\n        from matplotlib.patches import Patch\n        from matplotlib.patches import Rectangle\n        from matplotlib.lines import Line2D\n        from matplotlib.collections import PatchCollection\n        from matplotlib.colors import ListedColormap\n        from matplotlib import cm\n        import seaborn as sns\n        import warnings\n        warnings.filterwarnings('ignore')\n\n        # Import parameter configurations\n        from .config import (\n            model_params,\n            data_params,\n            data_params_names,\n            model_params_names\n        )\n\n        # Create a dictionary of parameters\n        params_dict = {\n            'model_params': model_params,\n            'data_params': data_params\n        }\n\n        # Create a list of parameter names\n        params_names = {\n            'model_params': model_params_names,\n            'data_params': data_params_names\n        }\n\n        # Create a list of parameters\n        params_list = [\n            'model_params',\n            'data_params'\n        ]\n\n        # Create a list of parameter names\n        params_names_list = [\n            'model_params',\n            'data_params'\n        ]\n\n        # Create a list of parameter names\n        params_names_list = [\n            'model_params',\n            'data_params'\n        ]\n\n        # Create a list of parameter names\n        params_names_list = [\n            'model_params',\n            'data_params'\n        ]\n\n        # Create a list of parameter names\n        params_names_list = [\n            'model_params',\n            'data_params'\n        ]\n\n        # Create a list of parameter names\n        params_names_list = [\n            'model_params',\n            'data_params'\n        ]\n\n        # Create a list of parameter names\n        params_names", "CodeBleu_score": 19.181409873603624}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    import matplotlib.pyplot as plt\n    import umap\n    import numpy as np\n\n    # Limit the number of speakers\n    speakers = list(embeds.keys())[:max_speakers]\n\n    # Flatten the embeddings\n    embeds_flat = np.array([embed.detach().cpu().numpy() for speaker in speakers for embed in embeds[speaker]])\n\n    # Assign colors based on speaker identity\n    colors = np.array([i for speaker in speakers for i in range(len(embeds[speaker]))])\n\n    # Reduce dimensionality\n    reducer = umap.UMAP(random_state=0)\n    embeds_2d = reducer.fit_transform(embeds_flat)\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(12, 10))\n    ax.scatter(embeds_2d[:, 0], embeds_2d[:, 1], c=colors, s=1)\n    ax.set_title(f\"Step {step}\")\n    fig.tight_layout()\n\n    # Save to file\n    if out_fpath:\n        fig.savefig(out_fpath)\n\n    # Update the visualization\n    if self.visualization_steps:\n        self.visualization.update_embedding_projections(fig)\n\n    # Close the figure\n    plt.close(fig)\n\n", "CodeBleu_score": 45.254506333717366}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from sklearn.manifold import TSNE\n    from sklearn.manifold import MDS\n    from sklearn.manifold import Isomap\n    from sklearn.manifold import SpectralEmbedding\n    from sklearn.manifold import LocallyLinearEmbedding\n    from sklearn.manifold import trustworthiness\n    from sklearn.manifold import fast_embedding_dimension\n    from sklearn.manifold import embedding_dimension\n    from sklearn.manifold import Embedding_dimension\n    from sklearn.manifold import Embedding_dimension_\n    from sklearn.manifold import Embedding_dimension__\n    from sklearn.manifold import Embedding_dimension___\n    from sklearn.manifold import Embedding_dimension____\n    from sklearn.manifold import Embedding_dimension_____\n    from sklearn.manifold import Embedding_dimension______\n    from sklearn.manifold import Embedding_dimension_______\n    from sklearn.manifold import Embedding_dimension________\n    from sklearn.manifold import Embedding_dimension_________\n    from sklearn.manifold import Embedding_dimension__________\n    from sklearn.manifold import Embedding_dimension___________\n    from sklearn.manifold import Embedding_dimension____________\n    from sklearn.manifold import Embedding_dimension_____________\n    from sklearn.manifold import Embedding_dimension______________\n    from sklearn.manifold import Embedding_dimension_______________\n    from sklearn.manifold import Embedding_dimension________________\n    from sklearn.manifold import Embedding_dimension_________________\n    from sklearn.manifold import Embedding_dimension__________________\n    from sklearn.manifold import Embedding_dimension___________________\n    from sklearn.manifold import Embedding_dimension____________________\n    from sklearn.manifold import Em", "CodeBleu_score": 7.952630487356221}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    import matplotlib.pyplot as plt\n    import matplotlib.colors as mcolors\n    import umap\n    import numpy as np\n    import random\n    import os\n\n    # Limit the number of speakers\n    speaker_ids = list(embeds.keys())\n    random.shuffle(speaker_ids)\n    speaker_ids = speaker_ids[:max_speakers]\n    embeds = {speaker_id: embeds[speaker_id] for speaker_id in speaker_ids}\n\n    # Flatten the embeddings\n    embeds_flat = []\n    for speaker_id, embeds_per_speaker in embeds.items():\n        embeds_flat.extend(embeds_per_speaker)\n    embeds_flat = np.array(embeds_flat)\n\n    # Assign colors based on speaker identity\n    colors = []\n    color_map = {}\n    color_idx = 0\n    for speaker_id in speaker_ids:\n        color = mcolors.CSS4_COLORS[color_idx]\n        colors.extend([color] * utterances_per_speaker[speaker_id])\n        color_map[speaker_id] = color\n        color_idx += 1\n    colors = np.array(colors)\n\n    # Reduce the dimensionality of the embeddings\n    reducer = umap.UMAP(n_components=2)\n    embeds_flat = reducer.fit_transform(embeds_flat)\n\n    # Plot the embeddings\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(embeds_flat[:, 0], embeds_flat[:, 1], c=colors)\n    ax.set_title(f\"Step {step}\")\n    plt.tight_layout()\n\n    # Display or save the plot\n    if out_fpath is None:\n        plt.show()\n    else:\n        out_dir = os.path.dirname(out_fpath)\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n        plt.savefig(out_", "CodeBleu_score": 39.365454807393306}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    batch_size, sequence_length, embedding_dimension = x.shape\n    # 1. Save the dimensions of the input tensor as human-readable names\n    batch_size = batch_size\n    sequence_length = sequence_length\n    embedding_dimension = embedding_dimension\n    # 2. Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dimension = speaker_embedding.shape[0]\n    speaker_embedding_index = embedding_dimension - speaker_embedding_dimension\n    # 3. Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = speaker_embedding.repeat(sequence_length, 1)\n    # 4. Reshape and transpose the speaker embedding\n    speaker_embedding = speaker_embedding.reshape(sequence_length, batch_size, speaker_embedding_dimension)\n    speaker_embedding = speaker_embedding.transpose(1, 0)\n    # 5. Concatenate the speaker embedding with the input tensor along the last dimension and return the result\n    x = torch.cat([x, speaker_embedding], dim=speaker_embedding_index)\n    return x\n\n", "CodeBleu_score": 16.568012218250274}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    if speaker_embedding is not None:\n        if len(x.shape) == 2:\n            x = tf.expand_dims(x, 1)\n            x = tf.concat([tf.tile(speaker_embedding, [1, tf.shape(x)[1], 1]), x], axis=-1)\n        else:\n            x = tf.concat([tf.tile(speaker_embedding, [1, tf.shape(x)[1], tf.shape(x)[2], 1]), x], axis=-1)\n    return x\n\n", "CodeBleu_score": 28.830097587794796}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    # Save dimensions of input tensor as human-readable names\n    batch_size, seq_length, embedding_dim = x.shape\n\n    # Determine the appropriate dimension index for the speaker embedding\n    # based on whether the input tensor is 2D or 3D\n    if len(x.shape) == 2:\n        dim_index = 1\n    elif len(x.shape) == 3:\n        dim_index = 2\n\n    # Repeat the speaker embedding to match the length of the input text\n    # and reshape and transpose it\n    speaker_embedding = speaker_embedding.repeat(seq_length, 1)\n    speaker_embedding = speaker_embedding.reshape(batch_size, seq_length, -1)\n    speaker_embedding = speaker_embedding.transpose(1, 2)\n\n    # Concatenate the speaker embedding with the input tensor\n    # along the last dimension and return the result\n    return torch.cat([x, speaker_embedding], dim=dim_index)\n\n", "CodeBleu_score": 33.12554149469492}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n    self.device = next(self.parameters()).device\n    self.seq_proj_size = encoder_seq_proj.shape[2]\n    self.batch_size = encoder_seq_proj.shape[1]\n    self.seq_len = encoder_seq_proj.shape[0]\n    self.cumulative_weights = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n    self.attention_weights = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n", "CodeBleu_score": 17.371773108388293}, {"code": "def init_attention(self, encoder_seq_proj)\n    self.device = next(self.parameters()).device\n    self.batch_size, self.seq_len, self.hidden_size = encoder_seq_proj.shape\n    self.cumulative_alphas = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n    self.attention_weights = torch.zeros(self.batch_size, self.seq_len).to(self.device)\n", "CodeBleu_score": 18.76682951949329}, {"code": "def init_attention(self, encoder_seq_proj)\n    self.device = next(self.parameters()).device\n    self.batch_size, self.seq_len, self.encoder_size = encoder_seq_proj.shape\n    self.cumulative_attention = torch.zeros((self.batch_size, self.seq_len)).to(self.device)\n    self.attention = torch.zeros((self.batch_size, self.seq_len)).to(self.device)\n\n", "CodeBleu_score": 23.740223511885972}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n    if path.endswith('.pt'):\n        checkpoint = torch.load(path, map_location=self.device)\n        self.load_state_dict(checkpoint['state_dict'])\n        if optimizer is not None:\n            optimizer.load_state_dict(checkpoint['optimizer'])\n        return checkpoint['epoch']\n    else:\n        raise ValueError('The path must be a .pt file')\n", "CodeBleu_score": 43.33239612172653}, {"code": "def load(self, path, optimizer=None)\n    if not os.path.exists(path):\n        raise ValueError(\"The given path does not exist.\")\n    if not os.path.isfile(path):\n        raise ValueError(\"The given path is not a file.\")\n    device = self.get_device()\n    checkpoint = torch.load(path, map_location=device)\n    self.load_state_dict(checkpoint[\"state_dict\"])\n    if optimizer is not None and \"optimizer\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n", "CodeBleu_score": 48.99222648071483}, {"code": "def load(self, path, optimizer=None)\n    if not os.path.isfile(path):\n        raise FileNotFoundError(f\"Checkpoint not found at '{path}'\")\n    device = next(self.parameters()).device\n    checkpoint = torch.load(path, map_location=device)\n    self.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint\n\n", "CodeBleu_score": 62.239801060497825}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save({\n            'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, path)\n\n", "CodeBleu_score": 58.06990691898981}, {"code": "def save(self, path, optimizer=None)\n    torch.save(self.state_dict(), path)\n    if optimizer:\n        torch.save(optimizer.state_dict(), path + \".opt\")", "CodeBleu_score": 34.01230612743799}, {"code": "def save(self, path, optimizer=None)\n    import torch\n    import os\n\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save({\"model\": self.state_dict(), \"optimizer\": optimizer.state_dict()}, path)\n\n", "CodeBleu_score": 54.574769910794}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n\n    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n    text = text.lower()\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 70.51977395619193}, {"code": "def english_cleaners(text)\n    text = text.encode('ascii', 'ignore').decode('ascii')\n    text = text.lower()\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n", "CodeBleu_score": 70.85585516644855}, {"code": "def english_cleaners(text)\n\n    # Convert to ASCII\n    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n\n    # Expand numbers\n    text = normalize_numbers(text)\n\n    # Expand abbreviations\n    text = normalize_abbreviations(text)\n\n    # Lowercase\n    text = text.lower()\n\n    # Collapse whitespace\n    text = collapse_whitespace(text)\n\n    return text\n\n", "CodeBleu_score": 60.05338773138183}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id[_eos])\n    return sequence\n\n", "CodeBleu_score": 78.69311884798235}, {"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(\n                _clean_text(text, cleaner_names), _symbol_to_id)\n            break\n        sequence += _symbols_to_sequence(m.group(1), _arpabet_to_sequence)\n        text = text[m.end():]\n\n    # Append EOS token\n    sequence.append(_eos_id)\n    return sequence\n\n", "CodeBleu_score": 54.392166248213336}, {"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id[_eos])\n    return sequence\n\n", "CodeBleu_score": 78.69311884798235}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n    text = ''\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            symbol = _symbol_to_id[symbol_id]\n            if len(symbol) > 1 and symbol[0] == '@':\n                text += ' '\n            text += symbol\n    text = text.replace('}{', ' ')\n    return text\n\n", "CodeBleu_score": 49.030508139071}, {"code": "def sequence_to_text(sequence)\n    text = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            symbol = _symbol_to_id[symbol_id]\n            if len(symbol) > 1:\n                symbol = \"{%s}\" % symbol\n            text += symbol\n    text = text.replace(\"}{\", \" \")\n    return text\n", "CodeBleu_score": 47.32537063049499}, {"code": "def sequence_to_text(sequence)\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            symbol = _symbol_to_id[symbol_id]\n            if len(symbol) > 1 and symbol[0] == '\\\\':\n                symbol = '{%s}' % symbol\n            result += symbol\n    result = result.replace('}{', ' ')\n    return result\n\n", "CodeBleu_score": 46.688022413867415}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n\n    # Load the encoder model\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Preprocess the utterance and save the embedding\n    for fpath in fpaths:\n        wav_fpath, embed_fpath = fpath\n        wav = preprocess_wav(wav_fpath)\n        embed = encoder.embed_utterance(wav)\n        np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 58.167393818477066}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    import librosa\n    import numpy as np\n    import os\n    import pickle\n    import soundfile as sf\n    import sys\n    import torch\n    from encoder import inference as encoder\n    from pathlib import Path\n    from pypinyin import pinyin, lazy_pinyin, Style\n    from tqdm import tqdm\n    from utils import logmmse\n    from utils import preprocess_wav\n\n    encoder.load_model(encoder_model_fpath)\n\n    # Process the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav, _ = librosa.load(wav_fpath, sr=16000)\n    wav = preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    embed = embed.cpu().numpy()\n    np.save(embed_fpath, embed, allow_pickle=False)\n\n", "CodeBleu_score": 51.9874865523148}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    import sys\n    import numpy as np\n    import tensorflow as tf\n    import librosa\n    import soundfile as sf\n    import encoder\n    import argparse\n\n    encoder.load_model(encoder_model_fpath)\n\n    for fpath in fpaths:\n        wav = encoder.preprocess_wav(fpath)\n        embed = encoder.embed_utterance(wav)\n        np.save(fpath.replace(\".wav\", \".npy\"), embed)", "CodeBleu_score": 31.943423836321728}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n    import librosa\n    import numpy as np\n    import pandas as pd\n    import tqdm\n    from encoder import inference as encoder\n    from pathlib import Path\n    from pydub import AudioSegment\n    from pydub.utils import make_chunks\n    from tqdm import tqdm\n\n    # verify the existence of required directories and files\n    assert synthesizer_root.exists(), \"synthesizer_root does not exist\"\n    assert encoder_model_fpath.exists(), \"encoder_model_fpath does not exist\"\n\n    # verify the existence of required directories and files\n    assert synthesizer_root.exists(), \"synthesizer_root does not exist\"\n    assert encoder_model_fpath.exists(), \"encoder_model_fpath does not exist\"\n\n    # read metadata\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert metadata_fpath.exists(), \"metadata_fpath does not exist\"\n    metadata = pd.read_csv(metadata_fpath, sep=\"|\", header=None)\n    metadata.columns = [\"audio_fpath\", \"raw_text\", \"normalized_text\", \"speaker_name\"]\n\n    # prepare file paths for audio and embeddings\n    audio_fpaths = []\n    embed_fpaths = []\n    for index, row in metadata.iterrows():\n        audio_fpath = synthesizer_root.joinpath(row[\"audio_fpath\"])\n        embed_fpath = audio_fpath.parent.joinpath(audio_fpath.stem + \".npy\")\n        audio_fpaths.append(audio_fpath)\n        embed_fpaths.append(embed_fpath)\n\n    # verify the existence of required directories and files\n    assert synthesizer_root.exists(), \"synthesizer_root does not exist\"\n    assert encoder_model_fpath.exists(), \"encoder_model_fpath does not exist\"\n\n    # verify the existence of required directories and files\n    assert synthesizer_root.exists(), \"synthesizer_root does not exist\"\n    assert encoder_model_fpath.exists(),", "CodeBleu_score": 34.23337606932496}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\n    # Verify the existence of required directories and files\n    synthesizer_root = Path(synthesizer_root).expanduser()\n    encoder_model_fpath = Path(encoder_model_fpath).expanduser()\n    if not synthesizer_root.exists():\n        raise Exception(\"The synthesizer root directory could not be found. Have you mounted it correctly?\")\n    if not encoder_model_fpath.exists():\n        raise Exception(\"The encoder model could not be found. Have you mounted it correctly?\")\n    if not encoder_model_fpath.is_file():\n        raise Exception(\"The specified encoder model is not a file. Please check the path.\")\n    if not synthesizer_root.joinpath(\"encoder/saved_models/pretrained.pt\").exists():\n        raise Exception(\"The synthesizer has not been trained. Please train it before attempting to create embeddings.\")\n    if not synthesizer_root.joinpath(\"encoder/saved_models/pretrained.pt\").is_file():\n        raise Exception(\"The synthesizer model is not a file. Please check the path.\")\n\n    # Load metadata\n    metadata_fpath = synthesizer_root.joinpath(\"encoder/saved_models/pretrained.pt\")\n    metadata = torch.load(metadata_fpath)\n    mel_fpaths = metadata[\"training_files\"]\n\n    # Prepare file paths for audio and embeddings\n    audio_fpaths = [fpath.with_suffix(\".wav\") for fpath in mel_fpaths]\n    embed_fpaths = [fpath.with_suffix(\".npy\") for fpath in mel_fpaths]\n    audio_dir, mel_dir = (synthesizer_root / \"audio\").as_posix(), (synthesizer_root / \"mels\").as_posix()\n    embed_dir = (synthesizer_root / \"encoder/embeddings\").as_posix()\n    os.makedirs(embed_dir, exist_ok=True)\n\n    # Create the embeddings using a multiprocessing pool\n    if n_processes == 1:", "CodeBleu_score": 33.95580681615548}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\n    # verify the existence of the synthesizer root directory\n    if not synthesizer_root.exists():\n        raise Exception(f\"Synthesizer root directory {synthesizer_root} does not exist.\")\n\n    # verify the existence of the encoder model file\n    if not encoder_model_fpath.exists():\n        raise Exception(f\"Encoder model {encoder_model_fpath} does not exist.\")\n\n    # verify the existence of the metadata file\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    if not metadata_fpath.exists():\n        raise Exception(f\"Metadata file {metadata_fpath} does not exist.\")\n\n    # verify the existence of the audio directory\n    audio_dir = synthesizer_root.joinpath(\"audio\")\n    if not audio_dir.exists():\n        raise Exception(f\"Audio directory {audio_dir} does not exist.\")\n\n    # verify the existence of the embeddings directory\n    embeddings_dir = synthesizer_root.joinpath(\"embeddings\")\n    if not embeddings_dir.exists():\n        raise Exception(f\"Embeddings directory {embeddings_dir} does not exist.\")\n\n    # read metadata\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n\n    # create embeddings directory\n    embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n    # create file paths for audio and embeddings\n    audio_fpaths = [audio_dir.joinpath(m[0]) for m in metadata]\n    embeddings_fpaths = [embeddings_dir.joinpath(m[1].replace(\".npy\", \".pt\")) for m in metadata]\n\n    # create a multiprocessing pool\n    pool = multiprocessing.Pool(processes=n_processes)\n\n    # create embeddings\n    for i, embedding_fpath in enumerate(tqdm(embeddings_fpaths, \"Creating embeddings\")):\n        # verify the existence of the audio file\n        if", "CodeBleu_score": 51.98076550372532}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    import matplotlib.pyplot as plt\n    import librosa\n    import librosa.display\n    import numpy as np\n    import os\n    import tensorflow as tf\n    from tensorflow.python.client import device_lib\n\n    # Save the attention plot\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attention, cmap='viridis')\n    ax.set_title('Attention Step: {}'.format(step))\n    plt.savefig(os.path.join(plot_dir, '{}_attention_step_{}.png'.format(sample_num, step)))\n    plt.close(fig)\n\n    # Save the predicted mel spectrogram\n    mel_prediction = mel_prediction.T\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(mel_prediction, cmap='magma')\n    ax.set_title('Predicted Mel Spectrogram Step: {}'.format(step))\n    plt.savefig(os.path.join(mel_output_dir, '{}_pred_mel_step_{}.png'.format(sample_num, step)))\n    plt.close(fig)\n\n    # Save the reconstructed wav file\n    wav = librosa.griffinlim(mel_prediction, n_iter=32, hop_length=hparams.hop_length, win_length=hparams.win_length)\n    wav = np.clip(wav, -1, 1)\n    wav_file = os.path.join(wav_dir, '{}_step_{}.wav'.format(sample_num, step))\n    librosa.output.write_wav(wav_file, wav, hparams.sample_rate)\n\n    # Generate the plot comparing the predicted and target mel spectrograms\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(2,", "CodeBleu_score": 37.52553765398016}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    # save attention plot\n    attention_plot = attention.T\n    fig = plt.figure(figsize=(12, 12))\n    plt.imshow(attention_plot, aspect=\"auto\", origin=\"lower\")\n    plt.colorbar()\n    plt.savefig(plot_dir + \"/attention_step{}_sample{}.png\".format(step, sample_num))\n    plt.close(fig)\n\n    # save mel spectrogram plot\n    mel_spectrogram = mel_prediction.T\n    fig = plt.figure(figsize=(12, 12))\n    plt.imshow(mel_spectrogram, aspect=\"auto\", origin=\"lower\")\n    plt.colorbar()\n    plt.savefig(plot_dir + \"/mel_step{}_sample{}.png\".format(step, sample_num))\n    plt.close(fig)\n\n    # save wav\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    audio.save_wav(wav, wav_dir + \"/step{}_sample{}.wav\".format(step, sample_num), hparams)\n\n    # save mel spectrogram plot with target\n    fig = plt.figure(figsize=(12, 12))\n    plt.subplot(2, 1, 1)\n    plt.title(\"Target Mel Spectrogram\")\n    plt.imshow(target_spectrogram.T, aspect=\"auto\", origin=\"lower\")\n    plt.colorbar()\n    plt.subplot(2, 1, 2)\n    plt.title(\"Predicted Mel Spectrogram\")\n    plt.imshow(mel_spectrogram, aspect=\"auto\", origin=\"lower\")\n    plt.colorbar()\n    plt.tight_layout()\n    plt.savefig(plot_dir + \"/step{}_sample{}.png\".format(step, sample_num))\n    plt.close(fig)\n\n    # save mel spectrogram to disk (for later reconstruction)\n    mel_filename = mel_output_dir + \"/mel-{}.npy\".format(step)\n    np.save(", "CodeBleu_score": 40.37334306454976}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    if not hparams.train_seq2seq:\n        print(\"Skipping eval step because train_seq2seq is set to False.\")\n        return\n    print(\"Evaluating model at step {}...\".format(step))\n    if hparams.model_type == 'attention':\n        attention_plot = attention.squeeze(1).cpu().data.numpy()\n        fig = plt.figure(figsize=(10, 10))\n        plt.imshow(attention_plot.T, aspect='auto', origin='lower')\n        plt.xlabel('Decoder timestep')\n        plt.ylabel('Encoder timestep')\n        plt.colorbar()\n        plt.tight_layout()\n        plt.savefig(os.path.join(plot_dir, \"attention_step{}.png\".format(step)))\n        plt.close(fig)\n\n    mel_prediction = mel_prediction.cpu().data.numpy()\n    mel_prediction = mel_prediction.reshape((-1, hparams.num_mels))\n    target_spectrogram = target_spectrogram.cpu().data.numpy()\n    target_spectrogram = target_spectrogram.reshape((-1, hparams.num_mels))\n    fig = plt.figure(figsize=(10, 10))\n    plt.subplot(2, 1, 1)\n    plt.title(\"Predicted mel spectrogram at step {}\".format(step))\n    plt.imshow(mel_prediction, origin=\"lower\", aspect=\"auto\")\n    plt.colorbar()\n    plt.subplot(2, 1, 2)\n    plt.title(\"Target mel spectrogram\")\n    plt.imshow(target_spectrogram, origin=\"lower\", aspect=\"auto\")\n    plt.colorbar()\n    plt.tight_layout()\n    plt.savefig(os.path.join(plot_dir, \"mel_step{}.png\".format(step)))\n    plt.close(fig)\n    mel_filename = \"mel-prediction-step-{}.npy\".format(step)\n    np.save(os", "CodeBleu_score": 34.24842921725313}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print(\"Trainable Parameters: %.3f million\" % (params / 1e6))", "CodeBleu_score": 40.4573308357239}, {"code": "def num_params(self)\n    params = [p for p in self.parameters() if p.requires_grad]\n    num_params = sum([np.prod(p.size()) for p in params])\n    print(f'The number of trainable parameters is {num_params / 1e6:.3f}M')\n\n", "CodeBleu_score": 33.30004361525521}, {"code": "def num_params(self)\n    total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    print(f\"Total params: {total_params:,}\")\n    return total_params\n\n", "CodeBleu_score": 13.840126813964659}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n\n    if side == 'both':\n        return torch.cat([torch.zeros(x.shape[0], pad, x.shape[2], x.shape[3], device=x.device), x, torch.zeros(x.shape[0], pad, x.shape[2], x.shape[3], device=x.device)], dim=1)\n    elif side == 'before':\n        return torch.cat([torch.zeros(x.shape[0], pad, x.shape[2], x.shape[3], device=x.device), x], dim=1)\n    elif side == 'after':\n        return torch.cat([x, torch.zeros(x.shape[0], pad, x.shape[2], x.shape[3], device=x.device)], dim=1)\n    else:\n        raise ValueError('side must be either \"before\", \"after\", or \"both\"')\n\n", "CodeBleu_score": 21.61797869691469}, {"code": "def pad_tensor(self, x, pad, side='both')\n\n    # Determine the new size of the tensor after padding\n    new_size = x.shape[1] + pad\n\n    # Create a new tensor filled with zeros on either the CPU or GPU, depending on availability\n    device = x.device\n    if device == torch.device('cpu'):\n        new_tensor = torch.zeros(x.shape[0], new_size, x.shape[2])\n    else:\n        new_tensor = torch.zeros(x.shape[0], new_size, x.shape[2]).cuda()\n\n    # Place the original tensor into the new padded tensor according to the specified side\n    if side == 'before':\n        new_tensor[:, pad:, :] = x\n    elif side == 'after':\n        new_tensor[:, :x.shape[1], :] = x\n    elif side == 'both':\n        new_tensor[:, :pad//2, :] = x[:, :pad//2, :]\n        new_tensor[:, pad//2:pad//2+x.shape[1], :] = x\n        new_tensor[:, pad//2+x.shape[1]:, :] = x[:, -pad//2:, :]\n    else:\n        raise ValueError(\"side must be one of 'before', 'after', or 'both'.\")\n\n    return new_tensor", "CodeBleu_score": 32.74911460263381}, {"code": "def pad_tensor(self, x, pad, side='both')\n\n    # The number of frames in the padded tensor.\n    size = x.size(0) + pad * 2\n\n    # Create a new tensor filled with zeros.\n    y = torch.zeros(size, *x.size()[1:]).to(x.device)\n\n    # Determine the side.\n    if side == 'both':\n        # If padding is to be applied to both sides...\n        left = pad\n        right = x.size(0) - 1 + pad\n    elif side == 'before':\n        # If padding is to be applied before...\n        left = pad\n        right = x.size(0) - 1\n    else:\n        # If padding is to be applied after...\n        left = 0\n        right = x.size(0) - 1 + pad\n\n    # Place the original tensor into the new tensor.\n    y[left:right] = x\n\n    # Return the new padded tensor.\n    return y\n\n", "CodeBleu_score": 29.24140491055352}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(12, 12))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + list(range(1, attn.shape[1] + 1)), fontdict={'fontsize': 14})\n    ax.set_yticklabels([''] + list(range(1, attn.shape[0] + 1)), fontdict={'fontsize': 14})\n    plt.tight_layout()\n    plt.savefig(path)\n    plt.close(fig)\n\n", "CodeBleu_score": 45.856440635050234}, {"code": "def save_attention(attn, path)\n\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n    ax.set_xticklabels([''] + list(range(1, attn.shape[1] + 1)))\n    ax.set_yticklabels([''] + list(range(1, attn.shape[0] + 1)))\n    plt.tight_layout()\n    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n    plt.close()", "CodeBleu_score": 44.80758750843705}, {"code": "def save_attention(attn, path)\n\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(15, 15))\n    plt.imshow(attn, interpolation='nearest', aspect='auto')\n    plt.ylabel('Target')\n    plt.xlabel('Source')\n    plt.savefig(path + '.png')\n    plt.close()\n", "CodeBleu_score": 48.624035615412886}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('x', color='red', fontsize=16)\n    ax.set_ylabel('y', color='red', fontsize=16)\n    ax.tick_params(axis='x', colors='red', labelsize=14)\n    ax.tick_params(axis='y', colors='red', labelsize=14)\n    ax.plot(array, color='blue', linewidth=2)\n    plt.show()\n\n", "CodeBleu_score": 35.41429061458958}, {"code": "def plot(array)\n    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10, 5))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel(\"Time (s)\", color=\"white\", fontsize=16)\n    ax.set_ylabel(\"Voltage (V)\", color=\"white\", fontsize=16)\n    ax.tick_params(axis=\"x\", colors=\"white\", labelsize=12)\n    ax.tick_params(axis=\"y\", colors=\"white\", labelsize=12)\n    ax.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 36.29448960366615}, {"code": "def plot(array)\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(12, 8))\n    ax = plt.subplot(111)\n    ax.set_xlabel('x', color='#1C1C1C', fontsize=18)\n    ax.set_ylabel('y', color='#1C1C1C', fontsize=18)\n    ax.tick_params(axis='x', colors='#1C1C1C', labelsize=14)\n    ax.tick_params(axis='y', colors='#1C1C1C', labelsize=14)\n    plt.plot(array)\n    plt.show()\n", "CodeBleu_score": 31.082734816516457}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n    import os\n    import csv\n    from os.path import isfile\n\n    # Create a dictionary with the data to write to the CSV file\n    data = {'Image': image_name, 'Prediction': prediction, 'Confidence': confidence}\n\n    # Create the CSV file if it doesn't exist\n    if not isfile('predictions.csv'):\n        with open('predictions.csv', 'w') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n            writer.writeheader()\n            writer.writerow(data)\n    else:\n        with open('predictions.csv', 'a') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n            writer.writerow(data)\n\n", "CodeBleu_score": 46.80208842379645}, {"code": "def write_to_csv(image_name, prediction, confidence)\n\n    import csv\n    from os.path import isfile\n\n    # Create a dictionary with the data to be written\n    data = {'image_name': image_name, 'prediction': prediction, 'confidence': confidence}\n\n    # Open the file in append mode\n    with open('predictions.csv', 'a', newline='') as csvfile:\n        # Create a CSV writer object\n        writer = csv.writer(csvfile, delimiter=',')\n\n        # If the file is new, write the header\n        if not isfile('predictions.csv'):\n            writer.writerow(data.keys())\n\n        # Write the data as a new row\n        writer.writerow(data.values())\n\n    return", "CodeBleu_score": 46.22234892974135}, {"code": "def write_to_csv(image_name, prediction, confidence)\n    import csv\n    import os\n\n    # Create a dictionary with the image name, prediction, and confidence\n    data = {\n        \"image\": image_name,\n        \"prediction\": prediction,\n        \"confidence\": confidence,\n    }\n\n    # Open the CSV file in append mode\n    csv_file_path = os.path.join(os.path.dirname(__file__), \"..\", \"data\", \"predictions.csv\")\n    with open(csv_file_path, \"a\") as csv_file:\n        # Create the CSV writer object\n        csv_writer = csv.DictWriter(csv_file, fieldnames=data.keys())\n\n        # If the file is new, write the header\n        if os.stat(csv_file_path).st_size == 0:\n            csv_writer.writeheader()\n\n        # Write the data as a new row\n        csv_writer.writerow(data)", "CodeBleu_score": 48.20643966022345}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    try:\n        from torch.jit import TracedModule, ScriptModule, trace\n    except ImportError:\n        print(colorstr(\"TorchScript not found. Please run: $ pip install torch\"))\n        return\n    try:\n        import torchvision\n    except ImportError:\n        print(colorstr(\"torchvision not found. Please run: $ pip install torchvision\"))\n        return\n\n    # YOLOv5 TorchScript model export\n    file = Path(file).with_suffix('.torchscript.pt')\n    prefix = colorstr(prefix)\n    print(f'{prefix} starting export with torch {torch.__version__}...')\n    t = time.time()\n\n    # Input\n    im = torch.zeros(1, 3, *im.shape[1:])  # image size(1,3,320,192) iDetection\n\n    # Model\n    model.model[-1].export = True  # set Detect() layer export=True\n    y = model(im)  # dry run\n\n    # Create ScriptModule\n    if optimize:\n        print(f'{prefix} saving optimized model for mobile in {file}...')\n        script = torch.jit.trace(model, im, check_trace=False)  # FIXME\n        script.save(file)\n    else:\n        print(f'{prefix} saving unoptimized model in {file}...')\n        traced_script = trace(model, im)\n        torch.jit.save(traced_script, file)\n\n    # Checks\n    print(f'{prefix} export success, saved as {file} ({time.time() - t:.1f}s)')\n    assert type(script) is TracedModule, 'TracedModule export failed'\n    if optimize:\n        assert type(script) is ScriptModule, 'ScriptModule export failed'\n\n    # TorchScript model card\n    # model_info(script, file)  # FIXME\n\n    # Return\n    return file, None", "CodeBleu_score": 41.70141156085221}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    try:\n        import torch\n    except ImportError:\n        raise ImportError('TorchScript export requires PyTorch. Install with `pip install torch`.')\n\n    print(f'{prefix} starting export with torch {torch.__version__}...')\n    f = str(file).replace('.pt', '')  # remove extension\n    jit_model = torch.jit.trace(model, im, check_trace=False)\n    jit_model = torch.jit.optimize_for_inference(jit_model) if optimize else jit_model\n    jit_model.save(f + '.pt')\n\n    # save model details\n    m = model.module if hasattr(model, 'module') else model\n    m.model_name = m.model_name if hasattr(m, 'model_name') else m.__class__.__name__\n    m.yaml = m.yaml if hasattr(m, 'yaml') else ''\n    m.version = m.version if hasattr(m, 'version') else '0.0'\n    m.git = m.git if hasattr(m, 'git') else ''\n    m.device = m.device if hasattr(m, 'device') else ''\n    m.timm_version = m.timm_version if hasattr(m, 'timm_version') else ''\n    m.description = m.description if hasattr(m, 'description') else ''\n    m.classes = m.classes if hasattr(m, 'classes') else None\n    m.channels = m.channels if hasattr(m, 'channels') else None\n    m.data = m.data if hasattr(m, 'data') else None\n    m.layers = m.layers if hasattr(m, 'layers') else None\n    m.stride = m.stride if hasattr(m, 'stride') else None\n    m.names = m.names if hasattr(m, 'names') else None\n    m.training = m.training if hasattr(m, 'training') else False\n    m.half = m.half if hasattr(m, 'half') else False\n    m.", "CodeBleu_score": 41.143133235927834}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    try:\n        import torch\n    except ImportError:\n        print(colorstr(\"TorchScript export requires PyTorch.\"))\n        return\n    try:\n        import torchvision\n    except ImportError:\n        print(colorstr(\"TorchScript export requires torchvision.\"))\n        return\n    try:\n        import torch.utils.mobile_optimizer as mobile_optimizer\n    except ImportError:\n        print(colorstr(\"TorchScript export requires torch.utils.mobile_optimizer.\"))\n        return\n\n    print(f\"{prefix} starting export with torch {torch.__version__}...\")\n    f = str(file).replace(\".pt\", \"\") + \".torchscript.pt\"\n    ts = torch.jit.trace(model, im)\n    extra_files = {\n        \"model.yaml\": model.yaml(),\n        \"ncnn_torchscript.py\": open(__file__).read(),\n    }\n    if optimize:\n        print(f\"{prefix} saving optimized model\")\n        ts = mobile_optimizer.optimize_for_mobile(ts)\n    else:\n        print(f\"{prefix} saving standard model\")\n    ts.save(f, _extra_files=extra_files)\n    print(f\"{prefix} export success, saved as {f} ({file_size(f):.1f} MB)\")\n    return f, None", "CodeBleu_score": 48.33951590536261}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError('CoreML export requires coremltools. Install by `pip install coremltools`.')\n\n    suffix = '.mlmodel'\n    file = Path(file).with_suffix(suffix)\n    LOGGER.info(f'{prefix} starting export with torch {torch.__version__}...')\n    # torch.onnx.export(model, im, file, verbose=False, opset_version=11, input_names=['images'], output_names=['output'])\n    # model = onnx.load(file)\n    # img = np.zeros((1, 3, *im.shape), np.float32)  # input image\n    # onnx.checker.check_model(model)  # check onnx model\n    # ct_model = ct.convert(\n    #     model,\n    #     inputs=[ct.ImageType(name='images', shape=img.shape, scale=1 / 255.0, bias=[0, 0, 0], color_layout='RGB')])\n    # ct_model.save(file)\n    # LOGGER.info(f'{prefix} export success, saved as {file} ({file.stat().st_size / 1E6} MB)')\n    # return file, ct_model\n\n    # Tracing\n    LOGGER.info(f'{prefix} tracing with torch {torch.__version__}...')\n    model.eval()\n    img = torch.zeros((1, 3, *im.shape), device=next(model.parameters()).device)  # input image\n    _ = model(img)  # dry run\n    traced_model = torch.jit.trace(model, img)\n\n    # Conversion to CoreML\n    LOGGER.info(f'{prefix} converting to CoreML with torch {torch.__version__}...')\n    model = ct.convert(traced_model, inputs=[ct.ImageType(shape=img.shape, scale=1 / 255.", "CodeBleu_score": 33.27491932084533}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(f\"{prefix} coremltools is not installed. Run `pip install coremltools`.\")\n    if not file.endswith('.mlmodel'):\n        file = file + '.mlmodel'\n    print(f\"{prefix} exporting to {file}...\")\n    t = time.time()\n    model.eval()\n    if half:\n        model.half()\n    img = torch.zeros((1, 3, *im), device=next(model.parameters()).device)  # image size(1,3,320,192) ipad\n    _ = model(img)  # dry run\n    if int8:\n        print(f\"{prefix} quantizing to INT8...\")\n        model = convert(model, img, int8=True)\n    if nms:\n        model = iOSModel(model, nms=nms)\n    torch.onnx.export(model, img, file, verbose=False, opset_version=11, input_names=['image'],\n                      output_names=['output'], dynamic_axes={'image': {0: 'batch'}, 'output': {0: 'batch'}})\n    model = ct.convert(file, minimum_deployment_target='13')\n    if int8:\n        model.save(file)\n    else:\n        model.save(file, include_neural_network_specification_upgrade_specification=False)\n    print(f\"{prefix} export success, saved as {file}, took {time.time() - t:.1f}s\")\n    return file, model\n\n", "CodeBleu_score": 40.94652162233824}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(f'CoreML export requires coremltools. Install with `pip install coremltools`.')\n\n    if isinstance(model, Model):\n        model = model.module\n\n    stride, names = model.stride, model.names\n    if half:\n        im = im.half()  # for export of FP16 models\n    map = model(im)  # get model outputs\n\n    if isinstance(map, Detect):\n        map = map.forward(return_loss=False, augment=False, visualize=False)  # inference\n        if isinstance(map, list):\n            map, _ = map  # only extract detections\n\n    class Detections:\n        def __init__(self, predictions, names):\n            self.names = names\n            self.predictions = predictions\n\n        def __iter__(self):\n            x = [next(iter(x)) for x in self.predictions]  # detections per image\n            x = sum(x, [])  # flatten\n            for p in x:\n                p.name = self.names[int(p.class_id)]  # assign class name\n                yield p\n\n    class Detection:\n        def __init__(self, xyxy, conf, cls_conf, cls_pred):\n            self.xyxy = xyxy\n            self.conf = conf\n            self.cls_conf = cls_conf\n            self.cls_pred = cls_pred\n\n    y = ct.TensorType(shape=(1, 3, stride[0], stride[0]), dtype='image',\n                      quantization_range=(0, 255), quantization_scale=1 / 255)\n    x = ct.TensorType(shape=(1, 3, stride[1], stride[1]), dtype='image',\n                      quantization_range=(0, 255), quantization_scale=1 / 255)\n\n    @ct.register_shape_calculator(Detections)\n    def shape_", "CodeBleu_score": 35.36470989234148}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length)\n    individual = []\n    for _ in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n\n", "CodeBleu_score": 39.41234669340793}, {"code": "def generate_individual(input_ranges, individual_length)\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}, {"code": "def generate_individual(input_ranges, individual_length)\n\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n\n", "CodeBleu_score": 47.630972577458174}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == 'discrete':\n        return self.marginal_log_alpha(t)\n    elif self.schedule_type == 'linear':\n        return self.marginal_log_alpha(t) + (self.marginal_log_alpha(t + 1) - self.marginal_log_alpha(t)) / self.delta\n    elif self.schedule_type == 'cosine':\n        return self.marginal_log_alpha(t) + np.log(1 + np.cos(np.pi * t / self.T)) / 2\n    else:\n        raise Exception(\"Schedule type not recognized.\")\n\n", "CodeBleu_score": 30.956660538422692}, {"code": "def marginal_log_mean_coeff(self, t)\n\n    if self.schedule == 'discrete':\n        return np.log(self.alpha_t[np.argmin(np.abs(self.alpha_t - t))])\n    elif self.schedule == 'linear':\n        return np.log(self.alpha_t[0] + (t - self.alpha_t[0]) / (self.alpha_t[-1] - self.alpha_t[0]) * (self.alpha_t[-1] - self.alpha_t[0]))\n    elif self.schedule == 'cosine':\n        return np.log(self.alpha_t[0] + 0.5 * (1 - np.cos(np.pi * t / self.alpha_t[-1])) * (self.alpha_t[-1] - self.alpha_t[0]))\n    else:\n        raise ValueError(\"Schedule type not recognized. Please use 'discrete', 'linear', or 'cosine'.\")", "CodeBleu_score": 36.381417112124986}, {"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == 'discrete':\n        return self.interpolate(t)\n    elif self.schedule_type == 'linear':\n        return self.linear_interpolation(t)\n    elif self.schedule_type == 'cosine':\n        return self.cosine_interpolation(t)\n    else:\n        raise ValueError(\"schedule_type should be 'discrete', 'linear', or 'cosine'\")\n\n", "CodeBleu_score": 24.2167268314564}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n    if self.schedule == \"linear\":\n        # Compute t using beta values and a temporary variable\n        beta_t = self.beta_t(lamb)\n        beta_t_plus_one = self.beta_t(lamb + 1)\n        temp = (\n            (1 / beta_t_plus_one)\n            * (\n                (1 / beta_t)\n                * (\n                    (1 / beta_t)\n                    * (self.beta_t_plus_one(self.T) - self.beta_t(self.T))\n                    + self.beta_t_plus_one(self.T)\n                )\n                - 1\n            )\n            + 1\n        )\n        return temp\n    elif self.schedule == \"discrete\":\n        # Interpolate log_alpha to find t\n        return self.interpolate_log_alpha(lamb)\n    else:\n        # Compute log_alpha and use an arccos function to derive t\n        log_alpha = self.log_alpha(lamb)\n        return np.arccos(np.cos(log_alpha)) / np.pi", "CodeBleu_score": 24.461639521765253}, {"code": "def inverse_lambda(self, lamb)\n\n    if self.schedule == 'linear':\n        if self.beta_0 == 0:\n            return lamb / self.beta_1\n        else:\n            temp = (self.beta_1 * lamb) / (self.beta_0 + self.beta_1)\n            return temp / (1 - temp)\n\n    elif self.schedule == 'discrete':\n        return self.t_list[np.argmin(np.abs(self.lamb_list - lamb))]\n\n    elif self.schedule == 'arccos':\n        log_alpha = 2 * np.arccos(np.sqrt(1 - lamb)) / np.pi\n        return self.t_list[np.argmin(np.abs(self.log_alpha_list - log_alpha))]\n\n    elif self.schedule == 'logarithmic':\n        return lamb / self.beta_1\n\n    else:\n        raise ValueError('Schedule type not recognized.')", "CodeBleu_score": 24.716695365262925}, {"code": "def inverse_lambda(self, lamb)\n    if self.schedule == \"linear\":\n        # compute t\n        t = (self.beta_1 + self.beta_2 * lamb) / (1 - self.beta_2)\n        return t\n    elif self.schedule == \"discrete\":\n        # interpolate log_alpha\n        log_alpha = np.interp(lamb, self.lamb_list, self.log_alpha_list)\n        # compute t\n        t = np.arccos(-np.exp(log_alpha)) / np.pi\n        return t\n    else:\n        # compute log_alpha\n        log_alpha = self.log_alpha_list[np.argmin(np.abs(self.lamb_list - lamb))]\n        # compute t\n        t = np.arccos(-np.exp(log_alpha)) / np.pi\n        return t", "CodeBleu_score": 23.661880282131886}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    if cond is not None:\n        if cond.shape[0] == 1:\n            cond = cond.repeat(x.shape[0], 1, 1)\n        else:\n            cond = cond.repeat(x.shape[0], 1)\n    if t_continuous.shape[0] == 1:\n        t_continuous = t_continuous.repeat(x.shape[0], 1)\n    if x.shape[0] == 1:\n        x = x.repeat(t_continuous.shape[0], 1, 1, 1)\n    elif t_continuous.shape[0] == 1:\n        t_continuous = t_continuous.repeat(x.shape[0], 1)\n    else:\n        t_continuous = t_continuous.repeat(x.shape[0], 1)\n    return x + t_continuous", "CodeBleu_score": 16.343752473143358}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    # expand time input to match the batch size\n    t_expanded = t_continuous[:, None]\n\n    # process the input through the model\n    if cond is None:\n        # if no conditioning is used, the input is simply the time\n        x_processed = x(t_expanded)\n    else:\n        # if conditioning is used, the input is the time and the conditioning\n        x_processed = x(t_expanded, cond)\n\n    # depending on the model type, apply different transformations to the model output\n    if x.model_type == \"nf\":\n        # for normalizing flow models, the output is the noise\n        noise = x_processed\n\n    elif x.model_type == \"nf_cond\":\n        # for normalizing flow models with conditioning, the output is the noise\n        noise = x_processed\n\n    elif x.model_type == \"nf_cond_scale\":\n        # for normalizing flow models with conditioning and scale, the output is the noise\n        noise = x_processed\n\n    elif x.model_type == \"nf_cond_scale_shift\":\n        # for normalizing flow models with conditioning, scale, and shift, the output is the noise\n        noise = x_processed\n\n    elif x.model_type == \"nf_cond_scale_shift_dropout\":\n        # for normalizing flow models with conditioning, scale, shift, and dropout, the output is the noise\n        noise = x_processed\n\n    elif x.model_type == \"nf_cond_scale_shift_dropout_learn_top\":\n        # for normalizing flow models with conditioning, scale, shift, dropout, and learned top, the output is the noise\n        noise = x_processed\n\n    elif x.model_type == \"nf_cond_scale_shift_dropout_learn_top_bottom\":\n        # for normalizing flow models with conditioning, scale, shift, dropout, and learned top and bottom, the output is the noise\n        noise = x_processed\n\n    elif x.model_type == \"nf_cond_scale_shift_dropout_learn_top_", "CodeBleu_score": 20.438487713102525}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    if cond is None:\n        cond = x\n    if len(t_continuous.shape) == 1:\n        t_continuous = t_continuous.unsqueeze(0).expand(x.shape[0], -1)\n    if len(cond.shape) == 2:\n        cond = cond.unsqueeze(1).expand(-1, x.shape[1], -1)\n    out = model(x, t_continuous, cond)\n    if model_type == \"classic\":\n        out = out.squeeze(1)\n        out = out * noise_schedule_params[\"pred_std\"] + noise_schedule_params[\"pred_mean\"]\n    elif model_type == \"residual\":\n        out = out.squeeze(1)\n        out = out * noise_schedule_params[\"pred_std\"] + x\n    elif model_type == \"residual_w_time\":\n        out = out.squeeze(1)\n        out = out * noise_schedule_params[\"pred_std\"] + x\n    return out\n\n", "CodeBleu_score": 30.453561768960835}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n\n    # Reshape the input tensor to a 2D tensor\n    x0_reshaped = tf.reshape(x0, [x0.shape[0], -1])\n\n    # Compute the quantile of absolute values\n    x0_reshaped_abs = tf.abs(x0_reshaped)\n    x0_quantile = tf.quantile(x0_reshaped_abs, 0.99, axis=1)\n\n    # Determine the scaling factor\n    x0_scaling_factor = x0_quantile / 255.0\n\n    # Clamp the values of the input tensor\n    x0_clamped = tf.clip_by_value(x0_reshaped, -x0_scaling_factor, x0_scaling_factor)\n\n    # Scale the values of the input tensor\n    x0_scaled = x0_clamped / x0_scaling_factor\n\n    # Reshape the scaled tensor\n    x0_scaled_reshaped = tf.reshape(x0_scaled, x0.shape)\n\n    return x0_scaled_reshaped\n\n", "CodeBleu_score": 23.775007254668797}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    import torch\n    import numpy as np\n\n    if t is None:\n        t = self.threshold\n\n    x0_shape = x0.shape\n    x0_flat = x0.view(-1)\n    x0_abs = torch.abs(x0_flat)\n\n    # Determine the quantile of absolute values\n    x0_abs_sorted, _ = torch.sort(x0_abs)\n    q = int(np.ceil(x0_abs_sorted.shape[0] * t))\n    threshold = x0_abs_sorted[q - 1]\n\n    # Determine the scaling factor\n    x0_scaled = x0_flat / (threshold + 1e-8)\n    x0_scaled_clamped = torch.clamp(x0_scaled, -1, 1)\n    x0_scaled_clamped_abs = torch.abs(x0_scaled_clamped)\n    x0_scaled_clamped_abs_sorted, _ = torch.sort(x0_scaled_clamped_abs)\n    q = int(np.ceil(x0_scaled_clamped_abs_sorted.shape[0] * t))\n    scaling_factor = x0_scaled_clamped_abs_sorted[q - 1]\n\n    # Clamp the input tensor based on the scaling factor\n    x0_scaled = x0_flat / (scaling_factor + 1e-8)\n    x0_scaled_clamped = torch.clamp(x0_scaled, -1, 1)\n    x0_scaled_clamped = x0_scaled_clamped.view(x0_shape)\n\n    return x0_scaled_clamped", "CodeBleu_score": 31.539802195703547}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    # Reshape x0 to compute the quantile of absolute values\n    x0_reshaped = x0.reshape(-1)\n    # Determine the scaling factor\n    k = np.quantile(np.abs(x0_reshaped), t)\n    # Clamp x0 values based on the scaling factor\n    x0_clamped = np.clip(x0, -k, k)\n    # Scale x0_clamped\n    x0_scaled = x0_clamped / k\n    return x0_scaled\n\n", "CodeBleu_score": 10.438888626648565}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n    # Calculate the threshold value\n    threshold = self.max_threshold * self.dynamic_threshold_ratio\n    # Clamp the input tensor\n    x_clamped = torch.clamp(x, threshold, torch.max(x))\n    # Normalize the clamped tensor\n    x_normalized = x_clamped / torch.max(x_clamped)\n    return x_normalized\n\n", "CodeBleu_score": 12.77080650769134}, {"code": "def data_prediction_fn(self, x, t)\n    x = x.clone()\n    x = x.view(x.shape[0], -1)\n    x = x.to(torch.float32)\n    x_max = x.max(dim=1, keepdim=True)[0]\n    x_max[x_max == 0] = 1\n    x = x / x_max\n    x_max = x_max.view(-1, 1, 1, 1)\n    x_max = x_max.to(torch.float32)\n    x_max = x_max.to(self.device)\n    x_min = x.min(dim=1, keepdim=True)[0]\n    x_min[x_min == 0] = 1\n    x = x / x_min\n    x_min = x_min.view(-1, 1, 1, 1)\n    x_min = x_min.to(torch.float32)\n    x_min = x_min.to(self.device)\n    x_mean = x.mean(dim=1, keepdim=True)\n    x_mean = x_mean.to(torch.float32)\n    x_mean = x_mean.to(self.device)\n    x_std = x.std(dim=1, keepdim=True)\n    x_std = x_std.to(torch.float32)\n    x_std = x_std.to(self.device)\n    x = (x - x_mean) / x_std\n    x_max = x_max.to(torch.float32)\n    x_max = x_max.to(self.device)\n    x_min = x_min.to(torch.float32)\n    x_min = x_min.to(self.device)\n    x_max = x_max.view(x_max.shape[0], 1, 1, 1)\n    x_min = x_min.view(x_min.shape[0], 1, 1, 1)\n    x_max = x_max.repeat(1", "CodeBleu_score": 24.05525312655778}, {"code": "def data_prediction_fn(self, x, t)\n    x0 = x\n    x1 = self.threshold_fn(x0, t)\n    x2 = self.normalize_fn(x1)\n    return x2\n\n", "CodeBleu_score": 2.386783878141885}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    import numpy as np\n    import torch\n    from torch import nn\n    from torch.nn import functional as F\n    from torch.autograd import Variable\n\n    if isinstance(input, np.ndarray):\n        input = torch.from_numpy(input)\n    if isinstance(sigma_in, np.ndarray):\n        sigma_in = torch.from_numpy(sigma_in)\n    if isinstance(sigma_in, float):\n        sigma_in = torch.tensor(sigma_in)\n\n    if isinstance(input, torch.Tensor):\n        input = input.to(model.device)\n    if isinstance(sigma_in, torch.Tensor):\n        sigma_in = sigma_in.to(model.device)\n\n    if len(sigma_in.shape) == 0:\n        sigma_in = sigma_in.unsqueeze(0)\n\n    if len(sigma_in.shape) == 1:\n        sigma_in = sigma_in.unsqueeze(1)\n\n    if len(sigma_in.shape) == 2:\n        sigma_in = sigma_in.unsqueeze(2)\n\n    if len(sigma_in.shape) == 3:\n        sigma_in = sigma_in.unsqueeze(3)\n\n    if len(sigma_in.shape) == 4:\n        sigma_in = sigma_in.unsqueeze(4)\n\n    if len(sigma_in.shape) == 5:\n        sigma_in = sigma_in.unsqueeze(5)\n\n    if len(sigma_in.shape) == 6:\n        sigma_in = sigma_in.unsqueeze(6)\n\n    if len(sigma_in.shape) == 7:\n        sigma_in = sigma_in.unsqueeze(7)\n\n    if len(sigma_in.shape) == 8:\n        sigma_in = sigma_in.unsqueeze(8)\n\n    if len(sigma", "CodeBleu_score": 27.778326624552175}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    # reshape sigma to match the dimensions of the input\n    sigma_in = sigma_in.reshape((input.shape[0], 1, 1, 1))\n    # scale the input\n    input = input / sigma_in\n    # compute the prediction\n    prediction = model(input, **kwargs)\n    # adjust the prediction with the scaled input and sigma\n    prediction = prediction * sigma_in + input\n    return prediction\n\n", "CodeBleu_score": 26.61558881259922}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    sigma_in = sigma_in.reshape((-1,1))\n    scaled_input = input / sigma_in\n    pred = model.predict(scaled_input, **kwargs)\n    scaled_pred = pred * sigma_in\n    return scaled_pred\n", "CodeBleu_score": 20.34503190825739}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n    import torch\n    from torch.autograd import Variable\n    import numpy as np\n    import math\n    # clone taus, set gradients to zero, and calculate alpha\n    taus_clone = taus.clone()\n    taus_clone.requires_grad = True\n    alpha = torch.exp(torch.log(taus_clone) + beta_0 + beta_1*taus_clone)\n    # calculate log_alpha\n    log_alpha = torch.log(alpha)\n    # calculate gradient of log_alpha with respect to taus\n    grad_log_alpha = torch.autograd.grad(outputs=log_alpha, inputs=taus_clone, grad_outputs=torch.ones(log_alpha.size()).cuda(), create_graph=True, retain_graph=True, only_inputs=True)[0]\n    # calculate integrand\n    integrand = grad_log_alpha * alpha\n    return integrand\n\n", "CodeBleu_score": 38.097563246258034}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n    import torch\n    import numpy as np\n    import math\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import matplotlib.pyplot as plt\n    import matplotlib.animation as animation\n    from torch.autograd import grad\n    import torch.autograd as autograd\n\n    # Set up the gradient-enabled context\n    taus.requires_grad = True\n\n    # Calculate alpha and its logarithm\n    alpha = torch.exp(beta_0) + torch.exp(beta_1)*taus\n    log_alpha = torch.log(alpha)\n\n    # Calculate the gradient of log_alpha with respect to taus\n    grad_log_alpha = grad(log_alpha, taus, create_graph=True)[0]\n\n    # Calculate the integrand\n    integrand = torch.exp(beta_0)*torch.exp(beta_1)*torch.exp(log_alpha)\n    integrand = integrand*(grad_log_alpha)\n\n    return integrand\n", "CodeBleu_score": 27.719831256308414}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n    import torch\n    from torch.autograd import Variable\n    import numpy as np\n    from scipy.special import gamma, gammainc\n    from scipy.integrate import quad\n\n    # Clone taus and set their gradients to 1\n    taus_clone = taus.clone()\n    taus_clone.requires_grad = True\n    taus_clone.grad = torch.ones(taus_clone.shape)\n\n    # Calculate alpha and its logarithm\n    alpha = torch.exp(beta_0 + beta_1*taus_clone)\n    log_alpha = torch.log(alpha)\n\n    # Perform backpropagation to obtain the gradient of log_alpha with respect to taus\n    log_alpha.backward(torch.ones(log_alpha.shape))\n\n    # Calculate the integrand using this gradient and alpha values\n    integrand = torch.exp(-alpha) * log_alpha.grad * alpha\n\n    return integrand\n\n", "CodeBleu_score": 38.637521617455164}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n\n    # Initialize extra arguments\n    if extra_args is None:\n        extra_args = {}\n\n    # Initialize noise sampler\n    if noise_sampler is None:\n        noise_sampler = lambda x, sigma: x + sigma * torch.randn_like(x)\n\n    # Iterate over sigmas\n    for i, sigma in enumerate(sigmas):\n        # Denoise\n        x = model.denoise(x, sigma=sigma, **extra_args)\n\n        # Calculate ancestral step\n        dx = model.ancestral_sample(x, sigma=sigma, **extra_args)\n\n        # Update input\n        x = x + eta * dx\n\n        # Call callback\n        if callback is not None:\n            callback(x, i)\n\n    # Add noise if required\n    if s_noise > 0:\n        x = noise_sampler(x, s_noise)\n\n    # Return sampled output\n    return x\n\n", "CodeBleu_score": 29.093818507006457}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda x: x.new_zeros(x.shape)\n    x = x.detach()\n    if disable is None:\n        disable = [False] * len(sigmas)\n    for i, sigma in enumerate(sigmas):\n        if disable[i]:\n            continue\n        x = model.denoise(x, sigma=sigma, **extra_args)\n        step = model.ancestral_sample(x, sigma=sigma, **extra_args)\n        x = x + eta * step\n        if callback is not None:\n            callback(x, i)\n    if s_noise > 0:\n        x = x + s_noise * noise_sampler(x)\n    return x\n\n", "CodeBleu_score": 37.17753234629807}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda shape: torch.randn(shape)\n    if disable is None:\n        disable = []\n    for sigma in sigmas:\n        if callback is not None:\n            callback(x, sigma)\n        x = model.denoise(x, sigma, **extra_args)\n        if \"ancestral_step\" not in disable:\n            x = x + eta * model.ancestral_step(x, sigma, **extra_args)\n        if \"euler\" not in disable:\n            x = x + eta * model.euler_step(x, sigma, **extra_args)\n    if s_noise > 0:\n        x = x + s_noise * noise_sampler(x.shape)\n    return x\n", "CodeBleu_score": 31.541438383258207}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n    def product_term(x):\n        \"\"\"The function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        return x**(order-1)\n    if (i-j) > order:\n        raise ValueError(\"The order is too high for the step index.\")\n    elif i == j:\n        return 1\n    else:\n        return (t[i+1]-t[i])*integrate(product_term, t[i], t[i+1])\n", "CodeBleu_score": 23.850586528416663}, {"code": "def linear_multistep_coeff(order, t, i, j)\n    def product(k):\n        \"\"\"The function computes the product term for the linear multistep coefficient, excluding the j-th term.\"\"\"\n        if k == j:\n            return 1\n        else:\n            return t[i + k] - t[i + k - 1]\n\n    if order > i - j:\n        raise ValueError(\"Order too high for step index\")\n    else:\n        return sum([product(k) for k in range(j - i + order + 1)])\n", "CodeBleu_score": 30.170541992678324}, {"code": "def linear_multistep_coeff(order, t, i, j)\n    def product(k):\n        \"\"\"This function computes the product term for the coefficient, excluding the j-th term.\"\"\"\n        if k == j:\n            return 1\n        else:\n            return t[i+k]-t[j+k]\n    if i+order-j < 0:\n        raise ValueError(\"Order too high for step index.\")\n    return sum([product(k)*linear_multistep_coeff(order, t, i, j+1) for k in range(order+1)])\n\n", "CodeBleu_score": 28.850632272379922}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda x: x\n    if disable is None:\n        disable = []\n    for sigma in sigmas:\n        if callback is not None:\n            callback()\n        if sigma > 0:\n            x = noise_sampler(x)\n        x = model(x, **extra_args)\n        if \"denoise\" not in disable:\n            x = denoise(x)\n    return x\n", "CodeBleu_score": 27.108423234796287}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\n    # initialize extra arguments\n    if extra_args is None:\n        extra_args = {}\n\n    # initialize noise sampler\n    if noise_sampler is None:\n        noise_sampler = lambda: torch.randn_like(x)\n\n    # iterate over sigmas\n    for sigma in sigmas:\n        # apply model to input\n        x_hat = model(x, **extra_args)\n\n        # update input with denoised values\n        x = x + (x_hat - x)\n\n        # execute callback\n        if callback is not None:\n            callback(x, sigma, **extra_args)\n\n        # if next sigma is greater than zero, noise scaling is applied to the updated input\n        if sigma > 0 and (disable is None or not disable):\n            x = x + sigma * noise_sampler()\n\n    return x", "CodeBleu_score": 26.454858389855573}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n\n    if extra_args is None:\n        extra_args = {}\n\n    if noise_sampler is None:\n        noise_sampler = lambda shape: tf.random.normal(shape=shape)\n\n    for i, sigma in enumerate(sigmas):\n        x_hat = model(x, **extra_args)\n        x_hat = x_hat.numpy()\n\n        if disable is None:\n            x_hat = x_hat + sigma * noise_sampler(x_hat.shape)\n        else:\n            x_hat = x_hat + sigma * noise_sampler(x_hat.shape) * (1 - disable)\n\n        x = x_hat\n\n        if callback is not None:\n            callback(i, x)\n\n    return x", "CodeBleu_score": 33.50628061174833}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)\n    if x.dim() > target_dims:\n        raise ValueError(\n            f\"The target dimensions ({target_dims}) are fewer than the current dimensions ({x.dim()}).\"\n        )\n    elif x.dim() == target_dims:\n        return x\n    else:\n        if x.device == \"mps\":\n            return x.clone().detach()\n        else:\n            return x.expand(\n                *(\n                    [1 for _ in range(target_dims - x.dim())]\n                    + list(x.shape)\n                )\n            )", "CodeBleu_score": 26.841784063823056}, {"code": "def append_dims(x, target_dims)\n    if len(x.shape) > target_dims:\n        raise ValueError(\"The target dimensions must be greater than or equal to the current dimensions.\")\n    elif len(x.shape) == target_dims:\n        return x\n    else:\n        if x.device.type == 'mps':\n            return x.clone().detach().expand(target_dims * [1])\n        else:\n            return x.expand(target_dims * [1])", "CodeBleu_score": 26.034283184041445}, {"code": "def append_dims(x, target_dims)\n    if len(x.shape) < target_dims:\n        raise ValueError(f'The target dimensions ({target_dims}) must be greater than or equal to the current dimensions ({len(x.shape)}).')\n    elif len(x.shape) == target_dims:\n        return x\n    else:\n        if x.device.type == 'mps':\n            return x.detach().clone()\n        else:\n            return x.expand(x.shape + (1,)*(target_dims - len(x.shape)))\n", "CodeBleu_score": 26.196938483133174}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n    import os\n    import hashlib\n    import requests\n    import tqdm\n\n    if os.path.exists(path):\n        print(\"File already exists.\")\n    else:\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        r = requests.get(url, stream=True)\n        total_size = int(r.headers.get('content-length', 0))\n        block_size = 1024\n        t = tqdm.tqdm(total=total_size, unit='iB', unit_scale=True)\n        with open(path, 'wb') as f:\n            for data in r.iter_content(block_size):\n                t.update(len(data))\n                f.write(data)\n        t.close()\n        if total_size != 0 and t.n != total_size:\n            raise Exception(\"Download interrupted.\")\n    if digest is not None:\n        with open(path, 'rb') as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n            if file_hash != digest:\n                raise Exception(\"Hash mismatch.\")\n    return path", "CodeBleu_score": 42.9387399297493}, {"code": "def download_file(path, url, digest=None)\n    import os\n    import urllib.request\n    import hashlib\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))\n\n    if not os.path.exists(path):\n        logger.info('Downloading {} to {}'.format(url, path))\n        urllib.request.urlretrieve(url, path)\n    else:\n        logger.info('File {} already exists, skipping download'.format(path))\n\n    if digest:\n        logger.info('Verifying hash of {}'.format(path))\n        with open(path, 'rb') as f:\n            hash = hashlib.sha256(f.read()).hexdigest()\n        if hash != digest:\n            raise ValueError('Digest mismatch for file {}'.format(path))\n\n    return path\n\n", "CodeBleu_score": 41.093347578215166}, {"code": "def download_file(path, url, digest=None)\n    import os\n    import urllib.request\n    import hashlib\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    filename = os.path.join(path, url.split('/')[-1])\n\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n    if digest is not None:\n        with open(filename, 'rb') as f:\n            sha256 = hashlib.sha256(f.read()).hexdigest()\n            if sha256 != digest:\n                raise ValueError('File hash mismatch: expected %s, got %s' % (digest, sha256))\n\n    return filename\n\n", "CodeBleu_score": 47.46054967023759}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n    if mode:\n        return model.train()\n    else:\n        return model.eval()\n", "CodeBleu_score": 13.777709534837959}, {"code": "def train_mode(model, mode=True)\n    if mode:\n        model.train()\n    else:\n        model.eval()\n    return model\n\n", "CodeBleu_score": 14.524436844388436}, {"code": "def train_mode(model, mode=True)\n    if mode:\n        return model.train()\n    else:\n        return model.eval()\n\n", "CodeBleu_score": 13.777709534837959}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n\n    for averaged_param, param in zip(averaged_model.parameters(), model.parameters()):\n        averaged_param.data.mul_(decay).add_(param.data, alpha=1-decay)\n\n    for averaged_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        averaged_buffer.data.copy_(buffer.data)", "CodeBleu_score": 28.74231113735321}, {"code": "def ema_update(model, averaged_model, decay)\n    if not isinstance(decay, float) or decay < 0.0 or decay > 1.0:\n        raise ValueError(\"Decay must be a float between 0 and 1\")\n    if len(list(averaged_model.parameters())) != len(list(model.parameters())):\n        raise ValueError(\n            \"Averaged model must have the same number of parameters as the model\"\n        )\n\n    averaged_model.train()\n    model.train()\n    for averaged_param, param in zip(\n        averaged_model.parameters(), model.parameters()\n    ):\n        averaged_param.data.mul_(decay).add_(1 - decay, param.data)\n    for averaged_buffer, buffer in zip(\n        averaged_model.buffers(), model.buffers()\n    ):\n        if averaged_buffer is not None:\n            averaged_buffer.data.copy_(buffer.data)\n\n", "CodeBleu_score": 37.49669093530892}, {"code": "def ema_update(model, averaged_model, decay)\n\n    for ema_param, param in zip(averaged_model.parameters(), model.parameters()):\n        ema_param.data = decay * ema_param.data + (1 - decay) * param.data\n\n    for ema_buffer, buffer in zip(averaged_model.buffers(), model.buffers()):\n        ema_buffer.data.copy_(buffer.data)\n\n    return averaged_model\n\n", "CodeBleu_score": 28.52849723774249}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    min_value, max_value = torch.tensor(min_value, device=device, dtype=dtype), torch.tensor(max_value, device=device, dtype=dtype)\n    min_value_cdf = torch.log(min_value / loc + 1) / scale\n    max_value_cdf = torch.log(max_value / loc + 1) / scale\n    u = u * (max_value_cdf - min_value_cdf) + min_value_cdf\n    return torch.exp(torch.log(u) * scale + loc)\n\n", "CodeBleu_score": 34.65852313509601}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    u = torch.rand(shape, device=device, dtype=dtype)\n    cdf_min = torch.log(min_value / scale) - loc\n    cdf_max = torch.log(max_value / scale) - loc\n    cdf_values = cdf_min + u * (cdf_max - cdf_min)\n    samples = torch.exp(torch.log(cdf_values) + loc) * scale\n    return samples\n\n", "CodeBleu_score": 38.52287045722428}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    if torch.is_tensor(min_value):\n        min_value = min_value.to(device)\n    if torch.is_tensor(max_value):\n        max_value = max_value.to(device)\n    if torch.is_tensor(loc):\n        loc = loc.to(device)\n    if torch.is_tensor(scale):\n        scale = scale.to(device)\n    if torch.is_tensor(shape):\n        shape = shape.to(device)\n    if torch.is_tensor(device):\n        device = device.to(device)\n    if torch.is_tensor(dtype):\n        dtype = dtype.to(device)\n    if torch.is_tensor(shape):\n        shape = shape.to(device)\n    if torch.is_tensor(device):\n        device = device.to(device)\n    if torch.is_tensor(dtype):\n        dtype = dtype.to(device)\n    if torch.is_tensor(shape):\n        shape = shape.to(device)\n    if torch.is_tensor(device):\n        device = device.to(device)\n    if torch.is_tensor(dtype):\n        dtype = dtype.to(device)\n    if torch.is_tensor(shape):\n        shape = shape.to(device)\n    if torch.is_tensor(device):\n        device = device.to(device)\n    if torch.is_tensor(dtype):\n        dtype = dtype.to(device)\n    if torch.is_tensor(shape):\n        shape = shape.to(device)\n    if torch.is_tensor(device):\n        device = device.to(device)\n    if torch.is_tensor(dtype):\n        dtype = dtype.to(device)\n    if torch.is_tensor(shape):\n        shape = shape.to(device)\n    if torch.is_tensor(device):\n        device = device.to(device)\n    if torch.is_tensor(dtype):\n        dtype = dtype.to(device)\n    if torch.is_tensor(", "CodeBleu_score": 20.055898333420263}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        if antialias:\n            return Activation1d(nn.ELU(inplace=True))\n        else:\n            return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation type: {activation}\")\n\n", "CodeBleu_score": 44.59349262531967}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        if antialias:\n            return Activation1d(nn.ELU(inplace=True))\n        else:\n            return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        if antialias:\n            return Activation1d(SnakeBeta(channels))\n        else:\n            return SnakeBeta(channels)\n    elif activation == \"none\":\n        if antialias:\n            return Activation1d(nn.Identity())\n        else:\n            return nn.Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} not supported.\")\n\n", "CodeBleu_score": 39.268745133738875}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        return SnakeBeta(channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation type {activation}\")\n\n", "CodeBleu_score": 42.853604666253815}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    if t.dtype == torch.float16:\n        t = t.float()\n    freqs = freqs.float()\n    n, l, d = t.shape\n    freqs = freqs.unsqueeze(1).repeat(1, l, 1)\n    t = t.reshape(n, l, d // 2, 2).permute(0, 1, 3, 2)\n    freqs = freqs.reshape(1, l, d // 2, 2).permute(0, 1, 3, 2)\n    t = torch.cat([apply_rotary_pos_emb_to_subset(t[:, :, :, i], freqs[:, :, :, i]) for i in range(2)], dim = -1)\n    t = t.permute(0, 1, 3, 2).reshape(n, l, d)\n    return t\n", "CodeBleu_score": 29.35097929807905}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    if t.dtype != tf.float32:\n        t = tf.cast(t, tf.float32)\n    freqs = tf.cast(freqs, tf.float32)\n    n, d = t.shape\n    if d % 2 != 0:\n        t, t_r = t[:, :-1], t[:, -1]\n        n, d = t.shape\n    t = tf.reshape(t, (n, d // 2, 2))\n    t_r = tf.reshape(t_r, (n, 1, 2))\n    t_pass = t_r[:, :, 0]\n    t_rot = t_r[:, :, 1]\n    freqs = tf.reshape(freqs, (1, -1, 2))\n    t_rot = t_rot * freqs\n    t_rot = tf.concat((-tf.sin(t_rot), tf.cos(t_rot)), axis = -1)\n    t_rot = tf.reshape(t_rot, (n, d))\n    t = t * tf.concat((tf.cos(t_rot), tf.sin(t_rot)), axis = -1)\n    t = tf.reshape(t, (n, d))\n    t = t * scale\n    t = tf.concat((t, t_pass), axis = -1)\n    return t\n", "CodeBleu_score": 36.37123605219718}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    t = tf.cast(t, tf.float32)\n    freqs = tf.cast(freqs, tf.float32)\n    seq_len, embed_dim = t.shape[1], t.shape[-1]\n    freqs = tf.concat([freqs[:, None], tf.zeros([1, embed_dim], dtype = tf.float32)], 0)\n    t = tf.concat([tf.math.cos(t[..., None] * freqs * scale), tf.math.sin(t[..., None] * freqs * scale)], -1)\n    t = tf.reshape(t, [-1, embed_dim * 2])\n    return t\n", "CodeBleu_score": 18.33971365734305}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1)\n    if dim != -1:\n        embeddings = torch.index_select(self.codebook, dim=0, index=idx).transpose(dim, -1)\n    else:\n        embeddings = torch.index_select(self.codebook, dim=0, index=idx)\n    return embeddings\n\n", "CodeBleu_score": 44.52856553778165}, {"code": "def idx2vq(self, idx, dim=-1)\n    if dim < 0:\n        dim += len(idx.shape)\n    idx = idx.unsqueeze(dim)\n    vq = self.codebook.weight[idx].squeeze(dim)\n    return vq\n\n", "CodeBleu_score": 47.83053561038114}, {"code": "def idx2vq(self, idx, dim=-1)\n    # retrieve the embeddings\n    embeddings = self.codebook[idx]\n    # move the last dimension to the given dimension\n    if dim != -1:\n        embeddings = embeddings.movedim(-1, dim)\n    # return the embeddings\n    return embeddings", "CodeBleu_score": 34.58885807486271}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n    if quantize:\n        # Encode the input\n        encoded = self.encoder(x)\n        # Apply the vector quantizer\n        encoded, indices, loss = self.vq(encoded)\n        # Return the quantized encoding, original encoding, indices, and combined loss\n        return encoded, x, indices, loss\n    else:\n        # Encode the input\n        encoded = self.encoder(x)\n        # Return the encoded output\n        return encoded\n\n", "CodeBleu_score": 34.591747444905835}, {"code": "def encode(self, x, quantize=False)\n    if quantize:\n        z = self.quantize_conv(x)\n        z_e = self.encode(z)\n        indices = self.quantize(z_e)\n        z_q = self.quantize.embedding(indices)\n        loss = F.mse_loss(z_q.detach(), z_e) + F.mse_loss(z_q, z_e.detach())\n        return z_q, z_e, indices, loss\n    else:\n        for block in self.blocks:\n            x = block(x)\n        return x\n\n", "CodeBleu_score": 35.854682291472585}, {"code": "def encode(self, x, quantize=False)\n    if self.quantize:\n        x = self.quantize_input(x)\n    for block in self.blocks:\n        x = block(x)\n    if self.quantize:\n        x_q, x_z, x_i, x_l = self.quantizer(x)\n        return x_q, x_z, x_i, x_l\n    else:\n        return x", "CodeBleu_score": 36.741154081514146}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if context is not None:\n        self.logger.log(\"EMA\")\n    self.ema_params = self.get_parameters()\n    self.set_parameters(self.get_ema_parameters())\n    yield\n    self.set_parameters(self.ema_params)\n    if context is not None:\n        self.logger.log(\"Restored\")\n\n", "CodeBleu_score": 41.61734204613883}, {"code": "def ema_scope(self, context=None)\n    if self.ema is not None:\n        if context is not None:\n            context.log(f\"Switching to EMA weights\")\n        self.ema.swap_parameters()\n    yield\n    if self.ema is not None:\n        if context is not None:\n            context.log(f\"Restoring original weights\")\n        self.ema.swap_parameters()\n\n", "CodeBleu_score": 49.6382120732238}, {"code": "def ema_scope(self, context=None)\n    if context is not None:\n        context.log(\"EMA\")\n    if self.ema_weights is not None:\n        self.ema_weights = self.ema_weights.to(self.device)\n        self.parameters = self.ema_weights\n    if context is not None:\n        context.log(\"EMA restored\")", "CodeBleu_score": 40.17658183245813}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n\n    x += attn * gate_msa\n    x = self.norm(x)\n    x = x * shift_mlp + scale_mlp\n    x = self.mlp(x) * gate_mlp\n    return x", "CodeBleu_score": 30.7485140950375}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    # Add the attention output, scaled by gate_msa, to x.\n    x += attn * gate_msa\n    # Normalize x.\n    x = self.norm_msa(x)\n    # Modulate x using shift_mlp and scale_mlp.\n    x = x + shift_mlp(x)\n    x = x * scale_mlp(x)\n    # Apply an MLP scaled by gate_mlp.\n    x = x * gate_mlp\n    return x\n\n", "CodeBleu_score": 32.835206527795094}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    # x: [*, N, C]\n    # attn: [*, N, C]\n    # gate_msa: [*, C]\n    # shift_mlp: [*, C]\n    # scale_mlp: [*, C]\n    # gate_mlp: [*, C]\n    #\n    # x: [*, N, C]\n\n    # Add the attention output to x.\n    x = x + attn * gate_msa\n\n    # Normalize x.\n    x = x / jnp.sqrt(jnp.sum(x ** 2, axis=-1, keepdims=True))\n\n    # Modulate x.\n    x = x * scale_mlp + shift_mlp\n\n    # Apply the MLP.\n    x = x * gate_mlp\n\n    return x", "CodeBleu_score": 37.200150555065}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n    # Pre-attention blocks\n    context_qkv, context_intermediates = context_block(context)\n    x_qkv, x_intermediates = x_block(x)\n\n    # Concatenate qkv tensors\n    context_qkv = tf.concat(context_qkv, axis=-1)\n    x_qkv = tf.concat(x_qkv, axis=-1)\n\n    # Apply attention\n    context_qkv, x_qkv = optimized_attention(context_qkv, x_qkv, c)\n\n    # Split qkv tensors\n    context_qkv = tf.split(context_qkv, num_or_size_splits=3, axis=-1)\n    x_qkv = tf.split(x_qkv, num_or_size_splits=3, axis=-1)\n\n    # Post-attention blocks\n    if context_block.post_attention_block is not None:\n        context = context_block.post_attention_block(context, context_qkv, context_intermediates)\n    if x_block.post_attention_block is not None:\n        x = x_block.post_attention_block(x, x_qkv, x_intermediates)\n\n    return context, x\n\n", "CodeBleu_score": 24.036712541263693}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n    # Pre-attention blocks\n    context_qkv, context_intermediate = context_block(context, training=c)\n    x_qkv, x_intermediate = x_block(x, training=c)\n\n    # Concatenate qkv tensors\n    qkv = tf.concat([context_qkv, x_qkv], axis=-1)\n\n    # Optimized attention\n    attention_output = _attention(qkv, training=c)\n\n    # Split attention results\n    context_attention_output, x_attention_output = tf.split(attention_output, 2, axis=-1)\n\n    # Post-attention blocks\n    context = context_block(context_attention_output, training=c)\n    x = x_block(x_attention_output, training=c)\n\n    return context, x\n", "CodeBleu_score": 16.172606455594266}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n    # Pre-attention blocks\n    context_pre_attn, context_intermediate = context_block(context, training=c.training)\n    x_pre_attn, x_intermediate = x_block(x, training=c.training)\n\n    # Concatenate qkv tensors\n    context_qkv = tf.concat([context_pre_attn, context_intermediate], axis=-1)\n    x_qkv = tf.concat([x_pre_attn, x_intermediate], axis=-1)\n\n    # Apply optimized attention\n    context_attn_out, x_attn_out = c.attention(context_qkv, x_qkv, training=c.training)\n\n    # Split attention results\n    context_attn_out, x_attn_out = tf.split(context_attn_out, 2, axis=-1), tf.split(x_attn_out, 2, axis=-1)\n\n    # Post-attention blocks\n    context_post_attn = context_block.post_attention(context_attn_out, training=c.training)\n    x_post_attn = x_block.post_attention(x_attn_out, training=c.training)\n\n    return context_post_attn, x_post_attn\n\n", "CodeBleu_score": 23.022298839251746}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert len(timesteps.shape) == 1, \"The timesteps input must be one-dimensional.\"\n\n    assert embedding_dim % 2 == 0, \"The embedding dimension must be even.\"\n\n    half_dim = embedding_dim // 2\n    emb = np.log(10000) / (half_dim - 1)\n    emb = np.exp(np.arange(half_dim, dtype=np.float32) * -emb)\n    emb = np.expand_dims(np.arange(len(timesteps), dtype=np.float32), 1) * emb\n    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:\n        emb = np.pad(emb, [[0, 0], [0, 1]], constant_values=0)\n    assert emb.shape[1] == embedding_dim\n    return emb\n\n", "CodeBleu_score": 57.77414989282844}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n\n    assert len(timesteps.shape) == 1, 'Timesteps must be a one-dimensional tensor.'\n    assert embedding_dim % 2 == 0, 'Embedding dimension must be even.'\n\n    half_dim = embedding_dim // 2\n    emb = np.log(10000) / (half_dim - 1)\n    emb = np.exp(np.arange(half_dim, dtype=np.float32) * -emb)\n    emb = np.expand_dims(np.arange(len(timesteps), dtype=np.float32), 1) * np.expand_dims(emb, 0)\n    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n    if emb.shape[1] < embedding_dim:\n        emb = np.pad(emb, [[0, 0], [0, embedding_dim - emb.shape[1]]], constant_values=0)\n    assert emb.shape[1] == embedding_dim\n    return emb\n\n", "CodeBleu_score": 55.95431927330441}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert len(timesteps.shape) == 1, \"The timesteps input must be one-dimensional\"\n    assert embedding_dim % 2 == 0, \"The embedding dimension must be even\"\n    half_dim = embedding_dim // 2\n    emb = np.log(10000) / (half_dim - 1)\n    emb = np.exp(np.arange(half_dim, dtype=np.float32) * -emb)\n    emb = np.expand_dims(np.arange(len(timesteps), dtype=np.float32), 1) * emb\n    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:\n        emb = np.pad(emb, [[0, 0], [0, 1]], constant_values=0)\n    assert emb.shape[1] == embedding_dim\n    return emb\n\n", "CodeBleu_score": 57.76934554726228}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n    try:\n        # reshape q, k, v for attention computation\n        q = q.view(q.shape[0], q.shape[1], q.shape[2], -1).permute(0, 3, 1, 2)\n        k = k.view(k.shape[0], k.shape[1], k.shape[2], -1).permute(0, 3, 1, 2)\n        v = v.view(v.shape[0], v.shape[1], v.shape[2], -1).permute(0, 3, 1, 2)\n        # compute attention\n        attn = torch.matmul(q, k.transpose(-2, -1))\n        attn = attn / math.sqrt(k.shape[-1])\n        attn = torch.softmax(attn, dim=-1)\n        attn = torch.matmul(attn, v)\n        # reshape back to original dimensions\n        attn = attn.permute(0, 2, 3, 1).reshape(attn.shape[0], attn.shape[1], attn.shape[2], -1)\n        return attn\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            print(\"| WARNING: out of memory, using slice attention\")\n            return slice_attention(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 30.524947484439597}, {"code": "def pytorch_attention(q, k, v)\n    # Reshape and transpose query, key, and value tensors\n    q = q.transpose(-2, -1).contiguous()\n    k = k.transpose(-2, -1).contiguous()\n    v = v.transpose(-2, -1).contiguous()\n    # Compute attention using scaled dot-product attention\n    try:\n        attn = torch.bmm(q, k.transpose(-2, -1))\n    except RuntimeError as exception:\n        if \"CUDA out of memory\" in str(exception):\n            attn = slice_attention(q, k, v)\n        else:\n            raise exception\n    # Reshape and transpose attention output back to the original dimensions\n    attn = attn.transpose(-2, -1).contiguous()\n    return attn\n\n", "CodeBleu_score": 20.607569417760875}, {"code": "def pytorch_attention(q, k, v)\n    try:\n        # reshape query, key, and value\n        q = q.view(q.shape[0], -1, q.shape[-1])\n        k = k.view(k.shape[0], -1, k.shape[-1])\n        v = v.view(v.shape[0], -1, v.shape[-1])\n\n        # transpose query, key, and value for batched matrix multiplication\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # compute attention using batched matrix multiplication\n        attn = torch.bmm(q, k)\n\n        # scale attention\n        attn = attn / math.sqrt(k.shape[-1])\n\n        # apply softmax to attention scores\n        attn = F.softmax(attn, dim=-1)\n\n        # apply attention weights\n        output = torch.bmm(attn, v)\n\n        # reshape attention output\n        output = output.transpose(1, 2)\n        output = output.contiguous().view(\n            output.shape[0], -1, output.shape[2] * output.shape[3]\n        )\n\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            print(\n                \"| WARNING: ran out of memory, using slice attention instead of batched attention\"\n            )\n            output = slice_attention(q, k, v)\n        else:\n            print(\"| ERROR: \", e)\n            exit()\n\n    return output\n\n", "CodeBleu_score": 31.157467608990913}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n    if control in h.controls:\n        if name in h.controls[control]:\n            if h.controls[control][name]:\n                try:\n                    h += h.controls[control][name]\n                except:\n                    print(\"WARNING: Could not apply control \" + control + \":\" + name + \" to \" + str(h))\n    return h\n\n", "CodeBleu_score": 37.971798525868266}, {"code": "def apply_control(h, control, name)\n    if control and name:\n        try:\n            h.add_control(control.pop(name))\n        except Exception as e:\n            print('Warning: failed to add control to ' + name + ': ' + str(e))\n    return h\n\n", "CodeBleu_score": 23.626383660011797}, {"code": "def apply_control(h, control, name)\n\n    if control is None:\n        return h\n\n    if name not in control:\n        return h\n\n    c = control[name]\n    if not c:\n        return h\n\n    try:\n        h.add_control(c)\n    except:\n        print(f\"Warning: {name} control could not be added to h\")\n\n    return h", "CodeBleu_score": 39.156435774332934}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n    indices = tf.cast(indices, tf.int32)\n    if self.use_residuals:\n        indices = indices + tf.random.uniform(tf.shape(indices), minval=0, maxval=self.num_embeddings, dtype=tf.int32)\n        indices = tf.math.mod(indices, self.num_embeddings)\n    one_hot = tf.one_hot(indices, self.num_embeddings, dtype=self.dtype)\n    quantized = tf.matmul(one_hot, self.embedding_weights)\n    if shape is not None:\n        quantized = tf.reshape(quantized, shape)\n        quantized = tf.transpose(quantized, [0, 3, 1, 2])\n    return quantized\n\n", "CodeBleu_score": 42.31158302181096}, {"code": "def get_codebook_entry(self, indices, shape)\n    if indices.shape[0] != shape[0]:\n        raise ValueError(\"The number of indices must match the number of samples in the original input.\")\n    if indices.max() >= self.codebook_size:\n        raise ValueError(\"All indices must be smaller than the codebook size.\")\n    if indices.min() < 0:\n        raise ValueError(\"All indices must be positive.\")\n    if shape is None:\n        return self.embedding_weights[indices]\n    else:\n        return self.embedding_weights[indices].reshape(shape).permute(0, 3, 1, 2)", "CodeBleu_score": 21.417797952271002}, {"code": "def get_codebook_entry(self, indices, shape)\n    # Create one-hot encoding\n    one_hot_encoding = F.one_hot(indices, self.num_embeddings)\n    # Multiply with embedding weights\n    quantized_vectors = torch.matmul(one_hot_encoding, self.weight)\n    # Reshape and permute to match original input shape\n    if shape is not None:\n        quantized_vectors = quantized_vectors.reshape(shape)\n        quantized_vectors = quantized_vectors.permute(0, 3, 1, 2)\n    return quantized_vectors\n\n", "CodeBleu_score": 35.22054566784512}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n    self.noise_list = []\n    self.noise_list.append(torch.randn(4, 4, device=self.device))\n    for i in range(3, self.log_size):\n        self.noise_list.append(torch.randn(2**i, 2**i, device=self.device))\n        self.noise_list.append(torch.randn(2**i, 2**i, device=self.device))\n    return self.noise_list\n", "CodeBleu_score": 28.345472976293877}, {"code": "def make_noise(self)\n    noise_list = []\n    noise_list.append(torch.randn(4, 4, device=self.device))\n    for size in range(3, self.log_size):\n        noise_list.append(torch.randn(2 ** size, 2 ** size, device=self.device))\n        noise_list.append(torch.randn(2 ** size, 2 ** size, device=self.device))\n    return noise_list\n\n", "CodeBleu_score": 25.501934409911005}, {"code": "def make_noise(self)\n    noise = torch.randn(4, 4, device=self.device)\n    noise_list = [noise]\n    for i in range(3, self.log_size):\n        noise = torch.randn(2 ** i, 2 ** i, device=self.device)\n        noise_list.append(noise)\n        noise_list.append(noise)\n    return noise_list\n\n", "CodeBleu_score": 34.72589432638579}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n    # Generate random latent inputs\n    latent_inputs = torch.randn(num_latent, self.latent_dim)\n    # Process latent inputs through MLP\n    latent_inputs = self.latent_mlp(latent_inputs)\n    # Compute mean latent vector\n    mean_latent = torch.mean(latent_inputs, dim=0)\n    return mean_latent\n\n", "CodeBleu_score": 21.430377331003065}, {"code": "def mean_latent(self, num_latent)\n    latents = [self.generate_latent() for i in range(num_latent)]\n    mean_latent = torch.mean(torch.stack([self.style_mlp(latent) for latent in latents]), dim=0)\n    return mean_latent\n\n", "CodeBleu_score": 24.68951347950562}, {"code": "def mean_latent(self, num_latent)\n\n    # Generate random latent inputs\n    latent_input = tf.random.normal((1, num_latent))\n\n    # Process latent inputs through MLP\n    latent_output = self.style_mlp(latent_input)\n\n    # Compute mean latent vector\n    mean_latent = tf.reduce_mean(latent_output, axis=0)\n\n    return mean_latent\n", "CodeBleu_score": 18.106587806286985}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n\n    # Define the range of possible jitter values\n    jitter_range = (-shift, shift)\n\n    # Generate random jitter values for each RGB channel\n    jitter = np.random.uniform(jitter_range, size=3)\n\n    # Add the jitter values to the image\n    jittered_img = img + jitter\n\n    # Clip the image to ensure pixel values remain between 0 and 1\n    jittered_img = np.clip(jittered_img, 0, 1)\n\n    return jittered_img\n\n", "CodeBleu_score": 45.14417429118293}, {"code": "def color_jitter(img, shift)\n    # Generate random jitter values\n    r_jitter = np.random.uniform(-shift, shift)\n    g_jitter = np.random.uniform(-shift, shift)\n    b_jitter = np.random.uniform(-shift, shift)\n    # Add the jitter to the image\n    img = img + np.array([r_jitter, g_jitter, b_jitter])\n    # Clip the image to ensure pixel values remain between 0 and 1\n    img = np.clip(img, 0, 1)\n    return img\n\n", "CodeBleu_score": 53.17171515427744}, {"code": "def color_jitter(img, shift)\n    # Generate random jitter values for each channel\n    r_jitter = np.random.randint(-shift, shift)\n    g_jitter = np.random.randint(-shift, shift)\n    b_jitter = np.random.randint(-shift, shift)\n    # Add jitter to each channel and clip result\n    img[:, :, 0] = np.clip(img[:, :, 0] + r_jitter, 0, 1)\n    img[:, :, 1] = np.clip(img[:, :, 1] + g_jitter, 0, 1)\n    img[:, :, 2] = np.clip(img[:, :, 2] + b_jitter, 0, 1)\n    return img\n\n", "CodeBleu_score": 36.365206256218684}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'gt_facial_component_locations' in data:\n        self.left_eye = data['gt_facial_component_locations']['left_eye'].to(self.device)\n        self.right_eye = data['gt_facial_component_locations']['right_eye'].to(self.device)\n        self.mouth = data['gt_facial_component_locations']['mouth'].to(self.device)", "CodeBleu_score": 53.78649846289502}, {"code": "def feed_data(self, data)\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data.keys():\n        self.gt = data['gt'].to(self.device)\n    if 'left_eye' in data.keys():\n        self.left_eye = data['left_eye'].to(self.device)\n    if 'right_eye' in data.keys():\n        self.right_eye = data['right_eye'].to(self.device)\n    if 'mouth' in data.keys():\n        self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 47.92648070144351}, {"code": "def feed_data(self, data)\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'face_locations' in data:\n        self.left_eye = data['face_locations']['left_eye'].to(self.device)\n        self.right_eye = data['face_locations']['right_eye'].to(self.device)\n        self.mouth = data['face_locations']['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.06936981021485}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n    pass\n\n", "CodeBleu_score": 0.8064516129305588}, {"code": "def construct_img_pyramid(self)\n\n    img_pyramid = []\n    img_pyramid.append(self.gt_img)\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], (0, 0), fx=self.scale_factor, fy=self.scale_factor, interpolation=cv2.INTER_LINEAR))\n\n    return img_pyramid", "CodeBleu_score": 21.334203523617003}, {"code": "def construct_img_pyramid(self)\n    pyramid = [self.gt_img]\n    for i in range(self.num_steps - 1):\n        pyramid.insert(0, cv2.resize(pyramid[0], (pyramid[0].shape[1] // 2, pyramid[0].shape[0] // 2), interpolation=cv2.INTER_LINEAR))\n    return pyramid\n\n", "CodeBleu_score": 18.61950435632681}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n    # save the model\n    self.model.save(self.output_path, epoch, current_iter)\n\n    # save the discriminator\n    self.discriminator.save(self.output_path, epoch, current_iter)\n\n    # save the facial discriminators\n    if self.use_facial_discriminator:\n        self.left_eye_discriminator.save(self.output_path, epoch, current_iter)\n        self.right_eye_discriminator.save(self.output_path, epoch, current_iter)\n        self.mouth_discriminator.save(self.output_path, epoch, current_iter)\n\n    # save the training state\n    self.save_training_state(epoch, current_iter)\n\n", "CodeBleu_score": 41.16713149865511}, {"code": "def save(self, epoch, current_iter)\n    # Save the generator network\n    torch.save(self.G.state_dict(), f\"{self.save_dir}/G_{epoch}_{current_iter}.pth\")\n    # Save the EMA version of the generator\n    torch.save(self.G_ema.state_dict(), f\"{self.save_dir}/G_ema_{epoch}_{current_iter}.pth\")\n    # Save the discriminator network\n    torch.save(self.D.state_dict(), f\"{self.save_dir}/D_{epoch}_{current_iter}.pth\")\n    # Save the facial discriminator networks\n    if self.use_facial_discriminators:\n        torch.save(self.D_left_eye.state_dict(), f\"{self.save_dir}/D_left_eye_{epoch}_{current_iter}.pth\")\n        torch.save(self.D_right_eye.state_dict(), f\"{self.save_dir}/D_right_eye_{epoch}_{current_iter}.pth\")\n        torch.save(self.D_mouth.state_dict(), f\"{self.save_dir}/D_mouth_{epoch}_{current_iter}.pth\")\n    # Save the training state\n    torch.save(self.state, f\"{self.save_dir}/training_state_{epoch}_{current_iter}.pth\")\n\n", "CodeBleu_score": 38.332306437456594}, {"code": "def save(self, epoch, current_iter)\n\n    # Save the generator network\n    torch.save(self.G.state_dict(), f'{self.save_dir}/G_{epoch}_{current_iter}.pth')\n\n    # Save the EMA version of the generator network\n    torch.save(self.G_EMA.state_dict(), f'{self.save_dir}/G_EMA_{epoch}_{current_iter}.pth')\n\n    # Save the discriminator network\n    torch.save(self.D.state_dict(), f'{self.save_dir}/D_{epoch}_{current_iter}.pth')\n\n    # Save the discriminator for the left eye\n    if self.use_eyes:\n        torch.save(self.D_eyes.state_dict(), f'{self.save_dir}/D_eyes_{epoch}_{current_iter}.pth')\n\n    # Save the discriminator for the right eye\n    if self.use_eyes:\n        torch.save(self.D_eyes_right.state_dict(), f'{self.save_dir}/D_eyes_right_{epoch}_{current_iter}.pth')\n\n    # Save the discriminator for the mouth\n    if self.use_mouth:\n        torch.save(self.D_mouth.state_dict(), f'{self.save_dir}/D_mouth_{epoch}_{current_iter}.pth')\n\n    # Save the overall training state\n    torch.save(self.state, f'{self.save_dir}/state_{epoch}_{current_iter}.pth')\n\n    # Save the optimizer states\n    torch.save(self.G_optimizer.state_dict(), f'{self.save_dir}/G_optimizer_{epoch}_{current_iter}.pth')\n    torch.save(self.D_optimizer.state_dict(), f'{self.save_dir}/D_optimizer_{epoch}_{current_iter}.pth')\n    if self.use_eyes:\n        torch.save(self.D_eyes_optimizer.state_dict(), f'{self.save_dir}/", "CodeBleu_score": 35.37799250066276}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n    import os\n    import shutil\n    for item in os.listdir(folder):\n        try:\n            path = os.path.join(folder, item)\n            if os.path.isfile(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print(f'Failed to delete {path}. Reason: {e}')\n\n", "CodeBleu_score": 52.60792185784473}, {"code": "def clean_folder(folder)\n    import os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 64.8786320172984}, {"code": "def clean_folder(folder)\n    import os\n    import shutil\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n\n", "CodeBleu_score": 64.8786320172984}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n    if self.size < sample_size:\n        sample_size = self.size\n    batch = self.experiences[0:sample_size]\n    self.experiences = self.experiences[sample_size:]\n    self.size = self.size - sample_size\n    return batch", "CodeBleu_score": 32.26926483515493}, {"code": "def next_batch(self, sample_size=1)\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)\n\n    batch = random.sample(self.experiences, sample_size)\n    for experience in batch:\n        self.experiences.remove(experience)\n    return batch", "CodeBleu_score": 49.82036836182859}, {"code": "def next_batch(self, sample_size=1)\n    if sample_size > len(self.memory):\n        sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    for experience in batch:\n        self.memory.remove(experience)\n    return batch", "CodeBleu_score": 49.02659354588814}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n\n    # Flatten all configurations\n    all_configs_flat = []\n    for config in self.all_configs:\n        all_configs_flat.append(self.flatten_config(config))\n\n    # Collect numerical values\n    all_configs_flat_num = []\n    for config in all_configs_flat:\n        all_configs_flat_num.append(self.collect_num_values(config))\n\n    # Convert to numpy array\n    all_configs_flat_num_np = np.array(all_configs_flat_num)\n\n    # Predict estimates\n    estimates = self.cost_model.predict(all_configs_flat_num_np)\n\n    # Sort estimates\n    if self.metric == 'min':\n        estimates_sorted = np.argsort(estimates)\n    elif self.metric == 'max':\n        estimates_sorted = np.argsort(estimates)[::-1]\n    else:\n        raise ValueError('Metric must be \"min\" or \"max\".')\n\n    # Return indices of top configurations\n    return estimates_sorted[:self.top_configs]", "CodeBleu_score": 32.4645132720006}, {"code": "def find_estimated_top_configs(self)\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.neighbors import KNeighborsRegressor\n\n    # Flatten all configurations\n    all_configs_flat = []\n    for config in self.all_configs:\n        all_configs_flat.append(self.flatten_config(config))\n\n    # Collect numerical values\n    numerical_values = []\n    for config in all_configs_flat:\n        numerical_values.append(self.get_numerical_values(config))\n\n    # Convert to numpy array\n    numerical_values = np.array(numerical_values)\n\n    # Scale values\n    scaler = StandardScaler()\n    numerical_values = scaler.fit_transform(numerical_values)\n\n    # Predict estimates\n    knn = KNeighborsRegressor(n_neighbors=10, weights=\"distance\")\n    knn.fit(numerical_values, self.all_estimates)\n    estimates = knn.predict(numerical_values)\n\n    # Sort estimates\n    if self.metric == \"time\":\n        indices = np.argsort(estimates)\n    elif self.metric == \"memory\":\n        indices = np.argsort(estimates)[::-1]\n    else:\n        raise ValueError(\"Unknown metric.\")\n\n    return indices", "CodeBleu_score": 32.80191876199233}, {"code": "def find_estimated_top_configs(self)\n    import numpy as np\n\n    # Flatten each configuration in all_configs, collect numerical values, and convert them into a numpy array\n    all_configs_flattened = []\n    for config in self.all_configs:\n        config_flattened = []\n        for value in config.values():\n            if isinstance(value, (int, float)):\n                config_flattened.append(value)\n        all_configs_flattened.append(config_flattened)\n    all_configs_flattened = np.array(all_configs_flattened)\n\n    # Predict estimates for these configurations\n    estimates = self.cost_model.predict(all_configs_flattened)\n\n    # Sort the estimates to identify the top configurations\n    if self.metric == \"min\":\n        top_configs_indices = np.argsort(estimates)[:self.top_configs_count]\n    elif self.metric == \"max\":\n        top_configs_indices = np.argsort(estimates)[-self.top_configs_count:]\n    else:\n        raise ValueError(\n            \"The metric for identifying the top configurations must be 'min' or 'max'.\"\n        )\n\n    return top_configs_indices", "CodeBleu_score": 35.99790664880545}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)", "CodeBleu_score": 74.68861952301768}, {"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 74.68861952301768}, {"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 74.68861952301768}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n\n    if self.optimal_cmd:\n        try:\n            self.logger.info(f\"Running optimal command: {self.optimal_cmd}\")\n            process = subprocess.Popen(self.optimal_cmd, shell=True)\n            process.wait()\n            self.logger.info(\"Command completed successfully.\")\n        except Exception as e:\n            self.logger.error(f\"Command failed: {e}\")\n    else:\n        self.logger.info(\"No optimal command found.\")", "CodeBleu_score": 47.92399643220995}, {"code": "def run_after_tuning(self)\n    import subprocess\n    import logging\n    import os\n\n    if self.optimal_cmd is not None:\n        logging.info(\"Executing the following command: %s\", self.optimal_cmd)\n        os.system(self.optimal_cmd)\n        logging.info(\"Command completed successfully!\")\n    else:\n        logging.info(\"Autotuning did not find an optimal configuration\")\n\n", "CodeBleu_score": 37.819244106700296}, {"code": "def run_after_tuning(self)\n    if self.optimal_cmd is not None:\n        print(f\"Running command: {self.optimal_cmd}\")\n        proc = subprocess.Popen(self.optimal_cmd, shell=True)\n        proc.wait()\n        print(f\"Command {self.optimal_cmd} completed successfully.\")\n    else:\n        print(\"No optimal command found.\")\n\n", "CodeBleu_score": 50.59030175017224}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n    import os\n    import threading\n    import time\n    import shutil\n    import json\n    from . import utils\n    from . import config\n\n    # set master port and result directory\n    exp[\"master_port\"] = utils.get_free_port()\n    exp[\"result_dir\"] = os.path.join(config.RESULT_DIR, exp[\"name\"], str(int(time.time())))\n\n    # update user arguments based on argument mappings\n    for arg_mapping in exp[\"arg_mappings\"]:\n        if arg_mapping[\"type\"] == \"static\":\n            exp[\"user_args\"][arg_mapping[\"arg_name\"]] = arg_mapping[\"value\"]\n        elif arg_mapping[\"type\"] == \"reservation\":\n            if arg_mapping[\"reservation_id\"] not in reservations:\n                raise Exception(\n                    \"Reservation with id {} not found.\".format(arg_mapping[\"reservation_id\"])\n                )\n            reservation = reservations[arg_mapping[\"reservation_id\"]]\n            exp[\"user_args\"][arg_mapping[\"arg_name\"]] = reservation[\"result\"][\n                arg_mapping[\"reservation_arg_name\"]\n            ]\n\n    # create result directory\n    os.makedirs(exp[\"result_dir\"])\n\n    # copy experiment script to result directory\n    shutil.copy(exp[\"script\"], exp[\"result_dir\"])\n\n    # create experiment thread\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n    thread.start()\n\n    # store running experiment details\n    self.running_experiments[exp[\"name\"]] = {\n        \"thread\": thread,\n        \"master_port\": exp[\"master_port\"],\n        \"result_dir\": exp[\"result_dir\"],\n    }\n\n", "CodeBleu_score": 30.60809355530538}, {"code": "def run_job(self, exp: dict, reservations)\n\n    # configure experiment parameters\n    exp['master_port'] = self.get_free_port()\n    exp['result_dir'] = self.result_dir\n    exp['user_args'] = self.update_user_args(exp['user_args'])\n    exp['reservation'] = reservations[0]\n\n    # start experiment in new thread\n    thread = threading.Thread(target=self.run_experiment, args=(exp,))\n    thread.start()\n\n    # store running experiment details\n    self.running_experiments[exp['job_id']] = {\n        'start_time': datetime.now(),\n        'thread': thread\n    }\n\n    # remove reservation from list\n    reservations.pop(0)\n\n    return reservations", "CodeBleu_score": 17.294675951753593}, {"code": "def run_job(self, exp: dict, reservations)\n    import threading\n    import time\n    import os\n    import shutil\n    import json\n    import sys\n    import traceback\n    import logging\n    import socket\n    import psutil\n    from datetime import datetime\n    from pathlib import Path\n    from multiprocessing import Process\n\n    from .utils.config_utils import get_config\n    from .utils.process_utils import get_process_info\n    from .utils.experiment_utils import get_experiment_path, get_experiment_result_path, get_experiment_log_path, get_experiment_log_file_path, get_experiment_script_path, get_experiment_script_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_path, get_experiment_log_file_path, get_experiment_result_file_", "CodeBleu_score": 7.042894426491353}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.distributed_training_initialized:\n        if self.rank == 0:\n            if ranks is None:\n                ranks = [0]\n            for rank in ranks:\n                if rank == 0:\n                    message = \"{}\".format(message)\n                else:\n                    message = \"{}\".format(message)\n                if path is None:\n                    path = \"./experiment_results.json\"\n                with open(path, \"a\") as f:\n                    f.write(message)\n    else:\n        if path is None:\n            path = \"./experiment_results.json\"\n        with open(path, \"a\") as f:\n            f.write(message)\n\n", "CodeBleu_score": 23.400517212212048}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.is_rank_0:\n        if path is None:\n            path = os.path.join(self.exp_dir, \"logs.json\")\n        if ranks is None:\n            ranks = [0]\n        for rank in ranks:\n            message[\"rank\"] = rank\n            with open(path, \"a+\") as f:\n                f.write(json.dumps(message) + \"\\n\")\n\n", "CodeBleu_score": 24.774366744698852}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.is_main_process:\n        if ranks is not None:\n            message = f\"{ranks} {message}\"\n        if path is not None:\n            with open(path, \"a\") as f:\n                f.write(message)\n                f.write(\"\\n\")\n", "CodeBleu_score": 18.716725284395803}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    if not self.layer_files:\n        raise Exception(\"No layer files found.\")\n    if layer_index < 0 or layer_index >= len(self.layer_files):\n        raise Exception(\"Layer index out of bounds.\")\n    layer_files = [f for f in self.layer_files if f.startswith(f\"{layer_index}_\")]\n    layer_files_map = {}\n    for f in layer_files:\n        tp_degree = int(f.split(\"_\")[1])\n        if tp_degree not in layer_files_map:\n            layer_files_map[tp_degree] = []\n        layer_files_map[tp_degree].append(f)\n    return layer_files_map", "CodeBleu_score": 35.0926635054603}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    layer_files = self._get_layer_files(layer_index)\n    if not layer_files:\n        return {}\n    if layer_index < 0 or layer_index >= self.num_layers:\n        raise IndexError(f\"layer index {layer_index} is out of range\")\n    layer_prefix = self._layer_file_prefix(layer_index)\n    layer_file_lists = {}\n    for file in layer_files:\n        if file.startswith(layer_prefix):\n            tp_degree = int(file.split(\"_\")[-1].split(\".\")[0])\n            if tp_degree not in layer_file_lists:\n                layer_file_lists[tp_degree] = []\n            layer_file_lists[tp_degree].append(file)\n    return layer_file_lists\n\n", "CodeBleu_score": 32.479157175130055}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    import os\n    import numpy as np\n    import pandas as pd\n    from ._build_tp_layer_map import _build_tp_layer_map\n    from ._get_tp_layer_files import _get_tp_layer_files\n\n    layer_files = _get_tp_layer_files(self)\n    if not layer_files:\n        raise Exception(f\"No layer files found for {self.name}\")\n    if layer_index < 0 or layer_index >= len(layer_files):\n        raise Exception(f\"Invalid layer index {layer_index} for {self.name}\")\n\n    layer_file = layer_files[layer_index]\n    layer_df = pd.read_csv(layer_file, sep=\"\\t\", header=None)\n    layer_df.columns = [\"tp_degree\", \"file_name\"]\n\n    layer_map = _build_tp_layer_map(layer_df, layer_index)\n\n    return layer_map", "CodeBleu_score": 30.422400717373066}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n    import platform\n    import sys\n\n    return {\n        \"os\": platform.system(),\n        \"architecture\": platform.machine(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": sys.version,\n    }", "CodeBleu_score": 67.61712827638593}, {"code": "def system_info()\n    import platform\n    import sys\n\n    return {\n        \"OS\": platform.system(),\n        \"Architecture\": platform.architecture(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": sys.version,\n    }\n\n", "CodeBleu_score": 66.43971938485207}, {"code": "def system_info()\n    import platform\n    return {\n        \"os\": platform.system(),\n        \"architecture\": platform.machine(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": platform.python_version()\n    }", "CodeBleu_score": 75.13529957684904}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n    import pynvml\n    from pynvml import *\n    import os\n    import platform\n    import torch\n    import sys\n\n    if os.environ.get(\"CUDA_HOME\"):\n        cuda_home = os.environ.get(\"CUDA_HOME\")\n    else:\n        cuda_home = \"\"\n\n    try:\n        pynvml.nvmlInit()\n        deviceCount = pynvml.nvmlDeviceGetCount()\n        devices = []\n        for i in range(deviceCount):\n            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n            name = pynvml.nvmlDeviceGetName(handle)\n            devices.append(name)\n        pynvml.nvmlShutdown()\n    except:\n        devices = [\"No CUDA-capable devices were detected.\"]\n\n    if torch.cuda.is_available():\n        cuda_availability = \"Yes\"\n        cuda_version = torch.version.cuda\n    else:\n        cuda_availability = \"No\"\n        cuda_version = \"\"\n\n    info = {\n        \"CUDA_HOME\": cuda_home,\n        \"CUDA_availability\": cuda_availability,\n        \"CUDA_version\": cuda_version,\n        \"GPU_devices\": devices,\n    }\n    return info\n\n", "CodeBleu_score": 40.52253585101628}, {"code": "def cuda_info()\n    import os\n    import subprocess\n    import numpy as np\n    import torch\n    import torch.cuda\n    import torch.backends.cudnn\n\n    def get_gpu_name(device):\n        import pynvml\n        pynvml.nvmlInit()\n        handle = pynvml.nvmlDeviceGetHandleByIndex(device)\n        name = pynvml.nvmlDeviceGetName(handle).decode(\"utf-8\")\n        pynvml.nvmlShutdown()\n        return name\n\n    def get_cuda_version():\n        import pynvml\n        pynvml.nvmlInit()\n        driver_version = pynvml.nvmlSystemGetDriverVersion().decode(\"utf-8\")\n        pynvml.nvmlShutdown()\n        return driver_version\n\n    def get_cudnn_version():\n        import ctypes\n        cudnn_version = torch.backends.cudnn.version()\n        cudnn_version = ctypes.cast(cudnn_version, ctypes.c_char_p).value.decode(\"utf-8\")\n        return cudnn_version\n\n    def get_cuda_info():\n        if torch.cuda.is_available():\n            device_count = torch.cuda.device_count()\n            device_names = [get_gpu_name(i) for i in range(device_count)]\n            cuda_version = get_cuda_version()\n            cudnn_version = get_cudnn_version()\n            return device_names, cuda_version, cudnn_version\n        else:\n            return \"CUDA not available\", \"CUDA not available\", \"CUDA not available\"\n\n    device_names, cuda_version, cudnn_version = get_cuda_info()\n    print(\"CUDA Device Names:\", device_names)\n    print(\"CUDA Version:\", cuda_version)\n    print(\"CUDNN Version:\", cudnn_version)\n    print(\"Number of GPUs:\", tor", "CodeBleu_score": 40.815105690589725}, {"code": "def cuda_info()\n\n    import pynvml\n    import subprocess\n    import re\n    import os\n\n    pynvml.nvmlInit()\n    deviceCount = pynvml.nvmlDeviceGetCount()\n    device_names = []\n    for i in range(deviceCount):\n        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n        device_names.append(pynvml.nvmlDeviceGetName(handle))\n    pynvml.nvmlShutdown()\n\n    cuda_version = subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"utf-8\")\n    cuda_version = re.search(\"release (.*),\", cuda_version).group(1)\n\n    cuda_path = os.getenv(\"CUDA_HOME\")\n\n    if cuda_path is None:\n        cuda_available = False\n    else:\n        cuda_available = True\n\n    return device_names, cuda_available, cuda_version", "CodeBleu_score": 32.81042039819864}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_class_name = encoder_manager.config[\"class_id_to_class_name\"]\n\n    class_names = []\n    predicted_labels = []\n    true_labels = []\n\n    for item in dataset_items:\n        audio_file = item[\"audio_file\"]\n        embedding = encoder_manager.embed_audio_file(audio_file)\n        predicted_label = encoder_manager.predict_class_label(embedding)\n        true_label = item[class_name_key]\n\n        class_names.append(true_label)\n        predicted_labels.append(predicted_label)\n        true_labels.append(true_label)\n\n    class_names = list(set(class_names))\n\n    class_accuracy = {}\n    for class_name in class_names:\n        class_accuracy[class_name] = []\n\n    for predicted_label, true_label in zip(predicted_labels, true_labels):\n        class_accuracy[true_label].append(predicted_label == true_label)\n\n    for class_name in class_names:\n        accuracy = sum(class_accuracy[class_name]) / len(class_accuracy[class_name])\n        class_accuracy[class_name] = accuracy\n\n    total_accuracy = sum(class_accuracy.values()) / len(class_accuracy)\n\n    print(f\"Total accuracy: {total_accuracy}\")\n    for class_name in class_names:\n        print(f\"{class_name}: {class_accuracy[class_name]}\")", "CodeBleu_score": 25.96961077409417}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_name_key = encoder_manager.config.class_name_key\n    class_id_to_name_map = encoder_manager.config.class_id_to_name_map\n    encoder = encoder_manager.encoder\n    embeddings = []\n    true_class_names = []\n    predicted_class_names = []\n    for item in dataset_items:\n        audio_file = item[class_name_key]\n        embedding = encoder.embed_audio_file(audio_file)\n        embeddings.append(embedding)\n        true_class_names.append(item[class_name_key])\n        if class_id_to_name_map is not None:\n            predicted_class_name = encoder.predict_class_name(embedding)\n            predicted_class_names.append(predicted_class_name)\n    if class_id_to_name_map is not None:\n        accuracy_dict = compute_accuracy_dict(true_class_names, predicted_class_names)\n        average_accuracy = compute_average_accuracy(accuracy_dict)\n        print(f\"Average accuracy: {average_accuracy * 100}%\")\n        for class_name, accuracy in accuracy_dict.items():\n            print(f\"{class_name}: {accuracy * 100}%\")\n    return embeddings, true_class_names, predicted_class_names\n\n", "CodeBleu_score": 26.369531488692065}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_name_key = encoder_manager.config[\"class_name_key\"]\n    class_id_to_class_name_map = encoder_manager.config.get(\n        \"class_id_to_class_name_map\", None\n    )\n    if class_id_to_class_name_map is None:\n        class_id_to_class_name_map = {}\n    class_name_to_accuracy = {}\n    for item in dataset_items:\n        audio_file = item[\"audio_file\"]\n        embedding = encoder_manager.embed_audio_file(audio_file)\n        predicted_class_name = encoder_manager.predict_class_name(embedding)\n        true_class_name = item[class_name_key]\n        if true_class_name in class_name_to_accuracy:\n            class_name_to_accuracy[true_class_name].append(\n                predicted_class_name == true_class_name\n            )\n        else:\n            class_name_to_accuracy[true_class_name] = [\n                predicted_class_name == true_class_name\n            ]\n    for class_name in class_name_to_accuracy:\n        accuracy = sum(class_name_to_accuracy[class_name]) / len(\n            class_name_to_accuracy[class_name]\n        )\n        print(f\"{class_name} accuracy: {accuracy}\")\n    total_accuracy = 0\n    for class_name in class_name_to_accuracy:\n        total_accuracy += sum(class_name_to_accuracy[class_name])\n    total_accuracy /= len(dataset_items)\n    print(f\"Total accuracy: {total_accuracy}\")", "CodeBleu_score": 31.023321762965082}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n    import os\n    import sys\n    import librosa\n    import soundfile as sf\n    from pathlib import Path\n    from vad import VoiceActivityDetector\n\n    output_path = audio_path.replace(\n        os.getenv(\"INPUT_DIR\"), os.getenv(\"OUTPUT_DIR\"))\n    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n\n    if os.path.isfile(output_path) and not os.getenv(\"FORCE\"):\n        return output_path, False\n\n    data, sr = sf.read(audio_path)\n    vad = VoiceActivityDetector(sr)\n    intervals = vad.detect_speech(data)\n\n    if len(intervals) == 0:\n        return output_path, False\n\n    speech = data[intervals[0][0]:intervals[-1][1]]\n    sf.write(output_path, speech, sr)\n    return output_path, True\n\n", "CodeBleu_score": 40.20942016033231}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\n    import os\n    import shutil\n    import librosa\n    import soundfile as sf\n    import audioread\n    import audioread.ffdec\n    import pydub\n    from pydub.silence import split_on_silence\n\n    # Adjust the output path\n    output_path = audio_path.replace(\n        \"input\", \"output\"\n    )  # Replace the input directory with the output directory\n\n    # Check if the file already exists\n    if os.path.exists(output_path):\n        return output_path, False  # Return the output path and a False flag\n\n    # Create the necessary directory structure\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Remove silence from the audio using specified parameters\n    y, sr = librosa.load(audio_path, sr=None)\n    y_nonsilent = librosa.effects.trim(y, top_db=20, frame_length=2048, hop_length=512)[\n        0\n    ]\n    sf.write(output_path, y_nonsilent, sr)\n\n    # Return the output path and a flag indicating if speech was detected\n    return output_path, True", "CodeBleu_score": 37.753033096520696}, {"code": "def adjust_path_and_remove_silence(audio_path)\n    # Import modules ...\n    import os\n\n    # Load sub-functions ...\n    from .find_speech import find_speech\n\n    # Load sub-functions ...\n    from .find_speech import find_speech\n\n    # Find speech ...\n    speech_start, speech_end = find_speech(\n        audio_path = audio_path,\n        force = True,\n        silence_threshold = 0.1,\n        minimum_duration = 0.01,\n        buffer_before = 0.1,\n        buffer_after = 0.1\n    )\n\n    # Return answer ...\n    return speech_start, speech_end", "CodeBleu_score": 17.47999854052422}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n\n    import os\n    import glob\n    from joblib import Parallel, delayed\n    from tqdm import tqdm\n    import librosa\n\n    # Set output directory to input directory if not specified\n    if output_dir is None:\n        output_dir = input_dir\n\n    # Copy input directory to output directory\n    os.system(f\"cp -r {input_dir} {output_dir}\")\n\n    # Find all audio files in the output directory\n    files = glob.glob(output_dir + \"**/*.\" + file_ext, recursive=True)\n\n    # Resample audio files in parallel\n    Parallel(n_jobs=n_jobs)(\n        delayed(resample_file)(file, output_sr) for file in tqdm(files)\n    )\n\n    # Print completion message\n    print(\"Resampling complete!\")\n\n", "CodeBleu_score": 27.511582252547797}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    import os\n    import librosa\n    import tqdm\n    from joblib import Parallel, delayed\n\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        os.makedirs(output_dir, exist_ok=True)\n        for root, dirs, files in os.walk(input_dir):\n            for d in dirs:\n                os.makedirs(os.path.join(output_dir, d), exist_ok=True)\n\n    def resample_file(f):\n        y, sr = librosa.load(f)\n        y_resampled = librosa.resample(y, sr, output_sr)\n        f_out = f.replace(input_dir, output_dir).replace(f.split(\".\")[-1], file_ext)\n        librosa.output.write_wav(f_out, y_resampled, output_sr)\n\n    files = []\n    for root, dirs, files in os.walk(input_dir):\n        for f in files:\n            if f.endswith(\".\" + file_ext):\n                files.append(os.path.join(root, f))\n\n    Parallel(n_jobs=n_jobs)(delayed(resample_file)(f) for f in tqdm.tqdm(files))\n    print(\"Resampling complete.\")", "CodeBleu_score": 34.66071021231033}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    import os\n    import sys\n    import shutil\n    import tqdm\n    from joblib import Parallel, delayed\n    from pydub import AudioSegment\n    from pydub.utils import mediainfo\n\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        shutil.copytree(input_dir, output_dir)\n\n    input_files = []\n    for root, dirs, files in os.walk(output_dir):\n        for file in files:\n            if file.endswith(file_ext):\n                input_files.append(os.path.join(root, file))\n\n    def resample_file(input_file):\n        output_file = input_file.replace(input_dir, output_dir)\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        sound = AudioSegment.from_file(input_file)\n        sound = sound.set_frame_rate(output_sr)\n        sound.export(output_file, format=file_ext)\n\n    Parallel(n_jobs=n_jobs)(delayed(resample_file)(input_file) for input_file in tqdm.tqdm(input_files))\n\n    print(\"Done!\")\n\n", "CodeBleu_score": 32.714848118223976}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n    model.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(data_loader):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n            eval_loss += loss.item()\n    eval_loss /= len(data_loader)\n    writer.add_scalar('Loss/Eval', eval_loss, global_step)\n    print('Validation Loss: {:.6f}'.format(eval_loss))\n    return eval_loss\n\n", "CodeBleu_score": 18.677389300161995}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n    # initialize the average loss\n    avg_loss = 0\n    # set the model to evaluation mode\n    model.eval()\n    # initialize the list of embeddings and labels\n    embeddings = []\n    labels = []\n    # loop over the data loader\n    for batch_idx, (data, target) in enumerate(data_loader):\n        # reshape data and labels to group samples of each class\n        data, target = data.view(-1, 1, 28, 28), target.view(-1, 1)\n        # dispatch data and labels to the GPU if available\n        data, target = data.to(model.device), target.to(model.device)\n        # get embeddings and predictions\n        embeddings.append(model.get_embeddings(data))\n        output = model(data)\n        # compute the loss\n        loss = criterion(output, target)\n        # update the average loss\n        avg_loss += loss.item()\n        # get labels\n        labels.append(target.cpu().detach().numpy())\n    # compute the average loss\n    avg_loss /= len(data_loader)\n    # concatenate embeddings and labels\n    embeddings = torch.cat(embeddings)\n    labels = np.concatenate(labels)\n    # log evaluation statistics\n    wandb.log({\"eval_loss\": avg_loss, \"global_step\": global_step})\n    # log UMAP plot\n    wandb.log({\"eval_umap\": wandb.plots.umap(embeddings, labels,\n                                             xaxis=\"0\", yaxis=\"1\",\n                                             num_columns=10)})\n    # return the average loss\n    return avg_loss", "CodeBleu_score": 30.685688432534995}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize running average loss\n    running_loss = 0.0\n\n    # Initialize running average number of samples\n    running_num_samples = 0\n\n    # Initialize running average number of correct predictions\n    running_num_correct_predictions = 0\n\n    # Initialize running average number of correct predictions per class\n    running_num_correct_predictions_per_class = [0] * len(data_loader.dataset.classes)\n\n    # Initialize running average number of predictions per class\n    running_num_predictions_per_class = [0] * len(data_loader.dataset.classes)\n\n    # Initialize running average number of samples per class\n    running_num_samples_per_class = [0] * len(data_loader.dataset.classes)\n\n    # Initialize running average number of correct predictions per class per batch\n    running_num_correct_predictions_per_class_per_batch = [0] * len(data_loader.dataset.classes)\n\n    # Initialize running average number of predictions per class per batch\n    running_num_predictions_per_class_per_batch = [0] * len(data_loader.dataset.classes)\n\n    # Initialize running average number of samples per class per batch\n    running_num_samples_per_class_per_batch = [0] * len(data_loader.dataset.classes)\n\n    # Initialize running average number of correct predictions per class per batch\n    running_num_correct_predictions_per_class_per_batch = [0] * len(data_loader.dataset.classes)\n\n    # Initialize running average number of predictions per class per batch\n    running_num_predictions_per_class_per_batch = [0] * len(data_loader.dataset.classes)\n\n    # Initialize running average number of samples per class per batch\n    running_num_samples_per_class_per_batch = [0] * len(data_loader.dataset.classes)\n\n    # Initialize running average number of correct predictions per class per batch\n    running_num_correct_predictions_per_class_per_batch = [0] * len(data_", "CodeBleu_score": 14.893674311330676}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n    config_name = model_name + \"_config\"\n    if model_name == \"xtts\":\n        try:\n            from . import xtts_config as config\n        except ModuleNotFoundError:\n            try:\n                from . import xtts_config as config\n            except ModuleNotFoundError:\n                from . import xtts_config as config\n    else:\n        try:\n            from . import config\n        except ModuleNotFoundError:\n            try:\n                from . import config\n            except ModuleNotFoundError:\n                from . import config\n    try:\n        config_class = getattr(config, config_name)\n    except AttributeError:\n        raise ModuleNotFoundError(\n            \"Could not find the configuration class for the model: {}\".format(model_name)\n        )\n    return config_class\n\n", "CodeBleu_score": 37.372834418221565}, {"code": "def register_config(model_name: str) -> Coqpit\n    config_class = None\n    if model_name == \"xtts\":\n        try:\n            from xtts.config import XttsConfig\n            config_class = XttsConfig\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            from config import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            from configs.config import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            from configs.configs import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            from configs.configs.config import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            from configs.configs.configs import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            from configs.configs.configs.config import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            from configs.configs.configs.configs import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            from configs.configs.configs.configs.config import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            from configs.configs.configs.configs.configs import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        try:\n            from configs.configs.configs.configs.configs.config import ModelConfig\n            config_class = ModelConfig\n        except ModuleNotFoundError:", "CodeBleu_score": 31.156723043665878}, {"code": "def register_config(model_name: str) -> Coqpit\n    if model_name == \"xtts\":\n        import xtts.config\n        config_class = xtts.config.XTTSConfig\n    else:\n        try:\n            import config\n            config_class = getattr(config, f\"{model_name.upper()}Config\")\n        except ModuleNotFoundError:\n            try:\n                import xtts.config\n                config_class = getattr(xtts.config, f\"{model_name.upper()}Config\")\n            except ModuleNotFoundError:\n                raise ModuleNotFoundError(f\"Could not find configuration class for {model_name} model\")\n    return config_class\n\n", "CodeBleu_score": 37.66343682193462}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n    if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n        with open(config_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.endswith(\".json\"):\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown configuration file extension: {config_path}\")\n\n    model_name = config_dict.pop(\"model_name\")\n    config_class = get_config_class(model_name)\n    config = config_class()\n    config.update(config_dict)\n    return config\n\n", "CodeBleu_score": 38.59709951253336}, {"code": "def load_config(config_path: str) -> Coqpit\n    if not os.path.exists(config_path):\n        raise FileNotFoundError(f\"Configuration file {config_path} does not exist.\")\n\n    config_ext = config_path.split(\".\")[-1]\n    if config_ext == \"yaml\" or config_ext == \"yml\":\n        with open(config_path, \"r\") as f:\n            config = yaml.safe_load(f)\n    elif config_ext == \"json\":\n        with open(config_path, \"r\") as f:\n            config = json.load(f)\n    else:\n        raise ValueError(f\"Unknown configuration file type: {config_ext}\")\n\n    # get the model name\n    if \"model_name\" not in config:\n        raise KeyError(\n            \"The configuration file must contain a field 'model_name' that specifies the model class name.\"\n        )\n    model_name = config[\"model_name\"]\n\n    # register the configuration class\n    config_class = register_config_class(model_name)\n\n    # create a config object and populate it with the loaded data\n    config_obj = config_class()\n    config_obj.load_dict(config)\n\n    return config_obj\n\n", "CodeBleu_score": 45.99880848066044}, {"code": "def load_config(config_path: str) -> Coqpit\n    config_path = Path(config_path)\n    if config_path.suffix == \".yaml\":\n        with open(config_path) as f:\n            config_dict = yaml.safe_load(f)\n    elif config_path.suffix == \".json\":\n        with open(config_path) as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown file type: {config_path.suffix}\")\n    model_name = config_dict.pop(\"model_name\")\n    config_cls = get_config_class(model_name)\n    config = config_cls()\n    config.load_dict(config_dict)\n    return config\n\n", "CodeBleu_score": 36.591331536274005}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n    import os\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            if contains is not None:\n                if contains in file:\n                    if validExts is not None:\n                        if os.path.splitext(file)[1] in validExts:\n                            yield os.path.join(root, file)\n                    else:\n                        yield os.path.join(root, file)\n            else:\n                if validExts is not None:\n                    if os.path.splitext(file)[1] in validExts:\n                        yield os.path.join(root, file)\n                else:\n                    yield os.path.join(root, file)\n\n", "CodeBleu_score": 29.295835878041622}, {"code": "def list_files(basePath, validExts=None, contains=None)\n    import os\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            if contains is not None and contains not in file:\n                continue\n            if validExts is None or os.path.splitext(file)[1] in validExts:\n                yield os.path.join(root, file)\n\n", "CodeBleu_score": 38.75933179851035}, {"code": "def list_files(basePath, validExts=None, contains=None)\n    import os\n    for root, dirs, files in os.walk(basePath):\n        for file in files:\n            if contains is not None and contains not in file:\n                continue\n            if validExts is not None and os.path.splitext(file)[1] not in validExts:\n                continue\n            yield os.path.join(root, file)\n", "CodeBleu_score": 38.04402417329341}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    import torch\n    import os\n    import sys\n    from tts_modules.xtts_model import XTTSModel\n    from tts_modules.utils import load_config\n\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Check if the required paths are provided\n    if xtts_checkpoint == \"\":\n        sys.exit(\"Please provide a path to a checkpoint file for the XTTS model.\")\n    if xtts_config == \"\":\n        sys.exit(\"Please provide a path to the XTTS model config file.\")\n    if xtts_vocab == \"\":\n        sys.exit(\"Please provide a path to the XTTS model vocabulary.\")\n\n    # Load the configuration\n    config = load_config(xtts_config)\n\n    # Initialize the XTTS model\n    model = XTTSModel(config)\n\n    # Load the model checkpoint and vocabulary\n    model.load_checkpoint(xtts_checkpoint)\n    model.load_vocabulary(xtts_vocab)\n\n    # Move the model to the GPU if available\n    if torch.cuda.is_available():\n        model.to_gpu()\n\n    # Print a success message\n    print(\"XTTS model loaded successfully.\")\n\n    return model\n\n", "CodeBleu_score": 45.05388196272786}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\n    import torch\n    import os\n    from xtts.model.xtts import XTTS\n\n    try:\n        torch.cuda.empty_cache()\n    except:\n        pass\n\n    if xtts_checkpoint == None:\n        return \"Please provide the path to the XTTS checkpoint.\"\n    elif xtts_config == None:\n        return \"Please provide the path to the XTTS config.\"\n    elif xtts_vocab == None:\n        return \"Please provide the path to the XTTS vocab.\"\n    else:\n        if not os.path.exists(xtts_checkpoint):\n            return \"The provided XTTS checkpoint path does not exist.\"\n        elif not os.path.exists(xtts_config):\n            return \"The provided XTTS config path does not exist.\"\n        elif not os.path.exists(xtts_vocab):\n            return \"The provided XTTS vocab path does not exist.\"\n        else:\n            print(\"Loading XTTS model...\")\n            config = torch.load(xtts_config)\n            model = XTTS(config)\n            model.load_state_dict(torch.load(xtts_checkpoint))\n            model.load_vocab(xtts_vocab)\n            if torch.cuda.is_available():\n                model.cuda()\n            print(\"XTTS model loaded.\")\n            return \"XTTS model loaded.\"\n\n", "CodeBleu_score": 44.03297605624979}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\n    import torch\n    import os\n    from xtts.model import XTTSForCTC\n    from xtts.tokenizer import XTTSTokenizer\n\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Check if paths are provided\n    if xtts_checkpoint is None:\n        return \"Please provide the path to the XTTS model checkpoint.\"\n    if xtts_config is None:\n        return \"Please provide the path to the XTTS model configuration.\"\n    if xtts_vocab is None:\n        return \"Please provide the path to the XTTS model vocabulary.\"\n\n    # Load configuration\n    print(\"Loading configuration...\")\n    config = XTTSForCTC.from_pretrained(xtts_config)\n\n    # Initialize model\n    print(\"Initializing model...\")\n    model = XTTSForCTC(config)\n\n    # Load model checkpoint\n    print(\"Loading model checkpoint...\")\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\"))\n\n    # Load vocabulary\n    print(\"Loading vocabulary...\")\n    tokenizer = XTTSTokenizer.from_pretrained(xtts_vocab)\n\n    # Move model to GPU if available\n    if torch.cuda.is_available():\n        model = model.cuda()\n\n    # Print success message\n    print(\"Model loaded successfully.\")\n\n    return model, tokenizer\n\n", "CodeBleu_score": 43.76256964443499}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    if self.hop_length is not None:\n        num_frames = int(np.ceil(num_frames * self.hop_length / self.win_length))\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n    if self.hop_length is not None:\n        num_frames = int(np.ceil(num_frames * self.win_length / self.hop_length))\n    if num_frames > x.shape[1]:\n        num_frames = x.shape[1]\n    if num_eval > num_frames:\n        num_eval = num_frames\n    if self.eval_offset is None:\n        eval_offset = np.linspace(0, num_frames - num_eval, num_eval)\n    else:\n        eval_offset = self.eval_offset\n    eval_offset = np.round(eval_offset).astype(int)\n    if self.hop_length is not None:\n        eval_offset = np.round(eval_offset * self.win_length / self.hop_length).astype(int)\n    if self.hop_length is not None:\n        eval_offset = np.round(eval_offset * self.hop_length / self.win_length).astype(int)\n    eval_offset = np.clip(eval_offset, 0, x.shape[1] - num_eval)\n    embeddings = []\n    for offset in eval_offset:\n        if self.hop_length is not None:\n            offset = int(np.round(offset * self.hop_length / self.win_length))\n        if self.hop_length is not None:\n            offset = int(np.round(offset * self.win_length / self.hop_length))\n        if self.hop_length is not None:\n            offset = int(np.round(offset * self.hop_length / self.win_length))\n        x_ = x[:, offset:offset + num_frames]\n        if self.hop_length is not None:\n            x_ = librosa.util.", "CodeBleu_score": 33.30892793309865}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    # Adjust the number of frames based on the hop length if using torch specifications\n    if self.torch_spec:\n        num_frames = int(np.ceil(num_frames / self.hop_length))\n    # Ensure the number of frames does not exceed the input length\n    num_frames = min(num_frames, len(x))\n    # Calculate offset positions for evaluation\n    offsets = self.compute_offsets(num_frames, num_eval)\n    # Extract frames at these offsets\n    batch = self.extract_batch(x, offsets)\n    # Perform inference to obtain embeddings\n    embeddings = self.inference(batch)\n    # If specified, compute the mean of the embeddings\n    if return_mean:\n        embeddings = self.compute_mean(embeddings)\n    # Optionally normalize the embeddings with L2 norm\n    if l2_norm:\n        embeddings = self.l2_norm(embeddings)\n    return embeddings\n\n", "CodeBleu_score": 28.7458917394619}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    # Adjust number of frames based on hop length\n    num_frames = (num_frames - self.hop_length) // self.hop_length\n    num_frames = min(num_frames, (x.shape[1] - self.hop_length) // self.hop_length)\n    # Compute offsets for evaluation\n    offsets = np.arange(0, num_frames * self.hop_length, self.hop_length)\n    offsets = np.expand_dims(offsets, axis=1)\n    # Extract frames at offsets, concatenate, and perform inference\n    x_batch = [x[:, offset:offset + self.hop_length * num_eval] for offset in offsets]\n    x_batch = np.concatenate(x_batch, axis=0)\n    embeddings = self.inference(x_batch)\n    # If specified, compute mean of embeddings and return\n    if return_mean:\n        embeddings = np.reshape(embeddings, (num_frames, num_eval, -1))\n        embeddings = np.mean(embeddings, axis=1)\n    # If specified, normalize embeddings with L2 norm\n    if l2_norm:\n        embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n    return embeddings", "CodeBleu_score": 31.75545099338901}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.18641475692186}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            conv1x1(self.inplanes, planes * block.expansion, stride),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 75.12404610027407}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = [0] * num_classes_in_batch\n    for embedding in embeddings:\n        num_utterances_per_class[embedding[\"class_id\"]] += 1\n\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        embeddings = [embedding for embedding in embeddings if embedding[\"class_id\"] < 10]\n\n    # Transform the embeddings\n    reducer = umap.UMAP()\n    transformed = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig, ax = plt.subplots(figsize=(10, 10))\n    scatter = ax.scatter(transformed[:, 0], transformed[:, 1], s=50, c=embeddings[:, 0], cmap=\"tab20\")\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    plt.colorbar(scatter, ax=ax, fraction=0.046, pad=0.04)\n\n    # Save the plot as \"umap\" and return it as a figure\n    fig.savefig(\"umap\")\n    return fig", "CodeBleu_score": 44.467770148851926}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    import numpy as np\n    import umap\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.manifold import TSNE\n\n    # Calculate the number of utterances per class\n    num_utterances_per_class = np.sum(num_classes_in_batch, axis=0)\n\n    # Limit the number of classes to 10 if necessary\n    if len(num_utterances_per_class) > 10:\n        num_utterances_per_class = num_utterances_per_class[:10]\n        embeddings = embeddings[:, :10]\n\n    # Transform the embeddings\n    embeddings = umap.UMAP(n_components=2).fit_transform(embeddings)\n\n    # Create a scatter plot\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    cmap = sns.color_palette(n_colors=len(num_utterances_per_class))\n    ax.scatter(embeddings[:, 0], embeddings[:, 1], c=cmap)\n    ax.set_title(\"UMAP projection\")\n    ax.set_aspect(\"equal\")\n    plt.savefig(\"umap.png\")\n    return fig", "CodeBleu_score": 39.94895974573326}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    # Limit the number of classes to 10 if necessary\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n\n    # Calculate the number of utterances per class\n    num_utts_per_class = [embeddings[embeddings['class_id'] == i].shape[0] for i in range(num_classes_in_batch)]\n\n    # Calculate the total number of utterances\n    total_num_utts = sum(num_utts_per_class)\n\n    # Calculate the fraction of utterances per class\n    frac_utts_per_class = [num_utts_per_class[i] / total_num_utts for i in range(num_classes_in_batch)]\n\n    # Calculate the weight for each class\n    class_weights = [frac_utts_per_class[i] / (frac_utts_per_class[i] + 0.01) for i in range(num_classes_in_batch)]\n\n    # Transform the embeddings with UMAP\n    reducer = umap.UMAP()\n    embedding_reduced = reducer.fit_transform(embeddings)\n\n    # Create a scatter plot with colors representing different classes\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    for i in range(num_classes_in_batch):\n        ax.scatter(embedding_reduced[embeddings['class_id'] == i, 0], embedding_reduced[embeddings['class_id'] == i, 1], s=10, c=np.ones(num_utts_per_class[i]) * i, cmap=plt.cm.jet, alpha=class_weights[i])\n    ax.set_title('UMAP projection')\n    ax.set_aspect('equal', 'datalim')\n    plt.savefig('umap.png')\n    return fig", "CodeBleu_score": 44.25248041072797}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n    cos_sim_matrix = []\n    for speaker in dvecs:\n        utterances = dvecs[speaker]\n        for utterance in utterances:\n            new_centroids = np.delete(centroids, speaker, axis=0)\n            new_centroids = np.array(new_centroids)\n            sim_vec = np.dot(new_centroids, utterance)\n            sim_vec = np.clip(sim_vec, 0.001, 1)\n            cos_sim_matrix.append(sim_vec)\n    return np.array(cos_sim_matrix)\n\n", "CodeBleu_score": 28.495754094677622}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n    # initialize cosine similarity matrix\n    cos_sim_matrix = np.zeros((len(dvecs), len(centroids)))\n\n    # iterate over each speaker\n    for i, speaker in enumerate(self.speakers):\n        # iterate over each utterance of the speaker\n        for j, utterance in enumerate(self.utterances[speaker]):\n            # calculate new centroids excluding current utterance\n            new_centroids = np.delete(centroids, i, axis=0)\n\n            # calculate cosine similarity between current utterance and new centroids\n            cos_sim_matrix[i, :] = np.maximum(\n                np.dot(dvecs[i], new_centroids.T) / (np.linalg.norm(dvecs[i]) * np.linalg.norm(new_centroids, axis=1)),\n                0,\n            )\n\n    # return cosine similarity matrix\n    return cos_sim_matrix\n\n", "CodeBleu_score": 28.81441292591488}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n    n_speakers = len(dvecs)\n    n_utts = [len(dvecs[i]) for i in range(n_speakers)]\n    n_feats = len(dvecs[0][0])\n    cos_sim_matrix = np.zeros((sum(n_utts), n_speakers))\n    start = 0\n    for i in range(n_speakers):\n        end = start + n_utts[i]\n        centroids_i = centroids[i]\n        for j in range(n_utts[i]):\n            # Compute new centroids excluding the current utterance\n            centroids_i_j = np.delete(centroids_i, j, axis=0)\n            # Compute cosine similarity between the current utterance and the new centroids\n            cos_sim_j = np.dot(dvecs[i][j], centroids_i_j.T) / (np.linalg.norm(dvecs[i][j]) * np.linalg.norm(centroids_i_j, axis=1))\n            # Clamp values to avoid small values\n            cos_sim_j = np.clip(cos_sim_j, 1e-7, 1.0)\n            # Concatenate cosine similarities\n            cos_sim_matrix[start + j] = cos_sim_j\n        start = end\n    return cos_sim_matrix", "CodeBleu_score": 30.161815315901087}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)    loss_vecs = []\n    for i in range(dvecs.shape[0]):\n        loss_vec = -F.log_softmax(cos_sim_matrix[i], dim=0)\n        loss_vecs.append(loss_vec)\n    loss_vecs = torch.stack(loss_vecs)\n    return loss_vecs\n\n  def embed_loss_softmax_avg(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using softmax.\\nIt iterates over the embeddings, computing the negative log softmax of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\\nThe average loss is also returned.\"\"\"\n    loss_vecs = []\n    for i in range(dvecs.shape[0]):\n        loss_vec = -F.log_softmax(cos_sim_matrix[i], dim=0)\n        loss_vecs.append(loss_vec)\n    loss_vecs = torch.stack(loss_vecs)\n    loss_vecs_avg = torch.mean(loss_vecs)\n    return loss_vecs, loss_vecs_avg\n\n  def embed_loss_softmax_avg_batch(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using softmax.\\nIt iterates over the embeddings, computing the negative log softmax of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\\nThe average loss is also returned.\\nThe function also returns the loss for each batch.\"\"\"\n    loss_vecs = []\n    loss_vecs_batch = []\n    for i in range(dvecs.shape[0]):\n        loss_vec = -F.log_softmax(cos_sim_matrix[i], dim=0)\n        loss_vecs.append(loss_vec)\n        loss_vecs_batch.append(loss_vec.mean())\n    loss_vecs =", "CodeBleu_score": 33.93679684748704}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)    loss_vector = []\n    for i in range(dvecs.shape[0]):\n        #print(dvecs[i])\n        #print(cos_sim_matrix[i])\n        #print(cos_sim_matrix[i].shape)\n        #print(dvecs[i].shape)\n        #print(dvecs[i].shape[0])\n        #print(cos_sim_matrix[i].shape[0])\n        #print(cos_sim_matrix[i].shape[1])\n        #print(cos_sim_matrix[i].shape[1]-dvecs[i].shape[0])\n        #print(cos_sim_matrix[i][0:dvecs[i].shape[0]])\n        #print(cos_sim_matrix[i][dvecs[i].shape[0]:])\n        #print(cos_sim_matrix[i][dvecs[i].shape[0]:].shape)\n        #print(cos_sim_matrix[i][dvecs[i].shape[0]:].shape[0])\n        #print(cos_sim_matrix[i][dvecs[i].shape[0]:].shape[1])\n        #print(cos_sim_matrix[i][dvecs[i].shape[0]:].shape[1]-dvecs[i].shape[0])\n        #print(cos_sim_matrix[i][dvecs[i].shape[0]:][0:dvecs[i].shape[0]])\n        #print(cos_sim_matrix[i][dvecs[i].shape[0]:][dvecs[i].shape[0]:])\n        #print(cos_sim_matrix[i][dvecs[i].shape[0]:][dvecs[i].shape[0]:].shape)\n        #print(cos_sim_matrix[i][dvecs[i].shape[0]:][dvecs[i].shape[0]:].shape[0])\n        #print(cos_sim_matrix[i][dvecs[i].shape[0]:][dvecs[i].shape[0]:].shape[1])\n        #print", "CodeBleu_score": 11.823955663971361}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n    # Initialize the loss tensor\n    loss_tensor = torch.tensor(0.0).to(self.device)\n\n    # Iterate over the embeddings\n    for dvec in dvecs:\n\n      # Compute the softmax of the cosine similarity matrix\n      softmax_sim_matrix = F.softmax(cos_sim_matrix, dim=1)\n\n      # Compute the loss for the current embedding\n      loss_tensor = torch.stack((loss_tensor, self.loss_function(softmax_sim_matrix, dvec)))\n\n    # Return the loss tensor\n    return loss_tensor\n\n  def embed_loss_nce(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using the noise contrastive estimation loss.\\nIt iterates over the embeddings, computing the noise contrastive estimation loss for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n\n    # Initialize the loss tensor\n    loss_tensor = torch.tensor(0.0).to(self.device)\n\n    # Iterate over the embeddings\n    for dvec in dvecs:\n\n      # Compute the noise contrastive estimation loss for the current embedding\n      loss_tensor = torch.stack((loss_tensor, self.loss_function(cos_sim_matrix, dvec)))\n\n    # Return the loss tensor\n    return loss_tensor\n\n  def embed_loss_triplet(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using the triplet loss.\\nIt iterates over the embeddings, computing the triplet loss for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n\n    # Initialize the loss tensor\n    loss_tensor = torch.tensor(0.0).to(self.device)\n\n    # Iterate over the embeddings\n    for dvec in dvecs:\n\n      # Compute the triplet loss for the current embedding\n      loss_tensor = torch.stack((loss_tensor, self.loss_", "CodeBleu_score": 19.141199835237774}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print(cos_sim_matrix)\n    # print(dvecs)\n    # print(dvecs.shape)\n    # print(cos_sim_matrix.shape)\n    # print", "CodeBleu_score": 1.3221153846153846}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    embed_loss = []\n    for i in range(len(dvecs)):\n        cos_sim_matrix_i = cos_sim_matrix[i]\n        cos_sim_matrix_i[i] = -1\n        embed_loss.append(self.contrast_loss(cos_sim_matrix_i))\n    embed_loss = torch.stack(embed_loss)\n    return embed_loss\n\n", "CodeBleu_score": 21.292765231871257}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    loss = []\n    for dvec in dvecs:\n        contrast_loss = 1 - torch.sigmoid(cos_sim_matrix[dvec])\n        contrast_loss[dvec] = 0\n        loss.append(contrast_loss)\n    return torch.stack(loss)\n", "CodeBleu_score": 15.574967700920972}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n    num_speakers, num_utts_per_speaker, dvec_feats = x.shape\n    assert num_utts_per_speaker >= 2, \"GE2E requires at least two utterances per speaker\"\n    # Compute centroids\n    centroids = x.mean(axis=1)\n    # Compute similarities between all pairs of embeddings\n    sim_mat = F.cosine_similarity(x.reshape(num_speakers * num_utts_per_speaker, dvec_feats),\n                                  centroids.reshape(num_speakers, dvec_feats).expand(num_speakers * num_utts_per_speaker, dvec_feats))\n    # Reshape to a matrix of size (num_speakers, num_utts_per_speaker)\n    sim_mat = sim_mat.reshape(num_speakers, num_utts_per_speaker)\n    # Shift the values of the similarity matrix\n    # to be in range [-1.0, 1.0]\n    sim_mat = sim_mat - 0.5\n    # Scale the similarity matrix\n    # to be in range [-1.0, 1.0]\n    sim_mat = sim_mat / 0.5\n    # Compute the GE2E loss\n    loss = 1.0 - sim_mat.diag()\n    loss = loss.clamp(min=1e-8).mean()\n    return loss\n\n", "CodeBleu_score": 35.13031812200149}, {"code": "def forward(self, x, _label=None)\n    num_speakers, num_utts_per_speaker, dvec_feats = x.shape\n    if num_utts_per_speaker < 2:\n        raise ValueError(\n            \"GE2ELoss requires at least two utterances per speaker. \"\n            \"Consider using a different loss.\"\n        )\n\n    # Compute centroids of each speaker\n    centroids = torch.mean(x, dim=1)\n\n    # Compute similarity matrix\n    similarity_matrix = torch.matmul(x, centroids.t())\n\n    # Scale and shift similarity matrix to be between [-1, 1]\n    scaled_similarity_matrix = (similarity_matrix + 1) / 2\n\n    # Compute loss\n    loss = 1 - scaled_similarity_matrix\n    loss = loss[loss != 0]  # Filter out 0 entries (i.e. where the speaker is the same)\n    loss = torch.clamp(loss, min=1e-8)\n    loss = -torch.log(loss)\n    loss = torch.mean(loss)\n\n    return loss", "CodeBleu_score": 33.17507666027792}, {"code": "def forward(self, x, _label=None)\n    num_speakers = x.shape[0]\n    num_utts_per_speaker = x.shape[1]\n    dvec_feats = x.shape[2]\n\n    if num_utts_per_speaker < 2:\n        raise ValueError(\"GE2E requires at least two utterances per speaker.\")\n\n    # Compute centroids.\n    centroids = torch.mean(x, dim=1)\n\n    # Compute similarity matrix.\n    sim_matrix = torch.matmul(x, centroids.t())\n\n    # Scale and shift similarity matrix.\n    sim_matrix = sim_matrix * 100\n    sim_matrix = sim_matrix - 50\n\n    # Compute embedding loss.\n    loss = torch.mean(torch.logsumexp(sim_matrix, dim=1))\n\n    return loss", "CodeBleu_score": 36.41188575940249}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])\n    voices = {}\n    for voice_dir in VOICE_DIRS + extra_voice_dirs:\n        for subdir in os.listdir(voice_dir):\n            if subdir.startswith(\".\"):\n                continue\n            subdir_path = os.path.join(voice_dir, subdir)\n            if not os.path.isdir(subdir_path):\n                continue\n            voice_files = []\n            for file in os.listdir(subdir_path):\n                if file.endswith(\".npz\"):\n                    voice_files.append(os.path.join(subdir_path, file))\n            if not voice_files:\n                for file in os.listdir(subdir_path):\n                    if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                        voice_files.append(os.path.join(subdir_path, file))\n            if voice_files:\n                voices[subdir] = voice_files\n    return voices\n\n", "CodeBleu_score": 29.04375566197318}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n\n    voices = {}\n    for voice_dir in extra_voice_dirs:\n        for dirpath, dirnames, filenames in os.walk(voice_dir):\n            voice_files = []\n            for filename in filenames:\n                if filename.endswith(\".npz\") or filename.endswith(\".wav\") or filename.endswith(\".mp3\"):\n                    voice_files.append(os.path.join(dirpath, filename))\n            if voice_files:\n                voices[dirpath.replace(voice_dir, \"\").lstrip(\"/\")] = voice_files\n\n    return voices", "CodeBleu_score": 26.13892587217291}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])\n    voices = {}\n    voice_dirs = [VOICE_DIR] + extra_voice_dirs\n    for voice_dir in voice_dirs:\n        for subdir, dirs, files in os.walk(voice_dir):\n            voice_files = []\n            for file in files:\n                if file.endswith(\".npz\") or file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voice_files.append(os.path.join(subdir, file))\n            if voice_files:\n                voices[subdir] = voice_files\n    return voices\n\n", "CodeBleu_score": 26.629137265840892}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n    if voice == \"random\":\n        return None, None, None\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) == 0:\n        raise RuntimeError(\"No voice file found for voice %s\" % voice)\n    elif len(voice_paths) > 1:\n        raise RuntimeError(\"Multiple voice files found for voice %s\" % voice)\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        data = np.load(voice_path)\n        return data[\"f0\"], data[\"sp\"], data[\"ap\"]\n    else:\n        import soundfile as sf\n        wav, sr = sf.read(voice_path)\n        if sr != model.fs:\n            raise RuntimeError(\"Voice file %s has sampling rate %d, but model sampling rate is %d\" % (voice_path, sr, model.fs))\n        f0, sp, ap = model.world_synth(wav)\n        np.savez_compressed(voice_path + \".npz\", f0=f0, sp=sp, ap=ap)\n        return f0, sp, ap\n", "CodeBleu_score": 41.94821548234413}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n\n    # get voices\n    voices = get_voices(model, voice, extra_voice_dirs)\n\n    # handle random voices\n    if voice == \"random\":\n        return None, None, None\n\n    # check if there are multiple paths\n    if len(voices) > 1:\n        raise RuntimeError(f\"Multiple paths for voice {voice}:\\n{voices}\")\n\n    # get voice path\n    voice_path = voices[0]\n\n    # check if the voice exists\n    if not os.path.exists(voice_path):\n        raise FileNotFoundError(f\"Voice {voice} not found at {voice_path}\")\n\n    # handle npz files\n    if voice_path.endswith(\".npz\"):\n        return np.load(voice_path)\n\n    # generate npz file\n    npz_path = voice_path.replace(\".wav\", \".npz\")\n    if not os.path.exists(npz_path):\n        generate_npz(voice_path, npz_path)\n\n    # recursively load the voice\n    return load_voice(model, voice, extra_voice_dirs)\n\n", "CodeBleu_score": 47.41449498620744}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])\n    voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise RuntimeError(f\"Multiple voices found for {voice}.\")\n    elif len(voice_paths) == 0:\n        raise FileNotFoundError(f\"No voices found for {voice}.\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        return load(voice_path)\n    else:\n        voice_data = load_voice(model, \"random\", extra_voice_dirs)\n        voice_data.audio = voice_path\n        return voice_data\n\n", "CodeBleu_score": 29.891143252841406}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    # Extract the tensor's dimensions\n    batch_size, num_heads, seq_len, _ = pos_score.size()\n\n    # Create a zero tensor with an additional dimension\n    zero_tensor = torch.zeros(\n        (batch_size, num_heads, seq_len, 1), device=pos_score.device\n    )\n\n    # Concatenate the zero tensor with the original position score tensor along the last dimension\n    concat_tensor = torch.cat((zero_tensor, pos_score), dim=-1)\n\n    # Reshape the concatenated tensor\n    reshaped_tensor = concat_tensor.view(\n        batch_size, num_heads, 2 * seq_len, -1\n    )\n\n    # Slice the reshaped tensor\n    shifted_tensor = reshaped_tensor[:, :, seq_len - 1 :, :]\n\n    return shifted_tensor\n\n", "CodeBleu_score": 30.214135994613812}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n\n    # Get the position score tensor's dimensions\n    batch_size, num_heads, seq_length, _ = pos_score.size()\n\n    # Create a zero tensor with an additional dimension\n    zero_tensor = torch.zeros((batch_size, num_heads, seq_length, 1), device=self.device)\n\n    # Concatenate the zero tensor with the original position score tensor\n    concat_tensor = torch.cat((zero_tensor, pos_score), dim=-1)\n\n    # Reshape the concatenated tensor\n    reshaped_tensor = concat_tensor.view(batch_size, num_heads, 2 * seq_length)\n\n    # Slice the tensor to achieve the relative shift\n    shifted_tensor = reshaped_tensor[:, :, seq_length - 1:2 * seq_length - 1]\n\n    return shifted_tensor\n\n", "CodeBleu_score": 32.08840085197046}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    batch_size, num_heads, seq_len, _ = pos_score.size()\n    zero_tensor = torch.zeros((batch_size, num_heads, seq_len, 1), device=self.device)\n    pos_score_padded = torch.cat([zero_tensor, pos_score], dim = -1)\n    pos_score_padded = pos_score_padded.reshape(batch_size, num_heads, seq_len*2, -1)\n    pos_score_shifted = pos_score_padded[:, :, self.rel_pos_shift:, :].reshape(batch_size, num_heads, seq_len, 2*self.rel_pos_shift)\n    pos_score_shifted = pos_score_shifted[:, :, :, :self.rel_pos_shift]\n    return pos_score_shifted\n", "CodeBleu_score": 32.72220124725913}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n\n    # Extract the context up to the specified length\n    context = context[:, :max(lens), :]\n\n    # Pad the contexts to create a consistent batch size\n    context = tf.pad(context, [[0, 0], [0, max(lens) - min(lens)], [0, 0]])\n\n    # Apply convolutional layers with ReLU activation and dropout\n    for i in range(self.num_layers):\n        context = tf.layers.conv1d(context, self.num_filters, self.kernel_size,\n                                   activation=tf.nn.relu,\n                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                   name='conv_layer' + str(i))\n        context = tf.layers.dropout(context, rate=self.dropout_rate,\n                                    training=self.training)\n\n    return context\n\n", "CodeBleu_score": 30.922465635398698}, {"code": "def run_padded_sequence(self, context, lens)\n    # Extract the context up to the specified length\n    context = context[:, :lens.max()]\n    # Apply the convolutional layers\n    for conv in self.convs:\n        context = conv(context)\n        context = nn.functional.relu(context)\n        context = nn.functional.dropout(context, p=0.5)\n    # Pad the contexts to create a consistent batch size\n    context = nn.utils.rnn.pad_sequence(context.unbind(), batch_first=True)\n    return context\n\n", "CodeBleu_score": 29.134005827655262}, {"code": "def run_padded_sequence(self, context, lens)\n\n    # Extract context up to the specified length\n    context = context[:, :lens]\n\n    # Apply convolutional layers with ReLU activation and dropout\n    context = self.conv1(context)\n    context = self.relu(context)\n    context = self.dropout(context)\n    context = self.conv2(context)\n    context = self.relu(context)\n    context = self.dropout(context)\n\n    # Pad the sequence to create a consistent batch size\n    context = torch.nn.utils.rnn.pad_sequence(context, batch_first=True)\n\n    return context", "CodeBleu_score": 26.543147015732536}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n\n    # sort the lengths and indices\n    sorted_lengths, sorted_idx = torch.sort(lens, descending=True)\n    sorted_idx = sorted_idx.long()\n    sorted_context = context[sorted_idx]\n\n    # pack the sequences\n    packed_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lengths, batch_first=True)\n\n    # apply the function\n    packed_out = fn(packed_context)\n\n    # unpack the sequences\n    unpacked_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n\n    # unsort the results\n    _, original_idx = torch.sort(sorted_idx)\n    unsorted_out = unpacked_out[original_idx]\n    return unsorted_out", "CodeBleu_score": 18.233247370136787}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n    # sort lengths and indices\n    sorted_lens, indices = torch.sort(lens, descending=True)\n    _, desorted_indices = torch.sort(indices)\n    # pack context\n    packed_context = pack_padded_sequence(context, sorted_lens, batch_first=True)\n    # apply function\n    packed_output, _ = fn(packed_context)\n    # unpack output\n    output, _ = pad_packed_sequence(packed_output, batch_first=True)\n    # unsort output\n    output = output[desorted_indices]\n    return output\n\n", "CodeBleu_score": 13.108988892463286}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n    sorted_lens, indices = torch.sort(lens, descending=True)\n    sorted_context = context[indices]\n    packed_context = nn.utils.rnn.pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    packed_output, _ = fn(packed_context)\n    output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n    _, inverse_indices = torch.sort(indices)\n    output = output[inverse_indices]\n    return output\n\n", "CodeBleu_score": 16.279812364373882}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_parameterization()\n    for block in self.conv_blocks:\n        block.remove_parameterization()\n", "CodeBleu_score": 35.69588762472886}, {"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for conv_block in self.conv_blocks:\n        conv_block.remove_weight_norm()", "CodeBleu_score": 27.580633709689707}, {"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n\n", "CodeBleu_score": 33.51046275391997}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)\n    mx_name = mx_name.lower()\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_', 'se-')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_', 'se-')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception-v3')\n    return 'gluon_' + mx_name\n", "CodeBleu_score": 46.74503741580066}, {"code": "def map_mx_to_torch_model(mx_name)\n    mx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_', 'se-')\n    mx_name = mx_name.replace('senet_', 'se-')\n    mx_name = mx_name.replace('inceptionv3', 'inception-v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 30.930007270767117}, {"code": "def map_mx_to_torch_model(mx_name)\n    mx_name = mx_name.lower()\n    mx_name = mx_name.replace('_', '-')\n    mx_name = mx_name.replace('se_', 'se-')\n    mx_name = mx_name.replace('senet_', 'seresnet-')\n    mx_name = mx_name.replace('inceptionv3', 'inception-v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 36.01193675699379}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n\n    num_samples_per_worker = int(\n        math.ceil(self.num_samples / self.num_replicas))\n\n    if self.train and self.batch_size:\n        num_samples_per_worker = int(\n            math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size)\n\n    return num_samples_per_worker\n\n", "CodeBleu_score": 27.999272304100543}, {"code": "def _num_samples_per_worker(self)\n    if self.repeats > 1:\n        samples_per_worker = self.total_samples // self.replicas\n    else:\n        samples_per_worker = self.total_samples // self.num_workers\n    if self.training:\n        samples_per_worker = math.ceil(samples_per_worker)\n    if self.batch_size > 1:\n        samples_per_worker = int(math.ceil(samples_per_worker / self.batch_size) * self.batch_size)\n    return samples_per_worker", "CodeBleu_score": 41.465144358370324}, {"code": "def _num_samples_per_worker(self)\n    num_samples = self._num_samples\n    num_workers = self._num_workers\n    num_replicas = self._num_replicas\n    if num_replicas > 1:\n        num_samples = num_samples * num_replicas\n    if num_workers > 1:\n        num_samples = num_samples // num_workers\n        num_samples = num_samples // self._repeats\n        num_samples = num_samples * self._repeats\n    if self._batch_size > 1:\n        num_samples = (num_samples + self._batch_size - 1) // self._batch_size\n        num_samples = num_samples * self._batch_size\n    return int(num_samples)", "CodeBleu_score": 28.740555651488847}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    if class_to_idx is None:\n        class_to_idx = {}\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder):\n        for file in files:\n            if leaf_name_only:\n                name, ext = os.path.splitext(file)\n                if ext.lower() in types:\n                    filenames.append(os.path.join(root, file))\n                    labels.append(name)\n            else:\n                name = file\n                if name.lower().endswith(tuple(types)):\n                    filenames.append(os.path.join(root, file))\n                    labels.append(name)\n\n    if sort:\n        filenames.sort()\n        labels.sort()\n\n    if not class_to_idx:\n        unique_labels = set(labels)\n        for label in unique_labels:\n            class_to_idx[label] = len(class_to_idx)\n\n    targets = [class_to_idx[label] for label in labels]\n\n    return filenames, targets, class_to_idx\n\n", "CodeBleu_score": 42.86979210481532}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\n    if types is None:\n        types = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n    elif isinstance(types, str):\n        types = [types]\n\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder):\n        for file in files:\n            if leaf_name_only:\n                name = os.path.basename(file)\n            else:\n                name = os.path.join(root, file)\n\n            if name.lower().endswith(tuple(types)):\n                labels.append(os.path.basename(root))\n                filenames.append(name)\n\n    if class_to_idx is None:\n        if sort:\n            labels, filenames = zip(*sorted(zip(labels, filenames)))\n        class_to_idx = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n    else:\n        if sort:\n            labels, filenames = zip(*sorted(zip(labels, filenames), key=lambda x: class_to_idx[x[0]]))\n\n    targets = [class_to_idx[label] for label in labels]\n\n    return filenames, targets, class_to_idx\n\n", "CodeBleu_score": 44.14961616060854}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n\n    if types is None:\n        types = set(['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif'])\n    elif isinstance(types, str):\n        types = {types}\n    else:\n        types = set(types)\n\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder):\n        for file in files:\n            if file.lower().endswith(tuple(types)):\n                if leaf_name_only:\n                    label = os.path.basename(os.path.normpath(root))\n                else:\n                    label = root[len(folder):].replace(os.path.sep, '_')\n                labels.append(label)\n                filenames.append(os.path.join(root, file))\n\n    if class_to_idx is None:\n        if sort:\n            labels, filenames = zip(*sorted(zip(labels, filenames)))\n        class_to_idx = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n    else:\n        if sort:\n            labels, filenames = zip(*sorted(zip(labels, filenames), key=lambda x: class_to_idx[x[0]]))\n\n    class_to_idx = {label: class_to_idx[label] for label in labels}\n\n    targets = [class_to_idx[label] for label in labels]\n\n    return filenames, targets, class_to_idx\n\n", "CodeBleu_score": 47.26127040047255}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n    import torch\n    import os\n    import wds\n\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return int(os.getenv(\"WDS_RANK\", 0)) + 1\n\n", "CodeBleu_score": 55.560636472671064}, {"code": "def pytorch_worker_seed()\n    import torch\n    from wds_pytorch_worker_info import get_worker_info\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        return worker_info.seed\n    else:\n        return torch.initial_seed() % 2**32\n\n", "CodeBleu_score": 74.49518945919672}, {"code": "def pytorch_worker_seed()\n    import torch\n    import os\n    import random\n    import socket\n\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed % 2 ** 32\n    else:\n        seed = int.from_bytes(socket.gethostname().encode(), \"little\") + os.getpid()\n        seed = (seed + _get_wds_rank()) % 2 ** 32\n    random.seed(seed)\n    return seed\n\n", "CodeBleu_score": 43.20014331568166}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode == \"L\":\n        lut = [(i if i < thresh else i + add) for i in range(256)]\n        return img.point(lut)\n    elif img.mode == \"RGB\":\n        lut = [(i if i < thresh else i + add) for i in range(256)]\n        return img.point(lut, mode=\"RGB\")\n    else:\n        return img", "CodeBleu_score": 41.08686069902671}, {"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode in (\"L\", \"RGB\"):\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == \"RGB\":\n            img = img.point(lut)\n        else:\n            img = img.point(lut, \"L\")\n    return img\n\n", "CodeBleu_score": 64.42474691322113}, {"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode == \"RGB\":\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == \"RGB\":\n            img = img.point(lut)\n        return img\n    elif img.mode == \"L\":\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, i + add))\n            else:\n                lut.append(i)\n        if img.mode == \"L\":\n            img = img.point(lut)\n        return img\n    else:\n        return img\n\n", "CodeBleu_score": 60.266748154461446}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names = []\n    transform_probs = []\n    for transform_name, prob in transforms.items():\n        transform_names.append(transform_name)\n        transform_probs.append(prob)\n    total_prob = sum(transform_probs)\n    norm_probs = [prob / total_prob for prob in transform_probs]\n    return transform_names, norm_probs\n\n", "CodeBleu_score": 28.880431247498077}, {"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names = list(transforms.keys())\n    probs = list(transforms.values())\n    probs = [p / sum(probs) for p in probs]\n    return transform_names, probs\n\n", "CodeBleu_score": 26.07782386501945}, {"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names = []\n    transform_probs = []\n    for name, prob in transforms.items():\n        transform_names.append(name)\n        transform_probs.append(prob)\n\n    transform_probs = [x / sum(transform_probs) for x in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 29.86753281798031}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n    # Initialize the mixed image\n    mixed_img = np.zeros(img.shape, dtype=np.float32)\n    # Iterate through the mixing weights\n    for w in mixing_weights:\n        # Apply the sequence of augmentations to the image\n        aug_img = self.apply_sequence(img)\n        # Accumulate the augmented image\n        mixed_img += w * aug_img\n    # Clip the mixed image to valid pixel values\n    mixed_img = np.clip(mixed_img, 0, 255)\n    # Convert the mixed image back to an image\n    mixed_img = mixed_img.astype(np.uint8)\n    # Blend the original image with the mixed image\n    blended_img = cv2.addWeighted(img, 1 - m, mixed_img, m, 0)\n    # Return the blended image\n    return blended_img\n\n", "CodeBleu_score": 22.94413528956972}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n    img_mixed = np.zeros_like(img)\n    for w in mixing_weights:\n        img_aug = img.copy()\n        for _ in range(m):\n            img_aug = self.augment_image(img_aug)\n        img_mixed += w * img_aug\n    img_mixed = np.clip(img_mixed, 0, 255).astype(np.uint8)\n    img_mixed = Image.fromarray(img_mixed)\n    img_blended = Image.blend(img, img_mixed, 0.5)\n    return img_blended\n\n", "CodeBleu_score": 26.685893141691018}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed_image = np.zeros(img.shape, dtype=np.float32)\n    for weight in mixing_weights:\n        mixed_image += weight * self._apply_sequence(img)\n    mixed_image = np.clip(mixed_image, 0, 255).astype(np.uint8)\n    mixed_image = Image.fromarray(mixed_image)\n    return Image.blend(img, mixed_image, m)\n\n", "CodeBleu_score": 22.70567438606474}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n    if not isinstance(x, (tuple, list)):\n        x = [x] * n\n        return x\n    elif len(x) != n:\n        print('The number of input channels is not equal to the required number of channels. The mean of the input channels is used.')\n        x = [sum(x)/len(x)] * n\n        return x\n    else:\n        assert len(x) == n, 'The number of input channels is not equal to the required number of channels.'\n        return x", "CodeBleu_score": 41.4737817284461}, {"code": "def adapt_to_chs(x, n)\n    if not isinstance(x, (list, tuple)):\n        x = [x] * n\n        return x\n    elif len(x) == n:\n        return x\n    elif len(x) == 1:\n        x = [x[0]] * n\n        return x\n    else:\n        print(\n            \"The number of channels in the input does not match the required number of channels. The input will be adapted by repeating the mean of the input channels.\"\n        )\n        x = [np.mean(x)] * n\n        return x\n\n", "CodeBleu_score": 35.61869225304248}, {"code": "def adapt_to_chs(x, n)\n    if not isinstance(x, (tuple, list)):\n        x = [x] * n\n    elif len(x) != n:\n        print(\"Warning: The number of channels in the input tensor does not match the required number of channels. The input tensor will be averaged over all channels.\")\n        x = [sum(x) / len(x)] * n\n    else:\n        assert len(x) == n, \"The number of channels in the input tensor does not match the required number of channels.\"\n    return x\n", "CodeBleu_score": 39.05908333335299}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n    import numpy as np\n    import random\n    import torch\n    import os\n    import sys\n    import pickle\n    import logging\n    from . import utils\n\n    worker_info = torch.utils.data.get_worker_info()\n\n    if worker_info is None:\n        logging.info('worker_info is None.')\n    else:\n        logging.info('worker_info: %s', worker_info)\n\n    if worker_info is not None:\n        if worker_info.id != worker_id:\n            raise ValueError(f'worker_info.id is {worker_info.id}, but worker_id is {worker_id}.')\n\n        if worker_seeding == 'all':\n            np.random.seed(worker_info.seed % np.iinfo(np.int32).max)\n        elif callable(worker_seeding):\n            worker_seeding(worker_info.seed)\n        else:\n            raise ValueError('worker_seeding must be a callable or \"all\".')\n\n        logging.info('worker_info.seed: %s', worker_info.seed)\n        logging.info('np.random.get_state(): %s', np.random.get_state())\n        logging.info('random.getstate(): %s', random.getstate())\n        logging.info('torch.get_rng_state(): %s', torch.get_rng_state())\n\n", "CodeBleu_score": 47.137821209325914}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n    import os\n    import numpy as np\n    import torch\n    import random\n\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        if worker_info.id != worker_id:\n            raise RuntimeError(f'Worker ID is {worker_id}, but it was called with worker ID {worker_info.id}.')\n        seed = worker_info.seed\n        if worker_seeding == 'all':\n            seed = seed % 2**32\n            np.random.seed(seed)\n            random.seed(seed)\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed(seed)\n        elif callable(worker_seeding):\n            seed = worker_seeding(worker_info)\n            np.random.seed(seed)\n            random.seed(seed)\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed(seed)\n        else:\n            raise RuntimeError('worker_seeding must be a callable or \"all\".')\n    else:\n        raise RuntimeError('This function must be called within a worker process.')\n\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n", "CodeBleu_score": 46.37664254448157}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n    import os\n    import numpy as np\n    import random\n    import torch\n    import torch.utils.data\n    import torch.multiprocessing as mp\n    import torch.distributed as dist\n\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise RuntimeError(\"Expected to be called within a worker process\")\n    worker_id = worker_info.id\n    seed = worker_info.seed\n    print(f\"worker {worker_id} seed: {seed}\")\n    if worker_seeding == 'all':\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n    elif callable(worker_seeding):\n        worker_seeding(seed)\n    else:\n        raise ValueError(\"worker_seeding must be a callable or 'all'\")\n\n", "CodeBleu_score": 46.578007645412065}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height, image_width = image_shape[0], image_shape[1]\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(\n        [image], [image_size, image_size], method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 58.77701012742078}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n\n    # Extract image shape\n    image_shape = tf.image.extract_jpeg_shape(image_bytes)\n\n    # Calculate padded center crop size\n    padded_center_crop_size = tf.cast(\n        ((image_size[0] / (image_size[0] + CROP_PADDING)) * tf.cast(image_shape[0], tf.float32),\n         (image_size[1] / (image_size[1] + CROP_PADDING)) * tf.cast(image_shape[1], tf.float32)),\n        tf.int32)\n\n    # Determine offset for crop window\n    offset_height = ((image_shape[0] - padded_center_crop_size[0]) + 1) // 2\n    offset_width = ((image_shape[1] - padded_center_crop_size[1]) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size[0], padded_center_crop_size[1]])\n\n    # Decode and crop image\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n\n    # Resize image\n    image = tf.image.resize(image, image_size, method=resize_method)\n\n    return image\n\n", "CodeBleu_score": 40.88173113522887}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + 32)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize(image, [image_size, image_size],\n                            method=resize_method)\n    return image\n\n", "CodeBleu_score": 59.34555091431084}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    # Decode and convert to float32.\n    image = tf.io.decode_jpeg(image_bytes, channels=3, dct_method='INTEGER_FAST')\n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    # Randomly crop a [HEIGHT, WIDTH] section of the image.\n    image = tf.image.random_crop(image, size=[image_size, image_size, 3])\n\n    # Randomly flip the image horizontally.\n    image = tf.image.random_flip_left_right(image)\n\n    if interpolation == 'bicubic':\n        # Resize the image to the target size.\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.BICUBIC)\n    elif interpolation == 'bilinear':\n        # Resize the image to the target size.\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.BILINEAR)\n    elif interpolation == 'nearest':\n        # Resize the image to the target size.\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    elif interpolation == 'area':\n        # Resize the image to the target size.\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.AREA)\n    else:\n        raise ValueError('Invalid interpolation method: {}'.format(interpolation))\n\n    # Rescale to [-1,1] instead of [0, 1]\n    image = tf.multiply(image, 2.0)\n    image = tf.subtract(image, 1.0)\n\n    if use_bfloat16:\n        # Cast to bfloat16.\n        image = tf.cast(image, tf.bfloat16)\n\n    return image\n\n", "CodeBleu_score": 41.08874027263116}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    image = tf.image.resize_with_crop_or_pad(image, image_size, image_size)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 38.90002637872042}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    # Decode the image bytes\n    image = tf.io.decode_jpeg(image_bytes, channels=3, dct_method='INTEGER_FAST')\n    # Randomly crop the image\n    image = tf.image.random_crop(image, size=[image_size, image_size, 3])\n    # Randomly flip the image horizontally\n    image = tf.image.random_flip_left_right(image)\n    # Resize the image to the target size\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    # Convert the image to the specified dtype\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    # Reshape the image to the target size\n    image = tf.reshape(image, [image_size, image_size, 3])\n    return image\n", "CodeBleu_score": 41.85193011031836}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    if interpolation == 'bicubic':\n        interpolation = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        interpolation = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'nearest':\n        interpolation = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        print('Invalid interpolation method!')\n        exit(1)\n    image_shape = tf.constant([image_size, image_size, 3])\n    image = decode_and_center_crop(image_bytes, image_size)\n    image = tf.image.resize(image, image_shape, method=interpolation)\n    image = tf.reshape(image, image_shape)\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 47.99382521701902}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    # Decode and center the crop\n    image = tf.image.decode_jpeg(image_bytes, channels=3, dct_method='INTEGER_FAST')\n    image = tf.image.resize_with_pad(image, image_size, image_size, method=interpolation)\n    # Reshape to the desired size\n    image = tf.reshape(image, [image_size, image_size, 3])\n    # Convert to bfloat16 if necessary\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n    return image\n", "CodeBleu_score": 40.05136793264927}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    # Decode the image from bytes\n    image = tf.image.decode_jpeg(image_bytes, channels=3, dct_method='INTEGER_FAST')\n    # Convert to float32 in [0, 1] range.\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if interpolation == 'bicubic':\n        # Resize the image to the target size.\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.BICUBIC)\n    elif interpolation == 'bilinear':\n        # Resize the image to the target size.\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.BILINEAR)\n    elif interpolation == 'nearest':\n        # Resize the image to the target size.\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    elif interpolation == 'area':\n        # Resize the image to the target size.\n        image = tf.image.resize(image, [image_size, image_size], method=tf.image.ResizeMethod.AREA)\n    else:\n        raise ValueError('Invalid interpolation method {}'.format(interpolation))\n    # Reshape to the target shape.\n    image = tf.reshape(image, [image_size, image_size, 3])\n    if use_bfloat16:\n        # Convert to bfloat16.\n        image = tf.cast(image, tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 46.750400427477956}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    model.eval()\n\n    # Prediction loop\n    with torch.no_grad():\n        predictions = []\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            batch = self.prepare_batch(batch)\n            outputs = model(**batch)\n            predictions.append(outputs.logits.cpu().numpy())\n\n    predictions = np.concatenate(predictions)\n    predictions = np.argmax(predictions, axis=1)\n\n    # Compute metrics\n    results = self.compute_metrics(predictions)\n    results = {k: v.item() for k, v in results.items()}\n\n    # Save predictions\n    if output_file is not None:\n        self.save_predictions(predictions, output_file)\n\n    return results", "CodeBleu_score": 49.73399729610092}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n\n    if eval_dataloader is None:\n        eval_dataloader = self.get_eval_dataloader()\n\n    output_file = os.path.join(self.args.output_dir, output_file)\n    # Prediction loop\n    preds = {}\n    label_ids = []\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        inputs = self._get_inputs_dict(batch)\n        with torch.no_grad():\n            outputs = model(**inputs)\n            logits = outputs.get(\"logits\")\n            if logits is not None:\n                logits = logits.detach().cpu().numpy()\n            label_ids += batch[\"labels\"].detach().cpu().numpy().tolist()\n            if logits is not None:\n                preds.update({k: v.detach().cpu().numpy() for k, v in outputs.items()})\n\n    if self.args.task_name is not None:\n        result = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\n    else:\n        result = {}\n\n    if self.args.task_name is not None:\n        output_eval_file = os.path.join(self.args.output_dir, \"eval_results.txt\")\n        with open(output_eval_file, \"w\") as writer:\n            logger.info(\"***** Eval results {} *****\".format(self.args.task_name))\n            for key in sorted(result.keys()):\n                logger.info(\"  %s = %s\", key, str(result[key]))\n                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return result, preds, label_ids\n\n", "CodeBleu_score": 41.73775223385428}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n\n    model.eval()\n    output_file = os.path.join(self.args.output_dir, output_file)\n    with open(output_file, \"w\") as writer:\n        for batch in eval_dataloader:\n            inputs = self._prepare_inputs(batch)\n            outputs = model(**inputs)\n            for key, value in outputs.items():\n                if key not in [\"logits\", \"loss\"]:\n                    writer.write(f\"{key} = {value}\\n\")\n            writer.write(\"\\n\")\n    metrics = self.compute_metrics(eval_dataloader)\n    return metrics\n\n", "CodeBleu_score": 50.63580565728443}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print(\"Rank1: {0:.4f}\".format(rank1))\n    print(\"Rank5: {0:.4f}\".format(rank5))\n    print(\"mIoU: {0:.4f}\".format(miou))", "CodeBleu_score": 26.837855598415434}, {"code": " def print_computed_metrics(self, metrics)    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('Rank1: {0:.4f}'.format(rank1))\n    print('Rank5: {0:.4f}'.format(rank5))\n    print('mIoU: {0:.4f}'.format(miou))\n", "CodeBleu_score": 26.40011266619594}, {"code": " def print_computed_metrics(self, metrics)    print(\"Rank1: {0:.4f}\".format(metrics[\"rank1\"]))\n    print(\"Rank5: {0:.4f}\".format(metrics[\"rank5\"]))\n    print(\"mIoU: {0:.4f}\".format(metrics[\"miou\"]))\n\n\n  def get_metrics(self, preds, targets):\n    \"\"\"The function computes the metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    metrics = {}\n    metrics[\"rank1\"], metrics[\"rank5\"] = self.compute_ranks(preds, targets)\n    metrics[\"miou\"] = self.compute_miou(preds, targets)\n    return metrics\n\n  def compute_ranks(self, preds, targets):\n    \"\"\"The function computes the rank1 and rank5 metrics.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    preds_sorted = torch.sort(preds, dim=1, descending=True)[1]\n    targets_sorted = torch.sort(targets, dim=1, descending=True)[1]\n    rank1 = (preds_sorted[:, :1] == targets_sorted[:, :1]).sum().item()\n    rank5 = (preds_sorted[:, :5] == targets_sorted[:, :1]).sum().item()\n    return rank1, rank5\n\n  def compute_miou(self, preds, targets):\n    \"\"\"The function computes the miou metric.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    preds_sorted = torch.sort(preds, dim=1, descending=True)[1]\n    targets_sorted = torch.sort(targets, dim=1, descending=True)[1]\n    intersection = (preds_sorted == targets_sorted).sum().item()\n    union = preds_sorted.shape[0] * preds_sorted.shape[1]\n    miou = intersection / union\n    return miou", "CodeBleu_score": 29.489382115269812}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n    rank1 = 0\n    rank5 = 0\n    iou = 0\n    for segment, gt in zip(segments, data):\n        if len(gt) == 0:\n            continue\n        iou_list = []\n        for g in gt:\n            iou_list.append(self._iou(segment, g))\n        iou_list = np.array(iou_list)\n        iou_list = iou_list[np.argsort(iou_list)[::-1]]\n        iou += iou_list[0]\n        if iou_list[0] > 0.5:\n            rank1 += 1\n        if iou_list[0] > 0.1:\n            rank5 += 1\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    iou /= len(segments)\n    return rank1, rank5, iou\n\n", "CodeBleu_score": 33.234155949568404}, {"code": "def _eval_predictions(self, segments, data)\n    rank1 = 0\n    rank5 = 0\n    mean_iou = 0\n    for segment, data in zip(segments, data):\n        ious = []\n        for gt in data:\n            iou = self.iou(segment, gt)\n            ious.append(iou)\n        ious.sort()\n        ious.reverse()\n        mean_iou += ious[0]\n        if ious[0] >= 0.5:\n            rank1 += 1\n        if ious[0] >= 0.5 or ious[1] >= 0.5:\n            rank5 += 1\n    mean_iou /= len(segments)\n    return rank1 / len(segments), rank5 / len(segments), mean_iou", "CodeBleu_score": 27.788286568974335}, {"code": "def _eval_predictions(self, segments, data)\n    rank1, rank5, mean_iou = 0, 0, 0\n    for seg, gt in zip(segments, data):\n        ious = []\n        for g in gt:\n            iou = self._intersection_over_union(g, seg)\n            ious.append(iou)\n        ious = np.array(ious)\n        mean_iou += np.mean(ious)\n        rank1 += int(np.any(ious >= 0.5))\n        rank5 += int(np.any(ious >= 0.55))\n    mean_iou /= len(segments)\n    rank1 /= len(segments)\n    rank5 /= len(segments)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 29.130375286811883}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\n    # Initialize empty list for scores\n    scores = []\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Transfer model to GPU 0\n    model.to(self.device)\n\n    # Iterate over the dataloader\n    for batch in eval_dataloader:\n\n        # Transfer data to GPU 0\n        batch = tuple(t.to(self.device) for t in batch)\n\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up prediction\n        with torch.no_grad():\n\n            # Forward pass, calculate logit predictions\n            outputs = model(\n                b_input_ids,\n                token_type_ids=None,\n                attention_mask=b_input_mask\n            )\n\n        # Update scores with the logit predictions\n        scores.extend(outputs.cpu().numpy())\n\n    # Finalize and return the results, optionally saving them to an output file\n    return self.finalize_and_return_results(scores, output_file)", "CodeBleu_score": 33.638443602095116}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    scores = []\n    model.eval()\n    model.to(self.device)\n    for batch in eval_dataloader:\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n            if self.args.model_type != \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.args.model_type in [\"bert\", \"xlnet\"] else None\n                )  # XLM, DistilBERT and RoBERTa don't use segment_ids\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n            preds = np.argmax(preds, axis=1)\n            preds = [self.label_list[int(i)] for i in preds]\n            out_label_list = [self.label_list[int(i)] for i in out_label_ids]\n            scores.extend(zip(preds, out_label_list))\n    if output_file is not None:\n        with open(output_file, \"w\") as f:\n            f.write(\"\\n\".join(scores))\n    return scores", "CodeBleu_score": 31.789819192309775}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\n    scores = []\n    model.eval()\n    model.to(self.device)\n    for batch in eval_dataloader:\n        batch = tuple(t.to(self.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n            if self.args.model_type != \"distilbert\":\n                inputs[\"token_type_ids\"] = (\n                    batch[2] if self.args.model_type in [\"bert\", \"xlnet\"] else None\n                )  # XLM, DistilBERT and RoBERTa don't use segment_ids\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n            preds = np.argmax(preds, axis=1)\n            scores.append(preds)\n    scores = np.concatenate(scores, axis=0)\n    if output_file is not None:\n        with open(output_file, \"w\") as writer:\n            for i, score in enumerate(scores):\n                writer.write(f\"{i}\\t{score}\\n\")\n    return scores", "CodeBleu_score": 34.822341832966906}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2, \"There must be exactly two sets of scores.\"\n    video_scores = scores[0]\n    text_scores = scores[1]\n    self.full_scores = np.concatenate((video_scores, text_scores), axis=0)\n    return np.dot(video_scores, text_scores.T)\n\n", "CodeBleu_score": 39.617707484195165}, {"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2\n    video_scores = scores[0]\n    text_scores = scores[1]\n    self.full_scores = torch.cat((video_scores, text_scores), dim=1)\n    return torch.matmul(video_scores, text_scores.t())\n\n", "CodeBleu_score": 40.561443580773194}, {"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2\n    self.full_scores = None\n    return torch.cat(scores, dim=0)\n", "CodeBleu_score": 18.50161550170645}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\n    if self.verbose:\n        print('Finalizing predictions...')\n\n    # Convert predictions to numpy arrays\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    # Check for prediction errors\n    if np.any(Y_pred != Y_true):\n        print('WARNING: Prediction errors were found!')\n        print('         Predicted:', Y_pred)\n        print('         True:     ', Y_true)\n        print('         Samples:  ', np.where(Y_pred != Y_true)[0])\n\n    # Save predictions and true labels as pickle file\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}", "CodeBleu_score": 35.457552599636585}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    import numpy as np\n    import pandas as pd\n    import pickle\n\n    # convert the predictions and true labels to numpy arrays\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    # check for prediction errors\n    if (Y_pred != Y_true).any():\n        print(\"Prediction Errors:\")\n        print(\n            pd.DataFrame(\n                np.concatenate((Y_pred, Y_true), axis=1),\n                columns=[\"predicted\", \"true\"],\n            ).loc[Y_pred != Y_true]\n        )\n\n    # save the predictions and true labels as a pickle file\n    if output_file is not None:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}", "CodeBleu_score": 39.695322400212}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    Y_pred = np.concatenate(Y_pred).ravel()\n    Y_true = np.concatenate(Y_true).ravel()\n\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump([Y_pred, Y_true], f)\n\n    if not np.all(np.isclose(Y_pred, Y_true)):\n        print('Prediction error!')\n        for i in range(Y_pred.shape[0]):\n            if not np.isclose(Y_pred[i], Y_true[i]):\n                print(Y_pred[i], Y_true[i])\n\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}", "CodeBleu_score": 34.44100482063264}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n    # Initialize the total loss and sample sizes\n    total_loss = 0.0\n    total_sample_size = 0\n\n    # Sum the loss values and sample sizes\n    for log in logging_outputs:\n        total_loss += log.get(\"loss\", 0)\n        total_sample_size += log.get(\"sample_size\", 0)\n\n    # Calculate the average loss\n    if total_sample_size > 0:\n        average_loss = total_loss / total_sample_size\n    else:\n        average_loss = total_loss\n\n    # Log the average loss\n    print(\"Average loss: {0:.3f}\".format(average_loss))\n\n", "CodeBleu_score": 26.780705010561213}, {"code": "def reduce_metrics(logging_outputs) -> None\n\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    n_samples = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n    metrics.log_scalar(\n        \"loss\", loss_sum / n_samples, n_samples, round=3\n    )\n    metrics.log_derived(\n        \"ppl\", lambda meters: utils.get_perplexity(meters[\"loss\"].avg)\n    )\n\n", "CodeBleu_score": 43.74193960827946}, {"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )\n\n", "CodeBleu_score": 57.00304501759299}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\n    if attention_mask is None:\n        attention_mask = torch.cat(\n            (\n                torch.ones(\n                    (\n                        cmasks.shape[0],\n                        cmasks.shape[1]\n                    ),\n                    dtype=torch.long,\n                    device=cmasks.device\n                ),\n                torch.zeros(\n                    (\n                        cmasks.shape[0],\n                        vmasks.shape[1]\n                    ),\n                    dtype=torch.long,\n                    device=cmasks.device\n                )\n            ),\n            dim=1\n        )\n\n    token_type_ids = torch.cat(\n        (\n            torch.zeros(\n                (\n                    cmasks.shape[0],\n                    cmasks.shape[1]\n                ),\n                dtype=torch.long,\n                device=cmasks.device\n            ),\n            torch.ones(\n                (\n                    cmasks.shape[0],\n                    vmasks.shape[1]\n                ),\n                dtype=torch.long,\n                device=cmasks.device\n            )\n        ),\n        dim=1\n    )\n\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 39.03400199392642}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = torch.cat([torch.ones(cmasks.shape), vmasks], dim=-1)\n        attention_mask = attention_mask.to(dtype=torch.long)\n    token_type_ids = torch.cat([torch.zeros(cmasks.shape), torch.ones(vmasks.shape)], dim=-1)\n    token_type_ids = token_type_ids.to(dtype=torch.long)\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 37.64571178594868}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = torch.cat(\n            [\n                torch.ones(\n                    (\n                        cmasks[0].shape[0],\n                        cmasks[0].shape[1]\n                    ),\n                    dtype=torch.long\n                ),\n                torch.zeros(\n                    (\n                        cmasks[0].shape[0],\n                        vmasks[0].shape[1]\n                    ),\n                    dtype=torch.long\n                )\n            ],\n            dim=1\n        )\n    token_type_ids = torch.cat(\n        [\n            torch.zeros(\n                (\n                    cmasks[0].shape[0],\n                    cmasks[0].shape[1]\n                ),\n                dtype=torch.long\n            ),\n            torch.ones(\n                (\n                    cmasks[0].shape[0],\n                    vmasks[0].shape[1]\n                ),\n                dtype=torch.long\n            )\n        ],\n        dim=1\n    )\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 41.43685415610977}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    input_ids, input_video_embeds = self._prepare_input_ids_for_generation(\n        input_ids, input_video_embeds\n    )\n\n    if attention_mask is not None:\n        attention_mask = self._prepare_attention_mask_for_generation(\n            attention_mask, input_ids, input_video_embeds\n        )\n\n    if token_type_ids is not None:\n        token_type_ids = self._prepare_token_type_ids_for_generation(\n            token_type_ids, input_ids, input_video_embeds\n        )\n\n    # special case if pad_token_id is not defined\n    if (\n        self.config.pad_token_id is None and\n        token_type_ids is not None\n    ):\n        logger.warning(\n            \"Setting `pad_token_id` to `eos_token_id`: {self.config.eos_token_id} for open-end generation.\"\n        )\n        self.config.pad_token_id = self.config.eos_token_id\n\n    # Prepare model kwargs\n    model_kwargs[\"input_ids\"] = input_ids\n    model_kwargs[\"input_video_embeds\"] = input_video_embeds\n    model_kwargs[\"attention_mask\"] = attention_mask\n    model_kwargs[\"token_type_ids\"] = token_type_ids\n\n    return model_kwargs\n\n", "CodeBleu_score": 29.664941065173444}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    batch_size, video_length, video_dim = input_video_embeds.size()\n    text_length = input_ids.size(1)\n    total_length = text_length + video_length\n\n    # If the input_ids is not empty, we add the text_length to the attention_mask\n    if attention_mask is not None:\n        attention_mask = self._prepare_attention_mask_for_generation(input_ids, attention_mask, text_length)\n\n    # If the input_ids is not empty, we add the text_length to the token_type_ids\n    if token_type_ids is not None:\n        token_type_ids = self._prepare_token_type_ids_for_generation(input_ids, token_type_ids, text_length)\n\n    # We create a tensor of size [batch_size, total_length] filled with the video_pad_token_id\n    video_pad_token_id = self.config.video_pad_token_id\n    video_pad_token_id = torch.tensor([video_pad_token_id], dtype=torch.long, device=input_video_embeds.device)\n    video_pad_token_id = video_pad_token_id.repeat(batch_size, total_length)\n\n    # We create a tensor of size [batch_size, total_length] filled with the video_bos_token_id\n    video_bos_token_id = self.config.video_bos_token_id\n    video_bos_token_id = torch.tensor([video_bos_token_id], dtype=torch.long, device=input_video_embeds.device)\n    video_bos_token_id = video_bos_token_id.repeat(batch_size, 1)\n\n    # We create a tensor of size [batch_size, total_length] filled with the video_eos_token_id\n    video_eos_token_id = self.config.video_eos_token_id\n    video_eos_token_id = torch.tensor([video_eos_token_id], dtype=torch.long", "CodeBleu_score": 29.78009608993773}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    # Generate the attention mask\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids)\n\n    # Generate the token type ids\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n\n    # Combine the input_ids and input_video_embeds\n    input_ids = torch.cat([input_ids, input_video_embeds], dim=1)\n\n    # Update the attention mask and token type ids\n    attention_mask = torch.cat([attention_mask, torch.ones_like(input_video_embeds)], dim=1)\n    token_type_ids = torch.cat([token_type_ids, torch.ones_like(input_video_embeds)], dim=1)\n\n    # Update the model_kwargs\n    model_kwargs[\"attention_mask\"] = attention_mask\n    model_kwargs[\"token_type_ids\"] = token_type_ids\n\n    # Return the inputs\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs,\n    }\n\n", "CodeBleu_score": 38.31578909885077}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n    if 'img_id' in cfg.data.test.dataset.pipeline[0].meta_keys:\n        cfg.data.test.dataset.pipeline[0].meta_keys.remove('img_id')\n\n    inferencer_loader_idx = None\n    for idx, transform in enumerate(cfg.data.test.dataset.pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            inferencer_loader_idx = idx\n\n    if inferencer_loader_idx is None:\n        raise ValueError(\"Could not find 'LoadImageFromFile' transform in the pipeline\")\n\n    cfg.data.test.dataset.pipeline[inferencer_loader_idx]['type'] = 'mmdet.InferencerLoader'\n\n    return Compose(cfg.data.test.dataset.pipeline)\n\n", "CodeBleu_score": 32.422768400403065}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n    if 'meta_keys' in cfg:\n        meta_keys = cfg['meta_keys']\n        if 'img_id' in meta_keys:\n            meta_keys.remove('img_id')\n    else:\n        meta_keys = ['filename']\n\n    load_idx = -1\n    for idx, transform in enumerate(cfg['transforms']):\n        if 'type' in transform and transform['type'] == 'LoadImageFromFile':\n            load_idx = idx\n            break\n\n    if load_idx == -1:\n        raise ValueError('Cannot find the LoadImageFromFile transform')\n\n    cfg['transforms'][load_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(cfg['transforms'])\n\n", "CodeBleu_score": 34.932257072855684}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n    if 'img_id' in cfg.data.test.dataset.pipeline[0].meta_keys:\n        cfg.data.test.dataset.pipeline[0].meta_keys.remove('img_id')\n    inferencer_loader_idx = -1\n    for idx, transform in enumerate(cfg.data.test.dataset.pipeline):\n        if transform.type == 'LoadImageFromFile':\n            inferencer_loader_idx = idx\n            break\n    if inferencer_loader_idx == -1:\n        raise ValueError(\"The 'LoadImageFromFile' transform must be in the pipeline\")\n    cfg.data.test.dataset.pipeline[inferencer_loader_idx].type = 'mmdet.InferencerLoader'\n    return Compose(cfg.data.test.dataset.pipeline)\n\n", "CodeBleu_score": 35.61047501123469}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, x) for x in os.listdir(inputs) if os.path.isfile(os.path.join(inputs, x))]\n        else:\n            raise ValueError(f\"The input must be a directory, a list of file paths, or a tuple of file paths. The input {inputs} is not a directory.\")\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 46.388869072116236}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if os.path.isfile(os.path.join(inputs, f))]\n        else:\n            raise ValueError(\"The input is not a valid directory.\")\n    if isinstance(inputs, (list, tuple)):\n        return inputs\n    else:\n        return [inputs]\n\n", "CodeBleu_score": 37.897076614179056}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, file) for file in os.listdir(inputs) if os.path.isfile(os.path.join(inputs, file))]\n        else:\n            inputs = [inputs]\n    elif not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 44.911002247461326}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict):\n            if 'img_path' in input:\n                img = self.read_image(input['img_path'])\n            elif 'img' in input:\n                img = input['img']\n            else:\n                raise ValueError(\n                    \"Input dictionary must contain either 'img' or 'img_path' key\"\n                )\n            chunk.append(self.pipeline(img, **input))\n        else:\n            chunk.append(self.pipeline(input))\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if len(chunk) > 0:\n        yield chunk\n\n", "CodeBleu_score": 41.094853178648364}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    chunk = []\n    for input in inputs:\n        if isinstance(input, dict) and ('img' in input or 'img_path' in input):\n            input = self.pipeline(input)\n        chunk.append(input)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n", "CodeBleu_score": 23.59926429365173}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n\n    chunk_data = []\n    for input in inputs:\n        if isinstance(input, dict) and (\"img\" in input or \"img_path\" in input):\n            chunk_data.append(self.pipeline(input))\n        else:\n            chunk_data.append(input)\n        if len(chunk_data) == chunk_size:\n            yield chunk_data\n            chunk_data = []\n    if len(chunk_data) > 0:\n        yield chunk_data", "CodeBleu_score": 27.03975706936462}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if not no_save_pred and not os.path.exists(pred_out_dir):\n        os.makedirs(pred_out_dir)\n    results = {}\n    if not return_datasamples:\n        results = [self.convert_pred_to_dict(pred) for pred in preds]\n    else:\n        if pred_out_dir:\n            warnings.warn('Saving datasamples is not supported.')\n    if visualization is not None:\n        results['visualization'] = visualization\n    if print_result:\n        print(results)\n    if not no_save_pred:\n        results['pred_out_dir'] = pred_out_dir\n    return results\n\n", "CodeBleu_score": 48.546016044422736}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    # initialize the output directory\n    if pred_out_dir == '':\n        pred_out_dir = self.pred_out_dir\n    else:\n        pred_out_dir = Path(pred_out_dir)\n        pred_out_dir.mkdir(parents=True, exist_ok=True)\n\n    # initialize the results dictionary\n    results = dict()\n\n    # convert predictions to dictionaries\n    if not return_datasamples:\n        results['preds'] = []\n        for i, pred in enumerate(preds):\n            if visualization is not None:\n                pred['visualization'] = visualization[i]\n            results['preds'].append(pred.dict())\n\n    # add predictions to the results dictionary\n    if not no_save_pred:\n        results['preds'] = preds\n\n    # add visualization to the results dictionary\n    if visualization is not None:\n        results['visualization'] = visualization\n\n    # print the results\n    if print_result:\n        print(results)\n\n    # warn that saving datasamples is not supported\n    if return_datasamples:\n        warnings.warn(\n            'Saving datasamples is not supported.',\n            RuntimeWarning,\n        )\n\n    return results\n\n", "CodeBleu_score": 45.11951680484745}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if not no_save_pred:\n        if not pred_out_dir:\n            pred_out_dir = self.out_dir\n        else:\n            if return_datasamples:\n                self.logger.warning(f'Saving predictions is not supported with return_datasamples=True.')\n        self.init_out_dir(pred_out_dir)\n\n    results = {}\n    if return_datasamples:\n        results['datasamples'] = self.datasamples\n    else:\n        results['preds'] = []\n        for pred in preds:\n            results['preds'].append(pred.to_dict())\n\n    if visualization:\n        results['visualization'] = visualization\n\n    if print_result:\n        self.print_result(results)\n\n    if not no_save_pred:\n        self.save_pred(results, pred_out_dir)\n\n    return results\n\n", "CodeBleu_score": 50.52303837736526}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n\n    # Copy the first transform in the dataset pipeline\n    test_pipeline = cfg.data.train.transforms[0].copy()\n\n    # Modify the first transform to include only the 'Resize' transform\n    test_pipeline.transforms = [test_pipeline.transforms[0]]\n\n    # Copy the last transform in the pipeline\n    test_pipeline_last_transform = cfg.data.train.transforms[-1].copy()\n\n    # Combine the modified first and copied last transforms into a new test pipeline\n    test_pipeline.transforms.append(test_pipeline_last_transform)\n\n    # Return the constructed test pipeline\n    return test_pipeline", "CodeBleu_score": 25.97389626461354}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    # get the first transform in the dataset pipeline\n    first_transform = cfg.dataset.pipeline[0]\n    # modify the first transform to include only the 'Resize' transform\n    first_transform = dict(type=\"Resize\", size=cfg.dataset.pipeline[0][\"size\"])\n    # get the last transform in the dataset pipeline\n    last_transform = cfg.dataset.pipeline[-1]\n    # combine the modified first and copied last transforms into a new test pipeline\n    test_pipeline = [first_transform, last_transform]\n    # return the constructed test pipeline\n    return test_pipeline", "CodeBleu_score": 22.193498894142316}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    # TODO: Implement the function\n    return None\n\n", "CodeBleu_score": 3.3114665037402617}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    # Prepare the input data\n    input_data = SampleList([{\"image\": img, \"frame_id\": frame_id, \"original_shape\": img.shape,\n                              \"image_id\": frame_id, \"video_id\": 0, \"video_len\": video_len}])\n\n    # Run the model on the data\n    with torch.no_grad():\n        if next(model.parameters()).is_cuda:\n            output = model(input_data)\n        else:\n            # Move model to CPU and disable RoIPool\n            model.cpu().eval()\n            model.heads[\"hm\"].use_roi_pool = False\n            output = model(input_data)\n\n    return output", "CodeBleu_score": 27.898113448269573}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n\n    # Prepare data\n    sample = Sample()\n    sample.image = img\n    sample.frame_id = frame_id\n    sample.original_shape = img.shape\n    sample.image_id = frame_id\n    sample.video_len = video_len\n    sample.dataset_name = \"coco_mot\"\n\n    # Perform inference\n    model.eval()\n    with torch.no_grad():\n        if next(model.parameters()).is_cuda:\n            sample = sample.to(\"cuda\")\n        else:\n            # Ensure no unsupported modules like RoIPool are used\n            for module in model.modules():\n                if isinstance(module, RoIPool):\n                    raise ValueError(\"RoIPooling is not supported on CPU\")\n        return model.test_step(sample, None)\n\n", "CodeBleu_score": 30.272447041863266}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    # Prepare the input data\n    sample = Sample()\n    sample.data = img\n    sample.info = SampleInfo(frame_id, video_len)\n    sample.dataset_name = \"mot\"\n\n    # Perform inference\n    with torch.no_grad():\n        sample = model.test_step(sample, 0)\n\n    return sample.prediction\n\n", "CodeBleu_score": 16.336635163735192}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 76.28687604964396}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 76.28687604964396}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 76.28687604964396}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n    if not isinstance(activation_string, str):\n        return activation_string\n    if activation_string == \"\":\n        return None\n    elif activation_string == \"relu\":\n        return tf.nn.relu\n    elif activation_string == \"tanh\":\n        return tf.nn.tanh\n    elif activation_string == \"sigmoid\":\n        return tf.nn.sigmoid\n    elif activation_string == \"softmax\":\n        return tf.nn.softmax\n    elif activation_string == \"linear\":\n        return None\n    else:\n        raise ValueError(\"Unsupported activation function: \" + activation_string)\n", "CodeBleu_score": 43.23717100329048}, {"code": "def get_activation(activation_string)\n    if not isinstance(activation_string, str):\n        return activation_string\n    if activation_string == \"\":\n        return None\n    if activation_string == \"relu\":\n        return tf.nn.relu\n    if activation_string == \"tanh\":\n        return tf.nn.tanh\n    if activation_string == \"sigmoid\":\n        return tf.nn.sigmoid\n    if activation_string == \"softmax\":\n        return tf.nn.softmax\n    if activation_string == \"leaky_relu\":\n        return tf.nn.leaky_relu\n    if activation_string == \"elu\":\n        return tf.nn.elu\n    if activation_string == \"selu\":\n        return tf.nn.selu\n    if activation_string == \"crelu\":\n        return tf.nn.crelu\n    if activation_string == \"softplus\":\n        return tf.nn.softplus\n    if activation_string == \"softsign\":\n        return tf.nn.softsign\n    if activation_string == \"swish\":\n        return tf.nn.swish\n    raise ValueError(\"Unsupported activation function: \" + activation_string)\n\n", "CodeBleu_score": 35.674578033183806}, {"code": "def get_activation(activation_string)\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string == 'relu':\n        return tf.nn.relu\n    elif activation_string == 'leaky_relu':\n        return tf.nn.leaky_relu\n    elif activation_string == 'tanh':\n        return tf.nn.tanh\n    elif activation_string == 'sigmoid':\n        return tf.nn.sigmoid\n    elif activation_string == 'elu':\n        return tf.nn.elu\n    elif activation_string == 'selu':\n        return tf.nn.selu\n    else:\n        raise ValueError('Unsupported activation function: {}'.format(activation_string))\n\n", "CodeBleu_score": 39.74925393756226}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, dropout_prob)\n", "CodeBleu_score": 53.31243274886709}, {"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)", "CodeBleu_score": 53.03103381915149}, {"code": "def dropout(input_tensor, dropout_prob)\n    import tensorflow as tf\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)\n\n", "CodeBleu_score": 54.15143397620772}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = bert_utils.get_shape_list(from_tensor, expected_rank=[2, 3])\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = bert_utils.get_shape_list(to_mask, expected_rank=2)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]),\n        dtype=from_tensor.dtype)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=from_tensor.dtype)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n", "CodeBleu_score": 50.8668260721201}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = from_tensor.shape\n    batch_size, from_seq_length = from_shape[0], from_shape[1]\n    to_shape = to_mask.shape\n    to_seq_length = to_shape[1]\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n    mask = broadcast_ones * to_mask\n    return mask\n", "CodeBleu_score": 45.51505434367119}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n\n    from_shape = tf.shape(from_tensor)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = tf.shape(to_mask)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't attend to padding tokens.\n    attention_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # Broadcast along two dimensions to create the mask.\n    attention_mask = attention_mask[:, tf.newaxis, :]\n    attention_mask = attention_mask[:, :, tf.newaxis, :]\n\n    return attention_mask\n\n", "CodeBleu_score": 40.53452594449436}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    input_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    return tf.transpose(input_tensor, [0, 2, 1, 3])\n", "CodeBleu_score": 49.719542738107044}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length,\n                                              num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n", "CodeBleu_score": 49.16131946047179}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    input_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    return tf.transpose(input_tensor, [0, 2, 1, 3])\n", "CodeBleu_score": 43.795606016373064}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"The input tensor must have a rank of at least 2.\")\n    if len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n", "CodeBleu_score": 32.20740575979235}, {"code": "def reshape_to_matrix(input_tensor)\n    if len(input_tensor.shape) < 2:\n        raise ValueError(\"Input tensor must have at least 2 dimensions\")\n    elif len(input_tensor.shape) == 2:\n        return input_tensor\n    else:\n        return tf.reshape(input_tensor, [-1, input_tensor.shape[-1]])\n\n", "CodeBleu_score": 35.291412653314445}, {"code": "def reshape_to_matrix(input_tensor)\n    if input_tensor.get_shape().ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" % input_tensor.get_shape().as_list())\n    elif input_tensor.get_shape().ndims == 2:\n        return input_tensor\n    else:\n        width = input_tensor.get_shape()[-1]\n        return tf.reshape(input_tensor, [-1, width])\n\n", "CodeBleu_score": 49.92983933437852}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n    train_data = pd.read_csv(os.path.join(data_dir, \"train.tsv\"), delimiter=\"\\t\", skiprows=1, header=None, names=[\"label\", \"text_a\", \"text_b\"])\n    train_examples = []\n    for (i, row) in train_data.iterrows():\n        guid = \"%s-%s\" % (self.task, i)\n        text_a = row[\"text_a\"]\n        text_b = row[\"text_b\"]\n        label = row[\"label\"]\n        if label == \"contradictory\":\n            label = \"contradiction\"\n        train_examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return train_examples\n", "CodeBleu_score": 29.204747631538723}, {"code": "def get_train_examples(self, data_dir)\n    train_file = os.path.join(data_dir, \"train.tsv\")\n    train_examples = []\n    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\n        header = next(reader)\n        for line in reader:\n            text_a = line[3]\n            text_b = line[4]\n            label = line[0]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(\n                InputExample(guid=None, text_a=text_a, text_b=text_b, label=label))\n    return train_examples\n", "CodeBleu_score": 32.46277904872494}, {"code": "def get_train_examples(self, data_dir)\n    train_data = pd.read_csv(os.path.join(data_dir, \"train.tsv\"), sep=\"\\t\")\n    train_data = train_data[[\"text\", \"label\"]]\n    train_data.columns = [\"text_a\", \"label\"]\n    train_data[\"text_b\"] = None\n    train_data[\"guid\"] = None\n    train_data = train_data.applymap(str)\n    train_data[\"label\"] = train_data[\"label\"].apply(lambda x: \"contradiction\" if x == \"contradictory\" else x)\n    train_data = train_data.applymap(lambda x: x.strip())\n    train_data = train_data.applymap(lambda x: x.lower())\n    train_data = train_data.applymap(lambda x: x.replace(\"\\\\\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"'\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"`\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"\\\"\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"-\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\":\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"_\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"*\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"+\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"/\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"(\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\")\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"[\", \"\"))\n    train_data = train_data.applymap(lambda x: x.replace(\"]\",", "CodeBleu_score": 20.309713198240946}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example\n    )\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 62.41740489696974}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(label_ids, predictions, weights=is_real_example)\n    loss = tf.metrics.mean(per_example_loss, weights=is_real_example)\n\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }", "CodeBleu_score": 54.387272564649315}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids,\n        predictions=predictions,\n        weights=is_real_example)\n    loss = tf.metrics.mean(\n        values=per_example_loss,\n        weights=is_real_example)\n\n    return {\n        'eval_accuracy': accuracy,\n        'eval_loss': loss,\n    }", "CodeBleu_score": 54.33077250728522}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.75375987448578}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.68693175472625}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.75375987448578}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n    output_weights = tf.get_variable(\n        \"cls/seq_relationship/output_weights\",\n        shape=[2, bert_config.hidden_size],\n        initializer=tf.truncated_normal_initializer(stddev=bert_config.initializer_range))\n\n    output_bias = tf.get_variable(\n        \"cls/seq_relationship/output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 54.37052956985961}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\n    # Retrieve the output weights and bias from the model.\n    output_weights = tf.get_variable(\n        \"cls/seq_relationship/output_weights\",\n        shape=[2, bert_config.hidden_size],\n        initializer=modeling.create_initializer(bert_config.initializer_range),\n    )\n    output_bias = tf.get_variable(\n        \"cls/seq_relationship/output_bias\",\n        shape=[2],\n        initializer=tf.zeros_initializer(),\n    )\n\n    # Apply dropout\n    output_layer = modeling.dropout(input_tensor, bert_config.hidden_dropout_prob)\n\n    # Compute logits\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # Convert labels to one-hot encoding\n    logits = tf.reshape(logits, [-1, 2])\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    # Compute loss\n    per_example_loss = -tf.reduce_sum(one_hot_labels * tf.nn.log_softmax(logits, axis=-1), axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    # Compute log probabilities\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    return loss, per_example_loss, log_probs", "CodeBleu_score": 45.69781939972895}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels)\n\n    # Define output weights and bias\n    output_weights = tf.get_variable(\n        \"cls/seq_relationship/output_weights\",\n        shape=[2, bert_config.hidden_size],\n        initializer=tf.truncated_normal_initializer(stddev=0.02),\n    )\n    output_bias = tf.get_variable(\n        \"cls/seq_relationship/output_bias\",\n        shape=[2],\n        initializer=tf.zeros_initializer(),\n    )\n\n    # Compute logits\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n\n    # Reshape labels\n    labels = tf.reshape(labels, [-1])\n\n    # Convert to one-hot encoding\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n\n    # Compute per-example loss\n    per_example_loss = -tf.reduce_sum(one_hot_labels * tf.nn.log_softmax(logits, axis=-1), axis=-1)\n\n    # Compute mean loss\n    loss = tf.reduce_mean(per_example_loss)\n\n    # Compute log probabilities\n    probabilities = tf.nn.softmax(logits, axis=-1)\n\n    return loss, per_example_loss, probabilities\n\n", "CodeBleu_score": 47.233128214524825}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n    sequence_tensor_shape = sequence_tensor.shape\n    batch_size = sequence_tensor_shape[0]\n    seq_length = sequence_tensor_shape[1]\n    width = sequence_tensor_shape[2]\n\n    flat_offsets = torch.arange(0, batch_size, dtype=torch.long, device=positions.device) * seq_length\n    flat_positions = positions + flat_offsets.view(-1, 1)\n    flat_positions = flat_positions.view(-1)\n    output_tensor = sequence_tensor.view(batch_size * seq_length, width)\n    output_tensor = output_tensor.index_select(0, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 27.530035483484667}, {"code": "def gather_indexes(sequence_tensor, positions)\n    batch_size = sequence_tensor.shape[0]\n    seq_length = sequence_tensor.shape[1]\n    width = sequence_tensor.shape[2]\n\n    flat_offsets = torch.arange(0, batch_size, device=positions.device) * seq_length\n    flat_positions = (positions + flat_offsets).view(-1)\n    flat_sequence_tensor = sequence_tensor.view(batch_size * seq_length, width)\n    output_tensor = flat_sequence_tensor.index_select(0, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 25.143427139283713}, {"code": "def gather_indexes(sequence_tensor, positions)\n    batch_size = sequence_tensor.shape[0]\n    seq_length = sequence_tensor.shape[1]\n    width = sequence_tensor.shape[2]\n\n    flat_offsets = torch.arange(0, batch_size, dtype=torch.long, device=positions.device) * seq_length\n    flat_positions = positions + flat_offsets\n    flat_sequence_tensor = sequence_tensor.view(batch_size * seq_length, width)\n    output_tensor = flat_sequence_tensor.index_select(0, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 29.53359907230763}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 71.32095143473128}, {"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 60.19566542534298}, {"code": "def _decode_record(record, name_to_features)\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.to_int32(t)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 71.32095143473128}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 68.30795308098193}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 68.30795308098193}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return new_start, new_end\n\n    return input_start, input_end\n\n", "CodeBleu_score": 68.30795308098193}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.04591257743547}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.04591257743547}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position)\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 76.04591257743547}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, 'r') as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 69.44081996693993}, {"code": "def load_vocab(vocab_file)\n    vocab = OrderedDict()\n    index = 0\n    with open(vocab_file, 'r') as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.31784733933864}, {"code": "def load_vocab(vocab_file)\n    vocab = OrderedDict()\n    index = 0\n    with open(vocab_file, 'r') as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 67.31784733933864}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n", "CodeBleu_score": 88.10762355418231}, {"code": "def tokenize(self, text)\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n            split_tokens.append(sub_token)\n    return split_tokens\n", "CodeBleu_score": 88.10762355418231}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        import librosa\n        audio_data, sample_rate = librosa.load(path, offset=offset, duration=duration, sr=sample_rate)\n        if dtype == 'float32':\n            audio_data = audio_data.astype('float32')\n        elif dtype == 'int16':\n            audio_data = audio_data.astype('int16')\n        else:\n            raise ValueError('Unsupported dtype: {}'.format(dtype))\n        print('Loaded {} seconds of audio data from {}'.format(duration, path))\n        return audio_data, False\n    except Exception as e:\n        print('Error loading audio data from {}'.format(path))\n        print(e)\n        return -1.0, True\n\n", "CodeBleu_score": 35.19247714830001}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        # Load the audio data.\n        data = load(path, offset, duration, sample_rate, dtype)\n        # Log the successful loading process.\n        print(\"Successfully loaded audio data from: \" + path)\n        # Return the audio data and the error flag.\n        return data, False\n    except Exception as e:\n        # Log the error.\n        print(\"Error: \" + str(e))\n        # Return the default value and the error flag.\n        return -1.0, True\n\n", "CodeBleu_score": 20.27686876149381}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\n    try:\n        print(f\"Loading audio data from {path}...\")\n        audio_data, sample_rate = librosa.load(path, offset=offset, duration=duration, sr=sample_rate)\n        print(f\"Successfully loaded audio data from {path}.\")\n        return audio_data, False\n    except Exception as e:\n        print(f\"An exception occurred while loading audio data from {path}.\")\n        print(f\"Exception: {e}\")\n        return np.array([-1.0]), True\n\n", "CodeBleu_score": 35.64553671664667}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    if waveform.shape[0] >= n_channels:\n        return waveform[:n_channels]\n    else:\n        return waveform.tile(n_channels // waveform.shape[0])[:n_channels]\n\n", "CodeBleu_score": 35.25998360269913}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    waveform_shape = tf.shape(waveform)\n    waveform_channels = waveform_shape[1]\n    if waveform_channels == n_channels:\n        return waveform\n    elif waveform_channels > n_channels:\n        return waveform[:, :n_channels]\n    else:\n        waveform_tiled = tf.tile(waveform, [1, n_channels // waveform_channels + 1])\n        return waveform_tiled[:, :n_channels]\n\n", "CodeBleu_score": 46.38453739263364}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    if n_channels < 1:\n        raise ValueError(\"The number of channels must be a positive integer.\")\n    elif n_channels == 1:\n        return tf.squeeze(waveform, axis=-1)\n    elif n_channels == tf.shape(waveform)[-1]:\n        return waveform\n    elif n_channels < tf.shape(waveform)[-1]:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1] * (tf.shape(waveform).rank - 1) + [n_channels // tf.shape(waveform)[-1]])[..., :n_channels]", "CodeBleu_score": 44.14299011927052}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    # Convert to dB\n    spectrogram_db = tf.math.maximum(\n        tf.math.abs(spectrogram), tf.keras.backend.epsilon()\n    )\n    spectrogram_db = tf.math.log(spectrogram_db)\n    spectrogram_db = 20 * tf.math.log(spectrogram_db) / tf.math.log(10)\n\n    # Clip to range\n    max_db = tf.math.reduce_max(spectrogram_db)\n    spectrogram_db = tf.math.maximum(spectrogram_db, max_db - db_range)\n\n    # Convert to uint8\n    spectrogram_db = (spectrogram_db - max_db + db_range) / db_range\n    spectrogram_db = tf.math.round(255 * spectrogram_db)\n    spectrogram_db = tf.cast(spectrogram_db, tf.uint8)\n\n    return spectrogram_db\n\n", "CodeBleu_score": 38.93180852578496}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n\n    # Convert the spectrogram to dB\n    db_spectrogram = tf.math.maximum(tf.math.abs(spectrogram), 1e-8)\n    db_spectrogram = tf.math.log(db_spectrogram)\n    db_spectrogram = tf.math.divide(db_spectrogram, tf.math.log(10.0))\n    db_spectrogram = tf.math.multiply(db_spectrogram, 20.0)\n\n    # Find the maximum dB value\n    max_db = tf.math.reduce_max(db_spectrogram)\n\n    # Clip the dB spectrogram to a specified range below this maximum\n    db_spectrogram = tf.math.subtract(db_spectrogram, max_db)\n    db_spectrogram = tf.math.divide(db_spectrogram, db_range)\n    db_spectrogram = tf.math.add(db_spectrogram, 1.0)\n    db_spectrogram = tf.math.multiply(db_spectrogram, 128.0)\n    db_spectrogram = tf.math.round(db_spectrogram)\n    db_spectrogram = tf.cast(db_spectrogram, tf.uint8)\n\n    return db_spectrogram\n\n", "CodeBleu_score": 36.545664876398256}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    # Convert spectrogram to dB-scaled float32\n    spectrogram = tf.math.maximum(spectrogram, 1e-5)\n    spectrogram = tf.math.log(spectrogram)\n    spectrogram = tf.math.divide(spectrogram, tf.math.log(10.0))\n    spectrogram = tf.math.multiply(spectrogram, 20.0)\n\n    # Clip spectrogram to a specified range below the maximum dB value\n    max_db = tf.math.reduce_max(spectrogram)\n    spectrogram = tf.math.maximum(spectrogram, max_db - db_range)\n\n    # Convert spectrogram from float32 to uint8\n    spectrogram = tf.cast(spectrogram, tf.uint8)\n    return spectrogram\n\n", "CodeBleu_score": 40.92452772075532}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\n    # Apply Hann window function\n    window = tf.signal.hann_window(frame_length, periodic=False)\n    window = tf.math.pow(window, window_exponent)\n\n    # Compute STFT\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        window_fn=window,\n        pad_end=False,\n    )\n\n    # Compute spectrogram\n    spectrogram = tf.math.pow(tf.math.abs(stft), spec_exponent)\n\n    # Transpose spectrogram\n    spectrogram = tf.transpose(spectrogram)\n\n    return spectrogram", "CodeBleu_score": 37.078132969207324}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\n    # Compute the spectrogram\n    spectrogram = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=tf.signal.hann_window(frame_length, periodic=False) ** window_exponent,\n    )\n\n    # Compute the absolute value and raise it to a specified exponent\n    spectrogram = tf.abs(spectrogram) ** spec_exponent\n\n    # Transpose the spectrogram\n    spectrogram = tf.transpose(spectrogram, perm=[1, 0, 2])\n\n    return spectrogram\n\n", "CodeBleu_score": 38.6838500546199}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n\n    # Apply a Hann window function raised to a specified exponent\n    window = tf.signal.hann_window(frame_length, periodic=True) ** window_exponent\n\n    # Perform the STFT with defined frame length and step\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=tf.signal.hann_window,\n        pad_end=True,\n    )\n\n    # Compute the spectrogram\n    spectrogram = tf.math.abs(stft) ** spec_exponent\n\n    # Transpose the spectrogram tensor\n    spectrogram = tf.transpose(spectrogram)\n\n    return spectrogram", "CodeBleu_score": 37.26531321399476}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\n    # Get the shape of the input spectrogram\n    shape = tf.shape(spectrogram)\n\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(shape[0] * factor, tf.int32)\n\n    # Resize the spectrogram\n    spectrogram_resized = tf.image.resize(\n        spectrogram, [new_time_dim, shape[1]], method=method\n    )\n\n    # Adjust the dimensions of the resized spectrogram\n    spectrogram_adjusted = tf.cond(\n        tf.less(new_time_dim, shape[0]),\n        lambda: spectrogram_resized[: shape[0], :],\n        lambda: tf.pad(spectrogram_resized, [[0, shape[0] - new_time_dim], [0, 0]]),\n    )\n\n    return spectrogram_adjusted", "CodeBleu_score": 32.10185529459416}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\n    # calculate new time dimension\n    new_time_dim = int(tf.shape(spectrogram)[0] * factor)\n\n    # resize spectrogram\n    spectrogram_resized = tf.image.resize(\n        spectrogram, [new_time_dim, tf.shape(spectrogram)[1]], method=method\n    )\n\n    # crop or pad spectrogram to original dimensions\n    if new_time_dim > tf.shape(spectrogram)[0]:\n        spectrogram_resized = spectrogram_resized[: tf.shape(spectrogram)[0], :]\n    elif new_time_dim < tf.shape(spectrogram)[0]:\n        spectrogram_resized = tf.pad(\n            spectrogram_resized,\n            [[0, tf.shape(spectrogram)[0] - new_time_dim], [0, 0]],\n            mode=\"SYMMETRIC\",\n        )\n\n    return spectrogram_resized", "CodeBleu_score": 39.8752750102713}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(tf.shape(spectrogram)[0] * factor, tf.int32)\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [new_time_dim, tf.shape(spectrogram)[1]], method=method\n    )\n\n    # Adjust the resized spectrogram to the original dimensions\n    if new_time_dim > tf.shape(spectrogram)[0]:\n        resized_spectrogram = resized_spectrogram[: tf.shape(spectrogram)[0], :]\n    elif new_time_dim < tf.shape(spectrogram)[0]:\n        resized_spectrogram = tf.pad(\n            resized_spectrogram,\n            [[0, tf.shape(spectrogram)[0] - new_time_dim], [0, 0]],\n        )\n\n    return resized_spectrogram", "CodeBleu_score": 39.69203156841845}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\n    # Calculate the resizing factor\n    factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram\n    pitch_shifted_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], int(spectrogram.shape[1] * factor)],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    pitch_shifted_spectrogram = tf.pad(\n        pitch_shifted_spectrogram,\n        [[0, 0], [0, spectrogram.shape[1] - pitch_shifted_spectrogram.shape[1]]],\n    )\n\n    return pitch_shifted_spectrogram", "CodeBleu_score": 30.41578764346175}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    if semitone_shift == 0.0:\n        return spectrogram\n\n    # Calculate the resizing factor\n    factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram\n    spectrogram_resized = tf.image.resize(\n        spectrogram,\n        size=[spectrogram.shape[0], int(spectrogram.shape[1] / factor)],\n        method=method,\n        preserve_aspect_ratio=False,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    spectrogram_shifted = tf.image.resize_with_pad(\n        spectrogram_resized,\n        target_height=spectrogram.shape[0],\n        target_width=spectrogram.shape[1],\n        method=method,\n    )\n\n    return spectrogram_shifted", "CodeBleu_score": 35.283645475170815}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\n    # Calculate the resizing factor\n    resize_factor = 2.0 ** (semitone_shift / 12.0)\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        [spectrogram.shape[0], tf.cast(spectrogram.shape[1] * resize_factor, tf.int32)],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        [\n            [0, 0],\n            [0, tf.cast(spectrogram.shape[1] - resized_spectrogram.shape[1], tf.int32)],\n        ],\n    )\n\n    return padded_spectrogram", "CodeBleu_score": 31.079945562236468}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    if params[\"conv_activation\"] == \"relu\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"leakyrelu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\n            f\"The provided conv_activation parameter {params['conv_activation']} is not supported!\"\n        )\n\n", "CodeBleu_score": 29.98158120566636}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    if params[\"conv_activation\"] == \"relu\":\n        return torch.nn.ReLU()\n    elif params[\"conv_activation\"] == \"elu\":\n        return torch.nn.ELU()\n    elif params[\"conv_activation\"] == \"leaky_relu\":\n        return torch.nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\n            f\"The value of the 'conv_activation' parameter is not valid: {params['conv_activation']}\"\n        )\n\n", "CodeBleu_score": 31.11779278138619}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    if \"conv_activation\" in params:\n        if params[\"conv_activation\"] == \"relu\":\n            return torch.nn.ReLU()\n        elif params[\"conv_activation\"] == \"elu\":\n            return torch.nn.ELU()\n        elif params[\"conv_activation\"] == \"leaky_relu\":\n            return torch.nn.LeakyReLU(0.2)\n        else:\n            raise ValueError(\n                f\"{params['conv_activation']} is not a valid convolutional activation function.\"\n            )\n    else:\n        return torch.nn.LeakyReLU(0.2)\n\n", "CodeBleu_score": 33.76006406323635}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\n    # Create the output dictionary\n    output_dict = {}\n\n    # Iterate over the instruments\n    for instrument in instruments:\n\n        # Apply the U-Net model to the input tensor for the current instrument\n        logit_mask = unet(\n            input_tensor,\n            params=params,\n            instrument=instrument,\n            logit=True,\n        )\n\n        # Combine the logit masks using a softmax function along the specified axis\n        logit_mask = tf.math.softmax(logit_mask, axis=params[\"axis\"])\n\n        # Multiply the input tensor with the mask to obtain the spectrogram for the current instrument\n        output_dict[instrument] = tf.math.multiply(input_tensor, logit_mask)\n\n    return output_dict\n\n", "CodeBleu_score": 33.045806234160004}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\n    # set default parameters\n    params.setdefault(\"instrument_axis\", 1)\n    params.setdefault(\"instrument_axis_length\", 2)\n    params.setdefault(\"kernel_size\", 3)\n    params.setdefault(\"pool_size\", 2)\n    params.setdefault(\"num_layers\", 3)\n    params.setdefault(\"activation\", \"relu\")\n    params.setdefault(\"output_activation\", \"sigmoid\")\n    params.setdefault(\"initializer\", \"he_normal\")\n    params.setdefault(\"padding\", \"same\")\n    params.setdefault(\"output_padding\", \"same\")\n    params.setdefault(\"kernel_regularizer\", None)\n    params.setdefault(\"bias_regularizer\", None)\n    params.setdefault(\"activity_regularizer\", None)\n    params.setdefault(\"kernel_constraint\", None)\n    params.setdefault(\"bias_constraint\", None)\n    params.setdefault(\"use_batch_norm\", False)\n    params.setdefault(\"dropout\", 0)\n    params.setdefault(\"dropout_type\", \"spatial\")\n    params.setdefault(\"name\", None)\n\n    # check for valid parameters\n    if params[\"instrument_axis\"] not in [0, 1, 2, 3]:\n        raise ValueError(\n            \"The parameter 'instrument_axis' must be an integer in the range [0, 3].\"\n        )\n    if params[\"instrument_axis_length\"] not in [1, 2]:\n        raise ValueError(\n            \"The parameter 'instrument_axis_length' must be an integer in the range [1, 2].\"\n        )\n    if params[\"kernel_size\"] not in [1, 3, 5, 7]:\n        raise ValueError(\n            \"The parameter 'kernel_size' must be an integer in the range [1, 7].\"\n        )\n    if params[\"pool_size\"] not in [1, 2, 3]:\n        raise ValueError(\n            \"The parameter 'pool_size' must be an integer in the range [1, 3].\"\n        )\n    if", "CodeBleu_score": 15.066799524788067}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n    # create a dictionary to hold the masks for each instrument\n    logit_masks = {}\n    # iterate over the instruments\n    for instrument in instruments:\n        # create a dictionary to hold the parameters for the instrument\n        instrument_params = {}\n        # iterate over the parameters\n        for param in params:\n            # if the parameter is a list, get the corresponding element of the list\n            if isinstance(params[param], list):\n                instrument_params[param] = params[param][instruments.index(instrument)]\n            # otherwise, assign the parameter to the dictionary\n            else:\n                instrument_params[param] = params[param]\n        # apply the U-Net model to the input tensor for the instrument\n        logit_masks[instrument] = unet(input_tensor, **instrument_params)\n    # create a dictionary to hold the spectrograms for each instrument\n    spectrograms = {}\n    # iterate over the instruments\n    for instrument in instruments:\n        # apply a softmax function along the specified axis to the logit masks for the instrument\n        mask = tf.nn.softmax(logit_masks[instrument], axis=params[\"axis\"])\n        # multiply the mask with the input tensor to obtain the spectrogram for the instrument\n        spectrograms[instrument] = tf.multiply(mask, input_tensor)\n    # return the dictionary of spectrograms\n    return spectrograms", "CodeBleu_score": 37.54828337509465}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n    import hashlib\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n", "CodeBleu_score": 37.024896758346344}, {"code": "def compute_file_checksum(path)\n    import hashlib\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        while True:\n            data = f.read(65536)\n            if not data:\n                break\n            sha256_hash.update(data)\n        return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 37.123835917334844}, {"code": "def compute_file_checksum(path)\n    import hashlib\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        # Read and update hash string value in blocks of 4K\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n", "CodeBleu_score": 52.430555747644036}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n    url = f'{self.host}/{self.repository}/{self.release}/{self.checksum_index}'\n    response = requests.get(url)\n    response.raise_for_status()\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(f'Model {name} not found in checksum index')\n    return checksum_index[name]", "CodeBleu_score": 33.96500358655998}, {"code": "def checksum(self, name: str) -> str\n    url = self.host + self.repository + self.release + self.checksum_index\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(\n            \"The request to the checksum index returned an unsuccessful response.\"\n        )\n    json_response = response.json()\n    if name not in json_response:\n        raise ValueError(\n            \"The specified model name was not found in the checksum index.\"\n        )\n    return json_response[name]", "CodeBleu_score": 37.8308620225601}, {"code": "def checksum(self, name: str) -> str\n\n    # Construct the URL\n    url = f\"{self.host}/{self.repository}/{self.release}/{self.checksum_index}\"\n\n    # Send the request and check for a successful response\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Request to {url} failed with status code {response.status_code}\")\n\n    # Parse the response as JSON and check if the model name is in the index\n    index = response.json()\n    if name not in index:\n        raise ValueError(f\"Model {name} not found in checksum index\")\n\n    # Return the checksum for the specified model name\n    return index[name]\n\n", "CodeBleu_score": 39.28896418338175}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n\n    url = self.get_url(name)\n    print(f\"Downloading {name} from {url} to {path}\")\n    r = requests.get(url, stream=True)\n    r.raise_for_status()\n    total_size = int(r.headers.get('content-length', 0))\n    block_size = 1024\n    temp_file = os.path.join(path, f\"{name}.zip\")\n    with open(temp_file, 'wb') as f:\n        for data in tqdm(r.iter_content(block_size), total=math.ceil(total_size//block_size), unit='KB', unit_scale=True):\n            f.write(data)\n\n    print(f\"Validating checksum for {name}\")\n    checksum = self.get_checksum(name)\n    if not validate_checksum(temp_file, checksum):\n        raise ValueError(f\"Checksum validation failed for {name}\")\n\n    print(f\"Extracting {name} to {path}\")\n    with zipfile.ZipFile(temp_file, 'r') as zip_ref:\n        zip_ref.extractall(path)\n\n    os.remove(temp_file)\n    print(f\"Extraction completed for {name}\")", "CodeBleu_score": 35.44776601164665}, {"code": "def download(self, name: str, path: str) -> None:\n    url = f\"https://storage.googleapis.com/mosaic-models/{name}.zip\"\n    logger.info(f\"Downloading model from {url}\")\n    with requests.get(url, stream=True) as r:\n        r.raise_for_status()\n        with open(path, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n    logger.info(f\"Download complete\")\n\n    checksum = get_checksum(path)\n    if checksum != self.checksum:\n        raise ValueError(\n            f\"The checksum of the downloaded archive ({checksum}) does not match the expected value ({self.checksum})\"\n        )\n\n    logger.info(f\"Extracting archive to {path}\")\n    with zipfile.ZipFile(path, \"r\") as z:\n        z.extractall(self.path)\n    os.remove(path)\n    logger.info(f\"Extraction complete\")\n\n", "CodeBleu_score": 33.1260431708117}, {"code": "def download(self, name: str, path: str) -> None:\n    self.download_url = f\"{self.base_url}/{name}.zip\"\n    self.checksum = self.checksums[name]\n    self.tmp_path = self.tmp_dir / f\"{name}.zip\"\n    self.extract_path = Path(path)\n    self.download_initiation_log = f\"Downloading {self.download_url} to {self.tmp_path}\"\n    self.download_completion_log = f\"Downloaded {self.download_url} to {self.tmp_path}\"\n    self.checksum_log = f\"Checksum for {name} is {self.checksum}\"\n    self.checksum_pass_log = f\"Checksum for {self.tmp_path} passed\"\n    self.checksum_fail_log = f\"Checksum for {self.tmp_path} failed\"\n    self.extract_initiation_log = f\"Extracting {self.tmp_path} to {self.extract_path}\"\n    self.extract_completion_log = f\"Extracted {self.tmp_path} to {self.extract_path}\"\n    self.download_initiation_log = f\"Downloading {self.download_url} to {self.tmp_path}\"\n    self.download_completion_log = f\"Downloaded {self.download_url} to {self.tmp_path}\"\n    self.checksum_log = f\"Checksum for {name} is {self.checksum}\"\n    self.checksum_pass_log = f\"Checksum for {self.tmp_path} passed\"\n    self.checksum_fail_log = f\"Checksum for {self.tmp_path} failed\"\n    self.extract_initiation_log = f\"Extracting {self.tmp_path} to {self.extract_path}\"\n    self.extract_completion_log = f\"Extracted {self.tmp_path} to {self.extract_path}\"\n    self.download_initiation_log = f\"Downloading {self.download_url} to {self.tmp_path}\"\n    self.download_completion_log = f\"Downloaded {self.download_", "CodeBleu_score": 20.906107093983024}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n    if descriptor.startswith(\"embedded:\"):\n        name = descriptor[9:]\n        try:\n            with open(f\"{__file__[:-12]}/config/{name}.json\") as f:\n                return json.load(f)\n        except:\n            raise FileNotFoundError(f\"Embedded configuration {name} not found\")\n    else:\n        if not os.path.exists(descriptor):\n            raise FileNotFoundError(f\"Configuration file {descriptor} not found\")\n        with open(descriptor) as f:\n            return json.load(f)\n\n", "CodeBleu_score": 45.501533402326736}, {"code": "def load_configuration(descriptor: str) -> Dict\n    import json\n    import os\n    from pathlib import Path\n    from typing import Dict\n\n    if descriptor.startswith(\"embedded://\"):\n        # Extract the name of the embedded configuration\n        name = descriptor[12:]\n        # Load the embedded configuration as a resource\n        resource = pkg_resources.resource_stream(__name__, name)\n        # Load the configuration as JSON\n        config = json.load(resource)\n    else:\n        # Check if the file exists\n        if not os.path.exists(descriptor):\n            raise FileNotFoundError(f\"The configuration file {descriptor} does not exist.\")\n        # Load the configuration as JSON\n        with open(Path(descriptor)) as config_file:\n            config = json.load(config_file)\n\n    return config\n\n", "CodeBleu_score": 42.63455327413065}, {"code": "def load_configuration(descriptor: str) -> Dict\n    import json\n    import os\n    from . import resources\n    from . import utils\n\n    if descriptor.startswith('embedded:'):\n        name = descriptor[9:]\n        if name in resources.list():\n            return json.loads(resources.read(name))\n        else:\n            raise FileNotFoundError('Embedded configuration not found: ' + name)\n    else:\n        if os.path.isfile(descriptor):\n            with open(descriptor, 'r') as f:\n                return json.load(f)\n        else:\n            raise FileNotFoundError('Configuration file not found: ' + descriptor)\n\n", "CodeBleu_score": 34.25874345039619}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"The concatenation axis must be either 0 or 1.\")\n\n    # Concatenate the tensors\n    tensor_dict_concat = tf.concat(\n        [tensor_dict[key] for key in tensor_dict], axis=concat_axis\n    )\n\n    # Apply the function\n    tensor_dict_concat = func(tensor_dict_concat)\n\n    # Split the tensor\n    tensor_dict_concat_split = tf.split(tensor_dict_concat, len(tensor_dict), axis=concat_axis)\n\n    # Create the new dictionary\n    tensor_dict_processed = {\n        key: tensor_dict_concat_split[index]\n        for index, key in enumerate(tensor_dict)\n    }\n\n    return tensor_dict_processed", "CodeBleu_score": 40.12004130660485}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"The concatenation axis must be either 0 or 1, but {concat_axis} was given.\"\n        )\n\n    # Concatenate the tensors\n    if concat_axis == 0:\n        concatenated_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    else:\n        concatenated_tensor = tf.concat(\n            [tf.expand_dims(t, axis=concat_axis) for t in tensor_dict.values()],\n            axis=concat_axis,\n        )\n\n    # Apply the function to the concatenated tensor\n    processed_tensor = func(concatenated_tensor)\n\n    # Split the processed tensor back into the original dictionary format\n    if concat_axis == 0:\n        split_tensors = tf.split(processed_tensor, num_or_size_splits=len(tensor_dict))\n    else:\n        split_tensors = tf.split(\n            processed_tensor, num_or_size_splits=len(tensor_dict), axis=concat_axis\n        )\n\n    # Create a new dictionary with the processed tensors\n    processed_dict = {}\n    for key, split_tensor in zip(tensor_dict.keys(), split_tensors):\n        processed_dict[key] = split_tensor\n\n    return processed_dict\n\n", "CodeBleu_score": 43.26593540048693}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n\n    if concat_axis not in (0, 1):\n        raise ValueError(\n            f\"concat_axis must be either 0 or 1, but got {concat_axis} instead.\"\n        )\n\n    concat_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    processed_tensor = func(concat_tensor)\n\n    if concat_axis == 0:\n        processed_tensor_dict = {\n            key: processed_tensor[i] for i, key in enumerate(tensor_dict.keys())\n        }\n    else:\n        processed_tensor_dict = {\n            key: processed_tensor[:, i] for i, key in enumerate(tensor_dict.keys())\n        }\n\n    return processed_tensor_dict\n\n", "CodeBleu_score": 44.66167662384079}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\n    if tensor.dtype != tf.float32:\n        raise ValueError(\"Tensor must be of type float32.\")\n\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    tensor_normalized = (tensor - tensor_min) / (tensor_max - tensor_min)\n    tensor_uint8 = tf.cast(tensor_normalized * 255, tf.uint8)\n\n    return {\n        tensor_key: tensor_uint8,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 68.76017560976135}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n\n    tensor_scaled = tf.cast(\n        (((tensor - tensor_min) * 255) / (tensor_max - tensor_min)),\n        tf.uint8,\n    )\n\n    return {\n        tensor_key: tensor_scaled,\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }\n\n", "CodeBleu_score": 57.73655114819448}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n\n    return {\n        tensor_key: tf.cast(\n            tf.round(((tensor - tensor_min) / (tensor_max - tensor_min)) * 255),\n            tf.uint8,\n        ),\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }", "CodeBleu_score": 64.04738245301868}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    tensor_len = tf.shape(tensor)[0]\n    pad_len = tf.cast(tf.math.ceil(tensor_len / segment_len), tf.int32) * segment_len - tensor_len\n    padded = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded, [-1, segment_len, tf.shape(tensor)[1]])\n\n", "CodeBleu_score": 29.785877724302228}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    pad_len = segment_len - tf.shape(tensor)[0] % segment_len\n    padded = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded, [-1, segment_len] + padded.shape[1:].as_list())\n\n", "CodeBleu_score": 22.86902693768712}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    tensor_len = tf.shape(tensor)[0]\n    pad_len = tf.cast(tf.math.ceil(tensor_len / segment_len), tf.int32) * segment_len - tensor_len\n    padded = tf.pad(tensor, [[0, pad_len], [0, 0]])\n    return tf.reshape(padded, [-1, segment_len] + padded.shape[1:])\n\n", "CodeBleu_score": 27.262539460898832}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\n    # Calculate the number of extra rows needed for padding\n    n_rows = frame_length - instr_spec.shape[1]\n\n    # Create an extension of zeros\n    extension = torch.zeros(instr_spec.shape[0], n_rows, instr_spec.shape[2])\n\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    extended_spec = torch.cat([instr_spec, extension], dim=1)\n\n    # Reshape the extended spectrogram by merging the first two dimensions\n    return extended_spec.reshape(instr_spec.shape[0] * F, frame_length)\n\n", "CodeBleu_score": 15.909134447246098}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n\n    # Calculate the number of extra rows needed for padding\n    extra_rows = frame_length - instr_spec.shape[0] % frame_length\n    # Create an extension of zeros\n    extension = torch.zeros(extra_rows, instr_spec.shape[1], dtype=torch.float32)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec_extended = torch.cat((instr_spec, extension), dim=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec_extended = instr_spec_extended.reshape(-1, F)\n\n    return instr_spec_extended\n\n", "CodeBleu_score": 16.30467735988293}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    # Calculate the number of extra rows needed for padding\n    extra_rows = (frame_length - instr_spec.shape[1]) // 2\n    # Create an extension of zeros\n    extension = torch.zeros(instr_spec.shape[0], extra_rows, instr_spec.shape[2])\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    instr_spec = torch.cat((extension, instr_spec, extension), dim=1)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(instr_spec.shape[0] * instr_spec.shape[1], instr_spec.shape[2])\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(instr_spec.shape[0] * instr_spec.shape[1], instr_spec.shape[2])\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(instr_spec.shape[0] * instr_spec.shape[1], instr_spec.shape[2])\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(instr_spec.shape[0] * instr_spec.shape[1], instr_spec.shape[2])\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(instr_spec.shape[0] * instr_spec.shape[1], instr_spec.shape[2])\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(instr_spec.shape[0] * instr_spec.shape[1], instr_spec.shape[2])\n    # Reshape the extended spectrogram by merging the first two dimensions\n    instr_spec = instr_spec.reshape(in", "CodeBleu_score": 16.784259084429127}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    import pandas as pd\n    import tensorflow as tf\n    from tensorflow.python.data.ops import dataset_ops\n\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path, **kwargs)\n\n    # Convert the DataFrame columns into a TensorFlow dataset\n    dataset = dataset_ops.Dataset.from_tensor_slices(dict(df))\n\n    return dataset\n\n", "CodeBleu_score": 32.00335699794863}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    import pandas as pd\n    import tensorflow as tf\n\n    # read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path, **kwargs)\n\n    # convert the DataFrame into a TensorFlow dataset\n    dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n\n    return dataset", "CodeBleu_score": 36.47017546402109}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    import pandas as pd\n    import tensorflow as tf\n\n    df = pd.read_csv(csv_path)\n    ds = tf.data.Dataset.from_tensor_slices(dict(df))\n    return ds", "CodeBleu_score": 24.704851579432535}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i, target_dim in enumerate(target_shape):\n        if target_dim is not None:\n            result = result and (tensor_tf.shape[i] == target_dim)\n    return result\n\n", "CodeBleu_score": 33.11617824719868}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i in range(len(target_shape)):\n        if target_shape[i] is not None:\n            result = result and (tensor_tf.shape[i] == target_shape[i])\n    return result\n\n", "CodeBleu_score": 31.157250730347887}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i in range(len(target_shape)):\n        if target_shape[i] is not None:\n            result = result and (tensor_tf.shape[i] == target_shape[i])\n    return result\n\n", "CodeBleu_score": 31.157250730347887}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        seed=audio_params[\"seed\"],\n    )\n    return builder.build(\n        train_csv=audio_params[\"train_csv\"],\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment_data=audio_params[\"augment_data\"],\n        cache_data=audio_params[\"cache_data\"],\n    )\n\n", "CodeBleu_score": 41.54630376360007}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\n    # Initialize dataset builder\n    builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        dataset_dir=audio_path,\n        chunk_duration=10,\n        seed=1,\n    )\n\n    # Build dataset\n    dataset = builder.build(\n        train_csv_path=\"train.csv\",\n        cache_dir=\"cache\",\n        batch_size=16,\n        num_workers=4,\n        num_chunks_per_song=10,\n        augment_audio_parameters={},\n        augment_spec_parameters={},\n        use_cache=True,\n    )\n\n    return dataset\n\n", "CodeBleu_score": 31.23893171348619}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n\n    # Initialize DatasetBuilder\n    dataset_builder = DatasetBuilder(\n        audio_params=audio_params,\n        audio_adapter=audio_adapter,\n        audio_path=audio_path,\n        chunk_duration=chunk_duration,\n        seed=seed,\n    )\n\n    # Build training dataset\n    training_dataset = dataset_builder.build(\n        train_csv=train_csv,\n        cache_dir=cache_dir,\n        batch_size=batch_size,\n        num_chunks_per_song=num_chunks_per_song,\n        augment=augment,\n        cache=cache,\n    )\n\n    return training_dataset\n\n", "CodeBleu_score": 27.899898887492082}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n    if not self._instrument_builders:\n        for instrument in self._instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(\n                    self._instrument_path, instrument, self._start_date, self._end_date\n                )\n            )\n    for builder in self._instrument_builders:\n        yield builder\n\n", "CodeBleu_score": 52.56092883054507}, {"code": "def instruments(self) -> Any\n    if self._instruments is None:\n        self._instruments = []\n        for instrument in self._instruments_list:\n            self._instruments.append(InstrumentDatasetBuilder(self._dataset_id, instrument))\n\n    for instrument in self._instruments:\n        yield instrument\n\n", "CodeBleu_score": 50.000178048473956}, {"code": "def instruments(self) -> Any\n    if self._instrument_builders is None:\n      self._instrument_builders = []\n      for instrument in self._instruments:\n        self._instrument_builders.append(\n            InstrumentDatasetBuilder(\n                instrument, self._dataset_builder_config, self._decoder))\n    for instrument_builder in self._instrument_builders:\n      yield instrument_builder\n\n", "CodeBleu_score": 60.14101787858303}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\n    if cache is not None:\n        if wait:\n            while not os.path.exists(os.path.join(cache, \"index\")):\n                self.logger.info(\"Waiting for cache to be created...\")\n                time.sleep(10)\n        if not os.path.exists(cache):\n            os.makedirs(cache)\n        return dataset.cache(cache)\n    else:\n        return dataset", "CodeBleu_score": 55.489871387866785}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is not None:\n        cache_path = Path(cache)\n        if wait:\n            while not (cache_path / \"index\").exists():\n                self.logger.info(f\"Waiting for cache at {cache_path}\")\n                time.sleep(10)\n        if not cache_path.exists():\n            self.logger.info(f\"Creating cache directory at {cache_path}\")\n            cache_path.mkdir(parents=True)\n        self.logger.info(f\"Caching dataset at {cache_path}\")\n        dataset.save_to_disk(cache_path)\n    return dataset", "CodeBleu_score": 45.376642762554795}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n\n    if cache is None:\n        return dataset\n    else:\n        from pathlib import Path\n        from os import makedirs\n        from os.path import exists\n        from time import sleep\n        from .. import logger\n\n        cache_path = Path(cache)\n        if not cache_path.exists():\n            makedirs(cache_path)\n\n        if wait:\n            while not exists(cache_path / \"index\"):\n                logger.info(f\"Waiting for cache at {cache_path}\")\n                sleep(10)\n\n        return dataset.cache(cache_path)", "CodeBleu_score": 41.27427131755007}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\n    model_provider = ModelProvider(params[\"model_dir\"])\n    model_provider.update_parameters(params)\n\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n\n    if MWF:\n        estimator = tf.estimator.Estimator(\n            model_fn=model_provider.model_fn,\n            model_dir=model_provider.model_dir,\n            params=params,\n            config=run_config,\n            warm_start_from=model_provider.model_dir,\n        )\n    else:\n        estimator = tf.estimator.Estimator(\n            model_fn=model_provider.model_fn,\n            model_dir=model_provider.model_dir,\n            params=params,\n            config=run_config,\n        )\n\n    return estimator\n\n", "CodeBleu_score": 38.27924803900494}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_provider = ModelProvider(params['model_dir'])\n    model_dir = model_provider.get_model_dir()\n    if MWF:\n        model_dir = model_dir + '/model.ckpt-0'\n    params['model_dir'] = model_dir\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(\n        model_dir=model_dir,\n        session_config=session_config\n    )\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(),\n        model_dir=model_dir,\n        params=params,\n        config=run_config\n    )\n    return estimator\n\n", "CodeBleu_score": 44.53037652494989}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n\n    # Load the model directory using a ModelProvider\n    model_dir = ModelProvider(params).get_model_dir()\n\n    # Update the parameters\n    params = update_params(params, model_dir)\n\n    # Set up the session configuration to limit GPU memory usage\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.7\n\n    # Create a RunConfig with the session configuration\n    run_config = tf.estimator.RunConfig(session_config=config)\n\n    # Initialize and return a TensorFlow estimator using the specified model function, model directory, parameters, and configuration\n    return tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, params=params, config=run_config)\n\n", "CodeBleu_score": 28.051343153412606}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\n    # Ensure the input waveform is stereo\n    if audio_descriptor.num_channels != 2:\n        waveform = self._convert_to_stereo(waveform, audio_descriptor)\n\n    # Obtain a prediction generator\n    prediction_generator = self._get_prediction_generator(\n        audio_descriptor.sample_rate\n    )\n\n    # Update the data generator with the waveform and audio descriptor\n    prediction_generator.update_data(\n        waveform, audio_descriptor\n    )\n\n    # Generate a prediction\n    prediction = next(prediction_generator)\n\n    # Remove the \"audio_id\" key from the prediction\n    del prediction[\"audio_id\"]\n\n    return prediction\n\n", "CodeBleu_score": 53.649523049423294}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n\n    # Ensure the input waveform is stereo\n    if waveform.ndim == 1:\n        waveform = np.expand_dims(waveform, axis=0)\n    if waveform.ndim == 2:\n        waveform = np.expand_dims(waveform, axis=0)\n    if waveform.shape[0] != 2:\n        raise ValueError(\"The input waveform must be stereo.\")\n\n    # Obtain a prediction generator\n    prediction_generator = self._obtain_prediction_generator()\n\n    # Update the data generator with the waveform and audio descriptor\n    prediction_generator.update(\n        waveform=waveform,\n        audio_descriptor=audio_descriptor\n    )\n\n    # Generate a prediction\n    prediction = next(prediction_generator)\n\n    # Remove the \"audio_id\" key from the prediction\n    del prediction[\"audio_id\"]\n\n    return prediction\n\n", "CodeBleu_score": 55.46804432492212}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    if audio_descriptor.channels != 2:\n        raise ValueError(\"TensorFlow requires stereo audio.\")\n    prediction_generator = self._get_prediction_generator(audio_descriptor)\n    prediction_generator.update(waveform, audio_descriptor)\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction", "CodeBleu_score": 52.76843977354779}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Currently, we only support ResNet backbones\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return [\n        image_shape[:2] // stride for stride in config.BACKBONE_STRIDES\n    ]\n\n", "CodeBleu_score": 52.92737156502222}, {"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        # Currently supports ResNet only.\n        assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n        return [\n            image_shape[:2] // stride\n            for stride in config.BACKBONE_STRIDES\n        ]\n\n", "CodeBleu_score": 50.477258471610355}, {"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only.\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return [\n        image_shape[:2] // stride\n        for stride in config.BACKBONE_STRIDES\n    ]\n\n", "CodeBleu_score": 53.70318257080007}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n\n    # Split the window coordinates into individual components\n    w_coords = tf.unstack(window)\n\n    # Split the box coordinates into individual components\n    boxes_coords = tf.unstack(boxes, axis=1)\n\n    # Clip the coordinates\n    boxes_coords = [tf.clip_by_value(boxes_coords[i], w_coords[i], w_coords[i+2]) for i in range(4)]\n\n    # Concatenate the coordinates back into a tensor\n    boxes = tf.stack(boxes_coords, axis=1)\n\n    # Return the clipped boxes with a fixed shape\n    return boxes", "CodeBleu_score": 12.45330279904105}, {"code": "def clip_boxes_graph(boxes, window)\n    x1, y1, x2, y2 = tf.split(boxes, 4, axis=1)\n    w = x2 - x1\n    h = y2 - y1\n    x1 = tf.clip_by_value(x1, window[0], window[2])\n    x2 = tf.clip_by_value(x2, window[0], window[2])\n    y1 = tf.clip_by_value(y1, window[1], window[3])\n    y2 = tf.clip_by_value(y2, window[1], window[3])\n    return tf.concat([x1, y1, x2, y2], axis=1)\n\n", "CodeBleu_score": 27.51660283432412}, {"code": "def clip_boxes_graph(boxes, window)\n\n    # Split the box coordinates into individual components\n    x1, y1, x2, y2 = tf.split(boxes, 4, axis=-1)\n\n    # Split the window coordinates into individual components\n    w_start, w_end, h_start, h_end = tf.split(window, 4, axis=-1)\n\n    # Clip the box coordinates\n    x1 = tf.clip_by_value(x1, w_start, w_end)\n    y1 = tf.clip_by_value(y1, h_start, h_end)\n    x2 = tf.clip_by_value(x2, w_start, w_end)\n    y2 = tf.clip_by_value(y2, h_start, h_end)\n\n    # Concatenate the box coordinates back into a tensor\n    boxes = tf.concat([x1, y1, x2, y2], axis=-1)\n\n    # Return the clipped boxes\n    return boxes\n\n", "CodeBleu_score": 17.813254425072664}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\n    # The shared convolutional layer\n    shared = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\")(\n        feature_map\n    )\n\n    # Class scores\n    x = Conv2D(2 * anchors_per_location, (1, 1), activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_out_class\")(\n        shared\n    )\n\n    # Reshape to [batch, anchors, 2]\n    molded_class_logits = Reshape((-1, 2))(x)\n\n    # Softmax activation\n    class_probs = Activation(\"softmax\", name=\"rpn_out_class_softmax\")(molded_class_logits)\n\n    # Bounding box regression deltas\n    x = Conv2D(anchors_per_location * 4, (1, 1), activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_out_regress\")(\n        shared\n    )\n\n    # Reshape to [batch, anchors, 4]\n    molded_bbox_deltas = Reshape((-1, 4))(x)\n\n    return [molded_class_logits, class_probs, molded_bbox_deltas]\n\n", "CodeBleu_score": 37.02713690538316}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\n    # Apply a shared convolutional layer to the feature map\n    shared_layer = mx.symbol.Convolution(data=feature_map,\n                                         kernel=(3, 3),\n                                         pad=(1, 1),\n                                         num_filter=512,\n                                         name=\"rpn_conv_shared\")\n    # Apply a convolutional layer to the shared layer for class scores\n    cls_score = mx.symbol.Convolution(data=shared_layer,\n                                      kernel=(1, 1),\n                                      pad=(0, 0),\n                                      num_filter=2 * anchors_per_location,\n                                      name=\"rpn_cls_score\")\n    # Apply a convolutional layer to the shared layer for bounding box predictions\n    bbox_pred = mx.symbol.Convolution(data=shared_layer,\n                                      kernel=(1, 1),\n                                      pad=(0, 0),\n                                      num_filter=4 * anchors_per_location,\n                                      name=\"rpn_bbox_pred\")\n\n    # Reshape the class scores to [batch_size, 2, num_anchors, height, width]\n    cls_score_reshape = mx.symbol.Reshape(data=cls_score,\n                                          shape=(0, 2, -1, 0, 0),\n                                          name=\"rpn_cls_score_reshape\")\n    # Reshape the bounding box predictions to [batch_size, 4, num_anchors, height, width]\n    bbox_pred_reshape = mx.symbol.Reshape(data=bbox_pred,\n                                          shape=(0, 4, -1, 0, 0),\n                                          name=\"rpn_bbox_pred_reshape\")\n\n    # Transpose the class scores to [batch_size, num_anchors, height, width, 2]\n    cls_prob = mx.symbol.transpose(data=cls_score_reshape,\n                                   axes=(0, 2, 3, 4, 1),", "CodeBleu_score": 19.500113127147735}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n\n    # Shared convolutional layer\n    shared = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv_shared\")(\n        feature_map\n    )\n\n    # Anchor classifier\n    x = Conv2D(2 * anchors_per_location, (1, 1), activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_out_class\")(\n        shared\n    )\n    # Reshape to [batch, anchors, 2]\n    x = Reshape((-1, 2))(x)\n    # Softmax activation\n    x = Activation(\"softmax\", name=\"rpn_out_class_sm\")(x)\n\n    # Bounding box regressor\n    y = Conv2D(anchors_per_location * 4, (1, 1), activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_out_regress\")(\n        shared\n    )\n    # Reshape to [batch, anchors, 4]\n    y = Reshape((-1, 4))(y)\n\n    return [x, y, feature_map]\n\n", "CodeBleu_score": 34.622115004668785}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n    abs_diff = tf.abs(y_true - y_pred)\n    mask = tf.less(abs_diff, 1)\n    squared_diff = 0.5 * tf.square(abs_diff)\n    linear_diff = abs_diff - 0.5\n    squared_mask = tf.cast(mask, dtype=tf.float32)\n    linear_mask = 1 - squared_mask\n    return squared_mask * squared_diff + linear_mask * linear_diff", "CodeBleu_score": 26.992181404124903}, {"code": "def smooth_l1_loss(y_true, y_pred)\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n", "CodeBleu_score": 60.928513203413225}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n    \n    # Reset the graph\n    tf.reset_default_graph()\n    \n    # Define the input layer\n    X_input = tf.placeholder(tf.float32, shape=(None, x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n    \n    # Define the model\n    X = tf.layers.conv2d(X_input, 32, (3, 3), activation=tf.nn.relu)\n    X = tf.layers.conv2d(X, 64, (3, 3), activation=tf.nn.relu)\n    X = tf.layers.max_pooling2d(X, (2, 2), (2, 2), padding='same')\n    X = tf.contrib.layers.flatten(X)\n    X = tf.layers.dense(X, 512, activation=tf.nn.relu)\n    X = tf.layers.dense(X, num_classes, activation=tf.nn.softmax)\n    \n    # Define the model output\n    model = tf.estimator.Estimator(model_fn=lambda features, labels, mode: tf.estimator.EstimatorSpec(mode,\n                                                                                                      loss=tf.losses.softmax_cross_entropy(tf.one_hot(labels, num_classes), X),\n                                                                                                      train_op=tf.train.AdamOptimizer().minimize(tf.losses.softmax_cross_entropy(tf.one_hot(labels, num_classes), X),\n                                                                                                                                              global_step=tf.train.get_global_step()),\n                                                                                                      predictions={'probabilities': X},\n                                                                                                      eval_metric_ops={'accuracy': tf.metrics.accuracy(labels=labels,\n                                                                                                                                                      predictions=tf.argmax(X, axis=1))}))\n    \n    return model", "CodeBleu_score": 27.15531242146071}, {"code": "def build_model(x_train, num_classes)\n    # Reset the graph\n    tf.reset_default_graph()\n\n    # Define the input layer\n    X_input = tf.placeholder(tf.float32, shape=(None, x_train.shape[1], x_train.shape[2], x_train.shape[3]))\n\n    # Define the model\n    X = tf.layers.conv2d(X_input, filters=16, kernel_size=[2, 2], padding='same', activation=tf.nn.relu)\n    X = tf.layers.conv2d(X, filters=32, kernel_size=[2, 2], padding='same', activation=tf.nn.relu)\n    X = tf.layers.max_pooling2d(X, pool_size=[2, 2], strides=2)\n    X = tf.layers.flatten(X)\n    X = tf.layers.dense(X, units=64, activation=tf.nn.relu)\n    X = tf.layers.dense(X, units=num_classes, activation=tf.nn.softmax)\n\n    return X_input, X", "CodeBleu_score": 27.795047586617255}, {"code": "def build_model(x_train, num_classes)\n    from tensorflow.keras import models\n    from tensorflow.keras import layers\n    from tensorflow.keras import optimizers\n    from tensorflow.keras import losses\n    from tensorflow.keras import metrics\n\n    # Reset the TensorFlow graph.\n    tf.keras.backend.clear_session()\n\n    # Define the input layer.\n    input_layer = layers.Input(shape=x_train.shape[1:])\n\n    # Add the first convolutional layer.\n    conv1 = layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(input_layer)\n\n    # Add the second convolutional layer.\n    conv2 = layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(conv1)\n\n    # Add the max-pooling layer.\n    pool1 = layers.MaxPool2D(pool_size=2)(conv2)\n\n    # Flatten the output of the previous layer.\n    flatten = layers.Flatten()(pool1)\n\n    # Add the first dense layer.\n    dense1 = layers.Dense(units=128, activation='relu')(flatten)\n\n    # Add the second dense layer.\n    dense2 = layers.Dense(units=num_classes, activation='softmax')(dense1)\n\n    # Build the model.\n    model = models.Model(inputs=input_layer, outputs=dense2)\n\n    # Compile the model.\n    model.compile(optimizer=optimizers.Adam(), loss=losses.categorical_crossentropy, metrics=[metrics.categorical_accuracy])\n\n    # Return the compiled model.\n    return model\n", "CodeBleu_score": 36.54987847775026}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n    # initialize array to store bboxes\n    bboxes = np.zeros((mask.shape[0], 4), dtype=np.int32)\n\n    # loop through mask layers\n    for i in range(mask.shape[0]):\n        # get horizontal and vertical indices where mask is present\n        hor_idx = np.where(np.any(mask[i, :, :], axis=0))[0]\n        ver_idx = np.where(np.any(mask[i, :, :], axis=1))[0]\n\n        # set bounding box based on indices\n        if hor_idx.shape[0]:\n            bboxes[i, :2] = (np.min(hor_idx), np.min(ver_idx))\n            bboxes[i, 2:] = (np.max(hor_idx), np.max(ver_idx))\n\n    return bboxes\n\n", "CodeBleu_score": 20.860680677276903}, {"code": "def extract_bboxes(mask)\n    boxes = []\n    for mask_layer in mask:\n        if mask_layer.any():\n            horizontal_indices = mask_layer.any(axis=0)\n            vertical_indices = mask_layer.any(axis=1)\n            x1, x2 = horizontal_indices.argmax(), len(horizontal_indices) - horizontal_indices[::-1].argmax()\n            y1, y2 = vertical_indices.argmax(), len(vertical_indices) - vertical_indices[::-1].argmax()\n            box = [x1, y1, x2, y2]\n        else:\n            box = [0, 0, 0, 0]\n        boxes.append(box)\n    return boxes\n\n", "CodeBleu_score": 29.72607094612183}, {"code": "def extract_bboxes(mask)\n    # initialize array to store bboxes\n    bboxes = np.zeros((mask.shape[0], 4), dtype=np.int32)\n    # loop through mask layers\n    for i in range(mask.shape[0]):\n        # calculate horizontal and vertical indices where mask is present\n        horizontal_indices = np.where(np.any(mask[i, :, :], axis=0))[0]\n        vertical_indices = np.where(np.any(mask[i, :, :], axis=1))[0]\n        # set bounding box to zeros if no mask is present\n        if horizontal_indices.shape[0] == 0 or vertical_indices.shape[0] == 0:\n            bboxes[i, :] = 0\n        else:\n            # determine bbox coordinates\n            x1 = horizontal_indices[0]\n            x2 = horizontal_indices[-1]\n            y1 = vertical_indices[0]\n            y2 = vertical_indices[-1]\n            # store bbox coordinates\n            bboxes[i, 0] = x1\n            bboxes[i, 1] = y1\n            bboxes[i, 2] = x2\n            bboxes[i, 3] = y2\n    return bboxes\n\n", "CodeBleu_score": 27.948488526292287}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # Calculate the intersection areas\n    x1 = np.maximum(box[0], boxes[:, 0])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    # Calculate the union areas\n    union = box_area + boxes_area[:] - intersection[:]\n    # Calculate the IoU\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 55.49021365037661}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(y2 - y1, 0) * np.maximum(x2 - x1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n", "CodeBleu_score": 56.12768883353916}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # Calculate the intersection areas\n    intersection_left_right = np.maximum(boxes[:, 0], box[0])\n    intersection_top_bottom = np.maximum(boxes[:, 1], box[1])\n    intersection_right_left = np.minimum(boxes[:, 2], box[2])\n    intersection_bottom_top = np.minimum(boxes[:, 3], box[3])\n    intersection_width_height = np.maximum(intersection_right_left - intersection_left_right, 0)\n    intersection_height_width = np.maximum(intersection_bottom_top - intersection_top_bottom, 0)\n    intersection_area = intersection_width_height * intersection_height_width\n\n    # Calculate the union areas\n    union_area = box_area + boxes_area[:] - intersection_area[:]\n\n    # Calculate the IoU\n    iou = intersection_area / union_area\n\n    return iou\n\n", "CodeBleu_score": 26.392133140401235}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n    n1 = boxes1.shape[0]\n    n2 = boxes2.shape[0]\n    overlaps = np.zeros((n1, n2))\n    for i in range(n1):\n        b1 = boxes1[i]\n        area1 = (b1[2] - b1[0]) * (b1[3] - b1[1])\n        for j in range(n2):\n            b2 = boxes2[j]\n            area2 = (b2[2] - b2[0]) * (b2[3] - b2[1])\n            x_max = min(b1[2], b2[2])\n            x_min = max(b1[0], b2[0])\n            y_max = min(b1[3], b2[3])\n            y_min = max(b1[1], b2[1])\n            if x_max > x_min and y_max > y_min:\n                intersection = (x_max - x_min) * (y_max - y_min)\n                union = area1 + area2 - intersection\n                overlaps[i, j] = intersection / union\n    return overlaps\n\n", "CodeBleu_score": 21.804374145872558}, {"code": "def compute_overlaps(boxes1, boxes2)\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(boxes1.shape[0]):\n        for j in range(boxes2.shape[0]):\n            x1 = max(boxes1[i, 0], boxes2[j, 0])\n            y1 = max(boxes1[i, 1], boxes2[j, 1])\n            x2 = min(boxes1[i, 2], boxes2[j, 2])\n            y2 = min(boxes1[i, 3], boxes2[j, 3])\n            w = max(0, x2 - x1)\n            h = max(0, y2 - y1)\n            intersection = w * h\n            overlaps[i, j] = intersection / (area1[i] + area2[j] - intersection)\n    return overlaps\n\n", "CodeBleu_score": 45.13468636626585}, {"code": "def compute_overlaps(boxes1, boxes2)\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(boxes2.shape[0]):\n        xx1 = np.maximum(boxes1[:, 0], boxes2[i, 0])\n        yy1 = np.maximum(boxes1[:, 1], boxes2[i, 1])\n        xx2 = np.minimum(boxes1[:, 2], boxes2[i, 2])\n        yy2 = np.minimum(boxes1[:, 3], boxes2[i, 3])\n\n        w = np.maximum(0, xx2 - xx1)\n        h = np.maximum(0, yy2 - yy1)\n        intersection = w * h\n        union = area1 + area2[i] - intersection\n        overlaps[:, i] = intersection / union\n\n    return overlaps\n\n", "CodeBleu_score": 45.84534893537517}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n    image_info = self.image_info[image_id]\n    image = skimage.io.imread(image_info[\"path\"])\n    image = image[:, :, :3]\n    return image\n", "CodeBleu_score": 37.95828403634383}, {"code": "def load_image(self, image_id):\n\n    # Read image from the path\n    image = cv2.imread(self.image_info[image_id]['path'])\n\n    # Convert to RGB if the image is not already in RGB format\n    if len(image.shape) == 2:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    elif image.shape[2] == 4:\n        image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n\n    # Return the processed image\n    return image\n\n", "CodeBleu_score": 44.888930944520666}, {"code": "def load_image(self, image_id):\n    image_path = self.image_info[image_id]['path']\n    image = skimage.io.imread(image_path)\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 83.29790966526292}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    titles = titles if titles is not None else ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure()\n    for i, (image, title) in enumerate(zip(images, titles)):\n        plt.subplot(rows, cols, i + 1)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n    plt.show()\n\n", "CodeBleu_score": 51.40184465519771}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    titles = titles if titles is not None else ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()\n\n", "CodeBleu_score": 63.80587234982848}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n\n    titles = titles if titles is not None else ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    rows = len(images) // cols + 1\n\n    plt.figure()\n    for i, (image, title) in enumerate(zip(images, titles)):\n        plt.subplot(rows, cols, i + 1)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n    plt.show()", "CodeBleu_score": 51.40184465519771}]}
