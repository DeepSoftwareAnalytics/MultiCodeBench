{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\ndef load_model_from_config(self, half_attention):\n    \"\"\"\n    This function loads a model from a configuration file, with an option to load from a cache if available. It first checks if a cached model is available and \n    valid, loading it if so. Otherwise, it loads the model from the specified path, handling different file formats (.safetensors or other PyTorch-compatible \n    formats). The model is configured using settings from a YAML file and moved to the appropriate device. Optional settings include half-precision attention and \n    memory format optimization. The function ensures the model is in evaluation mode and optionally caches the loaded model for future use. The function returns a \n    dictionary containing the loaded model.\n\n    Parameters:\n    half_attention (bool): If True, use half-precision for the attention layers of the model.\n\n    Returns:\n    dict: A dictionary containing the 'model' key with the loaded model and 'config' key with the configuration used.\n    \"\"\"\n    import torch\n    import yaml\n    import os\n    from pathlib import Path\n    \n    # Prepare the config path, cache path and model path based on the configurations\n    config_path = \"config/model_config.yaml\"\n    cache_path = \"cache/model_cache.pt\"\n    model_path = \"models/my_model\"\n\n    # Load the configuration\n    with open(config_path, 'r') as file:\n        config = yaml.safe_load(file)\n    \n    # Check if the cache exists and is valid\n    if os.path.exists(cache_path):\n        print(\"Loading model from cache...\")\n        model = torch.load(cache_path)\n    else:\n        # Load model from path depending on its format\n        if model_path.endswith('.safetensors'):\n            # Assuming use of safeTensors for robust serialization\n            # model = safetensors.load(model_path)\n            raise NotImplementedError(\"SafeTensors loading not implemented.\")\n        else:\n            # Load using PyTorch's native loading mechanism\n            model = torch.load(model_path)\n        \n        # Apply configuration settings to the model\n        device = config['device']\n        model.to(device)\n        \n        if half_attention:\n            # Configure attention mechanism to use half precision if needed\n            for module in model.modules():\n                if hasattr(module, 'attention'):\n                    module.attention.half()\n\n        # Optimization for memory format, if specified\n        if config.get('memory_format', None) == 'channels_last':\n            model = model.to(memory_format=torch.channels_last)\n        \n        # Ensure the model is in evaluation mode.\n        model.eval()\n        \n        # Optionally cache the model\n        if config.get('cache_model', False):\n            torch.save(model, cache_path)\n            print(\"Model is cached for future use.\")\n    \n    # Prepare the return data\n    return {'model': model, 'config': config}", "CodeBleu_score": 36.6620021723629}, {"code": "import os\nimport torch\nimport yaml\nfrom torch import nn\n\nclass ModelLoader:\n    def __init__(self, config_path, model_path, cache_dir=None):\n        self.config_path = config_path\n        self.model_path = model_path\n        self.cache_dir = cache_dir\n\n    def load_model_from_config(self, half_attention=False):\n        \"\"\"Loads a model from a configuration with optional use of cached model.\n        \n        Args:\n            half_attention (bool): if True, enables half precision for the attention mechanism.\n            \n        Returns:\n            dict: dictionary containing the loaded model and its configuration.\n        \"\"\"\n        # Load configuration\n        with open(self.config_path, 'r') as file:\n            config = yaml.safe_load(file)\n\n        # Check cache\n        model_cache_path = self._get_cache_path()\n        if self.cache_dir and os.path.exists(model_cache_path):\n            model = torch.load(model_cache_path)\n            print(\"Loaded model from cache.\")\n        else:\n            # Load model considering the file format\n            if self.model_path.endswith('.safetensors'):\n                model = self._load_from_safetensors(self.model_path)\n            else:\n                model = torch.load(self.model_path)\n            print(\"Loaded model from path.\")\n\n            # Move model to appropriate device\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            model.to(device)\n\n            # Apply model configurations\n            if half_attention:\n                # Assuming the model has an attribute `attention` to apply half precision.\n                if hasattr(model, 'attention'):\n                    model.attention = model.attention.half()\n            \n            model.eval()\n\n            if self.cache_dir:\n                os.makedirs(self.cache_dir, exist_ok=True)\n                torch.save(model, model_cache_path)\n                print(\"Cached model for future loading.\")\n\n        return {'model': model, 'config': config}\n\n    def _get_cache_path(self):\n        \"\"\"Construct the path for cached model based on configuration and model paths.\"\"\"\n        cache_file_name = f\"{os.path.basename(self.model_path)}.pt\"\n        return os.path.join(self.cache_dir, cache_file_name)\n\n    def _load_from_safetensors(self, filepath):\n        \"\"\"Custom loader for .safetensors format, currently not implemented.\"\"\"\n        raise NotImplementedError(\"Loading from .safetensors format is not supported yet.\")\n\n# Example usage of ModelLoader class\nif __name__ == \"__main__\":\n    config_path = 'path/to/config.yaml'\n    model_path = 'path/to/model.pth'\n    cache_dir = 'path/to/cache'\n    loader = ModelLoader(config_path, model_path, cache_dir)\n    loaded_data = loader.load_model_from_config(half_attention=True)\n    model = loaded_data['model']\n    print(\"Model loaded and configured successfully.\")", "CodeBleu_score": 37.20135607488877}, {"code": "import os\nimport torch\nimport yaml\nfrom pathlib import Path\n\nclass ModelHandler:\n    def __init__(self, config_path):\n        with open(config_path, 'r') as stream:\n            self.config = yaml.safe_load(stream)\n        \n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def load_model_from_config(self, half_attention):\n        model_path = self.config['model_path']\n        cache_path = self.config.get('cache_path', './cache/model_cache.pt')\n\n        # Check if a cached model exists and is valid\n        if os.path.exists(cache_path):\n            print(\"Loading model from cache\")\n            model = torch.load(cache_path, map_location=self.device)\n        else:\n            # Load from the specified model path\n            if model_path.endswith('.safetensors'):\n                # Assuming 'safe_gpu' is a placeholder for the actual library required to load .safetensors files\n                import safe_gpu\n                model = safe_gpu.load(model_path)\n            else:\n                model = torch.load(model_path, map_location=self.device)\n\n            # Apply configuration from the YAML file (like half precision)\n            if half_attention:\n                # Example of setting half precision for certain parts of the model\n                # Assuming that 'attention' is a known module in the model\n                if hasattr(model, 'attention'):\n                    model.attention = model.attention.half()\n\n            # Save to cache if caching is enabled\n            if self.config.get('use_cache', False):\n                torch.save(model, cache_path)\n\n        # Configure model to evaluation mode\n        model.eval()\n\n        # Shift model to the appropriate device\n        model.to(self.device)\n\n        # Option to use specific memory formats (if required by config)\n        if self.config.get('optimized_memory_format', False):\n            model.to(memory_format=torch.channels_last)\n\n        # Return a dictionary with model\n        return {'model': model}\n\n# Example Usage:\n# Create a ModelHandler with the path of the YAML configuration\nhandler = ModelHandler('path/to/config.yaml')\nloaded_model = handler.load_model_from_config(half_attention=True)\nprint(loaded_model)", "CodeBleu_score": 43.42629449161777}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None):\n    \"\"\"\n    This function performs conditional sampling using the DDIM (Denoising Diffusion Implicit Models) sampler.\n    It initializes the sampler with the provided model, extracts the batch size from the shape parameter,\n    and prints the eta and steps values. The function then calls the sample method of the DDIMSampler\n    instance, passing several parameters such as steps, batch size, shape, conditioning, eta, and other\n    optional arguments like callback, normals_sequence, mask, x0, quantize_x0, temperature, score_corrector,\n    corrector_kwargs, and x_t. Finally, it returns the generated samples and intermediate values.\n\n    Args:\n        model: The diffusion model used for the denoising process.\n        cond: Conditioning information used to guide the generation.\n        steps: The number of timesteps in the diffusion process.\n        shape: The shape of the output samples.\n        eta: The eta parameter for the DDIM algorithm.\n        callback: Function to call back with intermediate values.\n        normals_sequence: Prescribed sequence of normal values for generating noise.\n        mask: Optional mask to apply for conditional generation.\n        x0: Initial state to start the reverse diffusion process from.\n        quantize_x0: Whether to quantize x0 if specified.\n        temperature: Temperature parameter for adjusting the variance of the generation.\n        score_corrector: A corrector for adjusting the scores computed by the model.\n        corrector_kwargs: Additional parameters for the score corrector.\n        x_t: Intermediate generated values to initialize the reverse process with, if provided.\n    \n    Returns:\n        Tuple consisting of generated samples and (optionally) intermediate values.\n    \"\"\"\n    # Print the provided eta and step values\n    print(f\"eta: {eta}, steps: {steps}\")\n\n    # Assuming the existence of a DDIMSampler object that requires a model upon initialization\n    ddim_sampler = DDIMSampler(model)\n\n    # Extract batch size from shape parameter\n    batch_size = shape[0]\n\n    # Prepare keyword arguments dictionary for the sampling method of DDIMSampler\n    sampling_kwargs = {\n        \"cond\": cond,\n        \"steps\": steps,\n        \"shape\": shape,\n        \"eta\": eta,\n        \"callback\": callback,\n        \"normals_sequence\": normals_sequence,\n        \"mask\": mask,\n        \"x0\": x0,\n        \"quantize_x0\": quantize_x0,\n        \"temperature\": temperature,\n        \"score_corrector\": score_corrector,\n        \"corrector_kwargs\": corrector_kwargs,\n        \"x_t\": x_t,\n    }\n\n    # Call the sample method and return the result\n    samples, intermediates = ddim_sampler.sample(batch_size=batch_size, **sampling_kwargs)\n\n    return samples, intermediates", "CodeBleu_score": 55.14550270459411}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\nfrom typing import Any, Callable, Optional\nimport torch\n\nclass DDIMSampler:\n    def __init__(self, model):\n        self.model = model\n\n    def sample(self, *, steps, batch_size, shape, cond, eta=1.0, callback=None, normals_sequence=None,\n               mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n               corrector_kwargs=None, x_t=None):\n        # This is a simplified placeholder for the actual sampling implementation.\n        # Depending on the complexity of 'model', this would include the reverse diffusion\n        # process applying neural network predictions to progressively denoise a sample.\n        x = torch.randn(batch_size, *shape)  # Start from Gaussian noise\n        \n        # Dummy loop to represent the denoising steps\n        for step in range(steps):\n            # Reverse diffusion step could be more complex involving model predictions\n            x = x - 0.1 * torch.randn_like(x)  # Applying simple noise reduction\n\n            # If there's a callback, run it to possibly modify the sample or monitor the process\n            if callback is not None:\n                callback(x, step)\n\n        return x, {\"details\": \"Intermediates or additional data could be captured here\"}\n\ndef convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    ):\n    # Create a DDIM sampler instance\n    sampler = DDIMSampler(model=model)\n    \n    if corrector_kwargs is None:\n        corrector_kwargs = {}\n\n    # Extract batch size from the shape parameter; typically shape[0] is the expected batch size\n    batch_size = shape[0]\n\n    # Print the values of eta and steps as per the requirements\n    print(f\"Using eta: {eta}, steps: {steps}\")\n    \n    # Perform sampling using the DDIM sampler\n    samples, intermediary_outputs = sampler.sample(\n        steps=steps,\n        batch_size=batch_size,\n        shape=shape[1:], # Exclude batch size from the shape for individual sample dimensions\n        cond=cond,\n        eta=eta,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        quantize_x0=quantize_x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n        x_t=x_t\n    )\n\n    # Return the generated samples and intermediate values\n    return samples, intermediary_outputs\n\n# Example model should be defined here to use `convsample_ddim` function which is expected to be a neural network model\n# ideal for diffusion-based generative tasks like in text-to-image or super-resolution applications.", "CodeBleu_score": 44.138081230004026}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None):\n    \"\"\"\n    This function performs conditional sampling using the DDIM (Denoising Diffusion Implicit Models)\n    sampler. It initializes the sampler with the provided model, extracts the batch size from the\n    shape parameter, and prints the eta and steps values. The function then calls the sample method\n    of the DDIMSampler instance, passing several parameters such as steps, batch size, shape,\n    conditioning, eta, and other optional arguments like callback, normals_sequence, mask, x0,\n    quantize_x0, temperature, score_corrector, corrector_kwargs, and x_t. Finally, it returns the\n    generated samples and intermediate values.\n\n    Args:\n        model: A model implementing the noise prediction function necessary for the DDIM process.\n        cond: Conditioning information (e.g., class labels or starting images) necessary for\n              generating samples conditioned on specific data.\n        steps: Number of timesteps to use in the diffusion process.\n        shape: The shape of the output images (excluding batch size, e.g., (3, 64, 64) for 64x64\n               RGB images).\n        eta: A float specifying the noise level used in the noisy reverse process of diffusion.\n        callback: An optional function to be called after each sampling step (useful for logging).\n        normals_sequence: An optional sequence of normal random variables to use instead of generating\n                          new ones (useful for deterministic outputs).\n        mask: An optional binary mask for masked sampling in conditional image generation.\n        x0: An initial image to start the reverse diffusion process from.\n        quantize_x0: If True, quantization must be applied to x0, converting it to discrete values.\n        temperature: The scale of the noise used during the sampling.\n        score_corrector: An optional function used to modify the score computed by the model (useful\n                         for guided diffusion).\n        corrector_kwargs: Keyword arguments for the `score_corrector`.\n        x_t: Current state of the diffusion process, optionally used to resume incomplete diffusion.\n    \n    Returns:\n        A tuple (samples, intermediates) where `samples` are the final generated samples, and\n        `intermediates` could include intermediate states for analysis or debugging.\n    \"\"\"\n\n    from ddim_sampler import DDIMSampler\n\n    # Batch size is the first element of shape tuple.\n    batch_size = shape[0]\n\n    # Initialize the DDIMSampler with the given model.\n    sampler = DDIMSampler(model)\n\n    print(f\"Using eta: {eta}, with number of steps: {steps}\")\n\n    # Call sample method on DDIMSampler object.\n    samples, intermediates = sampler.sample(\n        steps=steps,\n        batch_size=batch_size,\n        shape=shape,\n        conditioning=cond,\n        eta=eta,\n        callback=callback,\n        normals_sequence=normals_sequence,\n        mask=mask,\n        x0=x0,\n        quantize_x0=quantize_x0,\n        temperature=temperature,\n        score_corrector=score_corrector,\n        corrector_kwargs=corrector_kwargs,\n        x_t=x_t\n    )\n\n    return samples, intermediates", "CodeBleu_score": 65.16998260728211}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "class Model:\n    def __init__(self):\n        self.weights = {'weight1': 1.0, 'weight2': 2.0}\n        # Exponential Moving Average weights\n        self.ema_weights = {'weight1': 0.9, 'weight2': 1.8}\n        self.use_ema = False\n\n    def apply_ema(self):\n        self.original_weights = self.weights.copy()\n        self.weights = self.ema_weights.copy()\n        self.use_ema = True\n    \n    def restore_original_weights(self):\n        self.weights = self.original_weights.copy()\n        self.use_ema = False\n\n    def ema_scope(self, context=None):\n        \"\"\"\n        This function manages the use of Exponential Moving Average (EMA) weights in a machine learning model.\n        When EMA is enabled, it temporarily replaces the model's weights with EMA weights and restores \n        the original weights afterward.\n        If a context is provided, it prints messages indicating when weights are switched and restored.\n        \"\"\"\n        # Check if EMA should be toggled\n        if not self.use_ema:\n            # Applying EMA weights\n            if context:\n                print(\"Applying EMA weights\")\n            self.apply_ema()\n            yield\n            # Restoring original weights\n            if context:\n                print(\"Restoring original weights\")\n            self.restore_original_weights()\n        else:\n            # Context when EMA is already applied\n            if context:\n                print(\"EMA already applied. Using EMA weights.\")\n            yield", "CodeBleu_score": 26.74188581285941}, {"code": "def ema_scope(self, context=None):\n    \"\"\"\n    This function manages the use of Exponential Moving Average (EMA) weights in a machine learning model.\n    When EMA is enabled, it temporarily replaces the model's weights with EMA weights and restores the original weights afterward.\n    If a context is provided, it prints messages indicating when weights are switched and restored.\n    \"\"\"\n    try:\n        # Assume there exist self.ema_enabled, self.ema_weights, and self.model_weights as part of the class\n        if not self.ema_enabled:\n            return  # EMA not enabled, do nothing\n        \n        # Save original weights\n        original_weights = self.model_weights.copy()\n        \n        # Apply EMA weights\n        if context:\n            print(f\"Switching to EMA weights in the context of {context}.\")\n        self.model_weights = self.ema_weights\n        \n        # Here you can plug in the function or code where you want to use the EMA weights.\n        # For example, evaluation of the model.\n        self.evaluate_model()  # This function should be defined elsewhere to use the model for evaluation or similar tasks\n        \n        # Restore original weights\n        if context:\n            print(f\"Restoring original weights after using EMA in the context of {context}.\")\n        self.model_weights = original_weights\n        \n    except Exception as e:\n        if context:\n            print(f\"An error occurred while switching weights in the context of {context}: {str(e)}\")\n        else:\n            print(f\"An error occurred while switching weights: {str(e)}\")", "CodeBleu_score": 33.47770623684008}, {"code": "class Model:\n    def __init__(self):\n        self.weights = {'original': None, 'ema': None}\n\n    def apply_ema_weights(self):\n        # Logic to apply EMA weights\n        self.weights['original'], self.weights['ema'] = self.weights['ema'], self.weights['original'] \n\n    def restore_original_weights(self):\n        # Logic to restore original weights\n        self.weights['original'], self.weights['ema'] = self.weights['ema'], self.weights['original'] \n\nclass EmaScope:\n    def __init__(self, model, context=None):\n        self.model = model\n        self.context = context\n\n    def __enter__(self):\n        if self.context:\n            print(f'Applying EMA weights in {self.context}')\n        self.model.apply_ema_weights()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.context:\n            print(f'Restoring original weights in {self.context}')\n        self.model.restore_original_weights()\n        # Handle exceptions if necessary\n        if exc_type:\n            return False  # Propagate exception\n        return True  # Suppress exceptions, if that's the desired behavior\n\n# Usage example\nif __name__ == \"__main__\":\n    model = Model()\n    model.weights['original'] = [1,2,3]  # Some example weights\n    model.weights['ema'] = [1.1,2.1,3.1]  # Example EMA weights\n\n    with EmaScope(model, \"training\"):\n        # Model training or evaluation would happen here with EMA weights\n        pass", "CodeBleu_score": 26.50662501596966}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "import torch\n\ndef init_from_ckpt(self, path, ignore_keys=None):\n    \"\"\"\n    This function loads a model's state dictionary from a checkpoint file at the specified path, optionally ignoring \n    specified keys. It first loads the state dictionary from the checkpoint and then deletes any keys that match \n    the ignore_keys list. The function then updates the model's state with the loaded state dictionary, printing out \n    any missing or unexpected keys after the update.\n    \"\"\"\n    \n    # Load the state dictionary from the checkpoint file\n    checkpoint = torch.load(path)\n    state_dict = checkpoint['state_dict']\n    \n    # Ignore specified keys if any\n    if ignore_keys is not None:\n        for key in ignore_keys:\n            state_dict.pop(key, None)  # Remove the key if it exists in the state dictionary\n\n    # Load the model's state dictionary from the loaded state dictionary\n    # The `strict=False` allows skipping unexpected or missing keys without errors\n    result = self.load_state_dict(state_dict, strict=False)\n    \n    # Print out the missing and unexpected keys\n    print(\"Missing keys:\", result.missing_keys)\n    print(\"Unexpected keys:\", result.unexpected_keys)\n    \n    # Optionally, return these details if needed by the caller\n    return result.missing_keys, result.unexpected_keys", "CodeBleu_score": 27.62499510992715}, {"code": "import torch\n\nclass ModelHandler:\n    def __init__(self, model):\n        self.model = model\n\n    def init_from_ckpt(self, path, ignore_keys=None):\n        \"\"\"\n        This function loads a model's state dictionary from a checkpoint file at the specified path, optionally ignoring specified keys.\n        It first loads the state dictionary from the checkpoint and then deletes any keys that match the ignore_keys list.\n        The function then updates the model's state with the loaded state dictionary, printing out any missing or unexpected keys after the update.\n        \n        Args:\n            path (str): Filepath to the checkpoint (.pt or .pth) file.\n            ignore_keys (list of str): List of keys to ignore while loading the state dict. Default is None.\n        \"\"\"\n        # Load checkpoint\n        checkpoint = torch.load(path)\n        \n        # Get the state dictionary from checkpoint\n        state_dict = checkpoint['state_dict']\n        \n        # Conditionally ignore keys\n        if ignore_keys is not None:\n            for key in ignore_keys:\n                if key in state_dict:\n                    del state_dict[key]\n        \n        # Load state dict into model, handling missing or unexpected keys\n        model_state = self.model.state_dict()\n        missing_keys, unexpected_keys = model_state.load_state_dict(state_dict, strict=False)\n        \n        # Print information about missing and unexpected keys\n        print(f\"Missing keys in the loaded state: {missing_keys}\")\n        print(f\"Unexpected keys in the loaded state: {unexpected_keys}\")\n        \n        # Update the model's state\n        self.model.load_state_dict(model_state)\n\n# To use this code, you would typically have another class or method to define your model structure.\n# Let's assume we have a model instance, we can create a ModelHandler instance and use `init_from_ckpt`:\n# model_instance = YourModelDefinition()\n# handler = ModelHandler(model_instance)\n# handler.init_from_ckpt('path_to_checkpoint.ckpt', ignore_keys=['some_layer.weights'])", "CodeBleu_score": 33.45354272850457}, {"code": "import torch\n\ndef init_from_ckpt(self, path, ignore_keys=None):\n    \"\"\"\n    Loads a model's state dictionary from a checkpoint file at the specified path,\n    optionally ignoring specified keys. It first loads the state dictionary from the\n    checkpoint and then deletes any keys that match the ignore_keys list. The function\n    then updates the model's state with the loaded state dictionary, printing out any\n    missing or unexpected keys after the update.\n\n    Args:\n        path (str): The file path to the checkpoint.\n        ignore_keys (list, optional): A list of keys to ignore in the checkpoint state dict.\n    \"\"\"\n    # Load the checkpoint from the given path\n    checkpoint = torch.load(path)\n    \n    # Get the state_dict from checkpoint\n    state_dict = checkpoint['state_dict']\n    \n    # If there are keys to ignore, remove them from the state_dict\n    if ignore_keys:\n        for key in list(state_dict.keys()):  # Use list to avoid dict size change error during iteration\n            for ignore_key in ignore_keys:\n                if ignore_key in key:\n                    del state_dict[key]\n                    break\n    \n    # Load current model state_dict\n    model_state_dict = self.state_dict()\n    \n    # Check for missing and unexpected keys\n    model_keys = set(model_state_dict.keys())\n    checkpoint_keys = set(state_dict.keys())\n    missing_keys = model_keys - checkpoint_keys\n    unexpected_keys = checkpoint_keys - model_keys\n    \n    # Print missing and unexpected keys\n    if missing_keys:\n        print(\"Missing keys in checkpoint:\", missing_keys)\n    if unexpected_keys:\n        print(\"Unexpected keys in checkpoint:\", unexpected_keys)\n    \n    # Update the existing model state dict with the checkpoint state dict\n    model_state_dict.update({k: v for k, v in state_dict.items() if k in model_keys})\n    \n    # Load the updated state dict back into the model\n    self.load_state_dict(model_state_dict)", "CodeBleu_score": 31.984670841026166}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False):\n    \"\"\"\n    This function processes an input through encoding and decoding steps, \n    returning the decoded output and a difference metric.\n    Optionally, it can also return prediction indices if specified.\n\n    Args:\n    input (any suitable datatype): The input data to be processed by the model.\n    return_pred_indices (bool): Indicator whether to return prediction indices.\n\n    Returns:\n    tuple: If return_pred_indices is False, returns (decoded_output, diff_metric).\n           If return_pred_indices is True, returns (decoded_output, diff_metric, pred_indices).\n    \"\"\"\n    \n    # Step 1: Encode the input\n    encoded_input = self.encoder(input)\n    \n    # Step 2: Decode the input\n    decoded_output = self.decoder(encoded_input)\n    \n    # Calculate the difference metric (example could be loss or error)\n    # Assuming we have a function to calculate this difference, such as self.calculate_difference_metric\n    diff_metric = self.calculate_difference_metric(input, decoded_output)\n    \n    # If prediction indices need to be returned:\n    if return_pred_indices:\n        # Assuming we have some method to extract prediction indices\n        pred_indices = self.get_prediction_indices(decoded_output)\n        return decoded_output, diff_metric, pred_indices\n    \n    # Regular return without prediction indices\n    return decoded_output, diff_metric", "CodeBleu_score": 37.4462422861915}, {"code": "class Model:\n    def __init__(self, encoder, decoder):\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, input, return_pred_indices=False):\n        \"\"\"\n        This function processes an input through encoding and decoding steps, returning the decoded output and a difference metric.\n        Optionally, it can also return prediction indices if specified.\n\n        Args:\n        input (np.array): The input data to be processed.\n        return_pred_indices (bool): If True, the function also returns the indices of the highest predictions.\n\n        Returns:\n        tuple: A tuple containing:\n            - np.array: The decoded output.\n            - float: The difference metric between the input and the decoded output.\n            - (optional) np.array: The indices of the highest predictions (if `return_pred_indices` is True).\n        \"\"\"\n        # Encode the input\n        encoded = self.encoder(input)\n\n        # Decode the encoded data\n        decoded = self.decoder(encoded)\n\n        # Calculate a difference metric (e.g., Mean Squared Error)\n        difference_metric = ((input - decoded) ** 2).mean()\n\n        # Prepare the output\n        output = (decoded, difference_metric)\n\n        if return_pred_indices:\n            # Example: Get indices of the top k predictions (this part of code depends on the specifics of the implementation)\n            # Assuming decoded output predicts classes or values per sample\n            k = 3  # for example, top 3 indices\n            pred_indices = np.argsort(-decoded, axis=1)[:, :k]\n            output += (pred_indices,)\n\n        return output", "CodeBleu_score": 25.966068817254335}, {"code": "def forward(self, input, return_pred_indices=False):\n    \"\"\"\n    This function processes an input through encoding and decoding steps, returning the decoded output\n    and a difference metric. Optionally, it can also return prediction indices if specified.\n\n    Args:\n    - input: The input data to be processed.\n    - return_pred_indices (bool): A flag to determine if prediction indices need to be returned.\n\n    Returns:\n    - A tuple containing the decoded output and the difference metric. If return_pred_indices is True,\n      the tuple will also include prediction indices.\n    \"\"\"\n\n    # Encoding the input\n    encoded_input = self.encode(input)\n\n    # Decoding the encoded input\n    decoded_output = self.decode(encoded_input)\n\n    # Calculating the difference metric\n    difference_metric = self.compute_difference_metric(input, decoded_output)\n\n    # Optionally computing and returning prediction indices\n    if return_pred_indices:\n        prediction_indices = self.predict_index(encoded_input)\n        return decoded_output, difference_metric, prediction_indices\n\n    return decoded_output, difference_metric\n\n# Assuming we would define `encode`, `decode`, `compute_difference_metric`, and optionally `predict_index` elsewhere within the class.", "CodeBleu_score": 35.0797332655081}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "import torch\nimport torch.nn.functional as F\n\nclass DataProcessor:\n    def __init__(self, enable_resize=False, min_size=64, max_size=256, resize_steps=1000):\n        self.enable_resize = enable_resize\n        self.min_size = min_size\n        self.max_size = max_size\n        self.resize_steps = resize_steps\n    \n    def get_input(self, batch, k, current_step):\n        \"\"\"\n        Processes a batch of input data by selecting a specific key from the batch, adjusting its dimensions,\n        and applying optional resizing based on the training step.\n\n        Args:\n        - batch (dict): The input data batch containing various data items.\n        - k (str): Key to extract the tensor from the batch.\n        - current_step (int): Current training step to determine resizing parameters.\n\n        Returns:\n        - torch.Tensor: The processed input tensor.\n\n        \"\"\"\n        # Extract data using key\n        data = batch[k]\n\n        # Ensure the input has four dimensions\n        if data.dim() < 4:\n            data = data.unsqueeze(0)  # Assuming batch size of 1 in case of 3 dimensions\n        elif data.dim() > 4:\n            raise ValueError('Input data has more than 4 dimensions and cannot be processed.')\n\n        # Optionally resize the data\n        if self.enable_resize:\n            target_size = int(self.min_size + (self.max_size - self.min_size) * (current_step / self.resize_steps))\n            target_size = min(self.max_size, max(self.min_size, target_size))\n            data = F.interpolate(data, size=(target_size, target_size), mode='bicubic', align_corners=False)\n\n        # Convert to contiguous float tensor\n        data = data.permute(0, 3, 1, 2).contiguous().float()\n\n        # Detach from the computation graph\n        data = data.detach()\n\n        return data", "CodeBleu_score": 33.74432119558208}, {"code": "import torch\n\nclass Model:\n    def __init__(self):\n        self.enable_resize = True\n        self.resize_start_step = 1000\n        self.resize_end_step = 5000\n        self.current_step = 0\n        self.initial_dims = (128, 128)\n        self.final_dims = (256, 256)\n\n    def get_input(self, batch, k):\n        \"\"\"\n        This function processes a batch of input data by selecting a specific key from the batch and adjusting\n        its dimensions if necessary. It ensures the input has four dimensions, permutes the axes, and converts\n        the data to a contiguous float tensor. If batch resizing is enabled, it adjusts the size of the input\n        based on the current training step and a specified range, using bicubic interpolation. Finally, it\n        detaches the tensor from the computation graph and returns it.\n        \"\"\"\n        # Extract the data for key `k`\n        data = batch[k]\n\n        # Ensure data has four dimensions; assume missing dimensions are spatial and should be singletons\n        while len(data.shape) < 4:\n            data = data.unsqueeze(0)\n\n        # Permute the dimensions if needed; example: from (N, H, W, C) to (N, C, H, W)\n        # This step depends on the original data format; adjust the permutation as required\n        data = data.permute(0, 3, 1, 2)\n\n        # Convert to float and make contiguous\n        data = data.float().contiguous()\n\n        # Resize logic, based on the current step\n        if self.enable_resize and self.resize_start_step <= self.current_step <= self.resize_end_step:\n            # Compute scale factor (this simple example assumes linear scaling between steps)\n            factor = (self.final_dims[0] / self.initial_dims[0]) * ((self.current_step - self.resize_start_step) / (self.resize_end_step - self.resize_start_step))\n            new_dims = (int(self.initial_dims[0] * factor), int(self.initial_dims[1] * factor))\n\n            # Apply bicubic interpolation\n            data = torch.nn.functional.interpolate(data, size=new_dims, mode='bicubic', align_corners=False)\n\n        # Detach tensor from the graph\n        data = data.detach()\n\n        return data\n\n# Example use case\n# model = Model()\n# batch = {'input_key': torch.randn(10, 100, 100, 3)}  # Example batch data\n# processed_data = model.get_input(batch, 'input_key')\n# print(processed_data.shape)", "CodeBleu_score": 34.69289794226684}, {"code": "import torch\nimport torch.nn.functional as F\n\nclass DataProcessor:\n    def __init__(self, resize_enabled=False, start_dim=256, end_dim=512, total_steps=10000):\n        self.resize_enabled = resize_enabled\n        self.start_dim = start_dim\n        self.end_dim = end_dim\n        self.total_steps = total_steps\n\n    def get_input(self, batch, k, current_step):\n        \"\"\"\n        This function processes a batch of input data by selecting a specific key from the batch and adjusting its dimensions if necessary.\n        It ensures the input has four dimensions, permutes the axes, and converts the data to a contiguous float tensor.\n        If batch resizing is enabled, it adjusts the size of the input based on the current training step and a specified range, using bicubic interpolation.\n        Finally, it detaches the tensor from the computation graph and returns it.\n        \n        Args:\n            batch (dict): The batch data containing tensors.\n            k (str): Key to access the specific tensor in the batch.\n            current_step (int): The current training step count for resizing purposes.\n\n        Returns:\n            torch.Tensor: The processed tensor detached from the computation graph.\n        \"\"\"\n\n        # Extracting tensor from batch with key k\n        data = batch[k]\n\n        # Ensuring the tensor has four dimensions, e.g., (B, C, H, W)\n        while len(data.shape) < 4:\n            data = data.unsqueeze(0)  # Adding a batch dimension if missing.\n\n        # Permute the axes if necessary, assume we need (B, C, H, W)\n        if data.shape[1] != 3:  # Typically the channel dimension to be at position 1.\n            # This is just a common scenario assuming RGB channels, this line may need adjustment.\n            data = data.permute(0, 3, 1, 2) \n\n        # Convert to contiguous float tensor\n        data = data.contiguous().float()\n\n        if self.resize_enabled:\n            # Calculate desired size based on training progression using linear interpolation\n            target_size = int(self.start_dim + (self.end_dim - self.start_dim) * (current_step / self.total_steps))\n            # Resize images using bicubic interpolation\n            data = F.interpolate(data, size=(target_size, target_size), mode='bicubic', align_corners=False)\n\n        # Detach tensor from computation graph\n        data = data.detach()\n\n        return data", "CodeBleu_score": 34.59184544043603}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\nHere's an example of how the function `training_step` in PyTorch Lightning might", "CodeBleu_score": 2.535054665409447}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx):\n    \"\"\"\n    This function performs a training step for a model, processing a batch of data and computing losses for \n    different optimizers. It first retrieves the input data and predictions, then computes either the \n    autoencoding loss or the discriminator loss based on the optimizer index.\n    The function logs relevant metrics and returns the computed loss for further optimization.\n    Args:\n    - batch: The batch of data to be processed. Typically a tuple (input_data, target_data).\n    - batch_idx: The index of the batch.\n    - optimizer_idx: The index of the optimizer (0 for the main model, 1 for a secondary component like a discriminator).\n    \n    Returns:\n    - The loss computed for the current step which will be used for backpropagation.\n    \"\"\"\n\n    input_data, target_data = batch  # Assuming batch tuple is structured as (input, target)\n\n    # Forward pass through the model\n    predictions = self.model(input_data)\n    \n    if optimizer_idx == 0:\n        # Calculate the \"autoencoder\" or primary loss\n        loss = self.autoencoder_loss(predictions, target_data)\n    elif optimizer_idx == 1:\n        # Calculate the \"discriminator\" or secondary loss\n        loss = self.discriminator_loss(predictions)\n    else:\n        raise ValueError(\"Unknown optimizer index\")\n\n    # Optionally log metrics here\n    self.log('train_loss', loss)\n\n    return loss", "CodeBleu_score": 20.406040929403762}, {"code": "import torch\n\ndef training_step(self, batch, batch_idx, optimizer_idx):\n    \"\"\"\n    This function performs a training step for a model, processing a batch of data \n    and computing losses for different optimizers.\n    It first retrieves the input data and predictions, then computes either the autoencoding loss \n    or the discriminator loss based on the optimizer index.\n    The function logs relevant metrics and returns the computed loss for further optimization.\n    \"\"\"\n    \n    # Assume batch is formatted correctly, e.g., (inputs, targets)\n    inputs, labels = batch\n    \n    # Forward pass to compute the model's predictions\n    predictions = self.model(inputs)\n    \n    if optimizer_idx == 0:\n        # Compute loss for the autoencoder part of the model\n        loss = self.autoencoder_loss(predictions, labels)\n    elif optimizer_idx == 1:\n        # Compute loss for the discriminator part of the model\n        loss = self.discriminator_loss(predictions, labels)\n    else:\n        raise ValueError(f\"Invalid optimizer index: {optimizer_idx}\")\n    \n    # Log metrics as required\n    self.log('loss', loss.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    \n    # Return the loss\n    return loss", "CodeBleu_score": 24.224757044082025}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n    \"\"\"\n    This function logs images during model training, including inputs and reconstructions,\n    and optionally uses Exponential Moving Average (EMA) weights.\n    It processes input data, logs either just the inputs or both inputs and reconstructions,\n    and applies colorization for multi-channel images if needed.\n    If EMA logging is enabled, it includes reconstructions using EMA weights in the log.\n    \n    Parameters:\n        batch (tuple or dict): A batch of data from a dataloader, usually a tuple (input, target) or a dict with keys.\n        only_inputs (bool): If True, only input images are logged. If False, both input and reconstructed images are logged.\n        plot_ema (bool): If True, the function also logs images using EMA weights.\n        **kwargs: Additional keyword arguments for logging configurations or processing options.\n        \n    Returns:\n        None. This function is primarily used for logging purposes during training.\n    \"\"\"\n\n    # Assuming `batch` could be either a tuple of (inputs, targets) or a dictionary\n    if isinstance(batch, tuple):\n        inputs, targets = batch\n    else:\n        inputs = batch['inputs']\n        targets = batch['targets']\n\n    if not only_inputs:\n        # Generate reconstructions, assuming the model and possibly model_ema are attributes of 'self'\n        reconstructions = self.model(inputs)\n\n        if plot_ema:\n            try:\n                reconstructions_ema = self.model_ema(inputs)\n            except AttributeError:\n                print(\"EMA model is not available.\")\n                reconstructions_ema = None\n        else:\n            reconstructions_ema = None\n\n    num_images = min(len(inputs), 10)  # Just plotting 10 images for brevity\n\n    for i in range(num_images):\n        plt.figure(figsize=(10, 4))\n        \n        # Plotting input image\n        ax = plt.subplot(1, 3 if plot_ema and reconstructions_ema is not None else 2, 1)\n        plt.imshow(inputs[i].permute(1, 2, 0))\n        ax.title.set_text('Input Image')\n        \n        # Plotting basic reconstruction\n        if not only_inputs:\n            ax = plt.subplot(1, 3 if plot_ema and reconstructions_ema is not None else 2, 2)\n            plt.imshow(reconstructions[i].detach().permute(1, 2, 0))\n            ax.title.set_text('Reconstruction')\n        \n        # Plotting EMA-based reconstruction\n        if plot_ema and reconstructions_ema is not None:\n            ax = plt.subplot(1, 3, 3)\n            plt.imshow(reconstructions_ema[i].detach().permute(1, 2, 0))\n            ax.title.set_text('EMA Reconstruction')\n        \n        plt.axis('off')\n        plt.show()\n\n    # Additional logging or saving could be added here (e.g., using TensorBoard)", "CodeBleu_score": 27.027332165959912}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n    \"\"\"\n    This function logs images during model training, including inputs and reconstructions,\n    and optionally uses Exponential Moving Average (EMA) weights.\n    It processes input data, logs either just the inputs or both inputs and reconstructions,\n    and applies colorization for multi-channel images if needed.\n    If EMA logging is enabled, it includes reconstructions using EMA weights in the log.\n\n    Args:\n    batch (Tensor): The batch of input images to be logged.\n    only_inputs (bool): If True, only input images will be logged. Otherwise, both inputs and outputs are logged.\n    plot_ema (bool): If True, reconstructions using EMA weights are logged.\n    kwargs: Additional keyword arguments potentially used for logging options.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Extract input images from the batch\n    inputs = batch['inputs']\n\n    # Plot input images\n    fig, axs = plt.subplots(1, inputs.shape[0], figsize=(inputs.shape[0] * 3, 3))\n    for i, img in enumerate(inputs):\n        axs[i].imshow(img.permute(1, 2, 0).cpu().numpy())  # Adjusting the dimensions to CWH -> WHC\n        axs[i].axis('off')\n    plt.savefig('input_images.png', bbox_inches='tight')\n    plt.close(fig)\n    \n    # If not only inputs, plot reconstructions\n    if not only_inputs:\n        outputs = self.model(inputs)  # Assuming a model is associated with 'self'\n        if plot_ema:\n            outputs_ema = self.ema_model(inputs)  # Assuming an EMA version of the model\n\n        # Plotting outputs\n        fig, axs = plt.subplots(1, outputs.shape[0], figsize=(outputs.shape[0] * 3, 3))\n        for i, img in enumerate(outputs):\n            axs[i].imshow(img.detach().cpu().permute(1, 2, 0).numpy())\n            axs[i].axis('off')\n        plt.savefig('reconstruction_images.png', bbox_inches='tight')\n        plt.close(fig)\n\n        # Plot EMA outputs if required\n        if plot_ema:\n            fig, axs = plt.subplots(1, outputs_ema.shape[0], figsize=(outputs_ema.shape[0] * 3, 3))\n            for i, img in enumerate(outputs_ema):\n                axs[i].imshow(img.detach().cpu().permute(1, 2, 0).numpy())\n                axs[i].axis('off')\n            plt.savefig('ema_reconstruction_images.png', bbox_inches='tight')\n            plt.close(fig)", "CodeBleu_score": 28.87939520330508}, {"code": "import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass ModelTrainer:\n    def __init__(self, model, log_dir='logs'):\n        self.model = model\n        self.log_dir = log_dir\n        self.summary_writer = tf.summary.create_file_writer(self.log_dir)\n\n    def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n        \"\"\"\n        This function logs images during model training, including inputs and reconstructions, \n        and optionally uses Exponential Moving Average (EMA) weights.\n        It processes input data, logs either just the inputs or both inputs and reconstructions, \n        and applies colorization for multi-channel images if needed.\n        If EMA logging is enabled, it includes reconstructions using EMA weights in the log.\n        \"\"\"\n        # Extract data from the batch\n        input_images = batch['input_images']\n        true_images = batch.get('true_images', None)  # Optional\n        \n        # Log input images\n        with self.summary_writer.as_default():\n            tf.summary.image(\"Inputs\", input_images, max_outputs=5, step=self.model.optimizer.iterations)\n\n        # Check if only inputs are to be logged\n        if not only_input:\n            if plot_ema:\n                # Apply Exponential Moving Average\n                # Note: Assumed that model has attribute ema_model for EMA weights\n                reconstructions = self.model.ema_model(input_images)\n                recon_name = \"Reconstructions (EMA)\"\n            else:\n                reconstructions = self.model(input_images)\n                recon_name = \"Reconstructions\"\n            \n            # Log reconstruction images\n            with self.summary_writer.as_default():\n                tf.summary.image(recon_name, reconstructions, max_outputs=5, step=self.model.optimizer.iterations)\n\n        # Optionally log true images, if available\n        if true_images is not None:\n            with self.summary_writer.as_default():\n                tf.summary.image(\"True Images\", true_images, max_outputs=5, step=self.model.optimizer.iterations)", "CodeBleu_score": 27.98267104735066}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "import torch\n\nclass LossCalculator:\n    def __init__(self, loss_type):\n        self.loss_type = loss_type\n\n    def get_loss(self, pred, target, mean=True):\n        \"\"\"\n        This function calculates the loss between predicted and target values based on the specified loss type, either 'l1' or 'l2'.\n        For 'l1' loss, it computes the mean absolute error if specified.\n        For 'l2' loss, it uses mean squared error, either averaged or not based on the mean parameter.\n        If an unknown loss type is provided, it raises an error.\n        \n        Args:\n        pred (Tensor): The tensor containing the predicted values.\n        target (Tensor): The tensor containing the true values.\n        mean (bool): A boolean to decide whether to return the mean loss or the sum of losses.\n\n        Returns:\n        float or Tensor: The calculated loss.\n        \n        Raises:\n        ValueError: If an unrecognized loss type is provided.\n        \"\"\"\n        if self.loss_type == 'l1':\n            loss = torch.abs(pred - target)\n        elif self.loss_type == 'l2':\n            loss = (pred - target) ** 2\n        else:\n            raise ValueError(\"Unknown loss type specified: {}\".format(self.loss_type))\n        \n        if mean:\n            return loss.mean()\n        else:\n            return loss.sum()\n\n# Example usage:\n# loss_calculator = LossCalculator(loss_type='l2')\n# prediction = torch.tensor([3.0, 4.0, 5.0])\n# target = torch.tensor([1.0, 2.0, 3.0])\n# print(loss_calculator.get_loss(prediction, target))", "CodeBleu_score": 36.83258539063097}, {"code": "import torch\nimport torch.nn.functional as F\n\nclass LossCalculator:\n    def __init__(self, loss_type):\n        if loss_type not in ['l1', 'l2']:\n            raise ValueError(\"loss_type must be 'l1' or 'l2'\")\n        self.loss_type = loss_type\n    \n    def get_loss(self, pred, target, mean=True):\n        \"\"\"\n        This function calculates the loss between predicted and target values \n        based on the specified loss type, either 'l1' or 'l2'.\n        For 'l1' loss, it computes the mean absolute error if specified.\n        For 'l2' loss, it uses mean squared error, either averaged or not based on the `mean` parameter.\n        If an unknown loss type is provided, it raises an error.\n\n        Parameters:\n            pred (Tensor): The predicted values.\n            target (Tensor): The actual target values to which `pred` should be compared.\n            mean (bool, optional): If True, returns the mean of the computed loss values; otherwise, returns the sum. Defaults to True.\n\n        Returns:\n            Tensor: The computed loss.\n        \"\"\"\n        if self.loss_type == 'l1':\n            loss = F.l1_loss(pred, target, reduction='none')\n        elif self.loss_type == 'l2':\n            loss = F.mse_loss(pred, target, reduction='none')\n        else:\n            raise ValueError(\"Unknown loss type: {}\".format(self.loss_type))\n        \n        if mean:\n            return loss.mean()\n        else:\n            return loss.sum()\n\n# Example Usage\nloss_calculator = LossCalculator(loss_type='l1')\npred = torch.tensor([3.0, 4.0, 5.0, 6.0])\ntarget = torch.tensor([3.5, 2.5, 5.0, 5.5])\nprint(loss_calculator.get_loss(pred, target, mean=True))  # Calculates mean absolute error", "CodeBleu_score": 40.68053784859671}, {"code": "def get_loss(self, pred, target, loss_type='l1', mean=True):\n    \"\"\"\n    This function calculates the loss between predicted and target values based on the specified loss type, either 'l1' or 'l2'.\n    For 'l1' loss, it computes the mean absolute error if specified.\n    For 'l2' loss, it uses mean squared error, either averaged or not based on the mean parameter.\n    If an unknown loss type is provided, it raises an error.\n\n    Parameters:\n        pred (array_like): Array of predicted values.\n        target (array_like): Array of true target values.\n        loss_type (str): Type of loss to compute ('l1' for L1 loss, 'l2' for L2 loss).\n        mean (bool): Whether to return the mean of the computed loss or the total sum.\n\n    Returns:\n        float: The computed loss.\n\n    Raises:\n        ValueError: If the `loss_type` is not recognized.\n    \"\"\"\n    import numpy as np\n\n    if loss_type == 'l1':\n        # Calculate L1 loss (mean absolute error)\n        loss = np.abs(pred - target)\n    elif loss_type == 'l2':\n        # Calculate L2 loss (mean squared error)\n        loss = (pred - target) ** 2\n    else:\n        raise ValueError(f\"Unknown loss type '{loss_type}'. Allowed values are 'l1' or 'l2'.\")\n\n    if mean:\n        # Return the mean of the loss\n        return np.mean(loss)\n    else:\n        # Return the sum of the loss\n        return np.sum(loss)", "CodeBleu_score": 29.876088647670507}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "import random\nimport torch\n\ndef forward(self, x, c, *args, **kwargs):\n    \"\"\"\n    This function performs a forward pass for a model, incorporating conditioning and time step information.\n    It randomly selects time steps, applies learned conditioning if applicable, and processes the inputs \n    according to the model's conditioning requirements.\n    Finally, it computes and returns the loss for the given inputs and conditioning.\n    \n    Args:\n    - x (Tensor): Input tensor to the neural network.\n    - c (Tensor): Conditioning tensor that provides additional context for processing the input.\n    \n    Returns:\n    - Tensor: The loss computed from the forward pass.\n    \"\"\"\n    \n    # We will assume that `c` could be related to the state or condition that modifies the input\n    # Randomly select time steps; assuming x is a sequence where the second dimension is time\n    time_steps = x.shape[1]\n    random_time_step = random.randint(0, time_steps - 1)\n    selected_time_step_data = x[:, random_time_step]\n\n    # Merging conditioning information if applicable\n    # Accommodate timestep into conditioning (e.g., replicating the conditioning for each selected time step)\n    conditioned_input = torch.cat((selected_time_step_data, c), dim=-1)\n    \n    # Processing the input through the model (This step will depend heavily on specific model details)\n    # Here, we would typically pass conditioned_input through some layers of the model:\n    output = self.model(conditioned_input)\n\n    # Assuming the need to calculate loss; provide ground truth labels or data `y` should be part of args or kwargs\n    y = kwargs.get('labels')\n    \n    if y is None:\n        raise ValueError(\"Labels for computing the loss are not provided\")\n\n    # Calculate loss; assuming a simple MSE Loss for demonstration\n    criterion = torch.nn.MSELoss()\n    loss = criterion(output, y)\n\n    return loss", "CodeBleu_score": 35.8942435682132}, {"code": "def forward(self, x, c, *args, **kwargs):\n    \"\"\"\n    This function performs a forward pass for a model, incorporating conditioning and time step information.\n    It randomly selects time steps, applies learned conditioning if applicable, and processes the inputs according to\n    the model's conditioning requirements.\n    Finally, it computes and returns the loss for the given inputs and conditioning.\n    \n    Parameters:\n        x (tensor): Input features or data to the model.\n        c (tensor): Conditional information that needs to be applied to the model input.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments, potential options could include 'time_steps'\n    \n    Returns:\n        Float: Computed loss for the given inputs and the conditioning.\n    \"\"\"\n    import torch\n\n    # Step 1: Select random time steps if 'time_steps' argument is provided in kwargs\n    if 'time_steps' in kwargs:\n        time_steps = kwargs['time_steps']\n        t = torch.randint(low=0, high=time_steps, size=(x.size(0),))\n    else:\n        # If no specific time_steps given, assume a default behavior or give an error\n        raise ValueError(\"time_steps must be provided in kwargs\")\n\n    # Step 2: Apply conditioning to the input based on t. Assuming we have a method `apply_conditioning`\n    # This should be defined in the class or must be appropriately handled\n    conditioned_input = self.apply_conditioning(x, c, t)\n\n    # Step 3: Pass the conditioned input through the model. Assuming `self.model` is a part of the class\n    output = self.model(conditioned_input)\n\n    # Step 4: Calculate loss. Assuming target labels are provided under kwargs with key 'target' and a defined `loss_function`\n    if 'target' in kwargs:\n        target = kwargs['target']\n    else:\n        raise ValueError(\"target must be provided in kwargs\")\n\n    loss = self.loss_function(output, target)\n\n    # Step 5: Return the computed loss\n    return loss\n\n# Assuming the rest of the model class implementation (e.g., model, loss_function, apply_conditioning) is defined elsewhere.", "CodeBleu_score": 33.73586264412837}, {"code": "import torch\nimport torch.nn as nn\nimport random\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        # Initialize your layers here\n        self.example_layer = nn.Linear(10, 10)  # Change according to your input and output sizes\n        self.conditioning_layer = nn.Linear(5, 10)  # Conditioning layer size depends on the size of 'c'\n        self.loss_function = nn.MSELoss()  # Example loss function, adjust depending on the problem\n\n    def forward(self, x, c, *args, **kwargs):\n        \"\"\"\n        This function performs a forward pass for a model, incorporating conditioning and time step information.\n        It randomly selects time steps, applies learned conditioning if applicable, and processes the inputs according to the model's conditioning requirements.\n        Finally, it computes and returns the loss for the given inputs and conditioning.\n        \"\"\"\n        # Randomly select a time step if 'x' is time-series data \n        # Example: Assuming x is (batch_size, time_steps, features)\n        if x.ndim == 3 and x.shape[1] > 1:\n            time_step_idx = random.randint(0, x.shape[1] - 1)\n            x = x[:, time_step_idx, :]  # Selecting a random time step from each sequence in the batch\n\n        # Apply learned conditioning using a conditioning layer\n        conditioned_input = self.conditioning_layer(c)\n\n        # Combine conditioned input with the main input\n        # Assuming x and conditioned_input are of the same shape or are broadcastable\n        combined_input = x + conditioned_input\n\n        # Pass the combined input through the main model layer (or network)\n        output = self.example_layer(combined_input)\n\n        # Compute loss using provided inputs and conditioning (assuming targets are also provided in **kwargs)\n        targets = kwargs.get('targets')\n        if targets is not None:\n            loss = self.loss_function(output, targets)\n        else:\n            raise ValueError(\"No targets provided for loss computation\")\n\n        return loss\n\n# Example usage:\nmodel = Model()\n# Example data:\nx = torch.randn(10, 15, 10)  # batch of 10, 15 time steps, 10 features per step\nc = torch.randn(10, 5)  # batch of 10, 5 features for conditioning\nloss = model.forward(x, c, targets=torch.randn(10, 10))  # Assuming target shape of (batch_size, output_size)\nprint(\"Computed Loss:\", loss)", "CodeBleu_score": 34.158608161037485}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "import numpy as np\n\ndef get_codebook_entry(self, indices, shape):\n    \"\"\"\n    This function retrieves the quantized latent vectors from the codebook based on given indices and\n    reshapes them to match a specified shape. It handles optional remapping of indices and ensures the\n    output latent vectors are correctly reshaped and permuted to match the original input dimensions.\n    \n    Args:\n        indices (np.ndarray): The indices of the latent vectors in the codebook to retrieve.\n        shape (tuple): The desired shape of the output array of latent vectors.\n        \n    Returns:\n        np.ndarray: The array of latent vectors reshaped to the specified shape.\n    \"\"\"\n    \n    # Check if there's an index remapping to apply\n    if hasattr(self, 'index_remap') and self.index_remap is not None:\n        # Remap indices if a remap dictionary or array exists\n        remapped_indices = self.index_remap[indices]\n    else:\n        remapped_indices = indices\n\n    # Retrieve vectors from the codebook using the possibly remapped indices\n    # Assuming codebook is a 2D tensor where each row corresponds to a vector\n    vectors = self.codebook[remapped_indices]\n\n    # Reshape the vectors to match the specified shape, ensuring the number of elements matches\n    if np.prod(vectors.shape) != np.prod(shape):\n        raise ValueError(\"The product of the desired shape does not match the number of elements in the retrieved vectors.\")\n    \n    reshaped_vectors = vectors.reshape(shape)\n\n    return reshaped_vectors", "CodeBleu_score": 33.336910364974926}, {"code": "import numpy as np\n\nclass VectorQuantizer:\n    def __init__(self, codebook):\n        \"\"\"\n        Initialize with a predefined codebook.\n        codebook: A NumPy array containing quantized latent vectors.\n        \"\"\"\n        self.codebook = codebook\n\n    def get_codebook_entry(self, indices, shape):\n        \"\"\"\n        Retrieves the quantized latent vectors from the codebook based on given indices\n        and reshapes them to match a specified shape.\n        \n        :param indices: An array of indices used to fetch vectors from the codebook.\n        :param shape: Desired shape of the output data tensor.\n        :return: A reshaped array of latent vectors.\n        \"\"\"\n        # Check if any index remapping logic is necessary and apply it here\n        # For simplicity, assuming no remapping, directly use the indices\n        # In a more complex scenario, you might have something like:\n        # indices = self.remap_indices(indices) if hasattr(self, 'remap_indices') else indices\n        \n        # Fetch the vectors from the codebook\n        vectors = self.codebook[indices]\n        \n        # Reshape the vector as per the desired shape\n        # Note: ensure the total number of elements match\n        if np.prod(vectors.shape) != np.prod(shape):\n            raise ValueError(\"Total elements mismatch between fetched vectors and the desired shape\")\n        \n        reshaped_vectors = vectors.reshape(shape)\n        \n        return reshaped_vectors\n\n# Example codebook initialization and usage, not to be included in the function:\n# vec_quantizer = VectorQuantizer(np.random.rand(10, 64))  # Example: 10 vectors each of dimension 64.\n# output_vectors = vec_quantizer.get_codebook_entry([2, 5, 3], (3, 64))", "CodeBleu_score": 28.113632641968582}, {"code": "def get_codebook_entry(self, indices, shape):\n    \"\"\"\n    This function retrieves the quantized latent vectors from the codebook based on given indices and reshapes them to match a specified shape.\n    It handles optional remapping of indices and ensures the output latent vectors are correctly reshaped and permuted to match the original input dimensions.\n\n    Args:\n    indices (torch.Tensor): Tensor of indices corresponding to the entries in the codebook.\n    shape (tuple): The desired output shape of the latent vectors after reshaping.\n\n    Returns:\n    torch.Tensor: The reshaped latent vectors retrieved from the codebook.\n    \"\"\"\n    # Assuming self.codebook is a tensor of shape (num_entries, entry_dimension)\n    \n    # Retrieve the latent vectors from the codebook using the provided indices\n    latent_vectors = self.codebook[indices]\n    \n    # Reshape the latent vector to the specified shape\n    reshaped_vectors = latent_vectors.view(shape)\n    \n    # If required, permute dimensions to match the original input's layout\n    # Example of a permutation, this might be adjusted based on specific use-case\n    # order = [2, 0, 1]  # This is an example, real permutation order should be known contextually\n    # reshaped_and_permuted = reshaped_vectors.permute(order)\n    \n    return reshaped_vectors", "CodeBleu_score": 19.05768612834641}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor: int = -1) -> tuple[int, int]:\n    \"\"\"\n    This function computes the factorization of a given dimension into two factors, m and n, \n    with an optional specified factor.\n    If the factor is positive and divides the dimension, it returns the factor and the quotient,\n    ensuring m is less than or equal to n.\n    If the factor is negative, it iteratively finds a factorization that minimizes the sum of\n    the factors, while ensuring m is less than or equal to n and does not exceed the specified factor.\n    \"\"\"\n\n    def compute_factors_close_to_square():\n        \"\"\"Find the pair of factors of `dimension` that is closest to a square-like dimension, i.e., m and n close to each other.\"\"\"\n        min_difference = float('inf')\n        best_pair = (1, dimension)\n        for i in range(1, int(dimension**0.5) + 1):\n            if dimension % i == 0:\n                j = dimension // i\n                if i <= j:\n                    difference = j - i\n                    if difference < min_difference:\n                        min_difference = difference\n                        best_pair = (i, j)\n        return best_pair\n\n    if factor > 0:\n        if dimension % factor == 0:\n            m = min(factor, dimension // factor)\n            n = max(factor, dimension // factor)\n            return (m, n)\n        else:\n            raise ValueError(\"The provided factor does not divide the dimension exactly.\")\n    elif factor < 0:\n        # When factor is negative, we take its absolute value as a maximum allowable factor\n        abs_factor = abs(factor)\n        closest_pair = compute_factors_close_to_square()\n        # Ensure m does not exceed the specified negative factor, adjusting if necessary\n        if closest_pair[0] > abs_factor:\n            if dimension % abs_factor == 0:\n                return (abs_factor, dimension // abs_factor)\n            else:\n                raise ValueError(\"The absolute value of the specified negative factor does not divide the dimension exactly.\")\n        else:\n            return closest_pair\n    else:\n        # factor == 0 is treated as not specifying any particular factor and aims for the closest factors\n        return compute_factors_close_to_square()", "CodeBleu_score": 26.174506476104657}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\ndef factorization(dimension: int, factor: int = -1) -> tuple[int, int]:\n    \"\"\"\n    This function computes the factorization of a given dimension into two factors, m and n, with an optional specified factor.\n    If the factor is positive and divides the dimension, it returns the factor and the quotient, ensuring m is less than or equal to n.\n    If the factor is negative or not specified, it iteratively finds a factorization that minimizes the sum of the factors, \n    while ensuring m is less than or equal to n.\n    \"\"\"\n    if factor > 0 and dimension % factor == 0:\n        factor_pair = (factor, dimension // factor)\n        return (min(factor_pair), max(factor_pair))\n    else:\n        # Find the factorization that minimizes the sum m + n\n        min_sum = float('inf')\n        best_pair = (1, dimension)  # Default to 1 x dimension\n        for i in range(1, int(dimension**0.5) + 1):\n            if dimension % i == 0:\n                pair = (i, dimension // i)\n                pair_sum = sum(pair)\n                if pair_sum < min_sum:\n                    min_sum = pair_sum\n                    best_pair = (min(pair), max(pair))\n\n        return best_pair\n\n# For example,\n# factorization(12) would return (3, 4)\n# factorization(100, 10) would return (10, 10)", "CodeBleu_score": 26.967565239045822}, {"code": "def factorization(dimension: int, factor: int = -1) -> tuple[int, int]:\n    if factor > 0 and dimension % factor == 0:\n        # Factor is valid and divides the dimension\n        quotient = dimension // factor\n        m, n = sorted((factor, quotient))\n        return (m, n)\n    \n    # Factor is negative or not provided, find optimal factors\n    import math\n    best_m, best_n = 1, dimension\n    min_sum = dimension + 1  # Initialize to the largest possible sum plus one\n\n    # Iteratively search for factor pairs\n    for i in range(1, int(math.sqrt(dimension)) + 1):\n        if dimension % i == 0:\n            m, n = i, dimension // i\n            \n            if factor < 0 and abs(factor) < m:\n                continue  # m exceeds the absolute value of factor\n\n            if m + n < min_sum:\n                best_m, best_n = m, n\n                min_sum = m + n  # Update minimal sum\n            \n            # Since we are iterating from 1, m will always be <= n\n\n    return (best_m, best_n)", "CodeBleu_score": 40.65628140821071}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "import torch\n\nclass ModelWeightAdjuster:\n    def __init__(self, scale_factor=1.0):\n        self.scale_factor = scale_factor\n\n    def apply_weight_decompose(self, updown, orig_weight):\n        \"\"\"\n        Adjusts the weight of a model by combining an input weight with an original weight,\n        normalizing the result, and scaling it by a predefined scale factor.\n        The final adjusted weight is calculated by subtracting the original weight \n        from the scaled and normalized combined weight.\n        Ensures all tensors are on the same device and data type.\n        \n        Args:\n        updown (torch.Tensor): The input weight tensor to be combined.\n        orig_weight (torch.Tensor): The original weight tensor of the model.\n        \n        Returns:\n        torch.Tensor: The adjusted weight tensor.\n        \"\"\"\n        if updown.device != orig_weight.device:\n            updown = updown.to(orig_weight.device)\n        \n        if updown.dtype != orig_weight.dtype:\n            updown = updown.type(orig_weight.dtype)\n        \n        # Combining the weights\n        combined_weight = updown + orig_weight\n        \n        # Normalizing the combined weight\n        norm_combined_weight = combined_weight / torch.norm(combined_weight)\n        \n        # Scaling the normalized weight\n        scaled_weight = norm_combined_weight * self.scale_factor\n        \n        # Calculating final adjusted weight\n        adjusted_weight = scaled_weight - orig_weight\n        \n        return adjusted_weight\n\n# Usage with hypothetical tensors:\n# model_adjuster = ModelWeightAdjuster(scale_factor=2.0)\n# updown_weight = torch.tensor(..., device='cuda', dtype=torch.float32)\n# original_weight = torch.tensor(..., device='cuda', dtype=torch.float32)\n# adjusted_weight = model_adjuster.apply_weight_decompose(updown_weight, original_weight)", "CodeBleu_score": 31.281424771988934}, {"code": "import torch\n\nclass ModelWeightAdjuster:\n    def __init__(self, scale_factor=1.0):\n        \"\"\"\n        Initializes the ModelWeightAdjuster with a scale factor for the weight adjustment.\n        \"\"\"\n        self.scale_factor = scale_factor\n\n    def apply_weight_decompose(self, updown, orig_weight):\n        \"\"\"\n        This function decomposes and adjusts the weight of a model by combining an input weight\n        with an original weight, normalizing the result, and scaling it using a predefined scale factor.\n        The final adjusted weight is then calculated by subtracting the original weight from the\n        scaled and normalized combined weight.\n        The function ensures that all tensors are on the same device and have the same data type\n        before performing these operations.\n        \n        Args:\n        updown (torch.Tensor): Input weight tensor.\n        orig_weight (torch.Tensor): Original weight tensor.\n        \n        Returns:\n        torch.Tensor: Adjusted weight tensor.\n        \"\"\"\n        # Ensure tensors are on the same device and dtype\n        device = orig_weight.device\n        dtype = orig_weight.dtype\n        updown = updown.to(device=device, dtype=dtype)\n        \n        # Combine weights and normalize\n        combined_weight = updown + orig_weight\n        normalized_weight = combined_weight / torch.norm(combined_weight)\n\n        # Scale the normalized weight\n        scaled_weight = self.scale_factor * normalized_weight\n        \n        # Calculate final adjusted weight\n        adjusted_weight = scaled_weight - orig_weight\n        \n        return adjusted_weight\n\n# Usage example\nif __name__ == \"__main__\":\n    model_weight_adjuster = ModelWeightAdjuster(scale_factor=0.5)\n    updown = torch.tensor([1.0, 2.0, 3.0])\n    orig_weight = torch.tensor([3.0, 2.0, 1.0])\n    adjusted_weight = model_weight_adjuster.apply_weight_decompose(updown, orig_weight)\n    print(\"Adjusted Weight:\", adjusted_weight)", "CodeBleu_score": 33.75869366366773}, {"code": "import torch\n\nclass ModelWeightManager:\n    def __init__(self, scale_factor=1.0):\n        self.scale_factor = scale_factor\n\n    def apply_weight_decompose(self, updown, orig_weight):\n        \"\"\"\n        This function decomposes and adjusts the weight of a model by combining an input weight with an original weight,\n        normalizing the result, and scaling it using a predefined scale factor.\n        The final adjusted weight is then calculated by subtracting the original weight from the scaled and normalized combined weight.\n        The function ensures that all tensors are on the same device and have the same data type before performing these operations.\n        \"\"\"\n        # Ensure both weights are on the same device and type\n        updown = updown.to(orig_weight.device).type_as(orig_weight)\n        \n        # Combine the weights\n        combined_weight = updown + orig_weight\n        \n        # Normalize the combined weights\n        normed_weight = combined_weight / torch.norm(combined_weight, p=1)\n        \n        # Scale the normalized weights\n        scaled_weight = normed_weight * self.scale_factor\n        \n        # Compute the adjusted weights\n        adjusted_weight = scaled_weight - orig_weight\n        \n        return adjusted_weight\n\n# Example usage:\n# Create model weight manager with a scale factor\nmanager = ModelWeightManager(scale_factor=2.0)\n# Assuming updown and orig_weight are tensors, e.g., updown = torch.randn(10), orig_weight = torch.randn(10)\nadjusted_weights = manager.apply_weight_decompose(torch.randn(10), torch.randn(10))\nprint(adjusted_weights)", "CodeBleu_score": 33.87363235155142}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None):\n    \"\"\"This function finalizes the upscaling or downscaling of weights by adjusting shapes and applying biases, scalars, and decompositions as needed.\n    \n    Parameters:\n    - updown: The torch.Tensor that represents up-scaled or down-scaled weights.\n    - orig_weight: The original torch.Tensor weights before scaling.\n    - output_shape: The desired output shape for updown tensor.\n    - ex_bias: Optional, an additional external bias tensor that might be scaled or modified.\n\n    Returns:\n    - A tuple containing the modified updown tensor and the possibly modified ex_bias tensor.\n    \"\"\"\n    import torch\n\n    # Check if a bias needs to be added to the updown tensor\n    if ex_bias is not None:\n        updown += ex_bias\n\n    # Reshape the updown tensor to match the desired output shape\n    updown = updown.reshape(output_shape)\n\n    # If the original weights and updown have the same number of elements, match shapes\n    if updown.numel() == orig_weight.numel():\n        updown = updown.view_as(orig_weight)\n\n    # If there is an external bias, apply a multiplier (typically radical dimensionality change scenarios)\n    if ex_bias is not None:\n        multiplier = torch.sqrt(torch.tensor(float(updown.numel()) / ex_bias.numel()))\n        ex_bias *= multiplier\n\n    # Example of applying a scaling factor or a weight decomposition\n    # scaling_factor could be part of self parameters\n    scaling_factor = getattr(self, 'scaling_factor', 1.0)\n    updown = updown * scaling_factor\n\n    # Assume a decomposition method is part of the class, apply if necessary\n    if hasattr(self, 'decompose_weight'):\n        updown = self.decompose_weight(updown)\n\n    return updown, ex_bias", "CodeBleu_score": 46.926206193962436}, {"code": "import numpy as np\n\ndef finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None):\n    \"\"\"\n    This function finalizes the upscaling or downscaling of weights by adjusting shapes and\n    applying biases, scalars, and decompositions as needed.\n    \n    - It first adjusts the updown tensor with the bias and reshapes it according to the output shape.\n    - If the original and updown weights have the same number of elements, it reshapes updown to match\n      the original weight's shape.\n    - If an additional bias is provided, it applies a multiplier to it.\n    - If a scaling factor is defined, it applies a weight decomposition to updown.\n    - Finally, it returns the scaled and adjusted updown tensor along with the modified additional bias.\n    \n    Parameters:\n        updown (np.ndarray): The tensor to be upscaled or downscaled.\n        orig_weight (np.ndarray): The original weight tensor before scaling.\n        output_shape (tuple): The desired shape of the updown tensor after processing.\n        ex_bias (np.ndarray, optional): An additional bias to apply. Default is None.\n        \n    Returns:\n        np.ndarray: The adjusted updown tensor.\n        np.ndarray: The optionally modified ex_bias.\n    \"\"\"\n    # Reshape the updown tensor to the output shape\n    updown = np.reshape(updown, output_shape)\n    \n    # Check if the number of elements match between updown and the original weight\n    if updown.size == orig_weight.size:\n        updown = np.reshape(updown, orig_weight.shape)\n    \n    # Apply additional bias if provided\n    if ex_bias is not None:\n        # Assume some multiplier or transformation if needed\n        ex_bias *= 1.1  # Example: increasing the bias by 10%\n    \n    # Check for an example scaling factor or decomposition (somewhat arbitrary here)\n    scaling_factor = 1.05  # Example scaling factor\n    updown *= scaling_factor\n    \n    return updown, ex_bias", "CodeBleu_score": 24.935255024326658}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None):\n        \"\"\"This function finalizes the upscaling or downscaling of weights by adjusting shapes and applying biases,\n        scalars, and decompositions as needed. It first adjusts the updown tensor with the bias and reshapes it\n        according to the output shape. If the original and updown weights have the same number of elements,\n        it reshapes updown to match the original weight's shape. If an additional bias is provided, it applies\n        a multiplier to it. If a scaling factor is defined, it applies a weight decomposition to updown.\n        Finally, it returns the scaled and adjusted updown tensor along with the modified additional bias.\n\n        Parameters:\n            updown (Tensor): The tensor representing weights that have been upscaled or downscaled.\n            orig_weight (Tensor): The original weight tensor.\n            output_shape (tuple): The desired shape of the output tensor.\n            ex_bias (Tensor, optional): An additional bias tensor. Defaults to None.\n\n        Returns:\n            Tensor: The scaled and adjusted updown tensor.\n            Tensor: The modified additional bias (if provided, otherwise returns unchanged ex_bias).\n        \"\"\"\n        import torch\n\n        # Reshape updown to the output shape\n        updown = updown.view(*output_shape)\n\n        # Check if the number of elements matches the original weights\n        if updown.numel() == orig_weight.numel():\n            updown = updown.view(orig_weight.shape)\n\n        # Apply an additional bias if provided\n        if ex_bias is not None:\n            # Assuming some kind of multiplier to apply, simplistically doubled for demonstration\n            ex_bias *= 2\n\n        # If a scaling factor is to be applied, apply a simple weight decomposition\n        # Assume a fictive scaling factor exists for the purpose of demonstration\n        scaling_factor = getattr(self, 'scaling_factor', None)\n        if scaling_factor is not None:\n            updown = torch.matmul(updown, scaling_factor)\n\n        return updown, ex_bias", "CodeBleu_score": 38.25928185658944}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "import torch\n\ndef another_function(orig_weight, new_weight):\n    \"\"\"Mock update function which could represent an actual weight update mechanism.\"\"\"\n    # Placeholder update logic, e.g., averaging the weights\n    return (orig_weight + new_weight) / 2\n\nclass WeightUpdater:\n    def __init__(self, current_weight, bias=None):\n        self.current_weight = current_weight\n        self.bias = bias\n\n    def calc_updown(self, orig_weight):\n        \"\"\"\n        This function calculates and returns an updated weight tensor based on the original weight tensor and the current weight tensor.\n        It transfers the current weight tensor (and optionally the bias) to the device of the original weight tensor, \n        then calls another function to finalize the updated weights.\n        \"\"\"\n        # Transfer current_weight (and bias if exists) to the device of orig_weight\n        device = orig_weight.device\n        self.current_weight = self.current_weight.to(device)\n        if self.bias is not None:\n            self.bias = self.bias.to(device)\n\n        # Call another function to finalize the weights\n        updated_weight = another_function(orig_weight, self.current_weight)\n\n        return updated_weight\n\n# Example usage:\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\norig_weight = torch.randn(10, 10, device=device)\ncurrent_weight = torch.randn(10, 10)\nbias = torch.randn(10, device=device)\nupdater = WeightUpdater(current_weight, bias)\nnew_weight = updater.calc_updown(orig_weight)\nprint(new_weight)", "CodeBleu_score": 26.9601927103712}, {"code": "class NeuralNetwork:\n    def __init__(self):\n        self.current_weight = None  # Assuming this holds the values of the current weights\n\n    def calc_updown(self, orig_weight):\n        \"\"\"\n        This function calculates and returns an updated weight tensor based on the original weight tensor and the current weight tensor.\n        It transfers the current weight tensor and optional bias to the device of the original weight tensor, then calls another function to finalize the updated weights.\n        \n        Args:\n        orig_weight (tensor): The original weight tensor (could be a numpy array, PyTorch tensor, TensorFlow tensor, etc.)\n        \n        Returns:\n        tensor: The updated weight tensor after processing.\n        \"\"\"\n        # Example: Transfer weights to the same device (if using PyTorch or a similar library).\n        if hasattr(orig_weight, 'device'):\n            updated_weight = self.current_weight.to(orig_weight.device)\n        else:\n            updated_weight = self.current_weight  # Just a placeholder for non-GPU/cross-device context handling.\n\n        # Optionally, handle bias transfer in a similar manner (not shown as bias is not detailed).\n\n        # Call another method to perform the actual update (the following method is hypothetical).\n        final_weight = self.finalize_weights(updated_weight, orig_weight)\n\n        return final_weight\n\n    def finalize_weights(self, updated_weight, orig_weight):\n        \"\"\"\n        Hypothetical method to perform some additional manipulations to finalize the weights.\n        \n        Args:\n        updated_weight (tensor): The weight tensor transferred to the appropriate device.\n        orig_weight (tensor): The original weight tensor.\n        \n        Returns:\n        tensor: The final updated weight tensor.\n        \"\"\"\n        # Hypothetical example: simple averaging of the original and updated weights.\n        final_weight = (updated_weight + orig_weight) / 2\n        return final_weight", "CodeBleu_score": 34.338819140078016}, {"code": "import torch\n\ndef calc_updown(self, orig_weight):\n    \"\"\"\n    This function calculates and returns an updated weight tensor based on the original weight tensor and \n    the current weight tensor.\n    It transfers the current weight tensor and optional bias to the device of the original weight tensor, \n    then calls another function to finalize the updated weights.\n    \n    Parameters:\n    - orig_weight (torch.Tensor): The original weight tensor\n    \n    Returns:\n    - torch.Tensor: The updated weight tensor\n    \"\"\"\n    \n    def finalize_weights(updated_weights):\n        # Placeholder for actual implementation details of final weight adjustments\n        return updated_weights - 0.1 * updated_weights  # example adjustment: 10% reduction\n\n    # Assume self.current_weight is the current state of weights\n    if not hasattr(self, 'current_weight'):\n        print(\"Error: Object must have 'current_weight' attribute.\")\n        return None\n\n    # Transfer current weights to the device of the original weights\n    device = orig_weight.device\n    current_weight_moved = self.current_weight.to(device)\n\n    # Perhaps combine or modify weights in some way (need specifics to implement correctly)\n    # For example, if we are just transferring the weights:\n    updated_weights = finalize_weights(current_weight_moved)\n\n    return updated_weights", "CodeBleu_score": 27.33833462272716}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "import gc  # Garbage collector interface\n\ndef purge_networks_from_memory(limit=10):\n    \"\"\"\n    This function purges networks from memory if the number of networks exceeds a specified limit.\n    It removes networks from memory until the count is within the limit, then triggers garbage collection to free up memory.\n\n    Args:\n    limit (int): The maximum number of networks that should remain in memory.\n    \"\"\"\n    global networks  # Assuming 'networks' is a global variable storing the network objects\n    \n    # Check if the number of networks exceeds the limit\n    if len(networks) > limit:\n        # Calculate how many networks to remove\n        remove_count = len(networks) - limit\n        \n        # Remove the specified number of networks\n        for _ in range(remove_count):\n            # Remove the first network (or last, depending on the business requirement)\n            networks.pop(0)  # Change the index as needed\n        \n        # Trigger garbage collection to free up memory\n        gc.collect()", "CodeBleu_score": 27.727190480046588}, {"code": "import gc\n\n# This would be set somewhere in your application, configuring the memory management behavior\nnetwork_limit = 100\nall_networks = []  # This list is hypothetical and should be defined to store network instances\n\ndef purge_networks_from_memory():\n    \"\"\"\n    This function purges networks from memory if the number of networks exceeds a specified limit.\n    It removes networks from memory until the count is within the limit, then triggers garbage collection to free up memory.\n    \"\"\"\n    # Check if the number of networks exceeds the limit\n    if len(all_networks) > network_limit:\n        # Calculate how many networks to remove\n        excess = len(all_networks) - network_limit\n        \n        # Removing excess networks; assuming FIFO removal for simplicity\n        del all_networks[:excess]\n        \n        # Trigger garbage collection\n        gc.collect()\n\n    print(f\"Networks purged. Total networks in memory: {len(all_networks)}\")\n\n# Example usage with a placeholder network list and a sample scenario\nall_networks = ['network_' + str(i) for i in range(120)]  # Example network names\npurge_networks_from_memory()  # Purges excess networks, reducing to 100", "CodeBleu_score": 25.232817083424404}, {"code": "import gc\n\n# Assume a global list for demonstration purposes; could also be part of a class or passed as an argument.\nnetworks = []\n\n# Assuming a configurable limit (could be a parameter too)\nMAX_NETWORKS = 10\n\ndef purge_networks_from_memory():\n    \"\"\"\n    This function purges networks from memory if the number of networks exceeds a specified limit.\n    It removes networks from memory until the count is within the limit, then triggers garbage collection to free up memory.\n    \"\"\"\n    # Check if the count of networks exceeds the allowed maximum\n    if len(networks) > MAX_NETWORKS:\n        # Calculate how many networks to remove\n        excess = len(networks) - MAX_NETWORKS\n        \n        # Remove the excess networks\n        for _ in range(excess):\n            # Assuming removing from the end, but could be based on other criteria like least recently used, etc.\n            networks.pop()\n\n    # Trigger garbage collection\n    gc.collect()\n\n# Example usage:", "CodeBleu_score": 31.62861095742616}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self):\n    \"\"\"\n    This function restores the weights and biases of a neural network layer from a backup.\n    It checks if backup weights and biases exist and, if so, copies them back to the corresponding attributes of the layer.\n    Special handling is included for the MultiheadAttention layer to correctly restore its projection weights and biases.\n    If no bias backup exists, the bias is set to None.\n    \"\"\"\n    # Check if the backup weights exist\n    if hasattr(self, 'backup_weights'):\n        self.weight.data = self.backup_weights.clone()\n\n    # Check if the backup biases exist\n    if hasattr(self, 'backup_biases'):\n        self.bias.data = self.backup_biases.clone()\n    else:\n        self.bias = None\n\n    # Special handling for MultiheadAttention layer\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, 'backup_in_proj_weight'):\n            self.in_proj_weight.data = self.backup_in_proj_weight.clone()\n        if hasattr(self, 'backup_in_proj_bias') and self.in_proj_bias is not None:\n            self.in_proj_bias.data = self.backup_in_proj_bias.clone()\n        if hasattr(self, 'backup_out_proj_weight'):\n            self.out_proj.weight.data = self.backup_out_proj_weight.clone()\n        if hasattr(self, 'backup_out_proj_bias'):\n            self.out_proj.bias.data = self.backup_out_proj_bias.clone()", "CodeBleu_score": 36.391220878311515}, {"code": "import torch\nfrom typing import Union\n\n\ndef network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    \"\"\"\n    This function restores the weights and biases of a neural network layer from a backup.\n    It checks if backup weights and biases exist and, if so, copies them back to the corresponding attributes of the layer.\n    Special handling is included for the MultiheadAttention layer to correctly restore its projection weights and biases.\n    If no backup for biases exists, the bias is set to None.\n    \"\"\"\n\n    # General assumption made here: the backups are stored in attributes named `weight_backup` and `bias_backup` in each layer\n    has_weight_backup = hasattr(self, 'weight_backup')\n    has_bias_backup = hasattr(self, 'bias_backup')\n\n    # Restoring weights and biases if backups exist\n    if has_weight_backup:\n        self.weight.data = self.weight_backup.data.clone()\n\n    # Handling the bias\n    if has_bias_backup:\n        self.bias.data = self.bias_backup.data.clone()\n    else:\n        self.bias = None  # Setting bias to None if there is no backup\n\n    # Special handling for MultiheadAttention (this layer has q_proj, k_proj, v_proj and potentially more)\n    if isinstance(self, torch.nn.MultiheadAttention):\n        if hasattr(self, \"q_proj_weight_backup\") and hasattr(self, \"k_proj_weight_backup\") and hasattr(self, \"v_proj_weight_backup\"):\n            self.in_proj_weight.data[:self.embed_dim] = self.q_proj_weight_backup.data\n            self.in_proj_weight.data[self.embed_dim:2*self.embed_dim] = self.k_proj_weight_backup.data\n            self.in_proj_weight.data[2*self.embed_dim:] = self.v_proj_weight_backup.data\n\n        # If separate biases are maintained and a backup exists\n        if hasattr(self, \"q_proj_bias_backup\") and hasattr(self, \"k_proj_bias_backup\") and hasattr(self, \"v_proj_bias_backup\"):\n            self.in_proj_bias.data[:self.embed_dim] = self.q_proj_bias_backup.data\n            self.in_proj_bias.data[self.embed_dim:2*self.embed_dim] = self.k_proj_bias_backup.data\n            self.in_proj_bias.data[2*self.embed_dim:] = self.v_proj_bias_backup.data", "CodeBleu_score": 38.78225057617775}, {"code": "import torch\nfrom typing import Union\n\ndef network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    \"\"\"\n    This function restores the weights and biases of a neural network layer from a backup.\n    It checks if backup weights and biases exist and, if so, copies them back to the corresponding attributes of the layer.\n    Special handling is included for the MultiheadAttention layer to correctly restore its projection weights and biases.\n    If no bias backup exists, the bias is set to None.\n    \"\"\"\n    # Check for attribute that contain backups, such as weight_backup and bias_backup \n    if hasattr(self, 'weight_backup'):\n        self.weight.data = self.weight_backup.data.clone()  # Restore the weight\n\n    # Check and restore bias if applicable\n    if hasattr(self, 'bias_backup') and self.bias_backup is not None:\n        if getattr(self, 'bias', None) is not None:\n            self.bias.data = self.bias_backup.data.clone()  # Restore the bias\n        else:\n            self.register_parameter('bias', torch.nn.Parameter(self.bias_backup.data.clone()))\n    elif hasattr(self, 'bias') and self.bias is not None:\n        self.bias = None  # Set bias to None if there's no backup and bias exists\n                \n    # Additional handling for the MultiheadAttention layer\n    if isinstance(self, torch.nn.MultiheadAttention):\n        # Ensure backups exist for all required parameters\n        if hasattr(self, 'in_proj_weight_backup'):\n            self.in_proj_weight.data = self.in_proj_weight_backup.data.clone()\n        if hasattr(self, 'in_proj_bias_backup') and self.in_proj_bias_backup is not None:\n            self.in_proj_bias.data = self.in_proj_bias_backup.data.clone()\n        \n        # Restore the out_proj layer separately\n        if hasattr(self, 'out_proj_weight_backup'):\n            self.out_proj.weight.data = self.out_proj_weight_backup.data.clone()\n        if hasattr(self, 'out_proj_bias_backup') and self.out_proj_bias_backup is not None:\n            self.out_proj.bias.data = self.out_proj_bias_backup.data.clone()", "CodeBleu_score": 39.39267793548101}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward):\n    # Check if any additional networks are loaded\n    if hasattr(org_module, 'loaded_networks') and org_module.loaded_networks:\n        additional_networks = org_module.loaded_networks\n    else:\n        additional_networks = []\n\n    # Move input to the appropriate device (assumed to be the same as org_module's device)\n    input_device = org_module.device\n    input = input.to(input_device)\n\n    # Restore the module's original weights if they were backed up\n    if hasattr(org_module, 'backup_weights'):\n        org_module.load_state_dict(org_module.backup_weights)\n\n    # Perform the original forward pass\n    output = original_forward(org_module, input)\n\n    # If additional networks are present, apply each sequentially\n    for network in additional_networks:\n        if network.should_apply(org_module):  # Assuming a method that decides applicability per layer\n            output = network.forward(output)  # Assuming each additional network can perform forward\n\n    return output", "CodeBleu_score": 36.4312504685612}, {"code": "def network_forward(org_module, input, original_forward):\n    \"\"\"\n    This function performs a forward pass through a neural network module, integrating additional processing from loaded networks if available.\n    It first checks if any additional networks are loaded and uses the original forward method if none are present.\n    The input is cast to the appropriate device, and the module's weights are restored from backup and reset.\n    The original forward method is called, and then each loaded network is applied sequentially if it matches the current layer, modifying the output accordingly.\n    The final output is returned.\n    \"\"\"\n    \n    # Assume we have a method to check if additional networks are loaded for this module.\n    # This would typically be stored in some form in the module or passed as an argument.\n    # For this example, let's assume `has_additional_networks` to be a method and `get_additional_networks` retrieves them.\n    has_additional_networks = hasattr(org_module, 'additional_networks')\n    \n    # Cast input to the appropriate device (which theoretically should be defined at the module level)\n    device = next(org_module.parameters()).device  # Assuming module has parameters for simplicity\n    input = input.to(device)\n    \n    # Assuming there's a mechanism to \"backup and restore\" weights, which is not typical in everyday framework use.\n    # Potentially, this part might handle context or states, which is not clear from the background; thus, skipping.\n    \n    # Call the original forward method of the module\n    output = original_forward(input)\n    \n    # Apply any additional loaded networks after the original forward pass\n    if has_additional_networks:\n        for add_net in org_module.additional_networks:\n            if is_network_applicable(add_net, org_module):\n                # Modify the output using the additional network\n                if hasattr(add_net, 'forward'):\n                    output = add_net.forward(output)\n                else:\n                    # If the additional network does not have a forward, just applying a callable network\n                    output = add_net(output)\n\n    # Returning the final output after modifications from all applicable networks\n    return output\n\ndef is_network_applicable(add_net, org_module):\n    \"\"\" For simplicity, suppose this method returns True if the network should be applied. In real scenarios, this may check matching conditions or types.\"\"\"\n    return True", "CodeBleu_score": 34.87498451981067}, {"code": "def network_forward(org_module, input, original_forward):\n    \"\"\"\n    This function performs a forward pass through a neural network module, integrating additional processing \n    from loaded networks if available.\n    It first checks if any additional networks are loaded and uses the original forward method if none are present.\n    The input is cast to the appropriate device, and the module's weights are restored from backup and reset.\n    The original forward method is called, and then each loaded network is applied sequentially if it matches \n    the current layer, modifying the output accordingly.\n    The final output is returned.\n    \"\"\"\n    # Assume org_module may have attributes like 'additional_networks', 'device' and a method to reset weights\n    \n    # Step 1: Check for additional network processing capabilities\n    additional_networks = getattr(org_module, 'additional_networks', None)\n    \n    # Step 2: Ensure the input is on the appropriate device\n    device = getattr(org_module, 'device', 'cpu')\n    input = input.to(device)  # Ensure the input tensor is on the correct device\n    \n    # Step 3: If no additional networks are defined, use the original forward pass method\n    if not additional_networks:\n        return original_forward(input)\n    \n    # Step 4: Apply original forward method as a base\n    output = original_forward(input)\n    \n    # Step 5: Apply each additional network\n    for net in additional_networks:\n        if net.matches(org_module):    # Assuming each 'net' has a 'matches' method to check layer applicability\n            output = net.forward(output)  # Assuming each 'net' can forward the output further whether modifying or enhancing it\n    \n    # Step 6: Return the final output\n    return output", "CodeBleu_score": 34.29061463017336}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "import torch\nimport torch.nn as nn\n\nclass CustomModel(nn.Module):\n    def __init__(self):\n        super(CustomModel, self).__init__()\n        # Example layers\n        self.linear1 = nn.Linear(in_features=10, out_features=20)\n        self.norm1 = nn.LayerNorm(20)\n        self.linear2 = nn.Linear(in_features=20, out_features=5)\n        self.norm2 = nn.LayerNorm(5)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.norm1(x)\n        x = self.linear2(x)\n        x = self.norm2(x)\n        return x\n\n    def trainables(self):\n        \"\"\"\n        This function returns a list of trainable parameters (weights and biases) for linear and layer normalization layers in the model.\n        It iterates through the layers in the model, collecting the weights and biases of layers that are instances of torch.nn.Linear or torch.nn.LayerNorm.\n        \"\"\"\n        trainable_params = []\n        # Iterate through all model modules\n        for module in self.modules():\n            # Check if the module is an instance of Linear or LayerNorm\n            if isinstance(module, (nn.Linear, nn.LayerNorm)):\n                # Extend the list with parameters of the module which are trainable\n                trainable_params.extend([p for p in module.parameters() if p.requires_grad])\n        return trainable_params\n\n# Example usage:\nmodel = CustomModel()\ntrainable_parameters = model.trainables()\nprint(trainable_parameters)", "CodeBleu_score": 27.946267484146865}, {"code": "import torch\nimport torch.nn as nn", "CodeBleu_score": 6.126844756204651e-06}, {"code": "import torch.nn as nn\n\nclass Network:\n    def __init__(self, model):\n        self.model = model\n\n    def trainables(self):\n        \"\"\"\n        This function returns a list of trainable parameters (weights and biases) for linear and layer normalization \n        layers in a model. It iterates through the layers in the model, collecting the weights and biases of layers \n        that are instances of torch.nn.Linear or torch.nn.LayerNorm.\n        \"\"\"\n        trainable_params = []\n        # Iterate through the modules in the model\n        for module in self.model.modules():\n            # Check if the module is an instance of Linear or LayerNorm\n            if isinstance(module, (nn.Linear, nn.LayerNorm)):\n                # Append the module's parameters (weights and possibly biases) to the list\n                trainable_params.extend([p for p in module.parameters() if p.requires_grad])\n\n        return trainable_params\n\n# Example usage:\n# Assuming 'some_model' is an instance of a PyTorch model\n# network = Network(some_model)\n# print(network.trainables())", "CodeBleu_score": 27.14458554596221}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "# Assuming a global list to hold loaded hypernetworks\nloaded_hypernetworks = []\n\nclass Hypernetwork:\n    def __init__(self, name, multiplier=None):\n        self.name = name\n        self.multiplier = multiplier\n\n    def load(self):\n        # Simulate loading process\n        print(f\"Hypernetwork '{self.name}' loaded with multiplier {self.multiplier}\")\n\ndef load_hypernetwork_by_name(name):\n    \"\"\"Simulated function to mimic loading a hypernetwork.\"\"\"\n    # Some database or storage retrieval logic could be here\n    # For simplicity, just return a new hypernetwork instance\n    return Hypernetwork(name)\n\ndef set_multiplier(hypernetwork, multiplier):\n    if multiplier is not None:\n        hypernetwork.multiplier = multiplier\n\ndef remove_all_loaded_hypernetworks():\n    # Clear the global list\n    loaded_hypernetworks.clear()\n    print(\"All previously loaded hypernetworks were cleared.\")\n\ndef load_hypernetworks(names, multipliers=None):\n    \"\"\"This function loads specified hypernetworks, sets their multipliers, and stores them in a shared list of loaded hypernetworks.\n    It first checks for already loaded hypernetworks, clears the current list, then loads or retrieves each hypernetwork by name,\n    setting its multiplier if provided. Finally, it appends each loaded hypernetwork to the shared list.\"\"\"\n\n    # Clear the current list of loaded hypernetworks\n    remove_all_loaded_hypernetworks()\n\n    # Process each name and corresponding multiplier\n    for i, name in enumerate(names):\n        hypernetwork = load_hypernetwork_by_name(name)\n        \n        # Obtain multiplier if provided\n        if multipliers and i < len(multipliers):\n            set_multiplier(hypernetwork, multipliers[i])\n        \n        # Load hypernetwork and append to shared list\n        hypernetwork.load()\n        loaded_hypernetworks.append(hypernetwork)\n\n# Examples of use:\nnames = [\"net1\", \"net2\", \"net3\"]\nmultipliers = [1.2, 0.9, 1.4]\nload_hypernetworks(names, multipliers)", "CodeBleu_score": 33.25561395300819}, {"code": "# Assuming an external library for network handling, e.g., network_library\nimport network_library\n\n# This will store the loaded hypernetworks and keep them accessible globally.\nloaded_hypernetworks = []\n\ndef load_hypernetworks(names, multipliers=None):\n    \"\"\"\n    This function loads specified hypernetworks, sets their multipliers, \n    and stores them in a shared list of loaded hypernetworks.\n    It first checks for already loaded hypernetworks, clears the current list, \n    then loads or retrieves each hypernetwork by name, setting its multiplier if provided.\n    Finally, it appends each loaded hypernetwork to the shared list.\n    \n    Args:\n    names (list of str): A list of names identifying the hypernetworks to load.\n    multipliers (list of float, optional): A list of multipliers corresponding \n    to each hypernetwork; should match the length of `names` if provided.\n\n    Returns:\n    list: The list of loaded hypernetworks after updating multipliers and reloads.\n    \"\"\"\n    global loaded_hypernetworks\n    # Clear current loaded hypernetworks list\n    loaded_hypernetworks.clear()\n\n    # If no multipliers were provided, create a list of `None` of the same length as `names`\n    if multipliers is None:\n        multipliers = [None] * len(names)\n    \n    # Load each network by name and apply multipliers\n    for name, multiplier in zip(names, multipliers):\n        # Assume there's a method to fetch a hypernetwork by name\n        hypernetwork = network_library.get_hypernetwork_by_name(name)\n        if hypernetwork is not None:\n            # Set the multiplier if provided\n            if multiplier is not None:\n                hypernetwork.set_multiplier(multiplier)\n            # Add network to the global list\n            loaded_hypernetworks.append(hypernetwork)\n\n    return loaded_hypernetworks", "CodeBleu_score": 32.5739250035752}, {"code": "# Assuming there's a global list to keep track of loaded hypernetworks\nloaded_hypernetworks = []\n\ndef load_hypernetworks(names, multipliers=None):\n    \"\"\"\n    This function loads specified hypernetworks, sets their multipliers, and stores them in a shared list of loaded hypernetworks.\n    It first checks for already loaded hypernetworks, clears the current list, then loads or retrieves each hypernetwork by name,\n    setting its multiplier if provided.\n    Finally, it appends each loaded hypernetwork to the shared list.\n    \n    Args:\n    names (list of str): List of the names of the hypernetworks to be loaded.\n    multipliers (list of float, optional): Corresponding multipliers for each hypernetwork. Defaults to None.\n    \n    Returns:\n    None: The function does not return anything but updates the global loaded_hypernetworks list.\n    \"\"\"\n    \n    # Clearing the currently loaded hypernetworks list\n    global loaded_hypernetworks\n    loaded_hypernetworks.clear()\n\n    # Loop through each name provided\n    for index, name in enumerate(names):\n        # Hypothetically load the hypernetwork (actual load mechanism depends on specifics not provided)\n        hypernetwork = load_hypernetwork_by_name(name)  # This is a placeholder function\n        \n        if multipliers is not None and index < len(multipliers):\n            # If multipliers are provided, assign them to the respective hypernetwork\n            hypernetwork.set_multiplier(multipliers[index])  # Assuming hypernetwork objects have a method `set_multiplier`\n        \n        # Append the loaded and potentially modified hypernetwork to the global list\n        loaded_hypernetworks.append(hypernetwork)\n\ndef load_hypernetwork_by_name(name):\n    \"\"\"\n    Placeholder function to simulate loading a hypernetwork by its name.\n    This should be replaced with actual loading logic appropriate to the application context.\n    \n    Args:\n    name (str): Name of the hypernetwork to load\n    \n    Returns:\n    object: A hypothetical hypernetwork object\n    \"\"\"\n    # The actual details would depend on how hypernetworks are implemented\n    return HyperNetwork(name)\n\nclass HyperNetwork:\n    \"\"\" Basic representation of a hypernetwork. \"\"\"\n    def __init__(self, name):\n        self.name = name\n        self.multiplier = 1.0  # default multiplier\n\n    def set_multiplier(self, multiplier):\n        \"\"\" Set the multiplier for the hypernetwork. \"\"\"\n        self.multiplier = multiplier", "CodeBleu_score": 31.033912172822742}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "def apply_hypernetworks(hypernetworks, context, layer=None):\n    \"\"\"\n    This function applies multiple hypernetworks to a given context.\n    It initializes two context variables, context_k and context_v, with the input context.\n    Then, it iteratively applies each hypernetwork to these context variables using a function \n    that handles single hypernetwork application.\n    Finally, it returns the modified context_k and context_v.\n\n    Args:\n    hypernetworks (list or tuple): A list or tuple of hypernetwork functions.\n    context (any): The initial context to be modified by hypernetworks.\n    layer (optional): An optional parameter that could be used in hypernetwork functions.\n\n    Returns:\n    tuple: A tuple containing the modified context_k and context_v.\n    \"\"\"\n    context_k = context.copy()\n    context_v = context.copy()\n\n    for hypernetwork in hypernetworks:\n        context_k = apply_single_hypernetwork(hypernetwork, context_k, layer)\n        context_v = apply_single_hypernetwork(hypernetwork, context_v, layer)\n\n    return context_k, context_v\n\ndef apply_single_hypernetwork(hypernetwork, context, layer):\n    \"\"\"\n    Mock implementation of applying a single hypernetwork.\n    Args:\n    hypernetwork (function): a function representing the hypernetwork.\n    context (any): the context to which the hypernetwork is applied.\n    layer (optional): an additional layer information for hypernetwork application.\n\n    Returns:\n    any: updated context.\n    \"\"\"\n    # Assume hypernetwork modifies context based on some internal logic\n    modified_context = hypernetwork(context, layer) if layer else hypernetwork(context)\n    return modified_context", "CodeBleu_score": 39.24125628695195}, {"code": "def apply_hypernetworks(hypernetworks, context, layer=None):\n    \"\"\"\n    This function applies multiple hypernetworks to a given context.\n    It initializes two context variables, context_k and context_v, with the input context.\n    Then, it iteratively applies each hypernetwork to these context variables using a function\n    that handles single hypernetwork application.\n    Finally, it returns the modified context_k and context_v.\n\n    Args:\n    hypernetworks (list): List of hypernetworks to apply.\n    context (any): The initial context to which the hypernetworks are applied.\n    layer (optional): Optional layer parameter that may influence how hypernetworks are applied.\n\n    Returns:\n    tuple: A tuple containing the modified context_k and context_v after processing.\n    \"\"\"\n    # Initialize context_k and context_v with the provided context\n    context_k = context\n    context_v = context\n\n    # Apply each hypernetwork to context_k and context_v\n    for hypernetwork in hypernetworks:\n        context_k = apply_single_hypernetwork(hypernetwork, context_k, layer)\n        context_v = apply_single_hypernetwork(hypernetwork, context_v, layer)\n\n    return context_k, context_v\n\ndef apply_single_hypernetwork(hypernetwork, context, layer):\n    \"\"\"\n    Stub function to simulate applying a single hypernetwork to a context. \n    This function should be fully implemented based on the actual functionality required.\n\n    Args:\n    hypernetwork (any): A single hypernetwork.\n    context (any): Context to be processed by the hypernetwork.\n    layer (optional): Optional layer information that may influence the application.\n\n    Returns:\n    any: The modified context after the hypernetwork has been applied.\n    \"\"\"\n    # Implement the actual logic for applying a hypernetwork to a context.\n    # This is a dummy implementation and should be replaced with actual code.\n    return context  # Dummy return; replace with actual processing logic.", "CodeBleu_score": 63.12000863132039}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\ndef apply_hypernetworks(hypernetworks, context, layer=None):\n    \"\"\"\n    This function applies multiple hypernetworks to a given context.\n    It initializes two context variables, context_k and context_v, with the input context.\n    Then, it iteratively applies each hypernetwork to these context variables using a function that handles single hypernetwork application.\n    Finally, it returns the modified context_k and context_v.\n    \n    Args:\n        hypernetworks (list): A list of functions, each modeling a hypernetwork.\n        context (any): The initial input context to be transformed by hypernetworks.\n        layer (Optional[callable]): A specific function to apply transformation within each hypernetwork.\n\n    Returns:\n        Tuple: A tuple containing the transformed context_k and context_v.\n    \"\"\"\n    \n    # Initialize context_k and context_v with the input context\n    context_k = context\n    context_v = context\n    \n    # If a custom layer is defined, use it to apply the hypernetworks\n    # Otherwise, each hypernetwork is supposed to directly accept a context and return a modified context\n    for hypernetwork in hypernetworks:\n        if layer:\n            context_k = layer(hypernetwork, context_k)\n            context_v = layer(hypernetwork, context_v)\n        else:\n            context_k = hypernetwork(context_k)\n            context_v = hypernetwork(context_v)\n\n    return context_k, context_v", "CodeBleu_score": 46.85039178115852}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs):\n    \"\"\"\n    This function performs the forward pass of a cross-attention mechanism.\n    It computes query, key, and value projections from the input and context, applies hypernetworks to the context, \n    and reshapes the tensors for multi-head attention.\n    The function then calculates scaled dot-product attention, optionally applies a mask to the attention scores,\n    and computes the final attention output.\n    The result is reshaped back and passed through a final linear layer to produce the output.\n    \"\"\"\n\n    # Check if context is provided, else set it to x\n    if context is None:\n        context = x\n\n    # Compute queries from x\n    queries = self.query_projection(x)\n\n    # Compute keys and values from context\n    keys = self.key_projection(context)\n    values = self.value_projection(context)\n\n    # Usually for multi-head attention, we split projections into multiple heads,\n    # but here we need to handle the reshaping to suit the model\u2019s expected input dimensions for multi-head attention\n    batch_size, num_queries, _ = queries.shape\n    _, num_keys, _ = keys.shape\n    num_heads = self.num_heads\n    head_dim = self.model_dim // num_heads\n\n    # Reshape queries, keys, and values to [batch_size, num_heads, num_queries, head_dim]\n    queries = queries.view(batch_size, num_queries, num_heads, head_dim).transpose(1, 2)\n    keys = keys.view(batch_size, num_keys, num_heads, head_dim).transpose(1, 2)\n    values = values.view(batch_size, num_keys, num_heads, head_dim).transpose(1, 2)\n\n    # Scaling keys by a factor of the square root of head_dim to stabilize training\n    sqrt_head_dim = math.sqrt(head_dim)\n    scaled_keys = keys / sqrt_head_dim\n\n    # Compute raw attention scores via matrix multiplication\n    attention_scores = torch.matmul(queries, scaled_keys.transpose(-2, -1))\n\n    # Optionally apply mask here\n    if mask is not None:\n        # Expanding mask for compatibility with attention scores shape\n        # (mask typically two dimensions or broadcastable to such)\n        mask = mask.unsqueeze(1).unsqueeze(1)  # Adding heads and batch_size dimensions\n        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n\n    # Softmax normalization to obtain attention probabilities\n    attention_probs = torch.softmax(attention_scores, dim=-1)\n\n    # Apply dropout if needed (optional, based on the model configuration)\n    attention_probs = self.dropout(attention_probs)\n\n    # Weight values by the attention probabilities\n    attention_output = torch.matmul(attention_probs, values)\n\n    # Reshape attention output back to original dimensions of values before heads were split\n    attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, num_queries, -1)\n\n    # Last linear layer after attention mechanism\n    output = self.output_projection(attention_output)\n\n    return output", "CodeBleu_score": 33.53416769467166}, {"code": "import torch\nimport torch.nn.functional as F\n\ndef attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs):\n    \"\"\"\n    This function performs the forward pass of a cross-attention mechanism.\n    It computes query, key, and value projections from the input and context, applies hypernetworks to\n    the context, and reshapes the tensors for multi-head attention.\n    The function then calculates scaled dot-product attention, optionally applies a mask to the attention scores,\n    and computes the final attention output.\n    The result is reshaped back and passed through a final linear layer to produce the output.\n    \"\"\"\n    \n    # Checking for the context; if not provided, use the input itself as context\n    if context is None:\n        context = x\n\n    # Linear projections for query, key, and value\n    q = self.query_proj(x)\n    k = self.key_proj(context)\n    v = self.value_proj(context)\n\n    # Reshaping q, k, v for multi-head attention: (batch_size, num_heads, seq_length, head_dim)\n    batch_size = q.shape[0]\n    q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n    k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n    v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n\n    # Scaled dot-product attention\n    d_k = q.shape[-1]  # dimension of the key\n    scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(d_k)\n\n    # Applying mask (if provided)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n\n    attention_weights = F.softmax(scores, dim=-1)\n\n    # Attention output\n    attention_output = torch.matmul(attention_weights, v)\n\n    # Concatenate heads and put through final linear layer\n    attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n    output = self.output_proj(attention_output)\n\n    return output", "CodeBleu_score": 35.58983792904974}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs):\n    # Check if 'context' is provided, else use 'x' itself as context\n    if context is None:\n        context = x\n\n    # Step 1: Linear projections for query, key, and value\n    # Assuming these linear layers are defined elsewhere in the class (e.g., self.query_layer, self.key_layer, self.value_layer)\n    query = self.query_layer(x)  # Query from 'x'\n    key = self.key_layer(context)   # Key from 'context'\n    value = self.value_layer(context)  # Value from 'context'\n\n    # Optionally apply transformations specific to this layer\n    # This could include transformations by hypernetworks if desired\n    #if self.hypernetwork is not None:\n    #    context = self.hypernetwork(context)\n\n    # Step 2: Reshape query, key, value for multi-head attention\n    # Number of heads 'num_heads' also assumed to be an attribute of the class\n    batch_size, _, feature_size = query.shape\n    head_dim = feature_size // self.num_heads\n    query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n    key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n    value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n\n    # Step 3: Scaled dot-product attention\n    scaling_factor = head_dim ** -0.5\n    attention_scores = torch.matmul(query, key.transpose(-2, -1)) * scaling_factor\n\n    # Step 4: Apply mask if provided\n    if mask is not None:\n        mask = mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1)  # Adjust mask shape for head dimension\n        attention_scores = attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n\n    # Softmax to calculate the attention weights\n    attention_weights = torch.softmax(attention_scores, dim=-1)\n\n    # Multiply attention weights with value\n    attention_output = torch.matmul(attention_weights, value)  # Weighted sum based on attention\n\n    # Step 5: Reshape back and combine heads\n    attention_output = attention_output.transpose(1, 2).contiguous() \n    attention_output = attention_output.view(batch_size, -1, feature_size)\n\n    # Step 6: Apply the final linear layer\n    output = self.output_layer(attention_output)\n\n    return output", "CodeBleu_score": 34.03473544653542}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "import copy\nimport pickle\n\ndef save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename):\n    \"\"\"\n    This function saves the state of a hypernetwork to a file with a specified filename,\n    temporarily updating its attributes with new checkpoint information.\n    It backs up the original attributes, updates the hypernetwork with the new checkpoint data,\n    and attempts to save it.\n    If an error occurs, it restores the original attributes before re-raising the exception.\n\n    Parameters:\n        hypernetwork: The hypernetwork object which state needs to be saved.\n        checkpoint: A dictionary containing the checkpoint data to temporarily update the hypernetwork.\n        hypernetwork_name: The attribute name within the hypernetwork where checkpoint data should be applied.\n        filename: The path and name of the file where the hypernetwork should be saved.\n    \n    Returns:\n        None\n    \"\"\"\n    # Backing up original attributes\n    original_attributes = getattr(hypernetwork, hypernetwork_name, None)\n    \n    # Try to update the hypernetwork and save it\n    try:\n        # Temporarily update the hypernetwork with checkpoint\n        setattr(hypernetwork, hypernetwork_name, checkpoint)\n\n        # Writing the updated hypernetwork to a file\n        with open(filename, 'wb') as file:\n            pickle.dump(hypernetwork, file)\n    \n    except Exception as e:\n        # Restore original attributes if an error occurs\n        setattr(hypernetwork, hypernetwork_name, original_attributes)\n        raise e  # Re-raise the exception to signal failure\n\n    finally:\n        # Ensure that the original state is restored\n        setattr(hypernetwork, hypernetwork_name, original_attributes)", "CodeBleu_score": 26.879624208310755}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename):\n    \"\"\"This function saves the state of a hypernetwork to a file with a specified filename, temporarily updating its attributes with new checkpoint information.\n    It backs up the original attributes, updates the hypernetwork with the new checkpoint data, and attempts to save it.\n    If an error occurs, it restores the original attributes before re-raising the exception.\n    \n    Args:\n        hypernetwork (object): The hypernetwork object whose state needs to be saved.\n        checkpoint (dict): A dictionary containing the state or parameters to be updated in the hypernetwork.\n        hypernetwork_name (str): The attribute or key under which to store the checkpoint data within the hypernetwork object.\n        filename (str): The file path where the hypernetwork object should be saved.\n    \"\"\"\n    \n    # Backup original hypernetwork state\n    original_state = getattr(hypernetwork, hypernetwork_name, None)\n    \n    try:\n        # Update hypernetwork with new checkpoint data\n        setattr(hypernetwork, hypernetwork_name, checkpoint)\n        \n        # Save the hypernetwork to file\n        # Assuming serialization with pickle for simplicity, adjust based on actual implementation need\n        import pickle\n        with open(filename, 'wb') as file:\n            pickle.dump(hypernetwork, file)\n    \n    except Exception as e:\n        # Restore the original attributes if there's an error\n        setattr(hypernetwork, hypernetwork_name, original_state)\n        raise e  # Re-raise the caught exception to handle it further up the stack\n    \n    finally:\n        # Always ensure the original state is restored\n        setattr(hypernetwork, hypernetwork_name, original_state)", "CodeBleu_score": 26.582243308768525}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename):\n    \"\"\"This function saves the state of a hypernetwork to a file with a specified filename, temporarily updating its attributes with new checkpoint information.\n    It backs up the original attributes, updates the hypernetwork with the new checkpoint data, and attempts to save it.\n    If an error occurs, it restores the original attributes before re-raising the exception.\n    \n    Args:\n        hypernetwork (object): The hypernetwork object whose state needs to be saved.\n        checkpoint (dict): Dictionary containing the checkpoint data to update the hypernetwork with.\n        hypernetwork_name (str): Name/attribute, if necessary, used within the hypernetwork to store the checkpoint.\n        filename (str): Location and name of the file where the hypernetwork state will be saved.\n    \"\"\"\n    import torch\n    \n    # Backup current state\n    original_state = getattr(hypernetwork, hypernetwork_name, {}).copy()\n    \n    try:\n        # Set new state using the checkpoint\n        setattr(hypernetwork, hypernetwork_name, checkpoint)\n        \n        # Save the updated hypernetwork\n        torch.save(hypernetwork.state_dict(), filename)\n    except Exception as e:\n        # Restore the original state on failure\n        setattr(hypernetwork, hypernetwork_name, original_state)\n        # Reraise the caught exception\n        raise e\n    finally:\n        # Optionally ensure that the original state is always restored post operation\n        setattr(hypernetwork, hypernetwork_name, original_state)", "CodeBleu_score": 23.0743653245759}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "class Model:\n    def __init__(self, use_ema=False, logger=None):\n        self.parameters = {'weights': [0.1, 0.2, 0.3]}\n        self.ema_parameters = {'weights': [0.15, 0.25, 0.35]}\n        self.use_ema = use_ema\n        self.logger = logger\n\n    def log(self, message):\n        if self.logger is not None:\n            self.logger.info(message)\n\ndef ema_scope(self, context=None):\n    \"\"\"\n    This function manages the use of Exponential Moving Average (EMA) weights for a model,\n    temporarily switching to EMA weights and restoring the original weights after execution.\n    If EMA is enabled, it stores the current model parameters, copies EMA weights to the model,\n    and optionally logs this action.\n    After the code block execution, it restores the original weights and optionally logs the restoration.\n    \"\"\"\n    \n    if not self.use_ema:\n        raise ValueError('EMA is not enabled for this model')\n    \n    # Store original weights\n    original_weights = self.parameters['weights'].copy()\n\n    # Switch to EMA weights\n    self.parameters['weights'] = self.ema_parameters['weights'].copy()\n    if self.logger:\n        self.log('Switched to EMA weights')\n\n    # Execute the provided context block if callable\n    if context and callable(context):\n        context()\n    \n    # Restore original weights\n    self.parameters['weights'] = original_weights\n    if self.logger:\n        self.log('Restored original weights')\n\n# Usage example with a hypothetical logger setup and model initialization\nimport logging\n\n# Setting up a basic logger\nlogger = logging.getLogger()\nhandler = logging.StreamHandler()\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nlogger.setLevel(logging.INFO)\n\n# Instantiate a model with EMA enabled and a logger\nmodel = Model(use_ema=True, logger=logger)\n\n# Execute some context while EMA weights are temporarily activated\nmodel.ema_scope(context=lambda: print(\"Performing operations with EMA weights\"))", "CodeBleu_score": 25.173131902022394}, {"code": "class ModelWithEMA:\n    def __init__(self, model):\n        self.model = model\n        self.ema_weights = None\n        self.original_weights = None\n\n    def enable_ema(self):\n        ''' A method to simulate enabling EMA and copying EMA weights to the model. '''\n        # Simulate by storing original weights and replacing with EMA weights.\n        self.original_weights = {name: param.clone() for name, param in self.model.named_parameters()}\n        self.ema_weights = {name: param * 0.99 for name, param in self.model.named_parameters()} # Just a dummy EMA effect\n        self.model.load_state_dict(self.ema_weights)\n        print(\"EMA weights enabled.\")\n\n    def restore_original_weights(self):\n        ''' A method to restore original weights after using EMA weights. '''\n        self.model.load_state_dict(self.original_weights)\n        print(\"Original weights restored.\")\n\n    def __enter__(self):\n        ''' Backup original weights and apply EMA weights at the entry of the context. '''\n        self.enable_ema()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        ''' Restore the original weights on exiting the context. '''\n        self.restore_original_weights()\n        # Handle any exception in a custom way or just pass\n        if exc_type:\n            print(f\"An error occurred: {exc_value}\")\n        return False  # To propagate exceptions, if any\n\n# Example usage, assuming 'model' is some neural network model object with parameters.\n# ema_manager = ModelWithEMA(model)\n# with ema_manager:\n#     # Perform operations with the model (now using EMA weights)\n#     # e.g., validation of the model on a batch of data.", "CodeBleu_score": 26.981031483427593}, {"code": "class ModelWithEMA:\n    def __init__(model, ema):\n        \"\"\"\n        Initiate a ModelWithEMA instance.\n        \n        :param model: The original model whose weights need to be handled.\n        :param ema: An object that can compute and provide access to EMA weights.\n        \"\"\"\n        self.model = model\n        self.ema = ema\n        self.original_weights = None\n\n    def apply_ema_weights(self):\n        \"\"\"Apply the EMA weights to the model.\"\"\"\n        self.original_weights = [w.copy() for w in self.model.get_weights()]\n\n        # Assuming the EMA object has a method to directly set the weights\n        ema_weights = self.ema.compute_ema_weights()\n        self.model.set_weights(ema_weights)\n        print(\"EMA weights applied to the model.\")\n\n    def restore_original_weights(self):\n        \"\"\"Restore the original weights to the model.\"\"\"\n        self.model.set_weights(self.original_weights)\n        print(\"Original model weights restored.\")\n\n    def __enter__(self):\n        \"\"\"Enter the runtime context and apply the EMA weights.\"\"\"\n        self.apply_ema_weights()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"Exit the runtime context and restore the original weights.\"\"\"\n        self.restore_original_weights()\n\n# Example Usage\nmodel = ...  # Your model instance\nema = ...  # Your EMA handling instance\n\n# Use the context manager like this:\nwith ModelWithEMA(model, ema) as model_ctx:\n    # Perform operations while EMA is applied\n    # Example: model_ctx.model.evaluate(data)\n    pass", "CodeBleu_score": 29.679314173154914}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "import numpy as np\n\ndef p_mean_variance(self, x, t, clip_denoised: bool):\n    \"\"\"\n    Computes the mean and variance for a denoising process based on the model's output and the given time step t.\n    \n    Parameters:\n        x (np.array): The noisy input data.\n        t (int): The time step, or an iteration in the denoising process.\n        clip_denoised (bool): A flag to clip the output to within [-1, 1] range.\n    \n    Returns:\n        tuple: The mean, variance, and log variance of the model's predictions.\n    \"\"\"\n    # Predict noise or denoised data from the model\n    model_output = self.model.predict(x, t)\n    \n    # Estimate the mean of the denoised data\n    # For simplicity, assume model_output is noise to be subtracted\n    model_mean = x - model_output\n    \n    # Clipping the denoised output if required\n    if clip_denoised:\n        model_mean = np.clip(model_mean, -1, 1)\n    \n    # Compute posterior variance (this would depend on your specific model and noise schedule)\n    # Just a placeholder: actual computation should be based on your model specifics\n    posterior_variance = np.var(model_output) # Placeholder implementation\n    \n    # Calculate log variance\n    posterior_log_variance = np.log(posterior_variance)\n    \n    return model_mean, posterior_variance, posterior_log_variance", "CodeBleu_score": 36.45674964932051}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool):\n    \"\"\"\n    This function computes the mean and variance for a denoising process based on the model's output and the given parameterization.\n    It first gets the model output for the input and time step, then reconstructs the input either directly or from predicted noise.\n    If specified, the reconstructed input is clipped to a range of -1 to 1.\n    Finally, it calculates and returns the model mean, posterior variance, and posterior log variance.\n    \n    Args:\n    x (torch.Tensor): The noisy input data.\n    t (torch.Tensor): The time step tensor, indicating the current step in the denoising process.\n    clip_denoised (bool): A boolean indicating whether to clip the denoised output to the range [-1, 1].\n    \n    Returns:\n    Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing the posterior mean, the posterior variance,\n                                                     and the posterior log variance of the denoised data.\n    \"\"\"\n    # Assuming a method `self.model` that predicts the additive noise or the image directly.\n    model_output = self.model(x, t)\n\n    if isinstance(model_output, tuple):\n        # If model outputs both mean and variance, unpack them\n        predicted_mean, predicted_variance = model_output\n    else:\n        # If model outputs only mean, calculate variance externally (dummy example)\n        predicted_mean = model_output\n        predicted_variance = torch.full_like(predicted_mean, 0.1)  # example fixed/small variance\n\n    # Reconstructing the image from the predicted mean\n    denoised_image = x - predicted_mean\n    \n    if clip_denoised:\n        denoised_image = torch.clamp(denoised_image, -1, 1)\n\n    # Computing the Posterior variance and log variance (dummy example)\n    posterior_variance = predicted_variance # Simplistic model with direct variance prediction\n    posterior_log_variance = torch.log(posterior_variance + 1e-8)  # Avoid log(0)\n\n    return denoised_image, posterior_variance, posterior_log_variance", "CodeBleu_score": 37.28403345893285}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool):\n    \"\"\"\n    This function computes the mean and variance for a denoising process based on the model's output and the given parameterization.\n    It first gets the model output for the input and time step, then reconstructs the input either directly or from predicted noise.\n    If specified, the reconstructed input is clipped to a range of -1 to 1.\n    Finally, it calculates and returns the model mean, posterior variance, and posterior log variance.\n    \n    Arguments:\n    x -- Input data (e.g., a noisy image as a numpy array or Tensor).\n    t -- Current time step or noise level (scalar).\n    clip_denoised -- Boolean. If True, clips the reconstructed input to the range [-1, 1].\n\n    Returns:\n    mean -- The estimated mean of the denoised data.\n    variance -- The variance of the reconstruction.\n    log_variance -- The logarithm of the variance.\n    \"\"\"\n    \n    # Get model output. Assuming the model outputs the predicted noise or denoised image directly.\n    # model_output might be a function of x and t, e.g., model(x, t)\n    model_output = self.model(x, t)  # Assuming a forward method for simplicity\n    \n    # Compute the reconstructed image.\n    # If the model predicts the denoised image directly:\n    reconstructed_image = model_output\n    \n    # Or if the model predicts noise to be subtracted:\n    # reconstructed_image = x - model_output\n    \n    # Clip if specified\n    if clip_denoised:\n        reconstructed_image = np.clip(reconstructed_image, -1, 1)\n    \n    # Assuming the model can also provide means and log variances\n    # These could also be derived from model outputs depending on the architecture\n    # mean, log_variance = some_statistics_computation_method(reconstructed_image)\n    # For simplicity let's assume these are also directly modeled outputs:\n    mean = reconstructed_image  # This is a simplification and might not be true for your case.\n    variance = np.var(model_output, axis=None)  # Computing variance across all channels and pixels\n    log_variance = np.log(variance)  # Natural log of the variance\n\n    return mean, variance, log_variance", "CodeBleu_score": 34.56087557584957}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, loss_type, mean=True):\n    \"\"\"\n    Computes the loss between predicted and target values based on the specified loss type.\n    For 'l1' loss, it calculates the absolute difference, optionally taking the mean.\n    For 'l2' loss, it uses mean squared error, optionally taking the mean.\n    If an unknown loss type is specified, it raises an error.\n    \n    Parameters:\n        pred (ndarray): Array of predicted values.\n        target (ndarray): Array of actual target values.\n        loss_type (str): Type of loss to compute ('l1' or 'l2').\n        mean (bool): Whether to compute the mean of the losses.\n\n    Returns:\n        float or ndarray: The computed loss.\n    \"\"\"\n    import numpy as np\n\n    if loss_type == 'l1':\n        loss = np.abs(pred - target)\n    elif loss_type == 'l2':\n        loss = (pred - target) ** 2\n    else:\n        raise ValueError(f\"Unknown loss type {loss_type}\")\n\n    if mean:\n        return np.mean(loss)\n    return loss", "CodeBleu_score": 27.316275432124677}, {"code": "import torch\n\nclass Model:\n    def __init__(self, loss_type='l2'):\n        self.loss_type = loss_type\n\n    def get_loss(self, pred, target, mean=True):\n        \"\"\"This function computes the loss between predicted and target values based on the specified loss type:\n        For 'l1' loss, it calculates the absolute difference, optionally taking the mean.\n        For 'l2' loss, it uses mean squared error, optionally without reduction.\n        If an unknown loss type is specified, it raises an error.\n\n        Args:\n        pred (torch.Tensor): Predicted values.\n        target (torch.Tensor): True values.\n        mean (bool, optional): Whether to compute the mean of the loss or not. Defaults to True.\n\n        Returns:\n        torch.Tensor: Computed loss.\n        \n        Raises:\n        ValueError: If an unknown loss type is provided.\n        \"\"\"\n        if self.loss_type == 'l1':\n            loss = torch.abs(pred - target)\n        elif self.loss_type == 'l2':\n            loss = (pred - target) ** 2\n        else:\n            raise ValueError(\"Unknown loss type specified: {}\".format(self.loss_type))\n\n        if mean:\n            loss = torch.mean(loss) if loss.numel() > 0 else 0\n        else:\n            loss = torch.sum(loss) if loss.numel() > 0 else 0\n\n        return loss\n\n# Usage example\nmodel = Model('l2')\npred = torch.tensor([3.0, 4.0, 5.0])\ntarget = torch.tensor([1.0, 2.0, 3.0])\nprint(model.get_loss(pred, target, mean=False))", "CodeBleu_score": 42.447597318129475}, {"code": "import torch\n\nclass LossCalculator:\n    def __init__(self, loss_type='l2'):\n        self.loss_type = loss_type\n\n    def get_loss(self, pred, target, mean=True):\n        \"\"\"\n        This function computes the loss between predicted and target values based on the \n        specified loss type, either 'l1' or 'l2'.\n        For 'l1' loss, it calculates the absolute difference, optionally taking the mean.\n        For 'l2' loss, it uses mean squared error, optionally with or without reduction.\n        If an unknown loss type is specified, it raises an error.\n\n        Arguments:\n            pred (torch.Tensor): The predicted values.\n            target (torch.Tensor): The actual target values.\n            mean (bool, optional): A boolean to decide whether to take the mean of the losses or not. Defaults to True.\n\n        Returns:\n            torch.Tensor: The computed loss.\n        \"\"\"\n        if self.loss_type == 'l1':\n            loss = torch.abs(pred - target)\n        elif self.loss_type == 'l2':\n            loss = (pred - target) ** 2\n        else:\n            raise ValueError(\"Unsupported loss type. Use 'l1' or 'l2'.\")\n\n        if mean:\n            return loss.mean()\n        else:\n            return loss\n\n# Example usage\n# model = LossCalculator(loss_type='l2')\n# prediction = torch.tensor([1.0, 2.0, 3.0])\n# true_value = torch.tensor([1.5, 2.5, 3.0])\n# calc_loss = model.get_loss(prediction, true_value, mean=True)\n# print(calc_loss) # Output will be the mean squared error between prediction and true_value tensors.", "CodeBleu_score": 39.58777106571115}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "import torch\nimport torch.nn.functional as F\n\nclass YourModelClass:\n    def _prior_bpd(self, x_start):\n        \"\"\"\n        This function calculates the prior bits-per-dimension (bpd) for the input tensor x_start.\n        It determines the batch size and creates a tensor t representing the final timestep for each batch element.\n        The function then computes the mean and log variance of the predicted distribution at this timestep\n        and calculates the KL divergence between this distribution and a standard normal distribution.\n        Finally, it returns the mean KL divergence normalized by the logarithm of 2.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        image_dimensions = torch.tensor(x_start.shape[1:]).prod().item()  # Assuming image-like data tensor\n        \n        # Assume that model has a method like 'predict' to provide log variance and mean at the last timestep\n        # We also assume the maximum timestep `T` is available in the context\n        T = self.betas.shape[0]\n        t = (torch.ones(batch_size, device=x_start.device) * (T-1)).long()\n\n        # Assuming `self.model` provides a method to return mean and log variance at specified timestep\n        mu, logvar = self.model(x_start, t)\n\n        # Compute KL divergence with standard normal distribution\n        # KL = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2) over all dimensions\n        # From logvar, sigma^2 = exp(logvar)\n        kl_divergence = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n        \n        # Normalize KL divergence by the number of image pixel dimensions and convert to bits per dimension (log base 2)\n        bpd = kl_divergence / (image_dimensions * torch.log(torch.tensor(2.0)))\n\n        # Return the mean over the batch\n        return bpd.mean()\n        \n# To use it within a broader program, ensure you have a `self.model` which can process your inputs accordingly.", "CodeBleu_score": 36.63283727537689}, {"code": "import torch\n\nclass Model:\n    def __init__(self):\n        # Assume required initializations or loadings\n        pass\n\n    def predict_mean_logvar(self, x):\n        # Dummy implementation - Replace this with actual model prediction logic\n        # Here it is assumed that the model returns a tuple (mean, log variance) for each data\n        mu = x.mean(dim=1, keepdim=True)     # Dummy mu calculation\n        logvar = torch.zeros_like(mu)        # Dummy logvar with zeros\n        return mu, logvar\n\n    def _prior_bpd(self, x_start):\n        \"\"\"\n        This function calculates the prior bits-per-dimension (bpd) for the input tensor x_start.\n        It determines the batch size and creates a tensor t representing the final timestep for each batch element.\n        The function then computes the mean and log variance of the predicted distribution at this timestep and calculates the KL divergence between this distribution and a standard normal distribution.\n        Finally, it returns the mean KL divergence normalized by the logarithm of 2.\n        \"\"\"\n        # Determine batch size\n        batch_size = x_start.size(0)\n\n        # Create tensor t representing the final timestep for each batch element\n        # Assume x_start is shape [batch_size, channels, height, width] like an image input\n        # If your model's timestep requires different handling, adjust below line\n        t = torch.full((batch_size, 1), fill_value=x_start.size(1) - 1, device=x_start.device)\n        \n        # Compute mean and log variance at this timestep\n        mu, logvar = self.predict_mean_logvar(x_start)\n\n        # Compute KL divergence using mu and logvar, against a standard normal distribution\n        kl_divergence = 0.5 * torch.sum(torch.exp(logvar) + mu**2 - 1 - logvar, dim=1)\n\n        # Average over all batch elements\n        mean_kl_div = kl_divergence.mean()\n\n        # Normalize by log(2) to convert to bits per dimension\n        bits_per_dim = mean_kl_div / torch.log(torch.tensor(2.0))\n        \n        return bits_per_dim\n\n# Example usage:\nmodel = Model()\nx_start = torch.randn(10, 3, 32, 32)  # Random tensor simulating 10 images of shape 3x32x32\nbpd = model._prior_bpd(x_start)\nprint(bpd)", "CodeBleu_score": 32.26216893018742}, {"code": "import torch\n\nclass Model:\n    def __init__(self):\n        # Construct or load the model here, possibly some layers or pre-trained weights\n        pass\n\n    def model(self, x, t):\n        # Assuming the model outputs mean and log variance at timestep `t`\n        # This would normally involve passing x through (a part of) the model and transforming it based on `t`\n        # Here we just return a dummy mean and log variance for the example.\n        mean = torch.zeros_like(x)\n        log_variance = torch.zeros_like(x)\n        return mean, log_variance\n    \n    def _prior_bpd(self, x_start):\n        \"\"\"\n        This function calculates the prior bits-per-dimension (bpd) for the tensor x_start.\n        It determines the batch size and creates a tensor t representing the final timestep for each batch element.\n        The function computes the mean and log variance of the predicted distribution at this timestep\n        and calculates the KL divergence between this distribution and a standard normal distribution.\n        Finally, it returns the mean KL divergence normalized by the logarithm of 2.\n        \"\"\"\n        batch_size = x_start.size(0)\n        # Create a tensor of the last timestep `t` for each element in the batch.\n        # Assuming that timesteps in the context are encoded numerically as the last possible timestep\n        T = torch.full((batch_size,), x_start.size(1) - 1, dtype=torch.long, device=x_start.device)\n\n        # Get mean and log variance from the model method\n        mean, log_variance = self.model(x_start, T)\n\n        # Calculate KL divergence between the approximation and a standard normal distribution:\n        # D_KL(q(z|x) || p(z)) = 0.5 * sum( exp(log_variance) + mean^2 - 1 - log_variance), reduced across features dimension\n        kl_divergence = 0.5 * torch.sum(torch.exp(log_variance) + mean**2 - 1 - log_variance, dim=1)\n\n        # Calculate mean KL divergence per datapoint across the batch\n        mean_kl_divergence = torch.mean(kl_divergence)\n\n        # Normalize by logarithm of 2 to convert to bits-per-dimension\n        log2 = torch.log(torch.tensor(2.0))\n        bpd = mean_kl_divergence / log2\n\n        return bpd\n\n# Example of the class in use\n# model_instance = Model()\n# x_start_sample = torch.randn(10, 784)  # Example input tensor\n# bpd = model_instance._prior_bpd(x_start_sample)\n# print(bpd)", "CodeBleu_score": 34.65848216863618}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "import diskcache\n\ndef make_cache(subsection: str) -> diskcache.Cache:\n    \"\"\"\n    This function initializes a disk-based cache for a specific subsection.\n    It creates a Cache object from the diskcache module with a path based on the provided subsection\n    and a predefined cache directory. The cache has a size limit of 4GB, and it prioritizes keeping\n    up to 256KB in an SQLite database by setting a minimum file size for disk storage.\n\n    :param subsection: The subsection name which is used to create a unique path for this cache.\n    :return: A diskcache.Cache object configured and ready to use.\n    \"\"\"\n    # Base directory for the cache; can be changed to reflect the appropriate path\n    base_directory = \"/tmp/diskcache/\"\n    subsection_path = f\"{base_directory}{subsection}\"\n\n    # Creating cache with the specified size limit and minimum file size for disk storage\n    cache = diskcache.Cache(directory=subsection_path, size_limit=4 * 1024**3, disk_min_file_size=256 * 1024)\n\n    return cache", "CodeBleu_score": 49.46894576148182}, {"code": "bash\n   pip install diskcache", "CodeBleu_score": 0.0}, {"code": "bash\npip install diskcache", "CodeBleu_score": 0.0}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "import os\nimport threading\n\n# Dict to store cache objects\ncaches = {}\n# Lock for thread-safety in modifying the caches dictionary\ncache_lock = threading.Lock()\n\ndef convert_old_cached_data():\n    # Example function to convert legacy cached data\n    print(\"Converting old cached data...\")\n\ndef make_cache(subsection):\n    # Example function to create a new cache object\n    print(f\"Creating new cache for {subsection}...\")\n    return f\"Cache for {subsection}\"\n\ndef cache(subsection):\n    # First, try to retrieve the subsection from the cache\n    cache_object = caches.get(subsection)\n    \n    if cache_object is None:\n        # Acquire lock to ensure thread-safe access to the caches dictionary\n        with cache_lock:\n            # Check again in case another thread has created the cache entry\n            cache_object = caches.get(subsection)\n            if cache_object is not None:\n                return cache_object\n            \n            # Assume cache root directory\n            cache_dir = \"/path/to/cache/directory\"\n            cache_filename = os.path.join(cache_dir, f\"{subsection}.cache\")\n\n            # Check if the cache directory exists and if the cache filename is a valid file\n            if not os.path.exists(cache_dir) or not os.path.isfile(cache_filename):\n                convert_old_cached_data()\n            \n            # Re-check if cache was created during conversion\n            cache_object = caches.get(subsection)\n            if cache_object is None:\n                # Create new cache object and store it in the dictionary\n                cache_object = make_cache(subsection)\n                caches[subsection] = cache_object\n\n    return cache_object\n\n# Example use of the cache function\nresult = cache(\"sample_section\")\nprint(\"Returned cache object:\", result)", "CodeBleu_score": 42.642878500602656}, {"code": "def cache(subsection)\nimport os\nimport threading\n\n# Dummy example implementations for functions and global structures that the function might interact with\ndef convert_old_cached_data(subsection):\n    print(f\"Converting old cache data for {subsection}\")\n\ndef make_cache(subsection):\n    return f\"Cache object for {subsection}\"\n\ncaches = {}  # This dictionary will store cache objects by subsection\ncache_directory = '/path/to/cache'\ncache_lock = threading.Lock()  # Mutex lock for thread-safe operations\n\ndef cache(subsection):\n    \"\"\"The function cache(subsection) aims to retrieve or create a cached object for a given subsection. It first attempts to get the cache object from a global caches dictionary. If the cache object does not exist, it acquires a lock (cache_lock) to ensure thread-safety and performs the following steps:\n    1. Checks if the cache directory exists and if the cache filename (assumed to be subsection.txt') is a valid file. If these conditions are not met, it calls convert_old_cached_data() to convert legacy cached data.\n    2. Re-checks the caches dictionary for the subsection. If the cache object still does not exist, it creates a new cache object using make_cache(subsection) and stores it in the caches dictionary.\n    Finally, it returns the retrieved or newly created cache object.\n    \n    Args:\n    subsection (str): The subsection identifier for which to retrieve or create a cache object.\n    \n    Returns:\n    object: The cache object associated with the given subsection.\n    \"\"\"\n    \n    cache_object = caches.get(subsection)\n    \n    if cache_object is None:\n        cache_filename = os.path.join(cache_directory, f\"{subsection}.txt\")\n        \n        with cache_lock:\n            if not (os.path.exists(cache_directory) and os.path.exists(cache_filename)):\n                convert_old_cached_data(subsection)\n\n            cache_object = caches.get(subsection)  # Re-check after potential data conversion\n            \n            if cache_object is None:\n                cache_object = make_cache(subsection)\n                caches[subsection] = cache_object\n\n    return cache_object\n\n# Example usage\n\nprint(cache(\"exampleSubsection\"))  # Depending on implementation details, this can demonstrate the logic flow.\n", "CodeBleu_score": 42.13757307589197}, {"code": "import os\nimport threading\n\n# Example cache dictionaries and structures\ncaches = {}\ncache_lock = threading.Lock()\ncache_directory = \"/path/to/cache/directory\"\n\ndef convert_old_cached_data():\n    \"\"\"Simulated function to convert old cache format to the new one.\"\"\"\n    # The actual logic for converting old cache data would be implemented here.\n    pass\n\ndef make_cache(subsection):\n    \"\"\"Simulated function to create a new cache object.\"\"\"\n    # Here we'd typically initialize a new cache object (could be a file, memory structure, etc.)\n    return f\"Cache Object for {subsection}\"\n\ndef cache(subsection):\n    \"\"\"\n    Retrieve or create a cached object for a given subsection.\n    It checks the caches dictionary for an existing cache object.\n    If the object does not exist, it creates a new object safely with a lock.\n    \"\"\"\n    global cache_directory, caches, cache_lock\n\n    # Try to get a cached object\n    cache_object = caches.get(subsection)\n\n    # If cache object doesn't exist\n    if cache_object is None:\n        with cache_lock:  # Acquire thread lock to ensure only one thread creates the cache object.\n            # Double-check after obtaining the lock if the subsection isn't already cached\n            cache_object = caches.get(subsection)\n\n            if cache_object is None:\n                # Check if the cache directory and filename is valid\n                cache_filename = os.path.join(cache_directory, f\"{subsection}_cache.txt\")\n\n                # If directory or file does not exist, handle old cached data\n                if not os.path.isdir(cache_directory) or not os.path.isfile(cache_filename):\n                    convert_old_cached_data()\n\n                # Create a new cache object\n                cache_object = make_cache(subsection)\n                # Store the created cache object in the caches dictionary\n                caches[subsection] = cache_object\n\n    # Return the retrieved or newly created cache object\n    return cache_object", "CodeBleu_score": 41.341958179871135}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\nimport os\nimport pickle\n\ndef cache(subsection):\n    \"\"\"Simulate retrieving a cache dictionary for a specific subsection.\n    If cache file does not exist, it returns an empty dictionary.\"\"\"\n    cache_filename = f\"{subsection}_cache.pkl\"\n    try:\n        with open(cache_filename, 'rb') as cache_file:\n            return pickle.load(cache_file)\n    except FileNotFoundError:\n        return {}\n\ndef dump_cache(subsection, data):\n    \"\"\"Storing the updated cache dictionary back into the appropriate cache file.\"\"\"\n    cache_filename = f\"{subsection}_cache.pkl\"\n    with open(cache_filename, 'wb') as cache_file:\n        pickle.dump(data, cache_file)\n        \ndef cached_data_for_file(subsection, title, filename, func):\n    \"\"\"\n    The function manages cached data for a specific file. \n    It starts by retrieving an existing cache for the given subsection using the cache(subsection) function. \n    It then gets the last modification time (mtime) of the specified file. \n    If a cache entry exists for the given title, it checks if the file's mtime is more recent than the cached mtime; \n    if so, it invalidates the cache entry. \n    If no valid cache entry exists, it calls the provided function func() to generate new data, \n    stores this data along with the file's mtime in the cache, and updates the cache storage with dump_cache(). \n    Finally, it returns the cached or newly generated data.\n    \"\"\"\n    \n    # Retrieve an existing cache for the given subsection\n    subsection_cache = cache(subsection)\n    \n    # Get the last modification time of the specified file\n    try:\n        file_mtime = os.path.getmtime(filename)\n    except OSError:\n        raise FileNotFoundError(f\"The file {filename} could not be found.\")\n    \n    # Check if the cache entry exists and is still valid\n    if title in subsection_cache:\n        cached_entry = subsection_cache[title]\n        cached_mtime, cached_data = cached_entry['mtime'], cached_entry['data']\n        \n        # Check if the file's mtime is more recent than the cached mtime\n        if file_mtime > cached_mtime:\n            # Invalidate the outdated cache entry\n            is_cache_valid = False\n        else:\n            # Return the cached data\n            return cached_data\n    else:\n        is_cache_valid = False\n    \n    # Generate new data if no valid cache entry exists\n    if not is_cache_valid:\n        new_data = func()\n        \n        # Update cache\n        subsection_cache[title] = {\n            'mtime': file_mtime,\n            'data': new_data\n        }\n        \n        # Store updated data in cache\n        dump_cache(subsection, subsection_cache)\n        \n        # Return new data\n        return new_data\n\n# Example usage:\n# Assume that generate_data() is some computation or data-fetching function\n# data = cached_data_for_file(\"example_subsection\", \"example_title\", \"path_to_file.txt\", generate_data)", "CodeBleu_score": 34.99338318644362}, {"code": "import os\nimport time\n\ndef cache(subsection):\n    \"\"\"Placeholder function to represent retrieving a cache dictionary by section.\"\"\"\n    return {}\n\ndef dump_cache():\n    \"\"\"Placeholder function to represent saving any changes made to cache to persistent storage.\"\"\"\n    pass\n\ndef get_current_time():\n    \"\"\"Utility function for getting the current time in seconds.\"\"\"\n    return int(time.time())\n\ndef cached_data_for_file(subsection, title, filename, func):\n    \"\"\"\n    Manages cached data for a specific file by either retrieving it from cache or re-computing it if file has changed.\n    \n    :param subsection: Cache subsection where the data belongs.\n    :param title: The identifier of the data within the cache.\n    :param filename: Name/path of the file whose data is being cached.\n    :param func: Function to be called to generate the data if not present or invalidated in cache.\n    :return: Cached or newly generated data.\n    \"\"\"\n    # Get the cache for the specified subsection\n    current_cache = cache(subsection)\n    \n    # Get the last modification time of the file\n    file_mtime = os.path.getmtime(filename)\n    \n    # Check if there is a valid cache and if it's up to date\n    if title in current_cache:\n        cached_entry = current_cache[title]\n        cached_mtime, cached_data = cached_entry.get('mtime', 0), cached_entry.get('data', None)\n        \n        if cached_mtime >= file_mtime:\n            # Cache is up to date\n            return cached_data\n        else:\n            # Invalidate the cache entry\n            del current_cache[title]\n    \n    # Cache is invalid or does not exist, compute new data\n    new_data = func()\n    current_cache[title] = {\n        'mtime': file_mtime,\n        'data': new_data\n    }\n    \n    # Update the cache in storage (not specifically required in the doc, but generally necessary)\n    dump_cache()\n    \n    # Return the newly computed data\n    return new_data", "CodeBleu_score": 41.03130867521317}, {"code": "import os\nfrom cache_module import cache, dump_cache\n\ndef cached_data_for_file(subsection, title, filename, func):\n    \"\"\"\n    Manages cached data for a specific file by retrieving an existing cache for the given subsection and\n    file, checking the file's modification time, updating cache if necessary, and returning the cached\n    or newly generated data.\n\n    Args:\n    subsection (str): The subsection of the cache to retrieve.\n    title (str): The title of the specific data within the subsection.\n    filename (str): The path to the file that is related to the data.\n    func (callable): A function with no arguments that generates the data if it's not in the cache\n                     or needs to be updated.\n\n    Returns:\n    any: The data retrieved from cache or generated by func().\n    \"\"\"\n\n    cache_data = cache(subsection)\n    try:\n        current_mtime = os.path.getmtime(filename)\n    except OSError:\n        raise FileNotFoundError(f\"File {filename} not found.\")\n\n    if title in cache_data and 'mtime' in cache_data[title]:\n        cached_mtime = cache_data[title]['mtime']\n        if cached_mtime >= current_mtime:\n            return cache_data[title]['data']\n        else:\n            # Invalidate the existing cache if the file has been modified\n            del cache_data[title]\n\n    # If cache entry is not valid or does not exist, run the function to get new data\n    new_data = func()\n    cache_data[title] = {\n        'mtime': current_mtime,\n        'data': new_data\n    }\n\n    # Update the cache storage\n    dump_cache(subsection, cache_data)\n    \n    return new_data", "CodeBleu_score": 37.869203837005635}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "import sys\nimport traceback\n\ndef record_exception():\n    # Simulated function to log exception details for this example.\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    traceback.print_exception(exc_type, exc_value, exc_traceback, file=sys.stderr)\n\ndef report(message: str, *, exc_info: bool = False) -> None:\n    \"\"\"\n    The function logs an error message to stderr. It first calls record_exception() to log the exception details.\n    Then, it prints each line of the provided message to stderr, prefixed with \"***\".\n    If the exc_info parameter is True, it additionally prints the formatted traceback, indented by four spaces, to stderr,\n    followed by a line of \"---\".\n    \"\"\"\n    record_exception()\n    \n    # Split message into lines for prefixed output\n    message_lines = message.split(\"\\n\")\n    for line in message_lines:\n        print(f\"*** {line}\", file=sys.stderr)\n\n    if exc_info:\n        # Print formatted traceback information indented\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        tb_lines = traceback.format_exception(exc_type, exc_value, exc_traceback)\n        for line in tb_lines:\n            print(\"    \" + line.strip(), file=sys.stderr)\n        print(\"---\", file=sys.stderr)", "CodeBleu_score": 41.329397347185825}, {"code": "import traceback\nimport sys\n\ndef record_exception():\n    \"\"\"Utility function to log the current exception details.\"\"\"\n    exc_info = sys.exc_info()\n    traceback.print_exception(*exc_info)\n\ndef report(message: str, *, exc_info: bool = False) -> None:\n    \"\"\"\n    Logs an error message to stderr. This function follows these steps:\n    - Calls record_exception() to log the current exception details if any.\n    - Prints each line of the provided message to stderr, each line prefixed with \"***\".\n    - If exc_info is True, it additionally prints the formatted traceback, indented by four spaces, to stderr, followed by a line of \"---\".\n    \"\"\"\n    # Log the exception if there's one captured in the context\n    if exc_info and sys.exc_info()[0] is not None:\n        record_exception()\n\n    # Print the message prefixed with \"***\"\n    for line in message.split('\\n'):\n        print(f\"*** {line}\", file=sys.stderr)\n\n    # Print traceback if exc_info is True and an exception is currently being handled\n    if exc_info and sys.exc_info()[0] is not None:\n        print(\"---\", file=sys.stderr)\n        formatted_traceback = traceback.format_exc()\n        for line in formatted_traceback.split('\\n'):\n            print(f\"    {line}\", file=sys.stderr)", "CodeBleu_score": 42.58266234772175}, {"code": "import sys\nimport traceback\n\ndef record_exception():\n    \"\"\"Logs the exception details.\"\"\"\n    # Assuming behavior as an example\n    exc_type, exc_value, exc_traceback = sys.exc_info()\n    traceback_details = traceback.format_exception(exc_type, exc_value, exc_traceback)\n    for line in traceback_details:\n        sys.stderr.write(line)\n\ndef report(message: str, *, exc_info: bool = False) -> None:\n    \"\"\"\n    The function report(message: str, *, exc_info: bool = False) -> None logs an error message to stderr.\n    It first calls record_exception() to log the exception details.\n    Then, it prints each line of the provided message to stderr, prefixed with \"***\". If the exc_info\n    parameter is True, it additionally prints the formatted traceback, indented by four spaces, to stderr,\n    followed by a line of \"---\".\n    \"\"\"\n    # Log the exception details regardless as the doc suggest it's always done:\n    record_exception()\n\n    # Print the provided message with \"***\" prefix:\n    for line in message.splitlines():\n        sys.stderr.write(f\"*** {line}\\n\")\n\n    # Check if traceback information needs to be included:\n    if exc_info:\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        # Format traceback and print, indented by 4 spaces:\n        traceback_details = traceback.format_exception(exc_type, exc_value, exc_traceback)\n        for line in traceback_details:\n            indented_line = '    ' + line.replace('\\n', '\\n    ')\n            sys.stderr.write(indented_line)\n        sys.stderr.write('---\\n')", "CodeBleu_score": 36.90478995094559}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "class PromptStyle:\n    def __init__(self, prompt_start: str, prompt_end: str):\n        self.prompt_start = prompt_start\n        self.prompt_end = prompt_end\n\ndef extract_original_prompts(style: PromptStyle, prompt: str, negative_prompt: str) -> (bool, str, str):\n    \"\"\"The function extract_original_prompts(style, prompt, negative_prompt) extracts specific style text from given prompts \n    based on a provided style object.  It checks if the style's prompt and negative prompt are empty; \n    if so, it returns False along with the original prompts. It then attempts to extract style\n    text from both the positive and negative prompts. If either extraction fails, it returns False \n    with the original prompts. If both extractions are successful, it returns True along with \n    the extracted positive and negative prompts.\"\"\"\n    \n    # Define a helper function for the extraction process\n    def extract_text(whole_text: str, start: str, end: str) -> str:\n        start_idx = whole_text.find(start)\n        if start_idx == -1:\n            return None\n        start_idx += len(start)\n        end_idx = whole_text.find(end, start_idx)\n        if end_idx == -1:\n            return None\n        return whole_text[start_idx:end_idx].strip()\n    \n    # Check if the prompts given are empty or not\n    if not prompt or not negative_prompt:\n        return False, prompt, negative_prompt\n    \n    # Extract text for prompt\n    extracted_prompt = extract_text(prompt, style.prompt_start, style.prompt_end)\n    extracted_negative_prompt = extract_text(negative_prompt, style.prompt_start, style.prompt_end)\n    \n    # Check if both extractions are successful\n    if extracted_prompt is None or extracted_negative_prompt is None:\n        return False, prompt, negative_prompt\n    \n    return True, extracted_prompt, extracted_negative_prompt\n\n# Example of usage:\n# Assuming style has start \"<\" and end \">\" and we want text between these in prompts\nstyle = PromptStyle('<', '>')\nsuccess, pos_prompt, neg_prompt = extract_original_prompts(style, 'Here is <extracted part> of the prompt', 'Here is <negative part> of the prompt')\nprint(success, pos_prompt, neg_prompt) # Expected output: True, 'extracted part', 'negative part'", "CodeBleu_score": 40.79688477419563}, {"code": "class PromptStyle:\n    def extract_style_from_prompt(self, prompt):\n        # Placeholder implementation\n        if \"style\" in prompt:\n            return prompt[prompt.index(\"style\"):prompt.index(\"style\") + 5]\n        return None\n    \n    def extract_style_from_negative_prompt(self, negative_prompt):\n        # Placeholder implementation\n        if \"style\" in negative_prompt:\n            return negative_prompt[negative_prompt.index(\"style\"):negative_prompt.index(\"style\") + 5]\n        return None\n\ndef extract_original_prompts(style: PromptStyle, prompt: str, negative_prompt: str) -> (bool, str, str):\n    \"\"\"\n    The function extract_original_prompts(style, prompt, negative_prompt) extracts specific style text from given prompts\n    based on a provided style object. It checks if the style's prompt and negative prompt are empty; if so, it returns False\n    along with the original prompts. It then attempts to extract style text from both the positive and negative prompts.\n    If either extraction fails, it returns False with the original prompts. If both extractions are successful, it returns\n    True along with the extracted positive and negative prompts.\n\n    Args:\n    style (PromptStyle): An object that contains methods to extract styled text.\n    prompt (str): The positive prompt to process.\n    negative_prompt (str): The negative prompt to process.\n\n    Returns:\n    tuple:\n        bool: True if both extractions are successful, False otherwise.\n        str: The extracted or original positive prompt.\n        str: The extracted or original negative prompt.\n    \"\"\"\n    \n    # Check if the prompt or negative_prompt is empty\n    if not prompt or not negative_prompt:\n        return False, prompt, negative_prompt\n\n    # Attempt to extract style-specific text from the prompts\n    extracted_prompt = style.extract_style_from_prompt(prompt)\n    extracted_negative_prompt = style.extract_style_from_negative_prompt(negative_prompt)\n\n    # Check if the extractions were successful\n    if extracted_prompt is None or extracted_negative_prompt is None:\n        return False, prompt, negative_prompt\n    \n    # Return true with the successfully extracted parts\n    return True, extracted_prompt, extracted_negative_prompt", "CodeBleu_score": 44.602505262816464}, {"code": "class PromptStyle:\n    def extract(self, text: str) -> str:\n        \"\"\"Extract a specific style's portion of the text\"\"\"\n        # This method should be implemented to extract part of the text based on a style\n        pass\n\ndef extract_original_prompts(style: PromptStyle, prompt: str, negative_prompt: str):\n    \"\"\"\n    The function extract_original_prompts(style, prompt, negative_prompt) extracts specific style text from given prompts\n    based on a provided style object. It checks if the style's prompt and negative prompt are empty; \n    if so, it returns False along with the original prompts. It then attempts to extract style text from both the positive and \n    negative prompts. If either extraction fails, it returns False with the original prompts. If both extractions are successful, \n    it returns True along with the extracted positive and negative prompts.\n\n    Parameters:\n        style (PromptStyle): An instance of PromptStyle to be used for extracting text.\n        prompt (str): The positive prompt.\n        negative_prompt (str): The negative prompt.\n\n    Returns:\n        (bool, str, str): Tuple containing a boolean indicating the success of extraction and the extracted texts.\n    \"\"\"\n    \n    # Check if there's anything to extract from\n    if not prompt or not negative_prompt:\n        return False, prompt, negative_prompt\n    \n    # Extract content using the given `style` object\n    extracted_prompt = style.extract(prompt)\n    extracted_negative_prompt = style.extract(negative_prompt)\n    \n    # Check for successful extraction\n    if extracted_prompt is None or extracted_negative_prompt is None:\n        return False, prompt, negative_prompt\n    \n    # Both extractions were successful\n    return True, extracted_prompt, extracted_negative_prompt", "CodeBleu_score": 48.66539580445107}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "import random\n\nclass MyModel:\n    def __init__(self):\n        self.utterances = []\n\n    def _load_utterances(self):\n        # Load utterances into self.utterances\n        # This is just a placeholder; load logic depends on the actual data source\n        self.utterances = [\"utterance1\", \"utterance2\", \"utterance3\", \"utterance4\"]\n\n    @staticmethod\n    def random_partial_segment(utterance, n_frames):\n        # Assuming the utterance can be indexed and has a length,\n        # This function generates a random segment of length n_frames\n        max_start_index = len(utterance) - n_frames\n        if max_start_index < 0:\n            raise ValueError(\"n_frames is larger than the utterance length.\")\n        start_index = random.randint(0, max_start_index)\n        return utterance[start_index:start_index + n_frames]\n\n    def random_partial(self, count, n_frames):\n        \"\"\"\n        Generates random partial segments from a specified number of utterances.\n        If the utterances are not loaded, calls self._load_utterances() to load them.\n        It then samples the specified count of utterances using self.utterance_cycler.sample(count).\n        For each sampled utterance, creates a tuple containing the utterance and a random partial segment of length n_frames,\n        obtained by calling self.random_partial_segment(utterance, n_frames).\n        Finally, returns a list of these tuples.\n        \"\"\"\n        if not self.utterances:\n            self._load_utterances()\n\n        sampled_utterances = random.sample(self.utterances, count)\n        result = [(utterance, self.random_partial_segment(utterance, n_frames)) for utterance in sampled_utterances]\n        return result\n\n# Example use:\nmodel = MyModel()\nprint(model.random_partial(2, 5))", "CodeBleu_score": 31.8011739868896}, {"code": "def random_partial(self, count, n_frames):\n    \"\"\"\n    Generates random partial segments from a specified number of utterances.\n    If the utterances are not loaded, it calls self._load_utterances() to load them.\n    It then samples the specified count of utterances using self.utterance_cycler.sample(count).\n    For each sampled utterance, it creates a tuple containing the utterance and a random\n    partial segment of length n_frames, obtained by calling u.random_partial(n_frames).\n    Finally, it returns a list of these tuples.\n    \n    Parameters:\n        count (int): The number of utterances to sample.\n        n_frames (int): The frame length of the partial segment to generate.\n        \n    Returns:\n        List[Tuple[Any, Any]]: A list of tuples, each containing an utterance and its corresponding random partial segment.\n    \"\"\"\n    \n    # Check if utterances are loaded, if not, load them\n    if not hasattr(self, 'utterances') or self.utterances is None:\n        self._load_utterances()\n\n    # Sample utterances\n    sampled_utterances = self.utterance_cycler.sample(count)\n    \n    # Generate random partials\n    partials = []\n    for utterance in sampled_utterances:\n        random_segment = utterance.random_partial(n_frames)\n        partials.append((utterance, random_segment))\n        \n    return partials", "CodeBleu_score": 47.558655555785506}, {"code": "def random_partial(self, count, n_frames):\n    # Step 1: Ensure that utterances are loaded\n    if not hasattr(self, 'utterances') or not self.utterances:\n        self._load_utterances()\n\n    # Step 2: Sample the specified 'count' number of utterances\n    sampled_utterances = self.utterance_cycler.sample(count)\n    \n    # Step 3: For each sampled utterance, generate a tuple with the utterance and a random partial segment\n    results = []\n    for utterance in sampled_utterances:\n        random_segment = utterance.random_partial(n_frames)\n        results.append((utterance, random_segment))\n        \n    # Step 5: Return the compiled list of tuples\n    return results", "CodeBleu_score": 42.478782411066454}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "import numpy as np\n\ndef normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):\n    \"\"\"\n    Adjusts the volume of an audio waveform to match a target decibel Full Scale (dBFS).\n    Arguments:\n    - wav: np.array, representing the audio waveform\n    - target_dBFS: float, the target volume level in dBFS\n    - increase_only: bool, if True, only increases the volume, default is False\n    - decrease_only: bool, if True, only decreases the volume, default is False\n    \n    Returns:\n    - np.array, the normalized audio waveform\n    \"\"\"\n    \n    if increase_only and decrease_only:\n        raise ValueError(\"increase_only and decrease_only cannot both be True\")\n    \n    # Calculate current dBFS\n    rms = np.sqrt(np.mean(wav ** 2))\n    current_dBFS = 20 * np.log10(rms) if rms > 0 else -np.inf\n    \n    # Calculate change needed\n    delta_dBFS = target_dBFS - current_dBFS\n    \n    if (increase_only and delta_dBFS < 0) or (decrease_only and delta_dBFS > 0):\n        return wav  # Return original waveform if no adjustment is required\n\n    # Calculate scale factor for dB change\n    scale = 10 ** (delta_dBFS / 20)\n    normalized_wav = wav * scale\n    \n    return normalized_wav", "CodeBleu_score": 49.4363270630635}, {"code": "bash\npip install pydub", "CodeBleu_score": 0.0}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\nimport numpy as np\n\ndef normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):\n    \"\"\"\n    The function normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False) adjusts the volume of an audio waveform to match a target decibel Full Scale (dBFS). \n    It first checks if both increase_only and decrease_only are set, raising a ValueError if so. \n    Then, it calculates the change in dBFS needed to reach the target. \n    If the change is negative and increase_only is True, or if the change is positive and decrease_only is True, it returns the original waveform without modification. \n    Otherwise, it scales the waveform by the calculated change in dBFS.\n    \n    Args:\n    :param wav: numpy array - the input waveform\n    :param target_dBFS: float - the target dBFS level\n    :param increase_only: bool - indicates that volume should only be increased, default False\n    :param decrease_only: bool - indicates that volume should only be decreased, default False\n    \n    Returns:\n    :return: numpy array - the volume adjusted waveform.\n    \"\"\"\n\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase_only and decrease_only cannot be True at the same time.\")\n\n    # Calculate current dBFS of the waveform\n    rms = np.sqrt(np.mean(wav ** 2))\n    current_dBFS = 20 * np.log10(rms) if rms > 0 else -float('inf')\n\n    # Calculate the change in dBFS needed\n    change_in_dBFS = target_dBFS - current_dBFS\n\n    if (change_in_dBFS < 0 and increase_only) or (change_in_dBFS > 0 and decrease_only):\n        # Return the original waveform if increase_only or decrease_only conditions are met\n        return wav\n\n    # Calculate scale factor for dBFS change\n    scale_factor = 10 ** (change_in_dBFS / 20)\n    normalized_wav = wav * scale_factor\n\n    return normalized_wav\n\n# This function will adjust the volume of the given waveform to match the specified dBFS target, \n# respecting conditions where only an increase or decrease is required. \n# It operates on numpy arrays representing the waveform data, and is capable of processing any range of audio waveform data in floating point format.", "CodeBleu_score": 52.989344753212606}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "import torch\nfrom pathlib import Path\nfrom torch import nn\n\n# Assuming an encoder class exists named SpeakerEncoder\nclass SpeakerEncoder(nn.Module):\n    def __init__(self):\n        super(SpeakerEncoder, self).__init__()\n        # Assuming some layers, these should be defined according to the specific model used, e.g.,\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(64*16*16, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU()\n        )\n        \n    def forward(self, x):\n        x = self.conv_layers(x)\n        return x\n\n# Global model instance\n_model = None\n\ndef load_model(weights_fpath: Path, device=None):\n    \"\"\"\n    Load a pre-trained model for a speaker encoder. It sets the device to either a specified one or defaults to\n    CUDA if available, otherwise CPU. It initializes a global speaker encoder model (_model) with the specified device\n    and loads the model's state dictionary from a checkpoint file located at weights_fpath.\n    The model is then set to evaluation mode, and a confirmation message is printed, \n    indicating the loaded model and the training step it was trained to.\n    \"\"\"\n    global _model\n    \n    # Determine the device\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Initialize the model and send to the device\n    _model = SpeakerEncoder().to(device)\n    \n    # Load the model weights\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    \n    # Set the model to evaluation mode\n    _model.eval()\n    \n    # Extract training step from model filename assuming it is in the filename\n    steps = weights_fpath.stem.split('_')[-1]  # assuming the format includes steps at the end\n    \n    print(f\"Loaded model from '{weights_fpath}' trained to step {steps} on device {device}.\")", "CodeBleu_score": 41.50482011886475}, {"code": "import torch\nfrom pathlib import Path\nfrom torch import nn\n\n# Define a global variable to hold the model\n_model = None\n\ndef load_model(weights_fpath: Path, device=None):\n    \"\"\"The function load_model(weights_fpath: Path, device=None) is responsible \n    for loading a pre-trained model for a speaker encoder. It sets the device to \n    either a specified one or defaults to CUDA if available, otherwise CPU. It \n    initializes a global speaker encoder model (_model) with the specified device \n    and loads the model's state dictionary from a checkpoint file located at \n    weights_fpath. The model is then set to evaluation mode, and a confirmation \n    message is printed, indicating the loaded model and the training step it \n    was trained to.\"\"\"\n    global _model\n\n    # Setup the device\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Assuming _model is an instance of some nn.Module class for the speaker encoder,\n    # create and initialize this model (you need to define the model's architecture elsewhere)\n    # e.g., _model = SpeakerEncoderModel()\n    # Note: You should define SpeakerEncoderModel class based on your specific model architecture.\n    \n    _model = nn.Module()  # Placeholder, replace with actual model class\n    _model.to(device)\n    \n    # Load the weights from file into the model\n    state_dict = torch.load(weights_fpath, map_location=device)\n    _model.load_state_dict(state_dict)\n    \n    # Put model in evaluation mode\n    _model.eval()\n    \n    # Print confirmation\n    print(f\"Model loaded successfully from {weights_fpath}. Set to evaluation mode on {device}.\")", "CodeBleu_score": 40.53385524532621}, {"code": "import torch\nfrom pathlib import Path\n\n# Let's assume that the model class SpeakerEncoder is already defined or imported elsewhere\nfrom model import SpeakerEncoder  # example import, replace with actual import statement\n\n# Global model variable\n_model = None\n\ndef load_model(weights_fpath: Path, device=None):\n    \"\"\"The function load_model(weights_fpath: Path, device=None) is responsible for loading a pre-trained model for a speaker encoder.\n    It sets the device to either a specified one or defaults to CUDA if available, otherwise CPU.\n    It initializes a global speaker encoder model (_model) with the specified device and loads the model's state dictionary from a \n    checkpoint file located at weights_fpath. The model is then set to evaluation mode, and a confirmation message is printed, \n    indicating the loaded model and the training step it was trained to.\"\"\"\n    \n    global _model\n    \n    # Determining the device to use\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Load model\n    _model = SpeakerEncoder()\n    _model.to(device)\n    \n    # Load weights\n    checkpoint = torch.load(weights_fpath, map_location=device)\n    _model.load_state_dict(checkpoint['model_state_dict'])\n    \n    _model.eval()  # Set model to evaluation mode\n    \n    # Confirm model has been loaded\n    step = checkpoint.get('step', 'unknown')  # Gets the training step if available\n    print(f\"Loaded model from {weights_fpath} trained to step {step}, on device {device}\")\n\n# Example usage:\n# load_model(Path(\"path_to_weights_file.pt\"))", "CodeBleu_score": 42.140102893527335}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "import numpy as np\nimport torch\n\n# Assuming _model and _device are predefined elsewhere in the module\n# _model: pre-loaded deep learning model (e.g., a neural network for feature extraction)\n# _device: the computational device (e.g., 'cuda' or 'cpu') where the model will process the data\n\ndef embed_frames_batch(frames_batch):\n    \"\"\"The function embed_frames_batch(frames_batch) performs the task of embedding a batch of frames using a pre-loaded deep learning model. It first checks if the model (_model) is loaded, raising an exception if it is not. The function then converts the input frames_batch from a NumPy array to a PyTorch tensor and transfers it to the appropriate device (_device). It performs a forward pass through the model to compute the embeddings, detaches the result from the computation graph, moves it to the CPU, converts it back to a NumPy array, and returns the embeddings.\n\n    Args:\n        frames_batch (np.ndarray): A batch of frames represented as a numpy array.\n\n    Returns:\n        np.ndarray: An array of embeddings for the given batch of frames.\n    \"\"\"\n    if _model is None:\n        raise ValueError(\"The deep learning model is not loaded.\")\n\n    # Convert frames_batch to a PyTorch tensor and transfer to the appropriate device\n    frames_tensor = torch.from_numpy(frames_batch).to(_device)\n\n    # Ensure the tensor is in the correct format (e.g., float) if not already\n    frames_tensor = frames_tensor.float()\n\n    # Perform forward pass through the model\n    with torch.no_grad():  # Disabling gradient calculation\n        embeddings_tensor = _model(frames_tensor)\n\n    # Detach the output from the computation graph, transfer to CPU, and convert to numpy array\n    embeddings_numpy = embeddings_tensor.detach().cpu().numpy()\n\n    return embeddings_numpy", "CodeBleu_score": 56.34099782307215}, {"code": "import torch  # Importing PyTorch library\nimport numpy as np  # Importing NumPy library\n\n# Assuming _model is a preloaded PyTorch model and _device is the device (CPU/GPU) where the model is loaded\n_model = None  # Placeholder to represent the preloaded model\n_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Setup device depending on CUDA availability\n\ndef embed_frames_batch(frames_batch):\n    \"\"\"\n    Embed a batch of frames by passing them through a pre-loaded deep learning model.\n\n    Args:\n    frames_batch (np.ndarray): A batch of frames as a NumPy array.\n\n    Returns:\n    np.ndarray: The embeddings of the frames as a NumPy array.\n    \n    Raises:\n    RuntimeError: If the model has not been loaded.\n    \"\"\"\n    # Check if the model is loaded\n    if _model is None:\n        raise RuntimeError(\"The model is not currently loaded.\")\n    \n    # Convert frames_batch from a NumPy array to a PyTorch tensor\n    frames_tensor = torch.from_numpy(frames_batch).float().to(_device)  # Ensure tensor is float for model compatibility\n\n    # Ensure model is in evaluation mode\n    _model.eval()\n\n    # Perform forward pass to get embeddings\n    with torch.no_grad():  # Context-manager that disables gradient calculation (for inference efficiency)\n        embeddings = _model(frames_tensor)\n\n    # Detach embeddings from the computation graph, move to CPU and convert to NumPy\n    embeddings_numpy = embeddings.detach().cpu().numpy()\n\n    return embeddings_numpy", "CodeBleu_score": 47.08090268491099}, {"code": "import numpy as np\nimport torch\n\n# Suppose these are globally defined and/or initialized elsewhere in the script or application\n_model = None  # This should be your pretrained model, loaded elsewhere in your code\n_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef embed_frames_batch(frames_batch):\n    \"\"\"\n    Embeds a batch of frames using a pre-loaded deep learning model.\n    \n    Args:\n    frames_batch (np.ndarray): A batch of frames as a NumPy array to be embedded.\n    \n    Returns:\n    np.ndarray: Embeddings of the input frames batch as a NumPy array.\n    \n    Raises:\n    RuntimeError: If the model _model is not loaded.\n    \"\"\"\n\n    # Check if the model is loaded\n    if _model is None:\n        raise RuntimeError(\"The model is not loaded.\")\n\n    # Convert frames from NumPy array to PyTorch tensor\n    frames_tensor = torch.from_numpy(frames_batch).float()\n\n    # Transfer the tensor to the appropriate device\n    frames_tensor = frames_tensor.to(_device)\n\n    # Ensure the model is in evaluation mode\n    _model.eval()\n\n    # Perform a forward pass through the model\n    with torch.no_grad():\n        embeddings = _model(frames_tensor)\n\n    # Detach the output, move it to CPU and convert it back to a NumPy array\n    embeddings = embeddings.cpu().numpy()\n\n    return embeddings", "CodeBleu_score": 48.92500290467639}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\ndef compute_partial_slices(n_samples, partial_utterance_n_frames, min_pad_coverage=0.75, overlap=0.5):\n    \"\"\"\n    Computes partial slices for audio processing, ensuring correct overlap and\n    minimum padding coverage.\n    \n    Parameters:\n    - n_samples: Total number of samples in the full audio clip.\n    - partial_utterance_n_frames: Number of frames (samples) in each partial utterance.\n    - min_pad_coverage: Minimum padding coverage required to keep the last slice.\n    - overlap: Fraction of the frame that should overlap with the previous frame.\n\n    Returns:\n    A tuple of lists containing the start and end sample indices of each slice.\n    \"\"\"\n\n    assert 0 <= overlap < 1, \"Overlap must be between 0 and 1 (non-inclusive).\"\n    assert 0 <= min_pad_coverage <= 1, \"min_pad_coverage must be between 0 and 1.\"\n\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))\n\n    # Calculate slice start indices\n    starts = list(range(0, n_samples, frame_step))\n    \n    # Calculate slice end indices ensuring no slice goes out of bounds\n    ends = [min(start + partial_utterance_n_frames, n_samples) for start in starts]\n    \n    # Evaluate if the last slice meets minimum padding coverage criteria\n    last_slice_length = ends[-1] - starts[-1]\n    if last_slice_length / partial_utterance_n_frames < min_pad_coverage:\n        starts.pop()\n        ends.pop()\n\n    return starts, ends\n\n# Example usage:\nn_samples = 8000\npartial_utterance_n_frames = 1600\nstarts, ends = compute_partial_slices(n_samples, partial_utterance_n_frames)\nprint(\"Start indices:\", starts)\nprint(\"End indices:\", ends)", "CodeBleu_score": 28.902327342411198}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=16000, min_pad_coverage=0.75, overlap=0.5):\n    \"\"\"\n    Compute the start and end frame indices for partially overlapping audio slices.\n    \n    Args:\n        n_samples (int): Total number of frames in the audio.\n        partial_utterance_n_frames (int): Number of frames per slice.\n        min_pad_coverage (float): Minimum required coverage of the last partial slice.\n        overlap (float): Fraction of overlap between consecutive slices.\n    \n    Returns:\n        List[tuple]: A list of tuples each representing the start and end indices (inclusive) for each slice.\n    \n    Raises:\n        AssertionError: If overlap is not in the range [0, 1] or if min_pad_coverage is not in the range [0, 1].\n    \"\"\"\n    assert 0 <= overlap <= 1, \"Overlap must be between 0 and 1.\"\n    assert 0 <= min_pad_coverage <= 1, \"min_pad_coverage must be between 0 and 1.\"\n\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))  # Calculate how much to move forward after a slice\n    slices = []\n    start = 0\n    while start + partial_utterance_n_frames <= n_samples:\n        end = start + partial_utterance_n_frames - 1\n        slices.append((start, end))\n        start += frame_step\n\n    # Check coverage of the final slice\n    if start < n_samples:  # Need for an additional last slice\n        final_slice_coverage = (n_samples - start) / partial_utterance_n_frames\n        if final_slice_coverage >= min_pad_coverage:\n            slices.append((start, n_samples - 1))\n        elif slices and final_slice_coverage < min_pad_coverage:\n            # If coverage is not enough and we have added any slices, remove the last slice\n            if (n_samples - slices[-1][0]) / partial_utterance_n_frames < min_pad_coverage:\n                slices.pop()\n\n    return slices", "CodeBleu_score": 25.16328151403372}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=16000, min_pad_coverage=0.75, overlap=0.5):\n    \"\"\"\n    Computes partial slices for audio processing with specified overlap and minimum padding coverage.\n    \n    Parameters:\n        n_samples (int): Total number of samples in the audio.\n        partial_utterance_n_frames (int, optional): Number of frames in one partial utterance slice.\n        min_pad_coverage (float, optional): Minimum coverage of the last utterance to avoid extra padding.\n        overlap (float, optional): Fraction of the frame overlapped by the next frame.\n\n    Returns:\n        list of tuples: List of (start, end) indices for valid wav slices.\n        list of tuples: List of (start, end) indices for valid mel slices.\n    \"\"\"\n    \n    # Assertions to ensure overlap and min_pad_coverage are within valid range\n    assert 0 <= overlap < 1, \"Overlap must be between 0 (exclusive) and 1 (inclusive).\"\n    assert 0 <= min_pad_coverage <= 1, \"Minimum pad coverage must be between 0 and 1.\"\n\n    # Calculate the step size between beginnings of successive frames\n    frame_step = int(partial_utterance_n_frames * (1 - overlap))\n    \n    # Create lists to hold the slice indices for wav and mel\n    wav_slices = []\n    mel_slices = []\n\n    # Iterate to get all slices based on frame step\n    position = 0\n    while position + partial_utterance_n_frames <= n_samples:\n        wav_slices.append((position, position + partial_utterance_n_frames))\n        mel_slices.append((position, position + partial_utterance_n_frames))\n        position += frame_step\n\n    # Check the last segment for minimum coverage condition\n    if position < n_samples:\n        remaining_samples = n_samples - position\n        if remaining_samples / partial_utterance_n_frames >= min_pad_coverage:\n            wav_slices.append((position, n_samples))\n            mel_slices.append((position, n_samples))\n        elif len(wav_slices) > 0 and len(mel_slices) > 0:\n            # Remove the last slice if not enough coverage and it is not the only one\n            if remaining_samples / partial_utterance_n_frames < min_pad_coverage:\n                wav_slices.pop()\n                mel_slices.pop()\n\n    return wav_slices, mel_slices", "CodeBleu_score": 27.191616159609477}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30)):\n    \"\"\"\n    Visualizes a given embedding as a heatmap using Matplotlib.\n    \n    Parameters:\n        embed (np.array): The embedding array to be visualized.\n        ax (matplotlib.axes.Axes, optional): The matplotlib Axes object on which to draw the heatmap. Defaults to None.\n        title (str, optional): The title of the plot. Defaults to an empty string.\n        shape (tuple, optional): The shape (height, width) to reshape the embedding into. If None, the shape is assumed to be a square that fits the data. Defaults to None.\n        color_range (tuple, optional): The minimum and maximum values for the colormap. Defaults to (0, 0.30).\n        \n    Returns:\n        matplotlib.axes.Axes: The Axes object for the heatmap.\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    if shape is None:\n        side_length = int(np.sqrt(len(embed)))\n        shape = (side_length, side_length)\n    \n    if len(embed) != np.prod(shape):\n        raise ValueError(\"The provided shape does not match the length of the embedding.\")\n\n    reshaped_embedding = embed.reshape(shape)\n    \n    heatmap = ax.imshow(reshaped_embedding, aspect='auto', cmap='viridis', vmin=color_range[0], vmax=color_range[1])\n    \n    ax.figure.colorbar(heatmap, ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n\n    return ax\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create an example embedding\n    embed = np.random.rand(100)\n    fig, ax = plt.subplots()\n    plot_embedding_as_heatmap(embed, ax=ax, title=\"Embedding Heatmap\")\n    plt.show()", "CodeBleu_score": 54.17307116331259}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30)):\n    \"\"\"\n    The function plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30)) visualizes a given\n    embedding as a heatmap using Matplotlib. It first checks if an Axes object (ax) is provided; if not, it uses the current\n    Axes. If the shape of the embedding is not specified, it calculates the height by taking the square root of the\n    embedding's length and reshapes the embedding accordingly. The function then plots the reshaped embedding as a heatmap\n    using a colormap (cmap). A color bar is added to the plot for reference, with its limits set by the color_range parameter.\n    The x and y ticks are removed for a cleaner visualization, and the specified title is set for the plot.\n    \"\"\"\n    \n    # Check if an axis object is provided, if not get the current axis\n    if ax is None:\n        ax = plt.gca()\n    \n    # Calculate the shape of the embedding if not provided\n    if shape is None:\n        embed_len = len(embed)\n        side_length = int(np.sqrt(embed_len))\n        if side_length * side_length != embed_len:\n            raise ValueError(\"Embedding size must be a perfect square if shape is not provided\")\n        shape = (side_length, side_length)\n    \n    # Reshape the embedding\n    reshaped_embed = np.reshape(embed, shape)\n    \n    # Creating the heatmap\n    heatmap = ax.imshow(reshaped_embed, cmap='viridis', aspect='auto', vmin=color_range[0], vmax=color_range[1])\n    \n    # Remove x and y ticks for a cleaner plot\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    # Set title of the plot\n    ax.set_title(title)\n    \n    # Add colorbar\n    plt.colorbar(heatmap, ax=ax)\n    \n    # Show the plot if no ax is passed\n    if ax is None:\n        plt.show()\n\n# Example usage\nif __name__ == \"__main__\":\n    embedding = np.random.rand(100)\n    plot_embedding_as_heatmap(embedding, title=\"Heatmap of Embedding\")", "CodeBleu_score": 50.26906830169364}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30)):\n    \"\"\"\n    Visualizes a given embedding as a heatmap using Matplotlib.\n    \n    Args:\n        embed (np.ndarray): The embedding array to visualize.\n        ax (matplotlib.axes.Axes, optional): Matplotlib Axes object where the heatmap is plotted. If None, uses the current axes.\n        title (str): Title of the heatmap.\n        shape (tuple, optional): Shape (height, width) to reshape the embedding. If None, assumes a square shape.\n        color_range (tuple): Tuple (min, max) representing the color range for normalization.\n    \n    Raises:\n        ValueError: If `embed` cannot be reshaped into the given `shape` or a square shape of equal dimensions.\n    \"\"\"\n    # If no Axes object is provided, use the current Axes\n    if ax is None:\n        fig, ax = plt.subplots()\n    \n    # If no shape is specified, assume the embedding is to be reshaped into a square\n    if shape is None:\n        length = len(embed)\n        height = int(np.sqrt(length))\n        if height * height != length:\n            raise ValueError(\"Embedding length is not a perfect square and no shape provided.\")\n        shape = (height, height)\n    \n    # Reshape the embedding\n    reshaped_embedding = embed.reshape(shape)\n    \n    # Create the heatmap\n    cax = ax.imshow(reshaped_embedding, aspect='equal', cmap='hot', vmin=color_range[0], vmax=color_range[1])\n    \n    # Add a color bar\n    fig.colorbar(cax, ax=ax)\n    \n    # Remove x and y ticks\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    # Set title\n    ax.set_title(title)\n    \n    # Show plot if plotting implicitly\n    if ax is None:\n        plt.show()", "CodeBleu_score": 45.232447780789435}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "import torch\n\ndef similarity_matrix(self, embeds):\n    \"\"\"\n    Computes a similarity matrix for speaker embeddings.\n    \n    Parameters:\n    - embeds: torch.Tensor, shape (speakers, utterances, features)\n    \n    Returns: torch.Tensor\n        Similarity matrix scaled and biased accordingly.\n    \"\"\"\n    num_speakers, num_utterances, _ = embeds.shape\n    \n    # Computing centroids including each utterance\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)  # Mean across utterances\n        \n    # Computing centroids excluding each utterance (leave-one-out)\n    expand_dims = embeds.unsqueeze(1)  # Expanded dims for broadcasting\n    sum_others = torch.sum(expand_dims, dim=2) - expand_dims\n    counts_others = num_utterances - 1\n    centroids_excl = sum_others / counts_others\n    \n    # Normalize the centroids\n    centroids_incl = centroids_incl / torch.norm(centroids_incl, dim=2, keepdim=True)\n    centroids_excl = centroids_excl / torch.norm(centroids_excl, dim=3, keepdim=True)\n\n    # Initialize similarity matrix\n    similarity_matrix = torch.zeros(num_speakers, num_utterances, num_speakers, num_utterances)\n\n    # Calculate similarities\n    for i in range(num_speakers):\n        for j in range(num_utterances):\n            v = embeds[i, j]  # Current utterance embed\n            # Including current\n            sim_incl = torch.matmul(centroids_incl, v.unsqueeze(-1)).squeeze(-1)\n            # Excluding current\n            sim_excl = torch.matmul(centroids_excl[i], v.unsqueeze(-1)).squeeze(-1)\n\n            # Fill in the matrix without the same speaker same utterance comparisons\n            sim = sim_excl\n            sim[i, j] = sim_incl[i, j]\n            similarity_matrix[i, j] = sim\n\n    # Assuming some form of scaling and biasing is needed\n    similarity_matrix = 0.5 * (similarity_matrix + 1)  # Example transformation\n\n    return similarity_matrix", "CodeBleu_score": 36.23044341290832}, {"code": "import torch\n\ndef similarity_matrix(self, embeds):\n    \"\"\"\n    Calculates a similarity matrix for speaker embeddings.\n\n    Args:\n    embeds (torch.Tensor): Tensor of shape [speakers, utterances, features].\n\n    Returns:\n    torch.Tensor: Scaled and biased similarity matrix of shape [speakers * utterances, speakers * utterances].\n    \"\"\"\n\n    speakers, utterances, features = embeds.shape\n    embeds = embeds.view(speakers * utterances, features)\n    \n    # Centroids including the current embedding\n    centroids_incl = embeds.view(speakers, utterances, features).mean(dim=1)\n    centroids_incl = centroids_incl / centroids_incl.norm(dim=1, keepdim=True)\n    \n    # Expand centroids for easier computation\n    centroids_incl_exp = centroids_incl.unsqueeze(1).expand(-1, utterances, -1).reshape(speakers * utterances, features)\n    \n    # Centroids excluding the current embedding\n    sum_embeddings = embeds.view(speakers, utterances, features).sum(dim=1, keepdim=True)\n    centroids_excl = (sum_embeddings - embeds.view(speakers, utterances, features)) / (utterances - 1)\n    centroids_excl = centroids_excl / centroids_excl.norm(dim=2, keepdim=True)\n    centroids_excl = centroids_excl.reshape(speakers * utterances, features)\n    \n    # Initialize similarity matrix\n    sim_matrix = torch.mm(embeds, centroids_incl_exp.t())\n    \n    # Initialize mask to exclude self-similarity\n    mask = torch.eye(speakers * utterances)\n    sim_matrix.masked_fill_(mask.bool(), 0)\n    \n    # Scale and bias the similarity matrix\n    sim_matrix = 10 * sim_matrix + 0.5\n\n    return sim_matrix", "CodeBleu_score": 34.20816666045643}, {"code": "import torch\n\ndef similarity_matrix(self, embeds):\n    \"\"\"\n    Calculates a similarity matrix for speaker embeddings using centroids.\n    \n    Args:\n    embeds (torch.Tensor): Tensor of shape (num_speakers, num_utterances_per_speaker, embed_dim)\n                           containing the embeddings per speaker.\n    \n    Returns:\n    torch.Tensor: A similarity matrix of shape (num_speakers, num_utterances_per_speaker, num_speakers, num_utterances_per_speaker).\n    \"\"\"\n    num_speakers, num_utterances_per_speaker, embed_dim = embeds.size()\n\n    # Initialize centroids and similarity matrix\n    centroids_incl = embeds.mean(dim=1, keepdim=True)  # shape: (num_speakers, 1, embed_dim)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds) / (num_utterances_per_speaker - 1)\n    similarity_matrix = torch.zeros((num_speakers, num_utterances_per_speaker, num_speakers, num_utterances_per_speaker),\n                                    device=embeds.device)\n\n    # Compute similarities and update the matrix\n    for i in range(num_speakers):\n        for j in range(num_utterances_per_speaker):\n            embedding_ij = embeds[i, j]\n            for k in range(num_speakers):\n                for l in range(num_utterances_per_speaker):\n                    if i == k and j == l:\n                        continue  # Skip self-similarity\n                    centroid_kl_excl = centroids_excl[k, 0] if (i != k or j != l) else centroids_incl[k, 0]\n                    similarity_matrix[i, j, k, l] = torch.nn.functional.cosine_similarity(embedding_ij, centroid_kl_excl, dim=0)\n\n    # Scale and bias the similarity matrix\n    scaled_similarity_matrix = 0.5 * (similarity_matrix + 1)  # scaling to make values positive\n\n    return scaled_similarity_matrix", "CodeBleu_score": 34.27868370988201}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "import torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_curve\n\ndef loss(self, embeds):\n    # Calculate shape values\n    num_speakers = 2 # Example: you might determine this from the problem setup\n    num_utterances_per_speaker = embeds.shape[0] // num_speakers\n    \n    # Similarity matrix computation\n    # A simple way is cosine similarity matrix\n    similarity_matrix = torch.mm(embeds, embeds.t())\n    \n    # Label matrix creation\n    labels = torch.zeros_like(similarity_matrix)\n    for i in range(num_speakers):\n        start_idx = i * num_utterances_per_speaker\n        end_idx = start_idx + num_utterances_per_speaker\n        labels[start_idx:end_idx, start_idx:end_idx] = 1\n\n    # Loss calculation\n    # We will use binary cross-entropy loss here\n    loss = F.binary_cross_entropy_with_logits(\n        input=similarity_matrix,\n        target=labels,\n        reduction='mean'\n    )\n\n    # Compute EER\n    # Flatten matrices and compute ROC curve, then find EER\n    sims = similarity_matrix.flatten()\n    lbls = labels.flatten()\n    \n    fpr, tpr, thresholds = roc_curve(lbls.detach().cpu().numpy(),\n                                     sims.detach().cpu().numpy())\n    eer_threshold = thresholds[(fpr - (1 - tpr)).argmin()]\n    eer = fpr[(fpr - (1 - tpr)).argmin()]\n\n    return loss, eer", "CodeBleu_score": 38.424058684650724}, {"code": "import torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_curve\nimport numpy as np\n\nclass SpeakerEmbeddingModel:\n\n    def pairwise_similarity(self, embeddings):\n        \"\"\" Compute pairwise cosine similarity between each pair of embeddings. \"\"\"\n        normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n        similarity_matrix = torch.mm(normalized_embeddings, normalized_embeddings.t())\n        return similarity_matrix\n\n    def generate_labels(self, num_speakers, utterances_per_speaker):\n        \"\"\" Generate a 2D binary matrix of labels for similarity. 1 if same speaker, 0 otherwise. \"\"\"\n        labels = torch.zeros(utterances_per_speaker * num_speakers, utterances_per_speaker * num_speakers)\n        for i in range(num_speakers):\n            start = i * utterances_per_speaker\n            end = start + utterances_per_speaker\n            labels[start:end, start:end] = 1\n        return labels\n\n    def calculate_loss(self, similarities, labels):\n        \"\"\" Calculate contrastive loss or another type of loss suitable for the problem. \"\"\"\n        # Here using binary cross-entropy for simplicity, modify according to actual requirement\n        # Flatten both the matrices to apply BCE loss\n        loss = F.binary_cross_entropy_with_logits(similarities, labels)\n        return loss\n\n    def compute_eer(self, scores, labels):\n        \"\"\" Compute the Equal Error Rate (EER). \"\"\"\n        fpr, tpr, thresholds = roc_curve(labels.reshape(-1), scores.reshape(-1))\n        eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n        return eer\n\n    def loss(self, embeds):\n        num_speakers = 10  # assuming fixed or dynamically determined earlier\n        utterances_per_speaker = embeds.shape[0] // num_speakers\n        \n        # Step 2: Compute the similarity matrix\n        similarity_matrix = self.pairwise_similarity(embeds)\n        \n        # Step 3: Generate ground truth labels for the similarity matrix\n        true_labels = self.generate_labels(num_speakers, utterances_per_speaker)\n        \n        # Step 4: Calculate the loss\n        loss_value = self.calculate_loss(similarity_matrix, true_labels)\n\n        # Step 5: Detach similarity matrix for EER calculation to avoid gradient computation\n        similarity_matrix_np = similarity_matrix.detach().cpu().numpy()\n\n        # Convert ground truth to numpy for computing EER\n        true_labels_np = true_labels.numpy()\n\n        # Calculate EER (Equal Error Rate)\n        eer = self.compute_eer(similarity_matrix_np, true_labels_np)\n\n        return loss_value, eer", "CodeBleu_score": 38.68981108327495}, {"code": "import torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_curve\nimport numpy as np\n\ndef loss(self, embeds):\n    \"\"\"\n    Calculates the loss and the Equal Error Rate (EER) for a batch of speaker embeddings.\n\n    Parameters:\n    - embeds (torch.Tensor): Tensor of shape (num_speakers * num_utterances_per_speaker, embed_dim)\n                             where `num_speakers` is the number of different speakers,\n                             and `num_utterances_per_speaker` is the number of utterances per speaker.\n\n    Returns:\n    - tuple(float, float): Containing (loss, EER) where loss is the contrastive or triplet loss,\n                           and EER is the Equal Error Rate.\n    \"\"\"\n    num_speakers, num_utterances_per_speaker, embed_dim = embeds.size(0), 1, embeds.size(1)\n    \n    # Calculate pairwise cosine similarity. More robust to scale than euclidean distance for embeddings\n    similarity_matrix = torch.mm(embeds, embeds.t())\n    \n    # Create ground truth labels. Diagonal blocks of size num_utterances_per_speaker x num_utterances_per_speaker are positive samples.\n    labels = torch.eye(num_speakers).repeat_interleave(num_utterances_per_speaker, dim=0)\n    labels = labels.repeat_interleave(num_utterances_per_speaker, dim=1)\n    \n    # Create a mask to ignore self-similarity (diagonal elements)\n    mask = torch.eye(num_speakers * num_utterances_per_speaker, dtype=torch.bool)\n    \n    # Calculate the loss as below:\n    # Positive similarity - Negative similarity (hard negatives, easy positives etc. can be taken based on the requirement)\n    margin_loss = F.margin_ranking_loss(similarity_matrix[mask == False], labels[mask == False], torch.ones(labels.size()[0]).to(embeds.device))\n    \n    # Compute EER:\n    # Flatten out the scores and labels \n    scores = similarity_matrix[mask == False].detach().cpu().numpy()\n    true_labels = labels[mask == False].cpu().numpy()\n    \n    # Calculate false positive rate (fpr), true positive rate (tpr), thresholds\n    fpr, tpr, thresholds = roc_curve(true_labels, scores)\n    \n    # EER: Find the threshold where the difference between FPR and 1-TPR (or TPR and FPR) is minimized\n    eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n    \n    return margin_loss.item(), eer", "CodeBleu_score": 38.93099403061137}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "class MyClass:\n    def __init__(self, params, display_params=True):\n        self.params = params\n        self.display_params = display_params\n\n    def log_params(self):\n        \"\"\"\n        The function logs model and data parameters if not disabled.\n        It imports parameter configurations, then iterates through the model and data parameters,\n        appending each name and value to a formatted string.\n        This string is displayed using a visualization tool, with the title 'Parameters'.\n        \"\"\"\n        if not self.display_params:\n            return\n\n        # Assuming 'self.params' is a dictionary containing all necessary parameters\n        # Typically, self.params could be {\"model\": model_params_dict, \"data\": data_params_dict}\n        \n        parameter_info = \"Parameters:\\n\"\n        \n        for category in self.params:\n            parameter_info += f\"\\nCategory: {category}\\n\"\n            for key, value in self.params[category].items():\n                parameter_info += f\"{key}: {value}\\n\"\n        \n        self.display_parameter_info(parameter_info)\n    \n    def display_parameter_info(self, parameter_info):\n        print(parameter_info)  # Here the visualization tool is simply the print function, but it can be replaced with any logging or visualization library\n\n# Example usage\nparams_example = {\n    \"model\": {\"learning_rate\": 0.001, \"epochs\": 50},\n    \"data\": {\"batch_size\": 32, \"dataset\": \"ImageNet\"}\n}\nmy_class_instance = MyClass(params_example)\nmy_class_instance.log_params()", "CodeBleu_score": 24.71893763640255}, {"code": "class Model:\n    def __init__(self, parameters, data_parameters):\n        self.parameters = parameters\n        self.data_parameters = data_parameters\n\n    def log_params(self):\n        \"\"\"\n        The function logs model and data parameters if not disabled.\n        It imports parameter configurations, then iterates through the model and data parameters, appending each name and value to a formatted string.\n        This string is displayed using a visualization tool, with the title \"Parameters\".\n        \"\"\"\n        # Assuming parameter configurations are already set up in __init__\n        \n        # Collect all parameters into a formatted string\n        params_str = \"Model Parameters:\\n\"\n        for name, value in self.parameters.items():\n            params_str += f\"{name}: {value}\\n\"\n        \n        params_str += \"Data Parameters:\\n\"\n        for name, value in self.data_parameters.items():\n            params_str += f\"{name}: {value}\\n\"\n        \n        # Display the formatted string\n        self.display_parameters(params_str)\n\n    def display_parameters(self, parameter_info):\n        # Simulation of using a visualization tool to display parameters\n        print(\"Parameters:\")\n        print(parameter_info)\n\n# Example of using the class\nmodel_parameters = {'learning_rate': 0.01, 'num_epochs': 100}\ndata_parameters = {'batch_size': 32, 'dataset': 'ImageNet'}\n\nmodel = Model(model_parameters, data_parameters)\nmodel.log_params()", "CodeBleu_score": 31.782175576400107}, {"code": "class ModelLogger:\n    def __init__(self, model_params, data_params, logging_enabled=True):\n        self.model_params = model_params\n        self.data_params = data_params\n        self.logging_enabled = logging_enabled\n\n    def log_params(self):\n        \"\"\"The function logs model and data parameters if not disabled.\n        It imports parameter configurations, then iterates through the model and data parameters,\n        appending each name and value to a formatted string.\n        This string is displayed using a visualization tool, with the title \"Parameters\".\"\"\"\n        if not self.logging_enabled:\n            return\n\n        param_str = \"Parameters:\\n\"\n        # Iterating through model parameters\n        param_str += \"Model Parameters:\\n\"\n        for key, value in self.model_params.items():\n            param_str += f\"  {key}: {value}\\n\"\n\n        # Iterating through data parameters\n        param_str += \"Data Parameters:\\n\"\n        for key, value in self.data_params.items():\n            param_str += f\"  {key}: {value}\\n\"\n\n        self.display_params(param_str)\n\n    def display_params(self, param_str):\n        \"\"\" A simple function that prints the parameter string but could be replaced with more sophisticated visualization tools \"\"\"\n        print(param_str)", "CodeBleu_score": 24.944923837648695}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom umap import UMAP\nimport seaborn as sns\n\ndef draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10):\n    \"\"\"\n    The function visualizes speaker embeddings using UMAP projections.\n    It limits the number of speakers, flattens the embeddings, and assigns colors based on speaker identity.\n    UMAP reduces the dimensionality of the embeddings, which are then plotted and displayed or saved to a file if specified.\n    The plot title includes the current step, and the visualization is updated unless disabled.\n    \n    Args:\n    embeds (List[np.ndarray]): List of embeddings where each element corresponds to a speaker and contains that speaker's embeddings.\n    utterances_per_speaker (int): Number of utterances per speaker, assumes equal for all speakers initially.\n    step (int): Current step or epoch number, to label the plot.\n    out_fpath (str, optional): Path to save the plot image file. If None, the plot is shown instead.\n    max_speakers (int, optional): Maximum number of speakers to plot for better visual clarity.\n    \"\"\"\n    \n    # Limit the number of speakers\n    embeds = embeds[:max_speakers]\n    \n    # Flatten the embeddings list and create labels for each point\n    all_embeddings = np.vstack(embeds)\n    speaker_labels = np.concatenate([[i] * utterances_per_speaker for i in range(len(embeds))])\n    \n    # Utilize UMAP to reduce dimensions of embeddings\n    umap = UMAP(n_components=2, random_state=42)\n    projections = umap.fit_transform(all_embeddings)\n    \n    # Generate a color palette\n    colors = sns.color_palette(\"hsv\", len(embeds))\n    \n    plt.figure(figsize=(10, 8))\n    for i in range(len(embeds)):\n        indices = speaker_labels == i\n        plt.scatter(projections[indices, 0], projections[indices, 1], label=f\"Speaker {i}\", s=50, color=colors[i])\n    \n    plt.title(f\"UMAP Projection of Speaker Embeddings at Step {step}\")\n    plt.xlabel(\"UMAP 1\")\n    plt.ylabel(\"UMAP 2\")\n    plt.legend(title=\"Speakers\")\n    plt.grid(True)\n    \n    # Save or show plot\n    if out_fpath:\n        plt.savefig(out_fpath)\n        plt.close()\n    else:\n        plt.show()", "CodeBleu_score": 39.9685629093219}, {"code": "import numpy as np\nimport umap\nimport matplotlib.pyplot as plt\n\ndef draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10):\n    \"\"\"\n    The function visualizes speaker embeddings using UMAP projections.\n    It limits the number of speakers, flattens the embeddings, and assigns colors based on speaker identity.\n    UMAP reduces the dimensionality of the embeddings, which are then plotted and displayed or saved to a file if specified.\n    The plot title includes the current step, and the visualization is updated unless disabled.\n    \"\"\"\n    \n    # Limit the number of speakers to either the maximum or total available, whichever is smaller\n    num_speakers = min(max_speakers, len(embeds))\n    \n    # Flatten the embeddings and correspondingly adjust the labels\n    flattened_embeds = []\n    labels = []\n    for i, speaker_embeds in enumerate(embeds[:num_speakers]):\n        for embed in speaker_embeds:\n            flattened_embeds.append(embed)\n            labels.append(i)\n    \n    # Convert the embeddings to numpy array for UMAP processing\n    flattened_embeds = np.array(flattened_embeds)\n    \n    # Reduce dimensions using UMAP\n    reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, metric='correlation')\n    embedding_2d = reducer.fit_transform(flattened_embeds)\n    \n    # Plot the embeddings\n    plt.figure(figsize=(10, 10))\n    scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=labels, cmap='tab20', s=5)\n    plt.colorbar(scatter)\n\n    # Set plot title\n    plt.title(f'Speaker Embedding Visualizations at Step {step}')\n\n    # Check if need to save the plot to a file\n    if out_fpath:\n        plt.savefig(out_fpath)\n        plt.close()  # Close the plotting figure to free memory\n    else:\n        plt.show()", "CodeBleu_score": 42.154909466472404}, {"code": "bash\npip install matplotlib numpy umap-learn", "CodeBleu_score": 0.26041666666666663}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "import torch\n\ndef add_speaker_embedding(self, x, speaker_embedding):\n    \"\"\"The function adds a speaker embedding to an input tensor.\n    It first saves the dimensions of the input tensor as human-readable names and determines the appropriate dimension index for the speaker embedding.\n    It then repeats the speaker embedding to match the length of the input text, reshapes, and transposes it.\n    Finally, it concatenates the speaker embedding with the input tensor along the last dimension and returns the result.\n\n    Parameters:\n        x (torch.Tensor): Tensor of input features with shape [batch_size, sequence_length, feature_dim].\n        speaker_embedding (torch.Tensor): Tensor of speaker embeddings with shape [batch_size, embedding_dim].\n\n    Returns:\n        torch.Tensor: The concatenated tensor with shape [batch_size, sequence_length, feature_dim + embedding_dim].\n    \"\"\"\n    batch_size, sequence_length, feature_dim = x.size()  # Get the tensor dimensions of the input tensor\n    embedding_dim = speaker_embedding.size(1)           # Get the dimension of the speaker embedding\n\n    # Repeat speaker embedding along the sequence length dimension\n    speaker_embedding = speaker_embedding.unsqueeze(1)                # Add sequence length dimension\n    speaker_embedding = speaker_embedding.expand(-1, sequence_length, -1)  # Repeat the embeddings\n\n    # Concatenate along the last dimension (feature dimensions)\n    x = torch.cat([x, speaker_embedding], dim=2)  # Concatenating the tensors\n\n    return x", "CodeBleu_score": 29.815054641682238}, {"code": "import torch\n\ndef add_speaker_embedding(self, x, speaker_embedding):\n    \"\"\"\n    The function adds a speaker embedding to an input tensor.\n    It first saves the dimensions of the input tensor as human-readable names and determines the appropriate dimension index for the speaker embedding.\n    It then repeats the speaker embedding to match the length of the input text, reshapes, and transposes it.\n    Finally, it concatenates the speaker embedding with the input tensor along the last dimension and returns the result.\n\n    Args:\n        x (Tensor): The input tensor with shape [batch_size, sequence_length, feature_dim].\n        speaker_embedding (Tensor): The speaker embedding to be added with shape [batch_size, embedding_dim].\n\n    Returns:\n        Tensor: The updated tensor after adding speaker embedding.\n    \"\"\"\n    \n    # Derive dimensions of input tensor\n    batch_size, sequence_length, feature_dim = x.shape\n    \n    # Repeat speaker_embedding to match sequence length and reshape for concatenation\n    speaker_embedding_expanded = speaker_embedding.unsqueeze(1).repeat(1, sequence_length, 1)\n    \n    # Concatenate along the feature dimension\n    x_with_speaker = torch.cat([x, speaker_embedding_expanded], dim=2)\n    \n    return x_with_speaker", "CodeBleu_score": 22.936922385938445}, {"code": "import torch  # Use torch.cuda.FloatTensor for GPU support\n\ndef add_speaker_embedding(self, x, speaker_embedding):\n    \"\"\"\n    The function adds a speaker embedding to an input tensor.\n    It first saves the dimensions of the input tensor as human-readable names and determines the appropriate dimension index for the speaker embedding.\n    It then repeats the speaker embedding to match the length of the input text, reshapes, and transposes it.\n    Finally, it concatenates the speaker embedding with the input tensor along the last dimension and returns the result.\n\n    Args:\n    x (tensor): The input tensor (e.g., a batch of sequences with shape [batch_size, seq_len, feature_dim]).\n    speaker_embedding (tensor): The speaker embedding vector (e.g., with shape [embedding_dim]).\n\n    Returns:\n    tensor: The concatenated tensor with speaker embeddings added.\n    \"\"\"\n\n    # Extract dimensions\n    batch_size, seq_len, feature_dim = x.size()\n\n    # Broadcasting the speaker_embedding to match the input tensor's dimensions\n    # We first unsqueeze to add necessary dimensions\n    speaker_embedding = speaker_embedding.unsqueeze(0).unsqueeze(0)  # shape now [1, 1, embedding_dim]\n\n    # Repeat the speaker_embedding across the batch and sequence length\n    speaker_embedding = speaker_embedding.expand(batch_size, seq_len, -1)  # shape [batch_size, seq_len, embedding_dim]\n\n    # Concatenate the speaker_embedding to each entry in the input tensor\n    output = torch.cat((x, speaker_embedding), dim=2)  # shape [batch_size, seq_len, feature_dim + embedding_dim]\n\n    return output", "CodeBleu_score": 31.12657210113557}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj):\n    \"\"\"\n    Initializes attention-related tensors for the model.\n    It first determines the device from the model's parameters and the shape of the input sequence projection.\n    Then, it creates and zeroes cumulative and attention tensors with the same batch size and sequence length\n    as the input, ensuring they are on the correct device.\n\n    Args:\n    encoder_seq_proj (torch.Tensor): A tensor with the encoder outputs projected for attention mechanisms.\n\n    Returns:\n    None, but initializes internal state for attention processing.\n    \"\"\"\n    # Determine the device to use from the encoder sequence projection tensor\n    device = encoder_seq_proj.device\n    \n    # Get the shape for the attention tensors\n    batch_size, seq_length, _ = encoder_seq_proj.shape\n    \n    # Create and zero-initialize the cumulative attention tensor\n    self.cumulative_attention = torch.zeros(batch_size, seq_length, device=device)\n    \n    # Create and zero-initialize the attention weights tensor\n    self.attention_weights = torch.zeros(batch_size, seq_length, device=device)", "CodeBleu_score": 36.23637669349974}, {"code": "def init_attention(self, encoder_seq_proj):\n    \"\"\"\n    The function initializes attention-related tensors for the model.\n    It first determines the device from the model's parameters and the shape of the input sequence projection.\n    Then, it creates and zeroes cumulative and attention tensors with the same batch size and sequence length as the input, ensuring they are on the correct device.\n\n    Parameters:\n        encoder_seq_proj (torch.Tensor): The projected encoder sequence tensor.\n\n    Returns:\n        None: This function modifies attributes of the class instance directly.\n    \"\"\"\n\n    # Determine the device that the model parameters are on\n    device = next(self.parameters()).device\n\n    # Get the batch size and sequence length of the encoder sequence projection\n    batch_size, seq_length, _ = encoder_seq_proj.size()\n\n    # Initialize cumulative attention weights tensor and set to zero\n    self.cumulative_attention_weights = torch.zeros(batch_size, seq_length, device=device)\n\n    # Initialize attention weights tensor and set to zero\n    self.attention_weights = torch.zeros(batch_size, seq_length, device=device)", "CodeBleu_score": 70.27570866849176}, {"code": "import torch\n\nclass AttentionModel:\n    def __init__(self):\n        # Initialize other necessary components related to the attention model here\n        pass\n    \n\n    def init_attention(self, encoder_seq_proj):\n        \"\"\"\n        The function initializes attention-related tensors for the model.\n        It first determines the device from the model's parameters and the shape of the\n        input sequence projection.\n        Then, it creates and zeroes cumulative and attention tensors with the same batch\n        size and sequence length as the input, ensuring they are on the correct device.\n        \n        Args:\n        encoder_seq_proj (torch.Tensor): The projected encoder sequences of size \n                                          (batch_size, seq_len, num_features)\n        \"\"\"\n        # Step 1: Determine the device where the encoder_seq_proj is\n        device = encoder_seq_proj.device\n        \n        # Step 2: Determine the shape of the input projection tensor\n        batch_size, seq_len, _ = encoder_seq_proj.shape\n\n        # Step 3: Create and zero-initialize the cumulative and attention tensors\n        self.cumulative_attn = torch.zeros(batch_size, seq_len, dtype=torch.float32, device=device)\n        self.attn_weights = torch.zeros(batch_size, seq_len, dtype=torch.float32, device=device)\n\n# Example usage\n# model = AttentionModel()\n# # Assuming encoder_seq_proj is a tensor of size (batch_size, seq_len, num_features)\n# model.init_attention(encoder_seq_proj=torch.randn(10, 15, 256))", "CodeBleu_score": 30.407377754721487}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "import torch\n\ndef load(self, path, optimizer=None):\n    \"\"\"\n    The function loads a model checkpoint from a given path.\n    It sets the device to the model's parameter device and loads the checkpoint.\n    The model's state dictionary is updated with the loaded state.\n    If the checkpoint contains an optimizer state and an optimizer is provided,\n    the optimizer's state dictionary is also updated.\n\n    Parameters:\n        path (str): Path to the model checkpoint.\n        optimizer (torch.optim.Optimizer, optional): The optimizer to restore state for.\n\n    Returns:\n        None\n    \"\"\"\n    # Load the checkpoint.\n    checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n\n    # Check if the checkpoint includes the model state dictionary.\n    if 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n    else:\n        raise KeyError(\"Checkpoint does not contain 'state_dict'\")\n    \n    # Set device based on the model's first parameter.\n    device = next(self.parameters()).device\n    state_dict = {k: v.to(device) for k, v in state_dict.items()}\n\n    # Load the checkpoint parameters into the model.\n    self.load_state_dict(state_dict)\n    \n    # If an optimizer is provided and 'optimizer_state' is in checkpoint, load optimizer state.\n    if optimizer and 'optimizer_state' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state'])", "CodeBleu_score": 51.454003681259664}, {"code": "import torch\n\ndef load(self, path, optimizer=None):\n    \"\"\"\n    The function loads a model checkpoint from a given path.\n    It sets the device to the model's parameter device and loads the checkpoint.\n    The model's state dictionary is updated with the loaded state.\n    If the checkpoint contains an optimizer state and an optimizer is provided,\n    the optimizer's state dictionary is also updated.\n    \"\"\"\n    # Load the checkpoint\n    checkpoint = torch.load(path)\n    \n    # Determine the device from the model's parameters\n    # If the model has parameters, we use the device of the first parameter\n    device = next(self.parameters()).device\n    \n    # Update the model's state dictionary\n    # Assume the state dictionary is saved with key 'model_state_dict'\n    if 'model_state_dict' in checkpoint:\n        self.load_state_dict(checkpoint['model_state_dict'])\n    \n    # If there is an optimizer and checkpoint includes optimizer state, load it\n    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    # Move the model to the appropriate device before returning\n    self.to(device)", "CodeBleu_score": 55.39831585805843}, {"code": "import torch\n\ndef load(self, path, optimizer=None):\n    \"\"\"\n    The function loads a model checkpoint from a given path.\n    It sets the device to the model's parameter device and loads the checkpoint.\n    The model's state dictionary is updated with the loaded state.\n    If the checkpoint contains an optimizer state and an optimizer is provided, the optimizer's state dictionary is also updated.\n    \n    Args:\n        path (str): The path to the checkpoint file.\n        optimizer (torch.optim.Optimizer, optional): The optimizer to load state into. Default is None.\n\n    Returns:\n        None\n    \"\"\"\n    # Check if CUDA is available and set the appropriate device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load the checkpoint from the given path\n    checkpoint = torch.load(path, map_location=device)\n    \n    # Load model state dictionary from checkpoint\n    if 'model_state_dict' in checkpoint:\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n    else:\n        raise KeyError(\"No model_state_dict found in checkpoint\")\n    \n    # Load optimizer state dictionary from checkpoint if optimizer is provided\n    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])", "CodeBleu_score": 50.04970970558947}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "import torch\n\nclass Model:\n    def __init__(self, model):\n        self.model = model\n\n    def save(self, path, optimizer=None):\n        \"\"\"\n        The function saves the model state to a specified path.\n        If an optimizer is provided, it saves both the model state and the optimizer state.\n        If no optimizer is provided, it saves only the model state.\n\n        Args:\n        path (str): The file path or file name where the state will be saved.\n        optimizer (optional, Optimizer): An instance of an optimizer (e.g., from torch.optim).\n                                        If provided, its state will also be saved.\n        \"\"\"\n        # Create a dictionary to hold any states we wish to save\n        state = {'model_state': self.model.state_dict()}\n        \n        # If an optimizer has been provided, add its state to the dictionary\n        if optimizer is not None:\n            state['optimizer_state'] = optimizer.state_dict()\n\n        # Use Pytorch to save the state dictionary to the specified file path\n        torch.save(state, path)\n\n# Example usage in Pytorch\nif __name__ == \"__main__\":\n    import torch.nn as nn\n    import torch.optim as optim\n\n    # Assume some kind of simple model\n    model = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2))\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Instance of the class containing the model\n    my_model = Model(model)\n    \n    # Save the model and optimizer state\n    my_model.save('model_checkpoint.pth', optimizer=optimizer)\n\n    # Save only the model state\n    my_model.save('model_only.pth')", "CodeBleu_score": 44.35286469569281}, {"code": "import torch\n\nclass Model:\n    def __init__(self, model):\n        self.model = model  # A PyTorch model\n\n    def save(self, path, optimizer=None):\n        \"\"\"\n        The function saves the model state to a specified path.\n        If an optimizer is provided, it saves both the model state and the optimizer state.\n        If no optimizer is provided, it saves only the model state.\n\n        Args:\n            path (str): The path to the file where the model and possibly the optimizer state are saved.\n            optimizer (torch.optim.Optimizer, optional): The optimizer whose state is to be saved. Default is None.\n\n        Returns:\n            None\n        \"\"\"\n        if optimizer is not None:\n            torch.save({\n                'model_state_dict': self.model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict()\n            }, path)\n        else:\n            torch.save(self.model.state_dict(), path)\n\n# Example usage:\n# Assuming `net` is an instance of some neural network model and `opt` is an optimizer\n# model = Model(net)\n# model.save('model_path.pth', optimizer=opt)", "CodeBleu_score": 50.496374789196395}, {"code": "import torch\n\nclass ModelHandler:\n    def __init__(self, model):\n        self.model = model\n\n    def save(self, path, optimizer=None):\n        \"\"\"The function saves the model state to a specified path.\n        If an optimizer is provided, it saves both the model state and the optimizer state.\n        If no optimizer is provided, it saves only the model state.\n        \n        Args:\n            path (str): The path where the model (and possibly the optimizer) will be saved.\n            optimizer (torch.optim.Optimizer, optional): The optimizer to save with the model. Defaults to None.\n        \"\"\"\n        state = {'model_state': self.model.state_dict()}\n        if optimizer is not None:\n            state['optimizer_state'] = optimizer.state_dict()\n        \n        torch.save(state, path)", "CodeBleu_score": 49.33910796236696}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "import re\nimport unicodedata\nimport inflect\n\ndef english_cleaners(text):\n    \"\"\"\n    The function processes English text through a series of cleaning steps.\n    It converts the text to ASCII, lowercases it, expands numbers and abbreviations, and collapses whitespace.\n    The cleaned text is then returned.\n    \"\"\"\n\n    def remove_non_ascii(text):\n        \"\"\"Remove non-ASCII characters from the text.\"\"\"\n        return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n\n    def lowercase_text(text):\n        \"\"\"Convert all characters in the text to lowercase.\"\"\"\n        return text.lower()\n\n    def expand_numbers(text):\n        \"\"\"Expand all numbers in the text to their word equivalents.\"\"\"\n        p = inflect.engine()\n        def replace_numbers(match):\n            number = int(match.group(0))\n            return p.number_to_words(number)\n        return re.sub(r'\\d+', replace_numbers, text)\n\n    def expand_abbreviations(text):\n        \"\"\"Expand common English abbreviations.\"\"\"\n        text = re.sub(r\"\\bDr\\b\", \"Doctor\", text)\n        text = re.sub(r\"\\bMr\\b\", \"Mister\", text)\n        text = re.sub(r\"\\bMrs\\b\", \"Mistress\", text)\n        text = re.sub(r\"\\bSt\\b\", \"Street\", text)\n        text = re.sub(r\"\\bRd\\b\", \"Road\", text)\n        text = re.sub(r\"\\bAve\\b\", \"Avenue\", text)\n        return text\n\n    def collapse_whitespace(text):\n        \"\"\"Reduce multiple whitespace to a single space and strip leading/trailing spaces.\"\"\"\n        return re.sub(r'\\s+', ' ', text).strip()\n\n    # Apply the cleaning functions in the defined sequence\n    text = remove_non_ascii(text)\n    text = lowercase_text(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n\n    return text", "CodeBleu_score": 58.162978174901134}, {"code": "import re\nfrom unidecode import unidecode\n\ndef expand_numbers_and_abbreviations(text):\n    # Example expansions (simple illustrative cases, real-life would be mor robust)\n    expansions = {\n        r'\\bDr\\.\\b': 'Doctor',\n        r'\\bJan\\.\\b': 'January',\n        r'\\bFeb\\.\\b': 'February',\n        # Numbers\n        r'\\b0\\b': 'zero',\n        r'\\b1\\b': 'one',  # Can continue with more\n    }\n    for pattern, replacement in expansions.items():\n        text = re.sub(pattern, replacement, text)\n    return text\n\ndef english_cleaners(text):\n    \"\"\"The function processes English text through a series of cleaning steps.\n    It converts the text to ASCII, lowercases it, expands numbers and abbreviations, and collapses whitespace.\n    The cleaned text is then returned.\"\"\"\n    # Convert text to ASCII\n    text = unidecode(text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Expand numbers and abbreviations\n    text = expand_numbers_and_abbreviations(text)\n\n    # Collapse whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text", "CodeBleu_score": 41.07816689435618}, {"code": "import re\nimport unidecode\nfrom num2words import num2words\n\ndef english_cleaners(text):\n    \"\"\"\n    The function processes English text through a series of cleaning steps.\n    It converts the text to ASCII, lowercases it, expands numbers and abbreviations, and collapses whitespace.\n    The cleaned text is then returned.\n    \"\"\"\n\n    # Convert text to ASCII using unidecode\n    text = unidecode.unidecode(text)\n\n    # Lowercase all characters in the text\n    text = text.lower()\n    \n    # Expand numbers into words (e.g., \"42\" to \"forty-two\")\n    text = re.sub(r'\\b\\d+\\b', lambda x: num2words(x.group()), text)\n\n    # Basic handling for common abbreviations\n    # This part can be very large depending on how many and which abbreviations you need to support\n    abbreviations = {\n        \"mr.\": \"mister\",\n        \"mrs.\": \"misses\",\n        \"dr.\": \"doctor\",\n        \"st.\": \"saint\",\n        \"co.\": \"company\",\n        \"jr.\": \"junior\",\n        \"maj.\": \"major\",\n        \"gen.\": \"general\",\n        \"drs.\": \"doctors\",\n        \"rev.\": \"reverend\",\n        \"lt.\": \"lieutenant\",\n        \"hon.\": \"honorable\",\n        \"sgt.\": \"sergeant\",\n        \"capt.\": \"captain\",\n        \"esq.\": \"esquire\",\n        \"ltd.\": \"limited\",\n        \"col.\": \"colonel\",\n        \"ft.\": \"fort\",\n    }\n    for abbr, full in abbreviations.items():\n        text = re.sub(r'\\b' + re.escape(abbr), full, text)\n\n    # Collapse multiple whitespaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n\n# Example Test\ntext_input = \"Dr. Smith earned $30,000 in 1999 & lives at St. Patrick's St.\"\nprint(english_cleaners(text_input))", "CodeBleu_score": 27.994069400150256}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names):\n    \"\"\"\n    The function converts text to a sequence of symbol IDs.\n    It processes the text, identifying segments within curly braces as ARPAbet phonetic notation, and converts these segments separately.\n    Regular text is cleaned and converted to symbol IDs using specified cleaners.\n    Finally, an end-of-sequence token is appended before returning the sequence.\n\n    Args:\n    - text (str): The input text to convert.\n    - cleaner_names (list of str): Names of the cleaning functions to apply to the text.\n\n    Returns:\n    - List[int]: The sequence of symbol IDs.\n    \"\"\"\n\n    import re\n\n    # Define functions to clean text\n    def simple_clean(text):\n        \"\"\"A very simple text cleaner that lowers text and strips spaces.\"\"\"\n        return text.strip().lower()\n\n    # Apply cleaning functions\n    for cleaner in cleaner_names:\n        if cleaner == \"simple_clean\":\n            text = simple_clean(text)\n\n    # Map characters to symbols in a simplistic mapping\n    char_to_id = {char: idx for idx, char in enumerate(\"abcdefghijklmnopqrstuvwxyz \", 1)}\n    arpabet_to_id = {char: idx for idx, char in enumerate(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", 27)}\n\n    sequence = []\n\n    # Regular expression to find ARPAbet segments\n    arpabet_regex = r\"\\{([A-Z]+)\\}\"\n    segments = re.split(arpabet_regex, text)\n\n    # Process segments\n    for i, segment in enumerate(segments):\n        if i % 2 == 0:  # Even segments are regular text\n            segment_ids = [char_to_id.get(char, 0) for char in segment]\n        else:  # Odd segments are ARPAbet\n            segment_ids = [arpabet_to_id.get(char, 0) for char in segment]\n        sequence.extend(segment_ids)\n    \n    # Append end-of-sequence token, let's assume it's the ID 0\n    sequence.append(0)\n\n    return sequence\n\n# Example usage\ntext = \"Hello {WORLD} this is an example {TEXT}\"\nsequence = text_to_sequence(text, ['simple_clean'])\nprint(\"Sequence:\", sequence)", "CodeBleu_score": 29.316851630988083}, {"code": "def text_to_sequence(text, cleaner_names):\n    \"\"\"\n    The function converts text to a sequence of symbol IDs.\n    It processes the text, identifying segments within curly braces as ARPAbet phonetic notation,\n    and converts these segments separately.\n    Regular text is cleaned and converted to symbol IDs using specified cleaners.\n    Finally, an end-of-sequence token is appended before returning the sequence.\n    \"\"\"\n    import re\n    \n    # Import necessary helpers and symbol mappings (example placeholders)\n    from cleaners import clean_text\n    from symbols import symbols, arpabet_to_id, text_to_id, end_of_sequence_id\n    \n    # Clean the text according to specified cleaner functions\n    cleaned_text = clean_text(text, cleaner_names)\n\n    # Initialize sequence list\n    sequence = []\n\n    # Regex to find ARPAbet phonetics inside curly braces\n    arpabet_pattern = re.compile(r'{([A-Z0-9\\s]+)}')\n    index = 0\n    \n    # Handle text with ARPAbet phonetic notation\n    while True:\n        match = arpabet_pattern.search(cleaned_text, index)\n        if not match:\n            break\n        \n        # Add regular text before ARPAbet\n        regular_text = cleaned_text[index:match.start()]\n        sequence.extend(text_to_id(regular_text))\n        \n        # Convert ARPAbet to symbol ids\n        arpabet_text = match.group(1)\n        arpabet_sequence = arpabet_to_id(arpabet_text)\n        sequence.extend(arpabet_sequence)\n        \n        # Update index to continue after this match\n        index = match.end()\n    \n    # Add any remaining regular text after the last ARPAbet block\n    if index < len(cleaned_text):\n        remaining_text = cleaned_text[index:]\n        sequence.extend(text_to_id(remaining_text))\n    \n    # Append end-of-sequence token\n    sequence.append(end_of_sequence_id())\n    \n    return sequence", "CodeBleu_score": 34.548603344667036}, {"code": "import re\n\n# Define mappings from characters/phonemes to IDs (example dictionary below)\nsymbol_to_id = {\n    'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5,  # Example mappings for characters\n    'AA': 6, 'AE': 7, 'AH': 8,  # Example mappings for ARPAbet phonemes\n    'EOS': 0  # Assume an end-of-sequence token's ID is 0\n}\n\n# Placeholder cleaning functions\ndef basic_cleaner(text):\n    # Implement cleaning logic (e.g., lowercase, remove punctuation)\n    return text.lower()\n\ndef arpabet_cleaner(text):\n    # Clean ARPAbet-specific issues if any, generally there may be none\n    return text\n\ndef clean_text(text, cleaner_names):\n    # Apply a sequence of cleaning functions\n    for name in cleaner_names:\n        if name == 'basic':\n            text = basic_cleaner(text)\n        elif name == 'arpabet':\n            text = arpabet_cleaner(text)\n    return text\n\ndef text_to_sequence(text, cleaner_names):\n    # Define regular expressions to find ARPAbet and regular text\n    arpabet_pattern = re.compile(r'\\{(.+?)\\}')\n    sequence = []\n\n    # Process text outside and inside of curly braces\n    cursor = 0\n    for match in arpabet_pattern.finditer(text):\n        # Regular text\n        start, end = match.span()\n        regular_text = text[cursor:start]\n        clean_regular_text = clean_text(regular_text, cleaner_names)\n        sequence.extend(symbol_to_id[char] for char in clean_regular_text)\n\n        # ARPAbet text\n        arpabet_text = match.group(1)\n        clean_arpabet_text = clean_text(arpabet_text, ['arpabet'])\n        sequence.extend(symbol_to_id[phoneme] for phoneme in clean_arpabet_text.split())\n\n        # Move cursor past this match\n        cursor = end\n\n    # Catch any remaining text after the last match\n    remaining_text = text[cursor:]\n    clean_remaining_text = clean_text(remaining_text, cleaner_names)\n    sequence.extend(symbol_to_id[char] for char in clean_remaining_text)\n    \n    # Append End-of-Sequence token\n    sequence.append(symbol_to_id['EOS'])\n\n    return sequence\n\n# Example of usage\ntext = \"Hello {AH L OW1} world\"\ncleaner_names = ['basic']\nseq = text_to_sequence(text, cleaner_names)\nprint(seq)", "CodeBleu_score": 31.607557975243406}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence):\n    \"\"\"\n    The function converts a sequence of IDs into a string.\n    It iterates through the sequence, retrieves corresponding symbols from a dictionary,\n    encloses ARPAbet symbols in curly braces, and concatenates them into a result string.\n    The function ensures that adjacent curly braces are replaced with a space.\n    \n    Args:\n    sequence (list[int]): A list of integer IDs which are keys in the symbol dictionary.\n\n    Returns:\n    str: The formatted string with symbols retrieved and concatenated as per the mapping.\n    \"\"\"\n\n    # Sample dictionary for symbols mapping\n    symbol_dict = {\n        1: 'Hello',\n        2: 'World',\n        3: 'AW',\n        4: 'EH1',\n        5: 'L',\n        6: ' OW1'\n    }\n    \n    result = ''\n    for id in sequence:\n        # Retrieve the symbol from dictionary\n        symbol = symbol_dict.get(id, '')\n        \n        # Check if the symbol corresponds to ARPAbet (contains numbers)\n        # and enclose it in curly braces if it does\n        if any(char.isdigit() for char in symbol):\n            symbol = '{' + symbol + '}'\n        \n        # Add the symbol to result\n        result += symbol + ' '\n    \n    # Replace adjacent curly braces with a space\n    result = result.replace('}{', ' ')\n    \n    # Return the trimmed string\n    return result.strip()", "CodeBleu_score": 23.196548343650004}, {"code": "def sequence_to_text(sequence):\n    \"\"\"\n    The function converts a sequence of IDs into a string.\n    It iterates through the sequence, retrieves corresponding symbols from a dictionary, \n    encloses ARPAbet symbols in curly braces, and concatenates them into a result string.\n    The function ensures that adjacent curly braces are replaced with a space.\n    \n    Args:\n    sequence (list of int): A list of integers where each integer corresponds to an ID for an ARPAbet symbol.\n    \n    Returns:\n    str: The resulting string with all symbols converted and concatenated according to the specifications.\n    \"\"\"\n    \n    # Example ARPAbet dictionary. The actual dictionary should map IDs to ARPAbet symbols properly.\n    id_to_arpabet = {\n        1: \"AH0\", 2: \"B\", 3: \"K\", 4: \"DH\", 5: \"EH1\", 6: \"F\", 7: \"G\", 8: \"HH\", 9: \"AY1\"\n    }\n    \n    # Converting sequence of IDs to symbols, enclosing in curly braces\n    symbols_with_braces = ['{' + id_to_arpabet.get(id, '') + '}' for id in sequence]\n    \n    # Joining all symbols to form the result string\n    result_string = ''.join(symbols_with_braces)\n    \n    # Replacing adjacent curly braces with a space (removing empty symbol representations if any)\n    result_string = result_string.replace('}{', ' ')\n    \n    return result_string\n\n# Example usage\nsequence = [1, 2, 3, 9, 4, 5, 6, 7, 8]\nprint(sequence_to_text(sequence))", "CodeBleu_score": 23.822586151027608}, {"code": "def sequence_to_text(sequence):\n    \"\"\"\n    The function converts a sequence of IDs into a string.\n    It iterates through the sequence, retrieves corresponding symbols from a dictionary,\n    encloses ARPAbet symbols in curly braces, and concatenates them into a result string.\n    The function ensures that adjacent curly braces are replaced with a space.\n    \"\"\"\n    # Define a sample dictionary mapping IDs to ARPAbet symbols (or similar)\n    id_to_symbol = {\n        1: \"AH0\",\n        2: \"S\",\n        3: \"K\",\n        4: \"IY1\",\n        5: \"M\",\n        6: \"AH0\",\n        7: \"L\"\n    }\n    \n    result = \"\"\n    last_was_arpabet = False\n\n    for item in sequence:\n        # Get the symbol from the dictionary\n        symbol = id_to_symbol.get(item, \"\")\n        \n        # Enclose in curly braces if it's a known ARPAbet symbol\n        if symbol:\n            # Add a space if the last character was a closing brace and we are adding another ARPAbet symbol\n            if last_was_arpabet and len(result) > 0:\n                result += ' '\n            result += '{' + symbol + '}'\n            last_was_arpabet = True\n        else:\n            # If the symbol does not exist in the dictionary, just concatenate\n            # Include space handling if required depending on the specifics\n            result += item\n            last_was_arpabet = False\n    \n    return result", "CodeBleu_score": 24.837474064613758}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "bash\npip install numpy librosa tensorflow", "CodeBleu_score": 0.0}, {"code": "import torch\nimport librosa\nimport numpy as np\nimport os\n\n# Assuming encoder_model_fpath is a path to a PyTorch model file (.pt or .pth), and fpaths is a list of file paths\n\n# Load the encoder model that is typically a PyTorch state_dict or entire model\ndef load_model(model_path):\n    model = torch.load(model_path)\n    model.eval()\n    return model\n\n# In PyTorch, GPU usage is recommended for model inference if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef preprocess_waveform(fpath, sample_rate=16000):\n    # Load and preprocess waveform using librosa\n    y, sr = librosa.load(fpath, sr=sample_rate)  # Load waveform at the specified sample rate\n    y = librosa.util.normalize(y)  # Normalize the waveform\n    return y, sr\n\ndef compute_embedding(waveform, model):\n    # Convert the waveform to a tensor and add batch dimension\n    waveform_tensor = torch.tensor(waveform).float().to(device)  # Assume waveform is mono\n    waveform_tensor = waveform_tensor.unsqueeze(0)  # Add batch dimension\n\n    # Forward pass through the model to get the embedding\n    with torch.no_grad():\n        embedding = model(waveform_tensor)\n\n    # Squeeze to remove batch dimension if necessary and return embedding\n    return embedding.squeeze().cpu().numpy()\n\ndef embed_utterance(fpaths, encoder_model_fpath):\n    \"\"\"\n    The function generates and saves the speaker embedding for a given utterance.\n    It first loads the encoder model if it is not already loaded.\n    It then loads and preprocesses the waveform from the specified file path,\n    computes the speaker embedding, and saves the embedding to the specified file path.\n    \"\"\"\n\n    # Load the encoder model\n    model = load_model(encoder_model_fpath).to(device)\n\n    # Process each file path in the list\n    for fpath in fpaths:\n        # Preprocess the audio file\n        waveform, sr = preprocess_waveform(fpath)\n\n        # Compute the embedding\n        embedding = compute_embedding(waveform, model)\n\n        # Save the embedding to a file (assuming .npy format for simplicity)\n        embedding_path = fpath + \"_embedding.npy\"\n        np.save(embedding_path, embedding)\n\n        print(f\"Saved embedding to {embedding_path}\")\n\n# Example usage:\n# embed_utterance([\"path_to_audio_file.wav\"], \"path_to_encoder_model.pt\")", "CodeBleu_score": 34.338816406621355}, {"code": "import numpy as np\nimport librosa\n\nclass EncoderModel:\n    \"\"\" This is a placeholder for the encoder model class,\n    replace it with actual model code if available.\"\"\"\n    def __init__(self, model_path):\n        # Initialize or load a model from the given path\n        pass\n\n    def embed_utterance(self, audio):\n        # Compute embedding from preprocessed audio\n        # This should return numpy array\n        return np.random.rand(256)  # example embedding\n\ndef load_model(model_path):\n    # Assuming model loading is defined elsewhere\n    # This function should return an instance of the model\n    return EncoderModel(model_path)\n\ndef embed_utterance(fpaths, encoder_model_fpath):\n    \"\"\"\n    The function generates and saves the speaker embedding for a given utterance.\n    It first loads the encoder model if it is not already loaded.\n    It then loads and preprocesses the waveform from the specified file path, computes the speaker embedding, \n    and saves the embedding to the specified file path.\n    \"\"\"\n    # Load the encoder model using a global variable to avoid reloading\n    global encoder_model\n    if 'encoder_model' not in globals():\n        encoder_model = load_model(encoder_model_fpath)\n    \n    # Iterate through each file path provided\n    for fpath in fpaths:\n        # Load and preprocess the waveform using librosa\n        waveform, sr = librosa.load(fpath, sr=None)  # Load at native sampling rate\n        waveform = librosa.resample(waveform, sr, 16000)  # Resample for compatibility if needed\n\n        # Compute the speaker embedding using the loaded model\n        embedding = encoder_model.embed_utterance(waveform)\n\n        # Save the embedding to a specified file path (a .npy file)\n        # Assuming the path ends with .npy\n        np.save(fpath.replace('.wav', '.npy'), embedding)  # Change extension to .npy\n\n# Example use case:\n# embed_utterance([\"path_to_utterance1.wav\", \"path_to_utterance2.wav\"], \"path_to_encoder_model\")", "CodeBleu_score": 34.63418455198444}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "import os\nfrom pathlib import Path\nfrom concurrent.futures import ProcessPoolExecutor\nfrom tqdm import tqdm\n# Assuming the existence of synthesizer.preprocess and synthesizer.read_audio\nfrom synthesizer.preprocess import preprocess_audios, read_metafile\nfrom synthesizer.read_audio import wav_to_embedding\nimport numpy as np\n\ndef create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int):\n    \"\"\"\n    The function creates embeddings for audio files.\n    It verifies the existence of required directories and files, then reads metadata and prepares file paths for audio and embeddings.\n    A multiprocessing pool is used to process the audio files in parallel, generating embeddings using a specified encoder model.\n    Progress is tracked and displayed using a progress bar.\n    \"\"\"\n    assert synthesizer_root.exists(), f\"Synthesizer root directory not found at '{synthesizer_root}'.\"\n    assert encoder_model_fpath.exists(), f\"Encoder model file not found at '{encoder_model_fpath}'.\"\n\n    # Load the encoder model, assuming a hypothetical function `load_model`\n    encoder = load_model(encoder_model_fpath)\n\n    # Assuming metadata is stored in a file called 'metadata.csv' in the synthesizer root\n    metadata_path = synthesizer_root / \"metadata.csv\"\n    assert metadata_path.exists(), f\"Metadata file not found at '{metadata_path}'.\"\n    \n    # Read metadata \n    audio_files = read_metafile(metadata_path)\n    \n    # Path for saving embeddings\n    embeddings_folder = synthesizer_root / \"embeddings\"\n    embeddings_folder.mkdir(exist_ok=True)\n    \n    def process_file(file_path):\n        wav_path = synthesizer_root / \"audio\" / file_path\n        embedding_path = embeddings_folder / f\"{file_path.stem}.npy\"\n        if not embedding_path.exists():\n            audio, sample_rate = preprocess_audios(wav_path)\n            embedding = wav_to_embedding(audio, sample_rate, encoder)\n            np.save(embedding_path, embedding)\n        return embedding_path\n    \n    with ProcessPoolExecutor(max_workers=n_processes) as executor:\n        futures = [executor.submit(process_file, audio_file) for audio_file in audio_files]\n        \n        # Progress bar setup with tqdm\n        for future in tqdm(concurrent.futures.as_completed(futures), total=len(audio_files), desc=\"Generating embeddings\"):\n            _ = future.result()\n\n    print(\"Embeddings have been successfully generated and stored.\")\n\ndef load_model(model_path):\n    # This is a placeholder function to simulate loading a model\n    pass\n\ndef preprocess_audios(audio_path):\n    # Placeholder to simulate audio preprocessing\n    return np.random.randn(16000), 16000\n\ndef wav_to_embedding(audio, sample_rate, model):\n    # Placeholder function to simulate converting a wav file to an embedding\n    return np.random.rand(128)", "CodeBleu_score": 32.886713563167284}, {"code": "import os\nimport multiprocessing\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport librosa\nimport numpy as np\nimport soundfile as sf\n\n# Simulate a model loading function for embeddings\ndef load_model(model_path):\n    # This function should load and return a model given the path\n    # This is a placeholder for loading an encoder model\n    return None\n\n# Simulate a function that uses model to compute embeddings\ndef compute_embedding(audio_data, sample_rate, model):\n    # This function should compute and return embeddings from audio data\n    # Implementation depends on the encoder model\n    return np.random.rand(10)  # returns dummy embeddings\n\ndef process_file(file_path, model):\n    \"\"\" Function to load an audio file, compute embedding and save it \"\"\"\n    audio, sample_rate = sf.read(file_path)\n    embedding = compute_embedding(audio, sample_rate, model)\n    output_path = file_path.with_suffix('.npy')\n    np.save(output_path, embedding)\n\ndef create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int):\n    # Verify the directories exist\n    assert synthesizer_root.exists(), f\"Directory {synthesizer_root} does not exist\"\n    assert encoder_model_fpath.exists(), f\"Encoder model file {encoder_model_fpath} does not exist\"\n\n    # Load the encoder model\n    model = load_model(encoder_model_fpath)\n\n    # Prepare paths of audio files\n    audio_files = list(synthesizer_root.glob('*.wav'))  # Assuming audio files are in WAV format\n    if not audio_files:\n        raise ValueError(\"No audio files found in the directory.\")\n\n    # Using multiprocessing to process files\n    with multiprocessing.Pool(n_processes) as pool:\n        tasks = [(file_path, model) for file_path in audio_files]\n        # Track progress with tqdm\n        for _ in tqdm(pool.imap_unordered(process_file, tasks), total=len(tasks)):\n            pass\n\n    print(\"Processing completed.\")", "CodeBleu_score": 35.176099293143196}, {"code": "import os\nfrom pathlib import Path\nfrom multiprocessing import Pool\nimport tqdm\nimport numpy as np\n\ndef create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int):\n    \"\"\"\n    The function creates embeddings for audio files.\n    It verifies the existence of required directories and files, then reads metadata and prepares file paths for audio and embeddings.\n    A multiprocessing pool is used to process the audio files in parallel, generating embeddings using a specified encoder model.\n    Progress is tracked and displayed using a progress bar.\n\n    :param synthesizer_root: Path to the root directory where the audio and metadata are stored.\n    :param encoder_model_fpath: Path to the saved encoder model file.\n    :param n_processes: Number of processes to use for multiprocessing.\n    \"\"\"\n    \n    # Verify the existence of directories and models\n    if not synthesizer_root.exists():\n        raise FileNotFoundError(f\"The synthesizer root directory does not exist: {synthesizer_root}\")\n    if not encoder_model_fpath.exists():\n        raise FileNotFoundError(f\"The encoder model file does not exist: {encoder_model_fpath}\")\n    \n    # Load the encoder model (this part is ambiguous without knowing the encoding library, assuming a generic load function)\n    encoder = load_encoder_model(encoder_model_fpath)\n\n    # Read metadata and prepare file paths\n    metadata_fpath = synthesizer_root / 'metadata.csv'\n    if not metadata_fpath.exists():\n        raise FileNotFoundError(f\"Metadata file does not exist: {metadata_fpath}\")\n    \n    # Prepare list of files and their corresponding embedding output paths\n    audio_fpaths = []\n    embed_fpaths = []\n    with open(metadata_fpath, \"r\") as file:\n        for line in file:\n            parts = line.strip().split(\"|\")\n            audio_path = synthesizer_root / 'wavs' / parts[0]\n            embed_path = synthesizer_root / 'embeds' / f\"{parts[0]}.npy\"\n            if not embed_path.parent.exists():\n                os.makedirs(embed_path.parent)\n            audio_fpaths.append(audio_path)\n            embed_fpaths.append(embed_path)\n    \n    # Function to process single file\n    def process_file(args):\n        audio_path, embed_path = args\n        audio_data = load_audio(audio_path)  # This function should load and possibly preprocess the audio\n        embedding = encoder.encode(audio_data)  # Assuming the encoder has an 'encode' method\n        np.save(embed_path, embedding)\n    \n    # Processing files using a pool of workers\n    with Pool(n_processes) as pool:\n        list(tqdm.tqdm(pool.imap(process_file, zip(audio_fpaths, embed_fpaths)), total=len(audio_fpaths)))\n    \ndef load_encoder_model(encoder_model_fpath):\n    # Placeholder load function\n    return EncoderModel.load(encoder_model_fpath)\n\nclass EncoderModel:\n    @staticmethod\n    def load(path):\n        # This would be replaced with actual loading logic\n        return EncoderModel()\n    \n    def encode(self, audio_data):\n        # This would be replaced with actual encoding logic\n        return np.random.rand(512)  # Assume 512 is the embedding size\n\ndef load_audio(audio_path):\n    # This function would actually load and preprocess the audio\n    return np.random.randn(16000)  # Dummy data assuming 1 sec at 16 kHz", "CodeBleu_score": 30.515524706557184}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nfrom datetime import datetime\n\ndef eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams):\n    \"\"\"The function evaluates the model by saving various outputs and visualizations for a given step and sample number.\n    It saves the attention plot, predicted mel spectrogram, and reconstructed waveform to specified directories.\n    It also generates and saves a plot comparing the predicted and target mel spectrograms with a title indicating the model type, current time, step number, and loss.\n    Finally, it prints the input sequence at the current step.\"\"\"\n    \n    # Ensure directories exist\n    os.makedirs(plot_dir, exist_ok=True)\n    os.makedirs(mel_output_dir, exist_ok=True)\n    os.makedirs(wav_dir, exist_ok=True)\n    \n    # Save attention plot\n    plt.figure(figsize=(10, 8))\n    plt.imshow(attention, aspect='auto', origin='lower', interpolation='none')\n    plt.colorbar()\n    plt.title(f'Attention Map - Step {step}')\n    plt.xlabel('Decoder timestep')\n    plt.ylabel('Encoder timestep')\n    plt.savefig(os.path.join(plot_dir, f'attention_step_{step}_sample_{sample_num}.png'))\n    plt.close()\n    \n    # Save mel prediction\n    np.save(os.path.join(mel_output_dir, f'mel_pred_step_{step}_sample_{sample_num}.npy'), mel_prediction)\n    \n    # Mel spectrogram plot\n    plt.figure(figsize=(10, 4))\n    plt.imshow(mel_prediction, aspect='auto', origin='lower', interpolation='none')\n    plt.title(f'Predicted Mel Spectrogram - Step {step} Sample {sample_num}')\n    plt.colorbar()\n    plt.xlabel('Time')\n    plt.ylabel('Mel frequency bin')\n    plt.savefig(os.path.join(plot_dir, f'mel_pred_step_{step}_sample_{sample_num}.png'))\n    plt.close()\n    \n    # Mock waveform reconstruction and saving\n    waveform = np.random.randn(2000)  # Mocked processing from mel to audio\n    sf.write(os.path.join(wav_dir, f'audio_step_{step}_sample_{sample_num}.wav'), waveform, hparams['audio_sample_rate'])\n    \n    # Comparison plot\n    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n    titles = ['Predicted Mel Spectrogram', 'Target Mel Spectrogram']\n    for i, data in enumerate([mel_prediction, target_spectrogram]):\n        axes[i].imshow(data, aspect='auto', origin='lower', interpolation='none')\n        axes[i].set_title(titles[i])\n        axes[i].set_xlabel('Time')\n        axes[i].set_ylabel('Mel frequency bin')\n    fig.suptitle(f'Mel Spectrogram Comparison - Model Time {datetime.now():%Y-%m-%d %H:%M:%S} Step {step} Loss {loss:.4f}')\n    plt.savefig(os.path.join(plot_dir, f'mel_comparison_step_{step}_sample_{sample_num}.png'))\n    plt.close()\n\n    # Print input sequence\n    print(f'Input sequence at step {step}: {input_seq}')", "CodeBleu_score": 29.314338383708172}, {"code": "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport librosa.display\nimport soundfile as sf\nfrom datetime import datetime\n\ndef eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams):\n    \"\"\"\n    The function evaluates the model by saving various outputs and visualizations for a given step and sample number.\n    It saves the attention plot, predicted mel spectrogram, and reconstructed waveform to specified directories.\n    It also generates and saves a plot comparing the predicted and target mel spectrograms with a title indicating the model type,\n    current time, step number, and loss.\n    Finally, it prints the input sequence at the current step.\n    \"\"\"\n    # Ensure required directories exist\n    os.makedirs(plot_dir, exist_ok=True)\n    os.makedirs(mel_output_dir, exist_ok=True)\n    os.makedirs(wav_dir, exist_ok=True)\n\n    # Prepare base filename for all saved files\n    base_filename = f\"step_{step}_sample_{sample_num}\"\n    time_stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    base_plot_title = f\"Model Evaluation at Step {step}, Sample {sample_num}, Loss: {loss:.4f}, Time: {time_stamp}\"\n\n    # Save attention plot\n    plt.figure(figsize=(10, 4))\n    plt.imshow(attention, aspect='auto', origin='lower', interpolation='none')\n    plt.title(f\"{base_plot_title}\\nAttention Map\")\n    plt.colorbar()\n    plt.savefig(os.path.join(plot_dir, f\"{base_filename}_attention.png\"))\n    plt.close()\n\n    # Save Mel spectrogram\n    plt.figure(figsize=(10, 4))\n    librosa.display.specshow(np.log(mel_prediction + 1e-9), sr=hparams.sampling_rate, hop_length=hparams.hop_size, x_axis='time', y_axis='mel')\n    plt.title(f\"{base_plot_title}\\nPredicted Mel Spectrogram\")\n    plt.colorbar(format='%+2.0f dB')\n    plt.savefig(os.path.join(mel_output_dir, f\"{base_filename}_predicted_mel.png\"))\n    plt.close()\n\n    # Synthesize audio from mel spectrogram and save\n    synthesized_audio = hparams.vocoder(mel_prediction)\n    sf.write(os.path.join(wav_dir, f\"{base_filename}_synthesized.wav\"), synthesized_audio, hparams.sampling_rate)\n\n    # Compare predicted and target mel spectrograms\n    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n    for i, (spec, title) in enumerate(zip([target_spectrogram, mel_prediction], [\"Target Mel\", \"Predicted Mel\"])):\n        ax = axes[i]\n        img = librosa.display.specshow(np.log(spec + 1e-9), sr=hparams.sampling_rate, hop_length=hparams.hop_size, x_axis='time', y_axis='mel', ax=ax)\n        ax.set_title(title)\n        fig.colorbar(img, ax=ax, format='%+2.0f dB')\n    plt.suptitle(base_plot_title)\n    plt.savefig(os.path.join(plot_dir, f\"{base_filename}_comparison.png\"))\n    plt.close()\n\n    # Print the analyzed input sequence\n    print(f\"Input sequence at step {step}: {input_seq}\")", "CodeBleu_score": 29.566671010285717}, {"code": "import matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport librosa\nimport soundfile as sf\nimport datetime\n\ndef eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams):\n    \"\"\"\n    The function evaluates the model by saving various outputs and visualizations for a given step and sample number.\n    It saves the attention plot, predicted mel spectrogram, and reconstructed waveform to specified directories.\n    It also generates and saves a plot comparing the predicted and target mel spectrograms with a title indicating\n    the model type, current time, step number, and loss.\n    Finally, it prints the input sequence at the current step.\n\n    Parameters:\n        attention: np.ndarray, the attention weights matrix\n        mel_prediction: np.ndarray, the predicted mel spectrogram\n        target_spectrogram: np.ndarray, the target mel spectrogram\n        input_seq: list or str, the input sequence used for prediction\n        step: int, the current training step\n        plot_dir: str, directory to save plots\n        mel_output_dir: str, directory to save mel spectrograms\n        wav_dir: str, directory to save waveforms\n        sample_num: int, sample identifier\n        loss: float, the current loss value\n        hparams: namespace or dict, hyperparameters for signal processing\n    \"\"\"\n    \n    # Ensure directories exist\n    os.makedirs(plot_dir, exist_ok=True)\n    os.makedirs(mel_output_dir, exist_ok=True)\n    os.makedirs(wav_dir, exist_ok=True)\n\n    # Create file basename\n    basename = f\"step{step}_sample{sample_num}\"\n\n    # Save attention plot\n    fig, ax = plt.subplots(figsize=(10, 4))\n    cax = ax.matshow(attention, aspect='auto', origin='lower', cmap='viridis')\n    fig.colorbar(cax)\n    plt.title('Attention Map')\n    plt.xlabel('Decoder timestep')\n    plt.ylabel('Encoder timestep')\n    plt.savefig(os.path.join(plot_dir, f\"{basename}_attention.png\"))\n    plt.close(fig)\n    \n    # Save mel spectrogram comparison plot\n    fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n    axs[0].imshow(np.flipud(target_spectrogram), aspect='auto', origin='lower', cmap='viridis')\n    axs[0].set_title('Target Mel-Spectrogram')\n    axs[1].imshow(np.flipud(mel_prediction), aspect='auto', origin='lower', cmap='viridis')\n    axs[1].set_title('Predicted Mel-Spectrogram')\n    plt.suptitle(f'Model - Step: {step}, Loss: {loss:.4f}')\n    plt.savefig(os.path.join(plot_dir, f\"{basename}_mel_comparison.png\"))\n    plt.close(fig)\n    \n    # Save predicted and target mel spectrogram as numpy\n    np.save(os.path.join(mel_output_dir, f\"{basename}_predicted_mel.npy\"), mel_prediction)\n    np.save(os.path.join(mel_output_dir, f\"{basename}_target_mel.npy\"), target_spectrogram)\n\n    # Convert mel spectrogram to audio waveform and save it\n    def mel_to_audio(mel_spectrogram, hparams):\n        # Assuming the existence of mel to audio conversion function according to the given hparams\n        # Typically, Griffin-Lim Algorithm or a learned vocoder (WaveNet, WaveGlow) would be used here.\n        # A direct librosa example for using Griffin-Lim:\n        S = librosa.feature.inverse.mel_to_stft(mel_spectrogram, **hparams['mel_to_stft'])\n        audio = librosa.griffinlim(S)\n        return audio\n    \n    audio = mel_to_audio(mel_prediction, hparams)\n    sf.write(os.path.join(wav_dir, f\"{basename}_reconstructed.wav\"), audio, hparams['sampling_rate'])\n    \n    # Print the input sequence\n    print(f\"Input sequence at step {step}:\", input_seq)", "CodeBleu_score": 32.740011786868905}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "import torch\nimport torch.nn as nn\n\nclass SampleModel(nn.Module):\n    def __init__(self):\n        super(SampleModel, self).__init__()\n        # Example: model with a single Linear layer\n        self.linear = nn.Linear(10, 5)\n\n    def num_params(self):\n        \"\"\"The function calculates and prints the number of trainable parameters in the model.\n        It filters the parameters that require gradients, computes the product of their sizes, sums them up,\n        and converts the total to millions. Finally, it prints the result with a precision of three decimal places.\"\"\"\n        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        total_params_millions = total_params / 1e6\n        print(f\"Trainable parameters: {total_params_millions:.3f} million\")\n\n# Example usage:\nmodel = SampleModel()\nmodel.num_params()", "CodeBleu_score": 29.260539091966166}, {"code": "def num_params(self):\n    \"\"\"The function calculates and prints the number of trainable parameters in the model.\n    It filters the parameters that require gradients, computes the product of their sizes, sums them up, \n    and converts the total to millions.\n    Finally, it prints the result with a precision of three decimal places.\"\"\"\n    total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    total_params_millions = total_params / 1_000_000\n    print(f\"Total trainable parameters: {total_params_millions:.3f}M\")", "CodeBleu_score": 34.67248730043803}, {"code": "def num_params(self):\n    \"\"\"The function calculates and prints the number of trainable parameters in the model.\n    It filters the parameters that require gradients, computes the product of their sizes, sums them up, and converts the total to millions.\n    Finally, it prints the result with a precision of three decimal places.\"\"\"\n    total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n    total_params_millions = total_params / 1_000_000\n    print(f\"Total trainable parameters: {total_params_millions:.3f}M\")", "CodeBleu_score": 34.67248730043803}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "import torch\n\ndef pad_tensor(self, x, pad, side='both'):\n    \"\"\"\n    The function pads a given tensor along the time dimension based on the specified padding and side.\n    It first determines the new total size of the tensor after padding.\n    Then, it creates a new tensor filled with zeros on either the CPU or GPU, depending on availability.\n    The original tensor is placed into this new padded tensor according to the specified side ('before', 'after', or 'both').\n    Finally, the padded tensor is returned.\n    \"\"\"\n    # Determine the device of the tensor (CPU or GPU)\n    device = x.device\n    \n    # Calculate the new size of the tensor after padding\n    if side == 'both':\n        new_size = (x.size(0), x.size(1) + 2 * pad)\n        padding = (pad, pad)\n    elif side == 'before':\n        new_size = (x.size(0), x.size(1) + pad)\n        padding = (pad, 0)\n    elif side == 'after':\n        new_size = (x.size(0), x.size(1) + pad)\n        padding = (0, pad)\n    else:\n        raise ValueError(\"side must be 'before', 'after', or 'both'\")\n    \n    # Create a new tensor filled with zeros\n    padded_tensor = torch.zeros(new_size, dtype=x.dtype, device=device)\n    \n    # Place the original tensor into the new padded tensor\n    if side == 'both':\n        padded_tensor[:, pad:-pad] = x\n    elif side == 'before':\n        padded_tensor[:, pad:] = x\n    elif side == 'after':\n        padded_tensor[:, :-pad] = x\n    \n    return padded_tensor", "CodeBleu_score": 32.758638859565266}, {"code": "import torch\n\ndef pad_tensor(self, x, pad, side='both'):\n    \"\"\"\n    The function pads a given tensor along the time dimension based on the specified padding and side.\n    It first determines the new total size of the tensor after padding.\n    Then, it creates a new tensor filled with zeros on either the CPU or GPU, depending on availability.\n    The original tensor is placed into this new padded tensor according to the specified side ('before', 'after', or 'both').\n    Finally, the padded tensor is returned.\n    \"\"\"\n    # Determine the device of the tensor (CPU or GPU)\n    device = x.device\n    \n    # Calculate padding for each side\n    if side == 'both':\n        pad_before = pad_after = pad\n    elif side == 'before':\n        pad_before = pad\n        pad_after = 0\n    elif side == 'after':\n        pad_before = 0\n        pad_after = pad\n    else:\n        raise ValueError(\"side must be 'before', 'after', or 'both'\")\n    \n    # Create a new tensor with padding\n    total_length = x.size(0) + pad_before + pad_after\n    padded_tensor = torch.zeros(total_length, *x.shape[1:], device=device)\n    \n    # Insert the original tensor into the padded tensor\n    padded_tensor[pad_before:pad_before + x.size(0)] = x\n    \n    return padded_tensor", "CodeBleu_score": 28.634286130714298}, {"code": "import torch\n\ndef pad_tensor(self, x, pad, side='both'):\n    \"\"\"\n    The function pads a given tensor along the time dimension based on the specified padding and side.\n    It first determines the new total size of the tensor after padding.\n    Then, it creates a new tensor filled with zeros on either the CPU or GPU, depending on availability.\n    The original tensor is placed into this new padded tensor according to the specified side ('before', 'after', or 'both').\n    Finally, the padded tensor is returned.\n    \"\"\"\n    # Assuming x is a 2D tensor where the time dimension is the second dimension (index 1)\n    # pad is expected to be an integer or a tuple\n    if isinstance(pad, int):\n        pad = (pad, pad)\n    \n    # Create a padded tensor\n    if side == 'both':\n        padding = (pad[0], pad[1])\n    elif side == 'before':\n        padding = (pad[0], 0)\n    elif side == 'after':\n        padding = (0, pad[1])\n    else:\n        raise ValueError(\"side must be 'before', 'after', or 'both'\")\n    \n    # Pad the tensor\n    # Padding format for `torch.nn.functional.pad` is (pad_left, pad_right, pad_top, pad_bottom)\n    padded_tensor = torch.nn.functional.pad(x, (0, 0, padding[0], padding[1]), mode='constant', value=0)\n    \n    return padded_tensor", "CodeBleu_score": 30.221628226991555}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef save_attention(attn, path):\n    \"\"\"\n    The function saves an attention matrix as an image file.\n    It imports the necessary plotting library, creates a figure, and visualizes the attention matrix with specific settings.\n    The resulting image is saved to the specified path with a .png extension, and the figure is then closed.\n    \"\"\"\n    # Ensure the attention matrix is a numpy array\n    attn = np.array(attn)\n\n    # Create a figure and an axis\n    fig, ax = plt.subplots()\n\n    # Display the attention matrix\n    cax = ax.matshow(attn, cmap='viridis')\n\n    # Add a color bar to the figure\n    fig.colorbar(cax)\n\n    # Save the figure to the specified path with a .png extension\n    plt.savefig(f\"{path}.png\")\n\n    # Close the figure to free up memory\n    plt.close(fig)", "CodeBleu_score": 45.193547612574704}, {"code": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef save_attention(attn, path):\n    \"\"\"\n    The function saves an attention matrix as an image file.\n    It imports the necessary plotting library, creates a figure, and visualizes the attention matrix with specific settings.\n    The resulting image is saved to the specified path with a .png extension, and the figure is then closed.\n    \"\"\"\n    # Create a figure to host the plot\n    plt.figure(figsize=(10, 10))\n    \n    # Visualize the attention matrix\n    plt.imshow(attn, aspect='auto', cmap='viridis')\n    plt.colorbar()  # Add a colorbar to help interpret the values\n    \n    # Save the figure to the specified path with a .png extension\n    plt.savefig(f\"{path}.png\")\n    \n    # Close the figure to free up memory\n    plt.close()", "CodeBleu_score": 33.1771933021112}, {"code": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef save_attention(attn, path):\n    \"\"\"\n    The function saves an attention matrix as an image file.\n    It imports the necessary plotting library, creates a figure, and visualizes the attention matrix with specific settings.\n    The resulting image is saved to the specified path with a .png extension, and the figure is then closed.\n    \"\"\"\n    # Ensure the attention matrix is a numpy array\n    attn = np.array(attn)\n\n    # Create a figure and an axis\n    fig, ax = plt.subplots()\n\n    # Display the attention matrix\n    cax = ax.matshow(attn, cmap='viridis')\n\n    # Add a colorbar to the figure\n    fig.colorbar(cax)\n\n    # Save the figure\n    plt.savefig(path)\n\n    # Close the figure to free up memory\n    plt.close(fig)", "CodeBleu_score": 45.08416163508894}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "import matplotlib.pyplot as plt\n\ndef plot(array):\n    \"\"\"The function plots a given array using matplotlib.\n    It creates a figure with a specified size, adds a subplot, and customizes the axis labels and tick parameters with specific colors and font sizes.\n    Finally, it plots the array on the customized subplot.\"\"\"\n    \n    # Create a figure with a specified size\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Customize axis labels and tick parameters\n    ax.set_xlabel('Index', fontsize=14, color='red')\n    ax.set_ylabel('Value', fontsize=14, color='blue')\n    ax.tick_params(axis='x', colors='purple', labelsize=12)\n    ax.tick_params(axis='y', colors='green', labelsize=12)\n    \n    # Plot the array\n    ax.plot(array)\n    \n    # Show the plot\n    plt.show()", "CodeBleu_score": 40.94159865243795}, {"code": "import matplotlib.pyplot as plt\n\ndef plot(array):\n    \"\"\"The function plots a given array using matplotlib.\n    It creates a figure with a specified size, adds a subplot, and customizes the axis labels and tick parameters with specific colors and font sizes.\n    Finally, it plots the array on the customized subplot.\"\"\"\n    \n    # Create a figure with a specified size\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Customize axis labels and tick parameters\n    ax.set_xlabel('Index', fontsize=14, color='red')\n    ax.set_ylabel('Value', fontsize=14, color='blue')\n    ax.tick_params(axis='x', colors='green', labelsize=12)\n    ax.tick_params(axis='y', colors='purple', labelsize=12)\n    \n    # Plot the array\n    ax.plot(array)\n    \n    # Show the plot\n    plt.show()", "CodeBleu_score": 40.94159865243795}, {"code": "import matplotlib.pyplot as plt\n\ndef plot(array):\n    \"\"\"The function plots a given array using matplotlib.\n    It creates a figure with a specified size, adds a subplot, and customizes the axis labels and tick parameters with specific colors and font sizes.\n    Finally, it plots the array on the customized subplot.\"\"\"\n    \n    # Create a figure with a specified size\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Customize axis labels and tick parameters\n    ax.set_xlabel('Index', fontsize=14, color='red')\n    ax.set_ylabel('Value', fontsize=14, color='blue')\n    ax.tick_params(axis='x', colors='purple', labelsize=12)\n    ax.tick_params(axis='y', colors='green', labelsize=12)\n    \n    # Plot the array\n    ax.plot(array)\n    \n    # Show the plot\n    plt.show()", "CodeBleu_score": 40.94159865243795}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "import csv\nimport os\n\ndef write_to_csv(image_name, prediction, confidence):\n    \"\"\"The function writes prediction data for an image to a CSV file, appending if the file exists.\n    It creates a dictionary with the image name, prediction, and confidence.\n    It opens the CSV file in append mode, writes the header if the file is new, and then writes the data as a new row.\"\"\"\n    \n    # Define the CSV file name\n    csv_file = 'predictions.csv'\n    \n    # Check if the file exists and if it is empty to decide on writing the header\n    file_exists = os.path.isfile(csv_file)\n    \n    # Define the fieldnames for the CSV\n    fieldnames = ['image_name', 'prediction', 'confidence']\n    \n    # Open the file in append mode, newline='' prevents adding extra newlines on Windows\n    with open(csv_file, 'a', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        # Write the header only if the file does not exist\n        if not file_exists:\n            writer.writeheader()\n        \n        # Create a dictionary with the data\n        data = {\n            'image_name': image_name,\n            'prediction': prediction,\n            'confidence': confidence\n        }\n        \n        # Write the data as a new row in the CSV file\n        writer.writerow(data)", "CodeBleu_score": 45.48100162540733}, {"code": "import csv\nimport os\n\ndef write_to_csv(image_name, prediction, confidence):\n    \"\"\"The function writes prediction data for an image to a CSV file, appending if the file exists.\n    It creates a dictionary with the image name, prediction, and confidence.\n    It opens the CSV file in append mode, writes the header if the file is new, and then writes the data as a new row.\"\"\"\n    \n    # Define the CSV file name\n    csv_file = 'predictions.csv'\n    \n    # Check if the file exists and if it is empty to decide on writing the header\n    file_exists = os.path.isfile(csv_file)\n    \n    # Define the fieldnames for the CSV\n    fieldnames = ['image_name', 'prediction', 'confidence']\n    \n    # Open the file in append mode, newline='' prevents adding extra newlines on Windows\n    with open(csv_file, 'a', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        # Write header only if the file does not exist\n        if not file_exists:\n            writer.writeheader()\n        \n        # Write the data row\n        writer.writerow({'image_name': image_name, 'prediction': prediction, 'confidence': confidence})", "CodeBleu_score": 44.50477793097318}, {"code": "import csv\nimport os\n\ndef write_to_csv(image_name, prediction, confidence):\n    \"\"\"The function writes prediction data for an image to a CSV file, appending if the file exists.\n    It creates a dictionary with the image name, prediction, and confidence.\n    It opens the CSV file in append mode, writes the header if the file is new, and then writes the data as a new row.\"\"\"\n    \n    # Define the CSV file name\n    csv_file = 'predictions.csv'\n    \n    # Check if the file exists and if it is empty to decide on writing the header\n    file_exists = os.path.isfile(csv_file)\n    \n    # Define the header\n    header = ['Image Name', 'Prediction', 'Confidence']\n    \n    # Open the file in append mode\n    with open(csv_file, 'a', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the header if the file is new\n        if not file_exists:\n            writer.writerow(header)\n        \n        # Write the data\n        writer.writerow([image_name, prediction, confidence])", "CodeBleu_score": 40.893840805674834}]}
{"instance_id": "436161602009", "generation_result": [{"code": "import torch\n\ndef export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\")):\n    \"\"\"\n    The function exports a PyTorch model to TorchScript format.\n    It logs the start of the export, traces the model with a given input, and includes extra files with model details.\n    If optimization is specified, it saves the model for mobile use; otherwise, it saves the standard TorchScript model.\n    The function returns the file path of the saved model and None.\n    \"\"\"\n    print(f\"{prefix} Exporting model to TorchScript...\")\n    \n    # Trace the model with the given input image\n    traced_model = torch.jit.trace(model, im)\n    \n    # Check if optimization is needed for mobile use\n    if optimize:\n        traced_model = torch.jit.optimize_for_mobile(traced_model)\n    \n    # Save the traced model to the specified file\n    traced_model.save(file)\n    \n    print(f\"{prefix} Model saved to {file}\")\n    \n    # Return the file path of the saved model\n    return file, None", "CodeBleu_score": 25.15456201110218}, {"code": "import torch\n\ndef export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\")):\n    \"\"\"\n    The function exports a PyTorch model to TorchScript format.\n    It logs the start of the export, traces the model with a given input, and includes extra files with model details.\n    If optimization is specified, it saves the model for mobile use; otherwise, it saves the standard TorchScript model.\n    The function returns the file path of the saved model and None.\n    \"\"\"\n    print(f\"{prefix} Exporting model to TorchScript...\")\n    \n    # Trace the model with the given input image\n    traced_model = torch.jit.trace(model, im)\n    \n    # Check if optimization is needed for mobile use\n    if optimize:\n        traced_model = torch.jit.optimize_for_mobile(traced_model)\n    \n    # Save the traced model to the specified file\n    traced_model.save(file)\n    \n    print(f\"{prefix} Model saved to {file}\")\n    \n    # Return the file path of the saved model\n    return file, None", "CodeBleu_score": 25.15456201110218}, {"code": "import torch\n\ndef export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\")):\n    \"\"\"\n    The function exports a PyTorch model to TorchScript format.\n    It logs the start of the export, traces the model with a given input, and includes extra files with model details.\n    If optimization is specified, it saves the model for mobile use; otherwise, it saves the standard TorchScript model.\n    The function returns the file path of the saved model and None.\n    \"\"\"\n    print(f\"{prefix} Exporting model to TorchScript...\")\n    \n    # Trace the model with the given input image\n    traced_model = torch.jit.trace(model, im)\n    \n    # Check if optimization is needed\n    if optimize:\n        # Optimize model for mobile if specified\n        traced_model = optimize_for_mobile(traced_model)\n    \n    # Save the traced model to a file\n    torch.jit.save(traced_model, file)\n    \n    # Optionally, you could include extra files or details here\n    # For example, saving model architecture or hyperparameters\n    # with open(file + \"_details.txt\", 'w') as f:\n    #     f.write(str(model))\n    \n    print(f\"{prefix} Model saved to {file}\")\n    \n    # Return the file path of the saved model\n    return file, None", "CodeBleu_score": 25.82455331863264}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "import coremltools as ct\nimport torch\n\ndef export_coreml(model, im, file, int8=False, half=False, nms=False, prefix=colorstr(\"CoreML:\")):\n    \"\"\"\n    The function exports a PyTorch model to CoreML format.\n    It checks for the coremltools requirement, logs the start of the export, and sets the file suffix to .mlmodel.\n    If specified, it wraps the model with an iOSModel for NMS support.\n    The function traces the model, converts it to CoreML, and applies quantization if needed and supported.\n    Finally, it saves the CoreML model and returns the file path and the CoreML model.\n    \"\"\"\n    print(f\"{prefix} Starting export...\")\n    file = file if file.endswith('.mlmodel') else file + '.mlmodel'\n\n    # Trace the model\n    traced_model = torch.jit.trace(model, im)\n\n    # Convert to CoreML\n    mlmodel = ct.convert(traced_model, inputs=[ct.ImageType()])\n\n    # Apply NMS if needed\n    if nms:\n        mlmodel = ct.models.neural_network.iOSModel(mlmodel).add_nms()\n\n    # Apply quantization if needed and supported\n    if int8:\n        mlmodel = mlmodel.quantize_weights(quantization_mode=ct.QuantizationMode.linear_quantization, nbits=8)\n    elif half:\n        mlmodel = mlmodel.quantize_weights(quantization_mode=ct.QuantizationMode.linear_quantization, nbits=16)\n\n    # Save the CoreML model\n    mlmodel.save(file)\n    print(f\"{prefix} Exported CoreML model to {file}\")\n\n    return file, mlmodel", "CodeBleu_score": 34.97541617853514}, {"code": "import coremltools as ct\nimport torch\n\ndef export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    \"\"\"\n    The function exports a PyTorch model to CoreML format.\n    It checks for the coremltools requirement, logs the start of the export, and sets the file suffix to .mlmodel.\n    If specified, it wraps the model with an iOSModel for NMS support.\n    The function traces the model, converts it to CoreML, and applies quantization if needed and supported.\n    Finally, it saves the CoreML model and returns the file path and the CoreML model.\n    \"\"\"\n    print(f\"{prefix} Starting export...\")\n    file = file if file.endswith('.mlmodel') else file + '.mlmodel'\n\n    # Check if NMS is required and wrap the model\n    if nms:\n        model = iOSModel(model)\n\n    # Trace the model\n    traced_model = torch.jit.trace(model, im)\n\n    # Convert to CoreML\n    coreml_model = ct.convert(traced_model, inputs=[ct.ImageType()])\n\n    # Apply quantization if needed\n    if int8:\n        coreml_model = coreml_model.quantize_weights(quantization_mode='linear', nbits=8)\n    elif half:\n        coreml_model = coreml_model.quantize_weights(quantization_mode='linear', nbits=16)\n\n    # Save the CoreML model\n    coreml_model.save(file)\n    print(f\"{prefix} Exported CoreML model to {file}\")\n\n    return file, coreml_model", "CodeBleu_score": 34.107946649026005}, {"code": "import coremltools as ct\nimport torch\n\ndef export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    \"\"\"\n    The function exports a PyTorch model to CoreML format.\n    It checks for the coremltools requirement, logs the start of the export, and sets the file suffix to .mlmodel.\n    If specified, it wraps the model with an iOSModel for NMS support.\n    The function traces the model, converts it to CoreML, and applies quantization if needed and supported.\n    Finally, it saves the CoreML model and returns the file path and the CoreML model.\n    \"\"\"\n    print(f\"{prefix} Starting export...\")\n    file = file if file.endswith('.mlmodel') else file + '.mlmodel'\n\n    # Check if NMS is needed and wrap the model\n    if nms:\n        model = iOSModel(model)\n\n    # Trace the model\n    traced_model = torch.jit.trace(model, im)\n\n    # Convert to CoreML\n    coreml_model = ct.convert(traced_model, inputs=[ct.ImageType()])\n\n    # Apply quantization if needed\n    if int8:\n        coreml_model = coreml_model.quantize_weights(8)\n    elif half:\n        coreml_model = coreml_model.quantize_weights(16)\n\n    # Save the CoreML model\n    coreml_model.save(file)\n    print(f\"{prefix} Exported CoreML model to {file}\")\n\n    return file, coreml_model", "CodeBleu_score": 31.57686888096568}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "import random\n\ndef generate_individual(input_ranges, individual_length):\n    \"\"\"\n    The function generates an individual with specified length using input ranges.\n    It iterates over the given length, and for each position, it appends a random float within the specified bounds to the individual list.\n    Finally, it returns the generated individual.\n    \"\"\"\n    individual = []\n    for _ in range(individual_length):\n        # Choose a random range from the input_ranges\n        chosen_range = random.choice(input_ranges)\n        # Generate a random float within the chosen range\n        random_value = random.uniform(chosen_range[0], chosen_range[1])\n        individual.append(random_value)\n    return individual", "CodeBleu_score": 40.76654998369077}, {"code": "import random\n\ndef generate_individual(input_ranges, individual_length):\n    \"\"\"\n    The function generates an individual with specified length using input ranges.\n    It iterates over the given length, and for each position, it appends a random float within the specified bounds to the individual list.\n    Finally, it returns the generated individual.\n    \n    Args:\n    input_ranges (list of tuples): Each tuple contains two floats (min, max), representing the range from which to generate a random float.\n    individual_length (int): The length of the individual to generate.\n    \n    Returns:\n    list: A list of floats representing the individual.\n    \"\"\"\n    individual = []\n    for _ in range(individual_length):\n        # Choose a random range from input_ranges\n        min_val, max_val = random.choice(input_ranges)\n        # Append a random float within the chosen range\n        individual.append(random.uniform(min_val, max_val))\n    \n    return individual", "CodeBleu_score": 57.91859746872956}, {"code": "import random\n\ndef generate_individual(input_ranges, individual_length):\n    \"\"\"\n    The function generates an individual with specified length using input ranges.\n    It iterates over the given length, and for each position, it appends a random float within the specified bounds to the individual list.\n    Finally, it returns the generated individual.\n\n    Args:\n    input_ranges (list of tuples): Each tuple contains two floats (min, max), representing the range of possible values for each element in the individual.\n    individual_length (int): The number of elements in the individual.\n\n    Returns:\n    list: The generated individual, a list of floats.\n    \"\"\"\n    individual = []\n    for _ in range(individual_length):\n        # Choose a random range from the input_ranges\n        min_val, max_val = random.choice(input_ranges)\n        # Generate a random float within the chosen range\n        value = random.uniform(min_val, max_val)\n        individual.append(value)\n    return individual", "CodeBleu_score": 51.74326229113898}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t):\n    \"\"\"\n    The function computes the log of the mean coefficient alpha_t for a given continuous-time label t.\n    Depending on the schedule type ('discrete', 'linear', or 'cosine'), it performs different calculations:\n    'interpolate for discrete schedule, use a quadratic expression for linear schedule, and apply a cosine-based formula for the cosine schedule.\n    It returns the computed log(alpha_t).\n    \"\"\"\n    import math\n\n    if self.schedule_type == 'discrete':\n        # Assuming `self.times` and `self.alphas` are defined and are lists of times and corresponding alphas\n        # Find the closest time index\n        idx = min(range(len(self.times)), key=lambda i: abs(self.times[i] - t))\n        alpha_t = self.alphas[idx]\n    elif self.schedule_type == 'linear':\n        # Assuming linear parameters `a` and `b` such that alpha_t = a * t^2 + b\n        a = self.linear_a\n        b = self.linear_b\n        alpha_t = a * t**2 + b\n    elif self.schedule_type == 'cosine':\n        # Assuming cosine parameters `A` and `B` such that alpha_t = A * cos(B * t) + 1\n        A = self.cosine_A\n        B = self.cosine_B\n        alpha_t = A * math.cos(B * t) + 1\n    else:\n        raise ValueError(\"Unsupported schedule type\")\n\n    return math.log(alpha_t)", "CodeBleu_score": 40.864132217860806}, {"code": "def marginal_log_mean_coeff(self, t):\n    \"\"\"\n    The function computes the log of the mean coefficient alpha_t for a given continuous-time label t.\n    Depending on the schedule type ('discrete', 'linear', or 'cosine'), it performs different calculations:\n    'interpolate for discrete schedule, use a quadratic expression for linear schedule, and apply a cosine-based formula for the cosine schedule.\n    It returns the computed log(alpha_t).\n    \"\"\"\n    import math\n\n    # Assuming self has attributes schedule_type and some parameters for calculations\n    if self.schedule_type == 'discrete':\n        # Example interpolation logic (simplified)\n        alpha_t = (1 - t) * self.alpha_start + t * self.alpha_end\n    elif self.schedule_type == 'linear':\n        # Example quadratic expression (simplified)\n        alpha_t = self.alpha_start + (self.alpha_end - self.alpha_start) * t**2\n    elif self.schedule_type == 'cosine':\n        # Example cosine-based formula (simplified)\n        alpha_t = 0.5 * (self.alpha_start + self.alpha_end + (self.alpha_end - self.alpha_start) * math.cos(math.pi * t))\n    else:\n        raise ValueError(\"Unsupported schedule type\")\n\n    return math.log(alpha_t)", "CodeBleu_score": 37.30890653383597}, {"code": "def marginal_log_mean_coeff(self, t):\n    \"\"\"\n    The function computes the log of the mean coefficient alpha_t for a given continuous-time label t.\n    Depending on the schedule type ('discrete', 'linear', or 'cosine'), it performs different calculations:\n    'interpolate for discrete schedule, use a quadratic expression for linear schedule, and apply a cosine-based formula for the cosine schedule.\n    It returns the computed log(alpha_t).\n    \"\"\"\n    import math\n\n    # Assuming self has attributes schedule_type and some necessary parameters for calculations\n    if self.schedule_type == 'discrete':\n        # Example interpolation logic (simplified)\n        alpha_t = (1 - t) * self.alpha_initial + t * self.alpha_final\n    elif self.schedule_type == 'linear':\n        # Example quadratic expression (simplified)\n        alpha_t = self.alpha_initial + (self.alpha_final - self.alpha_initial) * (t ** 2)\n    elif self.schedule_type == 'cosine':\n        # Example cosine-based formula (simplified)\n        alpha_t = 0.5 * (self.alpha_initial + self.alpha_final) + 0.5 * (self.alpha_initial - self.alpha_final) * math.cos(math.pi * t)\n    else:\n        raise ValueError(\"Unsupported schedule type\")\n\n    return math.log(alpha_t)", "CodeBleu_score": 37.37294330074599}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb):\n    \"\"\"\n    The function computes the continuous-time label t in [0, T] from a given half-logSNR lambda_t.\n    For a linear schedule, it calculates t using beta values and a temporary variable.\n    For a discrete schedule, it interpolates log_alpha to find t.\n    For other schedules, it computes log_alpha and uses an arccos function to derive t.\n    \"\"\"\n    if self.schedule_type == 'linear':\n        # Assuming beta_start and beta_end are defined\n        beta_t = (self.beta_end - self.beta_start) * lamb + self.beta_start\n        t = beta_t / (self.beta_end - self.beta_start) * self.T  # Normalize to [0, T]\n    elif self.schedule_type == 'discrete':\n        # Assuming log_alpha is a list or array of log_alpha values\n        # and we have a method to interpolate between them\n        t = np.interp(lamb, np.linspace(0, 1, len(self.log_alpha)), self.log_alpha)\n    else:\n        # Assuming other schedules use a specific formula involving arccos\n        log_alpha_t = np.arccos(np.exp(-lamb))  # Example placeholder\n        t = log_alpha_t / np.pi * self.T  # Normalize to [0, T] assuming a range of [0, pi]\n    \n    return t", "CodeBleu_score": 23.40969715984699}, {"code": "def inverse_lambda(self, lamb):\n    \"\"\"\n    The function computes the continuous-time label t in [0, T] from a given half-logSNR lambda_t.\n    For a linear schedule, it calculates t using beta values and a temporary variable.\n    For a discrete schedule, it interpolates log_alpha to find t.\n    For other schedules, it computes log_alpha and uses an arccos function to derive t.\n    \"\"\"\n    if self.schedule_type == 'linear':\n        # Placeholder for linear schedule computation\n        beta = self.get_beta()  # Assuming a method to get beta values\n        t = (lamb - self.lambda_0) / beta  # Example computation, adjust as needed\n    elif self.schedule_type == 'discrete':\n        # Placeholder for discrete schedule computation\n        log_alpha = self.interpolate_log_alpha(lamb)  # Assuming a method to interpolate log_alpha\n        t = self.compute_t_from_log_alpha(log_alpha)  # Example computation, adjust as needed\n    else:\n        # Placeholder for other types of schedules\n        log_alpha = self.compute_log_alpha(lamb)  # Assuming a method to compute log_alpha\n        t = np.arccos(np.exp(-log_alpha))  # Example using arccos, adjust as needed\n\n    return t", "CodeBleu_score": 22.95728962813363}, {"code": "def inverse_lambda(self, lamb):\n    \"\"\"\n    The function computes the continuous-time label t in [0, T] from a given half-logSNR lambda_t.\n    For a linear schedule, it calculates t using beta values and a temporary variable.\n    For a discrete schedule, it interpolates log_alpha to find t.\n    For other schedules, it computes log_alpha and uses an arccos function to derive t.\n    \"\"\"\n    if self.schedule_type == 'linear':\n        # Implement the calculation for a linear schedule\n        # Example: t = (lamb - self.beta_start) / (self.beta_end - self.beta_start) * self.T\n        pass\n    elif self.schedule_type == 'discrete':\n        # Implement the calculation for a discrete schedule\n        # Example: t = interpolate(self.log_alpha, lamb)\n        pass\n    else:\n        # Implement the calculation for other schedules\n        # Example: t = np.arccos(np.exp(-lamb)) * self.T / np.pi\n        pass\n\n    return t", "CodeBleu_score": 6.581669043722814}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "import numpy as np\n\ndef noise_pred_fn(x, t_continuous, cond=None):\n    \"\"\"\n    The function predicts noise for a given input based on the model type.\n    It ensures the time input is expanded to match the batch size, then processes the input through the model.\n    Depending on the model type, it applies different transformations to the model output using noise schedule parameters and returns the processed result.\n    \"\"\"\n    # Ensure t_continuous is expanded to match the batch size of x\n    if len(t_continuous.shape) == 1:\n        t_continuous = np.tile(t_continuous[:, None], (1, x.shape[1]))\n\n    # Placeholder for model processing\n    # Assuming 'model' is a predefined function or model instance\n    model_output = model(x, t_continuous, cond)\n\n    # Placeholder for transformation based on model type\n    # Assuming 'transform_noise' is a function that applies transformations based on the model type and noise parameters\n    processed_output = transform_noise(model_output, model_type='type1', noise_params={'param1': 0.1, 'param2': 0.2})\n\n    return processed_output\n\n# Example usage:\n# Assuming x is your input data, t_continuous is your time data, and cond is an optional condition\nx = np.random.randn(100, 10)  # Example input data\nt_continuous = np.linspace(0, 1, 100)  # Example time data\nresult = noise_pred_fn(x, t_continuous)", "CodeBleu_score": 23.842173379672204}, {"code": "import numpy as np\n\ndef noise_pred_fn(x, t_continuous, cond=None):\n    \"\"\"\n    The function predicts noise for a given input based on the model type.\n    It ensures the time input is expanded to match the batch size, then processes the input through the model.\n    Depending on the model type, it applies different transformations to the model output using noise schedule parameters and returns the processed result.\n    \"\"\"\n    # Ensure t_continuous is expanded to match the batch size of x\n    if len(t_continuous.shape) == 1:\n        t_continuous = np.expand_dims(t_continuous, axis=0)\n        t_continuous = np.repeat(t_continuous, repeats=x.shape[0], axis=0)\n\n    # Process the input through the model\n    # Assuming `model` is a predefined function or model instance\n    model_output = model(x, t_continuous, cond)\n\n    # Apply transformations based on the model type and noise schedule\n    # Assuming `apply_noise_schedule` is a function that modifies the model output\n    result = apply_noise_schedule(model_output, t_continuous, cond)\n\n    return result\n\ndef model(x, t, cond):\n    # Dummy model function for demonstration\n    return x * t  # This is just a placeholder\n\ndef apply_noise_schedule(output, t, cond):\n    # Dummy function to apply noise schedule transformations\n    return output * np.sin(t)  # This is just a placeholder", "CodeBleu_score": 23.51399275561759}, {"code": "import numpy as np\n\ndef noise_pred_fn(x, t_continuous, cond=None):\n    \"\"\"\n    The function predicts noise for a given input based on the model type.\n    It ensures the time input is expanded to match the batch size, then processes the input through the model.\n    Depending on the model type, it applies different transformations to the model output using noise schedule parameters and returns the processed result.\n    \"\"\"\n    # Ensure t_continuous has the same batch size as x\n    if len(t_continuous.shape) == 1:\n        t_continuous = np.expand_dims(t_continuous, axis=0)\n        t_continuous = np.repeat(t_continuous, x.shape[0], axis=0)\n\n    # Simulate model processing\n    # Assuming 'model' is a predefined model that takes x and t_continuous as inputs\n    model_output = model(x, t_continuous)\n\n    # Apply transformations based on the model type and condition\n    if cond == 'type1':\n        # Example transformation for type1\n        processed_output = model_output * np.sin(t_continuous)\n    elif cond == 'type2':\n        # Example transformation for type2\n        processed_output = model_output * np.log1p(t_continuous)\n    else:\n        # Default transformation\n        processed_output = model_output * t_continuous\n\n    return processed_output", "CodeBleu_score": 26.1668151335749}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "import torch\n\ndef dynamic_thresholding_fn(self, x0, t=None):\n    \"\"\"\n    The function applies dynamic thresholding to the input tensor x0.\n    It reshapes x0 to compute the quantile of absolute values, determines the scaling factor,\n    and clamps x0 values based on this factor.\n    The resulting tensor is then scaled and returned.\n    \"\"\"\n    # Compute the quantile of absolute values\n    if t is None:\n        t = 0.9  # Default quantile if not provided\n    quantile_value = torch.quantile(torch.abs(x0), t)\n\n    # Determine the scaling factor\n    scaling_factor = quantile_value\n\n    # Clamp the values based on the scaling factor\n    clamped_values = torch.clamp(x0, min=-scaling_factor, max=scaling_factor)\n\n    # Scale the tensor\n    scaled_tensor = clamped_values / scaling_factor\n\n    return scaled_tensor", "CodeBleu_score": 26.486356750570174}, {"code": "import torch\n\ndef dynamic_thresholding_fn(self, x0, t=None):\n    \"\"\"\n    The function applies dynamic thresholding to the input tensor x0.\n    It reshapes x0 to compute the quantile of absolute values, determines the scaling factor,\n    and clamps x0 values based on this factor.\n    The resulting tensor is then scaled and returned.\n    \"\"\"\n    # Compute the quantile of absolute values\n    if t is None:\n        t = 0.9  # Default quantile if not provided\n    quantile_value = torch.quantile(torch.abs(x0), t)\n\n    # Determine the scaling factor\n    scaling_factor = quantile_value / torch.max(torch.abs(x0))\n\n    # Clamp the values based on the scaling factor\n    clamped_x0 = torch.clamp(x0, min=-quantile_value, max=quantile_value)\n\n    # Scale the tensor\n    scaled_x0 = clamped_x0 * scaling_factor\n\n    return scaled_x0", "CodeBleu_score": 29.155501488118208}, {"code": "import torch\n\ndef dynamic_thresholding_fn(self, x0, t=None):\n    \"\"\"\n    The function applies dynamic thresholding to the input tensor x0.\n    It reshapes x0 to compute the quantile of absolute values, determines the scaling factor,\n    and clamps x0 values based on this factor.\n    The resulting tensor is then scaled and returned.\n    \"\"\"\n    if t is None:\n        t = 0.9  # Default quantile threshold if not provided\n\n    # Compute the quantile of absolute values\n    quantile_value = torch.quantile(torch.abs(x0), t)\n\n    # Determine the scaling factor (this could vary depending on specific requirements)\n    scaling_factor = quantile_value\n\n    # Clamp the values in x0 based on the scaling factor\n    clamped_x0 = torch.clamp(x0, min=-scaling_factor, max=scaling_factor)\n\n    # Scale the tensor (assuming scaling is by the quantile value itself)\n    scaled_x0 = clamped_x0 / scaling_factor\n\n    return scaled_x0", "CodeBleu_score": 26.67492682988495}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "import torch\n\ndef data_prediction_fn(self, x, t):\n    \"\"\"\n    The function applies dynamic thresholding to the input tensor x.\n    It calculates the threshold value based on the dynamic thresholding ratio and maximum threshold value,\n    then clamps the input tensor values within this threshold.\n    Finally, it normalizes the clamped values and returns the processed tensor.\n    \"\"\"\n    # Assuming t is the dynamic thresholding ratio and there's a predefined max_threshold\n    max_threshold = 1.0  # Example maximum threshold value\n    dynamic_threshold = t * max_threshold\n    \n    # Clamp the values of x within the range [0, dynamic_threshold]\n    clamped_x = torch.clamp(x, min=0, max=dynamic_threshold)\n    \n    # Normalize the clamped tensor\n    normalized_x = (clamped_x - torch.min(clamped_x)) / (torch.max(clamped_x) - torch.min(clamped_x))\n    \n    return normalized_x", "CodeBleu_score": 18.555150880985526}, {"code": "import torch\n\ndef data_prediction_fn(self, x, t):\n    \"\"\"\n    The function applies dynamic thresholding to the input tensor x.\n    It calculates the threshold value based on the dynamic thresholding ratio and maximum threshold value,\n    then clamps the input tensor values within this threshold.\n    Finally, it normalizes the clamped values and returns the processed tensor.\n    \n    Args:\n    x (Tensor): The input tensor.\n    t (float): The dynamic thresholding ratio.\n    \n    Returns:\n    Tensor: The processed tensor.\n    \"\"\"\n    # Assuming max_threshold is a predefined constant or could be another parameter\n    max_threshold = 1.0  # Example value, adjust as necessary\n    \n    # Calculate dynamic threshold\n    dynamic_threshold = t * max_threshold\n    \n    # Clamp the values of x within the range [0, dynamic_threshold]\n    clamped_x = torch.clamp(x, min=0, max=dynamic_threshold)\n    \n    # Normalize the clamped tensor\n    # Here we assume normalization to the range [0, 1]\n    # This could be adjusted based on specific requirements\n    if clamped_x.max() > 0:\n        normalized_x = clamped_x / clamped_x.max()\n    else:\n        normalized_x = clamped_x\n    \n    return normalized_x", "CodeBleu_score": 16.917851862032837}, {"code": "import torch\n\ndef data_prediction_fn(self, x, t):\n    \"\"\"\n    The function applies dynamic thresholding to the input tensor x.\n    It calculates the threshold value based on the dynamic thresholding ratio and maximum threshold value,\n    then clamps the input tensor values within this threshold.\n    Finally, it normalizes the clamped values and returns the processed tensor.\n    \n    Args:\n    x (torch.Tensor): The input tensor.\n    t (float): The dynamic thresholding ratio.\n    \n    Returns:\n    torch.Tensor: The processed tensor.\n    \"\"\"\n    # Assuming max_threshold is a predefined constant or could be another parameter\n    max_threshold = 1.0  # Example value, adjust as necessary\n    \n    # Calculate dynamic threshold\n    dynamic_threshold = t * max_threshold\n    \n    # Clamp the tensor values\n    clamped_tensor = torch.clamp(x, min=-dynamic_threshold, max=dynamic_threshold)\n    \n    # Normalize the clamped tensor\n    # Here we assume normalization to the range [0, 1]\n    # This could be adjusted depending on specific requirements\n    normalized_tensor = (clamped_tensor + dynamic_threshold) / (2 * dynamic_threshold)\n    \n    return normalized_tensor", "CodeBleu_score": 14.895303241316585}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs):\n    \"\"\"\n    The function predicts epsilon given a model, input, and sigma.\n    It reshapes sigma to match the dimensions of the input, scales the input,\n    and then computes the prediction by adjusting the model output with the scaled input and sigma.\n    Finally, it returns the adjusted prediction.\n    \"\"\"\n    # Reshape sigma to match the input dimensions\n    sigma_reshaped = sigma_in.reshape(input.shape)\n    \n    # Scale the input (example scaling, adjust as necessary)\n    scaled_input = input * kwargs.get('scale_factor', 1.0)\n    \n    # Compute the model prediction\n    prediction = model.predict(scaled_input)\n    \n    # Adjust the prediction using sigma\n    adjusted_prediction = prediction + sigma_reshaped\n    \n    return adjusted_prediction", "CodeBleu_score": 27.772631817226394}, {"code": "import numpy as np\n\ndef predict_eps_sigma(model, input, sigma_in, **kwargs):\n    \"\"\"\n    The function predicts epsilon given a model, input, and sigma.\n    It reshapes sigma to match the dimensions of the input, scales the input,\n    and then computes the prediction by adjusting the model output with the scaled input and sigma.\n    Finally, it returns the adjusted prediction.\n    \n    Parameters:\n        model: A machine learning model that has a predict method.\n        input: Input data (numpy array or similar).\n        sigma_in: Sigma values (numpy array or similar) to be reshaped and used in prediction adjustment.\n        **kwargs: Additional keyword arguments for model prediction or preprocessing steps.\n\n    Returns:\n        numpy array: Adjusted prediction.\n    \"\"\"\n    # Reshape sigma to match the input dimensions\n    sigma_reshaped = np.reshape(sigma_in, input.shape)\n    \n    # Scale the input (example scaling, adjust as necessary)\n    scaled_input = input * 0.5  # Example scaling factor\n    \n    # Compute the model prediction\n    raw_prediction = model.predict(scaled_input, **kwargs)\n    \n    # Adjust the prediction using the reshaped sigma\n    adjusted_prediction = raw_prediction + sigma_reshaped\n    \n    return adjusted_prediction", "CodeBleu_score": 28.808818954713804}, {"code": "import numpy as np\n\ndef predict_eps_sigma(model, input, sigma_in, **kwargs):\n    \"\"\"\n    The function predicts epsilon given a model, input, and sigma.\n    It reshapes sigma to match the dimensions of the input, scales the input,\n    and then computes the prediction by adjusting the model output with the scaled input and sigma.\n    Finally, it returns the adjusted prediction.\n    \"\"\"\n    # Reshape sigma to match the input dimensions\n    sigma_reshaped = np.reshape(sigma_in, input.shape)\n    \n    # Scale the input (example scaling, adjust as necessary)\n    scaled_input = input * sigma_reshaped\n    \n    # Compute the model prediction\n    prediction = model.predict(scaled_input, **kwargs)\n    \n    # Adjust the prediction by some function of sigma (example adjustment)\n    adjusted_prediction = prediction * sigma_reshaped\n    \n    return adjusted_prediction", "CodeBleu_score": 29.70855529064683}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "import torch\n\ndef cal_intergrand(beta_0, beta_1, taus):\n    \"\"\"\n    The function calculates the integrand for given parameters beta_0, beta_1, and taus.\n    It operates within a gradient-enabled context, cloning and setting the required gradients for taus.\n    It computes alpha and its logarithm, performs backpropagation to obtain the gradient of log_alpha with respect to taus,\n    and finally calculates the integrand using this gradient and alpha values.\n    \"\"\"\n    # Ensure taus is a tensor with gradient tracking\n    taus = taus.clone().detach().requires_grad_(True)\n    \n    # Compute alpha\n    alpha = beta_0 + beta_1 * taus\n    \n    # Compute the logarithm of alpha\n    log_alpha = torch.log(alpha)\n    \n    # Compute the gradient of log_alpha with respect to taus\n    log_alpha.backward(torch.ones_like(log_alpha))\n    \n    # Retrieve the gradient from taus\n    grad_log_alpha = taus.grad\n    \n    # Calculate the integrand\n    integrand = grad_log_alpha * alpha\n    \n    return integrand", "CodeBleu_score": 32.890918020751734}, {"code": "import torch\n\ndef cal_intergrand(beta_0, beta_1, taus):\n    \"\"\"\n    The function calculates the integrand for given parameters beta_0, beta_1, and taus.\n    It operates within a gradient-enabled context, cloning and setting the required gradients for taus.\n    It computes alpha and its logarithm, performs backpropagation to obtain the gradient of log_alpha with respect to taus,\n    and finally calculates the integrand using this gradient and alpha values.\n    \"\"\"\n    # Ensure taus is a tensor with gradient tracking\n    taus = taus.clone().detach().requires_grad_(True)\n    \n    # Calculate alpha\n    alpha = beta_0 + beta_1 * torch.exp(taus)\n    \n    # Calculate the logarithm of alpha\n    log_alpha = torch.log(alpha)\n    \n    # Compute gradients of log_alpha with respect to taus\n    log_alpha.backward(torch.ones_like(log_alpha))\n    \n    # Extract the gradients\n    grad_log_alpha = taus.grad\n    \n    # Calculate the integrand\n    integrand = alpha * grad_log_alpha\n    \n    return integrand", "CodeBleu_score": 35.66780338065563}, {"code": "import torch\n\ndef cal_intergrand(beta_0, beta_1, taus):\n    # Ensure taus is a tensor and clone it to avoid modifying the original tensor\n    taus = taus.clone().detach().requires_grad_(True)\n    \n    # Compute alpha using the given beta_0 and beta_1\n    alpha = beta_0 + beta_1 * taus\n    \n    # Compute the logarithm of alpha\n    log_alpha = torch.log(alpha)\n    \n    # Compute the gradient of log_alpha with respect to taus\n    log_alpha.backward(torch.ones_like(taus))\n    \n    # Retrieve the gradient from taus\n    grad_log_alpha = taus.grad\n    \n    # Calculate the integrand\n    integrand = grad_log_alpha * alpha\n    \n    return integrand", "CodeBleu_score": 32.598769650851054}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"\n    The function performs ancestral sampling using the Euler method.\n    It initializes extra arguments and a noise sampler if not provided, and iterates over the sigmas.\n    In each step, it denoises the input, calculates the ancestral step, and updates the input using the Euler method.\n    Optionally, it calls a callback function with the current state.\n    Finally, it adds noise if required and returns the sampled output.\n    \"\"\"\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda size: np.random.normal(0, 1, size)\n\n    sampled_output = x.copy()\n\n    for sigma in sigmas:\n        # Denoise the input\n        denoised_input = model.denoise(sampled_output, sigma, **extra_args)\n\n        # Calculate the ancestral step\n        ancestral_step = eta * (sampled_output - denoised_input) / sigma\n\n        # Update the input using the Euler method\n        sampled_output += ancestral_step\n\n        # Optionally call the callback with the current state\n        if callback is not None:\n            callback(sampled_output)\n\n        # Add noise if required\n        if disable is None or not disable:\n            noise = noise_sampler(sampled_output.shape) * s_noise * sigma\n            sampled_output += noise\n\n    return sampled_output", "CodeBleu_score": 30.387980441760217}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"\n    The function performs ancestral sampling using the Euler method.\n    It initializes extra arguments and a noise sampler if not provided, and iterates over the sigmas.\n    In each step, it denoises the input, calculates the ancestral step, and updates the input using the Euler method.\n    Optionally, it calls a callback function with the current state.\n    Finally, it adds noise if required and returns the sampled output.\n    \"\"\"\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda size: np.random.normal(0, 1, size)\n\n    sampled_output = x.copy()\n\n    for sigma in sigmas:\n        # Denoise the input\n        denoised_input = model.denoise(sampled_output, sigma, **extra_args)\n\n        # Calculate the ancestral step\n        noise = noise_sampler(sampled_output.shape)\n        ancestral_step = eta * (sampled_output - denoised_input) / sigma + s_noise * noise\n\n        # Update the input using the Euler method\n        sampled_output += ancestral_step\n\n        # Optionally call the callback with the current state\n        if callback is not None:\n            callback(sampled_output, sigma)\n\n        # Optionally add noise\n        if disable is None or not disable:\n            sampled_output += noise_sampler(sampled_output.shape)\n\n    return sampled_output", "CodeBleu_score": 28.791576816768277}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"\n    The function performs ancestral sampling using the Euler method.\n    It initializes extra arguments and a noise sampler if not provided, and iterates over the sigmas.\n    In each step, it denoises the input, calculates the ancestral step, and updates the input using the Euler method.\n    Optionally, it calls a callback function with the current state.\n    Finally, it adds noise if required and returns the sampled output.\n    \"\"\"\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda size: np.random.normal(0, 1, size)\n\n    sampled_output = x.copy()\n\n    for sigma in sigmas:\n        # Denoise the input\n        denoised_input = model.denoise(sampled_output, sigma, **extra_args)\n\n        # Calculate the ancestral step\n        ancestral_step = eta * (sampled_output - denoised_input) / sigma\n\n        # Update the input using the Euler method\n        sampled_output += ancestral_step\n\n        # Optionally call the callback with the current state\n        if callback is not None:\n            callback(sampled_output, sigma)\n\n        # Add noise if required\n        if disable is None or not disable:\n            noise = noise_sampler(sampled_output.shape) * s_noise * sigma\n            sampled_output += noise\n\n    return sampled_output", "CodeBleu_score": 29.628164122979943}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "import numpy as np\nfrom scipy.integrate import quad\n\ndef linear_multistep_coeff(order, t, i, j):\n    \"\"\"\n    Calculates the linear multistep coefficient for a given order of the numerical method,\n    time steps array t, current step index i, and coefficient index j.\n\n    Parameters:\n    - order (int): The order of the numerical method.\n    - t (list or numpy array): Array of time steps.\n    - i (int): Current index of the time step for which the coefficient is computed.\n    - j (int): Index of the coefficient.\n\n    Raises:\n    - ValueError: If j >= order or j < 0 or if i is out of valid array index range.\n\n    Returns:\n    - float: The computed linear multistep coefficient.\n    \"\"\"\n    if j >= order or j < 0:\n        raise ValueError(\"j must be within 0 and order-1\")\n    if i >= len(t) - 1 or i < 0:\n        raise ValueError(\"Index i must be within 0 and len(t)-2\")\n\n    def product_except_j(x):\n        \"Computes the product (x - t[m]) for all m != j within the given interval.\"\n        prod = 1\n        for m in range(order):\n            if m != j:\n                prod *= (x - t[m])\n        return prod\n    \n    # Computes the integral of the product from t[i] to t[i + 1]\n    integral, _ = quad(product_except_j, t[i], t[i+1])\n\n    # Factor from the analytical derivation of linear multistep coefficients\n    factorial_j = np.math.factorial(j)\n    factorial_order_minus_j_minus_1 = np.math.factorial(order - j - 1)\n\n    coefficient = integral / (factorial_j * factorial_order_minus_j_minus_1)\n    return coefficient", "CodeBleu_score": 28.9843768988297}, {"code": "import numpy as np\n\ndef linear_multistep_coeff(order, t, i, j):\n    \"\"\"The function calculates the linear multistep coefficient for a given order, time steps, and indices i and j.\n    It raises an error if the order is too high for the step index.\n    A nested function computes the product term for the coefficient, excluding the j-th term.\n    The function then integrates this nested function over the interval between the i-th and (i+1)-th time steps and returns the result.\"\"\"\n    \n    # Validate inputs\n    if order > len(t) - 1:\n        raise ValueError(\"Order too high for given time steps\")\n    \n    def product_term_exclude_j(s):\n        \"\"\"Helper function to compute the product terms excluding the j-th term.\"\"\"\n        result = 1\n        for k in range(order):\n            if k != j:\n                result *= (s - t[i-k])\n        return result\n    \n    # Integrate the nested function over the interval [t[i], t[i+1]]\n    def trapezoidal_integration(func, a, b, n=1000):\n        \"\"\"Trapezoidal rule for numerical integration.\"\"\"\n        h = (b - a) / float(n)\n        result = 0.5 * (func(a) + func(b))\n        for k in range(1, n):\n            result += func(a + k * h)\n        result *= h\n        return result\n\n    # The calculation interval is between t[i] and t[i+1]\n    if i+1 >= len(t):\n        raise IndexError(\"Index i+1 is out of bounds for time steps array\")\n    \n    result = trapezoidal_integration(lambda s: product_term_exclude_j(s), t[i], t[i+1])\n    return result\n\n# Example usage:\nt = np.linspace(0, 10, 6)  # Time steps from 0 to 10, total 6 steps (order can be up to 5 if considering all)\norder = 4\ni = 2\nj = 1\n\ntry:\n    coeff = linear_multistep_coeff(order, t, i, j)\n    print(\"Coefficient:\", coeff)\nexcept Exception as e:\n    print(\"Error:\", e)", "CodeBleu_score": 28.838670657264387}, {"code": "import numpy as np\nfrom scipy.integrate import quad\n\ndef linear_multistep_coeff(order, t, i, j):\n    \"\"\"\n    Calculates the linear multistep coefficient for a given order, time steps, and indices i and j.\n    It raises an error if the order is too high for the step index.\n    A nested function computes the product term for the coefficient, excluding the j-th term.\n    The function then integrates this nested function over the interval between the i-th and (i+1)-th time steps\n    and returns the result.\n\n    Parameters:\n        order (int): The order of the polynomial.\n        t (array): Array of time steps.\n        i (int): Index used to set the integration interval.\n        j (int): Index used to define which term to exclude in the Lagrange polynomial.\n\n    Returns:\n        float: The integrated value of the polynomial over the specified interval.\n    \"\"\"\n    \n    if (i + 1) >= len(t) or j >= order or order > len(t):\n        raise ValueError(\"Index out of range or order too high for number of time steps\")\n    \n    # Definition of the nested function for the Lagrange polynomial excluding j-th term:\n    def L_k_except_j(x):\n        \"\"\"\n        Lagrange polynomial L_k(x) where k is [0, order] but excluding j.\n        \"\"\"\n        L = 1\n        for k in range(order):\n            if k != j:\n                L *= (x - t[k]) / (t[j] - t[k]) if t[j] != t[k] else 1  # Avoid division by zero\n        return L\n\n    # Integration of this function over the interval [t_i, t_(i+1)]\n    result, error = quad(L_k_except_j, t[i], t[i + 1])\n    \n    return result\n\n# Example usage\nt = np.linspace(0, 1, 5)  # time steps from 0 to 1, total 5 steps\ni = 1  # interval from t[1] to t[2]\nj = 1  # exclude index 1 in polynomial computation\norder = 4  # polynomial order\nprint(linear_multistep_coeff(order, t, i, j))", "CodeBleu_score": 29.905564818496643}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    \"\"\"\n    The function performs latent code manipulation (LCM) sampling using a model and a set of sigmas.\n    It initializes extra arguments and a noise sampler if not provided, then iterates over the sigmas,\n    applying the model to the input and updating it with denoised values.\n    A callback function can be executed at each step to monitor progress.\n    If the next sigma is greater than zero, noise scaling is applied to the updated input.\n    The function returns the final sampled value.\n\n    Parameters:\n        model (callable): A function or model that takes an input x, a sigma, additional arguments, and noise\n                          to produce a manipulated output.\n        x (np.array): The initial input sample that will be manipulated.\n        sigmas (list of float): A sequence of standard deviations used for the scaling of noise.\n        extra_args (dict, optional): Extra arguments to pass to the model.\n        callback (callable, optional): A function to be called at each iteration, often for logging or monitoring.\n        disable (bool, optional): If set to True, disables any progress logging typically done during the iterations.\n        noise_sampler (callable, optional): A function to sample noise, should take the shape of x and a sigma as input\n                                            and return the sampled noise.\n\n    Returns:\n        np.array: The final manipulated sample.\n    \"\"\"\n    import numpy as np\n\n    if extra_args is None:\n        extra_args = {}\n\n    if noise_sampler is None:\n        noise_sampler = lambda shape, sigma: np.random.normal(0, sigma, shape)\n\n    # Iterate over the provided sigmas\n    for i in range(len(sigmas)):\n        sigma = sigmas[i]\n        noise = noise_sampler(x.shape, sigma)\n        x = model(x, sigma, extra_args, noise)\n\n        # Execute the callback function if provided\n        if callback is not None:\n            callback(i, x, sigma)\n\n        # Prepare for next iteration\n        if i < len(sigmas) - 1:\n            next_sigma = sigmas[i + 1]\n            if next_sigma > 0 and sigma > 0:\n                scaling_factor = next_sigma / sigma\n                noise = noise_sampler(x.shape, scaling_factor * sigma)\n                x += noise  # Apply scaled noise\n        \n        if disable:\n            continue  # Skip any logging or progress updates if disabled\n\n    return x", "CodeBleu_score": 36.03533920694364}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    \"\"\"\n    The function performs latent code manipulation (LCM) sampling using a model and\n    a set of sigmas. It initializes extra arguments and a noise sampler if not provided,\n    then iterates over the sigmas, applying the model to the input and updating it with\n    denoised values. A callback function can be executed at each step to monitor progress.\n    If the next sigma is greater than zero, noise scaling is applied to the updated input.\n    The function returns the final sampled value.\n    \n    Parameters:\n        model (callable): A function that models the denoising process.\n        x (np.ndarray): Initial latent code.\n        sigmas (list[float]): A list of noise levels to sample through.\n        extra_args (dict, optional): Additional arguments to pass to the model.\n        callback (callable, optional): Function to call after each step, typically used for monitoring.\n        disable (bool, optional): Flag to disable callback functionality, default to False if not provided.\n        noise_sampler (callable, optional): Function to sample noise given a shape and sigma, defaults to Gaussian if not provided.\n\n    Returns:\n        np.ndarray: The final sampled latent code after processing.\n    \"\"\"\n\n    if extra_args is None:\n        extra_args = {}\n\n    if noise_sampler is None:\n        noise_sampler = lambda shape, sigma: np.random.normal(size=shape) * sigma\n\n    # Iterate over each sigma in the list\n    for i, sigma in enumerate(sigmas):\n        # Running the model and updating 'x'\n        denoised = model(x, sigma, **extra_args)\n        x = x + denoised\n\n        # Optionally calling the callback function\n        if callback is not None and (disable is None or not disable):\n            callback(x, i)\n\n        # If more sigmas left, apply noise based on next sigma\n        if i < len(sigmas) - 1:\n            next_sigma = sigmas[i + 1]\n            if next_sigma > 0:\n                x += noise_sampler(x.shape, next_sigma)\n\n    return x", "CodeBleu_score": 36.87079339973415}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\ndef sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    \"\"\"\n    The function performs latent code manipulation (LCM) sampling using a model and a set of sigmas.\n    It initializes extra arguments and a noise sampler if not provided, then iterates over the sigmas,\n    applying the model to the input and updating it with denoised values.\n    A callback function can be executed at each step to monitor progress.\n    If the next sigma is greater than zero, noise scaling is applied to the updated input.\n    The function returns the final sampled value.\n\n    Args:\n    - model (callable): A function or a model that takes an input, sigma (and possibly additional arguments) and \n                        returns a \"denoised\" version of the input.\n    - x (ndarray): Initial input data to be manipulated.\n    - sigmas (list or array): Sequence of standard deviations used for the noise scheduling in LCM.\n    - extra_args (dict, optional): Additional arguments to be passed to the model. Defaults to None.\n    - callback (callable, optional): A function to be called after each manipulation step, useful for monitoring. Defaults to None.\n    - disable (bool, optional): If True, disables any progress display such as progress bars. Defaults to None.\n    - noise_sampler (callable, optional): A function to generate noise based on input shape and sigma. Defaults to standard Gaussian noise.\n\n    Returns:\n    - ndarray: The final denoised/manipulated sample.\n    \"\"\"\n    \n    import numpy as np\n\n    if extra_args is None:\n        extra_args = {}\n   \n    if noise_sampler is None:\n        noise_sampler = lambda shape, sigma: np.random.normal(0, sigma, shape)\n        \n    x_current = np.array(x)\n\n    for i, sigma in enumerate(sigmas[:-1]):\n        x_denoised = model(x_current, sigma, **extra_args)\n        \n        if callback is not None:\n            callback(i, x_denoised)\n        \n        next_sigma = sigmas[i+1]\n        \n        if next_sigma > 0:\n            noise = noise_sampler(x_current.shape, next_sigma)\n            x_current = x_denoised + noise\n        else:\n            x_current = x_denoised\n\n        if disable:\n            pass  # In a non-CLI environment, you might handle logging or actual progress disabling here\n    \n    # Process the last sigma separately without adding noise afterwards\n    x_final = model(x_current, sigmas[-1], **extra_args)\n\n    if callback is not None:\n        callback(len(sigmas) - 1, x_final)\n\n    return x_final", "CodeBleu_score": 35.34840031604202}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "import torch\n\ndef append_dims(x, target_dims):\n    \"\"\"\n    The function appends dimensions to a tensor until it reaches the specified target dimensions.\n    It calculates the number of dimensions to append and raises an error if the target dimensions\n    are fewer than the current dimensions.\n    It expands the tensor by adding new dimensions at the end and returns a detached clone if the\n    tensor is on an 'mps' device, otherwise it returns the expanded tensor.\n\n    Parameters:\n        x (torch.Tensor): The input tensor.\n        target_dims (int): The target number of dimensions.\n\n    Returns:\n        torch.Tensor: The expanded tensor.\n\n    Raises:\n        ValueError: If target_dims is less than the number of dimensions of x.\n    \"\"\"\n\n    current_dims = x.dim()\n    if target_dims < current_dims:\n        raise ValueError(\"target_dims must be greater than or equal to the number of dimensions of x\")\n\n    # Calculate how many new dimensions are needed.\n    needed_dims = target_dims - current_dims\n    \n    # Calculate new shape with additional dimensions at the end of the shape\n    # Each new dimension has a size of 1.\n    new_shape = x.shape + (1,) * needed_dims\n    \n    # Expand the tensor to the new shape.\n    expanded_tensor = x.reshape(new_shape)\n\n    # Check if the tensor's device is 'mps' and detach if needed\n    if x.device.type == 'mps':\n        return expanded_tensor.detach()\n    else:\n        return expanded_tensor", "CodeBleu_score": 35.61268838777442}, {"code": "import torch\n\ndef append_dims(x, target_dims):\n    \"\"\"\n    The function appends dimensions to a tensor until it reaches the specified target dimensions.\n    It calculates the number of dimensions to append and raises an error if the target dimensions are\n    fewer than the current dimensions.\n    It expands the tensor by adding new dimensions at the end and returns a detached clone if the tensor\n    is on an 'mps' device, otherwise, it returns the expanded tensor.\n\n    Parameters:\n    - x (torch.Tensor): The input tensor.\n    - target_dims (int): The target number of dimensions the tensor should have.\n\n    Returns:\n    - torch.Tensor: The expanded tensor.\n\n    Raises:\n    - ValueError: If target_dims is less than the current number of dimensions of x.\n    \"\"\"\n\n    # Check the current number of dimensions\n    current_dims = x.dim()\n\n    # Validate the target dimensions\n    if target_dims < current_dims:\n        raise ValueError(\"target_dims must be greater than or equal to x dimensions\")\n\n    # Calculate how many dimensions need to be added\n    dims_to_add = target_dims - current_dims\n\n    # Expand the dimensions (adding singleton dimensions at the end)\n    shape_to_add = tuple([1] * dims_to_add)  # Create a tuple of `1`s of length dims_to_add\n    new_shape = x.shape + shape_to_add       # Append the singleton dimensions to the existing shape\n\n    # Create a new tensor with expanded dimensions\n    expanded_tensor = x.reshape(new_shape)\n\n    # Clone and detach if on an MPS device\n    if x.device.type == 'mps':\n        return expanded_tensor.clone().detach()\n    else:\n        return expanded_tensor", "CodeBleu_score": 34.943325644644986}, {"code": "import torch\n\ndef append_dims(x, target_dims):\n    \"\"\"\n    The function appends dimensions to a tensor until it reaches the specified target dimensions.\n    It calculates the number of dimensions to append and raises an error if the target dimensions are fewer than the current dimensions.\n    It expands the tensor by adding new dimensions at the end and returns a detached clone if the tensor is on an 'mps' device, otherwise it returns the expanded tensor.\n    \n    Parameters:\n        x (torch.Tensor): The input tensor.\n        target_dims (int): The desired number of dimensions for the tensor.\n        \n    Returns:\n        torch.Tensor: The tensor with the number of dimensions expanded to target_dims. If the tensor is on 'mps' device, a detached clone is returned.\n        \n    Raises:\n        ValueError: If target_dims is less than the current number of dimensions of x.\n    \"\"\"\n    current_dims = x.dim()\n    if current_dims > target_dims:\n        raise ValueError(\"target_dims must be greater than or equal to the current dimensions of the tensor\")\n\n    # Calculate the number of dimensions we need to add\n    dims_to_add = target_dims - current_dims\n    \n    # Generate a tuple of size dims_to_add containing ones; these will be the new shape dimensions to add\n    shape_to_add = (1,) * dims_to_add\n    \n    # Use torch.Tensor.view to expand dimensions\n    expanded_x = x.view(*x.shape, *shape_to_add)\n    \n    # Check if the tensor is on an 'MPS' device, and clone and detach if it is\n    if x.device.type == 'mps':\n        expanded_x = expanded_x.clone().detach()\n    \n    return expanded_x", "CodeBleu_score": 37.11563142184451}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "import os\nimport hashlib\nimport requests\n\ndef download_file(path, url, digest=None):\n    \"\"\"\n    The function downloads a file from a given URL to a specified path if it does not already exist, optionally verifying its SHA-256 hash.\n    It ensures the target directory exists, downloads the file if necessary, and checks the file's hash against the provided digest, raising an error if they do not match.\n    The function returns the path to the downloaded file.\n    \"\"\"\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    \n    # Download the file if it does not exist\n    if not os.path.exists(path):\n        response = requests.get(url)\n        response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n        with open(path, 'wb') as f:\n            f.write(response.content)\n    \n    # Verify the SHA-256 digest if provided\n    if digest:\n        sha256 = hashlib.sha256()\n        with open(path, 'rb') as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                sha256.update(chunk)\n        file_digest = sha256.hexdigest()\n        if file_digest != digest:\n            raise ValueError(\"The downloaded file's hash does not match the provided digest.\")\n    \n    return path", "CodeBleu_score": 43.30400234398974}, {"code": "import os\nimport hashlib\nimport requests\n\ndef download_file(path, url, digest=None):\n    \"\"\"\n    The function downloads a file from a given URL to a specified path if it does not already exist, optionally verifying its SHA-256 hash.\n    It ensures the target directory exists, downloads the file if necessary, and checks the file's hash against the provided digest, raising an error if they do not match.\n    The function returns the path to the downloaded file.\n    \"\"\"\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    \n    # Download the file if it does not exist\n    if not os.path.exists(path):\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Will raise an HTTPError for bad responses\n        with open(path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n    \n    # Verify the SHA-256 digest if provided\n    if digest:\n        sha256 = hashlib.sha256()\n        with open(path, 'rb') as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                sha256.update(chunk)\n        file_digest = sha256.hexdigest()\n        if file_digest != digest:\n            raise ValueError(\"The downloaded file's hash does not match the provided digest.\")\n    \n    return path", "CodeBleu_score": 40.596145847746534}, {"code": "import os\nimport hashlib\nimport requests\n\ndef download_file(path, url, digest=None):\n    \"\"\"\n    The function downloads a file from a given URL to a specified path if it does not already exist, optionally verifying its SHA-256 hash.\n    It ensures the target directory exists, downloads the file if necessary, and checks the file's hash against the provided digest, raising an error if they do not match.\n    The function returns the path to the downloaded file.\n    \"\"\"\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    \n    # Download the file if it does not exist\n    if not os.path.exists(path):\n        response = requests.get(url)\n        response.raise_for_status()  # Will raise an HTTPError for bad responses\n        with open(path, 'wb') as f:\n            f.write(response.content)\n    \n    # If a digest is provided, verify the file's hash\n    if digest:\n        sha256_hash = hashlib.sha256()\n        with open(path, 'rb') as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        if sha256_hash.hexdigest() != digest:\n            raise ValueError(\"The downloaded file's hash does not match the provided digest.\")\n    \n    return path", "CodeBleu_score": 43.21483475545689}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "from contextlib import contextmanager\n\n@contextmanager\ndef train_mode(model, mode=True):\n    \"\"\"\n    The function is a context manager that sets a model to training mode and restores its previous mode upon exit.\n    It saves the current training modes of all modules, switches the model to the specified mode, and ensures the original modes are restored after the context is exited.\n    \"\"\"\n    # Save the original training mode of each module\n    original_modes = {module: module.training for module in model.modules()}\n    \n    try:\n        # Set the model to the specified mode\n        model.train(mode)\n        yield\n    finally:\n        # Restore the original training modes\n        for module, mode in original_modes.items():\n            module.train(mode)", "CodeBleu_score": 41.4294900650215}, {"code": "from contextlib import contextmanager\n\n@contextmanager\ndef train_mode(model, mode=True):\n    \"\"\"\n    The function is a context manager that sets a model to training mode and restores its previous mode upon exit.\n    It saves the current training modes of all modules, switches the model to the specified mode, and ensures the original modes are restored after the context is exited.\n    \"\"\"\n    # Save the original training mode states of all sub-modules\n    original_modes = {module: module.training for module in model.modules()}\n\n    try:\n        # Set the entire model to the specified mode\n        model.train(mode)\n        yield model\n    finally:\n        # Restore the original training mode states\n        for module, mode in original_modes.items():\n            module.train(mode)", "CodeBleu_score": 41.23369423354505}, {"code": "from contextlib import contextmanager\n\n@contextmanager\ndef train_mode(model, mode=True):\n    \"\"\"\n    The function is a context manager that sets a model to training mode and restores its previous mode upon exit.\n    It saves the current training modes of all modules, switches the model to the specified mode, and ensures the original modes are restored after the context is exited.\n    \"\"\"\n    original_modes = {module: module.training for module in model.modules()}\n    try:\n        model.train(mode)\n        yield\n    finally:\n        for module, mode in original_modes.items():\n            module.train(mode)", "CodeBleu_score": 41.4294900650215}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay):\n    \"\"\"\n    The function updates an exponentially moving averaged (EMA) model with the current model's parameters.\n    It ensures both models have matching parameters and buffers.\n    For each parameter, it updates the averaged model using a weighted average based on the decay factor.\n    It also copies the current model's buffers to the averaged model.\n    \"\"\"\n    with torch.no_grad():  # Ensure no gradients are computed in this operation\n        # Update parameters\n        for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n            avg_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n        \n        # Copy buffers (e.g., running means and variances in BatchNorm layers)\n        for buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n            avg_buffer.data.copy_(buffer.data)", "CodeBleu_score": 31.059187566658615}, {"code": "def ema_update(model, averaged_model, decay):\n    \"\"\"\n    The function updates an exponentially moving averaged (EMA) model with the current model's parameters.\n    It ensures both models have matching parameters and buffers.\n    For each parameter, it updates the averaged model using a weighted average based on the decay factor.\n    It also copies the current model's buffers to the averaged model.\n    \"\"\"\n    with torch.no_grad():  # Ensure no gradients are computed in this operation\n        # Update parameters\n        for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n            avg_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n        \n        # Copy buffers\n        for buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n            avg_buffer.data.copy_(buffer.data)", "CodeBleu_score": 31.059187566658615}, {"code": "import torch\n\ndef ema_update(model, averaged_model, decay):\n    \"\"\"The function updates an exponentially moving averaged (EMA) model with the current model's parameters.\n    It ensures both models have matching parameters and buffers.\n    For each parameter, it updates the averaged model using a weighted average based on the decay factor.\n    It also copies the current model's buffers to the averaged model.\"\"\"\n    \n    with torch.no_grad():\n        # Update parameters\n        for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n            avg_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n        \n        # Copy buffers\n        for buffer, avg_buffer in zip(model.buffers(), averaged_model.buffers()):\n            avg_buffer.data.copy_(buffer.data)", "CodeBleu_score": 30.47284169623104}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "import torch\n\ndef rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"\n    The function generates samples from a log-logistic distribution with optional truncation.\n    It converts the minimum and maximum values to tensors, computes their CDF values, and generates uniform random values within this range.\n    These values are transformed using the logit function, scaled, and exponentiated to obtain the final samples in the specified dtype.\n    \"\"\"\n    # Convert min and max values to tensors\n    min_value_tensor = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value_tensor = torch.tensor(max_value, device=device, dtype=dtype)\n    \n    # Compute the CDF of the min and max values\n    cdf_min = 1 / (1 + torch.exp(-(torch.log(min_value_tensor) - loc) / scale))\n    cdf_max = 1 / (1 + torch.exp(-(torch.log(max_value_tensor) - loc) / scale))\n    \n    # Generate uniform random values between cdf_min and cdf_max\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype) * (cdf_max - cdf_min) + cdf_min\n    \n    # Transform using the inverse of the CDF (logit function)\n    logit_samples = torch.log(uniform_samples / (1 - uniform_samples))\n    \n    # Scale and shift\n    scaled_samples = logit_samples * scale + loc\n    \n    # Exponentiate to get the samples from the log-logistic distribution\n    samples = torch.exp(scaled_samples)\n    \n    return samples", "CodeBleu_score": 40.67074439556222}, {"code": "import torch\n\ndef rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"\n    The function generates samples from a log-logistic distribution with optional truncation.\n    It converts the minimum and maximum values to tensors, computes their CDF values, and generates uniform random values within this range.\n    These values are transformed using the logit function, scaled, and exponentiated to obtain the final samples in the specified dtype.\n    \"\"\"\n    # Convert min and max values to tensors\n    min_value_tensor = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value_tensor = torch.tensor(max_value, device=device, dtype=dtype)\n    \n    # Compute the CDF of the min and max values\n    cdf_min = 1 / (1 + torch.exp(-(torch.log(min_value_tensor) - loc) / scale))\n    cdf_max = 1 / (1 + torch.exp(-(torch.log(max_value_tensor) - loc) / scale))\n    \n    # Generate uniform random values between cdf_min and cdf_max\n    uniform_randoms = torch.rand(shape, device=device, dtype=dtype) * (cdf_max - cdf_min) + cdf_min\n    \n    # Transform using the logit function and scale\n    logit_transform = torch.log(uniform_randoms / (1 - uniform_randoms))\n    scaled_logit = scale * logit_transform + loc\n    \n    # Exponentiate to get the samples\n    samples = torch.exp(scaled_logit)\n    \n    return samples", "CodeBleu_score": 40.70084305033603}, {"code": "import torch\n\ndef rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"\n    The function generates samples from a log-logistic distribution with optional truncation.\n    It converts the minimum and maximum values to tensors, computes their CDF values, and generates uniform random values within this range.\n    These values are transformed using the logit function, scaled, and exponentiated to obtain the final samples in the specified dtype.\n    \"\"\"\n    # Convert min_value and max_value to tensors\n    min_value_tensor = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value_tensor = torch.tensor(max_value, device=device, dtype=dtype)\n\n    # Compute the CDF of the min and max values\n    def cdf(x):\n        return 1 / (1 + torch.exp(-(torch.log(x) - loc) / scale))\n\n    cdf_min = cdf(min_value_tensor)\n    cdf_max = cdf(max_value_tensor)\n\n    # Generate uniform random values in the range [cdf_min, cdf_max]\n    uniform_randoms = torch.rand(shape, device=device, dtype=dtype) * (cdf_max - cdf_min) + cdf_min\n\n    # Transform using the inverse CDF (quantile function) of the log-logistic distribution\n    def quantile(p):\n        return torch.exp(loc + scale * (torch.log(p / (1 - p))))\n\n    samples = quantile(uniform_randoms)\n\n    return samples", "CodeBleu_score": 39.45045251453363}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "from typing import Literal\nimport torch.nn as nn\n\ndef get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module:\n    \"\"\"\n    The function returns a specified activation module based on the input string, which can be \"elu\", \"snake\", or \"none\".\n    If the activation type is \"elu\", it returns an ELU activation.\n    If the type is \"snake\", it returns a SnakeBeta activation with the specified channels.\n    If the type is \"none\", it returns an Identity module.\n    If the antialias flag is True, it wraps the activation in an Activation1d module before returning.\n    \"\"\"\n    if activation == \"elu\":\n        module = nn.ELU()\n    elif activation == \"snake\":\n        if channels is None:\n            raise ValueError(\"Channels must be specified for 'snake' activation.\")\n        module = SnakeBeta(channels)  # Assuming SnakeBeta is defined elsewhere\n    elif activation == \"none\":\n        module = nn.Identity()\n    else:\n        raise ValueError(\"Unsupported activation type provided.\")\n\n    if antialias:\n        module = Activation1d(module)  # Assuming Activation1d is defined elsewhere\n\n    return module", "CodeBleu_score": 50.85710097322269}, {"code": "from typing import Literal\nimport torch.nn as nn\n\nclass SnakeBeta(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        # Assuming some initialization here based on channels\n        pass\n\nclass Activation1d(nn.Module):\n    def __init__(self, activation_module):\n        super().__init__()\n        self.activation = activation_module\n\n    def forward(self, x):\n        return self.activation(x)\n\ndef get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module:\n    \"\"\"The function returns a specified activation module based on the input string, which can be \"elu\", \"snake\", or \"none\".\n    If the activation type is \"elu\", it returns an ELU activation.\n    If the type is \"snake\", it returns a SnakeBeta activation with the specified channels.\n    If the type is \"none\", it returns an Identity module.\n    If the antialias flag is True, it wraps the activation in an Activation1d module before returning.\"\"\"\n    \n    if activation == \"elu\":\n        module = nn.ELU()\n    elif activation == \"snake\":\n        if channels is None:\n            raise ValueError(\"Channels must be specified for 'snake' activation\")\n        module = SnakeBeta(channels)\n    elif activation == \"none\":\n        module = nn.Identity()\n    else:\n        raise ValueError(\"Unsupported activation type\")\n\n    if antialias:\n        module = Activation1d(module)\n\n    return module", "CodeBleu_score": 45.83168575842866}, {"code": "from typing import Literal\nimport torch.nn as nn\n\ndef get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module:\n    \"\"\"\n    The function returns a specified activation module based on the input string, which can be \"elu\", \"snake\", or \"none\".\n    If the activation type is \"elu\", it returns an ELU activation.\n    If the type is \"snake\", it returns a SnakeBeta activation with the specified channels.\n    If the type is \"none\", it returns an Identity module.\n    If the antialias flag is True, it wraps the activation in an Activation1d module before returning.\n    \"\"\"\n    if activation == \"elu\":\n        module = nn.ELU()\n    elif activation == \"snake\":\n        if channels is None:\n            raise ValueError(\"Channels must be specified for 'snake' activation.\")\n        module = SnakeBeta(channels)  # Assuming SnakeBeta is defined elsewhere\n    elif activation == \"none\":\n        module = nn.Identity()\n    else:\n        raise ValueError(\"Unsupported activation type provided.\")\n\n    if antialias:\n        module = Activation1d(module)  # Assuming Activation1d is defined elsewhere\n\n    return module", "CodeBleu_score": 50.85710097322269}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "import torch\n\ndef apply_rotary_pos_emb(t, freqs, scale=1):\n    \"\"\"\n    The function applies rotary position embeddings to a tensor using given frequency values.\n    It ensures numerical stability by casting to float32 if necessary, adjusts the shapes of the input tensor and frequency tensor,\n    and applies the rotary embeddings to a subset of the tensor.\n    The processed and unrotated parts of the tensor are concatenated and returned.\n    \"\"\"\n    # Ensure numerical stability by casting tensors to float32\n    t = t.to(torch.float32)\n    freqs = freqs.to(torch.float32)\n\n    # Adjust the shapes\n    freqs = freqs.view(1, -1, 1)  # Assuming freqs needs to be broadcasted over some dimension\n\n    # Apply rotary embeddings\n    # Rotary embeddings typically involve sinusoidal functions\n    seq_len = t.shape[1]\n    position = torch.arange(seq_len, dtype=torch.float32, device=t.device).unsqueeze(1)\n    rotations = torch.einsum('i,d->id', position, freqs) * scale\n    sin, cos = rotations.sin(), rotations.cos()\n    t_sin = t * sin\n    t_cos = t * cos\n    rotated_part = t_sin + t_cos\n\n    # Concatenate the processed and unrotated parts\n    result = torch.cat([rotated_part, t], dim=-1)\n\n    return result", "CodeBleu_score": 33.70314290351635}, {"code": "import torch\n\ndef apply_rotary_pos_emb(t, freqs, scale=1):\n    \"\"\"\n    The function applies rotary position embeddings to a tensor using given frequency values.\n    It ensures numerical stability by casting to float32 if necessary, adjusts the shapes of the input tensor and frequency tensor,\n    and applies the rotary embeddings to a subset of the tensor.\n    The processed and unrotated parts of the tensor are concatenated and returned.\n    \"\"\"\n    # Ensure numerical stability by casting to float32\n    if t.dtype != torch.float32:\n        t = t.to(torch.float32)\n    \n    # Adjust the shapes of the input tensor and frequency tensor\n    freqs = freqs.to(t.device).view(1, -1, 1)  # Assuming freqs is 1D and needs to be broadcasted over other dimensions\n    \n    # Apply the rotary embeddings to a subset of the tensor\n    # Assuming the rotary embeddings are applied to the last dimension\n    seq_len, d_model = t.shape[-2], t.shape[-1]\n    cos = torch.cos(freqs * torch.arange(seq_len).to(t.device) * scale).view(1, seq_len, 1)\n    sin = torch.sin(freqs * torch.arange(seq_len).to(t.device) * scale).view(1, seq_len, 1)\n    \n    # Apply cos and sin rotations\n    t_cos = t * cos\n    t_sin = t * sin\n    \n    # Concatenate the processed and unrotated parts of the tensor\n    # Assuming we rotate half of the dimensions and keep the other half unrotated\n    rotated_part = torch.cat((t_cos[:, :, :d_model//2], t_sin[:, :, :d_model//2]), dim=-1)\n    unrotated_part = t[:, :, d_model//2:]\n    \n    result = torch.cat((rotated_part, unrotated_part), dim=-1)\n    \n    return result", "CodeBleu_score": 39.62003035084698}, {"code": "import torch\n\ndef apply_rotary_pos_emb(t, freqs, scale=1):\n    \"\"\"\n    The function applies rotary position embeddings to a tensor using given frequency values.\n    It ensures numerical stability by casting to float32 if necessary, adjusts the shapes of the input tensor and frequency tensor,\n    and applies the rotary embeddings to a subset of the tensor.\n    The processed and unrotated parts of the tensor are concatenated and returned.\n    \"\"\"\n    # Ensure numerical stability by casting to float32\n    if t.dtype != torch.float32:\n        t = t.type(torch.float32)\n    \n    # Adjust the shapes of the input tensor and frequency tensor\n    freqs = freqs.to(t.device).type(t.dtype)\n    if len(freqs.shape) == 1:\n        freqs = freqs.unsqueeze(0)\n    \n    # Apply rotary embeddings to a subset of the tensor\n    # Assuming the last dimension of t is the one to be rotated\n    seq_len, feature_dim = t.shape[-2], t.shape[-1]\n    rot_dim = freqs.shape[-1]\n    \n    # Scale the frequencies\n    freqs_scaled = freqs * scale\n    \n    # Create the rotation matrix\n    cos_vals = torch.cos(freqs_scaled)\n    sin_vals = torch.sin(freqs_scaled)\n    rotation_matrix = torch.stack((cos_vals, -sin_vals, sin_vals, cos_vals), dim=-1).reshape(seq_len, 2 * rot_dim)\n    \n    # Apply the rotation to the tensor\n    t_rot = t[..., :2*rot_dim].reshape(*t.shape[:-1], -1, 2)\n    t_rot = torch.einsum('...ij,...jk->...ik', t_rot, rotation_matrix)\n    t_rot = t_rot.reshape(*t.shape[:-1], -1)\n    \n    # Concatenate the rotated and unrotated parts\n    if 2 * rot_dim < feature_dim:\n        t = torch.cat((t_rot, t[..., 2*rot_dim:]), dim=-1)\n    else:\n        t = t_rot\n    \n    return t", "CodeBleu_score": 36.95127807437868}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "import torch\n\nclass VQModel:\n    def __init__(self, codebook):\n        self.codebook = codebook\n\n    def idx2vq(self, idx, dim=-1):\n        \"\"\"\n        The function maps indices to vector quantized (VQ) embeddings using a codebook.\n        It retrieves the embeddings corresponding to the given indices and, if specified,\n        moves the last dimension to the given dimension.\n        The function then returns the resulting embeddings.\n\n        Parameters:\n            idx (torch.Tensor): A tensor of indices.\n            dim (int): The dimension to which the last dimension will be moved.\n\n        Returns:\n            torch.Tensor: The tensor containing the mapped embeddings.\n        \"\"\"\n        # Retrieve embeddings from the codebook\n        embeddings = self.codebook[idx]\n\n        # If dim is specified, permute the last dimension to the specified dimension\n        if dim != -1:\n            # Move the last dimension to 'dim'\n            embeddings = embeddings.permute(*[i for i in range(embeddings.dim()) if i != embeddings.dim() - 1] + [dim])\n\n        return embeddings", "CodeBleu_score": 36.772453680038225}, {"code": "import torch\n\ndef idx2vq(self, idx, dim=-1):\n    \"\"\"\n    The function maps indices to vector quantized (VQ) embeddings using a codebook.\n    It retrieves the embeddings corresponding to the given indices and, if specified,\n    moves the last dimension to the given dimension.\n    The function then returns the resulting embeddings.\n    \n    Parameters:\n        idx (torch.Tensor): A tensor of indices.\n        dim (int): The dimension to which the last dimension will be moved. Default is -1.\n    \n    Returns:\n        torch.Tensor: The tensor containing the mapped embeddings.\n    \"\"\"\n    # Assuming `self.codebook` is a tensor where each row is an embedding\n    embeddings = self.codebook[idx]\n    \n    # If dim is not -1, we permute the last dimension to the specified dimension\n    if dim != -1:\n        # Move the last dimension to `dim`\n        embeddings = embeddings.permute(*[i for i in range(embeddings.dim()) if i != embeddings.dim() - 1][:dim] + [embeddings.dim() - 1] + [i for i in range(embeddings.dim()) if i != embeddings.dim() - 1][dim:])\n    \n    return embeddings", "CodeBleu_score": 43.20448983263826}, {"code": "import torch\n\nclass VQModel:\n    def __init__(self, codebook):\n        self.codebook = codebook\n\n    def idx2vq(self, idx, dim=-1):\n        \"\"\"\n        The function maps indices to vector quantized (VQ) embeddings using a codebook.\n        It retrieves the embeddings corresponding to the given indices and, if specified,\n        moves the last dimension to the given dimension.\n        The function then returns the resulting embeddings.\n\n        Parameters:\n            idx (torch.Tensor): A tensor of indices.\n            dim (int): The dimension to which the last dimension will be moved (default is -1).\n\n        Returns:\n            torch.Tensor: The tensor containing the mapped embeddings.\n        \"\"\"\n        # Retrieve embeddings from the codebook using the provided indices\n        embeddings = self.codebook[idx]\n\n        # If dim is not -1, move the last dimension to the specified dimension\n        if dim != -1:\n            embeddings = embeddings.permute(*[i for i in range(embeddings.dim()) if i != embeddings.dim() - 1] + [dim])\n\n        return embeddings", "CodeBleu_score": 36.772453680038225}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False):\n    \"\"\"\n    The function encodes the input x using a series of blocks.\n    If quantization is enabled, it applies a vector quantizer to the encoded output and returns the quantized encoding, original encoding, indices, and combined loss.\n    If quantization is not enabled, it returns the encoded output.\n    \"\"\"\n    # Step 1: Encode the input x\n    encoded_output = self.encode_blocks(x)  # Assuming encode_blocks is a method for encoding\n\n    # Step 2: Check if quantization is enabled\n    if quantize:\n        # Step 2a: Apply vector quantization\n        quantized_output, indices, loss = self.vector_quantizer(encoded_output)  # Assuming vector_quantizer is a method\n        # Return quantized encoding, original encoding, indices, and combined loss\n        return quantized_output, encoded_output, indices, loss\n    else:\n        # Return the encoded output\n        return encoded_output\n\n# Assuming methods encode_blocks and vector_quantizer need to be defined or are already defined elsewhere", "CodeBleu_score": 30.964152172283022}, {"code": "def encode(self, x, quantize=False):\n    \"\"\"\n    The function encodes the input x using a series of blocks.\n    If quantization is enabled, it applies a vector quantizer to the encoded output and returns the quantized encoding, original encoding, indices, and combined loss.\n    If quantization is not enabled, it returns the encoded output.\n    \"\"\"\n    # Step 1: Encode the input x\n    encoded_output = self.encode_blocks(x)  # Assuming encode_blocks is a method for encoding\n\n    # Step 2: Check if quantization is enabled\n    if quantize:\n        # Step 2a: Apply vector quantization\n        quantized_output, indices, loss = self.vector_quantizer(encoded_output)  # Assuming vector_quantizer is a method\n        # Return quantized encoding, original encoding, indices, and combined loss\n        return quantized_output, encoded_output, indices, loss\n    else:\n        # Step 3: Return the encoded output if quantization is not enabled\n        return encoded_output\n\n# Assuming methods encode_blocks and vector_quantizer need to be defined or are already defined elsewhere in the class.", "CodeBleu_score": 30.726012708379567}, {"code": "def encode(self, x, quantize=False):\n    \"\"\"\n    The function encodes the input x using a series of blocks.\n    If quantization is enabled, it applies a vector quantizer to the encoded output and returns the quantized encoding, original encoding, indices, and combined loss.\n    If quantization is not enabled, it returns the encoded output.\n    \"\"\"\n    # Step 1: Encode the input x\n    encoded_output = self.encode_blocks(x)  # Assuming encode_blocks is a method for encoding\n\n    # Step 2: Check if quantization is enabled\n    if quantize:\n        # Step 2a: Apply vector quantization\n        quantized_output, indices, loss = self.vector_quantizer(encoded_output)  # Assuming vector_quantizer is a method\n        return quantized_output, encoded_output, indices, loss\n    else:\n        # Step 3: Return the encoded output if quantization is not enabled\n        return encoded_output\n\n# Example methods that might be part of the class\ndef encode_blocks(self, x):\n    # Placeholder for actual encoding logic\n    return x  # This should be replaced with actual encoding logic\n\ndef vector_quantizer(self, encoded):\n    # Placeholder for quantization logic\n    quantized = encoded  # This should be replaced with actual quantization logic\n    indices = range(len(encoded))  # Dummy indices for example\n    loss = 0.1  # Dummy loss for example\n    return quantized, indices, loss", "CodeBleu_score": 29.09041954332528}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "from contextlib import contextmanager\n\nclass Model:\n    def __init__(self):\n        self.parameters = {'weights': [1, 2, 3], 'bias': [0.1, 0.2]}\n        self.ema_parameters = {'weights': [0.9, 1.8, 2.7], 'bias': [0.09, 0.18]}\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        try:\n            # Store original parameters\n            original_parameters = self.parameters.copy()\n            \n            # Log the switch if context is provided\n            if context:\n                print(f\"Switching to EMA parameters in context: {context}\")\n            \n            # Apply EMA parameters\n            self.parameters = self.ema_parameters.copy()\n            \n            # Yield control back to the caller\n            yield\n        finally:\n            # Restore original parameters\n            self.parameters = original_parameters\n            \n            # Log the restoration if context is provided\n            if context:\n                print(f\"Restored original parameters in context: {context}\")\n\n# Example usage\nmodel = Model()\nprint(\"Original Parameters:\", model.parameters)\nwith model.ema_scope(context=\"training\"):\n    print(\"During EMA Scope:\", model.parameters)\nprint(\"After EMA Scope:\", model.parameters)", "CodeBleu_score": 27.477459068932664}, {"code": "class EMAHandler:\n    def __init__(self):\n        self.original_params = None\n        self.ema_params = None\n        self.is_logging = False\n\n    def ema_scope(self, context=None):\n        \"\"\"The function manages the application of Exponential Moving Average (EMA) weights in a context manager.\n        If EMA is used, it stores the current parameters and replaces them with EMA weights, optionally logging the switch.\n        Upon exiting the context, it restores the original parameters and logs the restoration if a context is provided.\"\"\"\n        self.context = context\n        return self\n\n    def __enter__(self):\n        # Store the current parameters\n        self.original_params = self.get_current_params()\n        \n        # Replace them with EMA weights\n        self.ema_params = self.get_ema_params()\n        self.apply_ema_params()\n\n        # Optionally log the switch\n        if self.context and self.is_logging:\n            print(\"Switched to EMA parameters.\")\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Restore the original parameters\n        self.apply_original_params()\n\n        # Optionally log the restoration\n        if self.context and self.is_logging:\n            print(\"Restored original parameters.\")\n\n    def get_current_params(self):\n        # Placeholder for getting current parameters\n        return {}\n\n    def get_ema_params(self):\n        # Placeholder for getting EMA parameters\n        return {}\n\n    def apply_ema_params(self):\n        # Placeholder for applying EMA parameters\n        pass\n\n    def apply_original_params(self):\n        # Placeholder for restoring original parameters\n        pass", "CodeBleu_score": 21.27855774703059}, {"code": "from contextlib import contextmanager\n\nclass Model:\n    def __init__(self):\n        self.parameters = {'weights': [1, 2, 3], 'bias': [0.1, 0.2]}\n        self.ema_parameters = {'weights': [0.9, 1.8, 2.7], 'bias': [0.09, 0.18]}\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        try:\n            # Store original parameters\n            original_parameters = self.parameters.copy()\n            \n            # Switch to EMA parameters\n            self.parameters = self.ema_parameters\n            if context:\n                print(f\"Switched to EMA parameters in context: {context}\")\n            \n            # Yield control back to the caller\n            yield\n            \n        finally:\n            # Restore original parameters\n            self.parameters = original_parameters\n            if context:\n                print(f\"Restored original parameters in context: {context}\")\n\n# Example usage:\nmodel = Model()\nwith model.ema_scope(context='training'):\n    print(\"Using EMA parameters:\", model.parameters)\nprint(\"After context:\", model.parameters)", "CodeBleu_score": 28.11238560224733}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp):\n    \"\"\"\n    The function applies post-attention processing to the input tensor x.\n    It first adds the attention output, scaled by gate_msa, to x.\n    Then, it normalizes x, modulates it using shift_mlp and scale_mlp, and applies an MLP scaled by gate_mlp.\n    The final result is returned.\n    \"\"\"\n    # Add the attention output, scaled by gate_msa, to x\n    x = x + attn * gate_msa\n\n    # Normalize x\n    x = x / x.norm(dim=-1, keepdim=True)\n\n    # Modulate x using shift_mlp and scale_mlp\n    x = (x + shift_mlp) * scale_mlp\n\n    # Apply an MLP scaled by gate_mlp\n    x = self.mlp(x) * gate_mlp\n\n    return x", "CodeBleu_score": 40.10789892831907}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp):\n    \"\"\"\n    The function applies post-attention processing to the input tensor x.\n    It first adds the attention output, scaled by gate_msa, to x.\n    Then, it normalizes x, modulates it using shift_mlp and scale_mlp, and applies an MLP scaled by gate_mlp.\n    The final result is returned.\n    \"\"\"\n    # Scale the attention output and add to x\n    x = x + attn * gate_msa\n\n    # Normalize x\n    x_normalized = self.normalize(x)\n\n    # Modulate x using shift and scale from MLP\n    x_modulated = x_normalized * scale_mlp + shift_mlp\n\n    # Apply MLP (assuming an MLP method is available in the class)\n    mlp_output = self.mlp(x_modulated) * gate_mlp\n\n    return mlp_output", "CodeBleu_score": 36.27600002210192}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp):\n    \"\"\"\n    The function applies post-attention processing to the input tensor x.\n    It first adds the attention output, scaled by gate_msa, to x.\n    Then, it normalizes x, modulates it using shift_mlp and scale_mlp, and applies an MLP scaled by gate_mlp.\n    The final result is returned.\n    \"\"\"\n    # Scale the attention output and add to x\n    x = x + attn * gate_msa\n\n    # Normalize x\n    x_normalized = self.normalize(x)\n\n    # Modulate x using shift and scale from MLP\n    x_modulated = (x_normalized + shift_mlp) * scale_mlp\n\n    # Apply MLP and scale by gate_mlp\n    x_mlp = self.mlp(x_modulated) * gate_mlp\n\n    return x_mlp", "CodeBleu_score": 36.60256566107732}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c):\n    \"\"\"\n    The function performs block mixing for attention mechanisms between context and input x.\n    It processes the context and x through their respective pre-attention blocks to obtain query, key, and value (qkv) tensors and intermediates.\n    It concatenates the qkv tensors, applies optimized attention, and splits the attention results back into context and x parts.\n    Finally, it processes these parts through their respective post-attention blocks, depending on the configuration of the context block, and returns the updated context and x.\n    \"\"\"\n    \n    # Step 1: Process context and x through pre-attention blocks\n    context_qkv = context_block.pre_attention(context, c)\n    x_qkv = x_block.pre_attention(x, c)\n    \n    # Step 2: Concatenate the qkv tensors\n    combined_qkv = concatenate_qkv(context_qkv, x_qkv)\n    \n    # Step 3: Apply optimized attention\n    attention_output = apply_attention(combined_qkv)\n    \n    # Step 4: Split the attention results back into context and x parts\n    context_result, x_result = split_attention_output(attention_output)\n    \n    # Step 5: Process through post-attention blocks\n    updated_context = context_block.post_attention(context_result, c)\n    updated_x = x_block.post_attention(x_result, c)\n    \n    return updated_context, updated_x\n\ndef concatenate_qkv(context_qkv, x_qkv):\n    # Dummy implementation for concatenating qkv tensors\n    return context_qkv + x_qkv  # This is a simplification\n\ndef apply_attention(qkv):\n    # Dummy implementation for applying attention\n    return qkv  # This is a simplification\n\ndef split_attention_output(attention_output):\n    # Dummy implementation for splitting attention output\n    half = len(attention_output) // 2\n    return attention_output[:half], attention_output[half:]\n\n# Note: The actual implementations of concatenate_qkv, apply_attention, and split_attention_output\n# would depend on the specific details of the tensors and the attention mechanism used.", "CodeBleu_score": 29.47136557588161}, {"code": "def _block_mixing(context, x, context_block, x_block, c):\n    \"\"\"\n    The function performs block mixing for attention mechanisms between context and input x.\n    It processes the context and x through their respective pre-attention blocks to obtain query, key, and value (qkv) tensors and intermediates.\n    It concatenates the qkv tensors, applies optimized attention, and splits the attention results back into context and x parts.\n    Finally, it processes these parts through their respective post-attention blocks, depending on the configuration of the context block, and returns the updated context and x.\n    \"\"\"\n    \n    # Step 1: Process through pre-attention blocks\n    context_qkv = context_block.pre_attention(context, c)\n    x_qkv = x_block.pre_attention(x, c)\n    \n    # Step 2: Concatenate qkv tensors\n    combined_qkv = torch.cat([context_qkv, x_qkv], dim=-1)\n    \n    # Step 3: Apply optimized attention\n    attention_output = optimized_attention(combined_qkv)\n    \n    # Step 4: Split the attention results\n    split_index = context_qkv.shape[-1]\n    updated_context = attention_output[:, :split_index]\n    updated_x = attention_output[:, split_index:]\n    \n    # Step 5: Process through post-attention blocks\n    final_context = context_block.post_attention(updated_context, c)\n    final_x = x_block.post_attention(updated_x, c)\n    \n    return final_context, final_x", "CodeBleu_score": 29.600122967112714}, {"code": "def _block_mixing(context, x, context_block, x_block, c):\n    \"\"\"\n    The function performs block mixing for attention mechanisms between context and input x.\n    It processes the context and x through their respective pre-attention blocks to obtain query, key, and value (qkv) tensors and intermediates.\n    It concatenates the qkv tensors, applies optimized attention, and splits the attention results back into context and x parts.\n    Finally, it processes these parts through their respective post-attention blocks, depending on the configuration of the context block, and returns the updated context and x.\n    \"\"\"\n    \n    # Process context and x through their respective pre-attention blocks\n    context_qkv = context_block.pre_attention(context, c)\n    x_qkv = x_block.pre_attention(x, c)\n    \n    # Concatenate the qkv tensors\n    combined_qkv = concatenate_qkv(context_qkv, x_qkv)\n    \n    # Apply optimized attention\n    attention_output = apply_attention(combined_qkv)\n    \n    # Split the attention results back into context and x parts\n    context_output, x_output = split_attention_output(attention_output)\n    \n    # Process the outputs through their respective post-attention blocks\n    updated_context = context_block.post_attention(context_output, c)\n    updated_x = x_block.post_attention(x_output, c)\n    \n    return updated_context, updated_x\n\ndef concatenate_qkv(context_qkv, x_qkv):\n    # Dummy implementation for concatenating qkv tensors\n    return context_qkv + x_qkv  # This is a placeholder\n\ndef apply_attention(qkv):\n    # Dummy implementation for applying attention\n    return qkv  # This is a placeholder\n\ndef split_attention_output(attention_output):\n    # Dummy implementation for splitting attention output\n    half = len(attention_output) // 2\n    return attention_output[:half], attention_output[half:]\n\n# Example usage of the function\ncontext = \"context data\"\nx = \"input data\"\ncontext_block = ContextBlock()\nx_block = XBlock()\nc = \"configuration\"\n\nupdated_context, updated_x = _block_mixing(context, x, context_block, x_block, c)", "CodeBleu_score": 30.314147895933335}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "import numpy as np\n\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    The function generates timestep embeddings for a given sequence of timesteps and embedding dimension.\n    It ensures the timesteps input is one-dimensional, then calculates a positional embedding using sine and cosine functions.\n    The embedding is scaled by a logarithmic factor, and the result is concatenated to form the final embedding.\n    If the embedding dimension is odd, it zero-pads the embedding to match the specified dimension before returning it.\n    \"\"\"\n    # Ensure timesteps is a numpy array and one-dimensional\n    timesteps = np.asarray(timesteps)\n    if timesteps.ndim != 1:\n        raise ValueError(\"Timesteps input must be one-dimensional\")\n\n    # Calculate the number of positions\n    num_positions = len(timesteps)\n    \n    # Generate the positional indices and compute the logarithmic scaling factor\n    position_indices = np.arange(num_positions)\n    scales = np.log(10000) / (embedding_dim // 2 - 1)\n    \n    # Compute the sine and cosine embeddings\n    sin_embeddings = np.sin(position_indices[:, None] * np.exp(-scales * np.arange(embedding_dim // 2)))\n    cos_embeddings = np.cos(position_indices[:, None] * np.exp(-scales * np.arange(embedding_dim // 2)))\n    \n    # Concatenate sine and cosine embeddings\n    embeddings = np.concatenate((sin_embeddings, cos_embeddings), axis=1)\n    \n    # If embedding_dim is odd, pad with zeros\n    if embedding_dim % 2 == 1:\n        embeddings = np.pad(embeddings, ((0, 0), (0, 1)), mode='constant')\n    \n    return embeddings", "CodeBleu_score": 37.66147616762716}, {"code": "import numpy as np\n\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    The function generates timestep embeddings for a given sequence of timesteps and embedding dimension.\n    It ensures the timesteps input is one-dimensional, then calculates a positional embedding using sine and cosine functions.\n    The embedding is scaled by a logarithmic factor, and the result is concatenated to form the final embedding.\n    If the embedding dimension is odd, it zero-pads the embedding to match the specified dimension before returning it.\n    \"\"\"\n    # Ensure timesteps is a numpy array and one-dimensional\n    timesteps = np.asarray(timesteps)\n    if timesteps.ndim != 1:\n        raise ValueError(\"Timesteps input must be one-dimensional\")\n\n    # Calculate the number of positions\n    num_positions = len(timesteps)\n    \n    # Generate position indices\n    positions = np.arange(num_positions)\n    \n    # Calculate the frequencies\n    frequencies = np.exp(-np.log(10000) * np.arange(0, embedding_dim, 2) / embedding_dim)\n    \n    # Calculate sine and cosine embeddings\n    sine_embeddings = np.sin(positions[:, None] * frequencies)\n    cosine_embeddings = np.cos(positions[:, None] * frequencies)\n    \n    # Concatenate sine and cosine embeddings\n    embeddings = np.concatenate([sine_embeddings, cosine_embeddings], axis=-1)\n    \n    # If embedding_dim is odd, zero-pad the last column\n    if embedding_dim % 2 == 1:\n        embeddings = np.pad(embeddings, ((0, 0), (0, 1)), mode='constant')\n    \n    return embeddings[:num_positions, :embedding_dim]", "CodeBleu_score": 36.62548030084528}, {"code": "import numpy as np\n\ndef get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    The function generates timestep embeddings for a given sequence of timesteps and embedding dimension.\n    It ensures the timesteps input is one-dimensional, then calculates a positional embedding using sine and cosine functions.\n    The embedding is scaled by a logarithmic factor, and the result is concatenated to form the final embedding.\n    If the embedding dimension is odd, it zero-pads the embedding to match the specified dimension before returning it.\n    \"\"\"\n    # Ensure timesteps is a numpy array and one-dimensional\n    timesteps = np.asarray(timesteps)\n    if timesteps.ndim != 1:\n        raise ValueError(\"Timesteps input must be one-dimensional\")\n\n    # Calculate the number of timescales\n    num_timescales = embedding_dim // 2\n\n    # Generate log-scaled positions\n    log_timescale_increment = np.log(10000.0) / (num_timescales - 1)\n    inv_timescales = np.exp(np.arange(num_timescales) * -log_timescale_increment)\n\n    # Scale the timesteps\n    scaled_time = np.expand_dims(timesteps, 1) * np.expand_dims(inv_timescales, 0)\n\n    # Calculate sine and cosine embeddings\n    sin_embeddings = np.sin(scaled_time)\n    cos_embeddings = np.cos(scaled_time)\n\n    # Concatenate sine and cosine embeddings\n    embeddings = np.concatenate([sin_embeddings, cos_embeddings], axis=1)\n\n    # If the embedding dimension is odd, pad with zeros\n    if embedding_dim % 2 == 1:\n        embeddings = np.pad(embeddings, ((0, 0), (0, 1)), mode='constant')\n\n    return embeddings", "CodeBleu_score": 38.546040597083596}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "import torch\nimport torch.nn.functional as F\n\ndef pytorch_attention(q, k, v):\n    \"\"\"The function computes attention for given query, key, and value tensors using PyTorch's scaled dot-product attention.\n    It reshapes and transposes the tensors to prepare them for attention computation.\n    If out-of-memory (OOM) exception occurs, it switches to slice attention as a fallback.\n    The function returns the attention output reshaped back to the original dimensions.\"\"\"\n    \n    d_k = k.size(-1)  # Assume the last dimension of k is the dimensionality of the keys/values\n    \n    try:\n        # Compute QK^T\n        qk_t = torch.matmul(q, k.transpose(-2, -1))\n        \n        # Scale by sqrt(d_k) and apply softmax\n        scaled_attention_logits = qk_t / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n        \n        # Multiply by V to get the output\n        output = torch.matmul(attention_weights, v)\n        \n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            print(\"OOM error occurred, switching to slice attention\")\n            # Implement slice-based attention here as a fallback\n            # This is a placeholder for the actual implementation\n            output = torch.zeros_like(v)  # Placeholder\n        else:\n            raise e\n    \n    return output", "CodeBleu_score": 24.781075250507534}, {"code": "import torch\nimport torch.nn.functional as F\n\ndef pytorch_attention(q, k, v):\n    \"\"\"\n    The function computes attention for given query, key, and value tensors using PyTorch's scaled dot-product attention.\n    It reshapes and transposes the tensors to prepare them for attention computation.\n    If out-of-memory (OOM) exception occurs, it switches to slice attention as a fallback.\n    The function returns the attention output reshaped back to the original dimensions.\n    \"\"\"\n    d_k = q.size(-1)  # Dimension of the query and key tensors\n\n    try:\n        # Compute the dot products of the query with all keys, divide each by sqrt(d_k), and apply softmax to obtain the weights on the values\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n        p_attn = F.softmax(scores, dim=-1)\n        # Multiply the softmax weights by the values to get the output\n        attention = torch.matmul(p_attn, v)\n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            print(\"OOM error occurred, switching to slice attention\")\n            # Implement slice-based attention as a fallback\n            # This is a simple example, adjust slicing strategy based on your specific requirements\n            slice_size = 1024  # Define slice size based on your hardware capabilities\n            attention = torch.zeros_like(v)\n            for i in range(0, q.size(0), slice_size):\n                scores = torch.matmul(q[i:i+slice_size], k.transpose(-2, -1)) / math.sqrt(d_k)\n                p_attn = F.softmax(scores, dim=-1)\n                attention[i:i+slice_size] = torch.matmul(p_attn, v)\n        else:\n            raise e\n\n    return attention", "CodeBleu_score": 24.289771710976897}, {"code": "import torch\nimport torch.nn.functional as F\n\ndef pytorch_attention(q, k, v):\n    \"\"\"\n    The function computes attention for given query, key, and value tensors using PyTorch's scaled dot-product attention.\n    It reshapes and transposes the tensors to prepare them for attention computation.\n    If out-of-memory (OOM) exception occurs, it switches to slice attention as a fallback.\n    The function returns the attention output reshaped back to the original dimensions.\n    \"\"\"\n    d_k = q.size(-1)  # Dimension of the key/query vectors for scaling\n\n    try:\n        # Compute the dot products of the query with all keys, divide each by sqrt(d_k), and apply softmax to obtain the weights on the values\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n        p_attn = F.softmax(scores, dim=-1)\n        # Multiply the softmax weights by the values to get the output\n        attention = torch.matmul(p_attn, v)\n    except RuntimeError as e:\n        if 'out of memory' in str(e):\n            print(\"OOM error occurred, switching to slice attention\")\n            # Implement slice-based attention as a fallback\n            # This is a placeholder for actual slice-based attention logic\n            attention = torch.zeros_like(v)  # Placeholder: replace with actual fallback logic\n        else:\n            raise e\n\n    return attention", "CodeBleu_score": 23.46413636991219}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name):\n    \"\"\"\n    The function applies a control adjustment to the input h if the specified control and name exist and are non-empty.\n    If a control is found, it is popped from the control list, and an attempt is made to add it to h.\n    If the addition fails, a warning is logged.\n    The function returns the potentially modified h.\n    \"\"\"\n    if control and name and name in control:\n        try:\n            # Assuming 'h' is a dictionary and 'control' is a dictionary of dictionaries\n            h[name] = control.pop(name)\n        except Exception as e:\n            import logging\n            logging.warning(f\"Failed to add control for {name}: {e}\")\n    return h", "CodeBleu_score": 36.25786788896066}, {"code": "def apply_control(h, control, name):\n    \"\"\"\n    The function applies a control adjustment to the input h if the specified control and name exist and are non-empty.\n    If a control is found, it is popped from the control list, and an attempt is made to add it to h.\n    If the addition fails, a warning is logged.\n    The function returns the potentially modified h.\n    \"\"\"\n    if control and name and name in control:\n        try:\n            # Assuming 'control' is a dictionary and 'h' is a list\n            h.append(control.pop(name))\n        except Exception as e:\n            # Log the warning if the addition fails\n            import logging\n            logging.warning(f\"Failed to add control {name}: {e}\")\n    return h", "CodeBleu_score": 27.849056132123913}, {"code": "def apply_control(h, control, name):\n    \"\"\"\n    The function applies a control adjustment to the input h if the specified control and name exist and are non-empty.\n    If a control is found, it is popped from the control list, and an attempt is made to add it to h.\n    If the addition fails, a warning is logged.\n    The function returns the potentially modified h.\n    \"\"\"\n    if control and name and name in control:\n        try:\n            # Assuming 'h' is a dictionary and 'control' is a dictionary of dictionaries\n            h[name] = control.pop(name)\n        except Exception as e:\n            import logging\n            logging.warning(f\"Failed to add control for {name}: {e}\")\n    return h", "CodeBleu_score": 36.25786788896066}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "import torch\n\nclass Codebook:\n    def __init__(self, embedding_dim, num_embeddings):\n        self.embedding_weights = torch.nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n\n    def get_codebook_entry(self, indices, shape=None):\n        \"\"\"\n        The function retrieves quantized latent vectors based on given indices and an optional shape.\n        It creates a one-hot encoding of the indices and multiplies it with the embedding weights to get the quantized vectors.\n        If a shape is provided, it reshapes and permutes the quantized vectors to match the original input shape before returning them.\n        \"\"\"\n        # Create a one-hot encoding of the indices\n        one_hot_indices = torch.nn.functional.one_hot(indices, num_classes=self.embedding_weights.shape[0])\n        \n        # Multiply with the embedding weights to get the quantized vectors\n        quantized_vectors = torch.matmul(one_hot_indices.float(), self.embedding_weights)\n        \n        # If a shape is provided, reshape and permute the quantized vectors\n        if shape is not None:\n            quantized_vectors = quantized_vectors.view(shape)\n            # Assuming the need to permute, though specifics depend on desired final shape\n            # This line might need to be adjusted based on the expected input and output dimensions\n            quantized_vectors = quantized_vectors.permute(0, 3, 1, 2)  # Example permutation\n\n        return quantized_vectors", "CodeBleu_score": 44.696046988678205}, {"code": "import torch\n\ndef get_codebook_entry(self, indices, shape=None):\n    \"\"\"\n    The function retrieves quantized latent vectors based on given indices and an optional shape.\n    It creates a one-hot encoding of the indices and multiplies it with the embedding weights to get the quantized vectors.\n    If a shape is provided, it reshapes and permutes the quantized vectors to match the original input shape before returning them.\n    \"\"\"\n    # Assuming `self.embedding_weights` is a tensor of shape [num_embeddings, embedding_dim]\n    \n    # Create a one-hot encoding of the indices\n    one_hot_indices = torch.nn.functional.one_hot(indices, num_classes=self.embedding_weights.shape[0])\n    \n    # Multiply the one-hot encoding by the embedding weights to get the quantized vectors\n    quantized_vectors = torch.matmul(one_hot_indices.float(), self.embedding_weights)\n    \n    # If a shape is provided, reshape and permute the quantized vectors\n    if shape is not None:\n        quantized_vectors = quantized_vectors.view(shape)\n        # Assuming the shape requires permutation, adjust the dimensions as needed\n        # This is a placeholder; the actual permutation depends on the expected output format\n        quantized_vectors = quantized_vectors.permute(0, 2, 3, 1)\n    \n    return quantized_vectors", "CodeBleu_score": 47.46827941982124}, {"code": "import torch\n\ndef get_codebook_entry(self, indices, shape=None):\n    \"\"\"\n    The function retrieves quantized latent vectors based on given indices and an optional shape.\n    It creates a one-hot encoding of the indices and multiplies it with the embedding weights to get the quantized vectors.\n    If a shape is provided, it reshapes and permutes the quantized vectors to match the original input shape before returning them.\n    \"\"\"\n    # Assuming `self.embedding_weights` is a pre-defined tensor of shape [num_embeddings, embedding_dim]\n    \n    # Create one-hot encoding of indices\n    one_hot_indices = torch.nn.functional.one_hot(indices, num_classes=self.embedding_weights.shape[0])\n    \n    # Multiply one-hot encodings by embedding weights to get the quantized vectors\n    quantized_vectors = torch.matmul(one_hot_indices.float(), self.embedding_weights)\n    \n    # If a shape is provided, reshape and permute the quantized vectors accordingly\n    if shape is not None:\n        quantized_vectors = quantized_vectors.view(shape)\n        # Assuming the need to permute, adjust the dimensions as necessary\n        # Example permutation, this would need to be adjusted based on specific requirements\n        quantized_vectors = quantized_vectors.permute(0, 2, 3, 1)\n    \n    return quantized_vectors", "CodeBleu_score": 47.46827941982124}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "import torch\n\ndef make_noise(self, log_size, device='cpu'):\n    \"\"\"\n    The function generates a list of noise tensors for noise injection.\n    It initializes the list with a noise tensor of size 4x4 on the appropriate device.\n    For each size from 2^3 to 2^log_size, it appends two noise tensors of the corresponding size to the list.\n    The function returns the list of noise tensors.\n    \"\"\"\n    noise_list = [torch.randn(4, 4, device=device)]\n    for i in range(3, log_size + 1):\n        size = 2 ** i\n        noise_list.append(torch.randn(size, size, device=device))\n        noise_list.append(torch.randn(size, size, device=device))\n    return noise_list", "CodeBleu_score": 38.680126604803796}, {"code": "import torch\n\ndef make_noise(self, log_size, device='cpu'):\n    \"\"\"\n    The function generates a list of noise tensors for noise injection.\n    It initializes the list with a noise tensor of size 4x4 on the appropriate device.\n    For each size from 2^3 to 2^log_size, it appends two noise tensors of the corresponding size to the list.\n    The function returns the list of noise tensors.\n    \"\"\"\n    noise_list = [torch.randn(4, 4, device=device)]\n    for i in range(3, log_size + 1):\n        size = 2 ** i\n        noise_list.append(torch.randn(size, size, device=device))\n        noise_list.append(torch.randn(size, size, device=device))\n    return noise_list", "CodeBleu_score": 38.680126604803796}, {"code": "import torch\n\ndef make_noise(self, log_size, device='cpu'):\n    \"\"\"\n    The function generates a list of noise tensors for noise injection.\n    It initializes the list with a noise tensor of size 4x4 on the appropriate device.\n    For each size from 2^3 to 2^log_size, it appends two noise tensors of the corresponding size to the list.\n    The function returns the list of noise tensors.\n    \n    Args:\n    log_size (int): The maximum power of 2 for the size of the noise tensors.\n    device (str): The device on which the tensors will be allocated (default is 'cpu').\n    \n    Returns:\n    list: A list containing the noise tensors.\n    \"\"\"\n    noise_list = [torch.randn(4, 4, device=device)]\n    for i in range(3, log_size + 1):\n        size = 2 ** i\n        noise_list.append(torch.randn(size, size, device=device))\n        noise_list.append(torch.randn(size, size, device=device))\n    return noise_list", "CodeBleu_score": 38.680126604803796}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "import torch\n\ndef mean_latent(self, num_latent):\n    \"\"\"\n    The function generates a mean latent vector.\n    It creates random latent inputs and processes them through a style MLP.\n    It then computes the mean of these processed latents along the first dimension and returns the resulting mean latent vector.\n    \"\"\"\n    # Assuming `latent_dim` is a predefined dimension of the latent vectors\n    # and `style_mlp` is a pre-defined MLP model that processes latent vectors.\n    random_latents = torch.randn(num_latent, self.latent_dim).to(self.device)\n    processed_latents = self.style_mlp(random_latents)\n    mean_latent = processed_latents.mean(dim=0)\n    return mean_latent", "CodeBleu_score": 42.7317307450983}, {"code": "import torch\n\ndef mean_latent(self, num_latent):\n    \"\"\"\n    The function generates a mean latent vector.\n    It creates random latent inputs and processes them through a style MLP.\n    It then computes the mean of these processed latents along the first dimension and returns the resulting mean latent vector.\n    \"\"\"\n    # Assuming `latent_dim` is the dimension of the latent space and `style_mlp` is the MLP model\n    latent_dim = 512  # Example dimension, adjust as necessary\n    random_latents = torch.randn(num_latent, latent_dim)  # Generate random latent vectors\n\n    # Assuming `style_mlp` is a method of the class that processes each latent vector\n    processed_latents = self.style_mlp(random_latents)  # Process latents through the MLP\n\n    # Compute the mean of the processed latents along the first dimension\n    mean_latent = processed_latents.mean(dim=0)\n\n    return mean_latent", "CodeBleu_score": 31.402355151737936}, {"code": "import torch\n\ndef mean_latent(self, num_latent):\n    \"\"\"\n    The function generates a mean latent vector.\n    It creates random latent inputs and processes them through a style MLP.\n    It then computes the mean of these processed latents along the first dimension and returns the resulting mean latent vector.\n    \"\"\"\n    # Assuming `latent_dim` and `style_mlp` are attributes of the class this method belongs to\n    # Generate random latent inputs\n    random_latents = torch.randn(num_latent, self.latent_dim)\n\n    # Process through a style MLP\n    processed_latents = self.style_mlp(random_latents)\n\n    # Compute the mean of these processed latents along the first dimension\n    mean_latent = processed_latents.mean(dim=0)\n\n    return mean_latent", "CodeBleu_score": 38.0240638688638}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "import numpy as np\n\ndef color_jitter(img, shift):\n    \"\"\"\n    The function applies color jitter to an image by randomly shifting its RGB values within a specified range.\n    It generates a random jitter value for each RGB channel, adds these values to the image, and clips the result to ensure pixel values remain between 0 and 1.\n    The function returns the jittered image.\n    \"\"\"\n    # Ensure the shift value is a numpy array of shape (3,) for RGB\n    shift = np.array(shift)\n    \n    # Generate random jitter values for each channel, within the range [-shift, shift]\n    jitter_values = np.random.uniform(-shift, shift, size=(1, 1, 3))\n    \n    # Add jitter to the image\n    jittered_img = img + jitter_values\n    \n    # Clip the values to ensure they are within the valid range [0, 1]\n    jittered_img = np.clip(jittered_img, 0, 1)\n    \n    return jittered_img", "CodeBleu_score": 47.091719209209465}, {"code": "import numpy as np\n\ndef color_jitter(img, shift):\n    \"\"\"\n    The function applies color jitter to an image by randomly shifting its RGB values within a specified range.\n    It generates a random jitter value for each RGB channel, adds these values to the image, and clips the result to ensure pixel values remain between 0 and 1.\n    The function returns the jittered image.\n    \n    Parameters:\n        img (numpy.ndarray): The input image as a NumPy array of shape (height, width, 3).\n        shift (float): The maximum amount by which each color channel can be randomly shifted.\n    \n    Returns:\n        numpy.ndarray: The color-jittered image.\n    \"\"\"\n    # Ensure the input image is a floating point type\n    if img.dtype != np.float32 and img.dtype != np.float64:\n        img = img.astype(np.float32)\n    \n    # Generate random shifts for each channel\n    random_shifts = np.random.uniform(-shift, shift, 3)\n    \n    # Add the shifts to the image\n    jittered_img = img + random_shifts\n    \n    # Clip the values to be between 0 and 1\n    jittered_img = np.clip(jittered_img, 0, 1)\n    \n    return jittered_img", "CodeBleu_score": 50.12522829638013}, {"code": "import numpy as np\n\ndef color_jitter(img, shift):\n    \"\"\"\n    The function applies color jitter to an image by randomly shifting its RGB values within a specified range.\n    It generates a random jitter value for each RGB channel, adds these values to the image, and clips the result to ensure pixel values remain between 0 and 1.\n    The function returns the jittered image.\n    \"\"\"\n    # Generate random shifts for each channel, ensuring they stay within the specified shift range\n    jitter_values = np.random.uniform(-shift, shift, 3)\n    \n    # Add the jitter to the image\n    jittered_img = img + jitter_values\n    \n    # Clip the values to ensure they are between 0 and 1\n    jittered_img = np.clip(jittered_img, 0, 1)\n    \n    return jittered_img", "CodeBleu_score": 48.42693534537251}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "class DataHandler:\n    def __init__(self):\n        self.lq = None\n        self.gt = None\n        self.left_eye = None\n        self.right_eye = None\n        self.mouth = None\n    \n    def feed_data(self, data):\n        \"\"\"\n        The function transfers input data to the specified device.\n        It assigns the 'lq' data to self.lq and, if present, assigns the 'gt' data to self.gt.\n        If facial component locations are included in the data, it assigns the left eye, \n        right eye, and mouth locations to the respective instance variables.\n\n        Args:\n        data (dict): A dictionary containing keys potentially among 'lq', 'gt', 'left_eye', 'right_eye', 'mouth'\n        \"\"\"\n        # Check and assign low-quality image data\n        if 'lq' in data:\n            self.lq = data['lq']\n        \n        # Check and assign ground-truth data\n        if 'gt' in data:\n            self.gt = data['gt']\n        \n        # Check and assign facial components if they exist\n        if 'left_eye' in data:\n            self.left_eye = data['left_eye']\n        if 'right_eye' in data:\n            self.right_eye = data['right_eye']\n        if 'mouth' in data:\n            self.mouth = data['mouth']", "CodeBleu_score": 39.932497437108935}, {"code": "class ImageProcessor:\n    def __init__(self):\n        self.lq = None\n        self.gt = None\n        self.left_eye = None\n        self.right_eye = None\n        self.mouth = None\n\n    def feed_data(self, data):\n        \"\"\"The function transfers input data to the specified device.\n        It assigns the 'lq' (low-quality) data to self.lq and, if present, assigns the 'gt' (ground truth) data to self.gt.\n        If facial component locations are included in the data, it assigns the left eye, right eye, and mouth locations to the respective instance variables.\n\n        Parameters:\n        data (dict): A dictionary potentially containing keys 'lq', 'gt', 'left_eye', 'right_eye', 'mouth'. Each key corresponds to its respective value.\n        \"\"\"\n        if 'lq' in data:\n            self.lq = data['lq']\n        if 'gt' in data:\n            self.gt = data.get('gt')  # Use .get() to return None if 'gt' is not in the dictionary.\n        if 'left_eye' in data:\n            self.left_eye = data['left_eye']\n        if 'right_eye' in data:\n            self.right_eye = data['right_eye']\n        if 'mouth' in data:\n            self.mouth = data['mouth']\n\n        # You can add more actions here depending on the purpose of the class, such as processing images or\n        # initiating further procedures with the data now stored in the instance variables.\n\n# Example usage:\nprocessor = ImageProcessor()\nprocessor.feed_data({\n    'lq': 'image_low_quality.jpg',\n    'gt': 'image_high_quality.jpg',\n    'left_eye': (100, 200),\n    'right_eye': (150, 200),\n    'mouth': (125, 250)\n})\n\nprint(\"Low quality image path:\", processor.lq)\nprint(\"Ground truth image path:\", processor.gt)\nprint(\"Left Eye Position:\", processor.left_eye)\nprint(\"Right Eye Position:\", processor.right_eye)\nprint(\"Mouth Position:\", processor.mouth)", "CodeBleu_score": 35.56617868105182}, {"code": "class DataHandler:\n    def __init__(self):\n        self.lq = None\n        self.gt = None\n        self.left_eye = None\n        self.right_eye = None\n        self.mouth = None\n\n    def feed_data(self, data):\n        \"\"\"The function transfers input data to the specified device.\n        It assigns the 'lq' data to self.lq and, if present, assigns the 'gt' data to self.gt.\n        If facial component locations are included in the data, it assigns the left eye, \n        right eye, and mouth locations to the respective instance variables.\n\n        Parameters:\n            data (dict): Data dictionary which may contain keys 'lq', 'gt', 'left_eye', 'right_eye', and 'mouth'.\n\n        Returns:\n            None\n        \"\"\"\n        # Assign lq data if present\n        if 'lq' in data:\n            self.lq = data['lq']\n        \n        # Assign gt data if present\n        if 'gt' in data:\n            self.gt = data['gt']\n\n        # Assign facial component locations if present\n        if 'left_eye' in data:\n            self.left_eye = data['left_eye']\n        if 'right_eye' in data:\n            self.right_eye = data['right_eye']\n        if 'mouth' in data:\n            self.mouth = data['mouth']", "CodeBleu_score": 39.932497437108935}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "import cv2\n\ndef construct_img_pyramid(self, image, num_levels=5):\n    \"\"\"\n    The function constructs an image pyramid for intermediate restoration loss.\n    It starts with the ground truth image and iteratively downscales it using bilinear interpolation.\n    The downscaled images are inserted at the beginning of the pyramid list.\n    The function returns the constructed image pyramid.\n\n    Args:\n    image (numpy.ndarray): The ground truth image.\n    num_levels (int): The number of pyramid levels to generate.\n\n    Returns:\n    list of numpy.ndarray: The image pyramid, with the original image at the end.\n    \"\"\"\n    pyramid = [image]\n    current_image = image\n    for _ in range(1, num_levels):\n        # Downscale the image using bilinear interpolation\n        scale_factor = 1 / (2 ** _)  # Each time reduce size by half\n        width = int(current_image.shape[1] * scale_factor)\n        height = int(current_image.shape[0] * scale_factor)\n        downscaled_image = cv2.resize(current_image, (width, height), interpolation=cv2.INTER_LINEAR)\n        pyramid.insert(0, downscaled_image)\n        current_image = downscaled_image\n\n    return pyramid", "CodeBleu_score": 25.904525880740124}, {"code": "import cv2\n\ndef construct_img_pyramid(self, image, scale_factor=0.5, min_size=(50, 50)):\n    \"\"\"\n    The function constructs an image pyramid for intermediate restoration loss.\n    It starts with the ground truth image and iteratively downscales it using bilinear interpolation.\n    The downscaled images are inserted at the beginning of the pyramid list.\n    The function returns the constructed image pyramid.\n\n    Parameters:\n    - image (numpy.ndarray): The ground truth image.\n    - scale_factor (float): The factor by which the image is downscaled in each iteration.\n    - min_size (tuple): The minimum size (width, height) of the downscaled images.\n\n    Returns:\n    - list of numpy.ndarray: The image pyramid as a list of images.\n    \"\"\"\n    pyramid = [image]\n    current_image = image\n    while True:\n        # Calculate new size\n        new_size = (int(current_image.shape[1] * scale_factor), int(current_image.shape[0] * scale_factor))\n        \n        # Check if the new size is below the minimum size\n        if new_size[0] < min_size[0] or new_size[1] < min_size[1]:\n            break\n        \n        # Resize the image using bilinear interpolation\n        current_image = cv2.resize(current_image, new_size, interpolation=cv2.INTER_LINEAR)\n        \n        # Insert the downscaled image at the beginning of the pyramid list\n        pyramid.insert(0, current_image)\n    \n    return pyramid", "CodeBleu_score": 27.588249492193917}, {"code": "bash\npip install opencv-python", "CodeBleu_score": 0.0}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter):\n    \"\"\"\n    The function saves the state of the model at a given epoch and iteration.\n    It saves the generator network, its EMA version, and the discriminator network.\n    If facial discriminators are used, it also saves the discriminators for the left eye, right eye, and mouth.\n    Finally, it saves the overall training state.\n    \"\"\"\n    # Save the generator network\n    torch.save(self.generator.state_dict(), f'generator_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the EMA version of the generator\n    if hasattr(self, 'generator_ema'):\n        torch.save(self.generator_ema.state_dict(), f'generator_ema_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the discriminator network\n    torch.save(self.discriminator.state_dict(), f'discriminator_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the facial discriminators if they exist\n    if hasattr(self, 'left_eye_discriminator'):\n        torch.save(self.left_eye_discriminator.state_dict(), f'left_eye_discriminator_epoch_{epoch}_iter_{current_iter}.pth')\n    if hasattr(self, 'right_eye_discriminator'):\n        torch.save(self.right_eye_discriminator.state_dict(), f'right_eye_discriminator_epoch_{epoch}_iter_{current_iter}.pth')\n    if hasattr(self, 'mouth_discriminator'):\n        torch.save(self.mouth_discriminator.state_dict(), f'mouth_discriminator_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the overall training state\n    training_state = {\n        'epoch': epoch,\n        'current_iter': current_iter,\n        'generator_state': self.generator.state_dict(),\n        'discriminator_state': self.discriminator.state_dict(),\n        'optimizer_generator_state': self.optimizer_generator.state_dict(),\n        'optimizer_discriminator_state': self.optimizer_discriminator.state_dict()\n    }\n    if hasattr(self, 'optimizer_facial_discriminators'):\n        training_state['optimizer_facial_discriminators_state'] = self.optimizer_facial_discriminators.state_dict()\n\n    torch.save(training_state, f'training_state_epoch_{epoch}_iter_{current_iter}.pth')", "CodeBleu_score": 33.80623755602845}, {"code": "def save(self, epoch, current_iter):\n    \"\"\"\n    The function saves the state of the model at a given epoch and iteration.\n    It saves the generator network, its EMA version, and the discriminator network.\n    If facial discriminators are used, it also saves the discriminators for the left eye, right eye, and mouth.\n    Finally, it saves the overall training state.\n    \"\"\"\n    # Save the generator network\n    torch.save(self.generator.state_dict(), f'generator_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the EMA version of the generator\n    if hasattr(self, 'generator_ema'):\n        torch.save(self.generator_ema.state_dict(), f'generator_ema_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the discriminator network\n    torch.save(self.discriminator.state_dict(), f'discriminator_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save facial discriminators if they exist\n    if hasattr(self, 'discriminator_left_eye'):\n        torch.save(self.discriminator_left_eye.state_dict(), f'discriminator_left_eye_epoch_{epoch}_iter_{current_iter}.pth')\n    if hasattr(self, 'discriminator_right_eye'):\n        torch.save(self.discriminator_right_eye.state_dict(), f'discriminator_right_eye_epoch_{epoch}_iter_{current_iter}.pth')\n    if hasattr(self, 'discriminator_mouth'):\n        torch.save(self.discriminator_mouth.state_dict(), f'discriminator_mouth_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the overall training state\n    training_state = {\n        'epoch': epoch,\n        'current_iter': current_iter,\n        'generator_state': self.generator.state_dict(),\n        'discriminator_state': self.discriminator.state_dict(),\n        'other_states': {}  # Add other necessary states\n    }\n    if hasattr(self, 'generator_ema'):\n        training_state['generator_ema_state'] = self.generator_ema.state_dict()\n\n    torch.save(training_state, f'training_state_epoch_{epoch}_iter_{current_iter}.pth')", "CodeBleu_score": 34.309663928313284}, {"code": "def save(self, epoch, current_iter):\n    \"\"\"\n    The function saves the state of the model at a given epoch and iteration.\n    It saves the generator network, its EMA version, and the discriminator network.\n    If facial discriminators are used, it also saves the discriminators for the left eye, right eye, and mouth.\n    Finally, it saves the overall training state.\n    \"\"\"\n    # Save the generator network\n    torch.save(self.generator.state_dict(), f'generator_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the EMA version of the generator\n    if hasattr(self, 'generator_ema'):\n        torch.save(self.generator_ema.state_dict(), f'generator_ema_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the discriminator network\n    torch.save(self.discriminator.state_dict(), f'discriminator_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the facial discriminators if they exist\n    if hasattr(self, 'discriminator_left_eye'):\n        torch.save(self.discriminator_left_eye.state_dict(), f'discriminator_left_eye_epoch_{epoch}_iter_{current_iter}.pth')\n    if hasattr(self, 'discriminator_right_eye'):\n        torch.save(self.discriminator_right_eye.state_dict(), f'discriminator_right_eye_epoch_{epoch}_iter_{current_iter}.pth')\n    if hasattr(self, 'discriminator_mouth'):\n        torch.save(self.discriminator_mouth.state_dict(), f'discriminator_mouth_epoch_{epoch}_iter_{current_iter}.pth')\n\n    # Save the overall training state (assuming it includes optimizers, epoch, iteration, etc.)\n    training_state = {\n        'epoch': epoch,\n        'current_iter': current_iter,\n        'generator_state_dict': self.generator.state_dict(),\n        'discriminator_state_dict': self.discriminator.state_dict(),\n        'optimizer_generator': self.optimizer_generator.state_dict(),\n        'optimizer_discriminator': self.optimizer_discriminator.state_dict()\n    }\n    if hasattr(self, 'generator_ema'):\n        training_state['generator_ema_state_dict'] = self.generator_ema.state_dict()\n\n    torch.save(training_state, f'training_state_epoch_{epoch}_iter_{current_iter}.pth')", "CodeBleu_score": 33.99769796739787}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "import os\nimport shutil\n\ndef clean_folder(folder):\n    \"\"\"\n    The function deletes all files and directories within a specified folder.\n    It iterates through the items in the folder, removing files and links using os.unlink, and directories using shutil.rmtree.\n    If an error occurs during deletion, it prints an error message with the file path and reason.\n    \n    Args:\n    folder (str): The path to the folder to clean.\n    \n    \"\"\"\n    # Iterate through all items in the specified folder\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            # Check if it is a file or a directory\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)  # Remove the file or link\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)  # Remove the directory\n        except Exception as e:\n            # Handle any exceptions that are raised\n            print(f\"Failed to delete {file_path}. Reason: {e}\")", "CodeBleu_score": 91.06889836817112}, {"code": "import os\nimport shutil\n\ndef clean_folder(folder):\n    \"\"\"The function deletes all files and directories within a specified folder.\n    It iterates through the items in the folder, removing files and links using os.unlink, and directories using shutil.rmtree.\n    If an error occurs during deletion, it prints an error message with the file path and reason.\n\n    Args:\n    folder (str): The path to the folder which needs to be cleaned.\n    \"\"\"\n    if not os.path.isdir(folder):\n        print(f\"Error: The provided path '{folder}' is not a directory.\")\n        return\n        \n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(f\"Failed to delete {item_path}. Reason: {e}\")\n\n# Example usage:\n# Suppose you have a folder path \"example_path/to/folder\"\n# clean_folder(\"example_path/to/folder\")", "CodeBleu_score": 57.04258196850537}, {"code": "import os\nimport shutil\n\ndef clean_folder(folder):\n    \"\"\"\n    The function deletes all files and directories within a specified folder.\n    It iterates through the items in the folder, removing files and links using os.unlink, \n    and directories using shutil.rmtree.\n    If an error occurs during deletion, it prints an error message with the file path and reason.\n    \n    Args:\n    folder (str): The path to the folder that needs to be cleaned.\n    \n    \"\"\"\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f'Failed to delete {file_path}. Reason: {e}')", "CodeBleu_score": 97.23717932270262}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "import random\n\ndef next_batch(self, sample_size=1):\n    \"\"\"The function retrieves the next batch of samples from a list of experiences.\n    It adjusts the sample size if it exceeds the available experiences.\n    It randomly selects the specified number of samples, removes them from the original list, and returns the sampled batch.\"\"\"\n    \n    # Ensure the sample size does not exceed the number of available experiences\n    sample_size = min(sample_size, len(self.experiences))\n    \n    # Randomly select samples\n    selected_samples = random.sample(self.experiences, sample_size)\n    \n    # Remove the selected samples from the original list\n    self.experiences = [exp for exp in self.experiences if exp not in selected_samples]\n    \n    # Return the sampled batch\n    return selected_samples", "CodeBleu_score": 46.1913261918727}, {"code": "import random\n\ndef next_batch(self, sample_size=1):\n    \"\"\"The function retrieves the next batch of samples from a list of experiences.\n    It adjusts the sample size if it exceeds the available experiences.\n    It randomly selects the specified number of samples, removes them from the original list, and returns the sampled batch.\"\"\"\n    \n    # Check if the sample size is greater than the number of available experiences\n    if sample_size > len(self.experiences):\n        sample_size = len(self.experiences)  # Adjust sample size to the number of available experiences\n    \n    # Randomly select samples\n    selected_samples = random.sample(self.experiences, sample_size)\n    \n    # Remove the selected samples from the original list\n    self.experiences = [exp for exp in self.experiences if exp not in selected_samples]\n    \n    # Return the sampled batch\n    return selected_samples", "CodeBleu_score": 63.66852531147911}, {"code": "import random\n\ndef next_batch(self, sample_size=1):\n    \"\"\"The function retrieves the next batch of samples from a list of experiences.\n    It adjusts the sample size if it exceeds the available experiences.\n    It randomly selects the specified number of samples, removes them from the original list, and returns the sampled batch.\"\"\"\n    \n    # Ensure the sample size does not exceed the number of available experiences\n    sample_size = min(sample_size, len(self.experiences))\n    \n    # Randomly select samples\n    selected_samples = random.sample(self.experiences, sample_size)\n    \n    # Remove the selected samples from the original list\n    self.experiences = [exp for exp in self.experiences if exp not in selected_samples]\n    \n    # Return the sampled batch\n    return selected_samples", "CodeBleu_score": 46.1913261918727}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "import numpy as np\n\ndef find_estimated_top_configs(self):\n    \"\"\"\n    The function finds the estimated top configurations based on a cost model.\n    It flattens each configuration in all_configs, collects numerical values, and converts them into a numpy array.\n    The cost model predicts estimates for these configurations, which are then sorted to identify the top configurations.\n    The sorting order depends on the specified metric, and the function returns the indices of the top configurations.\n    \"\"\"\n    # Assuming all_configs is a list of dictionaries where values are numerical\n    # Flattening configurations and collecting numerical values\n    config_values = [np.array(list(config.values())) for config in self.all_configs]\n    \n    # Assuming we have a cost model that can predict based on the numpy array of configurations\n    estimates = self.cost_model.predict(config_values)\n    \n    # Sorting indices based on the estimates (assuming lower cost is better)\n    # If higher cost is better, use np.argsort(-estimates)\n    top_indices = np.argsort(estimates)\n    \n    # Return the indices of the top configurations\n    return top_indices", "CodeBleu_score": 19.326269391543963}, {"code": "import numpy as np\n\ndef find_estimated_top_configs(self):\n    \"\"\"The function finds the estimated top configurations based on a cost model.\n    It flattens each configuration in all_configs, collects numerical values, and converts them into a numpy array.\n    The cost model predicts estimates for these configurations, which are then sorted to identify the top configurations.\n    The sorting order depends on the specified metric, and the function returns the indices of the top configurations.\"\"\"\n    \n    # Example of flattening and collecting numerical values\n    flattened_configs = [np.array(config).flatten() for config in self.all_configs]\n    \n    # Convert list of numpy arrays into a single numpy array\n    config_matrix = np.vstack(flattened_configs)\n    \n    # Predicting estimates using the cost model\n    estimates = self.cost_model.predict(config_matrix)\n    \n    # Sorting indices based on the estimates; assuming lower values are better\n    sorted_indices = np.argsort(estimates)\n    \n    # Return the indices of the top configurations\n    return sorted_indices", "CodeBleu_score": 23.15724575502054}, {"code": "import numpy as np\n\nclass ConfigModel:\n    def __init__(self, all_configs):\n        self.all_configs = all_configs\n        self.cost_model = self.load_cost_model()\n\n    def load_cost_model(self):\n        # Placeholder for loading a cost model, e.g., a trained machine learning model\n        pass\n\n    def predict_costs(self, data):\n        # Placeholder for predicting costs using the cost model\n        # This should return a list or array of costs\n        return np.random.rand(len(data))  # Random predictions for demonstration\n\n    def find_estimated_top_configs(self):\n        # Flatten each configuration and collect numerical values\n        flattened_configs = [np.array(config).flatten() for config in self.all_configs]\n\n        # Convert list of flattened configurations to a numpy array\n        config_array = np.array(flattened_configs)\n\n        # Use the cost model to predict estimates for these configurations\n        cost_estimates = self.predict_costs(config_array)\n\n        # Sort the indices of the configurations based on the predicted costs\n        # Assuming lower cost is better; change to `[::-1]` if higher is better\n        top_indices = np.argsort(cost_estimates)\n\n        # Return the indices of the top configurations\n        return top_indices\n\n# Example usage:\nall_configs = [\n    [[1, 2], [3, 4]],\n    [[5, 6], [7, 8]],\n    [[9, 10], [11, 12]]\n]\nmodel = ConfigModel(all_configs)\ntop_config_indices = model.find_estimated_top_configs()\nprint(\"Indices of top configurations:\", top_config_indices)", "CodeBleu_score": 36.11006215413088}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_'):\n    \"\"\"\n    The function recursively flattens a nested dictionary.\n    It concatenates parent and child keys using a specified separator to create new keys.\n    If a value is a dictionary, it recursively flattens it and extends the items list.\n    Otherwise, it adds the key-value pair to the items list and returns the flattened dictionary.\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)", "CodeBleu_score": 67.57665242534038}, {"code": "def flatten(d, parent_key='', sep='_'):\n    \"\"\"\n    The function recursively flattens a nested dictionary.\n    It concatenates parent and child keys using a specified separator to create new keys.\n    If a value is a dictionary, it recursively flattens it and extends the items list.\n    Otherwise, it adds the key-value pair to the items list and returns the flattened dictionary.\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)", "CodeBleu_score": 67.57665242534038}, {"code": "def flatten(d, parent_key='', sep='_'):\n    \"\"\"\n    The function recursively flattens a nested dictionary.\n    It concatenates parent and child keys using a specified separator to create new keys.\n    If a value is a dictionary, it recursively flattens it and extends the items list.\n    Otherwise, it adds the key-value pair to the items list and returns the flattened dictionary.\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten(v, new_key, sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)", "CodeBleu_score": 67.57665242534038}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "import subprocess\nimport logging\n\ndef run_after_tuning(self):\n    \"\"\"The function executes a command after tuning if an optimal command is found.\n    If optimal_cmd is available, it runs the command using subprocess.Popen and waits for it to complete, then logs a success message.\n    If no optimal command is found, it logs a message indicating that autotuning did not find an optimal configuration.\"\"\"\n    \n    if hasattr(self, 'optimal_cmd') and self.optimal_cmd:\n        try:\n            # Execute the optimal command\n            process = subprocess.Popen(self.optimal_cmd, shell=True)\n            process.wait()  # Wait for the command to complete\n            \n            # Log success message\n            logging.info(\"Successfully executed the optimal command.\")\n        except Exception as e:\n            # Log any errors that occur\n            logging.error(f\"Failed to execute the optimal command: {e}\")\n    else:\n        # Log that no optimal command was found\n        logging.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 47.22134145501972}, {"code": "import subprocess\nimport logging\n\ndef run_after_tuning(self):\n    \"\"\"\n    The function executes a command after tuning if an optimal command is found.\n    If optimal_cmd is available, it runs the command using subprocess.Popen and waits for it to complete, then logs a success message.\n    If no optimal command is found, it logs a message indicating that autotuning did not find an optimal configuration.\n    \"\"\"\n    if hasattr(self, 'optimal_cmd') and self.optimal_cmd:\n        try:\n            # Execute the optimal command\n            process = subprocess.Popen(self.optimal_cmd, shell=True)\n            process.wait()  # Wait for the command to complete\n            logging.info(\"Successfully executed the optimal command.\")\n        except Exception as e:\n            logging.error(f\"Failed to execute the optimal command: {e}\")\n    else:\n        logging.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 47.22134145501972}, {"code": "import subprocess\nimport logging\n\ndef run_after_tuning(self):\n    \"\"\"\n    The function executes a command after tuning if an optimal command is found.\n    If optimal_cmd is available, it runs the command using subprocess.Popen and waits for it to complete, then logs a success message.\n    If no optimal command is found, it logs a message indicating that autotuning did not find an optimal configuration.\n    \"\"\"\n    if hasattr(self, 'optimal_cmd') and self.optimal_cmd:\n        try:\n            # Execute the optimal command\n            process = subprocess.Popen(self.optimal_cmd, shell=True)\n            process.wait()  # Wait for the command to complete\n\n            # Log the success message\n            logging.info(\"Successfully executed the optimal command.\")\n        except Exception as e:\n            # Log any errors that occur\n            logging.error(f\"Failed to execute the optimal command: {e}\")\n    else:\n        # Log the message if no optimal command is found\n        logging.info(\"Autotuning did not find an optimal configuration.\")", "CodeBleu_score": 47.22134145501972}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "import threading\nimport time\n\ndef run_job(self, exp: dict, reservations):\n    \"\"\"The function runs an experiment job by configuring its parameters and starting it in a new thread.\n    It sets the experiment's master port and result directory, and updates user arguments based on argument mappings.\n    A new thread is created to run the experiment with the specified script and arguments, and the thread is started.\n    The running experiment details are stored with a timestamp in running_experiments.\"\"\"\n\n    # Set experiment parameters\n    exp['master_port'] = self.get_available_port()\n    exp['result_directory'] = self.create_result_directory(exp['name'])\n\n    # Update user arguments based on mappings\n    user_args = self.update_arguments(exp['user_args'], reservations)\n\n    # Define the experiment run function\n    def experiment_run():\n        script = exp['script']\n        args = user_args\n        # Assuming there's a function to execute the script with arguments\n        self.execute_script(script, args)\n\n    # Create and start a new thread for the experiment\n    experiment_thread = threading.Thread(target=experiment_run)\n    experiment_thread.start()\n\n    # Store experiment details\n    self.running_experiments[time.time()] = {\n        'experiment': exp['name'],\n        'thread': experiment_thread\n    }", "CodeBleu_score": 25.928077464798772}, {"code": "import threading\nimport time\n\ndef run_job(self, exp: dict, reservations):\n    \"\"\"The function runs an experiment job by configuring its parameters and starting it in a new thread.\n    It sets the experiment's master port and result directory, and updates user arguments based on argument mappings.\n    A new thread is created to run the experiment with the specified script and arguments, and the thread is started.\n    The running experiment details are stored with a timestamp in running_experiments.\"\"\"\n\n    # Set the master port and result directory\n    exp['master_port'] = self.get_available_port()\n    exp['result_directory'] = self.create_result_directory(exp['name'])\n\n    # Update user arguments based on mappings\n    user_args = self.map_user_arguments(exp['user_args'], reservations)\n\n    # Define the function to run the experiment in a new thread\n    def experiment_thread(script, args):\n        self.run_script(script, args)\n\n    # Create and start the new thread\n    thread = threading.Thread(target=experiment_thread, args=(exp['script'], user_args))\n    thread.start()\n\n    # Store experiment details\n    self.running_experiments[time.time()] = {'experiment': exp, 'thread': thread}\n\ndef get_available_port(self):\n    # Dummy implementation to get an available port\n    return 8080\n\ndef create_result_directory(self, experiment_name):\n    # Dummy implementation to create a directory for experiment results\n    return f\"/path/to/results/{experiment_name}\"\n\ndef map_user_arguments(self, user_args, reservations):\n    # Dummy implementation to map user arguments\n    return {arg: reservations.get(arg, None) for arg in user_args}\n\ndef run_script(self, script, args):\n    # Dummy implementation to simulate running a script\n    print(f\"Running {script} with arguments {args}\")", "CodeBleu_score": 27.15770892320815}, {"code": "import threading\nimport os\nimport time\n\ndef run_job(self, exp: dict, reservations):\n    \"\"\"\n    The function runs an experiment job by configuring its parameters and starting it in a new thread.\n    It sets the experiment's master port and result directory, and updates user arguments based on argument mappings.\n    A new thread is created to run the experiment with the specified script and arguments, and the thread is started.\n    The running experiment details are stored with a timestamp in running_experiments.\n    \"\"\"\n    # Set the master port and results directory from the experiment configuration\n    master_port = exp.get('master_port', 8000)\n    results_dir = exp.get('results_dir', '/path/to/results')\n    os.makedirs(results_dir, exist_ok=True)\n\n    # Update user arguments based on argument mappings\n    user_args = exp.get('user_args', {})\n    argument_mappings = exp.get('argument_mappings', {})\n    updated_args = {k: argument_mappings.get(v, v) for k, v in user_args.items()}\n\n    # Define the function to run the experiment in a new thread\n    def experiment_thread():\n        script = exp['script']\n        os.system(f\"python {script} --port {master_port} --results_dir {results_dir} \" + \" \".join(f\"--{k} {v}\" for k, v in updated_args.items()))\n\n    # Create and start a new thread for the experiment\n    thread = threading.Thread(target=experiment_thread)\n    thread.start()\n\n    # Store experiment details with a timestamp\n    self.running_experiments[time.time()] = {\n        'thread': thread,\n        'experiment': exp,\n        'reservations': reservations\n    }", "CodeBleu_score": 29.00069466443901}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "import json\nimport os\n\ndef save_exp_results_to_database(self, message, ranks=None, path=None):\n    \"\"\"\n    The function saves experimental results to a database if certain logging conditions are met.\n    It checks if distributed training is initialized and sets logging permissions accordingly.\n    If logging is permitted, it adds the rank to the message and appends the message to a specified file in JSON format.\n    \"\"\"\n    # Check if distributed training is initialized (simulated check)\n    if hasattr(self, 'is_distributed') and self.is_distributed:\n        # Check if logging is permitted (simulated condition)\n        if hasattr(self, 'logging_enabled') and self.logging_enabled:\n            # Add rank to the message if ranks are provided\n            if ranks is not None:\n                message += f\" Rank: {ranks}\"\n            \n            # Ensure the path is specified\n            if path is not None:\n                # Append the message to the specified file in JSON format\n                with open(path, 'a') as file:\n                    json.dump({'message': message}, file)\n                    file.write('\n')  # Ensure each entry is on a new line\n            else:\n                print(\"No path specified for logging.\")\n        else:\n            print(\"Logging not enabled.\")\n    else:\n        print(\"Distributed training not initialized.\")", "CodeBleu_score": 21.960319276957126}, {"code": "import json\nimport os\n\ndef save_exp_results_to_database(self, message, ranks=None, path=None):\n    \"\"\"\n    The function saves experimental results to a database if certain logging conditions are met.\n    It checks if distributed training is initialized and sets logging permissions accordingly.\n    If logging is permitted, it adds the rank to the message and appends the message to a specified file in JSON format.\n    \"\"\"\n    # Check if distributed training is initialized (simulated check)\n    is_distributed_initialized = self.check_distributed_training()\n\n    # Set logging permissions based on the initialization status\n    logging_allowed = is_distributed_initialized\n\n    if logging_allowed:\n        # Format the message by adding the rank if provided\n        if ranks is not None:\n            message = f\"Rank {ranks}: {message}\"\n        \n        # Ensure the path is specified\n        if path is None:\n            raise ValueError(\"Path must be specified to save results.\")\n\n        # Append the message to the specified file in JSON format\n        with open(path, 'a') as file:\n            json.dump({'message': message}, file)\n            file.write('\n')  # For readability in the JSON file\n\ndef check_distributed_training(self):\n    # Placeholder for the actual check for distributed training initialization\n    return True  # Assuming it returns True for this example", "CodeBleu_score": 26.590345073628868}, {"code": "import json\nimport os\n\ndef save_exp_results_to_database(self, message, ranks=None, path=None):\n    \"\"\"\n    The function saves experimental results to a database if certain logging conditions are met.\n    It checks if distributed training is initialized and sets logging permissions accordingly.\n    If logging is permitted, it adds the rank to the message and appends the message to a specified file in JSON format.\n    \"\"\"\n    # Check if distributed training is initialized (assuming a method or variable `is_distributed` exists)\n    if hasattr(self, 'is_distributed') and self.is_distributed:\n        # Check if logging is permitted (assuming a method or variable `logging_permitted` exists)\n        if hasattr(self, 'logging_permitted') and self.logging_permitted:\n            # Add rank to the message if ranks are provided\n            if ranks is not None:\n                message += f\" Rank: {ranks}\"\n            \n            # Ensure the path is not None and the directory exists\n            if path is not None and os.path.isdir(os.path.dirname(path)):\n                # Append the message to the specified file in JSON format\n                with open(path, 'a') as file:\n                    json.dump({'message': message}, file)\n                    file.write('\n')  # For readability in the JSON file\n            else:\n                print(\"Error: Path is not specified or the directory does not exist.\")\n        else:\n            print(\"Logging is not permitted.\")\n    else:\n        print(\"Distributed training is not initialized.\")", "CodeBleu_score": 22.313075752755847}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int):\n    \"\"\"\n    The function builds a data map for a specific layer index.\n    It first checks if there are any layer files and verifies the layer index.\n    It retrieves files with a specific prefix for the given layer and partitions these files based on tp_degree.\n    The function then creates and returns a dictionary mapping partition indices to their corresponding file lists.\n    \"\"\"\n    # Check if layer files exist and verify the layer index\n    if not self.layer_files or layer_index >= len(self.layer_files):\n        raise ValueError(\"Invalid layer index or no layer files available\")\n\n    # Retrieve files with a specific prefix for the given layer\n    prefix = f\"layer_{layer_index}_\"\n    filtered_files = [f for f in self.layer_files if f.startswith(prefix)]\n\n    # Partition these files based on tp_degree\n    tp_map = {}\n    for file in filtered_files:\n        # Assuming file names are in the format \"layer_index_tp_degree_otherinfo.ext\"\n        parts = file.split('_')\n        tp_degree = int(parts[2])  # Extract tp_degree assuming it's the third part of the filename\n        if tp_degree not in tp_map:\n            tp_map[tp_degree] = []\n        tp_map[tp_degree].append(file)\n\n    # Return the dictionary mapping partition indices to their corresponding file lists\n    return tp_map", "CodeBleu_score": 32.54779940187817}, {"code": "def _build_tp_other_layer_map(self, layer_index: int):\n    \"\"\"\n    The function builds a data map for a specific layer index.\n    It first checks if there are any layer files and verifies the layer index.\n    It retrieves files with a specific prefix for the given layer and partitions these files based on tp_degree.\n    The function then creates and returns a dictionary mapping partition indices to their corresponding file lists.\n    \"\"\"\n    # Check for layer files and verify the layer index\n    if not self.layer_files or layer_index >= len(self.layer_files):\n        raise ValueError(\"Invalid layer index or no layer files available\")\n\n    # Retrieve files with a specific prefix\n    prefix = f\"layer_{layer_index}_\"\n    layer_files = [f for f in self.layer_files if f.startswith(prefix)]\n\n    # Partition files based on tp_degree\n    partition_map = {}\n    for file in layer_files:\n        partition_index = self.get_partition_index(file, tp_degree)  # Assuming a method to calculate partition index\n        if partition_index not in partition_map:\n            partition_map[partition_index] = []\n        partition_map[partition_index].append(file)\n\n    return partition_map", "CodeBleu_score": 32.32455763289653}, {"code": "def _build_tp_other_layer_map(self, layer_index: int):\n    \"\"\"\n    The function builds a data map for a specific layer index.\n    It first checks if there are any layer files and verifies the layer index.\n    It retrieves files with a specific prefix for the given layer and partitions these files based on tp_degree.\n    The function then creates and returns a dictionary mapping partition indices to their corresponding file lists.\n    \"\"\"\n    # Check if layer files exist and verify the layer index\n    if not self.layer_files or layer_index >= len(self.layer_files):\n        raise ValueError(\"Invalid layer index or no layer files available\")\n\n    # Retrieve files with a specific prefix for the given layer\n    layer_files = [f for f in self.layer_files if f.startswith(f\"layer_{layer_index}_\")]\n\n    # Partition files based on tp_degree\n    partition_map = {}\n    for file in layer_files:\n        partition_index = self.get_tp_degree(file)  # Assuming get_tp_degree is a method to determine the partition\n        if partition_index not in partition_map:\n            partition_map[partition_index] = []\n        partition_map[partition_index].append(file)\n\n    return partition_map", "CodeBleu_score": 31.99022510543992}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "import platform\nimport sys\n\ndef system_info():\n    \"\"\"The function retrieves system information and returns it as a dictionary.\n    It includes the operating system, architecture, version, processor, and Python version.\"\"\"\n    info = {\n        \"operating_system\": platform.system(),\n        \"architecture\": platform.machine(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": sys.version\n    }\n    return info", "CodeBleu_score": 45.220627596950024}, {"code": "import platform\nimport sys\n\ndef system_info():\n    \"\"\"The function retrieves system information and returns it as a dictionary.\n    It includes the operating system, architecture, version, processor, and Python version.\"\"\"\n    info = {\n        \"operating_system\": platform.system(),\n        \"architecture\": platform.machine(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": sys.version\n    }\n    return info", "CodeBleu_score": 45.220627596950024}, {"code": "import platform\nimport sys\n\ndef system_info():\n    \"\"\"The function retrieves system information and returns it as a dictionary.\n    It includes the operating system, architecture, version, processor, and Python version.\"\"\"\n    info = {\n        \"operating_system\": platform.system(),\n        \"architecture\": platform.machine(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python_version\": sys.version\n    }\n    return info", "CodeBleu_score": 45.220627596950024}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "import torch\n\ndef cuda_info():\n    \"\"\"The function returns information about the CUDA setup.\n    It provides a list of GPU device names, the availability status of CUDA, and the CUDA version being used.\"\"\"\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    gpu_devices = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    \n    return {\n        \"CUDA Available\": cuda_available,\n        \"CUDA Version\": cuda_version,\n        \"GPU Devices\": gpu_devices\n    }", "CodeBleu_score": 56.846002429730945}, {"code": "import torch\n\ndef cuda_info():\n    \"\"\"The function returns information about the CUDA setup.\n    It provides a list of GPU device names, the availability status of CUDA, and the CUDA version being used.\"\"\"\n    \n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    gpu_devices = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    \n    return {\n        \"CUDA Available\": cuda_available,\n        \"CUDA Version\": cuda_version,\n        \"GPU Devices\": gpu_devices\n    }", "CodeBleu_score": 56.846002429730945}, {"code": "import torch\n\ndef cuda_info():\n    \"\"\"The function returns information about the CUDA setup.\n    It provides a list of GPU device names, the availability status of CUDA, and the CUDA version being used.\"\"\"\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    gpu_devices = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]\n    \n    return {\n        'CUDA Available': cuda_available,\n        'CUDA Version': cuda_version,\n        'GPU Devices': gpu_devices\n    }", "CodeBleu_score": 56.07954672799275}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager):\n    \"\"\"\n    The function computes the encoder accuracy for a given dataset.\n    It retrieves the class name key and optional class ID to class name mapping from the encoder manager's configuration.\n    For each item in the dataset, it extracts the audio file, computes the embedding, and predicts the class label if criteria and mappings are available.\n    It records the accuracy for each class by comparing the true class name with the predicted label.\n    Finally, it calculates and prints the accuracy for each class and the average accuracy across all classes.\n    \"\"\"\n    \n    # Retrieve configuration details from the encoder manager\n    class_name_key = encoder_manager.config['class_name_key']\n    class_id_to_name = encoder_manager.config.get('class_id_to_name', {})\n    \n    # Initialize counters and accumulators\n    class_accuracies = {}\n    total_correct = 0\n    total_items = 0\n    \n    # Process each item in the dataset\n    for item in dataset_items:\n        audio_file = item['audio_file']\n        true_class_id = item['class_id']\n        true_class_name = class_id_to_name.get(true_class_id, true_class_id)\n        \n        # Compute embedding and predict class\n        embedding = encoder_manager.compute_embedding(audio_file)\n        predicted_class_id = encoder_manager.predict_class(embedding)\n        predicted_class_name = class_id_to_name.get(predicted_class_id, predicted_class_id)\n        \n        # Check prediction accuracy\n        is_correct = predicted_class_name == true_class_name\n        if true_class_name not in class_accuracies:\n            class_accuracies[true_class_name] = {'correct': 0, 'total': 0}\n        class_accuracies[true_class_name]['total'] += 1\n        if is_correct:\n            class_accuracies[true_class_name]['correct'] += 1\n            total_correct += 1\n        total_items += 1\n    \n    # Calculate and print accuracy for each class and overall\n    for class_name, stats in class_accuracies.items():\n        accuracy = stats['correct'] / stats['total']\n        print(f\"Accuracy for {class_name}: {accuracy:.2f}\")\n    \n    if total_items > 0:\n        average_accuracy = total_correct / total_items\n        print(f\"Average accuracy across all classes: {average_accuracy:.2f}\")\n    else:\n        print(\"No items to process.\")", "CodeBleu_score": 33.16745702240165}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager):\n    \"\"\"\n    The function computes the encoder accuracy for a given dataset.\n    It retrieves the class name key and optional class ID to class name mapping from the encoder manager's configuration.\n    For each item in the dataset, it extracts the audio file, computes the embedding, and predicts the class label if criteria and mappings are available.\n    It records the accuracy for each class by comparing the true class name with the predicted label.\n    Finally, it calculates and prints the accuracy for each class and the average accuracy across all classes.\n    \"\"\"\n    \n    # Retrieve configuration details from the encoder manager\n    class_name_key = encoder_manager.config['class_name_key']\n    class_id_to_name = encoder_manager.config.get('class_id_to_name', {})\n    \n    # Initialize counters and accumulators\n    class_accuracies = {}\n    total_correct = 0\n    total_items = 0\n    \n    # Process each item in the dataset\n    for item in dataset_items:\n        audio_file = item['audio_file']\n        true_class_id = item['class_id']\n        true_class_name = class_id_to_name.get(true_class_id, true_class_id)\n        \n        # Compute embedding and predict class\n        embedding = encoder_manager.compute_embedding(audio_file)\n        predicted_class_id = encoder_manager.predict_class(embedding)\n        predicted_class_name = class_id_to_name.get(predicted_class_id, predicted_class_id)\n        \n        # Check if prediction is correct\n        is_correct = predicted_class_name == true_class_name\n        total_correct += is_correct\n        total_items += 1\n        \n        # Record accuracy for the class\n        if true_class_name not in class_accuracies:\n            class_accuracies[true_class_name] = {'correct': 0, 'total': 0}\n        class_accuracies[true_class_name]['correct'] += is_correct\n        class_accuracies[true_class_name]['total'] += 1\n    \n    # Calculate and print accuracy for each class and overall\n    print(\"Accuracy per class:\")\n    for class_name, stats in class_accuracies.items():\n        accuracy = stats['correct'] / stats['total']\n        print(f\"{class_name}: {accuracy:.2f}\")\n    \n    overall_accuracy = total_correct / total_items\n    print(f\"Overall accuracy: {overall_accuracy:.2f}\")", "CodeBleu_score": 32.886771436883755}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager):\n    \"\"\"\n    The function computes the encoder accuracy for a given dataset.\n    It retrieves the class name key and optional class ID to class name mapping from the encoder manager's configuration.\n    For each item in the dataset, it extracts the audio file, computes the embedding, and predicts the class label if criteria and mappings are available.\n    It records the accuracy for each class by comparing the true class name with the predicted label.\n    Finally, it calculates and prints the accuracy for each class and the average accuracy across all classes.\n    \"\"\"\n    # Retrieve configuration details from the encoder manager\n    class_name_key = encoder_manager.config['class_name_key']\n    class_id_to_name = encoder_manager.config.get('class_id_to_name', {})\n\n    # Initialize counters for accuracy calculation\n    class_accuracy = {}\n    total_correct = 0\n    total_items = 0\n\n    # Process each item in the dataset\n    for item in dataset_items:\n        audio_file = item['audio_file']\n        true_class_id = item['class_id']\n        true_class_name = class_id_to_name.get(true_class_id, true_class_id)\n\n        # Compute embedding and predict class\n        embedding = encoder_manager.compute_embedding(audio_file)\n        predicted_class_id = encoder_manager.predict_class(embedding)\n        predicted_class_name = class_id_to_name.get(predicted_class_id, predicted_class_id)\n\n        # Check if prediction is correct\n        is_correct = predicted_class_name == true_class_name\n        if true_class_name not in class_accuracy:\n            class_accuracy[true_class_name] = {'correct': 0, 'total': 0}\n        class_accuracy[true_class_name]['correct'] += is_correct\n        class_accuracy[true_class_name]['total'] += 1\n\n        # Update total counts\n        total_correct += is_correct\n        total_items += 1\n\n    # Calculate and print accuracy for each class and overall\n    for class_name, counts in class_accuracy.items():\n        accuracy = counts['correct'] / counts['total']\n        print(f\"Accuracy for {class_name}: {accuracy:.2f}\")\n\n    if total_items > 0:\n        overall_accuracy = total_correct / total_items\n        print(f\"Overall accuracy: {overall_accuracy:.2f}\")\n    else:\n        print(\"No items to evaluate.\")", "CodeBleu_score": 32.79181126929646}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "import os\nimport shutil\nfrom pydub import AudioSegment\nfrom pydub.silence import split_on_silence\n\ndef adjust_path_and_remove_silence(audio_path, output_dir, force=False, silence_thresh=-40, min_silence_len=700):\n    \"\"\"\n    The function adjusts the output path and removes silence from an audio file.\n    It replaces the input directory path with the output directory path to determine the new file location.\n    If the file already exists and force is not specified, it returns the output path and a False flag.\n    Otherwise, it creates the necessary directory structure, removes silence from the audio using specified parameters,\n    and saves the processed audio.\n    Finally, it returns the output path and a flag indicating if speech was detected.\n\n    Args:\n    audio_path (str): Path to the input audio file.\n    output_dir (str): Directory where the processed file should be saved.\n    force (bool): If True, overwrite existing files. Defaults to False.\n    silence_thresh (int): Silence threshold (in dB). Lower values mean more silence detected. Defaults to -40 dB.\n    min_silence_len (int): Minimum length of silence to be used for splitting (in milliseconds). Defaults to 700 ms.\n\n    Returns:\n    tuple: (output_path, was_speech_detected)\n    \"\"\"\n    # Determine new file location\n    base_name = os.path.basename(audio_path)\n    output_path = os.path.join(output_dir, base_name)\n\n    # Check if the file already exists\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Load audio file\n    audio = AudioSegment.from_file(audio_path)\n\n    # Split audio where silence is 700ms or more and get chunks\n    chunks = split_on_silence(audio,\n                              min_silence_len=min_silence_len,\n                              silence_thresh=silence_thresh)\n\n    # Process and concatenate chunks\n    processed_audio = AudioSegment.empty()\n    for chunk in chunks:\n        processed_audio += chunk\n\n    # Check if there is any non-silent audio detected\n    was_speech_detected = len(processed_audio) > 0\n\n    # Save the processed audio\n    processed_audio.export(output_path, format=\"wav\")\n\n    return output_path, was_speech_detected", "CodeBleu_score": 41.31953701998714}, {"code": "import os\nfrom pydub import AudioSegment\nfrom pydub.silence import split_on_silence\n\ndef adjust_path_and_remove_silence(audio_path, input_dir='input/', output_dir='output/', force=False):\n    \"\"\"\n    The function adjusts the output path and removes silence from an audio file.\n    It replaces the input directory path with the output directory path to determine the new file location.\n    If the file already exists and force is not specified, it returns the output path and a False flag.\n    Otherwise, it creates the necessary directory structure, removes silence from the audio using specified parameters, and saves the processed audio.\n    Finally, it returns the output path and a flag indicating if speech was detected.\n    \"\"\"\n    # Adjust the output path\n    output_path = audio_path.replace(input_dir, output_dir)\n\n    # Check if the file already exists\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n\n    # Create the directory if it does not exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Load the audio file\n    audio = AudioSegment.from_file(audio_path)\n\n    # Process the audio to remove silence\n    chunks = split_on_silence(audio, min_silence_len=500, silence_thresh=-40)\n    if not chunks:\n        return output_path, False  # No speech detected\n\n    # Combine chunks back into a single audio file\n    processed_audio = sum(chunks)\n\n    # Save the processed audio\n    processed_audio.export(output_path, format='wav')\n\n    # Return the output path and a flag indicating speech was detected\n    return output_path, True", "CodeBleu_score": 41.63097805425028}, {"code": "import os\nimport shutil\nfrom pydub import AudioSegment\nfrom pydub.silence import split_on_silence\n\ndef adjust_path_and_remove_silence(audio_path, output_dir, force=False, silence_thresh=-50, min_silence_len=700):\n    \"\"\"\n    The function adjusts the output path and removes silence from an audio file.\n    It replaces the input directory path with the output directory path to determine the new file location.\n    If the file already exists and force is not specified, it returns the output path and a False flag.\n    Otherwise, it creates the necessary directory structure, removes silence from the audio using specified parameters, and saves the processed audio.\n    Finally, it returns the output path and a flag indicating if speech was detected.\n\n    Parameters:\n        audio_path (str): Path to the input audio file.\n        output_dir (str): Path to the output directory where the processed file will be saved.\n        force (bool): If True, overwrite the existing file. Default is False.\n        silence_thresh (int): Silence threshold in dB. Default is -50 dB.\n        min_silence_len (int): Minimum length of silence to be used for splitting. Default is 700 ms.\n\n    Returns:\n        tuple: (output_path, was_speech_detected)\n    \"\"\"\n    # Determine the new file location\n    base_name = os.path.basename(audio_path)\n    output_path = os.path.join(output_dir, base_name)\n\n    # Check if the file already exists\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n\n    # Create the output directory if it does not exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Load the audio file\n    audio = AudioSegment.from_file(audio_path)\n\n    # Split audio on silence\n    chunks = split_on_silence(audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)\n\n    # Check if any speech was detected\n    was_speech_detected = len(chunks) > 0\n\n    # Combine chunks back into a single audio file\n    processed_audio = AudioSegment.silent(duration=0)  # Create a silent segment to concatenate with\n    for chunk in chunks:\n        processed_audio += chunk\n\n    # Save the processed audio\n    processed_audio.export(output_path, format=\"wav\")\n\n    return output_path, was_speech_detected", "CodeBleu_score": 41.39801097428118}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "import os\nimport librosa\nfrom joblib import Parallel, delayed\nimport soundfile as sf\nfrom tqdm import tqdm\n\ndef resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10):\n    \"\"\"\n    The function resamples audio files in a specified directory to a given sample rate.\n    If an output directory is provided, it copies the input directory to the output directory.\n    It then recursively finds all audio files with the specified extension and resamples them in parallel using a specified number of jobs.\n    Progress is displayed with a progress bar, and a completion message is printed at the end.\n    \"\"\"\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n    # Collect all files with the specified extension\n    files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(input_dir) for f in filenames if f.endswith(file_ext)]\n\n    def process_file(file_path):\n        # Load audio file\n        y, sr = librosa.load(file_path, sr=None)\n        # Resample audio\n        y_resampled = librosa.resample(y, sr, output_sr)\n        # Define output path\n        output_path = os.path.join(output_dir, os.path.relpath(file_path, input_dir))\n        # Ensure the output directory exists\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        # Save resampled audio\n        sf.write(output_path, y_resampled, output_sr)\n\n    # Process files in parallel\n    Parallel(n_jobs=n_jobs)(delayed(process_file)(file) for file in tqdm(files))\n\n    print(\"Resampling completed. All files have been processed and saved to\", output_dir)", "CodeBleu_score": 32.06378535740279}, {"code": "import os\nimport librosa\nfrom joblib import Parallel, delayed\nimport soundfile as sf\n\ndef resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10):\n    \"\"\"\n    The function resamples audio files in a specified directory to a given sample rate.\n    If an output directory is provided, it copies the input directory to the output directory.\n    It then recursively finds all audio files with the specified extension and resamples them in parallel using a specified number of jobs.\n    Progress is displayed with a progress bar, and a completion message is printed at the end.\n    \"\"\"\n    if output_dir is None:\n        output_dir = input_dir\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Function to resample a single file\n    def resample_audio(file_path):\n        # Load audio file\n        data, sr = librosa.load(file_path, sr=None)\n        # Resample audio\n        data_resampled = librosa.resample(data, sr, output_sr)\n        # Define output file path\n        output_file_path = os.path.join(output_dir, os.path.relpath(file_path, input_dir))\n        # Ensure the output directory exists\n        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n        # Save resampled audio\n        sf.write(output_file_path, data_resampled, output_sr)\n        return output_file_path\n\n    # List all files in the directory that match the extension\n    files_to_resample = [os.path.join(dp, f) for dp, dn, filenames in os.walk(input_dir) \n                         for f in filenames if f.endswith(file_ext)]\n\n    # Process files in parallel\n    resampled_files = Parallel(n_jobs=n_jobs)(\n        delayed(resample_audio)(file_path) for file_path in files_to_resample\n    )\n\n    print(f\"All files have been resampled and saved to {output_dir}.\")", "CodeBleu_score": 33.801961324769806}, {"code": "import os\nimport librosa\nfrom joblib import Parallel, delayed\nimport shutil\nimport soundfile as sf\nfrom tqdm import tqdm\n\ndef resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10):\n    \"\"\"\n    The function resamples audio files in a specified directory to a given sample rate.\n    If an output directory is provided, it copies the input directory to the output directory.\n    It then recursively finds all audio files with the specified extension and resamples them in parallel using a specified number of jobs.\n    Progress is displayed with a progress bar, and a completion message is printed at the end.\n    \"\"\"\n    if output_dir is None:\n        output_dir = input_dir\n    else:\n        # Copy the input directory structure to the output directory\n        if os.path.exists(output_dir):\n            shutil.rmtree(output_dir)\n        shutil.copytree(input_dir, output_dir)\n\n    # List all files in the directory that match the extension\n    audio_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(output_dir) for f in filenames if f.endswith(file_ext)]\n\n    # Function to resample a single file\n    def resample_audio(file_path):\n        # Load audio file\n        audio, sr = librosa.load(file_path, sr=None)\n        # Resample audio\n        audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=output_sr)\n        # Save resampled audio\n        sf.write(file_path, audio_resampled, output_sr)\n\n    # Process all files in parallel\n    Parallel(n_jobs=n_jobs)(delayed(resample_audio)(file) for file in tqdm(audio_files))\n\n    print(\"Resampling completed for all files.\")\n\n# Example usage\n# resample_files('path/to/input_dir', 16000, 'path/to/output_dir')", "CodeBleu_score": 37.07089359165561}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "import torch\nimport umap\nimport matplotlib.pyplot as plt\n\ndef evaluation(model, criterion, data_loader, global_step):\n    \"\"\"\n    The function evaluates a model using a given criterion and data loader at a specific global step.\n    It computes the average evaluation loss over all batches without updating model parameters.\n    Inputs and labels are reshaped to group samples of each class, and data is dispatched to the GPU if available.\n    After computing the loss for each batch, it calculates the average loss and logs evaluation statistics.\n    It also generates and logs a UMAP plot of the embeddings from the last batch.\n    The function returns the average evaluation loss.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    total_loss = 0\n    total_batches = 0\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    embeddings = []\n    labels = []\n\n    with torch.no_grad():  # No gradients needed\n        for inputs, targets in data_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            total_loss += loss.item()\n            total_batches += 1\n\n            # Collect embeddings and labels for UMAP\n            embeddings.append(outputs.cpu().numpy())\n            labels.append(targets.cpu().numpy())\n\n    average_loss = total_loss / total_batches\n\n    # Log the average loss and the global step\n    print(f\"Global Step: {global_step}, Average Loss: {average_loss}\")\n\n    # Generate UMAP plot from the last batch's embeddings\n    reducer = umap.UMAP()\n    embedding_plot = reducer.fit_transform(embeddings[-1])\n    plt.scatter(embedding_plot[:, 0], embedding_plot[:, 1], c=labels[-1], cmap='Spectral', s=5)\n    plt.colorbar()\n    plt.title(f\"UMAP projection at Global Step {global_step}\")\n    plt.show()\n\n    return average_loss", "CodeBleu_score": 32.89813974544927}, {"code": "import torch\nimport umap\nimport matplotlib.pyplot as plt\n\ndef evaluation(model, criterion, data_loader, global_step):\n    \"\"\"\n    The function evaluates a model using a given criterion and data loader at a specific global step.\n    It computes the average evaluation loss over all batches without updating model parameters.\n    Inputs and labels are reshaped to group samples of each class, and data is dispatched to the GPU if available.\n    After computing the loss for each batch, it calculates the average loss and logs evaluation statistics.\n    It also generates and logs a UMAP plot of the embeddings from the last batch.\n    The function returns the average evaluation loss.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    total_loss = 0\n    total_batches = 0\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    with torch.no_grad():  # No gradients needed\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            total_batches += 1\n\n            # Save the last batch's embeddings for UMAP visualization\n            last_embeddings = outputs.detach().cpu().numpy()\n            last_labels = labels.detach().cpu().numpy()\n\n    average_loss = total_loss / total_batches\n\n    # UMAP plot of the embeddings from the last batch\n    reducer = umap.UMAP()\n    embedding = reducer.fit_transform(last_embeddings)\n    plt.scatter(embedding[:, 0], embedding[:, 1], c=last_labels, cmap='Spectral', s=5)\n    plt.colorbar()\n    plt.title(f'UMAP plot at Global Step {global_step}')\n    plt.savefig(f'umap_global_step_{global_step}.png')\n    plt.close()\n\n    return average_loss", "CodeBleu_score": 34.84021057143184}, {"code": "import torch\nimport umap\nimport matplotlib.pyplot as plt\n\ndef evaluation(model, criterion, data_loader, global_step):\n    \"\"\"\n    The function evaluates a model using a given criterion and data loader at a specific global step.\n    It computes the average evaluation loss over all batches without updating model parameters.\n    Inputs and labels are reshaped to group samples of each class, and data is dispatched to the GPU if available.\n    After computing the loss for each batch, it calculates the average loss and logs evaluation statistics.\n    It also generates and logs a UMAP plot of the embeddings from the last batch.\n    The function returns the average evaluation loss.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    total_loss = 0.0\n    total_batches = 0\n    last_batch_embeddings = None\n    \n    with torch.no_grad():  # No gradients needed\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            total_batches += 1\n            last_batch_embeddings = outputs.detach().cpu().numpy()  # Save embeddings from the last batch\n    \n    average_loss = total_loss / total_batches if total_batches > 0 else 0\n    \n    # Log the average loss and the global step\n    print(f\"Global Step: {global_step}, Average Loss: {average_loss}\")\n    \n    # Generate UMAP plot of the embeddings from the last batch\n    if last_batch_embeddings is not None:\n        reducer = umap.UMAP()\n        embedding = reducer.fit_transform(last_batch_embeddings)\n        plt.scatter(embedding[:, 0], embedding[:, 1])\n        plt.title('UMAP plot of embeddings')\n        plt.show()\n    \n    return average_loss", "CodeBleu_score": 34.23084108638488}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "from importlib import import_module\n\nclass Coqpit:\n    # Assuming Coqpit is a base class for configurations\n    pass\n\ndef register_config(model_name: str) -> Coqpit:\n    \"\"\"\n    The function registers a configuration class for a given model name.\n    It constructs the configuration class name and checks if the model is \"xtts\" to import its specific configuration.\n    It searches through predefined paths to find and assign the configuration class, handling ModuleNotFoundError exceptions.\n    If the configuration class is not found, it raises a ModuleNotFoundError.\n    The function returns the found configuration class.\n    \"\"\"\n    config_class_name = f\"{model_name.capitalize()}Config\"\n    try:\n        if model_name == \"xtts\":\n            config_module = import_module(f\"models.{model_name}.{config_class_name}\")\n        else:\n            config_module = import_module(f\"config.{config_class_name}\")\n        config_class = getattr(config_module, config_class_name)\n        return config_class\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(f\"No configuration class found for model '{model_name}'\")", "CodeBleu_score": 40.67015210301228}, {"code": "from typing import Type\n\nclass Coqpit:\n    pass\n\ndef register_config(model_name: str) -> Type[Coqpit]:\n    \"\"\"\n    The function registers a configuration class for a given model name.\n    It constructs the configuration class name and checks if the model is \"xtts\" to import its specific configuration.\n    It searches through predefined paths to find and assign the configuration class, handling ModuleNotFoundError exceptions.\n    If the configuration class is not found, it raises a ModuleNotFoundError.\n    The function returns the found configuration class.\n    \"\"\"\n    config_class_name = f\"{model_name.capitalize()}Config\"\n    \n    try:\n        if model_name == \"xtts\":\n            from xtts_configurations import Coqpit as ConfigClass\n        else:\n            module_path = f\"models.{model_name}_configurations\"\n            ConfigClass = getattr(__import__(module_path, fromlist=[config_class_name]), config_class_name)\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(f\"Configuration class for model '{model_name}' could not be found.\")\n    \n    return ConfigClass", "CodeBleu_score": 42.13191534405095}, {"code": "from importlib import import_module\n\nclass Coqpit:\n    # Assuming Coqpit is a base class for configurations\n    pass\n\ndef register_config(model_name: str) -> Coqpit:\n    \"\"\"\n    The function registers a configuration class for a given model name.\n    It constructs the configuration class name and checks if the model is \"xtts\" to import its specific configuration.\n    It searches through predefined paths to find and assign the configuration class, handling ModuleNotFoundError exceptions.\n    If the configuration class is not found, it raises a ModuleNotFoundError.\n    The function returns the found configuration class.\n    \"\"\"\n    config_class_name = f\"{model_name.capitalize()}Config\"\n    try:\n        if model_name == \"xtts\":\n            config_module = import_module(f\"models.{model_name}.config\")\n        else:\n            config_module = import_module(f\"configurations.{config_class_name}\")\n        config_class = getattr(config_module, config_class_name)\n        return config_class()\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(f\"Configuration class {config_class_name} could not be found for model {model_name}.\")\n    except AttributeError:\n        raise ModuleNotFoundError(f\"The module does not contain the class {config_class_name}.\")", "CodeBleu_score": 39.06596708901697}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "import json\nimport yaml\nfrom typing import Type\n\nclass Coqpit:\n    # Assuming Coqpit is a base class for configuration classes\n    pass\n\ndef load_config(config_path: str) -> Coqpit:\n    # Determine the file extension\n    if config_path.endswith('.yaml') or config_path.endswith('.yml'):\n        with open(config_path, 'r') as file:\n            config_data = yaml.safe_load(file)\n    elif config_path.endswith('.json'):\n        with open(config_path, 'r') as file:\n            config_data = json.load(file)\n    else:\n        raise ValueError(\"Unsupported file type. Please use a .yaml or .json file.\")\n\n    # Process the configuration dictionary to obtain the model name\n    model_name = config_data.get('model_name')\n    if not model_name:\n        raise ValueError(\"Model name is required in the configuration.\")\n\n    # Register the appropriate configuration class based on model_name\n    # This assumes there's a mapping of model names to configuration classes\n    config_class = _register_config_class(model_name)\n    if not config_class:\n        raise ValueError(f\"No configuration class registered for model '{model_name}'.\")\n\n    # Create an instance of the configuration class and populate it\n    config_instance = config_class()\n    config_instance.populate(config_data)  # Assuming a populate method exists\n\n    return config_instance\n\ndef _register_config_class(model_name: str) -> Type[Coqpit]:\n    # Dummy implementation of registering config classes\n    # Replace this with actual logic to map model names to config classes\n    config_classes = {\n        'example_model': ExampleConfig,  # ExampleConfig should be defined elsewhere\n    }\n    return config_classes.get(model_name)", "CodeBleu_score": 39.66050053549446}, {"code": "import json\nimport yaml\nfrom typing import Any\n\nclass Coqpit:\n    def __init__(self, data: dict):\n        for key, value in data.items():\n            setattr(self, key, value)\n\ndef load_config(config_path: str) -> Coqpit:\n    # Determine the file extension\n    if config_path.endswith('.yaml') or config_path.endswith('.yml'):\n        with open(config_path, 'r') as file:\n            config_data = yaml.safe_load(file)\n    elif config_path.endswith('.json'):\n        with open(config_path, 'r') as file:\n            config_data = json.load(file)\n    else:\n        raise ValueError(\"Unsupported file type. Please use a .yaml or .json file.\")\n\n    # Assuming the configuration class is determined by a key in the config_data\n    model_name = config_data.get('model_name')\n    if model_name == 'ModelX':\n        config_class = Coqpit  # Example: replace with actual class lookup/logic\n    else:\n        raise ValueError(\"Unknown model name.\")\n\n    # Create an instance of the configuration class with the loaded data\n    config_instance = config_class(config_data)\n    \n    return config_instance", "CodeBleu_score": 40.230468678532205}, {"code": "import json\nimport yaml\nfrom typing import Any\n\nclass Coqpit:\n    def __init__(self, **kwargs):\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\ndef load_config(config_path: str) -> Coqpit:\n    \"\"\"The function loads a configuration from a given file path.\n    It determines the file extension and reads the content accordingly as YAML or JSON, raising an error for unknown types.\n    The function processes the configuration dictionary to obtain the model name and registers the appropriate configuration class.\n    It then creates an instance of the configuration class, populates it with the loaded data, and returns the configuration object.\"\"\"\n    \n    # Determine the file extension\n    if config_path.endswith('.yaml') or config_path.endswith('.yml'):\n        with open(config_path, 'r') as file:\n            config_data = yaml.safe_load(file)\n    elif config_path.endswith('.json'):\n        with open(config_path, 'r') as file:\n            config_data = json.load(file)\n    else:\n        raise ValueError(\"Unsupported file type. Please use a .yaml or .json file.\")\n    \n    # Assuming the model name is stored under 'model_name' in the configuration\n    model_name = config_data.get('model_name')\n    if not model_name:\n        raise ValueError(\"Model name not specified in the configuration.\")\n    \n    # Register the appropriate configuration class based on the model name\n    # This is a placeholder: replace with actual registration logic if available\n    config_class = Coqpit  # This should be dynamically determined based on model_name\n    \n    # Create an instance of the configuration class and populate it\n    config_instance = config_class(**config_data)\n    \n    return config_instance", "CodeBleu_score": 37.519133470960846}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "import os\n\ndef list_files(basePath, validExts=None, contains=None):\n    \"\"\"\n    The function lists files in a directory structure based on specified criteria.\n    It recursively walks through the base directory and processes each file.\n    If a 'contains' string is provided, it skips files that do not contain this string in their names.\n    It checks the file extension against valid extensions if provided.\n    For valid files, it constructs and yields the full file path.\n    \"\"\"\n    # Normalize the valid extensions to be in a consistent format\n    if validExts is not None:\n        validExts = tuple([ext.lower() for ext in validExts])\n\n    # Walk through the directory\n    for dirpath, dirnames, filenames in os.walk(basePath):\n        for filename in filenames:\n            # Check if the file contains the specified string\n            if contains and contains not in filename:\n                continue\n\n            # Check the file extension\n            ext = os.path.splitext(filename)[1].lower()\n            if validExts and ext not in validExts:\n                continue\n\n            # Yield the full path of the file\n            yield os.path.join(dirpath, filename)", "CodeBleu_score": 45.63089182538964}, {"code": "import os\n\ndef list_files(basePath, validExts=None, contains=None):\n    \"\"\"\n    The function lists files in a directory structure based on specified criteria.\n    It recursively walks through the base directory and processes each file.\n    If a 'contains' string is provided, it skips files that do not contain this string in their names.\n    It checks the file extension against valid extensions if provided.\n    For valid files, it constructs and yields the full file path.\n    \"\"\"\n    # Normalize the valid extensions to be in a consistent format\n    if validExts is not None:\n        validExts = tuple([ext.lower() if ext.startswith('.') else f\".{ext.lower()}\" for ext in validExts])\n\n    # Walk through the directory structure\n    for dirpath, dirnames, filenames in os.walk(basePath):\n        for filename in filenames:\n            # Check if the file contains the specified string\n            if contains and contains not in filename:\n                continue\n\n            # Check the file extension\n            ext = os.path.splitext(filename)[1].lower()\n            if validExts and ext not in validExts:\n                continue\n\n            # Yield the full path of the file\n            yield os.path.join(dirpath, filename)", "CodeBleu_score": 46.265731466038204}, {"code": "import os\n\ndef list_files(basePath, validExts=None, contains=None):\n    \"\"\"\n    The function lists files in a directory structure based on specified criteria.\n    It recursively walks through the base directory and processes each file.\n    If a 'contains' string is provided, it skips files that do not contain this string in their names.\n    It checks the file extension against valid extensions if provided.\n    For valid files, it constructs and yields the full file path.\n    \"\"\"\n    # Normalize the valid extensions to be in a consistent format\n    if validExts is not None:\n        validExts = tuple(ext.lower() for ext in validExts)\n\n    # Walk through the directory\n    for dirpath, dirnames, filenames in os.walk(basePath):\n        for filename in filenames:\n            # Check if the file contains the specified string\n            if contains is not None and contains not in filename:\n                continue\n\n            # Check the file extension\n            ext = os.path.splitext(filename)[1].lower()\n            if validExts is not None and ext not in validExts:\n                continue\n\n            # Yield the full path of the file\n            yield os.path.join(dirpath, filename)", "CodeBleu_score": 48.37037701154435}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "import torch\n\ndef load_model(xtts_checkpoint, xtts_config, xtts_vocab):\n    \"\"\"\n    The function loads an XTTS model using specified checkpoint, config, and vocab paths.\n    It first clears the GPU cache and checks if the required paths are provided, returning an error message if not.\n    It then loads the configuration from the provided path, initializes the XTTS model, and loads the model checkpoint and vocabulary.\n    If a GPU is available, it moves the model to the GPU.\n    The function prints messages indicating the loading progress and returns a success message once the model is loaded.\n    \"\"\"\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Check if the paths are provided\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: All file paths must be provided.\"\n\n    # Load the configuration\n    try:\n        config = torch.load(xtts_config)\n    except Exception as e:\n        return f\"Failed to load configuration: {e}\"\n\n    # Initialize the XTTS model\n    try:\n        model = XTTSModel(config)\n    except Exception as e:\n        return f\"Failed to initialize the XTTS model: {e}\"\n\n    # Load the model checkpoint\n    try:\n        model.load_state_dict(torch.load(xtts_checkpoint))\n    except Exception as e:\n        return f\"Failed to load model checkpoint: {e}\"\n\n    # Load the vocabulary\n    try:\n        with open(xtts_vocab, 'r') as file:\n            vocab = file.read()\n    except Exception as e:\n        return f\"Failed to load vocabulary: {e}\"\n\n    # Move the model to GPU if available\n    if torch.cuda.is_available():\n        model.cuda()\n        print(\"Model moved to GPU.\")\n\n    print(\"Model loading complete.\")\n    return \"Model loaded successfully.\"", "CodeBleu_score": 51.48870741026581}, {"code": "import torch\n\ndef load_model(xtts_checkpoint, xtts_config, xtts_vocab):\n    \"\"\"\n    The function loads an XTTS model using specified checkpoint, config, and vocab paths.\n    It first clears the GPU cache and checks if the required paths are provided, returning an error message if not.\n    It then loads the configuration from the provided path, initializes the XTTS model, and loads the model checkpoint and vocabulary.\n    If a GPU is available, it moves the model to the GPU.\n    The function prints messages indicating the loading progress and returns a success message once the model is loaded.\n    \"\"\"\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n    \n    # Check if all paths are provided\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: All file paths must be provided.\"\n    \n    # Load configuration\n    try:\n        config = torch.load(xtts_config)\n    except Exception as e:\n        return f\"Failed to load configuration: {e}\"\n    \n    # Initialize the XTTS model\n    try:\n        model = XTTSModel(config)\n    except Exception as e:\n        return f\"Failed to initialize the XTTS model: {e}\"\n    \n    # Load model checkpoint\n    try:\n        model.load_state_dict(torch.load(xtts_checkpoint))\n    except Exception as e:\n        return f\"Failed to load model checkpoint: {e}\"\n    \n    # Load vocabulary\n    try:\n        with open(xtts_vocab, 'r') as file:\n            vocab = file.read()\n    except Exception as e:\n        return f\"Failed to load vocabulary: {e}\"\n    \n    # Move model to GPU if available\n    if torch.cuda.is_available():\n        model.cuda()\n        print(\"Model moved to GPU.\")\n    else:\n        print(\"GPU is not available, model is on CPU.\")\n    \n    print(\"Model loading complete.\")\n    return \"Model loaded successfully.\"", "CodeBleu_score": 50.84756494801718}, {"code": "import torch\n\ndef load_model(xtts_checkpoint, xtts_config, xtts_vocab):\n    \"\"\"\n    The function loads an XTTS model using specified checkpoint, config, and vocab paths.\n    It first clears the GPU cache and checks if the required paths are provided, returning an error message if not.\n    It then loads the configuration from the provided path, initializes the XTTS model, and loads the model checkpoint and vocabulary.\n    If a GPU is available, it moves the model to the GPU.\n    The function prints messages indicating the loading progress and returns a success message once the model is loaded.\n    \"\"\"\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Check if all paths are provided\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"Error: All file paths must be provided.\"\n\n    print(\"Loading configuration...\")\n    # Load configuration\n    config = torch.load(xtts_config)\n\n    print(\"Initializing the XTTS model...\")\n    # Initialize the XTTS model\n    model = XTTSModel(config)\n\n    print(\"Loading model checkpoint...\")\n    # Load model checkpoint\n    model.load_state_dict(torch.load(xtts_checkpoint))\n\n    print(\"Loading vocabulary...\")\n    # Load vocabulary\n    vocab = torch.load(xtts_vocab)\n\n    # Check if GPU is available and move model to GPU\n    if torch.cuda.is_available():\n        model = model.cuda()\n        print(\"Model moved to GPU.\")\n\n    print(\"Model loading complete.\")\n    return \"Model loaded successfully.\"", "CodeBleu_score": 54.974356844609005}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "import torch\n\ndef compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True):\n    \"\"\"\n    The function computes embeddings for a given input x.\n    It adjusts the number of frames based on the hop length if using torch specifications.\n    It ensures the number of frames does not exceed the input length and calculates offset positions for evaluation.\n    It extracts frames at these offsets, concatenates them into a batch, and performs inference to obtain embeddings.\n    If specified, it computes the mean of the embeddings and returns the result, optionally normalizing them with L2 norm.\n    \"\"\"\n    # Assuming x is a 1D tensor (audio signal) and self.model is a pre-trained model that accepts batches of frames.\n    \n    # Calculate the hop length to evenly distribute the frames\n    hop_length = len(x) // num_frames if len(x) > num_frames else 1\n    \n    # Calculate offsets for evaluation\n    offsets = torch.linspace(0, len(x) - hop_length, num_eval).long()\n    \n    # Extract frames at these offsets\n    frames = torch.stack([x[offset:offset + hop_length] for offset in offsets])\n    \n    # Perform inference to obtain embeddings\n    embeddings = self.model(frames)\n    \n    # Optionally compute the mean of the embeddings\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n    \n    # Optionally apply L2 normalization\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=0)\n    \n    return embeddings", "CodeBleu_score": 33.88584081401017}, {"code": "import torch\n\ndef compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True):\n    \"\"\"\n    The function computes embeddings for a given input x.\n    It adjusts the number of frames based on the hop length if using torch specifications.\n    It ensures the number of frames does not exceed the input length and calculates offset positions for evaluation.\n    It extracts frames at these offsets, concatenates them into a batch, and performs inference to obtain embeddings.\n    If specified, it computes the mean of the embeddings and returns the result, optionally normalizing them with L2 norm.\n    \"\"\"\n    # Assuming x is a 1D tensor (audio signal) and self.model is a pre-trained model for embedding extraction\n    input_length = x.shape[0]\n    hop_length = input_length // num_frames\n    \n    # Ensure the number of frames does not exceed the input length\n    num_frames = min(num_frames, input_length // hop_length)\n    \n    # Calculate offset positions for evaluation\n    offsets = torch.linspace(0, input_length - hop_length, num_eval).long()\n    \n    # Extract frames at these offsets\n    frames = [x[offset:offset + hop_length] for offset in offsets]\n    \n    # Concatenate them into a batch\n    batch = torch.stack(frames)\n    \n    # Perform inference to obtain embeddings\n    embeddings = self.model(batch)\n    \n    # Compute the mean of the embeddings if specified\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n    \n    # Normalize embeddings with L2 norm if specified\n    if l2_norm:\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=0)\n    \n    return embeddings", "CodeBleu_score": 37.884785308131505}, {"code": "import torch\nimport torch.nn.functional as F\n\ndef compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True):\n    \"\"\"\n    The function computes embeddings for a given input x.\n    It adjusts the number of frames based on the hop length if using torch specifications.\n    It ensures the number of frames does not exceed the input length and calculates offset positions for evaluation.\n    It extracts frames at these offsets, concatenates them into a batch, and performs inference to obtain embeddings.\n    If specified, it computes the mean of the embeddings and returns the result, optionally normalizing them with L2 norm.\n    \"\"\"\n    # Assuming `x` is a 1D tensor (audio signal) and `self.model` is your embedding model\n    input_length = x.shape[0]\n    frame_length = input_length // num_frames  # Adjust frame length based on total frames desired\n\n    # Ensure the number of frames does not exceed the input length\n    if frame_length * num_frames > input_length:\n        num_frames = input_length // frame_length\n\n    # Calculate offset positions for evaluation\n    offsets = torch.linspace(0, input_length - frame_length, num_eval).long()\n\n    # Extract frames at these offsets\n    frames = torch.stack([x[offset:offset + frame_length] for offset in offsets])\n\n    # Perform inference to obtain embeddings\n    embeddings = self.model(frames)\n\n    # Compute the mean of the embeddings if specified\n    if return_mean:\n        embeddings = embeddings.mean(dim=0)\n\n    # Normalize embeddings with L2 norm if specified\n    if l2_norm:\n        embeddings = F.normalize(embeddings, p=2, dim=0)\n\n    return embeddings", "CodeBleu_score": 37.01130567299663}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "import torch.nn as nn\n\ndef create_layer(self, block, planes, blocks, stride=1):\n    \"\"\"\n    The function creates a neural network layer consisting of a specified block repeated a given number of times.\n    It initializes a downsample operation if needed based on the stride and input planes.\n    The function appends the first block with the downsample to the layer list, updates the input planes, and then appends the remaining blocks.\n    Finally, it returns the constructed layer as an nn.Sequential object.\n    \"\"\"\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)", "CodeBleu_score": 98.1493024004157}, {"code": "import torch.nn as nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n\n    def create_layer(self, block, planes, blocks, stride=1):\n        \"\"\"\n        The function creates a neural network layer consisting of a specified block repeated a given number of times.\n        It initializes a downsample operation if needed based on the stride and input planes.\n        The function appends the first block with the downsample to the layer list, updates the input planes, and then appends the remaining blocks.\n        Finally, it returns the constructed layer as an nn.Sequential object.\n        \"\"\"\n        downsample = None\n        layers = []\n\n        # Assuming the block requires a parameter called 'expansion' to adjust output planes\n        out_planes = planes * block.expansion\n        \n        # Condition to apply downsample, here we downsample when stride is different from 1 or plane sizes need adjustment\n        if stride != 1 or self.inplanes != out_planes:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, out_planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_planes)\n            )\n        \n        # First block with potentially a downsample\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = out_planes  # Update inplanes for the next blocks\n\n        # Remaining blocks with default stride of 1\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        # Construct the layer as nn.Sequential and return\n        return nn.Sequential(*layers)\n\n# For demonstration, definition of a block like ResNet's BasicBlock could be added here or imported if defined elsewhere.", "CodeBleu_score": 62.87815131681018}, {"code": "import torch.nn as nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n\n    def create_layer(self, block, planes, blocks, stride=1):\n        \"\"\"\n        The function creates a neural network layer consisting of a specified block repeated a given number of times.\n        It initializes a downsample operation if needed based on the stride and input planes.\n        The function appends the first block with the downsample to the layer list, updates the input planes, and\n        then appends the remaining blocks.\n        Finally, it returns the constructed layer as an nn.Sequential object.\n        \n        Args:\n            block (nn.Module): The type of block to use for building the layer.\n            planes (int): The number of output planes for each block.\n            blocks (int): The total number of blocks to add to the layer.\n            stride (int): The stride for the first block. Default is 1.\n            \n        Returns:\n            nn.Sequential: The constructed layer with the specified blocks.\n        \"\"\"\n        layers = []\n        downsample = None\n\n        # Check if downsample is needed\n        if stride != 1:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes)\n            )\n        \n        # Append the first block, potentially with downsampling\n        layers.append(block(self.in_planes, planes, stride, downsample))\n        self.in_planes = planes  # Update in_planes for the next blocks\n        \n        # Append the remaining blocks\n        for _ in range(1, blocks):\n            layers.append(block(self.in_planes, planes))\n        \n        return nn.Sequential(*layers)", "CodeBleu_score": 57.49138520758652}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "bash\npip install umap-learn matplotlib numpy", "CodeBleu_score": 0.0}, {"code": "import matplotlib.pyplot as plt\nimport umap\nimport numpy as np\n\ndef plot_embeddings(embeddings, labels, num_classes_in_batch):\n    \"\"\"\n    The function plots embeddings using UMAP for dimensionality reduction.\n    It calculates the number of utterances per class and limits the number of classes to 10 if necessary.\n    UMAP transforms the embeddings, and a scatter plot is created with colors representing different classes.\n    The plot is titled \"UMAP projection\", adjusted for equal aspect ratio, saved as \"umap.png\", and returned as a figure.\n    \n    Parameters:\n    embeddings (array): Numpy array of embeddings.\n    labels (array): Corresponding labels for embeddings.\n    num_classes_in_batch (int): Total number of distinct classes present in the batch.\n    \n    Returns:\n    matplotlib.figure.Figure: The figure object containing the UMAP plot.\n    \"\"\"\n\n    # Calculate the number of utterances per class\n    unique, counts = np.unique(labels, return_counts=True)\n    class_counts = dict(zip(unique, counts))\n\n    # Filtering to only top 10 classes if there are more than 10\n    if num_classes_in_batch > 10:\n        # Sorting classes based on the number of samples and select top 10\n        sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)\n        selected_classes = sorted_classes[:10]\n        selected_labels = [cls[0] for cls in selected_classes]\n        indices = [i for i, label in enumerate(labels) if label in selected_labels]\n        embeddings = embeddings[indices]\n        labels = [labels[i] for i in indices]\n    \n    # Fitting UMAP\n    reducer = umap.UMAP()\n    embedding_2d = reducer.fit_transform(embeddings)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=labels, cmap='viridis', s=5)\n    legend1 = ax.legend(*scatter.legend_elements(), loc=\"best\", title=\"Classes\")\n    ax.add_artist(legend1)\n    ax.set_title('UMAP projection')\n    ax.set_aspect('equal', 'datalim')\n    plt.savefig(\"umap.png\")\n    plt.show()\n    \n    return fig", "CodeBleu_score": 42.74518121533969}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport umap\n\ndef plot_embeddings(embeddings, labels, num_classes_in_batch):\n    \"\"\"\n    The function plots embeddings using UMAP for dimensionality reduction.\n    It calculates the number of utterances per class and limits the number of classes to 10 if necessary.\n    UMAP transforms the embeddings, and a scatter plot is created with colors representing different classes.\n    The plot is titled \"UMAP projection\", adjusted for equal aspect ratio, saved as \"umap.png\", and returned as a figure.\n\n    Args:\n    embeddings (np.ndarray): The embeddings array (n_samples, n_features).\n    labels (np.array): Array of labels associated with each embedding.\n    num_classes_in_batch (int): The number of classes to display. If more than 10, only 10 will be shown.\n\n    Returns:\n    matplotlib.figure.Figure: The figure object containing the plot.\n    \"\"\"\n    # Calculate the frequency of each class\n    unique_labels, counts = np.unique(labels, return_counts=True)\n    \n    # Determine the classes to display\n    if len(unique_labels) > num_classes_in_batch:\n        # Sort classes by frequency and select the top `num_classes_in_batch`\n        selected_indices = np.argsort(counts)[::-1][:num_classes_in_batch]\n        selected_labels = unique_labels[selected_indices]\n        mask = np.isin(labels, selected_labels)\n        filtered_embeddings = embeddings[mask]\n        filtered_labels = labels[mask]\n    else:\n        filtered_embeddings = embeddings\n        filtered_labels = labels\n\n    # Reduce dimensions to 2D using UMAP\n    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean')\n    umap_embeddings = reducer.fit_transform(filtered_embeddings)\n\n    # Plotting\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=filtered_labels, cmap='viridis', alpha=0.6)\n    plt.colorbar(scatter, ax=ax)\n    ax.set_title('UMAP projection')\n    ax.set_aspect('equal', 'datalim')\n\n    # Save the plot\n    plt.savefig(\"umap.png\")\n\n    return fig\n\n# Example usage:\n# Assuming `example_embeddings` (a NumPy array of embeddings) and `example_labels` (labels for each embedding) are defined:\n# fig = plot_embeddings(example_embeddings, example_labels, 10)", "CodeBleu_score": 39.68308627392139}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "import numpy as np\n\ndef calc_cosine_sim(self, dvecs, centroids):\n    \"\"\"\n    Calculates the cosine similarity matrix for given d-vectors and centroids.\n    Args:\n    dvecs (list of np.array): List of arrays where each array contains d-vectors for a particular speaker.\n    centroids (list of np.array): List of centroid vectors, one for each speaker.\n\n    Returns:\n    np.array: A matrix of cosine similarity values.\n    \"\"\"\n    \n    # Initialize a list to hold all the cosine similarities for each utterance against new centroids\n    all_cos_sims = []\n\n    # Iterate over each speaker's d-vectors\n    for speaker_index, speaker_dvecs in enumerate(dvecs):\n        speaker_centroid = centroids[speaker_index]\n\n        # Calculate the new centroids by excluding the current utterance\n        num_utterances = len(speaker_dvecs)\n        \n        # Pre-compute centroid adjustments\n        sum_dvecs = np.sum(speaker_dvecs, axis=0)\n        \n        # Initialize a list to store cosine similarities for the current speaker\n        speaker_cos_sims = []\n        \n        for utterance_index, utterance_dvec in enumerate(speaker_dvecs):\n            # Exclude the current utterance from the centroid calculation\n            new_centroid = (sum_dvecs - utterance_dvec) / (num_utterances - 1)\n            \n            # Normalize vectors to ensure they are unit vectors\n            norm_new_centroid = new_centroid / np.linalg.norm(new_centroid)\n            norm_utterance_dvec = utterance_dvec / np.linalg.norm(utterance_dvec)\n            \n            # Calculate cosine similarity\n            cosine_similarity = np.dot(norm_utterance_dvec, norm_new_centroid)\n            \n            # Clamp the value to avoid very small numbers\n            clamped_cos_sim = max(cosine_similarity, 1e-12)\n            \n            speaker_cos_sims.append(clamped_cos_sim)\n        \n        all_cos_sims.append(speaker_cos_sims)\n    \n    # Convert list of cosine similarities into a numpy array\n    final_cos_sim_matrix = np.array(all_cos_sims)\n    \n    return final_cos_sim_matrix", "CodeBleu_score": 37.70312678514342}, {"code": "import numpy as np\n\ndef calc_cosine_sim(self, dvecs, centroids):\n    \"\"\"\n    The function calculates a cosine similarity matrix for given d-vectors and centroids.\n    It iterates over each speaker and their corresponding utterances, computing new centroids \n    excluding the current utterance.\n    For each utterance, it calculates the cosine similarity with the new centroids using \n    vector-based operations for speed.\n    The similarity values are clamped to avoid small values, concatenated, and stacked to \n    form the final cosine similarity matrix.\n    \"\"\"\n    n_speakers = len(dvecs)\n    cosine_sim_matrices = []\n\n    for i in range(n_speakers):\n        speaker_dvecs = dvecs[i]\n        num_utterances = speaker_dvecs.shape[0]\n        similarity_matrix = np.zeros((num_utterances, num_utterances))\n\n        for j in range(num_utterances):\n            # Exclude current utterance to get the 'new centroid'\n            others = np.delete(speaker_dvecs, j, axis=0)\n            new_centroid = np.mean(others, axis=0)\n\n            # Normalize vectors to compute cosine similarity\n            norm_new_centroid = new_centroid / np.linalg.norm(new_centroid)\n            for k in range(num_utterances):\n                utterance_vec = speaker_dvecs[k]\n                norm_utterance_vec = utterance_vec / np.linalg.norm(utterance_vec)\n                similarity = np.dot(norm_utterance_vec, norm_new_centroid)\n                \n                # Clamp values (assuming clamping to avoid negative or very small positive values)\n                similarity = max(0, similarity)\n                \n                similarity_matrix[j, k] = similarity\n\n        cosine_sim_matrices.append(similarity_matrix)\n\n    # Stack all cosine similarity matrices\n    final_matrix = np.vstack(cosine_sim_matrices)\n    return final_matrix", "CodeBleu_score": 34.24719995506825}, {"code": "import numpy as np\nfrom scipy.spatial.distance import cosine\n\ndef calc_cosine_sim(self, dvecs, centroids):\n    \"\"\"\n    The function calculates a cosine similarity matrix for given d-vectors and centroids.\n    It iterates over each speaker and their corresponding utterances, computing new centroids\n    excluding the current utterance. For each utterance, it calculates the cosine similarity\n    with the new centroids using vector-based operations for speed. The similarity values are\n    clamped to avoid small values, concatenated, and stacked to form the final cosine similarity matrix.\n    \n    Args:\n    dvecs (np.ndarray): A 2D array where each row represents a d-vector for an utterance.\n    centroids (np.ndarray): A 2D array where each row represents an initial centroid vector for each cluster/speaker.\n\n    Returns:\n    np.ndarray: A 2D array representing the cosine similarity matrix.\n    \"\"\"\n    \n    num_utterances = dvecs.shape[0]\n    num_speakers = centroids.shape[0]\n    cosine_similarity_matrix = np.zeros((num_utterances, num_speakers))\n\n    # Iterate over each utterance\n    for i in range(num_utterances):\n        # Compute a new centroid for each speaker, excluding the current utterance\n        updated_centroids = np.zeros_like(centroids)\n        for j in range(num_speakers):\n            # Consider using all utterances for the speaker except the current one to update centroids\n            # For simplicity in this example, we recreate the centroids assuming dvecs are centroids\n            # In a real-world use case, you would calculate these excluding the current utterance\n            if num_utterances > 1:  # Prevent division by zero\n                updated_centroids[j] = (np.sum(dvecs, axis=0) - dvecs[i]) / (num_utterances - 1)\n            else:\n                updated_centroids[j] = dvecs[j]  # Fallback if there's only one utterance\n\n        # Calculate cosine similarity with the new centroids\n        for j in range(num_speakers):\n            cos_sim = 1 - cosine(dvecs[i], updated_centroids[j])\n            cosine_similarity_matrix[i, j] = max(0, cos_sim)  # Clamp values to avoid negative due to precision issues\n\n    return cosine_similarity_matrix", "CodeBleu_score": 33.30585556537534}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": "import torch\nimport torch.nn.functional as F\n\nclass EmbedLossModule:\n    def __init__(self):\n        pass\n    \n    def embed_loss_softmax(self, dvecs, cos_sim_matrix):\n        \"\"\"\n        The function calculates the embedding loss using softmax.\n        It iterates over the embeddings, computing the negative log softmax of the cosine similarity matrix for each embedding.\n        The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\n        \n        Args:\n        dvecs: Tensor containing the speaker embeddings (usually output of some neural network layer)\n        cos_sim_matrix: Precomputed cosine similarity matrix of shape (batch_size, batch_size) where entry (i, j)\n                        describes the cosine similarity between embedding i and embedding j.\n\n        Returns:\n        Tensor: A tensor where each element contains the softmax loss for the corresponding embedding.\n        \"\"\"\n\n        # Applying the softmax along each row (dim=1)\n        # which corresponds to each embedding against all other embeddings\n        softmax_scores = F.softmax(cos_sim_matrix, dim=1)\n        \n        # Since the diagonal elements of cos_sim_matrix represent the similarity\n        # of each embedding with itself, these are our \"targets\"\n        # We use gather to pick the diagonal entries from the softmax scores tensor\n        diagonals = torch.diag(softmax_scores)\n\n        # Compute negative log likelihood loss\n        # -log(diagonal elements) where diagonal elements should ideally be close to 1\n        loss = -torch.log(diagonals)\n\n        return loss\n\n# Example initialization and usage:\n# emb_module = EmbedLossModule()\n# loss_output = emb_module.embed_loss_softmax(dvecs, cosine_similarity_matrix)", "CodeBleu_score": 24.92134891250225}, {"code": "import torch\nimport torch.nn.functional as F\n\ndef embed_loss_softmax(self, dvecs, cos_sim_matrix):\n    \"\"\"\n    Calculates the embedding loss using softmax.\n    It iterates over the embeddings, computing the negative log softmax of\n    the cosine similarity matrix for each embedding.\n\n    Parameters:\n        dvecs (tensor): The tensor of embedding vectors, shape (batch_size, embedding_dim).\n        cos_sim_matrix (tensor): Precomputed cosine similarity matrix, shape (batch_size, batch_size).\n\n    Returns:\n        Tensor: The stacked tensor of losses for each embedding.\n    \"\"\"\n    # Apply softmax on the cosine similarity scores along each row\n    softmax_scores = F.softmax(cos_sim_matrix, dim=1)\n    \n    # Get the diagonal entries which are the softmax probabilities of the \n    # true pairs (i.e., similarity of each embedding with itself in the context)\n    true_pair_softmax = torch.diag(softmax_scores)\n    \n    # Compute the negative log of these probabilities\n    loss = -torch.log(true_pair_softmax)\n    \n    return loss", "CodeBleu_score": 23.49479005890517}, {"code": "import torch\nimport torch.nn.functional as F\n\ndef embed_loss_softmax(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using softmax.\n    It iterates over the embeddings, computing the negative log softmax of the cosine similarity matrix for each embedding.\n    The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\n\n    Args:\n        dvecs: Tensor of embeddings with shape [batch_size, embedding_dim].\n        cos_sim_matrix: Tensor with shape [batch_size, batch_size] containing cosine similarities between embeddings.\n    \n    Returns:\n        Tensor: A tensor with the losses for each embedding.\n    \"\"\"\n    # Apply softmax on cosine similarity matrix along each row to get probability distribution.\n    prob_dists = F.softmax(cos_sim_matrix, dim=1)\n    \n    # Now compute the negative log likelihood loss (nll_loss). \n    # We consider correct class indices are along the diagonal (i.e., each embedding's match to itself).\n    # We need to extract indices of diagonal elements.\n    targets = torch.arange(cos_sim_matrix.size(0), device=cos_sim_matrix.device)  # gets 0 to batch_size - 1 as target indices\n    loss = F.nll_loss(torch.log(prob_dists), targets)\n    \n    # If you want individual losses per example instead of a single mean loss over the batch,\n    # you would use the following:\n    # log_prob_dists = torch.log(prob_dists + 1e-9)  # Adding a small term for numerical stability\n    # losses_per_example = -log_prob_dists[torch.arange(cos_sim_matrix.size(0)), torch.arange(cos_sim_matrix.size(0))]\n    \n    return loss\n\n# Example usage assuming torch and F imported from torch.nn.functional:\n# Initialize dvecs, cos_sim_matrix with appropriate tensors and then:\n# model = SomeModel()  # Assuming embed_loss_softmax belongs to SomeModel class\n# loss = model.embed_loss_softmax(dvecs, cos_sim_matrix)\n# print(loss)", "CodeBleu_score": 26.321140787314917}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "import torch\n\ndef embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"\n    Calculate the embedding loss using a contrast loss with the closest centroid.\n    It iterates over the embeddings, computing the sigmoid of the cosine similarity matrix for each embedding.\n    For each embedding, it excludes the current centroid, computes the contrast loss, and appends the result to the loss row.\n    The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\n    \n    Args:\n    dvecs (Tensor): Tensor of embeddings of shape (batch_size, embedding_size).\n    cos_sim_matrix (Tensor): Precomputed cosine similarity matrix of shape (batch_size, batch_size), \n                             where each entry (i, j) represents the cosine similarity between embeddings i and j.\n    \n    Returns:\n    Tensor: Tensor of losses for each embedding.\n    \"\"\"\n\n    batch_size = dvecs.size(0)\n    losses = []\n\n    for i in range(batch_size):\n        # Obtain all cosine similarities for the current embedding, apply sigmoid\n        similarities = cos_sim_matrix[i]\n        sigmoid_similarities = torch.sigmoid(similarities)\n\n        # Set the similarity with the current embedding's centroid to 0 to exclude it from loss calculation\n        sigmoid_similarities[i] = 0 \n\n        # Contrast loss might typically be some aggregate measure deviating the similarities from 1\n        # Here, summing over negative log likelihoods is one common approach; you can define this as needed\n        loss = -torch.sum(torch.log(1 - sigmoid_similarities + 1e-9))  # Adding epsilon for numerical stability\n\n        losses.append(loss)\n\n    # Stack all the calculated losses \n    losses = torch.stack(losses)\n\n    return losses", "CodeBleu_score": 33.51200569518254}, {"code": "import torch\n\ndef embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"\n    The function calculates the embedding loss using contrast loss with the closest centroid.\n    It iterates over the embeddings, computing the sigmoid of the cosine similarity matrix for each embedding.\n    For each embedding, it excludes the current centroid, computes the contrast loss, and appends the result to the loss row.\n    The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\n\n    Parameters:\n    - dvecs: Tensor of embeddings (N, D) where N is the number of embeddings and D is the dimension of each embedding.\n    - cos_sim_matrix: Precomputed cosine similarity matrix (N, N) where each element i, j represents the cosine\n                      similarity between embeddings i and j.\n\n    Returns:\n    - torch.Tensor: A tensor of contrast losses, one for each embedding.\n    \"\"\"\n\n    # We assume that the function is being called within a class that has already imported torch.\n\n    N = cos_sim_matrix.size(0)  # Number of embeddings\n    losses = []\n\n    # Sigmoid activation of cosine similarities\n    sigmoid_sim = torch.sigmoid(cos_sim_matrix)\n\n    for i in range(N):\n        # Mask the similarity of the current embedding to itself\n        mask = torch.ones(N, dtype=torch.bool)\n        mask[i] = 0\n        \n        # Excluded the current embedding's own similarity score\n        relevant_sims = sigmoid_sim[i][mask]\n        \n        # Computing the contrast loss: mean of negative logits (assumes labels would have been 1 for correct class, which is excluded)\n        loss = -torch.log(1 - relevant_sims).mean()  # Minus log of (1-sigmoid) since sigmoid represents 'similarity'\n        losses.append(loss)\n\n    # Stack into a single tensor\n    loss_tensor = torch.stack(losses)\n    return loss_tensor", "CodeBleu_score": 33.758019741506764}, {"code": "import torch\n\ndef embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"\n    The function calculates the embedding loss using contrast loss with the closest centroid.\n    It iterates over the embeddings, computing the sigmoid of the cosine similarity matrix for each embedding.\n    For each embedding, it excludes the current centroid, computes the contrast loss, and appends the result to the loss row.\n    The losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\n\n    Args:\n    - dvecs (torch.Tensor): The embeddings tensor of shape (n_samples, n_features).\n    - cos_sim_matrix (torch.Tensor): Precomputed cosine similarity matrix of the embeddings, shape (n_samples, n_samples).\n\n    Returns:\n    - torch.Tensor: The tensor containing the losses for each embedding.\n    \"\"\"\n    num_samples = dvecs.size(0)\n    losses = []\n\n    # Sigmoid activation over cosine similarity matrix\n    sigmoid_similarities = torch.sigmoid(cos_sim_matrix)\n\n    # Iterate over each row in cosine similarity matrix\n    for i in range(num_samples):\n        # Exclude the i-th sample itself from consideration. Assuming diagonal contains self similarities.\n        mask = torch.ones(num_samples, dtype=bool)\n        mask[i] = False\n        relevant_similarities = sigmoid_similarities[i, mask]\n\n        # Compute contrastive loss, one possibility is using negative log likelihood of the rest\n        # We assume the most straightforward contrastive loss, just the mean negative similarity.\n        # This can vary based on the specific type of contrastive learning being implemented.\n        loss = -torch.mean(relevant_similarities)\n        losses.append(loss)\n\n    # Stack all loss values into a tensor\n    loss_tensor = torch.stack(losses)\n\n    return loss_tensor", "CodeBleu_score": 31.487802471912723}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "import torch\nimport torch.nn.functional as F\n\ndef forward(self, x, _label=None):\n    \"\"\"\n    The function calculates the GE2E loss for input data with dimensions (num_speakers, num_utts_per_speaker, dvec_feats).\n    It ensures the input has at least two utterances per speaker, computes centroids by averaging the embeddings,\n    and calculates a cosine similarity matrix between the embeddings and centroids.\n    The similarity matrix is scaled and shifted, then the embedding loss is computed and averaged.\n    The function returns the mean loss.\n    \n    Args:\n        x (torch.Tensor): A tensor with dimensions (num_speakers, num_utts_per_speaker, dvec_feats) representing speaker embeddings.\n        _label (Any): Placeholder parameter for potential labels, unused in this function.\n\n    Returns:\n        torch.Tensor: Mean Generalized End-to-End (GE2E) loss.\n    \"\"\"\n\n    # Ensure the batch contains at least two utterances per speaker\n    if x.shape[1] < 2:\n        raise ValueError(\"Each speaker needs at least two utterances, found {}\".format(x.shape[1]))\n    \n    # Calculating centroids by averaging the embeddings per speaker\n    centroids = torch.mean(x, dim=1, keepdim=True)  # Shape: (num_speakers, 1, dvec_feats)\n    \n    # Expanding centroids to match the number of utterances for broadcasting in similarity calculation\n    centroids_exp = centroids.expand(-1, x.shape[1], -1)  # Shape: (num_speakers, num_utts_per_speaker, dvec_feats)\n    \n    # Flatten structure to compute pair-wise cosine similarity\n    embeddings_reshape = x.reshape(-1, x.shape[2])  # Shape: (num_speakers * num_utts_per_speaker, dvec_feats)\n    centroids_reshape = centroids_exp.reshape(-1, x.shape[2])  # Shape same as embeddings_reshape\n    \n    # Compute the cosine similarity\n    cos_sim = F.cosine_similarity(embeddings_reshape.unsqueeze(1), centroids_reshape.unsqueeze(0), dim=2)\n    cos_sim = cos_sim.view(x.shape[0], x.shape[1], x.shape[1])  # Reshape to (num_speakers, num_utts_per_speaker, num_utts_per_speaker)\n\n    # Apply scaling and shifting\n    w = 10.0  # example scaling factor\n    b = -5.0  # example shifting factor\n    cos_sim = w * cos_sim + b\n\n    # Compute GE2E loss\n    identity_matrix = torch.eye(x.shape[1]).to(x.device).bool()\n    positive_terms = cos_sim.masked_select(identity_matrix.unsqueeze(0))\n    negative_terms = cos_sim.masked_select(~identity_matrix.unsqueeze(0))\n    positive_loss = -F.log_softmax(positive_terms, dim=0)\n    negative_loss = -F.log_softmax(negative_terms, dim=0)\n    loss = positive_loss.mean() + negative_loss.mean()\n\n    return loss", "CodeBleu_score": 27.697188086091153}, {"code": "import torch\n\ndef forward(self, x):\n    \"\"\"\n    The function calculates the GE2E loss for input data with dimensions \n    (num_speakers, num_utts_per_speaker, dvec_feats).\n    It ensures the input has at least two utterances per speaker, computes centroids by averaging the embeddings,\n    and calculates a cosine similarity matrix between the embeddings and centroids.\n    The similarity matrix is scaled and shifted, then the embedding loss is computed and averaged.\n    The function returns the mean loss.\n    \n    Args:\n        x (tensor): Tensor of shape (num_speakers, num_utts_per_speaker, dvec_feats) representing \n                    speaker embeddings.\n    \n    Returns:\n        torch.Tensor: The mean GE2E loss.\n    \"\"\"\n\n    # Ensure each speaker has at least two utterances\n    if x.size(1) < 2:\n        raise ValueError(\"Each speaker needs at least two utterances for GE2E loss calculation.\")\n        \n    # Calculate the centroid of embeddings by averaging along the utterance dimension\n    num_speakers, num_utts_per_speaker, dvec_feats = x.shape\n    centroids = x.mean(dim=1, keepdim=True)  # Shape: (num_speakers, 1, dvec_feats)\n    \n    # Compute cosine similarity between embeddings and centroids\n    centroids_expanded = centroids.expand(num_speakers, num_utts_per_speaker, dvec_feats)\n    embeddings_flattened = x.reshape(-1, dvec_feats)  # Shape: (num_speakers * num_utts_per_speaker, dvec_feats)\n    centroids_flattened = centroids_expanded.reshape(-1, dvec_feats)  # Same shape as embeddings_flattened\n    cos_sim = torch.nn.functional.cosine_similarity(embeddings_flattened, centroids_flattened, dim=-1)\n    cos_sim = cos_sim.view(num_speakers, num_utts_per_speaker)  # Restore to (num_speakers, num_utts_per_speaker)\n\n    # Scale and shift cosine matrix (similarity scores)\n    # Example scale and offset, in practice they may be learned parameters or constants\n    scale = 10.0\n    shift = -5.0\n    sim_matrix = scale * cos_sim + shift\n    \n    # Create mask to ignore self-to-self comparisons\n    mask = 1 - torch.eye(num_utts_per_speaker, device=x.device).unsqueeze(0).expand(num_speakers, -1, -1)\n    sim_matrix = sim_matrix * mask\n    \n    # Calculate the loss\n    # Assuming loss computation as softmax loss for simplicity with temperature-related scale\n    log_probs = torch.nn.functional.log_softmax(sim_matrix, dim=2)\n    loss = -log_probs[:, torch.arange(num_utts_per_speaker), torch.arange(num_utts_per_speaker)]\n    mean_loss = loss.mean()\n    \n    return mean_loss", "CodeBleu_score": 30.039803337207825}, {"code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GE2ELoss(nn.Module):\n    def __init__(self, w=10.0, b=-5.0):\n        super(GE2ELoss, self).__init__()\n        self.w = nn.Parameter(torch.tensor(w))\n        self.b = nn.Parameter(torch.tensor(b))\n    \n    def forward(self, x, _label=None):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Tensor of shape (num_speakers, num_utts_per_speaker, dvec_feats).\n        \n        Returns:\n            torch.Tensor: Mean GE2E loss.\n        \"\"\"\n        # Check dimensions to confirm at least two utterances per speaker\n        if x.size(1) < 2:\n            raise ValueError(\"Each speaker needs at least two utterances, found {}\".format(x.size(1)))\n\n        # Calculate centroids for each speaker\n        centroids = x.mean(dim=1)\n        # Calculate expanded centroids and embeddings\n        centroids_exp = centroids.unsqueeze(0).expand(x.size(0), -1, -1)\n        embeddings_exp = x.unsqueeze(1).expand(-1, x.size(0), -1, -1)\n\n        # Cosine similarity between embeddings and centroids\n        cos_sim = F.cosine_similarity(embeddings_exp, centroids_exp, dim=-1)\n\n        # Applying scale w and shift b to cosine similarities\n        cos_sim = self.w * cos_sim + self.b\n\n        # Mask to exclude current example for the centroid computation\n        mask = 1 - torch.eye(x.size(0)).unsqueeze(1).to(x.device)\n        cos_sim = cos_sim * mask\n\n        # Log softmax of similarities\n        log_probs = F.log_softmax(cos_sim, dim=2)\n\n        # Using diagonal elements, log probability where embedding belongs to its centroid\n        # Creating target labels\n        targets = torch.arange(x.size(0)).unsqueeze(1).expand(-1, x.size(1)).to(x.device)\n        loss = F.nll_loss(log_probs, targets, reduction='mean')\n\n        return loss", "CodeBleu_score": 30.868592697306607}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "import os\nfrom typing import List, Dict\n\ndef get_voices(extra_voice_dirs: List[str] = []) -> Dict[str, List[str]]:\n    \"\"\"\n    The function retrieves voice files from specified directories.\n    It iterates through each directory and subdirectory, collecting .npz, .wav, and .mp3 files for each subdirectory.\n    If no .npz files are found, it looks for .wav and .mp3 files.\n    The function returns a dictionary where keys are subdirectory names and values are lists of file paths.\n    \"\"\"\n    result = {}\n\n    # Helper function to collect files with given extensions\n    def collect_files(directory, extensions):\n        file_paths = []\n        for root, _, files in os.walk(directory):\n            for file in files:\n                if any(file.endswith(ext) for ext in extensions):\n                    file_paths.append(os.path.join(root, file))\n        return file_paths\n    \n    for dir_path in extra_voice_dirs:\n        if os.path.isdir(dir_path):\n            for subdir, _, _ in os.walk(dir_path):\n                subdirectory_name = os.path.basename(subdir)\n                npz_files = collect_files(subdir, ['.npz'])\n                if npz_files:\n                    result[subdirectory_name] = npz_files\n                else:\n                    wav_and_mp3_files = collect_files(subdir, ['.wav', '.mp3'])\n                    if wav_and_mp3_files:\n                        result[subdirectory_name] = wav_and_mp3_files\n\n    return result", "CodeBleu_score": 25.47207328908459}, {"code": "import os\nfrom typing import List, Dict\n\ndef get_voices(extra_voice_dirs: List[str] = []) -> Dict[str, List[str]]:\n    \"\"\"\n    The function retrieves voice files from specified directories.\n    It iterates through each directory and subdirectory, collecting .npz, .wav, and .mp3 files for each subdirectory.\n    If no .npz files are found, it looks for .wav and .mp3 files.\n    The function returns a dictionary where keys are subdirectory names and values are lists of file paths.\n    \n    :param extra_voice_dirs: List[str] - List of directories to search for voice files\n    :return: Dict[str, List[str]] - Dictionary mapping subdirectories to lists of relevant voice file paths\n    \"\"\"\n    voice_files = {}  # Initialize the dictionary to store results\n\n    # Iterate over each directory and its subdirectories\n    for directory in extra_voice_dirs:\n        for sub_dir, _, files in os.walk(directory):\n            npz_files = [os.path.join(sub_dir, file) for file in files if file.endswith('.npz')]\n            wav_files = [os.path.join(sub_dir, file) for file in files if file.endswith('.wav')]\n            mp3_files = [os.path.join(sub_dir, file) for file in files if file.endswith('.mp3')]\n\n            # Determine what to include, prioritizing .npz files\n            if npz_files:\n                voice_files[os.path.basename(sub_dir)] = npz_files\n            else:\n                voice_files[os.path.basename(sub_dir)] = wav_files + mp3_files\n\n    return voice_files", "CodeBleu_score": 29.472140816538246}, {"code": "import os\nfrom typing import List\n\ndef get_voices(extra_voice_dirs: List[str] = []):\n    \"\"\"The function retrieves voice files from specified directories.\n    It iterates through each directory and subdirectory, collecting .npz, .wav, and .mp3 files for each subdirectory.\n    If no .npz files are found, it looks for .wav and .mp3 files.\n    The function returns a dictionary where keys are subdirectory names and values are lists of file paths.\"\"\"\n    \n    file_extensions = ['.npz', '.wav', '.mp3']\n    voice_files = {}\n\n    for directory in extra_voice_dirs:\n        for root, _, files in os.walk(directory):\n            sub_dir = os.path.relpath(root, directory)\n            collected_files = []\n            found_npz = False\n\n            # Filter out files based on extension and collect relevant ones\n            for file in files:\n                if file.endswith('.npz'):\n                    found_npz = True\n                    collected_files.append(os.path.join(root, file))\n            if found_npz:\n                # If any .npz files are found, ignore .wav and .mp3 in this directory\n                voice_files[sub_dir] = collected_files\n            else:\n                # Collect .wav and .mp3 files if no .npz found\n                voice_files[sub_dir] = [os.path.join(root, file) for file in files if file.endswith('.wav') or file.endswith('.mp3')]\n    \n    return voice_files\n\n# Example usage:\nvoice_dirs = [\"/path/to/dir1\", \"/path/to/dir2\"]\nvoices = get_voices(voice_dirs)\nprint(voices)", "CodeBleu_score": 26.3979584940685}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "import os\nimport numpy as np\nfrom typing import List\nimport sys\nimport random\nimport pathlib\n\ndef get_voices(directory):\n    \"\"\" Simulates fetching npz or audio files that represent voices. \"\"\"\n    # List all files that are either npz or audio files\n    return [f for f in os.listdir(directory) if f.endswith('.npz') or f.endswith('.wav') or f.endswith('.mp3') or f.endswith('.flac')]\n\ndef load_npz(file_path):\n    \"\"\"Load npz file directly.\"\"\"\n    return np.load(file_path, allow_pickle=True)\n\ndef audio_to_npz(audio_path, save_path):\n    \"\"\"Mockup function to convert audio file to NPZ file, should be implemented.\"\"\"\n    # Assume conversion and save somehow\n    return save_path\n\ndef load_voice(model, voice: str, extra_voice_dirs: List[str] = []):\n    \"\"\"The function loads a voice for a given model from specified directories.\n    If the voice is \"random\", it returns None for all values.\n    It retrieves voice paths using get_voices and checks if there are multiple paths, raising an error if so.\n    It verifies the existence of the voice and handles .npz files directly by loading them.\n    For other audio files, it generates a .npz file and recursively loads the voice.\n    The function returns the loaded voice data. \"\"\"\n    \n    if voice == \"random\":\n        return None\n    \n    # Gather all possible directories\n    directories = list(extra_voice_dirs)\n \n    # Search in directories for voice files\n    found_files = []\n    for directory in directories:\n        if os.path.exists(directory):\n            found_files.extend([os.path.join(directory, fname) for fname in get_voices(directory)])\n\n    # Filter for requested voice\n    voice_files = [f for f in found_files if voice in f]\n    \n    if len(voice_files) == 0:\n        raise FileNotFoundError(f\"No files found for voice '{voice}' in provided directories.\")\n    elif len(voice_files) > 1:\n        raise ValueError(f\"Multiple files found for voice '{voice}': {voice_files}\")\n    \n    # Assuming always one file after above checks\n    voice_file = voice_files[0]\n    \n    if voice_file.endswith('.npz'):\n        return load_npz(voice_file)\n    else:\n        # Convert audio file to .npz format then load it\n        npz_file_path = os.path.splitext(voice_file)[0] + '.npz'\n        if not os.path.exists(npz_file_path):\n            audio_to_npz(voice_file, npz_file_path)\n        \n        return load_voice(model, os.path.basename(npz_file_path), extra_voice_dirs)", "CodeBleu_score": 41.7892815934867}, {"code": "import os\nimport numpy as np\n\ndef get_voices(directory, voice):\n    \"\"\"Simulated function to find voice paths in a directory.\"\"\"\n    # This should be replaced with actual logic to search for files.\n    paths = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if voice in file:\n                paths.append(os.path.join(root, file))\n    return paths\n\ndef load_voice(model, voice: str, extra_voice_dirs: List[str] = []):\n    \"\"\"The function loads a voice for a given model from specified directories.\n    If the voice is \"random\", it returns None for all values.\n    It retrieves voice paths using get_voices and checks if there are multiple paths, raising an error if so.\n    It verifies the existence of the voice and handles .npz files directly by loading them.\n    For other audio files, it generates a .npz file and recursively loads the voice.\n    The function returns the loaded voice data.\"\"\"\n    if voice == \"random\":\n        return None\n\n    # Collecting all possible voice file paths\n    voice_paths = []\n    for directory in [model] + extra_voice_dirs:\n        voice_paths.extend(get_voices(directory, voice))\n\n    # Check for multiple ambiguous paths\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Ambiguous paths found for the voice: {voice_paths}\")\n    elif not voice_paths:\n        raise ValueError(\"No valid voice file found.\")\n\n    voice_path = voice_paths[0]  # There should be exactly one valid path\n\n    # Load the voice file based on its extension\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)[\"data\"]\n    else:\n        # Assume it's an audio file that needs to be converted to .npz\n        new_voice_path = convert_to_npz(voice_path)  # `convert_to_npz` needs to be implemented\n        voice_data = load_voice(model, new_voice_path)[0]  # Recursive call with the new .npz file\n\n    return voice_data\n\ndef convert_to_npz(audio_path):\n    \"\"\"Convert an audio file to a .npz file. This is a placeholder and should be implemented.\"\"\"\n    new_path = audio_path.replace(os.path.splitext(audio_path)[1], \".npz\")\n    # Dummy conversion: This should actually involve processing the audio file.\n    np.savez(new_path, data=np.random.rand(10, 10))  # Dummy data\n    return new_path", "CodeBleu_score": 35.23205170857068}, {"code": "import os\nfrom typing import List, Optional\n\ndef get_voices(directory: str):\n    \"\"\" Placeholder helper function to scan available voice files in a directory. \"\"\"\n    return [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n\ndef load_npy_file(file_path):\n    \"\"\" Placeholder helper function to load a .npz file. \"\"\"\n    import numpy as np\n    return np.load(file_path)\n\ndef convert_audio_to_npz(file_path):\n    \"\"\"Convert other audio file formats to .npz and return the path to .npz file.\"\"\"\n    import numpy as np\n    import soundfile as sf\n    audio_data, samplerate = sf.read(file_path)\n    new_file_path = file_path.rsplit('.', 1)[0] + '.npz'\n    np.savez(new_file_path, data=audio_data, rate=samplerate)\n    return new_file_path\n\ndef load_voice(model, voice: str, extra_voice_dirs: List[str] = []):\n    if voice.lower() == \"random\":\n        return None\n    \n    voice_paths = []\n    \n    # Gather voice files from additional directories\n    for directory in extra_voice_dirs:\n        voice_files = get_voices(directory)\n        for vfile in voice_files:\n            if voice in vfile:\n                voice_paths.append(os.path.join(directory, vfile))\n                \n    # Check for multiple voice definitions conflict\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple paths found for voice '{voice}': {voice_paths}\")\n    elif len(voice_paths) == 0:\n        raise FileNotFoundError(f\"No voice file found for '{voice}' in the given directories.\")\n    \n    voice_path = voice_paths[0]\n    \n    # Check if file is a .npz\n    if voice_path.endswith('.npz'):\n        voice_data = load_npy_file(voice_path)\n    else:\n        # Assume it needs conversion to .npz\n        npz_path = convert_audio_to_npz(voice_path)\n        voice_data = load_npy_file(npz_path)\n    \n    return voice_data\n\n# This function handling and specific File IO, audio conversion, and error handling needs further refinement with real implementations of get_voices, load_npy_file, and convert_audio_to_npz among others.", "CodeBleu_score": 41.280664983527174}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "import torch\n\ndef _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    The function performs a relative shift on a position score tensor.\n    It first extracts the tensor's dimensions and creates a zero tensor with an additional dimension.\n    This zero tensor is concatenated with the original position score tensor along the last dimension, then reshaped and sliced to achieve the relative shift.\n    The function returns the shifted position score tensor.\n    \"\"\"\n    zero_pad = torch.zeros((pos_score.size(0), 1, *pos_score.size()[2:]), device=pos_score.device, dtype=pos_score.dtype)  # Adding a column of zeros\n    padded = torch.cat([zero_pad, pos_score], dim=1)  # Concatenate along the first dimension (considering batches)\n\n    # Size adjustment for the shift\n    shape = pos_score.shape\n    new_shape = (shape[0], shape[1] + 1, *shape[2:])\n    reshaped = padded.view(new_shape)\n\n    # Now remove the first time-step for each batch\n    shifted_pos_score = reshaped[:, 1:]\n    \n    return shifted_pos_score", "CodeBleu_score": 41.4929187853683}, {"code": "import torch\n\ndef _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor:\n    \"\"\"The function performs a relative shift on a position score tensor.\n    It first extracts the tensor's dimensions and creates a zero tensor with an additional dimension.\n    This zero tensor is concatenated with the original position score tensor along the last dimension,\n    then reshaped and sliced to achieve the relative shift.\n    The function returns the shifted position score tensor.\"\"\"\n    \n    # Step 2: Extract dimensions\n    batch_size, heads, seq_length, _ = pos_score.size()\n    \n    # Step 3: Create a zero tensor\n    # We often need to add the zeroes on the second to last dim 'seq_length', hence we expand that dimension by 1\n    zero_pad = torch.zeros((batch_size, heads, 1, seq_length), device=pos_score.device, dtype=pos_score.dtype)\n    \n    # Step 4: Concatenate zero tensor with the original tensor along the last dimension\n    pos_score_padded = torch.cat([zero_pad, pos_score], dim=2)\n    \n    # Step 5: Reshape\n    # We reshape the tensor to change its dimensions suitably for slicing\n    pos_score_padded = pos_score_padded.view(batch_size, heads, seq_length + 1, seq_length)\n    \n    # Step 6: Slice\n    # We take all except the last element along the dimension 2 which corresponds to `seq_length + 1`\n    pos_score_shifted = pos_score_padded[:, :, :seq_length]\n    \n    return pos_score_shifted", "CodeBleu_score": 54.24173757967156}, {"code": "import torch\n\ndef _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor:\n    \"\"\"The function performs a relative shift on a position score tensor.\n    It first extracts the tensor's dimensions and creates a zero tensor with an additional dimension.\n    This zero tensor is concatenated with the original position score tensor along the last dimension, then reshaped and sliced to achieve the relative shift.\n    The function returns the shifted position score tensor.\"\"\"\n\n    # Extract the dimensions of the incoming tensor\n    batch_size, seq_len = pos_score.size()\n\n    # Create a zero tensor with an additional row\n    zero_pad = torch.zeros((batch_size, 1), device=pos_score.device, dtype=pos_score.dtype)\n\n    # Concatenate the zero tensor with the original tensor along the last dimension (column-wise)\n    pos_score_padded = torch.cat([zero_pad, pos_score], dim=1)\n\n    # Reshape the padded tensor to \"shift\" the values\n    pos_score_padded = pos_score_padded.view(batch_size, seq_len + 1)\n\n    # Slicing to get rid of the first column (effectively shifting all other values left)\n    pos_score_shifted = pos_score_padded[:, 1:]\n\n    return pos_score_shifted", "CodeBleu_score": 41.59139527944312}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SequenceProcessor(nn.Module):\n    def __init__(self):\n        super(SequenceProcessor, self).__init__()\n        # Adding exemplary conv layers, adjust according to specific needs.\n        self.conv_layers = nn.Sequential(\n            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, padding=2), \n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n\n    def run_padded_sequence(self, context, lens):\n        \"\"\"\n        The function processes a padded sequence through a series of convolutional layers with ReLU activation and dropout.\n        For each batch element, it extracts the context up to the specified length, applies the convolutional layers, and stores the results.\n        The processed contexts are then padded back to create a consistent batch size and the function returns the padded sequence.\n        \n        Parameters:\n        - context (torch.Tensor): Tensor representing batched padded sequences. Shape: [batch_size, channels, max_seq_length]\n        - lens (list or torch.Tensor): Actual lengths of each sequence in the batch before padding.\n        \n        Returns:\n        torch.Tensor: Processed and re-padded sequence tensor. Shape: [batch_size, channels, max_seq_length]\n        \"\"\"\n        batch_size, channels, max_seq_length = context.size()\n        processed_batch = []\n\n        # Process each sequence individually, respecting their original length\n        for i in range(batch_size):\n            current_sequence = context[i, :, :lens[i]]  # Extract up to the valid length\n            current_sequence = current_sequence.unsqueeze(0)  # Add batch_dim to comply with Conv1d expected input\n            processed_sequence = self.conv_layers(current_sequence)  # Apply convolutional layers\n            # Padding back to max_seq_length\n            pad_size = max_seq_length - processed_sequence.size(2)\n            processed_sequence = F.pad(processed_sequence, (0, pad_size), \"constant\", 0)\n            processed_batch.append(processed_sequence)\n        \n        # Stack all processed sequences into a single tensor\n        processed_batch = torch.cat(processed_batch, dim=0)\n\n        return processed_batch\n\n# Example usage:\n# Assuming a context tensor and lens tensor/vector to hold the actual lengths of each sequence\n# context = torch.randn(batch_size, channels, max_seq_length)\n# lens = torch.tensor(some_list_of_lengths)\n# model = SequenceProcessor()\n# result = model.run_padded_sequence(context, lens)", "CodeBleu_score": 27.61928272077091}, {"code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass YourModel(nn.Module):\n    def __init__(self):\n        super(YourModel, self).__init__()\n        # Assuming the input features dimension is known, configure accordingly:\n        self.conv1 = nn.Conv1d(in_channels=feature_dim, out_channels=out_channels, kernel_size=kernel_size)\n        self.dropout = nn.Dropout(p=0.5)\n        self.relu = nn.ReLU()\n        \n    def run_padded_sequence(self, context, lens):\n        \"\"\"\n        Processes a padded sequence through series of convolutional layers with ReLU activation and dropout.\n        \n        Parameters:\n            context (Tensor): The tensor of shape (batch_size, sequence_length, feature_dim)\n            lens (Tensor): The tensor/list of valid lengths for each sequence in the batch\n            \n        Returns:\n            Tensor: The padded output tensor from the convolutional processing\n        \"\"\"\n        batch_size, seq_length, feature_dim = context.shape\n        \n        # Apply convolutions and activations element-wise\n        outputs = torch.zeros_like(context)\n        for i in range(batch_size):\n            valid_seq = context[i, :lens[i]]  # Extract the valid part of the sequence\n            valid_seq = valid_seq.transpose(0, 1).unsqueeze(0)  # Conv1d expects (batch_size, channels, length)\n            \n            # Apply Convolution + ReLU + Dropout\n            conv_out = self.conv1(valid_seq)\n            conv_out = self.relu(conv_out)\n            conv_out = self.dropout(conv_out)\n            \n            # Collapsing the output back to the original dimension (removing channel)\n            # Adjust as needed based on output size of conv1\n            conv_out = conv_out.squeeze().transpose(0, 1)\n            \n            # Assign the processed sequence back to outputs\n            outputs[i, :lens[i]] = conv_out\n        \n        return outputs  # Return the modified context with processed and padded outputs\n\n# Configuration for creating an instance of YourModel, you need to substitute actual values\nfeature_dim = 128\nout_channels = 64\nkernel_size = 3\n\nmodel = YourModel()\n# Use model.run_padded_sequence(context, lens) to process your data where context and lens are provided", "CodeBleu_score": 24.175284960748815}, {"code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SequenceProcessor(nn.Module):\n    def __init__(self, input_dim, num_filters, kernel_size, dropout_rate=0.5):\n        super(SequenceProcessor, self).__init__()\n        self.conv = nn.Conv1d(input_dim, num_filters, kernel_size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.relu = nn.ReLU()\n\n    def run_padded_sequence(self, context, lens):\n        \"\"\"\n        The function processes a padded sequence through a series of convolutional layers with ReLU activation and dropout.\n        For each batch element, it extracts the context up to the specified length, applies the convolutional layers, and stores the results.\n        The processed contexts are then padded to create a consistent batch size, and the function returns the padded sequence.\n\n        Arguments:\n        context -- (torch.Tensor) The input tensor with shape (batch_size, channels, max_length).\n        lens -- (List[int]) The actual lengths of each sequence in the batch.\n\n        Returns:\n        torch.Tensor -- Processed and re-padded sequence tensor.\n        \"\"\"\n        batch_size, channels, max_length = context.size()\n\n        # Initialize a tensor to store the outputs of the convolution operation for each sequence.\n        output_padded = torch.zeros_like(context)\n\n        # Process each sequence according to its length\n        for i in range(batch_size):\n            sequence = context[i, :, :lens[i]]\n            sequence = sequence.unsqueeze(0)  # Add batch dimension\n            # Apply convolution, ReLU activation, and dropout\n            sequence = self.conv(sequence)\n            sequence = self.relu(sequence)\n            sequence = self.dropout(sequence)\n\n            # Add the processed sequence back to the output tensor\n            sequence_length = sequence.size(2)\n            output_padded[i, :, :sequence_length] = sequence.squeeze(0)\n\n        return output_padded\n\n# Example usage (Assuming 10 features per timestep, 5 filters in convolution, kernel size of 3, and dropout of 0.3)\nprocessor = SequenceProcessor(input_dim=10, num_filters=5, kernel_size=3, dropout_rate=0.3)\ncontext_tensor = torch.randn(4, 10, 50)  # batch_size=4, channels=10, max_length=50\nlengths = [45, 50, 30, 40]  # Actual lengths of each sequence\nresult = processor.run_padded_sequence(context_tensor, lengths)\nprint(result.shape)  # To check the resultant tensor shape", "CodeBleu_score": 25.80813233997824}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens):\n    \"\"\"\n    The function processes unsorted inputs by sorting them based on their lengths, running a given \n    function on the sorted inputs, and then unsorting the results to match the original order.\n    It sorts the lengths and indices, packs the context sequences, applies the function, \n    unpacks the sequences, and finally maps the results back to the original indices.\n    The function returns the context in its original order.\n    \n    Args:\n    - fn (callable): A function to be applied to the sorted contexts.\n    - context (list): A list of context items (could be sequences, strings, etc.).\n    - lens (list of int): A list of the lengths of each item in the context.\n\n    Returns:\n    - list: Processed items in their original order after applying the function `fn`.\n    \"\"\"\n    \n    # Pair each context with its index and length\n    indexed_context = list(enumerate(zip(context, lens)))\n    \n    # Sort based on lengths\n    sorted_indexed_context = sorted(indexed_context, key=lambda ic: ic[1][1])\n    \n    # Extract the sorted contexts\n    sorted_contexts = [x[1][0] for x in sorted_indexed_context]\n    \n    # Apply the function to the sorted contexts\n    processed_sorted_contexts = fn(sorted_contexts)\n    \n    # Create a placeholder list for the results in their original order\n    results = [None] * len(context)\n    \n    # Place processed context back in original order\n    for ((original_index, _), processed_context) in zip(sorted_indexed_context, processed_sorted_contexts):\n        results[original_index] = processed_context\n    \n    return results", "CodeBleu_score": 23.691130238830972}, {"code": "def run_unsorted_inputs(self, fn, context, lens):\n    \"\"\"\n    The function processes unsorted inputs by sorting them based on their lengths, \n    running a given function on the sorted inputs, and then unsorting the results \n    to match the original order.\n    It sorts the lengths and indices, packs the context sequences, applies the \n    function, unpacks the sequences, and finally maps the results back to the \n    original indices.\n    The function returns the context in its original order.\n\n    Parameters:\n        fn: A function that processes each context element.\n        context: A list of arbitrary elements to be processed.\n        lens: A list representing the lengths of each element in context.\n\n    Returns:\n        A list containing the outputs of `fn` in the original order of `context`.\n    \"\"\"\n    # Sort contexts by length:\n    # Combine contexts and lengths into a list of tuples, sort by length\n    indexed_lens = list(enumerate(lens))\n    indexed_lens.sort(key=lambda x: x[1])\n\n    # Sort contexts based on the ordered lengths\n    sorted_contexts = [context[idx] for idx, _ in indexed_lens]\n\n    # Apply the function to the sorted contexts\n    processed_contexts = [fn(ctx) for ctx in sorted_contexts]\n\n    # Unsort process results back into original order\n    # Prepare result list initialized with None or placeholders\n    result = [None] * len(context)\n    for i, (orig_idx, _) in enumerate(indexed_lens):\n        result[orig_idx] = processed_contexts[i]\n\n    return result", "CodeBleu_score": 19.329765205206684}, {"code": "def run_unsorted_inputs(self, fn, context, lens):\n    \"\"\"\n    The function processes unsorted inputs by sorting them based on their lengths,\n    running a given function on the sorted inputs, and then unsorting the results to match\n    the original order.\n    It sorts the lengths and indices, packs the context sequences, applies the function,\n    unpacks the sequences, and finally maps the results back to the original indices.\n    The function returns the context in its original order.\n    \"\"\"\n    # Step 1: Sort lengths and context based on lengths\n    sorted_indices_lens = sorted(enumerate(lens), key=lambda x: x[1])\n    sorted_indices = [idx for idx, length in sorted_indices_lens]\n    sorted_context = [context[idx] for idx in sorted_indices]\n    \n    # Step 2: Apply the function to the sorted context\n    processed_context = fn(sorted_context)\n    \n    # Step 3: Unsort the processed result based on the original indices\n    # Initialize an empty list of the same size as the context to store results\n    unsorted_results = [None] * len(context)\n    \n    # Map the results back using the original indices\n    for original_idx, processed_item in zip(sorted_indices, processed_context):\n        unsorted_results[original_idx] = processed_item\n\n    # Step 4: Return the original ordered context with processed items\n    return unsorted_results", "CodeBleu_score": 20.900153797321362}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "import torch.nn.utils as nn_utils\n\nclass YourModel(torch.nn.Module):\n    def __init__(self):\n        super(YourModel, self).__init__()\n        # Initialization of layers with weight normalization\n        self.kernel_predictor = nn_utils.weight_norm(torch.nn.Linear(10, 10))\n        self.convt_pre = nn_utils.weight_norm(torch.nn.ConvTranspose2d(16, 33, 3, stride=2))\n        self.conv_blocks = [nn_utils.weight_norm(torch.nn.Conv2d(16, 16, 3, padding=1)) for _ in range(5)]\n    \n    def remove_weight_norm(self):\n        \"\"\"The function removes weight normalization from specific layers in a neural network.\n        It calls the remove_weight_norm method on the kernel_predictor, removes parameterizations from the convt_pre layer,\n        and iterates through the conv_blocks to remove parameterizations from each block's weight.\"\"\"\n        # Removing weight norm from kernel_predictor\n        nn_utils.remove_weight_norm(self.kernel_predictor)\n        \n        # Removing weight norm from convt_pre\n        nn_utils.remove_weight_norm(self.convt_pre)\n        \n        # Removing weight norm from all layers in conv_blocks\n        for block in self.conv_blocks:\n            nn_utils.remove_weight_norm(block)", "CodeBleu_score": 38.98704679054877}, {"code": "import torch.nn.utils\n\ndef remove_weight_norm(self):\n    \"\"\"\n    The function removes weight normalization from specific layers in a neural network.\n    It calls the remove_weight_norm method on the kernel_predictor, removes parameterizations\n    from the convt_pre layer, and iterates through the conv_blocks to remove parameterizations\n    from each block's weight.\n    \"\"\"\n\n    # Check if kernel_predictor has the weight normalization and remove it\n    if hasattr(self.kernel_predictor, 'weight_g'):\n        torch.nn.utils.remove_weight_norm(self.kernel_predictor)\n\n    # Check if convt_pre has the weight normalization and remove it\n    if hasattr(self.convt_pre, 'weight_g'):\n        torch.nn.utils.remove_weight_norm(self.convt_pre)\n\n    # Iterate through each block in conv_blocks and remove weight normalization if applicable\n    for block in self.conv_blocks:\n        if hasattr(block, 'weight_g'):\n            torch.nn.utils.remove_weight_norm(block)", "CodeBleu_score": 48.68824429736013}, {"code": "import torch.nn.utils as nn_utils\n\nclass YourNetworkClass:\n\n    def remove_weight_norm(self):\n        \"\"\"The function removes weight normalization from specific layers in a neural network.\n        It calls the remove_weight_norm method on the kernel_predictor, removes parameterizations from \n        the convt_pre layer, and iterates through the conv_blocks to remove parameterizations from each block's weight.\"\"\"\n\n        # Remove weight normalization from kernel_predictor\n        if hasattr(self.kernel_predictor, 'weight'):\n            nn_utils.remove_weight_norm(self.kernel_predictor)\n\n        # Remove weight normalization from convt_pre\n        if hasattr(self.convt_pre, 'weight'):\n            nn_utils.remove_weight_norm(self.convt_pre)\n\n        # Remove weight normalization from each block in conv_blocks\n        for block in self.conv_blocks:\n            if hasattr(block, 'weight'):\n                nn_utils.remove_weight_norm(block)\n\n        print(\"Weight normalization removed from specified layers.\")\n\n# Example usage\n# Make sure to replace 'YourNetworkClass' by the actual class name used in your code\n# net = YourNetworkClass()\n# net.remove_weight_norm()", "CodeBleu_score": 43.043268530858576}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name):\n    \"\"\"\n    The function maps a model name from MXNet to its corresponding PyTorch model name.\n    It converts the input name to lowercase and applies specific replacements for 'se_', 'senet_', and 'inceptionv3' prefixes.\n    Finally, it prefixes the name with 'gluon_' and returns the modified name.\n\n    Args:\n    mx_name (str): MXNet model name.\n\n    Returns:\n    str: Corresponding PyTorch model name with necessary transformations.\n    \"\"\"\n    # Convert the MXNet model name to lowercase\n    mx_name = mx_name.lower()\n    \n    # Replace certain substrings as specified\n    mx_name = mx_name.replace('se_', 'seres_')\n    mx_name = mx_name.replace('senet_', 'seresnet_')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    \n    # Prefix the modified model name with 'gluon_'\n    torch_name = 'gluon_' + mx_name\n    \n    return torch_name", "CodeBleu_score": 47.90864520348273}, {"code": "def map_mx_to_torch_model(mx_name):\n    \"\"\"\n    The function maps a model name from MXNet to its corresponding PyTorch model name.\n    It converts the input name to lowercase and applies specific replacements for 'se_', 'senet_', and 'inceptionv3' prefixes.\n    Finally, it prefixes the name with 'gluon_' and returns the modified name.\n    \n    Args:\n    mx_name (str): The MXNet model name to be converted.\n    \n    Returns:\n    str: The corresponding PyTorch model name prefixed by 'gluon_'.\n    \"\"\"\n    # Convert to lowercase\n    mx_name = mx_name.lower()\n    \n    # Replace specific substrings as per requirements\n    mx_name = mx_name.replace('se_', 'res_')\n    mx_name = mx_name.replace('senet_', 'resnet_').replace('inceptionv3', 'inception_v3')\n    \n    # Prefix the model name\n    torch_model_name = 'gluon_' + mx_name\n    \n    return torch_model_name", "CodeBleu_score": 43.589528614565225}, {"code": "def map_mx_to_torch_model(mx_name):\n    \"\"\"\n    The function maps a model name from MXNet to its corresponding PyTorch model name.\n    It converts the input name to lowercase and applies specific replacements for 'se_', 'senet_', and 'inceptionv3' prefixes.\n    Finally, it prefixes the name with 'gluon_' and returns the modified name.\n    \n    Parameters:\n        mx_name (str): The model name in MXNet framework.\n    \n    Returns:\n        str: The corresponding model name in PyTorch framework, preceded by 'gluon_'.\n    \"\"\"\n    # Convert input model name to lowercase\n    torch_model_name = mx_name.lower()\n    \n    # Replace specific substrings\n    if 'se_' in torch_model_name:\n        torch_model_name = torch_model_name.replace('se_', 'squeeze_')\n    elif 'senet_' in torch_model_name:\n        torch_model_name = torch_model_name.replace('senet_', 'squeeze_excitation_')\n    elif 'inceptionv3' in torch_model_name:\n        torch_model_name = torch_model_name.replace('inceptionv3', 'inception_v3')\n    \n    # Prefix the name with 'gluon_'\n    torch_model_name = 'gluon_' + torch_model_name\n    \n    return torch_model_name", "CodeBleu_score": 58.98014689514473}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "import math\nimport numpy as np\n\nclass Simulation:\n    def __init__(self, total_samples, num_workers, repeats=1, has_training=True, batch_size=None):\n        self.total_samples = total_samples\n        self.num_workers = num_workers\n        self.repeats = repeats\n        self.has_training = has_training\n        self.batch_size = batch_size\n\n    def _num_samples_per_worker(self):\n        \"\"\"\n        Calculates the number of samples to be processed by each worker.\n        It computes the initial number of samples per worker based on the repeats, total samples, and the number of workers.\n        If training or multiple replicas are used, it rounds up the number of samples.\n        If a batch size is specified during training, it adjusts the number of samples to be a multiple of the batch size.\n        Finally, it returns the calculated number of samples as an integer.\n        \"\"\"\n        \n        # Calculate initial number of samples per worker, with consideration for repeats\n        initial_samples_per_worker = self.total_samples * self.repeats / self.num_workers\n\n        # If training or multiple replicas are involved, round up the number of samples\n        if self.has_training or self.num_workers > 1:\n            samples_per_worker = math.ceil(initial_samples_per_worker)\n        else:\n            samples_per_worker = int(initial_samples_per_worker)\n        \n        # Adjust the number of samples to be a multiple of the batch size if specified\n        if self.batch_size is not None:\n            samples_per_worker = int(np.ceil(samples_per_worker / self.batch_size) * self.batch_size)\n        \n        return samples_per_worker", "CodeBleu_score": 38.426454515028915}, {"code": "import math\n\nclass SampleDistributor:\n    def __init__(self, total_samples, num_workers, batch_size=None):\n        \"\"\"\n        Initializes the distributor with the sample and parallelism parameters.\n\n        Args:\n        total_samples (int): Total number of samples to be processed.\n        num_workers (int): Number of workers or replicas to distribute the work.\n        batch_size (int, optional): The size of the batch used in training (defaults to None).\n        \"\"\"\n        self.total_samples = total_samples\n        self.num_workers = num_workers\n        self.batch_size = batch_size\n\n    def _num_samples_per_worker(self):\n        \"\"\"\n        The function calculates the number of samples to be processed by each worker.\n        It computes the initial number of samples per worker based on the repeats, total samples,\n        and the number of workers or replicas.\n        If training or multiple replicas are used, it rounds up the number of samples.\n        If a batch size is specified during training, it adjusts the number of samples to be a multiple of the batch size.\n        Finally, it returns the calculated number of samples as an integer.\n        \"\"\"\n        # Compute the initial number of samples per worker\n        initial_samples_per_worker = self.total_samples / self.num_workers\n\n        # Round up if using multiple workers\n        if self.num_workers > 1:\n            initial_samples_per_worker = math.ceil(initial_samples_per_worker)\n\n        # Adjusting the number of samples to be a multiple of the batch size if specified\n        if self.batch_size is not None:\n            samples_per_worker = (math.ceil(initial_samples_per_worker / self.batch_size)) * self.batch_size\n        else:\n            samples_per_worker = initial_samples_per_worker\n\n        # Return as an integer\n        return int(samples_per_worker)\n\n# Example usage\ndistributor = SampleDistributor(total_samples=1000, num_workers=4, batch_size=10)\nprint(distributor._num_samples_per_worker())  # Output should account for the batch size and number of workers", "CodeBleu_score": 37.671191236526454}, {"code": "import math\n\nclass MyClass:\n    def __init__(self, repeats, total_samples, num_workers, is_training, batch_size=None):\n        self.repeats = repeats\n        self.total_samples = total_samples\n        self.num_workers = num_workers\n        self.is_training = is_training\n        self.batch_size = batch_size\n\n    def _num_samples_per_worker(self):\n        \"\"\"The function calculates the number of samples to be processed by each worker.\n        It computes the initial number of samples per worker based on the repeats, total samples, and the number of workers or replicas.\n        If training or multiple replicas are used, it rounds up the number of samples.\n        If a batch size is specified during training, it adjusts the number of samples to be a multiple of the batch size.\n        Finally, it returns the calculated number of samples as an integer.\"\"\"\n        \n        # Calculate initial samples per worker\n        total_effective_samples = self.repeats * self.total_samples\n        initial_samples_per_worker = total_effective_samples / float(self.num_workers)\n        \n        # If training or using multiple replicas, round up to ensure all samples are processed\n        if self.is_training or self.num_workers > 1:\n            initial_samples_per_worker = math.ceil(initial_samples_per_worker)\n        \n        # Adjust for batch size if specified\n        if self.batch_size and self.is_training:\n            num_batches = math.ceil(initial_samples_per_worker / self.batch_size)\n            adjusted_samples_per_worker = num_batches * self.batch_size\n        else:\n            adjusted_samples_per_worker = initial_samples_per_worker\n\n        return int(adjusted_samples_per_worker)\n\n# Example usage:\nexample = MyClass(repeats=3, total_samples=100, num_workers=4, is_training=True, batch_size=5)\nprint(example._num_samples_per_worker())  # Output will depend on the parameters", "CodeBleu_score": 34.5245625925256}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "import os\nfrom typing import List, Tuple, Set, Dict, Optional, Union\n\ndef find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n):\n    if types is None:\n        types = {'jpg', 'jpeg', 'png', 'bmp', 'tiff', 'gif'}  # Default image types\n    else:\n        types = set(types)  # To ensure it's a set for efficient lookups\n\n    if not os.path.isdir(folder):\n        raise FileNotFoundError(f\"The provided folder path {folder} does not exist.\")\n    \n    # Used to store the path and its corresponding label index\n    images_and_labels = []\n\n    # Temporary dictionary to build class_to_idx if not provided\n    if class_to_idx is None:\n        class_to_idx = {}\n        index = 0\n\n    # Traverse all directories and files in the given folder\n    for root, dirs, files in os.walk(folder):\n        # Get class/label name from folder structure\n        if leaf_name_only:\n            class_name = os.path.basename(root)\n        else:\n            class_name = root.replace(folder, '').strip(os.sep())\n        \n        if class_name and files:\n            # Ensure class is recorded in class_to_idx, assign new index if needed\n            if class_name not in class_to_idx:\n                class_to_idx[class_name] = index\n                index += 1\n            \n            # Corresponding index for the current class\n            class_index = class_to_idx[class_name]\n        \n            # Add files to the image list if they match the required types\n            for file in files:\n                ext = file.split('.')[-1].lower()\n                if ext in types:\n                    images_and_labels.append((os.path.join(root, file), class_index))\n    \n    # Sort the list of images and labels if required\n    if sort:\n        images_and_labels.sort(key=lambda x: x[0])\n\n    return images_and_labels, class_to_idx\n\n# Example usage\nfolder_path = \"/path/to/your/dataset/folder\"\nimages_and_labels, class_indices = find_images_and_targets(folder_path, types={'png', 'jpg'})\nprint(images_and_labels)\nprint(class_indices)", "CodeBleu_score": 36.43126522322873}, {"code": "import os\nfrom typing import Dict, List, Tuple, Set, Optional, Union\n\ndef find_images_and_targets(\n    folder: str,\n    types: Optional[Union[List, Tuple, Set]] = None,\n    class_to_idx: Optional[Dict] = None,\n    leaf_name_only: bool = True,\n    sort: bool = True\n) -> Tuple[List[Tuple[str, int]], Dict[str, int]]:\n    \"\"\"\n    The function searches for images and their corresponding class labels in a given folder.\n    It optionally filters by file types and builds a list of filenames and labels based on the folder structure.\n    If no class-to-index mapping is provided, it creates one from the found labels.\n    It then pairs each image file with its class index, optionally sorting the results, and returns the list of\n    image-path and class-index pairs along with the class-to-index mapping.\n    \"\"\"\n    if types is None:\n        types = {'png', 'jpg', 'jpeg', 'bmp', 'gif'}\n\n    # Initialize an empty list for the image paths and class label pairs\n    image_label_pairs = []\n    \n    # Initialize a dictionary for class to index mapping if none is provided\n    if class_to_idx is None:\n        class_to_idx = {}\n        dynamic_class_assignment = True\n    else:\n        dynamic_class_assignment = False\n\n    # Walk through the folder to locate image files and their classes\n    for root, _, filenames in os.walk(folder):\n        class_name = os.path.basename(root) if leaf_name_only else root.replace(folder, '').lstrip(os.sep)\n        \n        # Process each file in the directory\n        for filename in filenames:\n            if any(filename.lower().endswith(ext.lower()) for ext in types):\n                # Create full image path\n                image_path = os.path.join(root, filename)\n                \n                # Assign or retrieve class_index\n                if dynamic_class_assignment and class_name not in class_to_idx:\n                    # Assign a new class index if the class name is not already in the dictionary\n                    class_to_idx[class_name] = len(class_to_idx)\n                \n                class_index = class_to_idx.get(class_name)\n                \n                # Add the image path and class index to the list\n                image_label_pairs.append((image_path, class_index))\n    \n    # Optionally sort the image_path and class_index pairs by the path\n    if sort:\n        image_label_pairs.sort()\n\n    return image_label_pairs, class_to_idx", "CodeBleu_score": 36.51616174512018}, {"code": "import os\nfrom typing import List, Tuple, Set, Dict, Optional, Union\n\ndef find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n):\n    \"\"\"\n    The function searches for images and their corresponding class labels in a given folder.\n    It optionally filters by file types and builds a list of filenames and labels based on the folder structure.\n    If no class-to-index mapping is provided, it creates one from the found labels.\n    It then pairs each image file with its class index, optionally sorting the results, \n    and returns the list of image-path and class-index pairs along with the class-to-index mapping.\n    \n    Parameters:\n        folder (str): The directory to search for image files.\n        types (Optional[Union[List, Tuple, Set]]): Allowed file extensions e.g. {'.jpg', '.png'}\n        class_to_idx (Optional[Dict]): A dictionary mapping class names to numeric labels.\n        leaf_name_only (bool): If True, returns only the leaf filename, else it returns their entire paths.\n        sort (bool): If True, sorts the resulting list by filenames.\n    \n    Returns:\n        Tuple[List[Tuple[str, int]], Dict]: A tuple containing:\n                                            1. A list of tuples each consisting of (image path, class index)\n                                            2. A dictionary mapping class names to indices\n    \"\"\"\n    \n    if types is None:\n        types = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff'}\n    \n    # Walking through the folder to find classes and their associated images\n    dataset = []\n    discovered_classes = set()\n    \n    for root, dirs, files in os.walk(folder):\n        class_name = os.path.basename(root) if not leaf_name_only else \"\"\n        for file in files:\n            if any(file.lower().endswith(t.lower()) for t in types):\n                absolute_path = os.path.join(root, file)\n                relative_path = os.path.join(class_name, file) if not leaf_name_only else file\n                discovered_classes.add(class_name)\n                dataset.append((absolute_path, class_name))\n    \n    # Create class_to_idx mapping if not already provided\n    if class_to_idx is None:\n        class_to_idx = {class_name: idx for idx, class_name in enumerate(sorted(discovered_classes))}\n    \n    # Convert class names in dataset to indices\n    labeled_data = [(path if not leaf_name_only else os.path.basename(path), class_to_idx[class_name]) \n                    for (path, class_name) in dataset]\n    \n    # Optionally sort the dataset by file path\n    if sort:\n        labeled_data.sort(key=lambda x: x[0])\n    \n    return labeled_data, class_to_idx", "CodeBleu_score": 39.441406949793475}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "import torch\nimport os\n\ndef pytorch_worker_seed():\n    \"\"\"The function retrieves the seed for a PyTorch dataloader worker.\n    It first attempts to get the seed from the PyTorch worker info.\n    If the worker info is not available, it falls back to a seed based on the worker_id or a predefined environmental variable.\"\"\"\n    \n    # Get worker info from PyTorch DataLoader, assuming it's being called from a DataLoader\n    worker_info = torch.utils.data.get_worker_info()\n    \n    if worker_info is not None:\n        # Use the seed specified for the worker or base it off the worker_id\n        seed = worker_info.seed  # this is typically a seed generated by the DataLoader - computed as base_seed + worker_id\n    else:\n        # Fallback seed (perhaps your application has a specific environment variable or just use os.getpid as somewhat unique)\n        seed = int(os.environ.get('WORKER_SEED', os.getpid()))\n    \n    return seed\n\n# Example use case within a data loader\nif __name__ == \"__main__\":\n    # Depending on your specific utilization, setting up a DataLoader with multiple workers\n    # Here's an example with a simple dummy dataset\n    from torch.utils.data import DataLoader, Dataset\n\n    class DummyDataset(Dataset):\n        def __len__(self):\n            return 100\n        \n        def __getitem__(self, index):\n            return index\n\n    def worker_init_fn(worker_id):\n        seed = pytorch_worker_seed()\n        torch.manual_seed(seed)\n        print(f\"Worker {worker_id} is initialized with seed {seed}\")\n\n    dataset = DummyDataset()\n    loader = DataLoader(dataset, num_workers=4, worker_init_fn=worker_init_fn)\n\n    # Fetch data to observe action\n    for data in loader:\n        # Each worker prints its seed, only once per worker initialization\n        pass", "CodeBleu_score": 47.258183980696906}, {"code": "import torch\nimport torch.utils.data\n\ndef pytorch_worker_seed():\n    \"\"\"\n    The function retrieves the seed for a PyTorch DataLoader worker.\n    It first attempts to get the seed from the PyTorch worker info.\n    If the worker info is not available, it falls back to a seed based on the random method.\n    \"\"\"\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None: # if we are in a worker process\n        # The worker id uniquely identifies the worker process\n        seed = worker_info.id\n    else:\n        # fallback seed in the main process\n        seed = torch.initial_seed() % (2**32)  # a common practice of using initial_seed for different scenarios\n\n    return seed\n\n# Example of how this function might be used within a DataLoader context\nif __name__ == \"__main__\":\n    dataset = torch.utils.data.TensorDataset(torch.randn(100, 2))\n    loader = torch.utils.data.DataLoader(\n        dataset, batch_size=10, shuffle=True, num_workers=4,\n        worker_init_fn=lambda worker_id: torch.manual_seed(pytorch_worker_seed())\n    )\n\n    for batch in loader:\n        print(batch)", "CodeBleu_score": 39.99055967733482}, {"code": "import torch\n\ndef pytorch_worker_seed():\n    \"\"\"The function retrieves the seed for a PyTorch DataLoader worker.\n    It first attempts to get the seed from the PyTorch worker info.\n    If the worker info is not available, it falls back to a random seed.\n    \"\"\"\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        # Use the worker seed provided by the DataLoader context\n        return worker_info.seed\n    else:\n        # Fallback to a random seed if not in a worker process\n        return torch.initial_seed()", "CodeBleu_score": 60.10411279837757}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "from PIL import Image\n\ndef solarize_add(img, add, thresh=128):\n    \"\"\"The function applies a solarize add effect to an image.\n    It creates a lookup table (LUT) that adds a specified value to pixel values below a threshold and keeps other pixels unchanged.\n    If the image mode is \"L\" or \"RGB\", it applies the LUT to the image.\n    It returns the modified image or the original image if the mode is not supported.\"\"\"\n    \n    # Create a lookup table\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            # Add 'add' to values under the threshold but ensure not to exceed 255.\n            new_val = min(i + add, 255)\n        else:\n            new_val = i\n        lut.append(new_val)\n    \n    # Check if the image mode is either grayscale \"L\" or \"RGB\"\n    if img.mode in ('L', 'RGB'):\n        # Apply the lookup table to each channel in the image\n        if img.mode == 'RGB':\n            # For RGB, the LUT needs to be applied to all three channels\n            luts = [lut] * 3  # replicate LUT for R, G, B channels\n            img = img.convert(\"RGB\")  # Ensure image is in RGB mode\n        else:\n            luts = lut\n        # Point to apply image.point which applies the LUT to each channel\n        img = img.point(luts)\n        return img\n    else:\n        # Return the original image if not in 'L' or 'RGB' mode\n        return img\n\n# Example usage:\n# Load an image\nimage = Image.open(\"path_to_your_image.jpg\")\n# Apply the solarize add effect\nsolarized_image = solarize_add(image, add=50, thresh=128)\n# Optionally, save or show the modified image\nsolarized_image.show()", "CodeBleu_score": 39.7308520194775}, {"code": "from PIL import Image\n\ndef solarize_add(img, add, thresh=128):\n    \"\"\"\n    The function applies a solarize add effect to an image.\n    It creates a lookup table (LUT) that adds a specified value to pixel values below a threshold\n    and keeps other pixels unchanged.\n    If the image mode is \"L\" or \"RGB\", it applies the LUT to the image.\n    It returns the modified image or the original image if the mode is not supported.\n\n    Args:\n    img (PIL.Image): The image to apply the solarize effect on.\n    add (int): The value to add to pixel values under the threshold.\n    thresh (int): The threshold below which to add the value.\n\n    Returns:\n    PIL.Image: The modified or original image depending on the image mode.\n    \"\"\"\n    \n    # Support either grayscale (\"L\") or RGB (\"RGB\") modes\n    if img.mode not in [\"L\", \"RGB\"]:\n        return img  # Return the original image if not supported mode\n\n    # Creating a lookup table\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(i + add, 255))  # Add 'add' to pixel values below the threshold, capped at 255\n        else:\n            lut.append(i)  # Keep other pixel values unchanged\n\n    # The PIL image's `point` method allows us to apply the lookup table to each pixel in an image\n    return img.point(lut)\n\n# Example usage:\n# Assuming you have an image variable 'image' which is a PIL Image instance\n# modified_image = solarize_add(image, add=50, thresh=128)", "CodeBleu_score": 44.32154655681466}, {"code": "from PIL import Image\n\ndef solarize_add(img, add, thresh=128):\n    \"\"\"The function applies a solarize add effect to an image.\n    It creates a lookup table (LUT) that adds a specified value to pixel values below a threshold and keeps other pixels unchanged.\n    If the image mode is \"L\" or \"RGB\", it applies the LUT to the image.\n    It returns the modified image or the original image if the mode is not supported.\"\"\"\n\n    # Check if image mode is supported\n    if img.mode not in [\"L\", \"RGB\"]:\n        return img  # Return original image if image mode is not supported\n\n    # Create a lookup table for pixel transformation\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n\n    # Apply LUT for grayscale image\n    if img.mode == 'L':\n        img = img.point(lut)\n    \n    # Apply LUT for each channel of the RGB image\n    elif img.mode == 'RGB':\n        img = img.convert('RGB')\n        channels = []\n        for channel in img.split():\n            channels.append(channel.point(lut))\n        img = Image.merge('RGB', channels)\n    \n    return img\n\n# Example Usage (if you would like to test the function)\nif __name__ == \"__main__\":\n    # Load image (ensure the path is correct)\n    image = Image.open(\"path_to_your_image.jpg\")\n    # Apply the solarize add effect\n    modified_image = solarize_add(image, add=50, thresh=100)\n    # Save or show the modified image\n    modified_image.show()", "CodeBleu_score": 51.70814685469388}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict):\n    \"\"\"\n    The function processes a dictionary of transforms and their probabilities.\n    It extracts the transform names and their corresponding probabilities, normalizes\n    the probabilities so they sum to one, and returns the transforms and normalized\n    probabilities as separate tuples.\n    \n    Args:\n    transforms (Dict): A dictionary where keys are transform names and values\n                       are the corresponding probabilities (non-negative numbers).\n                       \n    Returns:\n    Tuple[Tuple, Tuple]: First tuple contains transform names, second tuple contains\n                         normalized probabilities corresponding to each transform.\n    \"\"\"\n    total_probability = sum(transforms.values())\n    if total_probability == 0:\n        raise ValueError(\"Total probability cannot be zero\")\n    \n    normalized_probabilities = {transform: prob / total_probability for transform, prob in transforms.items()}\n    \n    transform_names = tuple(transforms.keys())\n    normalized_prob_values = tuple(normalized_probabilities.values())\n    \n    return transform_names, normalized_prob_values\n\n# Example usage:\ntransforms = {\n    'rotate': 0.5,\n    'flip': 0.3,\n    'color_adjust': 0.2\n}\ntransform_names, normalized_probs = _get_weighted_transforms(transforms)\nprint(\"Transform Names:\", transform_names)\nprint(\"Normalized Probabilities:\", normalized_probs)", "CodeBleu_score": 28.642664457161658}, {"code": "def _get_weighted_transforms(transforms: Dict):\n    \"\"\"\n    The function processes a dictionary of transforms and their probabilities.\n    It extracts the transform names and their corresponding probabilities, \n    normalizes the probabilities so they sum to one, and \n    returns the transforms and normalized probabilities as separate tuples.\n\n    Args:\n    transforms (Dict): A dictionary where keys are the names of transforms and values are their corresponding probabilities.\n\n    Returns:\n    Tuple: A tuple containing two elements:\n        1. A tuple of transform names.\n        2. A tuple of corresponding normalized probabilities.\n    \"\"\"\n    # Extract transform names and their probabilities\n    transform_names = tuple(transforms.keys())\n    probabilities = tuple(transforms.values())\n\n    # Calculate the sum of probabilities to normalize them\n    total_prob = sum(probabilities)\n    \n    # Normalize the probabilities\n    normalized_probabilities = tuple(prob / total_prob for prob in probabilities)\n\n    return (transform_names, normalized_probabilities)", "CodeBleu_score": 29.690785207598214}, {"code": "def _get_weighted_transforms(transforms: Dict):\n    \"\"\"\n    The function processes a dictionary of transforms and their probabilities.\n    It extracts the transform names and their corresponding probabilities, \n    normalizes the probabilities so they sum to one, \n    and returns the transforms and normalized probabilities as separate tuples.\n    \n    Parameters:\n    - transforms (Dict): a dictionary with transform names as keys and probabilities as values\n    \n    Returns:\n    - Tuple[List[str], List[float]]: A tuple containing list of transform names and list of normalized probabilities\n    \"\"\"\n    # Extracting the keys (transform names) and values (probabilities)\n    transform_names = list(transforms.keys())\n    probabilities = list(transforms.values())\n    \n    # Sum of all probabilities for normalization\n    total_probability = sum(probabilities)\n    \n    # Normalizing probabilities\n    if total_probability == 0:\n        raise ValueError(\"Total sum of probabilities cannot be zero.\")\n    \n    normalized_probabilities = [p / total_probability for p in probabilities]\n    \n    # Returning as separate tuples\n    return (transform_names, normalized_probabilities)", "CodeBleu_score": 27.38070751957039}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "import numpy as np\nfrom PIL import Image, ImageOps\nimport random\n\ndef _apply_basic(self, img, mixing_weights, m):\n    \"\"\"\n    The function applies a series of augmentations to an image and blends the results.\n    It initializes an empty array for the mixed image and iterates through given mixing weights.\n    For each weight, it applies a random sequence of operations to the image, accumulating the results.\n    The mixed image is then clipped to valid pixel values, converted back to an image, and blended with the original image using a specified blending factor.\n    The final blended image is returned.\n    \"\"\"\n    original_img = img.copy()\n    img = np.array(img, dtype=np.float32)\n    mixed_image = np.zeros_like(img)\n\n    def random_augment(image):\n        \"\"\"Apply a random augmentation to the image.\"\"\"\n        # Implementing simple rotations and translations as examples\n        if random.choice([True, False]):\n            image = image.rotate(random.randint(-30, 30))  # Random rotation\n\n        if random.choice([True, False]):\n            translate_x = random.randint(-10, 10)\n            translate_y = random.randint(-10, 10)\n            image = ImageOps.offset(image, translate_x, translate_y)  # Random translation\n\n        return image\n\n    for weight in mixing_weights:\n        aug_img = random_augment(img.copy())\n        aug_img_array = np.array(aug_img, dtype=np.float32)\n        mixed_image += weight * aug_img_array\n\n    # Clipping the mixed image to valid pixel values (0-255)\n    np.clip(mixed_image, 0, 255, out=mixed_image)\n    \n    # Converting the array back to an image\n    mixed_image = Image.fromarray(mixed_image.astype(np.uint8))\n\n    # Blending mixed image with the original image by factor `m`\n    final_image = Image.blend(original_img, mixed_image, m)\n    \n    return final_image", "CodeBleu_score": 39.85497712366408}, {"code": "import numpy as np\nfrom PIL import Image, ImageEnhance, ImageFilter\nimport random\n\ndef _apply_basic(self, img, mixing_weights, m):\n    \"\"\"\n    The function applies a series of augmentations to an image and blends the results.\n    It initializes an empty array for the mixed image and iterates through given mixing weights.\n    For each weight, it applies a random sequence of operations to the image, accumulating the results.\n    The mixed image is then clipped to valid pixel values, converted back to an image, and blended with the original image using a specified blending factor.\n    The final blended image is returned.\n    \"\"\"\n    # Convert input image to numpy array\n    img_array = np.array(img, dtype=np.float32)\n    # Initialize a mixed image array\n    mixed_image = np.zeros_like(img_array)\n\n    # Define a simple list of possible enhancements or filters here\n    filters = [ImageEnhance.Color, ImageEnhance.Contrast, ImageEnhance.Brightness, ImageEnhance.Sharpness]\n    options = [ImageFilter.BLUR, ImageFilter.CONTOUR, ImageFilter.DETAIL]\n\n    # Apply augmentations according to the weights\n    for weight in mixing_weights:\n        # Create a copy of the image for applying random operations\n        temp_image = img.copy()\n        \n        # Apply some random filters and enhancements\n        enhancer = random.choice(filters)(temp_image)\n        factor = random.uniform(0.5, 1.5)  # Random factor for enhancement\n        temp_image = enhancer.enhance(factor)\n        \n        # Randomly apply one of the image filters\n        if random.choice([True, False]):  # 50% chance\n            temp_image = temp_image.filter(random.choice(options))\n        \n        # Blend the operations\n        mixed_image += weight * np.array(temp_image, dtype=np.float32)\n\n    # Clip to valid range and convert to uint8\n    mixed_image = np.clip(mixed_image, 0, 255).astype(np.uint8)\n\n    # Convert mixed image back to PIL Image\n    mixed_image = Image.fromarray(mixed_image)\n    \n    # Blend mixed image with original image using the blending factor 'm'\n    final_image = Image.blend(img, mixed_image, m)\n    return final_image", "CodeBleu_score": 35.4509454314757}, {"code": "import numpy as np\nfrom PIL import Image\nimport random\n\ndef _apply_basic(self, img, mixing_weights, m):\n    \"\"\"\n    The function applies a series of augmentations to an image and blends the results.\n    It initializes an empty array for the mixed image and iterates through given mixing weights.\n    For each weight, it applies a random sequence of operations to the image, accumulating the results.\n    The mixed image is then clipped to valid pixel values, converted back to an image, and blended with \n    the original image using a specified blending factor.\n    The final blended image is returned.\n    \"\"\"\n    # Initialize an array for the mixed image\n    img_array = np.array(img, dtype=np.float32)\n    mixed_image = np.zeros_like(img_array)\n\n    # Height and Width of the image\n    height, width = img_array.shape[:2]\n    \n    # Apply random sequence of augmentations and accumulate results\n    for weight in mixing_weights:\n        augmentation_image = img_array.copy()\n        \n        # Example augmentations: Apply a rotation and/or change brightness randomly\n        if random.choice([True, False]):\n            # Rotate the image\n            rotation_angle = random.choice([90, 180, 270])\n            augmentation_image = Image.fromarray(np.uint8(augmentation_image)).rotate(rotation_angle)\n            augmentation_image = np.array(augmentation_image, dtype=np.float32)\n\n        if random.choice([True, False]):\n            # Change brightness\n            brightness_factor = random.uniform(0.5, 1.5)\n            augmentation_image *= brightness_factor\n            \n        # Accumulate the weighted results\n        mixed_image += weight * augmentation_image\n    \n    # Clip the pixel values to be in the valid range [0, 255]\n    mixed_image = np.clip(mixed_image, 0, 255)\n\n    # Convert back to an image\n    mixed_image = Image.fromarray(np.uint8(mixed_image))\n\n    # Blend with the original image using the blending factor 'm'\n    final_image = Image.blend(img, mixed_image, alpha=m)\n\n    return final_image", "CodeBleu_score": 34.966754800668255}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n):\n    \"\"\"\n    The function adapts input x to match the required number of channels n.\n    If x is not a tuple or list, it repeats x n times to form a tuple.\n    If x is a tuple or list but its length is not equal to n, it calculates the mean of x and repeats it n times, logging a warning.\n    If x is already a tuple or list of length n, it asserts the length matches n.\n    The function returns the adapted tuple x.\n    \"\"\"\n\n    import warnings\n\n    # Check if x is not a tuple or list\n    if not isinstance(x, (tuple, list)):\n        adapted_x = tuple([x] * n)\n    else:\n        # If x is a tuple or list, check if its length is exactly n\n        if len(x) == n:\n            adapted_x = tuple(x)\n        elif len(x) != n:\n            # Calc the mean of elements in x, since its length differs\n            mean_value = sum(x) / len(x)\n            adapted_x = tuple([mean_value] * n)\n            # Log a warning about adapting the input\n            warnings.warn(f'Input list/tuple length ({len(x)}) is not equal to n ({n}). Using the mean value {mean_value}.')\n    \n    # Assert the length after adaptation\n    assert len(adapted_x) == n, \"Length of the adapted input does not match the required number of channels.\"\n    \n    return adapted_x", "CodeBleu_score": 40.59745187039339}, {"code": "def adapt_to_chs(x, n):\n    \"\"\"\n    The function adapts input x to match the required number of channels n.\n    If x is not a tuple or list, it repeats x n times to form a tuple.\n    If x is a tuple or list but its length is not equal to n, it calculates the mean of x and repeats it n times, logging a warning.\n    If x is already a tuple or list of length n, it asserts the length matches n.\n    The function returns the adapted tuple x.\n    \"\"\"\n    import logging\n    from statistics import mean\n\n    # Set up logging\n    logging.basicConfig(level=logging.WARNING)\n    \n    # Check if x is neither a tuple nor a list type\n    if not isinstance(x, (tuple, list)):\n        x = tuple([x] * n)  # Replicate x, n times\n    elif len(x) != n:\n        # Log a warning if the length of x is not equal to n\n        logging.warning(f\"Length mismatch: Expected {n} elements, but got {len(x)}. Calculating mean and adapting.\")\n        avg = mean(x)\n        x = tuple([avg] * n)  # Create tuple consisting of mean, replicated n times\n    else:\n        # Assert length is exactly n if x is already the correct type\n        assert len(x) == n, f\"Length of the tuple/list should match n, expected {n} got {len(x)}\"\n    \n    return x", "CodeBleu_score": 42.30155813673133}, {"code": "def adapt_to_chs(x, n)\ndef adapt_to_chs(x, n):\n    \"\"\"\n    The function adapts input x to match the required number of channels n.\n    If x is not a tuple or list, it repeats x n times to form a tuple.\n    If x is a tuple or list but its length is not equal to n, it calculates the mean of x and repeats it n times, logging a warning.\n    If x is already a tuple or list of length n, it asserts the length matches n.\n    The function returns the adapted tuple x.\n    \"\"\"\n    import logging\n    # Setup basic configuration for logging\n    logging.basicConfig(level=logging.WARNING)\n\n    # Check if x is a tuple or list\n    if not isinstance(x, (tuple, list)):\n        # x is not a tuple or list, repeat x n times\n        return tuple([x] * n)\n    else:\n        # x is a tuple or list\n        if len(x) == n:\n            # x already has the correct length\n            return tuple(x)\n        else:\n            # x does not have the correct length, calculate its mean and repeat it\n            mean_value = sum(x) / len(x)  # Calculate the mean of x\n            logging.warning(\n                f\"The length of x ({len(x)}) does not match the required number of channels ({n}).\"\n                \" The mean value of x is used and repeated for all channels.\"\n            )\n            return tuple([mean_value] * n)  # Repeat the mean value n times", "CodeBleu_score": 29.764288389646477}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "import numpy as np\nimport random\nimport torch  # Assuming PyTorch is used and installed\n\ndef _worker_init(worker_id, worker_seeding='all'):\n    \"\"\"\n    The function initializes a worker with a specified seed for reproducibility.\n    It retrieves the worker information and verifies the worker ID.\n    If a callable worker_seeding is provided, it generates and sets the seed for random, torch, and numpy.\n    If worker_seeding is a string, it sets the numpy seed based on the worker_info seed for 'all' seeding.\n    \"\"\"\n    \n    # Verify worker ID (assuming worker IDs should be non-negative integers)\n    if not isinstance(worker_id, int) or worker_id < 0:\n        raise ValueError(\"worker_id should be a non-negative integer\")\n    \n    # Store the seed for possible use in both scenarios (callable or 'all')\n    base_seed = 42  # A placeholder seed, use any appropriate seed mechanism, e.g., hash(worker_id)\n\n    # Check if worker_seeding is callable to generate and apply a custom seed function\n    if callable(worker_seeding):\n        seed = worker_seeding(worker_id)\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n    elif worker_seeding == 'all':\n        # Seed all necessary components with a derived seed (could extend this logic further)\n        seed = base_seed + worker_id\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n    else:\n        raise ValueError(\"worker_seeding should be 'all' or a callable\")", "CodeBleu_score": 35.02427945130604}, {"code": "import random\nimport numpy as np\n\ntry:\n    import torch\nexcept ImportError:\n    torch = None  # Handle cases where PyTorch is not installed\n\ndef _worker_init(worker_id, worker_seeding='all'):\n    \"\"\"\n    The function initializes a worker with a specified seed for reproducibility.\n    It retrieves the worker information and verifies the worker ID.\n    If a callable worker_seeding is provided, it generates and sets the seed for random, torch, and numpy.\n    If worker_seeding is a string, it sets the numpy seed based on the worker_info seed for 'all' seeding.\n    \"\"\"\n    # Worker information verification (simulated here, would likely be more complex in a full system)\n    print(f\"Initializing worker with ID: {worker_id}\")\n\n    seed = None\n    if callable(worker_seeding):\n        seed = worker_seeding(worker_id)\n    elif isinstance(worker_seeding, str) and worker_seeding == 'all':\n        # For simplification here, we just use worker_id as the base for seed generation\n        # This can be replaced by any seed generation based on worker_id\n        seed = worker_id\n\n    if seed is not None:\n        print(f\"Setting seed for worker {worker_id}: {seed}\")\n        random.seed(seed)  # Seed Python's built-in random module\n        np.random.seed(seed)  # Seed numpy's random number generator\n        \n        if torch is not None:\n            torch.manual_seed(seed)  # Seed PyTorch random generator\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(seed)  # Seed for all CUDA devices to ensure reproducibility on GPU as well\n    else:\n        print(\"No seeding mode provided, continuing without setting seed.\")\n\n# Example usage:\n_worker_init(123, worker_seeding=lambda x: x*2)  # Example with a lambda function that doubles worker_id to generate a seed\n_worker_init(321, 'all')  # Example with string-based seeding", "CodeBleu_score": 37.224251947458676}, {"code": "import numpy as np\nimport random\nimport torch\n\ndef _worker_init(worker_id, worker_seeding='all'):\n    \"\"\"\n    The function initializes a worker with a specified seed for reproducibility.\n    It retrieves the worker information and verifies the worker ID.\n    If a callable worker_seeding is provided, it generates and sets the seed for random, torch, and numpy.\n    If worker_seeding is a string, it sets the numpy seed based on the worker_info seed for 'all' seeding.\n\n    Parameters:\n    - worker_id (int or any): The identifier for the worker.\n    - worker_seeding (callable or str): Function to generate seed or a string directive for pre-defined seeding.\n\n    Returns:\n    None\n    \"\"\"\n\n    # Assume worker_info is a global function/repository that holds worker information including seeds\n    worker_info = {\n        'seed': worker_id * 1337 + 42   # Example of generating a reproducible but unique seed for each worker_id\n    }\n    \n    if callable(worker_seeding):\n        # Generate seed using the provided function and then set the seed\n        seed = worker_seeding(worker_info)\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n    elif isinstance(worker_seeding, str) and worker_seeding.lower() == 'all':\n        # Set seed using the worker_info 'seed'\n        seed = worker_info['seed']\n        np.random.seed(seed)\n        random.seed(seed)\n        torch.manual_seed(seed)\n    else:\n        raise ValueError(\"Invalid worker_seeding parameter, it should either be a callable or 'all' as string\")\n\n# Note: To fully use this code, guarantees about environment structure and detailed requirements are needed.\n# The work on worker_info and handling of torch, random, etc., is illustrative and may require adjustments to fit specific implementations.", "CodeBleu_score": 44.67184606269651}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "import tensorflow as tf\n\ndef _decode_and_center_crop(image_bytes, image_size, resize_method):\n    \"\"\"\n    The function decodes a JPEG image, crops it to the center with padding, and resizes it to a specified size.\n    It first extracts the image shape and calculates the padded center crop size based on the given image size and \n    a padding constant. It determines the offset for the crop window, decodes and crops the image, and then resizes \n    the cropped image using the specified resize method. The processed image is returned.\n    \n    Args:\n    - image_bytes (bytes): Raw image bytes.\n    - image_size (int): Desired size of the output image (height and width will be the same).\n    - resize_method (str): Method used for resizing; options include 'bilinear', 'nearest', 'bicubic', 'area'.\n    \n    Returns:\n    - Tensor: The decoded, cropped, and resized image as a Tensor.\n    \"\"\"\n    \n    if resize_method not in ['bilinear', 'nearest', 'bicubic', 'area']:\n        raise ValueError(\"resize_method must be one of 'bilinear', 'nearest', 'bicubic', 'area'\")\n    \n    # Decode the image\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    \n    # Retrieve the dimensions of the image\n    shape = tf.shape(image)\n    height, width = shape[0], shape[1]\n    \n    # Calculate padding\n    padding = tf.cast(tf.math.minimum(height, width) * 0.1, tf.int32)\n    \n    # Calculate padded center crop size\n    target_height = image_size + 2 * padding\n    target_width = image_size + 2 * padding\n    \n    # Make the dimensions compatible\n    target_height = tf.minimum(target_height, height)\n    target_width = tf.minimum(target_width, width)\n    \n    # Compute the offset for the center crop (make sure to handle odd dimensions)\n    offset_height = (height - target_height + 1) // 2\n    offset_width = (width - target_width + 1) // 2\n    \n    # Crop the image\n    cropped_image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)\n    \n    # Resize the image\n    resize_fn = getattr(tf.image, 'resize_' + resize_method)\n    resized_image = resize_fn(cropped_image, [image_size, image_size])\n    \n    return resized_image", "CodeBleu_score": 41.22972907010368}, {"code": "from PIL import Image\nimport io\n\ndef _decode_and_center_crop(image_bytes, image_size, resize_method='bilinear'):\n    \"\"\"\n    The function decodes a JPEG image, crops it to the center with padding, and resizes it to a specified size.\n    It first extracts the image shape and calculates the padded center crop size based on the given image size and a padding constant.\n    It determines the offset for the crop window, decodes and crops the image, and then resizes the cropped image using the specified resize method.\n    The processed image is returned.\n    \n    :param image_bytes: Bytes object of the image.\n    :param image_size: Target size (width, height) the image should be resized to.\n    :param resize_method: Method to be used for resizing. Can be one from ['bilinear', 'nearest', 'bicubic', 'lanczos'].\n    :returns: A PIL Image object of the resized and cropped image.\n    \"\"\"\n    \n    # Read the image from bytes and open it with PIL\n    image = Image.open(io.BytesIO(image_bytes))\n    \n    # Original dimensions\n    orig_width, orig_height = image.size\n    \n    # Define padding\n    padding = min(orig_width, orig_height) * 0.10  # 10% of the minimum dimension\n    \n    # Calculate padded center crop size\n    if orig_width > orig_height:\n        crop_height = int(orig_height - 2 * padding)\n        crop_width = int(crop_height)\n    else:\n        crop_width = int(orig_width - 2 * padding)\n        crop_height = int(crop_width)\n    \n    # Calculate crop window offset\n    left = (orig_width - crop_width) // 2\n    top = (orig_height - crop_height) // 2\n    \n    # Crop the image\n    cropped_image = image.crop((left, top, left + crop_width, top + crop_height))\n    \n    # Resize options\n    resize_options = {\n        'bilinear': Image.BILINEAR,\n        'nearest': Image.NEAREST,\n        'bicubic': Image.BICUBIC,\n        'lanczos': Image.LANCZOS,\n    }\n    \n    # Resize the image\n    resized_image = cropped_image.resize(image_size, resize_options[resize_method])\n    \n    return resized_image", "CodeBleu_score": 31.415250724463288}, {"code": "from PIL import Image, ImageOps\nimport io\n\ndef _decode_and_center_crop(image_bytes, image_size, resize_method=Image.ANTIALIAS):\n    \"\"\"\n    The function decodes a JPEG image, crops it to the center with padding, and resizes it to a specified size.\n    It first extracts the image shape and calculates the padded center crop size based on the given image size and a padding constant.\n    It determines the offset for the crop window, decodes and crops the image, and then resizes the cropped image using the specified resize method.\n    The processed image is returned.\n    \n    Args:\n    - image_bytes (bytes): Byte array of the JPEG image.\n    - image_size (tuple): Desired size of the image as (width, height).\n    - resize_method: The method used for resizing (default PIL.Image.ANTIALIAS).\n\n    Returns:\n    - Image: The processed PIL Image object.\n    \"\"\"\n    # Decode the image\n    image = Image.open(io.BytesIO(image_bytes))\n    \n    # Calculate central crop dimensions\n    width, height = image.size\n    aspect_ratio = width / height\n    target_aspect_ratio = image_size[0] / image_size[1]\n    \n    # Calculate the padding and dimension for cropping\n    if aspect_ratio > target_aspect_ratio:\n        # more wide than tall, lock height to image_size[1], adjust width\n        new_height = image_size[1]\n        new_width = int(new_height * aspect_ratio)\n    else:\n        # more tall than wide, lock width to image_size[0], adjust height\n        new_width = image_size[0]\n        new_height = int(new_width / aspect_ratio)\n    \n    image = image.resize((new_width, new_height), resize_method)\n    \n    # Calculate center crop\n    left = (new_width - image_size[0]) / 2\n    top = (new_height - image_size[1]) / 2\n    right = left + image_size[0]\n    bottom = top + image_size[1]\n    \n    # Crop and resize image\n    image = image.crop((left, top, right, bottom))\n    \n    return image\n\n# Example usage\n# Assuming image_bytes is already defined\n# processed_image = _decode_and_center_crop(image_bytes, (224, 224))", "CodeBleu_score": 30.882835271198694}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "import tensorflow as tf\n\ndef preprocess_for_train(image_bytes, use_bfloat16, image_size=224, interpolation='bicubic'):\n    \"\"\"The function preprocesses image bytes for training.\n    It decodes and randomly crops the image using a specified interpolation method, flips the image, and reshapes it to the target size.\n    The image is then converted to the specified dtype, either bfloat16 or float32, and returned.\n\n    Args:\n    image_bytes: A tensor of type string representing the raw image bytes.\n    use_bfloat16: A boolean indicating whether to convert the image to bfloat16 or keep it in float32.\n    image_size: An integer or tuple indicating the size to which the image should be resized.\n    interpolation: String, the interpolation method used in resizing. Options include 'bicubic', 'nearest', 'bilinear', etc.\n\n    Returns:\n    A tensor representing the processed image.\n    \"\"\"\n    # Decode the image bytes\n    image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n\n    # Apply random crop and flip for data augmentation\n    image = tf.image.random_crop(image, size=[image_size, image_size, 3])\n    image = tf.image.random_flip_left_right(image)\n\n    # Resize the decoded and augmented image to a fixed size using the specified interpolation method\n    if isinstance(image_size, int):\n        size = [image_size, image_size]\n    else:\n        size = image_size\n\n    image = tf.image.resize(image, size, method=interpolation)\n\n    # Convert to appropriate dtype\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n\n    return image", "CodeBleu_score": 44.13276455469507}, {"code": "import tensorflow as tf\n\ndef preprocess_for_train(image_bytes, use_bfloat16, image_size=(224, 224), interpolation='bicubic'):\n    \"\"\"\n    The function preprocesses image bytes for training.\n    It decodes and randomly crops the image using a specified interpolation method, flips the image, \n    and reshapes it to the target size. The image is then converted to the specified dtype, \n    either bfloat16 or float32, and returned.\n\n    Args:\n    image_bytes (bytes): Raw image bytes.\n    use_bfloat16 (bool): Whether to use bfloat16 type or float32.\n    image_size (tuple): Tuple of two integers specifying the width and height to resize images to after cropping.\n    interpolation (str): Method of interpolation ('bicubic', 'nearest', etc.).\n\n    Returns:\n    Tensor: Preprocessed image tensor.\n    \"\"\"\n    # Decode the image from bytes and ensure output has 3 color channels (RGB)\n    image = tf.io.decode_image(image_bytes, channels=3)\n\n    # Set the resizing and cropping sizes\n    crop_size = tf.random.uniform(shape=(), minval=int(0.8 * image_size[0]), maxval=image_size[0], dtype=tf.int32)\n    \n    # Resizing to a larger dimension before random cropping to the target dimension\n    if interpolation == 'bicubic':\n        resize_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'nearest':\n        resize_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError(f\"Unsupported interpolation type: {interpolation}\")\n\n    image = tf.image.resize(image, [crop_size, crop_size], method=resize_method)\n    \n    # Randomly crop to the target size\n    image = tf.image.random_crop(image, size=[image_size[0], image_size[1], 3])\n\n    # Randomly flip the image horizontally\n    image = tf.image.random_flip_left_right(image)\n\n    # Convert image to the desired data type\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n\n    # Normalizing the image to [-1, 1]\n    image = tf.divide(tf.subtract(image, 127.5), 127.5)\n    \n    return image", "CodeBleu_score": 37.68775540838279}, {"code": "import tensorflow as tf\n\nIMAGE_SIZE = 224  # Assuming default image size; adjust if necessary.\n\ndef preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic'):\n    \"\"\"\n    The function preprocesses image bytes for training.\n    It decodes and randomly crops the image using a specified interpolation method, flips the image horizontally,\n    and reshapes it to the target size. The image is then converted to the specified dtype, either bfloat16 or float32, and returned.\n\n    Args:\n        image_bytes (bytes): The raw image bytes.\n        use_bfloat16 (bool): Whether to use bfloat16 data type (True) or float32 (False).\n        image_size (int, optional): The target size to which image is resized. Defaults to 224.\n        interpolation (str, optional): The interpolation method used for resizing ('bicubic', 'nearest', etc.). Defaults to 'bicubic'.\n\n    Returns:\n        tf.Tensor: A preprocessed image tensor ready for training.\n    \"\"\"\n    # Decode the image from bytes\n    image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n\n    # Convert image to float32\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Randomly crop the image\n    image = tf.image.random_crop(image, size=[image_size, image_size, 3])\n\n    # Resize the image\n    if interpolation == 'bicubic':\n        method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'nearest':\n        method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    elif interpolation == 'bilinear':\n        method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'lanczos3':\n        method = tf.image.ResizeMethod.LANCZOS3\n    elif interpolation == 'lanczos5':\n        method = tf.image.ResizeMethod.LANCZOS5\n    else:\n        method = tf.image.ResizeMethod.BICUBIC  # Default to bicubic if not specified or unknown.\n    image = tf.image.resize(image, [image_size, image_size], method=method)\n\n    # Randomly flip the image\n    image = tf.image.random_flip_left_right(image)\n\n    # Convert the data type if required\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        # Use float32\n        image = tf.cast(image, dtype=tf.float32)\n    \n    return image", "CodeBleu_score": 37.490853771647814}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "import tensorflow as tf\n\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size, interpolation='bicubic'):\n    \"\"\"\n    The function preprocesses an image for evaluation by decoding and centering the crop,\n    resizing it, and converting its data type.\n    It selects the resize method based on the interpolation parameter, decodes and centers the image crop,\n    reshapes it to the specified size, and converts the image data type to either bfloat16 or float32\n    based on the use_bfloat16 flag.\n    The preprocessed image is then returned.\n\n    Args:\n        image_bytes (bytes): Byte string of the image data.\n        use_bfloat16 (bool): Flag to determine whether output image dtype should be bfloat16.\n        image_size (tuple of ints): The target size for the image as (height, width).\n        interpolation (str): Interpolation method to be used for resizing. Supported values include\n            'bicubic', 'nearest', 'bilinear', 'area', 'lanczos3', 'lanczos5', 'gaussian', or 'mitchellcubic'.\n    \n    Returns:\n        tf.Tensor: Preprocessed image tensor.\n    \"\"\"\n    # Decode image bytes\n    image = tf.io.decode_image(image_bytes, channels=3, expand_animations=False)\n    \n    # Determine the shape of the decoded image\n    shape = tf.shape(image)\n    \n    # Calculate padding for center crop\n    target_height, target_width = image_size\n    offset_height = (shape[0] - target_height) // 2\n    offset_width = (shape[1] - target_width) // 2\n    \n    # Center crop the image\n    image_cropped = tf.image.crop_to_bounding_box(\n        image, \n        offset_height, \n        offset_width, \n        target_height, \n        target_width\n    )\n    \n    # Resize using specified interpolation method if crop dimensions not met by the initial crop\n    if interpolation not in ['bicubic', 'nearest', 'bilinear', 'area', 'lanczos3', 'lanczos5', 'gaussian', 'mitchellcubic']:\n        interpolation = 'bicubic'  # default to bicubic if unspecified or wrong value provided\n        \n    resize_method = getattr(tf.image.ResizeMethod, interpolation.upper())\n    image_resized = tf.image.resize(image_cropped, image_size, method=resize_method)\n    \n    # Convert data type\n    if use_bfloat16:\n        image_resized = tf.cast(image_resized, tf.bfloat16)\n    else:\n        image_resized = tf.cast(image_resized, tf.float32)\n        \n    return image_resized", "CodeBleu_score": 30.578622726112236}, {"code": "import tensorflow as tf\n\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size=224, interpolation='bicubic'):\n    \"\"\"\n    Preprocesses an image for evaluation by decoding and centering the crop,\n    resizing it, and converting its data type.\n\n    Args:\n    image_bytes (bytes): Raw image bytes typically read from a file.\n    use_bfloat16 (bool): Flag to decide the precision of the floating point numbers.\n    image_size (int, optional): Size to which the image is resized (both width and height).\n    interpolation (str, optional): Method to use for resizing ('bicubic', 'bilinear', 'nearest', etc.).\n\n    Returns:\n    tf.Tensor: Processed image tensor.\n    \"\"\"\n    # Decode the image\n    image = tf.image.decode_image(image_bytes, channels=3, expand_animations=False)\n    image.set_shape([None, None, 3])\n\n    # Center the crop\n    image = tf.image.central_crop(image, central_fraction=0.875)\n\n    # Resize the image to the specified size\n    if interpolation == 'bicubic':\n        interpolation_method = tf.image.ResizeMethod.BICUBIC\n    elif interpolation == 'bilinear':\n        interpolation_method = tf.image.ResizeMethod.BILINEAR\n    elif interpolation == 'nearest':\n        interpolation_method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n    else:\n        raise ValueError(\"Unsupported interpolation type.\")\n\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation_method)\n\n    # Convert data type\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    else:\n        image = tf.cast(image, tf.float32)\n\n    return image", "CodeBleu_score": 42.55980093383743}, {"code": "import tensorflow as tf\n\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size=224, interpolation='bicubic'):\n    \"\"\"\n    Preprocesses an image for evaluation by decoding and centering the crop, resizing it,\n    and converting its data type.\n\n    Args:\n    image_bytes (bytes): Image data in byte format.\n    use_bfloat16 (bool): Flag to determine if the output tensor should be in bfloat16 format.\n    image_size (int, optional): Size to which the image is resized (assumes square image). Default is 224.\n    interpolation (str, optional): Resizing method. Supports 'bicubic', 'bilinear', 'nearest', etc. Default is 'bicubic'.\n\n    Returns:\n    tf.Tensor: Preprocessed image tensor.\n    \"\"\"\n    # Decode the image from bytes\n    image = tf.image.decode_image(image_bytes, channels=3)\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    \n    # Calculate padding to center crop the image\n    shape = tf.shape(image)\n    short_dim = tf.minimum(shape[0], shape[1])\n    offset_height = (shape[0] - short_dim) // 2\n    offset_width = (shape[1] - short_dim) // 2\n    \n    # Center crop the image\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, short_dim, short_dim)\n    \n    # Resize the image to the required dimensions\n    method = getattr(tf.image.ResizeMethod, interpolation.upper())\n    image = tf.image.resize(image, [image_size, image_size], method=method)\n    \n    # Convert image to bfloat16 if requested\n    if use_bfloat16:\n        image = tf.cast(image, tf.bfloat16)\n    \n    return image", "CodeBleu_score": 39.68549309549878}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "class ModelEvaluator:\n    def evaluate(self, model, eval_dataloader=None, output_file=\"merged\"):\n        \"\"\"\n        The function evaluates a model using a specified or default evaluation dataloader.\n        It runs a prediction loop with the model and dataloader, saving the outputs to a file.\n        It then computes evaluation metrics based on the predictions and returns the results.\n\n        Parameters:\n            model (torch.nn.Module): The model to be evaluated.\n            eval_dataloader (DataLoader, optional): The DataLoader for evaluation. Defaults to None.\n            output_file (str, optional): The name of the file to save the predictions. Defaults to \"merged\".\n\n        Returns:\n            dict: A dictionary containing evaluation metrics.\n        \"\"\"\n        import torch\n\n        if eval_dataloader is None:\n            raise ValueError(\"No evaluation dataloader provided\")\n\n        # Set model to evaluation mode\n        model.eval()\n\n        # Storing results\n        results = []\n        \n        # Loop over dataloader\n        with torch.no_grad():\n            for inputs, targets in eval_dataloader:\n                # Predict the outputs\n                outputs = model(inputs)\n                \n                # Store the outputs; convert outputs to a simpler format if necessary\n                results.extend(outputs.cpu().numpy())\n                \n        # Optionally, outputs can be saved to a file\n        with open(output_file + \".txt\", \"w\") as f:\n            for result in results:\n                f.write(str(result) + \"\\n\")\n        \n        # Calculate evaluation metrics\n        # Note: Actual code will depend on problem type (classification, regression, etc.)\n        # Simple accuracy example (for classification problems with integer labels)\n        correct_count = 0\n        total_count = 0\n        for inputs, targets in eval_dataloader:\n            with torch.no_grad():\n                outputs = model(inputs)\n                predicted_labels = outputs.argmax(dim=1)\n                correct_count += (predicted_labels == targets).sum().item()\n                total_count += targets.size(0)\n        \n        accuracy = correct_count / total_count\n        \n        # Return a dictionary containing metrics\n        return {'accuracy': accuracy}\n\n# Example of use\n# evaluator = ModelEvaluator()\n# model = YourModel()\n# eval_dataloader = DataLoader(your_dataset, batch_size=32, shuffle=False)\n# results = evaluator.evaluate(model, eval_dataloader, \"output_predictions\")", "CodeBleu_score": 23.547805379729265}, {"code": "import json\nfrom sklearn.metrics import accuracy_score, classification_report\n\ndef evaluate(self, model, eval_dataloader=None, output_file=\"merged\"):\n    \"\"\"\n    The function evaluates a model using a specified or default evaluation dataloader.\n    It runs a prediction loop with the model and dataloader, saving the outputs to a file.\n    It then computes evaluation metrics based on the predictions and returns the results.\n\n    Parameters:\n        model: The model to be evaluated.\n        eval_dataloader (Optional): A dataloader containing the evaluation dataset. If None, a default should be used.\n        output_file (str): The filename where the outputs will be saved.\n\n    Returns:\n        A dictionary containing evaluation metrics.\n    \"\"\"\n    if eval_dataloader is None:\n        raise ValueError(\"eval_dataloader is required but it's None.\")\n    \n    model.eval()  # Set the model to evaluation mode\n\n    predictions = []\n    references = []\n\n    # Loop over batches in the evaluation dataloader\n    for batch in eval_dataloader:\n        inputs, labels = batch\n\n        # Assuming the model has a method .predict() to make predictions on the inputs\n        outputs = model.predict(inputs)\n\n        predictions.extend(outputs)\n        references.extend(labels)\n    \n    # Save predictions to a file\n    with open(f\"{output_file}.json\", 'w') as outfile:\n        json.dump(predictions, outfile)\n\n    # Evaluate the model's performance\n    accuracy = accuracy_score(references, predictions)\n    report = classification_report(references, predictions, output_dict=True)\n\n    # Return the results\n    return {\n        'accuracy': accuracy,\n        'report': report\n    }", "CodeBleu_score": 24.81628139283651}, {"code": "import torch\n\ndef evaluate(self, model, eval_dataloader=None, output_file=\"merged\"):\n    \"\"\"\n    The function evaluates a model using a specified or default evaluation dataloader.\n    It runs a prediction loop with the model and dataloader, saving the outputs to a file.\n    It then computes evaluation metrics based on the predictions and returns the results.\n\n    Parameters:\n        model (torch.nn.Module): The machine learning model to be evaluated.\n        eval_dataloader (torch.utils.data.DataLoader): The dataloader to use for evaluation. If `None`, assumes a default dataloader is set on `self`.\n        output_file (str): The name of the file where the outputs will be saved.\n\n    Returns:\n        dict: A dictionary containing evaluation metrics.\n    \"\"\"\n    \n    if eval_dataloader is None:\n        if hasattr(self, 'eval_dataloader') and self.eval_dataloader is not None:\n            eval_dataloader = self.eval_dataloader\n        else:\n            raise ValueError(\"No evaluation dataloader provided or set as an attribute of the instance.\")\n    \n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.eval()  # Set the model to evaluation mode\n    model.to(device)\n    \n    predictions = []\n    actuals = []\n\n    with torch.no_grad():\n        for batch in eval_dataloader:\n            inputs = batch['input'].to(device)\n            labels = batch['label'].to(device)\n            outputs = model(inputs)  # Forward pass: compute the output of the model\n            \n            # Assuming outputs are given in a classification format\n            _, predicted = torch.max(outputs.data, 1)\n            predictions.extend(predicted.cpu().numpy())\n            actuals.extend(labels.cpu().numpy())\n    \n    results = self.calculate_metrics(actuals, predictions)\n    \n    with open(f\"{output_file}.txt\", \"w\") as f:\n        for item in zip(actuals, predictions):\n            f.write(f\"{item[0]}, {item[1]}\\n\")\n    \n    return results\n\ndef calculate_metrics(self, actuals, predictions):\n    \"\"\"\n    Calculate and return accuracy or other metrics normally used in model evaluation.\n    \"\"\"\n    from sklearn.metrics import accuracy_score, classification_report\n    accuracy = accuracy_score(actuals, predictions)\n    report = classification_report(actuals, predictions)\n    return {\"accuracy\": accuracy, \"report\": report}", "CodeBleu_score": 37.60009578346176}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": "def print_computed_metrics(self, metrics):\n    \"\"\"\n    The function prints the computed metrics for rank1, rank5, and miou.\n    It retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\n\n    Args:\n    metrics (dict): A dictionary containing the computed metrics values for 'rank1', 'rank5', and 'miou'.\n\n    \"\"\"\n    rank1 = metrics.get('rank1', 0)\n    rank5 = metrics.get('rank5', 0)\n    miou = metrics.get('miou', 0)\n\n    print(\"Rank1: {:.4f}, Rank5: {:.4f}, mIoU: {:.4f}\".format(rank1, rank5, miou))", "CodeBleu_score": 40.224280583908765}, {"code": " def print_computed_metrics(self, metrics)\ndef print_computed_metrics(self, metrics):\n    \"\"\"\n    The function prints the computed metrics for rank1, rank5, and miou.\n    It retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\n    \n    Args:\n    metrics (dict): A dictionary containing the metric values with keys 'rank1', 'rank5', and 'miou'.\n    \"\"\"\n    rank1 = metrics.get('rank1', 0)\n    rank5 = metrics.get('rank5', 0)\n    miou = metrics.get('miou', 0)\n    \n    print(f\"Rank1: {rank1:.4f}\")\n    print(f\"Rank5: {rank5:.4f}\")\n    print(f\"mIoU: {miou:.4f}\")\n\n# Example of using the function:\n# metrics_example = {'rank1': 99.1234, 'rank5': 97.5678, 'miou': 75.9876}\n# obj.print_computed_metrics(metrics_example)", "CodeBleu_score": 28.59122535357826}, {"code": "def print_computed_metrics(self, metrics):\n    \"\"\"\n    The function prints the computed metrics for rank1, rank5, and miou.\n    It retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\n\n    Args:\n    metrics (dict): A dictionary containing the metrics with keys 'rank1', 'rank5', and 'miou'\n\n    Returns:\n    None. It just prints the formatted metrics.\n    \"\"\"\n\n    # Retrieve each metric from the dictionary\n    rank1 = metrics.get('rank1', 0)  # Default to 0 if not found\n    rank5 = metrics.get('rank5', 0)  # Default to 0 if not found\n    miou = metrics.get('miou', 0)    # Default to 0 if not found\n\n    # Print each metric formatted to four decimal places\n    print(f\"Rank1: {rank1:.4f}\")\n    print(f\"Rank5: {rank5:.4f}\")\n    print(f\"MIOU: {miou:.4f}\")", "CodeBleu_score": 31.682116809429562}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data):\n    \"\"\"\n    The function evaluates prediction segments against ground truth data.\n    It computes average Intersection over Union (IoU) and rank for the top predictions.\n    For each segment-data pair, it calculates IoUs and ranks, averaging the top values for each.\n    It then calculates the percentage of segments with rank 1 and rank 5, and the mean IoU across all segments.\n    The function returns rank1, rank5, and mean IoU.\n    \n    :param segments: list of predicted segments\n    :param data: list of ground truth data\n    :return: (rank1, rank5, mean_iou)\n    \"\"\"\n    def iou(segment, truth):\n        intersection = max(0, min(segment[1], truth[1]) - max(segment[0], truth[0]))\n        union = max(segment[1], truth[1]) - min(segment[0], truth[0])\n        return intersection / union if union != 0 else 0\n\n    ranks = []\n    ious = []\n\n    for segment in segments:\n        segment_ious = [iou(segment, truth) for truth in data]\n        if segment_ious:\n            best_iou = max(segment_ious)\n            ious.append(best_iou)\n            ranked_ious = sorted(segment_ious, reverse=True)\n            rank = ranked_ious.index(best_iou) + 1\n            ranks.append(rank)\n    \n    rank1 = ranks.count(1) / len(segments) * 100 if segments else 0  # Percentage of segments with rank 1\n    rank5 = len([rank for rank in ranks if rank <= 5]) / len(segments) * 100 if segments else 0  # Percentage with rank <=5\n    mean_iou = sum(ious) / len(ious) if ious else 0\n\n    return rank1, rank5, mean_iou", "CodeBleu_score": 31.007849798222193}, {"code": "def _eval_predictions(self, segments, data):\n    \"\"\"\n    The function evaluates prediction segments against ground truth data.\n    It computes average Intersection over Union (IoU) and ranks for the top predictions.\n    For each segment-data pair, it calculates IoUs and ranks, averaging the top values for each.\n    It then calculates the percentage of segments with rank 1 and rank 5, and the mean IoU across all segments.\n    The function returns rank1, rank5, and mean IoU.\n    \n    Parameters:\n        segments (list): A list of predicted segments, where each segment is represented as a tuple or list\n                        (xmin, xmax, ymin, ymax).\n        data (list): A list of ground truth data segments, where each segment is represented in the same\n                        format as the predicted segments.\n                    \n    Returns:\n        tuple: A tuple containing rank1, rank5, and mean IoU values.\n    \"\"\"\n    \n    def calculate_iou(pred, truth):\n        \"\"\"\n        Calculate the Intersection over Union (IoU) of two segments.\n        \n        Parameters:\n            pred (tuple): A predicted segment (xmin, xmax, ymin, ymax).\n            truth (tuple): A ground truth segment (xmin, xmax, ymin, ymax).\n        \n        Returns:\n            float: The IoU score.\n        \"\"\"\n        pred_xmin, pred_xmax, pred_ymin, pred_ymax = pred\n        truth_xmin, truth_xmax, truth_ymin, truth_ymax = truth\n\n        # Compute intersection coordinates\n        inter_xmin = max(pred_xmin, truth_xmin)\n        inter_xmax = min(pred_xmax, truth_xmax)\n        inter_ymin = max(pred_ymin, truth_ymin)\n        inter_ymax = min(pred_ymax, truth_ymax)\n\n        # Compute intersection area\n        if inter_xmax > inter_xmin and inter_ymax > inter_ymin:\n            inter_area = (inter_xmax - inter_xmin) * (inter_ymax - inter_ymin)\n        else:\n            inter_area = 0\n\n        # Compute union area\n        pred_area = (pred_xmax - pred_xmin) * (pred_ymax - pred_ymin)\n        truth_area = (truth_xmax - truth_xmin) * (truth_ymax - truth_ymin)\n        union_area = pred_area + truth_area - inter_area\n\n        # Compute IoU\n        if union_area == 0:\n            return 0\n        else:\n            return inter_area / union_area\n\n    ious = []\n    rankings = []\n\n    for segment in segments:\n        segment_ious = [calculate_iou(segment, t_segment) for t_segment in data]\n        ious.extend(segment_ious)\n        segment_ious_sorted = sorted(segment_ious, reverse=True)\n        rank_1 = segment_ious_sorted[0]\n        rank_5 = segment_ious_sorted[:5]\n        rankings.append((rank_1, rank_5))\n\n    mean_iou = sum(ious) / len(ious)\n    top1_percentage = sum(1 for rank_tuple in rankings if rank_tuple[0] == max(ious)) / len(segments)\n    top5_percentage = sum(1 for rank_tuple in rankings if max(rank_tuple[1]) == max(ious)) / len(segments)\n\n    return (top1_percentage, top5_percentage, mean_iou)", "CodeBleu_score": 25.954988136451064}, {"code": "def _eval_predictions(self, segments, data)\ndef _eval_predictions(self, segments, data):\n    \"\"\"The function evaluates prediction segments against ground truth data.\n    It computes average Intersection over Union (IoU) and rank for the top predictions.\n    For each segment-data pair, it calculates IoUs and ranks, averaging the top values for each.\n    It then calculates the percentage of segments with rank 1 and rank 5, and the mean IoU across all segments.\n    The function returns rank1, rank5, and mean IoU.\n    \n    Args:\n        segments (list): A list of predicted segments, where each segment is defined with some attributes that can be compared to ground truth data.\n        data (list): A list of ground truth data segments, similarly structured to predictions.\n\n    Returns:\n        tuple: A tuple containing:\n            rank1 (float): The percentage of the top ranked prediction being in the first place.\n            rank5 (float): The percentage of the top ranked prediction being within the top five.\n            meanIoU (float): The mean Intersection over Union for rated segments.\n    \"\"\"\n    import numpy as np\n\n    def calculate_iou(segment, truth):\n        \"\"\"Calculate Intersection over Union for two segments.\"\"\"\n        # Assume each segment is a tuple or list like (start, end)\n        intersection = max(0, min(segment[1], truth[1]) - max(segment[0], truth[0]))\n        union = (segment[1] - segment[0]) + (truth[1] - truth[0]) - intersection\n        if union == 0:\n            return 0\n        return intersection / union\n\n    ious = []\n    ranks = []\n    \n    for seg in segments:\n        # Calculate IoU for the segment against all data points\n        seg_ious = [calculate_iou(seg, d) for d in data]\n        ious.append(seg_ious)\n        \n        # Determine the rank (1-based index of sorted IoUs)\n        sorted_ious_idx = np.argsort(seg_ious)[::-1]\n        ranks.append(sorted_ious_idx.tolist().index(data.index(seg)) + 1 if seg in data else len(data) + 1)\n    \n    # Calculate mean IoU\n    meanIoU = np.mean([np.max(seg_iou) for seg_iou in ious])\n    \n    # Calculate rank1 and rank5\n    rank1_count = sum(1 for rank in ranks if rank == 1)\n    rank1 = (rank1_count / len(segments)) * 100\n    \n    rank5_count = sum(1 for rank in ranks if rank <= 5)\n    rank5 = (rank5_count / len(segments)) * 100\n\n    return rank1, rank5, meanIoU\n\n# Note: This is a simple approach with a straightforward IoU positional logic that assumes exact matches between segments.\n# Real-world scenarios might need modifications to adjust how segments and ground truths are matched and indexed.", "CodeBleu_score": 30.82033375383147}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "import torch\n\ndef predict_loop(self, model, eval_dataloader, output_file=None):\n    \"\"\"\n    The function performs on-the-fly predictions using a given model and evaluation dataloader on a single GPU.\n    It initializes an empty list for scores, sets the model to evaluation mode, and transfers it to GPU 0.\n    It iterates over the dataloader, transferring data to the appropriate context, and makes predictions without computing gradients.\n    The outputs are updated with the input data and processed further.\n    Finally, it finalizes and returns the results, optionally saving them to an output file.\n    \"\"\"\n    \n    # Initialize an empty list to gather scores or results\n    results = []\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Transfer model to GPU 0\n    model.cuda(device=0)  # Ensure that a single GPU is being used.\n\n    # Iterate over the dataloader\n    for data in eval_dataloader:\n        # Transfer data to GPU 0\n        inputs = {k: v.to(device=0) for k, v in data.items() if isinstance(v, torch.Tensor)}\n\n        # Perform prediction without computing gradients\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # Update the outputs as needed before storing in results\n        # Assume `outputs` is a tensor or a dictionary of tensors\n        if isinstance(outputs, torch.Tensor):\n            results.append(outputs.cpu().numpy())  # Convert tensor to numpy and collect\n        elif isinstance(outputs, dict):  # Handling dictionary outputs\n            processed_outputs = {k: v.cpu().numpy() for k, v in outputs.items()}\n            results.append(processed_outputs)\n\n    # Finalize results\n    # Optional: Saving results to a file\n    if output_file is not None:\n        import json\n        with open(output_file, 'w') as f:\n            json.dump(results, f)\n\n    # Return the results\n    return results", "CodeBleu_score": 40.40190507424208}, {"code": "import torch\n\ndef predict_loop(self, model, eval_dataloader, output_file=None):\n    \"\"\"The function performs on-the-fly predictions using a given model and evaluation dataloader on a single GPU.\n       It initializes an empty list for scores, sets the model to evaluation mode, and transfers it to GPU 0.\n       It iterates over the dataloader, transferring data to the appropriate context, and makes predictions without computing gradients.\n       The outputs are updated with the input data and processed further.\n       Finally, it finalizes and returns the results, optionally saving them to an output file.\"\"\"\n\n    # Setting the model to evaluation mode and moving it to GPU\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model.eval()\n    model.to(device)\n\n    # Prepare a list to store the results\n    results = []\n    \n    # Iterate over the dataloader\n    with torch.no_grad():  # Disable gradient computation to speed up the process and reduce memory usage\n        for batch in eval_dataloader:\n            # Transfer the batch to GPU\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # Perform predictions\n            outputs = model(**batch)\n\n            # Assuming the outputs need to be detached from GPU and moved to CPU\n            outputs = outputs.cpu().numpy()  \n            \n            # Collect all outputs for further processing or evaluation\n            results.append(outputs)\n    \n    # Process to finalize the outputs, e.g., concatenate results, process them etc.\n    # This step will depend on what you want to achieve with the results\n    # For simplicity, we concatenate results if they are numpy arrays\n    final_results = np.concatenate(results, axis=0)\n\n    # Optionally saving the output to a file\n    if output_file is not None:\n        with open(output_file, 'wb') as file:  # 'wb' since we generally save predictions in binary format\n            np.save(file, final_results)\n\n    return final_results", "CodeBleu_score": 38.05784279304267}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None):\n    \"\"\"\n    The function performs on-the-fly predictions using a given model and evaluation dataloader on a single GPU.\n    It initializes an empty list for scores, sets the model to evaluation mode, and transfers it to GPU 0.\n    It iterates over the dataloader, transferring data to the appropriate context, and makes predictions without computing gradients.\n    The outputs are updated with the input data and processed further.\n    Finally, it finalizes and returns the results, optionally saving them to an output file.\n    \"\"\"\n    import torch\n\n    # Set the model to evaluation mode and move to GPU.\n    model.eval()\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Initialize a list to store predictions.\n    results = []\n\n    # Loop through the evaluation data loader.\n    with torch.no_grad():\n        for data in eval_dataloader:\n            # Move data to the appropriate device (GPU).\n            input_data = {k: v.to(device) for k, v in data.items()}\n\n            # Perform the prediction.\n            output = model(**input_data)\n\n            # Process the outputs if needed (e.g., applying a threshold, taking argmax, etc.)\n            processed_output = output  # Assuming output is already as desired; modify according to needs\n\n            # Store or accumulate the results.\n            results.append(processed_output)\n\n    # Optionally write results to a file.\n    if output_file:\n        with open(output_file, 'w') as f:\n            # Write the results to file, need to handle conversion from tensors and formatting accordingly\n            for result in results:\n                f.write(str(result) + '\\n')\n\n    # Return the results.\n    return results", "CodeBleu_score": 45.31901295400248}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "import numpy as np\n\ndef _aggregate_scores(self, scores):\n    \"\"\"\n    The function aggregates scores by concatenating video and text hidden representations from the input scores.\n    It asserts that there are exactly two sets of scores, concatenates each set along the first axis,\n    clears the full_scores attribute, and returns the dot product of the text and video hidden representations.\n\n    :param scores: List containing two np.ndarray items: one for video scores and one for text scores.\n    :return: np.ndarray, the dot product of the aggregated video and text scores.\n    \"\"\"\n\n    # Asserting that the number of score sets is exactly two.\n    assert len(scores) == 2, \"Expected exactly two sets of scores but got {}\".format(len(scores))\n\n    # Concatenating video and text scores along the first axis.\n    video_scores = scores[0]\n    text_scores = scores[1]\n\n    if hasattr(self, 'full_scores'):\n        self.full_scores = None  # Clearing the full_scores attribute if exists.\n\n    # Calculating the dot product of video and text representations.\n    # Note: We concatenate along the first axis assuming it aligns with how data is structured for the dot product.\n    # Here, video_scores and text_scores are expected to be matrices where columns are features.\n    result = np.dot(video_scores, text_scores.T)\n\n    return result", "CodeBleu_score": 32.90722451620974}, {"code": "def _aggregate_scores(self, scores):\n    \"\"\"\n    The function aggregates scores by concatenating video and text hidden representations from the input scores.\n    It asserts that there are exactly two sets of scores, concatenates each set along the first axis, \n    clears the full_scores attribute, and returns the dot product of the text and video hidden representations.\n\n    :param scores: A dictionary containing two keys 'video' and 'text', each associated to a list or an array of hidden representations.\n    :return: The dot product of the concatenated video and text hidden representations.\n    \"\"\"\n    \n    # Check the provided input to ensure it contains exactly two sets of scores.\n    assert 'video' in scores and 'text' in scores, \"Scores must include both 'video' and 'text' keys\"\n\n    # Retrieve video and text scores\n    video_scores = scores['video']\n    text_scores = scores['text']\n\n    # Concatenate along the first axis (for example, numpy axis=0 if these are numpy arrays)\n    import numpy as np\n    video_concatenated = np.concatenate(video_scores, axis=0)\n    text_concatenated = np.concatenate(text_scores, axis=0)\n    \n    # Assuming class attribute `full_scores` to clear (not provided in snippet but mentioned in the docstring)\n    if hasattr(self, 'full_scores'):\n        self.full_scores = None  # Clears the full_scores attribute\n    \n    # Compute the dot product of the concatenated video and text scores\n    # Assuming dot product is valid in the context, i.e., dimensions are aligned\n    dot_product = np.dot(video_concatenated, text_concatenated)\n\n    # Return the resultant dot product\n    return dot_product", "CodeBleu_score": 26.890260889352234}, {"code": "import numpy as np\n\nclass ScoreAggregator:\n    def __init__(self):\n        self.full_scores = None\n\n    def _aggregate_scores(self, scores):\n        \"\"\"\n        The function aggregates scores by concatenating video and text hidden representations from the input scores.\n        It asserts that there are exactly two sets of scores, concatenates each set along the first axis, \n        clears the full_scores attribute, and returns the dot product of the text and video hidden representations.\n        \n        Args:\n        scores (dict): Dictionary containing two sets of scores with keys 'video' and 'text', each a list or an ndarray.\n        \n        Returns:\n        float: The dot product of the aggregated text and video scores.\n        \"\"\"\n        # Check that exactly two sets of representations are given\n        assert len(scores) == 2 and 'video' in scores and 'text' in scores, \"There must be exactly two keys: 'text' and 'video'.\"\n\n        # Convert lists to numpy arrays if they aren't already\n        video_scores = np.array(scores['video'])\n        text_scores = np.array(scores['text'])\n\n        # Clear the previous full_scores attribute\n        self.full_scores = None\n\n        # Concatenate scores along the first axis\n        self.full_scores = {\n            'video': np.concatenate(video_scores, axis=0),\n            'text': np.concatenate(text_scores, axis=0)\n        }\n\n        # Compute the dot product of the text and video scores\n        return np.dot(self.full_scores['text'], self.full_scores['video'])", "CodeBleu_score": 32.18528690643885}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "import numpy as np\nimport pickle\n\ndef finalize(self, Y_pred, Y_true, output_file=None):\n    \"\"\"\n    The function finalizes predictions by concatenating and converting predicted and true labels to numpy arrays.\n    It checks for prediction errors and prints samples of these errors.\n    If an output file is specified, it saves the predictions and true labels as a pickle file.\n    The function returns a dictionary containing the predicted and true labels.\n    \n    Parameters:\n        Y_pred (list or array-like): Predicted labels.\n        Y_true (list or array-like): True labels.\n        output_file (str, optional): Path to the file where the concatenated output will be saved as a pickle.\n        \n    Returns:\n        dict: Dictionary containing 'predictions' and 'true_labels' as numpy arrays.\n    \"\"\"\n\n    # Convert Y_pred and Y_true to numpy arrays if they aren't already\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    # Check for prediction errors and print them\n    error_indices = np.where(Y_pred != Y_true)[0]\n    if len(error_indices) > 0:\n        print(f\"Prediction errors at indices: {error_indices}\")\n        for idx in error_indices[:10]:  # Print out details for the first 10 errors\n            print(f\"Index {idx}: Predicted - {Y_pred[idx]}, Actual - {Y_true[idx]}\")\n    else:\n        print(\"No prediction errors found.\")\n\n    # Prepare the results dictionary\n    results = {\n        'predictions': Y_pred,\n        'true_labels': Y_true\n    }\n\n    # If an output file is specified, save results there\n    if output_file:\n        with open(output_file, 'wb') as file:\n            pickle.dump(results, file)\n        print(f\"Results saved to {output_file}\")\n\n    return results", "CodeBleu_score": 35.65526494768415}, {"code": "import numpy as np\nimport pickle\n\ndef finalize(self, Y_pred, Y_true, output_file=None):\n    \"\"\"\n    The function finalizes predictions by concatenating and converting predicted and true labels to numpy arrays.\n    It checks for prediction errors and prints samples of these errors.\n    If an output file is specified, it saves the predictions and true labels as a pickle file.\n    The function returns a dictionary containing the predicted and true labels.\n\n    Parameters:\n        Y_pred (list or array-like): Predicted labels in the form of a list or array.\n        Y_true (list or array-like): True labels in the form of a list or array.\n        output_file (str, optional): The path to the output file where the result should be saved. If not given, no file is saved.\n\n    Returns:\n        dict: A dictionary containing numpy arrays of 'predictions' and 'true_labels'.\n    \"\"\"\n    \n    # Convert lists to numpy arrays\n    Y_pred_np = np.array(Y_pred)\n    Y_true_np = np.array(Y_true)\n    \n    # Check for errors in predictions and report some\n    errors = Y_pred_np != Y_true_np\n    error_indices = np.where(errors)[0]\n    \n    if len(error_indices) > 0:\n        print(\"Some prediction errors (sample up to 10 errors):\")\n        max_samples = min(10, len(error_indices))\n        for i in range(max_samples):\n            idx = error_indices[i]\n            print(f\"Index {idx}: Predicted = {Y_pred_np[idx]}, True = {Y_true_np[idx]}\")\n    else:\n        print(\"No prediction errors found.\")\n    \n    # Prepare the result dictionary\n    result = {\n        \"predictions\": Y_pred_np,\n        \"true_labels\": Y_true_np\n    }\n    \n    # Save to file if output path is provided\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump(result, f)\n        print(f\"Results saved to {output_file}\")\n    \n    return result", "CodeBleu_score": 38.15086221025129}, {"code": "import numpy as np\nimport pickle\n\ndef finalize(self, Y_pred, Y_true, output_file=None):\n    \"\"\"\n    The function finalizes predictions by concatenating and converting predicted and true labels to numpy arrays.\n    It checks for prediction errors and prints samples of these errors.\n    If an output file is specified, it saves the predictions and true labels as a pickle file.\n    The function returns a dictionary containing the predicted and true labels.\n    \n    Args:\n    - Y_pred (list or similar iterable): Predicted values from the model.\n    - Y_true (list or similar iterable): True labels from the dataset.\n    - output_file (str, optional): The path where the predicted and true labels should be saved as a pickle. Default is None.\n\n    Returns:\n    - dict: A dictionary with keys 'predictions' and 'true_labels', containing the numpy arrays of predictions and true labels respectively.\n    \"\"\"\n    \n    # Convert the predictions and true labels to numpy arrays\n    Y_pred_array = np.array(Y_pred)\n    Y_true_array = np.array(Y_true)\n    \n    # Calculate error samples\n    error_mask = Y_pred_array != Y_true_array\n    error_samples = np.where(error_mask)[0]\n    \n    # Print samples of prediction errors\n    if len(error_samples):\n        print(\"Sample Prediction Errors (index: predicted -> true):\")\n        for idx in error_samples[:min(10, len(error_samples))]:  # print only up to 10 errors\n            print(f\"Index {idx}: {Y_pred_array[idx]} -> {Y_true_array[idx]}\")\n    \n    # Create the result dictionary\n    result = {\n        'predictions': Y_pred_array,\n        'true_labels': Y_true_array\n    }\n    \n    # If an output file is specified, save the result dictionary\n    if output_file is not None:\n        with open(output_file, 'wb') as f:\n            pickle.dump(result, f)\n            print(f\"Results saved to {output_file}\")\n    \n    return result", "CodeBleu_score": 39.196831314956285}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs):\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n\n    Parameters:\n    logging_outputs (List[Dict[str, float]]): A list of dictionaries where each dictionary contains at least\n            'loss' and 'sample_size' keys, representing the loss and the number of samples for each logging output.\n\n    Returns:\n    None: This function directly prints the calculated average loss.\n    \"\"\"\n\n    total_loss = 0.0\n    total_samples = 0\n\n    # Sum up all the losses and sample sizes\n    for output in logging_outputs:\n        total_loss += output['loss']\n        total_samples += output['sample_size']\n\n    # Calculate the average loss\n    if total_samples != 0:\n        average_loss = total_loss / total_samples\n    else:\n        average_loss = 0  # Avoid division by zero, handle the case with no samples gracefully.\n\n    # Print the average loss, formatted to 3 decimal places\n    print(f\"Average Loss: {average_loss:.3f}\")", "CodeBleu_score": 16.498722167609387}, {"code": "def reduce_metrics(logging_outputs):\n    \"\"\"\n    The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \n    Args:\n    logging_outputs (list of dict): Each dictionary in the list contains details of logging outputs, \n                                    where each dictionary should have at least 'loss' and 'sample_size'.\n\n    Returns:\n    None: This function prints the average loss to the console.\n    \"\"\"\n    \n    # Initialize the total loss and total sample size\n    total_loss = 0.0\n    total_samples = 0\n    \n    # Loop through each logging output\n    for log in logging_outputs:\n        # Sum up the total loss and total sample size\n        if 'loss' in log and 'sample_size' in log:\n            total_loss += log['loss']\n            total_samples += log['sample_size']\n    \n    # Calculating the average loss\n    if total_samples > 0:  # avoid division by zero\n        average_loss = total_loss / total_samples\n    else:\n        average_loss = 0.0\n    \n    # Logging the average with three decimal places\n    print(f\"Average Loss: {average_loss:.3f}\")\n\n# Example usage:\nlogging_outputs = [\n    {'loss': 100, 'sample_size': 50},\n    {'loss': 150, 'sample_size': 50},\n    {'loss': 200, 'sample_size': 100}\n]\nreduce_metrics(logging_outputs)", "CodeBleu_score": 17.78426394503504}, {"code": "def reduce_metrics(logging_outputs):\n    \"\"\"The function reduces metrics by calculating the average loss from logging outputs.\n    It sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.\n    \n    Args:\n    logging_outputs (list of dict): A list of dictionaries where each dictionary contains 'loss' and 'sample_size' keys.\n    \n    Returns:\n    None: This function prints out the average loss.\"\"\"\n\n    total_loss = 0\n    total_samples = 0\n\n    # Loop through each output to sum up the losses and the sample sizes\n    for output in logging_outputs:\n        total_loss += output['loss'] * output['sample_size']\n        total_samples += output['sample_size']\n    \n    # Calculate the average loss\n    if total_samples > 0:\n        average_loss = total_loss / total_samples\n    else:\n        average_loss = 0.0  # If no samples are available\n\n    # Print the average loss rounded to three decimal places\n    print(f\"Average Loss: {average_loss:.3f}\")", "CodeBleu_score": 15.497064133864852}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "import numpy as np\n\ndef _mm_on_the_fly(self, cmasks, vmasks, attention_mask=None):\n    \"\"\"\n    The function generates an attention mask and token type IDs for multimodal inputs.\n    If an attention mask is not provided, it creates one using cmasks and vmasks.\n    It then constructs token type IDs by concatenating zeros for visual tokens and ones for text tokens.\n    The function returns the attention mask and token type IDs.\n    \n    Args:\n    cmasks (array-like): 1D array representing the mask for textual (caption) tokens.\n    vmasks (array-like): 1D array representing the mask for visual tokens.\n    attention_mask (array-like, optional): pre-existing attention mask. If None, a new one is generated.\n    \n    Returns:\n    tuple: A tuple containing the combined attention mask and the token type IDs.\n    \"\"\"\n    if attention_mask is None:\n        # Combine the masks from textual and visual inputs\n        attention_mask = np.concatenate([cmasks, vmasks])\n\n    # Create token type IDs: zeros for text (caption), ones for visual input\n    token_type_ids = np.concatenate([np.zeros_like(cmasks), np.ones_like(vmasks)])\n\n    return attention_mask, token_type_ids", "CodeBleu_score": 21.326502517934784}, {"code": "def _mm_on_the_fly(self, cmasks, vmasks, attention_mask=None):\n    \"\"\"\n    The function generates an attention mask and token type IDs for multimodal inputs.\n    If an attention mask is not provided, it creates one using cmasks and vmasks.\n    It then constructs token type IDs by concatenating zeros for visual tokens and ones for text tokens.\n    The function returns the attention mask and token type IDs.\n    \n    Args:\n    cmasks (np.array): Mask for text tokens, where 0 indicates padded tokens and 1 indicates real tokens.\n    vmasks (np.array): Mask for visual tokens, where 0 indicates padded tokens and 1 indicates real tokens.\n    attention_mask (np.array, optional): Combined mask for both text and visual inputs.\n    \n    Returns:\n    tuple: Tuple containing:\n        - Attention Mask (np.array): Combined attention mask.\n        - Token Type IDs (np.array): Token types with bins for visual and text data.\n    \"\"\"\n    \n    import numpy as np\n    \n    # Create attention mask if not provided\n    if attention_mask is None:\n        attention_mask = np.concatenate((cmasks, vmasks), axis=-1)\n    \n    # Create token type IDs\n    # Assuming visual tokens come first, followed by text tokens\n    token_type_ids_visual = np.zeros_like(vmasks)  # Visual = 0\n    token_type_ids_text = np.ones_like(cmasks)     # Text = 1\n    \n    # Combine token types\n    token_type_ids = np.concatenate((token_type_ids_visual, token_type_ids_text), axis=-1)\n    \n    return attention_mask, token_type_ids", "CodeBleu_score": 28.09302909821629}, {"code": "def _mm_on_the_fly(self, cmasks, vmasks, attention_mask=None):\n    \"\"\"\n    The function generates an attention mask and token type IDs for multimodal inputs.\n    If an attention mask is not provided, it creates one using cmasks and vmasks.\n    It then constructs token type IDs by concatenating zeros for visual tokens and ones for text tokens.\n    The function returns the attention mask and token type IDs.\n\n    Args:\n    - cmasks (np.array): Boolean or integer mask array indicating text token attentions.\n    - vmasks (np.array): Boolean or integer mask array indicating visual token attentions.\n    - attention_mask (np.array, optional): Pre-provided attention mask. If None, a new one will be computed.\n\n    Returns:\n    - np.array: the computed or provided attention mask.\n    - np.array: token type ids where '0' corresponds to text tokens and '1' corresponds to visual tokens.\n    \"\"\"\n    # Generate attention mask if not provided\n    if attention_mask is None:\n        attention_mask = np.concatenate([cmasks, vmasks], axis=1)\n\n    # Generate token type IDs\n    # Assume cmasks and vmasks have a shape that allows concatenation along axis 1\n    text_token_type_ids = np.zeros_like(cmasks, dtype=int)  # Text tokens as '0'\n    visual_token_type_ids = np.ones_like(vmasks, dtype=int)  # Visual tokens as '1'\n    token_type_ids = np.concatenate([text_token_type_ids, visual_token_type_ids], axis=1)\n\n    return attention_mask, token_type_ids", "CodeBleu_score": 27.877970257600502}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    ):\n    \"\"\"\n    The function prepares inputs for text and video generation models.\n    It adjusts the sequence length to match the combined length of input_ids and input_video_embeds.\n    It modifies the attention_mask and token_type_ids to fit the new sequence length.\n    The function returns a dictionary containing the updated input_ids, input_video_embeds, attention_mask, and token_type_ids.\n    \n    Args:\n        input_ids (Tensor): IDs of input text tokens.\n        input_video_embeds (Tensor): Corresponding video embeddings.\n        attention_mask (Optional[Tensor]): Mask to avoid performing attention on padding token indices.\n        token_type_ids (Optional[Tensor]): Segment token indices to indicate first and second parts of the inputs.\n        **model_kwargs: Additional keyword arguments for models.\n\n    Returns:\n        dict: A dictionary with the processed input data:\n              - 'input_ids': updated text input IDs\n              - 'input_video_embeds': video embeddings\n              - 'attention_mask': updated attention mask (if given)\n              - 'token_type_ids': updated token type IDs (if given)\n    \"\"\"\n\n    # First, concatenate input_ids and video embeddings along the sequence dimension\n    # Assuming input_ids and input_video_embeds dimensions mean that they can be concatenated.\n    # For instance, input_ids could be of shape (batch_size, seq_length)\n    # and input_video_embeds could be of shape (batch_size, video_seq_length, embed_dim).\n    total_sequence_length = input_ids.shape[1] + input_video_embeds.shape[1]  # adds the sequence lengths\n\n    # Update or create an attention_mask and token_type_ids if not provided\n    if attention_mask is None:\n        # If no attention mask is provided, assume all elements should be attended to\n        attention_mask = torch.ones((input_ids.shape[0], total_sequence_length))\n    else:\n        # Extend the existing attention mask for the video embeddings part\n        video_attention_mask = torch.ones((input_ids.shape[0], input_video_embeds.shape[1]))\n        attention_mask = torch.cat([attention_mask, video_attention_mask], dim=1)\n\n    if token_type_ids is None:\n        # If no token_type_ids are provided, use zeros for text and ones for video embeddings\n        text_token_type_ids = torch.zeros_like(input_ids)\n        video_token_type_ids = torch.ones((input_ids.shape[0], input_video_embeds.shape[1]))\n        token_type_ids = torch.cat([text_token_type_ids, video_token_type_ids], dim=1)\n    else:\n        # Extend the existing token_type_ids with ones for the video embeddings part\n        video_token_type_ids = torch.ones((input_ids.shape[0], input_video_embeds.shape[1]))\n        token_type_ids = torch.cat([token_type_ids, video_token_type_ids], dim=1)\n\n    # Gather all adjusted inputs into a dictionary\n    return {\n        'input_ids': input_ids,\n        'input_video_embeds': input_video_embeds,\n        'attention_mask': attention_mask,\n        'token_type_ids': token_type_ids,\n        **model_kwargs\n    }", "CodeBleu_score": 35.56660460168482}, {"code": "def prepare_inputs_for_generation(self, input_ids, input_video_embeds, attention_mask=None, token_type_ids=None, **model_kwargs):\n    \"\"\"\n    The function prepares inputs for text and video generation models.\n    It adjusts the sequence length to match the combined length of input_ids and input_video_embeds.\n    It modifies the attention_mask and token_type_ids to fit the new sequence length.\n    The function returns a dictionary containing the updated input_ids, input_video_embeds, attention_mask, and token_type_ids.\n    \n    Parameters:\n    - input_ids (torch.Tensor): Tensor of token ids to be fed to a language model.\n    - input_video_embeds (torch.Tensor): Tensor of video embeddings to be fed to a multimodal model.\n    - attention_mask (torch.Tensor, optional): Tensor indicating which elements should be attended to, and which should not.\n    - token_type_ids (torch.Tensor, optional): Tensor of segmentation ids to indicate which tokens belong to which segment of the inputs.\n    \n    Returns:\n    - dict: Dictionary containing the processed input tensors.\n    \"\"\"\n    # Initialize the process inputs dictionary\n    processed_inputs = {}\n\n    # Calculate combined sequence length\n    # Assume input_video_embed has shape (batch_size, num_frames, embed_size)\n    input_video_length = input_video_embeds.size(1)\n    total_sequence_length = input_ids.size(1) + input_video_length\n\n    # Update input_ids if needed (concatenation of zeroes or adjustements might depend on the specific model assumptions)\n    padding_length = total_sequence_length - input_ids.size(1)\n    if padding_length > 0:\n        padding = torch.zeros((input_ids.size(0), padding_length), dtype=input_ids.dtype, device=input_ids.device)\n        input_ids = torch.cat([input_ids, padding], dim=1)\n    \n    processed_inputs['input_ids'] = input_ids\n    processed_inputs['input_video_embeds'] = input_video_embeds\n\n    # Update attention_mask\n    if attention_mask is not None:\n        padding_mask = torch.zeros((attention_mask.size(0), padding_length), dtype=attention_mask.dtype, device=attention_mask.device)\n        attention_mask = torch.cat([attention_mask, padding_mask], dim=1)\n    else:\n        # Default behavior: treat all inputs as part of the attention\n        attention_mask = torch.ones((input_ids.size(0), total_sequence_length), device=input_video_embeds.device)\n    \n    processed_inputs['attention_mask'] = attention_mask\n\n    # Update token_type_ids\n    if token_type_ids is not None:\n        padding_token_types = torch.zeros((token_type_ids.size(0), padding_length), dtype=token_type_ids.dtype, device=token_type_ids.device)\n        token_type_ids = torch.cat([token_type_ids, padding_token_types], dim=1)\n    else:\n        # Default behavior might differ; here assumed all-zero tensor for simplicity\n        token_type_ids = torch.zeros((input_ids.size(0), total_sequence_length), device=input_ids.device)\n    \n    processed_inputs['token_type_ids'] = token_type_ids\n\n    # Include any additional model kwargs\n    processed_inputs.update(model_kwargs)\n\n    return processed_inputs", "CodeBleu_score": 32.194741720594486}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    ):\n    \"\"\"\n    The function prepares inputs for text and video generation models.\n    It adjusts the sequence length to match the combined length of input_ids and input_video_embeds.\n    It modifies the attention_mask and token_type_ids to fit the new sequence length.\n    The function returns a dictionary containing the updated input_ids, input_video_embeds, attention_mask, and token_type_ids.\n    \n    Parameters:\n    self: Object reference\n    input_ids (Tensor): Tensor of input ids for the text model.\n    input_video_embeds (Tensor): Tensor of video embeddings.\n    attention_mask (Tensor, optional): Tensor representing attention masks, default is None.\n    token_type_ids (Tensor, optional): Tensor representing token type ids, default is None.\n    **model_kwargs (dict): Additional keyword arguments for models.\n\n    Returns:\n    dict: A dictionary containing updated:\n        - 'input_ids' : Tensor\n        - 'input_video_embeds' : Tensor\n        - 'attention_mask' : Tensor (if provided)\n        - 'token_type_ids' : Tensor (if provided)\n        - additional model-related keyword arguments\n    \"\"\"\n\n    # Calculate new sequence length\n    text_length = input_ids.shape[1]\n    video_length = input_video_embeds.shape[1]\n    sequence_length = text_length + video_length\n\n    # Update input_ids to include video embeds' identifiers if relevant (need some method to combine both)\n    # Assuming we can concatenate them directly, but this depends on actual use case\n    new_input_ids = torch.cat((input_ids, torch.zeros((input_ids.shape[0], video_length), dtype=input_ids.dtype)), dim=1)\n\n    # Update attention_mask if provided\n    if attention_mask is None:\n        new_attention_mask = torch.ones((input_ids.shape[0], sequence_length))\n    else:\n        new_attention_mask = torch.cat((attention_mask, torch.zeros((attention_mask.shape[0], video_length))), dim=1)\n\n    # Update token_type_ids if provided\n    if token_type_ids is None:\n        new_token_type_ids = torch.cat((torch.ones_like(input_ids), 2 * torch.ones((input_ids.shape[0], video_length), dtype=input_ids.dtype)), dim=1)\n    else:\n        new_token_type_ids = torch.cat((token_type_ids, torch.zeros((token_type_ids.shape[0], video_length), dtype=token_type_ids.dtype)), dim=1)\n\n    # Prepare return dictionary\n    return_dict = {\n        'input_ids': new_input_ids,\n        'input_video_embeds': input_video_embeds,\n        'attention_mask': new_attention_mask,\n        'token_type_ids': new_token_type_ids\n    }\n    return_dict.update(model_kwargs)\n\n    return return_dict", "CodeBleu_score": 33.562576033126476}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "class Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n# For illustration, let's assume ConfigType is a dictionary, \n# where the pipeline configuration is stored under some keys.\n# Please import or define the necessary 'ConfigType' and other relevant classes or methods\n\ndef _init_pipeline(self, cfg: ConfigType) -> Compose:\n    \"\"\"\n    The function initializes the test pipeline based on the provided configuration.\n    It adjusts the pipeline configuration by removing the 'img_id' key from meta_keys if present.\n    It finds the index of the 'LoadImageFromFile' transform, raising an error if not found.\n    It updates the transform type to 'mmdet.InferencerLoader' and returns the modified pipeline configuration wrapped in a Compose object.\n    \"\"\"\n    # Step 1: Adjust the pipeline configuration to remove 'img_id' from meta_keys if present\n    for step in cfg['pipeline']:\n        if 'meta_keys' in step and 'img_id' in step['meta_keys']:\n            step['meta_keys'].remove('img_id')\n            \n    # Step 2: Find the index of the 'LoadImageFromFile'\n    load_image_index = -1\n    for index, step in enumerate(cfg['pipeline']):\n        if step['type'] == 'LoadImageFromFile':\n            load_image_index = index\n            break\n\n    if load_image_index == -1:\n        raise ValueError(\"No 'LoadImageFromFile' transformation found in the pipeline.\")\n    \n    # Step 3: Update the transform type at the found index to 'mmdet.InferencerLoader'\n    cfg['pipeline'][load_image_index]['type'] = 'mmdet.InferencerLoader'\n    \n    # Step 4: Return the modified pipeline configuration wrapped in a Compose object\n    return Compose(cfg['pipeline'])", "CodeBleu_score": 34.189022838803126}, {"code": "from typing import Any, Dict, List\n\n# Assuming these classes need to be appropriately defined or imported from a respective library\nclass Compose:\n    def __init__(self, transforms: List[Any]):\n        self.transforms = transforms\n\nclass ConfigType:\n    # Assuming pipeline configuration is accessible as a list of transform configurations\n    def __init__(self, pipeline: List[Dict[str, Any]]):\n        self.pipeline = pipeline\n\nclass InferencerLoader:\n    def __init__(self, **kwargs):\n        self.config = kwargs\n\ndef _init_pipeline(self, cfg: ConfigType) -> Compose:\n    # Find the index of 'LoadImageFromFile'\n    load_index = None\n    for i, transform in enumerate(cfg.pipeline):\n        if transform.get('type') == 'LoadImageFromFile':\n            load_index = i\n            break\n\n    # Check if 'LoadImageFromFile' transform was found\n    if load_index is None:\n        raise ValueError(\"LoadImageFromFile transform not found in the pipeline\")\n\n    # Update to 'mmdet.InferencerLoader' while keeping original arguments\n    cfg.pipeline[load_index]['type'] = 'mmdet.InferencerLoader'\n\n    # Remove 'img_id' from 'meta_keys' if it's present in any transforms\n    for transform in cfg.pipeline:\n        if 'meta_keys' in transform:\n            transform['meta_keys'] = [key for key in transform['meta_keys'] if key != 'img_id']\n\n    # Create the Compose object with updated pipeline\n    return Compose(cfg.pipeline)\n\n# Example initialization and usage\n# config = ConfigType([{'type': 'LoadImageFromFile', 'meta_keys': ['img_id', 'filename']}, {'type': 'Rescale', 'scale': (256, 256)}])\n# pipeline = _init_pipeline(config)\n# This would print the modified pipeline with InferencerLoader and updated meta_keys\n# print(pipeline.transforms)", "CodeBleu_score": 33.294921132304125}, {"code": "class Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\ndef _init_pipeline(self, cfg: ConfigType) -> Compose:\n    \"\"\"The function initializes the test pipeline based on the provided configuration.\n    It adjusts the pipeline configuration by removing the 'img_id' key from meta_keys if present.\n    It finds the index of the 'LoadImageFromFile' transform, raising an error if not found.\n    It updates the transform type to 'mmdet.InferencerLoader' and returns the modified pipeline configuration wrapped in a Compose object.\"\"\"\n    \n    # Assuming `cfg` has an attribute `pipeline` which is a list of dictionaries,\n    # where each dictionary represents a transform.\n    \n    pipeline = cfg.pipeline\n\n    # Remove 'img_id' from 'meta_keys' in the pipeline, if it exists\n    for transform in pipeline:\n        if 'meta_keys' in transform:\n            if 'img_id' in transform['meta_keys']:\n                transform['meta_keys'].remove('img_id')\n\n    # Find the index of 'LoadImageFromFile'\n    load_index = next((i for i, t in enumerate(pipeline) if t['type'] == 'LoadImageFromFile'), None)\n    if load_index is None:\n        raise ValueError(\"Transform 'LoadImageFromFile' not found in the pipeline configuration.\")\n\n    # Change type of 'LoadImageFromFile' to 'mmdet.InferencerLoader'\n    pipeline[load_index]['type'] = 'mmdet.InferencerLoader'\n\n    # Create a Compose object with the updated pipeline\n    updated_pipeline = Compose(pipeline)\n\n    return updated_pipeline", "CodeBleu_score": 41.40801095459943}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "import os\n\ndef _inputs_to_list(self, inputs: \"InputsType\") -> list:\n    \"\"\"\n    The function converts inputs to a list format.\n    If the input is a string representing a directory, it lists image files in the directory and updates the \n    input to be a list of file paths.\n    If the input is not already a list or tuple, it wraps the input in a list.\n    Finally, it returns the input as a list.\n    \"\"\"\n    # If the input is a string, it might be a directory.\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            # Add full path to each file, assuming we are listing image files with typical image file extensions.\n            inputs = [os.path.join(inputs, file) for file in os.listdir(inputs) if file.lower().endswith(('png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff'))]\n        else:\n            # If it's a single file path or a string, wrap it in a list.\n            inputs = [inputs]\n    elif isinstance(inputs, tuple):\n        # Convert tuple to list\n        inputs = list(inputs)\n    elif not isinstance(inputs, list):\n        # Wrap any other input type in a list.\n        inputs = [inputs]\n\n    return inputs", "CodeBleu_score": 44.375104086346525}, {"code": "import os\n\ndef _inputs_to_list(self, inputs: any) -> list:\n    \"\"\"The function converts inputs to a list format.\n    If the input is a string representing a directory, it lists image files in the directory and updates the input to be a list of file paths.\n    If the input is not already a list or tuple, it wraps the input in a list.\n    Finally, it returns the input as a list.\"\"\"\n\n    if isinstance(inputs, str) and os.path.isdir(inputs):\n        # It's a directory path\n        images_list = [os.path.join(inputs, filename) for filename in os.listdir(inputs)\n                       if any(filename.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.bmp', '.gif'])]\n        return images_list\n    elif not isinstance(inputs, (list, tuple)):\n        # It's a single item that is not a list or tuple, wrap it in a list\n        return [inputs]\n    else:\n        # Input is already a list or tuple, cast to list if tuple\n        return list(inputs)", "CodeBleu_score": 46.12679198943622}, {"code": "import os\n\ndef _inputs_to_list(self, inputs: InputsType) -> list:\n    \"\"\"\n    The function converts inputs to a list format.\n    If the input is a string representing a directory, it lists image files in the directory and updates \n    the input to be a list of file paths.\n    If the input is not already a list or tuple, it wraps the input in a list.\n    Finally, it returns the input as a list.\n    \"\"\"\n    if isinstance(inputs, str) and os.path.isdir(inputs):\n        # Create a list of file paths if the input is a directory that exists\n        inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n    elif not isinstance(inputs, (list, tuple)):\n        # Wrap the input in a list if it is not already a list or tuple\n        inputs = [inputs]\n    elif isinstance(inputs, tuple):\n        # Convert tuple to list\n        inputs = list(inputs)\n\n    return inputs", "CodeBleu_score": 47.24492063024046}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int):\n    \"\"\"\n    The function processes input data in chunks.\n    It iterates through the inputs, collecting a specified chunk size of data.\n    For each input, it checks if the input is a dictionary containing an 'img' or 'img_path' key and processes it with the pipeline function.\n    It yields each chunk of processed data until all inputs are processed.\n    If there are remaining inputs after the last chunk, it yields the final chunk.\n    \"\"\"\n    chunk = []\n    for item in inputs:\n        if isinstance(item, dict) and ('img' in item or 'img_path' in item):\n            # We assume 'pipeline' is a method that processes each input item\n            processed_item = self.pipeline(item)  # You need to define the pipeline method accordingly.\n            chunk.append(processed_item)\n        else:\n            continue\n\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    \n    # Yield any remaining chunk of data that is not empty\n    if chunk:\n        yield chunk", "CodeBleu_score": 26.856339119515344}, {"code": "from typing import Iterable, List, Dict, Any\n\nclass ImageProcessor:\n    def __init__(self):\n        pass\n    \n    def _get_chunk_data(self, inputs: Iterable[Dict[str, Any]], chunk_size: int):\n        \"\"\"\n        The function processes input data in chunks.\n        \n        Args:\n        inputs (Iterable[Dict[str, Any]]): An iterable collection containing dictionaries.\n        chunk_size (int): The number of inputs to process at a time.\n        \n        Yields:\n        Iterable[Iterable[Dict[str, Any]]]: Yield processed chunks of data until all inputs are processed.\n        \"\"\"\n        current_chunk = []\n        for data in inputs:\n            if \"img\" in data or \"img_path\" in data:\n                processed_input = self.pipeline(data)  # Assuming the pipeline method processes the data.\n                current_chunk.append(processed_input)\n                if len(current_chunk) == chunk_size:\n                    yield current_chunk\n                    current_chunk = []\n        \n        # After the loop, handle the last chunk if it exists.\n        if current_chunk:\n            yield current_chunk\n    \n    def pipeline(self, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        A placeholder method for an actual processing of data.\n        Process each input which contains 'img' or 'img_path' and return processed data.\n        \n        Args:\n        data (Dict[str, Any]): Dictionary that may contain 'img' or 'img_path' keys.\n\n        Returns:\n        Dict[str, Any]: Processed data structured similarly to input data.\n        \"\"\"\n        # The actual processing logic should depend on specifics of 'img' or 'img_path'.\n        # For example, this could be loading an image, performing some transformations, and returning the result.\n        # Return mock-up process data to exemplify\n        return {\"processed_data\": data}\n\n# Usage example (if needed):\n# processor = ImageProcessor()\n# inputs = [{\"img\": \"path/to/image1.jpg\"}, {\"text\": \"sample text\"}, {\"img_path\": \"path/to/image2.jpg\"}]\n# for chunk in processor._get_chunk_data(inputs, 2):\n#     print(chunk)", "CodeBleu_score": 34.55145745316735}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int):\n    \"\"\"The function processes input data in chunks.\n    It iterates through the inputs, collecting a specified chunk size of data.\n    For each input, it checks if the input is a dictionary containing an 'img' or 'img_path' key, and processes it with the pipeline function.\n    It yields each chunk of processed data until all inputs are processed.\n    If there are remaining inputs after the last chunk, it yields the final chunk.\"\"\"\n\n    chunk = []\n    for input_data in inputs:\n        # Assuming the presence of a method 'process_input' that handles input preprocessing\n        processed_input = self.process_input(input_data) if 'img' in input_data or 'img_path' in input_data else input_data\n        chunk.append(processed_input)\n\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n\n    if chunk:\n        yield chunk\n\ndef process_input(self, input_data):\n    \"\"\"Process input data based on whether it is an image itself or a path to an image.\"\"\"\n    if 'img' in input_data:\n        # hypothetical processing, e.g., performing some manipulations on the image\n        return self.process_image(input_data['img'])\n    elif 'img_path' in input_data:\n        # Load the image from the path and process it\n        image = self.load_image(input_data['img_path'])\n        return self.process_image(image)\n\ndef process_image(self, image):\n    \"\"\"Process an image (placeholder function).\"\"\"\n    # Placeholder for actual image processing logic.\n    return image\n\ndef load_image(self, path):\n    \"\"\"Load an image from a given path (placeholder function).\"\"\"\n    # Placeholder for actual image loading logic.\n    return path  # Would normally return a loaded image object.", "CodeBleu_score": 35.19871734609471}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "import numpy as np\nimport os\nimport warnings\nfrom typing import Dict, List, Optional\n\nclass PredictionPostProcessor:\n    def postprocess(\n            self,\n            preds: List[Dict],\n            visualization: Optional[List[np.ndarray]] = None,\n            return_datasamples: bool = False,\n            print_result: bool = False,\n            no_save_pred: bool = False,\n            pred_out_dir: str = '',\n            **kwargs,\n        ) -> Dict:\n        \"\"\"The function post-processes prediction results, optionally saving and printing them.\n        It initializes the output directory if predictions are not to be saved and prepares a result dictionary.\n        If return_datasamples is False, it converts predictions to dictionaries and appends them to results.\n        If pred_out_dir is not empty and return_datasamples is True, it issues a warning that saving datasamples is not supported.\n        The function adds predictions and visualization to the result dictionary, prints the results if required, and returns the result dictionary.\n        \n        Args:\n            preds (List[Dict]): List of dictionaries containing prediction results.\n            visualization (Optional[List[np.ndarray]]): List of arrays for visual representation.\n            return_datasamples (bool): Flag to determine whether datasamples should be returned.\n            print_result (bool): Flag to determine printing of results.\n            no_save_pred (bool): Flag to skip saving predictions.\n            pred_out_dir (str): Output directory to save prediction results.\n\n        Returns:\n            Dict: Dictionary containing potentially modified predictions and optionally visualization data.\n        \"\"\"\n        \n        result = {'predictions': []}\n        \n        # Initialize Output Directory\n        if not no_save_pred and pred_out_dir and not os.path.exists(pred_out_dir):\n            os.makedirs(pred_out_dir)\n        \n        # Process Predictions\n        for i, pred in enumerate(preds):\n            if not return_datasamples:\n                pred_dict = {'prediction': pred}\n                result['predictions'].append(pred_dict)\n\n                if not no_save_pred and pred_out_dir:\n                    pred_path = os.path.join(pred_out_dir, f\"pred_{i}.npy\")\n                    np.save(pred_path, np.array(pred))\n\n            if return_datasamples:\n                warnings.warn(\"Saving data samples is not supported when return_datasamples is True.\")\n\n        # Manage Visualization\n        if visualization is not None:\n            result['visualization'] = visualization\n\n        # Print Results\n        if print_result:\n            from pprint import pprint\n            pprint(result)\n        \n        return result", "CodeBleu_score": 41.23582906560112}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\ndef postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict:\n    \"\"\"\n    The function post-processes prediction results, optionally saving and printing them.\n    It initializes the output directory if predictions are not to be saved and prepares a result dictionary.\n    If return_datasamples is False, it converts predictions to dictionaries and appends them to results.\n    If pred_out_dir is not empty and return_datasamples is True, it issues a warning that saving datasamples is not supported.\n    The function adds predictions and visualization to the result dictionary, prints the results if required, and returns the result dictionary.\n    \"\"\"\n    \n    import os\n    import json\n    import warnings\n\n    # Prepare the result dictionary\n    result = {\n        \"predictions\": [],\n        \"visualization\": visualization if visualization is not None else []\n    }\n\n    # Initialize output directory\n    if not no_save_pred and pred_out_dir:\n        os.makedirs(pred_out_dir, exist_ok=True)\n\n    # Process predictions\n    if not return_datasamples:\n        # Here preds are assumed to be list-like or dict-like so they can be converted or directly appended\n        result[\"predictions\"].extend(preds if isinstance(preds, list) else [preds])\n    else:\n        # Issue warning if saving datasamples is not supported but directory provided\n        if pred_out_dir:\n            warnings.warn(\"Saving data samples to a directory is not supported.\", UserWarning)\n\n    # Save predictions\n    if not no_save_pred and pred_out_dir:\n        pred_file = os.path.join(pred_out_dir, \"predictions.json\")\n        with open(pred_file, 'w') as f:\n            json.dump(result[\"predictions\"], f)\n\n    # Print results\n    if print_result:\n        print(\"Results:\")\n        print(result)\n\n    return result", "CodeBleu_score": 36.8669020644999}, {"code": "import numpy as np\nimport os\nimport warnings\nfrom typing import List, Optional, Dict, Any\n\nclass PredictionPostProcessor:\n    def postprocess(\n            self,\n            preds: Any,  # assuming PredType is a placeholder for actual prediction type\n            visualization: Optional[List[np.ndarray]] = None,\n            return_datasamples: bool = False,\n            print_result: bool = False,\n            no_save_pred: bool = False,\n            pred_out_dir: str = '',\n            **kwargs,\n        ) -> Dict:\n        \"\"\"\n        The function post-processes prediction results, optionally saving and printing them.\n        It initializes the output directory if predictions are not to be saved and prepares a result dictionary.\n        If return_datasamples is False, it converts predictions to dictionaries and appends them to results.\n        If pred_out_dir is not empty and return_datasamples is True, it issues a warning that saving datasamples is not supported.\n        The function adds predictions and visualization to the result dictionary, prints the results if required, and returns the result dictionary.\n        \"\"\"\n        # Initialize the result dictionary\n        results = {}\n\n        # Prepare the directory for saving predictions if not empty\n        if pred_out_dir and not no_save_pred:\n            if not os.path.exists(pred_out_dir):\n                os.makedirs(pred_out_dir)\n\n        # Handle the insertion of prediction data based on 'return_datasamples'\n        if return_datasamples:\n            if pred_out_dir:\n                warnings.warn(\"Saving datasamples is not supported; received `pred_out_dir` but 'return_datasamples' is True.\")\n            results['predictions'] = preds\n        else:\n            # As an example, converting each prediction to a dictionary if not already one\n            if isinstance(preds, list) and all(not isinstance(pred, dict) for pred in preds):\n                preds = [{'prediction': pred} for pred in preds]\n            results['predictions'] = preds\n\n        # Include visualization data if provided\n        if visualization is not None:\n            results['visualizations'] = visualization\n\n        # Optionally print the results\n        if print_result:\n            print(\"Results:\", results)\n\n        # Optionally save predictions to a file\n        if not no_save_pred and pred_out_dir:\n            pred_file_path = os.path.join(pred_out_dir, 'predictions.txt')\n            with open(pred_file_path, 'w') as f:\n                f.write(str(results['predictions']))\n\n        # Return the result dictionary\n        return results\n\n# Example usage:\n# processor = PredictionPostProcessor()\n# result = processor.postprocess([1, 2, 3], print_result=True, pred_out_dir='/tmp/output')", "CodeBleu_score": 40.5629076656812}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "from typing import Dict, List, Union, Any\nimport copy\n\n# Assuming that a transformation in the pipeline might be represented as dictionary, \n# and Compose might be a type of transformation combining others together.\nConfigType = Dict[str, Any]\n\n\ndef build_test_pipeline(cfg: ConfigType) -> ConfigType:\n    \"\"\"\n    The function builds a test pipeline from a given configuration.\n    It copies the first transform in the dataset pipeline and modifies it to include only the 'Resize' transform.\n    It then copies the last transform in the pipeline and combines the modified first and copied last transforms\n    into a new test pipeline using Compose.\n    The function returns the constructed test pipeline.\n    \"\"\"\n    \n    # Deep copy the original configuration to ensure we don't modify it\n    new_cfg = copy.deepcopy(cfg)\n    \n    # Extract the pipeline\n    original_pipeline = new_cfg.get(\"pipeline\", [])\n    \n    # Assuming the pipeline is a list of transformation steps:\n    if not original_pipeline:\n        raise ValueError(\"The 'pipeline' key must contain at least one transformation.\")\n    \n    # Select and modify the first transformation\n    # Here, we assume each transformation might be a dict and looking for where type might be \"Resize\"\n    first_transformation = copy.deepcopy(original_pipeline[0])\n    if 'transforms' in first_transformation:\n        # Here, we filter only for a Resize transformation if it's a list of transforms\n        first_transformation['transforms'] = [t for t in first_transformation['transforms'] if t['type'] == 'Resize']\n    else:\n        # If not a list of transforms, directly check the type.\n        if first_transformation.get('type') != 'Resize':\n            first_transformation = {'type': 'Resize', 'size': (256, 256)}  # or whatever default we assume for Resize\n\n    # Copy the last transformation\n    last_transformation = copy.deepcopy(original_pipeline[-1])\n    \n    # Compose the new test pipeline with the two transformations\n    test_pipeline = [first_transformation, last_transformation]\n    \n    # Assign the newly constructed pipeline to the test configuration\n    new_cfg['pipeline'] = test_pipeline\n    \n    return new_cfg\n\n# Example Configuration dictionary\nexample_cfg = {\n    \"pipeline\": [\n        {\"type\": \"Load\", \"path\": \"/images\"},\n        {\"type\": \"Resize\", \"size\": (244, 244)},\n        {\"type\": \"Normalize\", \"means\": [0.485, 0.456, 0.406], \"stds\": [0.229, 0.224, 0.225]},\n        {\"type\": \"ToTensor\"}\n    ]\n}\n\n# Use the function\nupdated_cfg = build_test_pipeline(example_cfg)\nprint(updated_cfg)", "CodeBleu_score": 30.70311018672548}, {"code": "def build_test_pipeline(cfg: dict) -> dict:\n    \"\"\"\n    The function builds a test pipeline from a given configuration.\n    It copies the first transform in the dataset pipeline and modifies it to include only the 'Resize' transform.\n    It then copies the last transform in the pipeline and combines the modified first and copied last transforms into a new test pipeline using Compose.\n    The function returns the constructed test pipeline.\n    \"\"\"\n    import copy\n    \n    # The cfg dict must at least have a 'dataset' key, and 'transforms' must be a list of transforms\n    if 'dataset' not in cfg or 'transforms' not in cfg['dataset']:\n        raise ValueError(\"Configuration must include 'dataset' key with 'transforms' list\")\n\n    transformations = cfg['dataset']['transforms']\n    \n    # First, create a shallow copy of the configuration to maintain the separation of the original and the test configs\n    test_cfg = copy.deepcopy(cfg)\n    \n    # Extract and modify the first transform - We retain only the 'Resize' sub-component if it exists\n    first_transform = transformations[0]\n    resize_transform = None\n    \n    # Check if it's a Compose transform or a direct Resize transform\n    if hasattr(first_transform, 'transforms'):  # Assuming Compose object has `transforms` attribute\n        # Scan for a 'Resize' transform component if using typical definition (like torchvision.transforms)\n        for transf in first_transform.transforms:\n            if type(transf).__name__ == 'Resize':\n                resize_transform = transf\n                break\n    elif type(first_transform).__name__ == 'Resize':\n        resize_transform = first_transform\n    \n    if resize_transform is None:\n        # No Resize transform was found, cannot proceed according to the specified method\n        raise RuntimeError(\"No 'Resize' transform found in the first transform of the dataset pipeline.\")\n    \n    # Assume the last transform is suitable as-is.\n    last_transform = transformations[-1]\n    \n    # Now, build the new test pipeline using Compose (again assuming a structure similar to torchvision).\n    from torchvision.transforms import Compose\n    test_transforms = Compose([resize_transform, last_transform])\n    \n    # Replace or add the transform pipeline for testing in the copied configuration\n    test_cfg['dataset']['test_transforms'] = test_transforms\n    \n    return test_cfg", "CodeBleu_score": 31.199344335324717}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType:\n    \"\"\"\n    The function builds a test pipeline from a given configuration.\n    It copies the first transform in the dataset pipeline and modifies it to include only the 'Resize' transform.\n    It then copies the last transform in the pipeline and combines the modified first and copied last transforms into a new test pipeline using Compose.\n    The function returns the constructed test pipeline.\n    \"\"\"\n    import copy\n\n    # Copy the configuration to avoid altering the original\n    new_cfg = copy.deepcopy(cfg)\n\n    # Retrieve the dataset pipeline which contains a list of transforms\n    original_pipeline = cfg['dataset']['pipeline']\n\n    # Copy the first transform and modify it to keep only the 'Resize' operation\n    first_transform = copy.deepcopy(original_pipeline[0])\n    first_transform['transforms'] = [t for t in first_transform['transforms'] if t['type'] == 'Resize']\n\n    # Copy the last transform\n    last_transform = copy.deepcopy(original_pipeline[-1])\n\n    # Create a new pipeline with the modified first and the unaltered last transform\n    new_cfg['dataset']['pipeline'] = [first_transform, last_transform]\n\n    return new_cfg", "CodeBleu_score": 37.44271710222956}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\nimport torch\nfrom torchvision.transforms import functional as F\nfrom types import SimpleNamespace as SampleList\n\ndef inference_mot(model: torch.nn.Module, img: np.ndarray, frame_id: int, video_len: int) -> SampleList:\n    \"\"\"\n    The function performs inference on a multi-object tracking (MOT) model.\n    It prepares the input data by constructing a dictionary with image, frame ID, original shape, image ID, and video length.\n    The data is processed through a test pipeline built from the model's configuration.\n    If the model is on the CPU, it ensures no unsupported modules like RoIPool are used.\n    The function performs a forward pass of the model without gradient computation and returns the inference result.\n    \"\"\"\n    \n    # Convert the image from a NumPy array to a torch Tensor with appropriate transformation.\n    input_image = torch.from_numpy(img).float()\n    # If the image has no channel dimension (grayscale), unsqueeze to add one\n    if input_image.dim() == 2:\n        input_image = input_image.unsqueeze(0)\n    # Normalize to [0, 1] and add batch dimension ([C, H, W] -> [1, C, H, W])\n    input_image = F.to_tensor(input_image)\n    input_image = input_image.unsqueeze(0)\n\n    # Move the tensor to the same device as the model\n    device = next(model.parameters()).device\n    input_image = input_image.to(device)\n\n    # Preparing input dictionary similar to what the model expects during training/inference\n    data = {\n        \"image\": input_image,\n        \"frame_id\": frame_id,\n        \"original_shape\": img.shape[:2],\n        \"image_id\": frame_id,  # simplify by using frame_id as image_id\n        \"video_len\": video_len\n    }\n    \n    # Ensure the model is in evaluation mode\n    model.eval()\n\n    # Disable gradient calculation for inference to save memory and computation\n    with torch.no_grad():\n        # Perform the forward pass\n        output = model(data)\n\n    # Convert output to the desired format or handle it according to the needs\n    result = SampleList(output)\n\n    return result\n\n# Note: Actual attribute names in the output of model and initialization of SampleList might need adjustments\n# based on specific model architecture and definition of `SampleList`. Adjust accordingly based on implementation detail.", "CodeBleu_score": 39.14282613853706}, {"code": "import numpy as np\nimport torch\nfrom torch import nn\nfrom torchvision.transforms import Compose, ToTensor, Resize\n\nclass SampleList:\n    def __init__(self, data_dict):\n        self.data = data_dict\n\ndef inference_mot(model: nn.Module, img: np.ndarray, frame_id: int, video_len: int) -> SampleList:\n    \"\"\"\n    The function performs inference on a multi-object tracking (MOT) model.\n    It prepares the input data by constructing a dictionary with image, frame id, original shape, image ID,\n    and video length. The data is processed through a test pipeline built from the model's configuration. \n    If the model is on the CPU, it ensures no unsupported modules like RoIPool are used.\n    The function performs a forward pass of the model without gradient computation and returns the inference result.\n    \"\"\"\n    \n    # Check if any unsupported layers are used when the model is on CPU\n    if not torch.cuda.is_available():\n        # Assuming RoIPool is a custom nn.Module class that should not be used on CPU\n        for module in model.modules():\n            if isinstance(module, RoIPool):\n                raise Exception(\"RoIPool is unsupported on CPU\")\n    \n    # Image preprocessing\n    transform = Compose([\n        Resize((640, 480)),  # Resize to the input size required by the model\n        ToTensor(),  # Convert the image to tensor\n    ])\n    \n    # Apply transformations\n    img_tensor = transform(img)\n    \n    # Add a batch dimension since models expect batches\n    img_tensor = img_tensor.unsqueeze(0)\n    \n    # Construct input data dictionary\n    data_dict = {\n        \"images\": img_tensor,\n        \"frame_id\": frame_id,\n        \"original_shape\": img.shape,\n        \"image_id\": frame_id,  # Often the frame_id can be used as an image_id\n        \"video_len\": video_len\n    }\n    \n    # Wrap data in a SampleList or similar class for compatibility with the model\n    input_data = SampleList(data_dict)\n    \n    # Forward pass with no gradient computation\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():\n        output = model(input_data)\n    \n    return output", "CodeBleu_score": 37.827561639125065}, {"code": "import torch\nimport torchvision.transforms as transforms\nfrom torchvision.transforms.functional import to_tensor\nfrom torch import nn\nimport numpy as np\n\n# Assuming SampleList is a defined class or an imported one that the model expects.\nclass SampleList(dict):\n    def to(self, device):\n        for key, value in self.items():\n            if isinstance(value, torch.Tensor):\n                self[key] = value.to(device)\n        return self\n\ndef inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList:\n    \"\"\"\n    The function performs inference on a multi-object tracking (MOT) model.\n    It prepares the input data by constructing a dictionary with image, frame ID,\n    original shape, image ID, and video length. The data is processed through a\n    test pipeline built from the model's configuration. If the model is on the\n    CPU, it ensures no unsupported modules like RoIPool are used. The function\n    performs a forward pass of the model without gradient computation and\n    produces the inference result.\n    \"\"\"\n\n    # Convert the input image to a tensor and normalize it if necessary.\n    # The mean and std need to be adapted according to the expected model input.\n    transform = transforms.Compose([\n        to_tensor,\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    img_tensor = transform(img)\n\n    # Prepare the input data in the format the model expects\n    data = SampleList({\n        'image': img_tensor.unsqueeze(0),  # Add batch dimension\n        'frame_id': torch.tensor([frame_id]),\n        'original_shape': torch.tensor(img.shape[:2]),\n        'image_id': torch.tensor([frame_id]),  # Using frame_id as image_id\n        'video_len': torch.tensor([video_len])\n    })\n\n    # Ensure model is in evaluation mode and no_grad context is used\n    model.eval()\n    with torch.no_grad():\n        # If model is on the CPU, ensure no CUDA tensors are passed.\n        if next(model.parameters()).device.type == 'cpu':\n            data = data.to('cpu')\n        \n        # Perform inference\n        output = model(data)\n\n    return output\n\n# Usage example would typically include loading the model and evaluating an image,\n# but those details depend on the specific model, image read methodology and environment setup.", "CodeBleu_score": 38.677466563516475}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    import random\n\n    while len(tokens_a) + len(tokens_b) > max_num_tokens:\n        if len(tokens_a) > len(tokens_b):\n            sequence_to_trim = tokens_a\n        else:\n            sequence_to_trim = tokens_b\n\n        # Randomly choose to remove from start or end\n        if random.random() < 0.5:\n            # Remove from the beginning\n            sequence_to_trim.pop(0)\n        else:\n            # Remove from the end\n            sequence_to_trim.pop()\n    \n    return tokens_a, tokens_b\n\n# Example usage:\n# truncated_a, truncated_b = truncate_seq_pair(['hello', 'world', 'token'], ['example', 'sequence'], 5, random)\n# print(truncated_a, truncated_b)\n", "CodeBleu_score": 37.84920248130175}, {"code": "import random\n\ndef truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n    \"\"\"\n    The function truncates two sequences of tokens to ensure their combined \n    length does not exceed a specified maximum. It repeatedly removes tokens \n    from the longer sequence until the total length is within the limit. \n    Tokens are randomly removed from either the start or the end of the longer \n    sequence.\n\n    Args:\n    tokens_a (list): First list of tokens.\n    tokens_b (list): Second list of tokens.\n    max_num_tokens (int): Maximum allowed total number of tokens.\n    rng (random.Random): Instance of a random number generator.\n\n    Returns:\n    None: The modification is done in-place so it does not return anything.\n    \"\"\"\n\n    # Continue removing tokens while the total length of both token lists exceeds the maximum\n    while len(tokens_a) + len(tokens_b) > max_num_tokens:\n        # Determine which token list is longer (or choose randomly if they are the same length)\n        if len(tokens_a) > len(tokens_b):\n            longer_list = tokens_a\n        elif len(tokens_b) > len(tokens_a):\n            longer_list = tokens_b\n        else:\n            longer_list = tokens_a if rng.random() > 0.5 else tokens_b\n\n        # Randomly choose to remove from the start or the end\n        if rng.random() > 0.5:\n            longer_list.pop(0)  # Remove from start\n        else:\n            longer_list.pop()  # Remove from end", "CodeBleu_score": 37.250309677699434}, {"code": "import random\n\ndef truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n    \"\"\"\n    The function truncates two sequences of tokens to ensure their combined length does not exceed a specified maximum.\n    It repeatedly removes tokens from the longer sequence until the total length is within the limit.\n    Tokens are randomly removed from either the start or the end of the longer sequence.\n    \n    Args:\n    tokens_a (list of str): The first list of tokens to be truncated.\n    tokens_b (list of str): The second list of tokens to be truncated.\n    max_num_tokens (int): The maximum total number of tokens allowed in the truncated result.\n    rng (random.Random): An instance of a random number generator used to ensure reproducibility.\n    \n    Returns:\n    tuple: A tuple (truncated_tokens_a, truncated_tokens_b) of the truncated sequences.\n    \"\"\"\n    while len(tokens_a) + len(tokens_b) > max_num_tokens:\n        # Decide which sequence to truncate\n        if len(tokens_a) > len(tokens_b):\n            to_truncate = tokens_a\n        else:\n            to_truncate = tokens_b\n        \n        # Decide where to truncate: 0 for start, 1 for end\n        if rng.randint(0, 1) == 0:\n            # Remove from the start\n            del to_truncate[0]\n        else:\n            # Remove from the end\n            to_truncate.pop()\n    \n    return tokens_a, tokens_b\n\n# Example usage:\nif __name__ == \"__main__\":\n    tokens_a = [\"hello\", \"world\", \"good\", \"morning\"]\n    tokens_b = [\"how\", \"are\", \"you\", \"doing\", \"today\"]\n    max_num_tokens = 6\n    rng = random.Random(42)  # Seeded for reproducibility\n    truncated_a, truncated_b = truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    print(\"Truncated tokens_a:\", truncated_a)\n    print(\"Truncated tokens_b:\", truncated_b)", "CodeBleu_score": 35.45015023395836}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "import numpy as np\n\ndef get_activation(activation_string):\n    \"\"\"The function retrieves the activation function based on a given string.\n    It first checks if the input is a string and returns it directly if not.\n    If the string is empty, it returns None.\n    It then matches the string to known activation functions, returning the corresponding function or raising an error if unsupported.\n\n    Args:\n    activation_string (str): The name of the activation function.\n\n    Returns:\n    function: The corresponding activation function as a Python lambda or function.\n\n    Raises:\n    ValueError: If the activation_string does not correspond to a known activation function.\n    \"\"\"\n\n    # If the input is not a string, return it directly (as per the function doc)\n    if not isinstance(activation_string, str):\n        return activation_string\n    \n    # If the string is empty, return None\n    if activation_string == \"\":\n        return None\n    \n    # Define a mapping from strings to activation functions\n    activations = {\n        \"relu\": lambda x: np.maximum(0, x),\n        \"sigmoid\": lambda x: 1 / (1 + np.exp(-x)),\n        \"tanh\": lambda x: np.tanh(x),\n        \"softmax\": lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)  # assuming x is a vector\n    }\n    \n    # Check if the provided string matches one of the activations\n    if activation_string in activations:\n        return activations[activation_string]\n    \n    # If no valid activation is found, raise an error\n    raise ValueError(f\"Unsupported activation function: {activation_string}\")\n\n# Use the function with examples:\ntry:\n    relu = get_activation(\"relu\")\n    print(\"ReLU function example with input -3, 0, 3:\", relu([-3, 0, 3]))\n    sigmoid = get_activation(\"sigmoid\")\n    print(\"Sigmoid function example with input -1, 0, 1:\", sigmoid([-1, 0, 1]))\n    \nexcept ValueError as e:\n    print(e)", "CodeBleu_score": 38.24891819662169}, {"code": "import numpy as np\n\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef relu(x):\n    \"\"\"ReLU activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef tanh(x):\n    \"\"\"Tanh activation function.\"\"\"\n    return np.tanh(x)\n\ndef none_activation(x):\n    \"\"\"Placeholder function for no activation.\"\"\"\n    return x\n\ndef get_activation(activation_string):\n    \"\"\"\n    The function retrieves the activation function based on a given string.\n    It first checks if the input is a string and returns it directly if not.\n    If the string is empty, it returns None.\n    It then matches the string to known activation functions, returning the corresponding function \n    or raising an error if unsupported.\n    \n    Args:\n    activation_string (str): The name of the activation function.\n\n    Returns:\n    function: The corresponding activation function or None if the input is an empty string.\n    \n    Raises:\n    ValueError: If the activation_string does not correspond to a known activation function.\n    \"\"\"\n    if not isinstance(activation_string, str):\n        return activation_string\n    if activation_string == \"\":\n        return None\n\n    activations = {\n        \"sigmoid\": sigmoid,\n        \"relu\": relu,\n        \"tanh\": tanh,\n        \"none\": none_activation  # Example for no activation\n    }\n\n    if activation_string.lower() in activations:\n        return activations[activation_string.lower()]\n    else:\n        raise ValueError(f\"Unsupported activation function: {activation_string}\")", "CodeBleu_score": 37.020632416171395}, {"code": "import numpy as np\nimport math\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef get_activation(activation_string):\n    \"\"\"\n    The function retrieves the activation function based on a given string.\n    It first checks if the input is a string and returns it directly if not.\n    If the string is empty, it returns None.\n    It then matches the string to known activation functions, returning the corresponding function\n    or raising an error if unsupported.\n    \n    Args:\n    activation_string (str): A string that specifies the activation function to retrieve.\n    \n    Returns:\n    function: Corresponding Python function that implements the activation.\n    \n    Raises:\n    ValueError: If the provided string does not correspond to any known activation function.\n    \"\"\"\n    # Check the type of the input\n    if not isinstance(activation_string, str):\n        return activation_string\n\n    # Check if the string is empty\n    if activation_string == \"\":\n        return None\n\n    # Dictionary to map strings to activation function implementations\n    activations = {\n        'sigmoid': sigmoid,\n        'tanh': tanh,\n        'relu': relu,\n        'softmax': softmax\n    }\n\n    # Retrieve the activation function\n    if activation_string.lower() in activations:\n        return activations[activation_string.lower()]\n    else:\n        raise ValueError(\"Unsupported activation function: \" + activation_string)\n\n# Example usage:\n# activation_func = get_activation('relu')\n# result = activation_func(np.array([1, -2, 3]))\n# print(result)", "CodeBleu_score": 37.67945021313}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "import tensorflow as tf\n\ndef dropout(input_tensor, dropout_prob):\n    \"\"\"\n    The function applies dropout to the input tensor based on the specified dropout probability.\n    If the dropout probability is None or 0.0, it returns the input tensor unchanged.\n    If a dropout probability is provided, it applies TensorFlow's dropout operation to the input tensor.\n\n    Args:\n    input_tensor (tf.Tensor): The input tensor to which dropout will be applied.\n    dropout_prob (float or None): The probability of dropping out a unit, or None to disable dropout.\n\n    Returns:\n    tf.Tensor: The tensor after applying dropout.\n    \"\"\"\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        # Make sure we're in training mode, as dropout should only be applied then.\n        # TensorFlow 2.x automatically manages this if you correctly set up your training loop,\n        # but it's a good practice to consider this scenario.\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)", "CodeBleu_score": 53.29923118457486}, {"code": "import tensorflow as tf\n\ndef dropout(input_tensor, dropout_prob):\n    \"\"\"\n    The function applies dropout to the input tensor based on the specified dropout probability.\n    If the dropout probability is None or 0.0, it returns the input tensor unchanged.\n    If a dropout probability is provided, it applies TensorFlow's dropout operation to the input tensor.\n    \n    Args:\n    input_tensor: A Tensor which the dropout will be applied to.\n    dropout_prob: float or None, the probability of dropping out a node.\n    \n    Returns:\n    Tensor: A Tensor after applying dropout.\n    \"\"\"\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        # Ensure working in a training-like context where dropout is meaningful\n        # dropout() randomly sets the values in `input_tensor` to zero\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)", "CodeBleu_score": 53.29923118457486}, {"code": "import tensorflow as tf\n\ndef dropout(input_tensor, dropout_prob):\n    \"\"\"\n    The function applies dropout to the input tensor based on the specified dropout probability.\n    If the dropout probability is None or 0.0, it returns the input tensor unchanged.\n    If a dropout probability is provided, it applies TensorFlow's dropout operation to the input tensor.\n    \n    Parameters:\n        input_tensor (tf.Tensor): The input tensor to which dropout will be applied.\n        dropout_prob (float or None): The probability of dropping each element of the input tensor.\n\n    Returns:\n        tf.Tensor: A tensor with dropout applied if dropout_prob is not None or 0.0; otherwise, returns the original tensor.\n    \"\"\"\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        # Ensure that dropout is only applied during training, not during testing or inference.\n        # TensorFlow's dropout function requires a rate = 1 - keep_prob\n        return tf.nn.dropout(input_tensor, rate=dropout_prob)", "CodeBleu_score": 53.29923118457486}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "import tensorflow as tf\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n    \"\"\"The function creates an attention mask for a tensor based on an input mask.\n    It retrieves the shape of the input tensor and mask, reshapes the mask, and casts it to a float tensor.\n    It then creates a tensor of ones with a shape matching the from_tensor sequence length and multiplies it by the reshaped mask.\n    The function returns the resulting attention mask.\n\n    Args:\n        from_tensor (Tensor): A tensor typically of shape [batch_size, from_seq_length, ...]\n        to_mask (Tensor): A tensor typically of shape [batch_size, to_seq_length]\n\n    Returns:\n        Tensor: Attention mask of shape [batch_size, from_seq_length, to_seq_length]\n    \"\"\"\n    # Get the shape of from_tensor and to_mask\n    from_shape = from_tensor.shape.as_list()\n    to_shape = to_mask.shape.as_list()\n\n    # Retrieve `batch_size` and `from_seq_length` from `from_tensor`\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    \n    # Retrieve `to_seq_length` from `to_mask`\n    to_seq_length = to_shape[1]\n\n    # Reshape `to_mask` to [batch_size, 1, to_seq_length]. Adding dimension for broadcasting.\n    to_mask = tf.cast(tf.reshape(to_mask, [batch_size, 1, to_seq_length]), dtype=tf.float32)\n\n    # Create a tensor of ones with shape [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Multiply `broadcast_ones` by `to_mask` to create the attention mask\n    attention_mask = broadcast_ones * to_mask\n\n    return attention_mask", "CodeBleu_score": 59.90782826748898}, {"code": "import tensorflow as tf\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n    \"\"\"\n    Creates an attention mask for `from_tensor` based on `to_mask`.\n\n    Args:\n        from_tensor (tf.Tensor): Tensor of shape [batch_size, from_sequence_length, ...]\n        to_mask (tf.Tensor): Tensor of shape [batch_size, to_sequence_length]\n\n    Returns:\n        tf.Tensor: An attention mask of shape [batch_size, from_sequence_length, to_sequence_length]\n    \"\"\"\n    from_shape = tf.shape(from_tensor)  # Get dynamic shape of from_tensor\n    batch_size = from_shape[0]\n    from_sequence_length = from_shape[1]\n\n    to_shape = tf.shape(to_mask)\n    to_sequence_length = to_shape[1]\n\n    to_mask = tf.cast(to_mask, dtype=tf.float32)  # Convert to float as required\n    \n    # Expand mask to [batch_size, 1, to_sequence_length].\n    # We add a new axis to allow broadcasting when multiplying by ones.\n    to_mask = tf.expand_dims(to_mask, axis=1)\n\n    # Create a tensor of ones with shape [batch_size, from_sequence_length, 1]\n    broadcast_ones = tf.ones(shape=[batch_size, from_sequence_length, 1], dtype=tf.float32)\n\n    # Broadcast the to_mask to each position of from sequence length dimension\n    # resulting in a shape [batch_size, from_sequence_length, to_sequence_length]\n    attention_mask = broadcast_ones * to_mask\n\n    return attention_mask", "CodeBleu_score": 51.01862454874227}, {"code": "import tensorflow as tf\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n    \"\"\"\n    The function creates an attention mask for a tensor based on an input mask.\n    It retrieves the shape of the input tensor and mask, reshapes the mask,\n    and casts it to a float tensor.\n    It then creates a tensor of ones with a shape matching the from_tensor sequence length\n    and multiplies it by the reshaped mask.\n    The function returns the resulting attention mask.\n\n    Args:\n    from_tensor: tf.Tensor of shape [batch_size, from_seq_length, ...]\n    to_mask: tf.Tensor of shape [batch_size, to_seq_length]\n\n    Returns:\n    tf.Tensor of shape [batch_size, from_seq_length, to_seq_length]\n    \"\"\"\n    # from_tensor shape : [batch_size, from_seq_length, ...]\n    # to_mask shape : [batch_size, to_seq_length]\n    \n    # Cast the mask tensor to float32\n    casted_mask = tf.cast(to_mask, dtype=tf.float32)\n    \n    # Get the shapes to create the attention mask\n    from_shape = tf.shape(from_tensor)  # Get the dynamic shape\n    from_seq_length = from_shape[1]     # Index 1 is the sequence length of from_tensor\n    \n    to_shape = tf.shape(to_mask)\n    to_seq_length = to_shape[1]         # Index 1 is the sequence of to_mask\n    \n    # Extend mask shape to [batch size, 1, to_seq_length]\n    mask_extended = casted_mask[:, tf.newaxis, :]\n    \n    # Create a tensor of ones shaped [batch_size, from_seq_length, 1]\n    ones = tf.ones(shape=[from_shape[0], from_seq_length, 1], dtype=tf.float32)\n    \n    # Broadcast multiply ones with the extended mask to create the full attention mask\n    attention_mask = ones * mask_extended\n    \n    return attention_mask", "CodeBleu_score": 41.26238905044583}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, width):\n    \"\"\"\n    The function reshapes and transposes an input tensor for attention scores.\n\n    It first reshapes the input tensor to a 4D tensor with dimensions [batch_size, seq_length, num_attention_heads, width].\n    Then, it transposes the tensor to reorder the dimensions to [batch_size, num_attention_heads, seq_length, width].\n    The function returns the transposed tensor.\n\n    Args:\n    input_tensor (Tensor): The input tensor to reshape and transpose.\n    batch_size (int): The size of the batch in the input tensor.\n    num_attention_heads (int): The number of attention heads.\n    seq_length (int): The sequence length.\n    width (int): The width of each attention head feature.\n\n    Returns:\n    Tensor: The transposed tensor with dimensions [batch_size, num_attention_heads, seq_length, width].\n    \"\"\"\n    # Reshape the input tensor to the new dimensions\n    reshaped_tensor = input_tensor.view(batch_size, seq_length, num_attention_heads, width)\n    \n    # Transpose the tensor to swap the sequence length and number of attention heads dimensions\n    transposed_tensor = reshaped_tensor.permute(0, 2, 1, 3)\n    \n    return transposed_tensor", "CodeBleu_score": 38.28915322050672}, {"code": "import torch\n\ndef transpose_for_scores(input_tensor, batch_size, num_attention_heads, seq_length, width):\n    \"\"\"\n    The function reshapes and transposes an input tensor for attention scores.\n    It first reshapes the input tensor to a 4D tensor with dimensions \n    [batch_size, seq_length, num_attention_heads, width].\n    Then, it transposes the tensor to reorder the dimensions to \n    [batch_size, num_attention_heads, seq_length, width].\n    \n    Parameters:\n        input_tensor (torch.Tensor): The input tensor to be reshaped and transposed.\n        batch_size (int): The size of each batch.\n        num_attention_heads (int): The number of attention heads.\n        seq_length (int): The length of the sequence for each sample in the batch.\n        width (int): The dimensionality or width of each attention head.\n\n    Returns:\n        torch.Tensor: The reshaped and transposed tensor suitable for computing attention scores.\n    \"\"\"\n\n    # Reshape the input tensor to match the designated format: [batch_size, seq_length, num_attention_heads, width]\n    reshaped_tensor = input_tensor.view(batch_size, seq_length, num_attention_heads, width)\n\n    # Transpose the tensor to change the order of the num_attention_heads and seq_length dimensions\n    # The new order becomes: [batch_size, num_attention_heads, seq_length, width]\n    transposed_tensor = reshaped_tensor.permute(0, 2, 1, 3)\n\n    return transposed_tensor", "CodeBleu_score": 39.94478381149946}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                         seq_length, width):\n    \"\"\"\n    The function reshapes and transposes an input tensor for attention scores.\n    It first reshapes the input tensor to a 4D tensor with dimensions \n    [batch_size, seq_length, num_attention_heads, width].\n    Then, it transposes the tensor to reorder the dimensions to \n    [batch_size, num_attention_heads, seq_length, width].\n    The function returns the transposed tensor.\n    \n    :param input_tensor: Input tensor, expected to be of shape \n                         (batch_size * seq_length, num_attention_heads * width)\n    :param batch", "CodeBleu_score": 26.874339549457115}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor):\n    \"\"\"\n    The function reshapes a tensor of rank 2 or higher into a matrix (rank 2 tensor).\n    It first checks the number of dimensions of the input tensor, raising an error if it is less than 2.\n    If the tensor is already rank 2, it returns the tensor as is.\n    Otherwise, it reshapes the tensor to have a shape of [-1, width], where width is the size of the last dimension, and returns the reshaped tensor.\n    \"\"\"\n    if input_tensor.ndim < 2:\n        raise ValueError(\"Input tensor must have at least 2 dimensions\")\n    \n    if input_tensor.ndim == 2:\n        return input_tensor\n    \n    width = input_tensor.shape[-1]\n    reshaped_tensor = input_tensor.reshape(-1, width)\n    return reshaped_tensor", "CodeBleu_score": 49.675303250051485}, {"code": "import numpy as np\n\ndef reshape_to_matrix(input_tensor):\n    \"\"\"\n    The function reshapes a tensor of rank 2 or higher into a matrix (rank 2 tensor).\n    It first checks the number of dimensions of the input tensor, raising an error if it is less than 2.\n    If the tensor is already rank 2, it returns the tensor as is.\n    Otherwise, it reshapes the tensor to have a shape of [-1, width], where width is the size of the last dimension, \n    and returns the reshaped tensor.\n\n    Args:\n    input_tensor (np.ndarray): A numpy array with rank >= 2.\n\n    Returns:\n    np.ndarray: A reshaped 2D numpy array.\n    \"\"\"\n    \n    # Check the number of dimensions\n    if input_tensor.ndim < 2:\n        raise ValueError(\"Input tensor must have at least 2 dimensions.\")\n\n    # If it's already rank 2, return it\n    if input_tensor.ndim == 2:\n        return input_tensor\n\n    # Get the size of the last dimension\n    last_dim = input_tensor.shape[-1]\n\n    # Reshape to [-1, last_dim]\n    reshaped_tensor = input_tensor.reshape(-1, last_dim)\n\n    return reshaped_tensor", "CodeBleu_score": 47.70705149210268}, {"code": "import numpy as np\n\ndef reshape_to_matrix(input_tensor):\n    \"\"\"\n    The function reshapes a tensor of rank 2 or higher into a matrix (rank 2 tensor).\n    It first checks the number of dimensions of the input tensor, raising an error if it is less than 2.\n    If the tensor is already rank 2, it returns the tensor as is.\n    Otherwise, it reshapes the tensor to have a shape of [-1, width], where width is the size of the last dimension, and returns the reshaped tensor.\n    \"\"\"\n    if input_tensor.ndim < 2:\n        raise ValueError(\"Input tensor must have at least 2 dimensions\")\n    \n    if input_tensor.ndim == 2:\n        return input_tensor\n    \n    width = input_tensor.shape[-1]\n    reshaped_tensor = input_tensor.reshape(-1, width)\n    return reshaped_tensor", "CodeBleu_score": 50.18913651049528}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "import os\nimport csv\nfrom typing import List\n\nclass InputExample:\n    def __init__(self, text: str, label: str):\n        self.text = text\n        self.label = label\n\ndef get_train_examples(self, data_dir: str) -> List[InputExample]:\n    \"\"\"\n    The function retrieves training examples from a specified directory.\n    It reads a TSV file containing training data, skipping the header row.\n    For each line, it extracts the text and label, converting them to unicode.\n    If the label is \"contradictory\", it changes it to \"contradiction\".\n    It creates an InputExample object for each line, storing the examples in a list.\n    The function returns the list of training examples.\n    \"\"\"\n    \n    # Define the path to the TSV file within the data directory\n    tsv_file = os.path.join(data_dir, \"train.tsv\")\n    \n    # Initialize a list to store the training examples\n    examples = []\n    \n    # Open the TSV file for reading\n    with open(tsv_file, 'r', encoding='utf-8') as file:\n        reader = csv.reader(file, delimiter=\"\\t\")\n        \n        # Skip the header row\n        next(reader)\n        \n        # Process each row in the TSV file\n        for row in reader:\n            # Extract text and label from the row\n            text = str(row[0])  # Adjust the index as appropriate for your data\n            label = str(row[1])  # Adjust the index as appropriate for your data\n            \n            # Convert \"contradictory\" label to \"contradiction\"\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            \n            # Create an InputExample and append to the list\n            example = InputExample(text=text, label=label)\n            examples.append(example)\n\n    # Return the list of InputExample objects\n    return examples", "CodeBleu_score": 33.08580765487555}, {"code": "class InputExample:\n    def __init__(self, guid, text_a, label=None):\n        self.guid = guid\n        self.text_a = text_a\n        self.label = label", "CodeBleu_score": 5.461231605732301}, {"code": "import os\nimport csv\nfrom dataclasses import dataclass\n\n# Define a simple structure for InputExample as a dataclass for demonstration.\n@dataclass\nclass InputExample:\n    text: str\n    label: str\n\nclass DataProcessor:\n    def get_train_examples(self, data_dir):\n        \"\"\"\n        The function retrieves training examples from a specified directory.\n        It reads a TSV file containing training data, skipping the header row.\n        For each line, it extracts the text and label, converting them to unicode (handling assumed).\n        If the label is \"contradictory\", it changes it to \"contradiction\".\n        It creates an InputExample object for each line, storing the examples in a list.\n        The function returns the list of training examples.\n        \"\"\"\n        file_path = os.path.join(data_dir, \"train.tsv\")\n        examples = []\n\n        with open(file_path, 'r', encoding='utf-8') as file:\n            reader = csv.reader(file, delimiter='\\t')\n            next(reader)  # Skip header row\n            for line in reader:\n                if len(line) != 2:\n                    continue  # skip lines that don't have exactly two columns\n                text, label = line\n                if label.lower() == \"contradictory\":\n                    label = \"contradiction\"\n                examples.append(InputExample(text=text, label=label))\n\n        return examples", "CodeBleu_score": 29.10372719790467}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "import numpy as np\n\ndef metric_fn(per_example_loss, label_ids, logits, is_real_example):\n    \"\"\"\n    The function calculates evaluation metrics for a model.\n    It computes predictions by taking the argmax of the logits and calculates accuracy using the true labels\n    and a mask for real examples.\n    It also calculates the mean loss for the examples, again weighted by the real example mask.\n    The function returns a dictionary containing the evaluation accuracy and loss.\n\n    Args:\n        per_example_loss (np.array): Array of losses for each example.\n        label_ids (np.array): Array of true labels.\n        logits (np.array): Logits returned by the model for each example.\n        is_real_example (np.array): Boolean array indicating which examples should be considered for metrics.\n    \n    Returns:\n        dict: A dictionary with keys 'accuracy' and 'loss' providing the respective evaluation metrics.\n    \"\"\"\n    # Compute predictions from logits\n    predictions = np.argmax(logits, axis=-1)\n    \n    # Create mask for valid(real) examples\n    valid_mask = is_real_example.astype(bool)\n    \n    # Filter only valid examples for accuracy calculation\n    filtered_labels = label_ids[valid_mask]\n    filtered_predictions = predictions[valid_mask]\n    \n    # Calculate accuracy\n    correct_predictions = (filtered_labels == filtered_predictions)\n    accuracy = np.mean(correct_predictions.astype(np.float32))\n    \n    # Calculate masked mean loss\n    mean_loss = np.sum(per_example_loss * is_real_example) / np.sum(is_real_example)\n    \n    return {'accuracy': accuracy, 'loss': mean_loss}", "CodeBleu_score": 38.72904547212851}, {"code": "import numpy as np\n\ndef metric_fn(per_example_loss, label_ids, logits, is_real_example):\n    \"\"\"\n    The function calculates evaluation metrics for a model.\n    It computes predictions by taking the argmax of the logits and calculates accuracy using the true labels and a \n    mask for real examples.\n    It also calculates the mean loss for the examples, again weighted by the real example mask.\n    The function returns a dictionary containing the evaluation accuracy and loss.\n\n    Parameters:\n        per_example_loss (numpy.ndarray): Array of loss values per example.\n        label_ids (numpy.ndarray): Array of actual labels.\n        logits (numpy.ndarray): Logits returned by the model. Each logit is an array of scores for each class.\n        is_real_example (numpy.ndarray): Boolean array (0 or 1), where 1 indicates a real example and 0 indicates a padded or non-real example.\n\n    Returns:\n        dict: Dictionary containing 'accuracy' and 'mean_loss' as keys.\n    \"\"\"\n    # Calculate predictions by taking the argmax of logits on the last axis\n    predictions = np.argmax(logits, axis=-1)\n    \n    # Mask to focus only on real examples\n    real_example_mask = is_real_example.astype(bool)\n\n    # Calculate accuracy only for real examples\n    correct_predictions = (predictions == label_ids) & real_example_mask\n    accuracy = np.sum(correct_predictions) / np.sum(real_example_mask)\n    \n    # Calculate mean loss for real examples\n    mean_loss = np.sum(per_example_loss * real_example_mask) / np.sum(real_example_mask)\n    \n    # Return the results as a dictionary\n    results = {\n        'accuracy': accuracy,\n        'mean_loss': mean_loss\n    }\n    return results", "CodeBleu_score": 37.203049265111154}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\nimport numpy as np\n\ndef metric_fn(per_example_loss, label_ids, logits, is_real_example):\n    \"\"\"\n    The function calculates evaluation metrics for a model.\n    It computes predictions by taking the argmax of the logits and calculates accuracy using the true labels\n    and a mask for real examples. It also calculates the mean loss for the examples,\n    again weighted by the real example mask.\n    The function returns a dictionary containing the evaluation accuracy and loss.\n    \n    Args:\n    per_example_loss (np.array): Array of losses per example.\n    label_ids (np.array): Array of actual labels.\n    logits (np.array): Logits returned by the model.\n    is_real_example (np.array): Boolean array where 1 indicates a real example and 0 indicates a padded or synthetic example.\n    \n    Returns:\n    dict: A dictionary with keys 'accuracy' and 'loss' with their respective computed values.\n    \"\"\"\n\n    # Compute the predictions by taking the argmax of logits along the last dimension\n    predictions = np.argmax(logits, axis=-1)\n    \n    # Calculate the accuracy. Count only real examples.\n    correct_predictions = np.equal(predictions, label_ids)\n    mask_correct_predictions = correct_predictions * is_real_example\n    accuracy = np.sum(mask_correct_predictions) / np.sum(is_real_example)\n    \n    # Calculate the mean loss, weighted by the real example mask\n    weighted_losses = per_example_loss * is_real_example\n    mean_loss = np.sum(weighted_losses) / np.sum(is_real_example)\n    \n    return {'accuracy': accuracy, 'loss': mean_loss}\n", "CodeBleu_score": 37.09165939662009}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n    \"\"\"\n    The function converts a set of InputExamples to a list of InputFeatures.\n    It initializes an empty list to store the features and iterates over the examples.\n    For every 10,000 examples, it logs the progress.\n    For each example, it converts the example to features using the convert_single_example function and appends the result to the features list.\n    Finally, it returns the list of features.\n    \n    Args:\n    examples (list): A list of InputExample instances to be processed.\n    label_list (list): A list of possible labels for the classification task.\n    max_seq_length (int): The maximum sequence length that the tokenized input should adhere to.\n    tokenizer: An instance of a tokenizer that can convert text into tokens.\n\n    Returns:\n    list: A list of InputFeatures created from the provided InputExamples.\n    \"\"\"\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            print(f\"Writing example {ex_index} of {len(examples)}\")\n        \n        # Assuming the existence of a function `convert_single_example`\n        feature = convert_single_example(example, label_list, max_seq_length, tokenizer)\n        features.append(feature)\n\n    return features", "CodeBleu_score": 57.19190176273749}, {"code": "import logging\n\ndef mock_convert_single_example(example, label_list, max_seq_length, tokenizer):\n    # Since convert_single_example isn't defined, this is a mockup/sample function\n    # Typically, this function would transform text data to a machine-readable format, like token IDs.\n    # Prepends [CLS], appends [SEP], truncates/pads sequence to max_seq_length, converts labels, etc.\n\n    features = {\n        'input_ids': tokenizer.encode(example.text_a, max_length=max_seq_length, truncation=True),\n        'label': label_list.index(example.label) if example.label in label_list else -1\n    }\n    return features\n\ndef convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n    \"\"\"\n    The function converts a set of InputExamples to a list of InputFeatures.\n    It initializes an empty list to store the features and iterates over the examples.\n    For every 10,000 examples, it logs the progress.\n    For each example, it converts the example to features using the convert_single_example function and appends the result to the features list.\n    Finally, it returns the list of features.\n    \"\"\"\n    features = []\n    for index, example in enumerate(examples):\n        # Log the progress for every 10000 examples\n        if index % 10000 == 0:\n            logging.info(f\"Processing example {index}/{len(examples)}\")\n\n        # Convert example to feature using predefined/mocked functionality\n        feature = mock_convert_single_example(example, label_list, max_seq_length, tokenizer)\n        features.append(feature)\n    \n    return features\n\n# Set up logger for output\nlogging.basicConfig(level=logging.INFO)", "CodeBleu_score": 40.040656953621685}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n    \"\"\"\n    The function converts a set of InputExamples to a list of InputFeatures.\n    It initializes an empty list to store the features and iterates over the examples.\n    For every 10,000 examples, it logs the progress.\n    For each example, it converts the example to features using the convert_single_example function and appends the result to the features list.\n    Finally, it returns the list of features.\n\n    Args:\n    examples: A list of InputExample, which typically has attributes like `text` and `label`.\n    label_list: A list of possible labels.\n    max_seq_length: The maximum length of the sequence to be considered.\n    tokenizer: Tokenizer instance for converting text into tokens.\n\n    Returns:\n    A list of InputFeatures, where each InputFeature might typically contain input_ids, input_masks, \n    segment_ids and label_id depends on the implementation of convert_single_example.\n    \"\"\"\n    features = []\n    for index, example in enumerate(examples):\n        if index % 10000 == 0:\n            print(f\"Processed {index} examples\")\n        feature = convert_single_example(example, label_list, max_seq_length, tokenizer)\n        features.append(feature)\n    \n    return features", "CodeBleu_score": 46.860924370548986}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef get_next_sentence_output(bert_config, input_tensor, labels):\n    \"\"\"\n    The function computes the next sentence prediction loss and related outputs for a BERT model using PyTorch.\n    \n    Args:\n    - bert_config: a configuration object containing the size of the embeddings and other model parameters.\n    - input_tensor: Tensor of shape [batch_size, hidden_size], usually the output from the [CLS] token.\n    - labels: Tensor of shape [batch_size], containining the labels for next sentence prediction (0 or 1).\n    \n    Returns:\n    - mean_loss: float, average loss across the batch.\n    - per_example_loss: Tensor of shape [batch_size], containing the loss for each example.\n    - log_probs: Tensor of shape [batch_size, 2], log probabilities for NSP.\n    \"\"\"\n    \n    batch_size = input_tensor.shape[0]\n    hidden_size = bert_config.hidden_size\n    num_classes = 2  # 'next sentence' or 'not next sentence'\n    \n    # Output layer weights and bias\n    output_weights = nn.Parameter(torch.Tensor(num_classes, hidden_size))\n    output_bias = nn.Parameter(torch.Tensor(num_classes))\n    \n    # Initialize parameters\n    nn.init.xavier_normal_(output_weights)\n    nn.init.zeros_(output_bias)\n    \n    # Compute logits: input_tensor is of shape [batch_size, hidden_size]\n    logits = torch.mm(input_tensor, output_weights.t()) + output_bias\n    \n    # Log softmax to get log probabilities\n    log_probs = F.log_softmax(logits, dim=1)\n    \n    # Reshape labels to calculate loss\n    # Labels are of shape [batch_size]\n    label_one_hot = F.one_hot(labels, num_classes).to(torch.float32)\n    \n    # Cross-entropy loss as negative log probability\n    per_example_loss = -torch.sum(label_one_hot * log_probs, dim=1)\n    \n    # Mean loss across the batch\n    mean_loss = torch.mean(per_example_loss)\n    \n    return mean_loss, per_example_loss, log_probs", "CodeBleu_score": 37.33000119299164}, {"code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef get_next_sentence_output(bert_config, input_tensor, labels):\n    \"\"\"\n    The function computes the next sentence prediction loss and related outputs for a BERT model.\n    It defines the output weights and bias, computes logits through matrix multiplication and bias addition,\n    and applies a log softmax to get log probabilities.\n    Labels are reshaped and converted to one-hot encoding.\n    The per-example loss is calculated as the negative sum of the product of one-hot labels and log probabilities,\n    and the mean loss is computed.\n    The function returns the mean loss, per-example loss, and log probabilities.\n    \"\"\"\n    # Dimensions from bert_config\n    hidden_size = bert_config['hidden_size']\n    num_labels = 2  # Typically, next sentence prediction is a binary task\n\n    # Initialize weights and biases for output layer\n    output_weights = torch.randn(hidden_size, num_labels, requires_grad=True)\n    output_bias = torch.randn(num_labels, requires_grad=True)\n\n    # Compute logits\n    logits = torch.matmul(input_tensor, output_weights) + output_bias\n\n    # Apply log softmax to get log probabilities\n    log_probs = F.log_softmax(logits, dim=-1)\n\n    # Convert labels to one-hot encoding\n    one_hot_labels = F.one_hot(labels, num_classes=num_labels).type_as(log_probs)\n\n    # Calculate per-example loss\n    per_example_loss = -torch.sum(one_hot_labels * log_probs, dim=1)\n\n    # Calculate mean loss\n    mean_loss = torch.mean(per_example_loss)\n\n    return mean_loss, per_example_loss, log_probs\n\n# Example use-case (bear in mind this requires a valid bert_config and input_tensor suitable for your BERT model)\nbert_config = {'hidden_size': 768}\ninput_tensor = torch.randn((10, 768))  # batch of 10 with hidden size 768\nlabels = torch.randint(0, 2, (10,))   # random binary labels for 10 examples\n\nmean_loss, per_example_loss, log_probs = get_next_sentence_output(bert_config, input_tensor, labels)\nprint(\"Mean Loss:\", mean_loss)\nprint(\"Per Example Loss:\", per_example_loss)\nprint(\"Log Probabilities:\", log_probs)", "CodeBleu_score": 38.51901864728388}, {"code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef get_next_sentence_output(bert_config, input_tensor, labels):\n    \"\"\"\n    The function computes the next sentence prediction loss and related outputs for a BERT model.\n    It defines the output weights and bias, computes logits through matrix multiplication and bias addition,\n    and applies a log softmax to get log probabilities.\n    Labels are reshaped and converted to one-hot encoding.\n    The per-example loss is calculated as the negative sum of the product of one-hot labels and log probabilities,\n    and the mean loss is computed.\n    The function returns the mean loss, per-example loss, and log probabilities.\n\n    Parameters:\n        bert_config: An object containing configuration parameters for the BERT model.\n        input_tensor: A tensor of shape (batch_size, hidden_size) that represents the transformed output of a BERT model.\n        labels: A tensor of shape (batch_size,) containing the true labels (0 or 1) for next sentence prediction.\n\n    Returns:\n        mean_loss: A scalar tensor representing the average loss across the batch.\n        per_example_loss: A tensor of shape (batch_size,) representing the loss for each example in the batch.\n        log_probs: A tensor of shape (batch_size, 2) representing the log probabilities for each class (next sentence prediction).\n    \"\"\"\n    # Typically, the output layer weights and biases would be part of the BERT model class,\n    # Here we assume they are initialized separately for this example.\n\n    # Define output layer weights and biases\n    output_weights = nn.Parameter(torch.randn(bert_config.hidden_size, 2)).to(input_tensor.device)\n    output_bias = nn.Parameter(torch.zeros(2)).to(input_tensor.device)\n\n    # Compute logits\n    logits = torch.mm(input_tensor, output_weights) + output_bias\n\n    # Apply log softmax to get log probabilities\n    log_probs = F.log_softmax", "CodeBleu_score": 16.30862011716161}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "import tensorflow as tf\n\ndef gather_indexes(sequence_tensor, positions):\n    \"\"\"\n    The function gathers vectors at specific positions from a sequence tensor over a minibatch.\n    It retrieves the shape of the sequence tensor and computes flat offsets based on the batch size and sequence length.\n    The positions are adjusted by these offsets and flattened.\n    The sequence tensor is also flattened, and the function gathers the vectors at the specified positions from this flattened tensor.\n    The function returns the gathered vectors as the output tensor.\n    \"\"\"\n    batch_size, seq_length = tf.shape(sequence_tensor)[0], tf.shape(sequence_tensor)[1]\n    flat_offsets = tf.range(batch_size) * seq_length\n    flat_positions = flat_offsets + positions\n    flat_sequence = tf.reshape(sequence_tensor, [-1])\n    gathered = tf.gather(flat_sequence, flat_positions)\n    return gathered", "CodeBleu_score": 29.621119626891492}, {"code": "bash\npip install torch", "CodeBleu_score": 0.0}, {"code": "import numpy as np\n\ndef gather_indexes(sequence_tensor, positions):\n    \"\"\"\n    The function gathers vectors at specific positions from a sequence tensor over a minibatch.\n    It retrieves the shape of the sequence tensor and computes flat offsets based on the batch size\n    and sequence length.\n    The positions are adjusted by these offsets and flattened.\n    The sequence tensor is also flattened, and the function gathers the vectors at the specified\n    positions from this flattened tensor.\n    The function returns the gathered vectors as the output tensor.\n    \"\"\"\n    # Get the shape components\n    batch_size, seq_length, feature_size = sequence_tensor.shape\n\n    # Compute flat offsets\n    flat_offsets = np.arange(batch_size) * seq_length\n\n    # Adjust positions by offsets\n    flat_positions = positions + flat_offsets[:, np.newaxis]\n\n    # Flatten the sequence tensor\n    flat_sequence_tensor = sequence_tensor.reshape(-1, feature_size)\n\n    # Flatten the adjusted positions\n    flat_positions = flat_positions.ravel()\n\n    # Gather vectors\n    gathered_vectors = flat_sequence_tensor[flat_positions]\n\n    # Reshape gathered vectors to match the original positions array dimensions and the feature size\n    output_tensor = gathered_vectors.reshape(batch_size, -1, feature_size)\n\n    return output_tensor\n\n# Example usage\nsequence_tensor = np.random.rand(10, 20, 30)  # e.g., 10 batches, each with 20 sequences, each sequence with 30 features\npositions = np.array([\n    [1, 5, 10],\n    [2, 6, 11],\n    # ... total 10 rows, each one corresponding to a batch\n])\noutput_vectors = gather_indexes(sequence_tensor, positions)\nprint(output_vectors.shape)  # Should output (10, 3, 30), as we gather 3 positions for each batch", "CodeBleu_score": 31.959996009922182}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "import tensorflow as tf\n\ndef _decode_record(record, name_to_features):\n    \"\"\"\n    The function decodes a TensorFlow record into a TensorFlow example based on the provided feature specifications.\n    It parses the record using the name_to_features dictionary.\n    Since tf.Example only supports tf.int64 and TPUs only support tf.int32, it casts all int64 types to int32.\n    The function returns the decoded example with the necessary type conversions.\n\n    Args:\n    record: A tf.string tensor that is a serialized tf.Example.\n    name_to_features: A dictionary mapping feature names to fixed feature specifications meant to provide instructions\n                      on how to parse the features from the record.\n\n    Returns:\n    A dictionary where each key-value pair corresponds to the feature name and its associated tensor from the original record.\n    \"\"\"\n    \n    # Parse all features of the record according to the feature specification\n    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n    \n    # Convert all int64 features to int32 if necessary\n    for name, feature in example.items():\n        if feature.dtype == tf.int64:\n            example[name] = tf.cast(feature, tf.int32)\n    \n    return example", "CodeBleu_score": 40.379319741139305}, {"code": "import tensorflow as tf\n\ndef _decode_record(record, name_to_features):\n    \"\"\"\n    The function decodes a TensorFlow record into a TensorFlow example based on the provided feature specifications.\n    It parses the record using the name_to_features dictionary.\n    Since tf.Example only supports tf.int64, and TPUs only support tf.int32, it casts all int64 types to int32.\n    The function returns the decoded example with the necessary type conversions.\n    \n    Args:\n    record (tf.Tensor): A scalar string tensor, a single serialized example.\n    name_to_features (dict): A dictionary describing the features needed for the model. The keys are feature\n                             names, and the values are FixedLenFeature or VarLenFeature objects.\n\n    Returns:\n    dict: A dictionary where keys are feature names and values are tensor objects corresponding to the features\n          in the example, with int64 features cast to int32 if needed.\n    \"\"\"\n    \n    # Parse the input `record` into a TensorFlow example.\n    example = tf.io.parse_single_example(serialized=record, features=name_to_features)\n    \n    # Cast all int64 features to int32 if required as per specification\n    for feature_name in example:\n        tensor = example[feature_name]\n        if tensor.dtype == tf.int64:\n            example[feature_name] = tf.cast(tensor, tf.int32)\n    \n    return example", "CodeBleu_score": 41.43606314728385}, {"code": "import tensorflow as tf\n\ndef _decode_record(record, name_to_features):\n    \"\"\"The function decodes a TensorFlow record into a TensorFlow example based on the provided feature specifications.\n    It parses the record using the name_to_features dictionary.\n    Since tf.Example only supports tf.int64 and TPUs only support tf.int32, it casts all int64 types to int32.\n    The function returns the decoded example with the necessary type conversions.\n    \n    Args:\n        record: A single record of a TensorFlow tf.data.Dataset.\n        name_to_features: A dict that maps feature names to `tf.FixedLenFeature` or `tf.VarLenFeature` to describe\n                          each feature type in the record.\n\n    Returns:\n        A dictionary that maps feature names to Tensor values, with data types adjusted if necessary.\n    \"\"\"\n    # Parse the single record\n    parsed_example = tf.io.parse_single_example(record, features=name_to_features)\n\n    # Convert all int64 features to int32 if applicable\n    for feature_name, feature_type in name_to_features.items():\n        if tf.dtypes.as_dtype(feature_type.dtype) == tf.int64:\n            parsed_example[feature_name] = tf.cast(parsed_example[feature_name], tf.int32)\n\n    return parsed_example", "CodeBleu_score": 35.616362941848536}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):\n    \"\"\"\n    The function improves the answer span to better match the annotated answer text.\n    It tokenizes the original answer text and then iterates through possible new start and end positions within the input span.\n    For each possible span, it checks if the text matches the tokenized answer text.\n    If a match is found, it returns the new start and end positions.\n    If no match is found, it returns the original input start and end positions.\n\n    Parameters:\n    - doc_tokens (list): List of tokens (words or subwords) representing the document.\n    - input_start (int): The original start position of the answer span in `doc_tokens`.\n    - input_end (int): The original end position of the answer span in `doc_tokens`.\n    - tokenizer: Tokenizer used to convert texts to tokens to match format of `doc_tokens`.\n    - orig_answer_text (str): The actual text of the answer provided in plain text.\n\n    Returns:\n    - (int, int): Tuple containing the new or original start and end indices of the answer span in `doc_tokens`.\n    \"\"\"\n\n    # Tokenize the original answer text using the provided tokenizer\n    answer_tokens = tokenizer.tokenize(orig_answer_text)\n    \n    # Define the best match found so far\n    best_match_start, best_match_end = input_start, input_end\n    # value for the minimum length of sequence of answer tokens\n    min_tokens_len = len(answer_tokens)\n    \n    for start_index in range(input_start, input_end + 1):\n        for end_index in range(start_index + min_tokens_len - 1, input_end + 1):\n            span_tokens = doc_tokens[start_index:end_index + 1]\n\n            # To ensure correct matching, both should be strings for comparison\n            span_text = tokenizer.convert_tokens_to_string(span_tokens)\n            answer_text = tokenizer.convert_tokens_to_string(answer_tokens)\n\n            # Normalize both texts to avoid mismatches due to casing or extra spaces\n            if span_text.strip().lower() == answer_text.strip().lower():\n                best_match_start, best_match_end = start_index, end_index\n                return best_match_start, best_match_end\n\n    # Return the best matching span or the original if no better match is found\n    return best_match_start, best_match_end", "CodeBleu_score": 30.647571560137482}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):\n    \"\"\"\n    The function improves the answer span to better match the annotated answer text.\n    It tokenizes the original answer text and then iterates through possible new start and end positions within the input span.\n    For each possible span, it checks if the text matches the tokenized answer text.\n    If a match is found, it returns the new start and end positions.\n    If no match is found, it returns the original input start and end positions.\n\n    Parameters:\n        doc_tokens (list of str): Tokens from the document.\n        input_start (int): Original start position of the answer in token indices.\n        input_end (int): Original end position of the answer in token indices.\n        tokenizer (callable): Function to tokenize text.\n        orig_answer_text (str): The original text of the answer as annotated.\n\n    Returns:\n        tuple: A tuple containing two integers (new_start, new_end) representing the improved start and end positions of the answer.\n    \"\"\"\n    # Tokenize the original answer text\n    answer_tokens = tokenizer(orig_answer_text)\n\n    # Initialize best match positions to original input positions\n    best_start, best_end = input_start, input_end\n\n    # Convert answer tokens to a string for matching\n    answer_concat = ''.join(answer_tokens)\n\n    # Lower tolerance for missing matches, found_match ensures a match is found before changing span\n    found_match = False\n\n    # Iterate through all possible spans within the given range to find the best fit\n    for start in range(input_start, input_end + 1):\n        for end in range(start, input_end + 1):\n            # Generate text for current span\n            span_tokens = doc_tokens[start:end + 1]\n            span_concat = ''.join(span_tokens)\n\n            # If span matches the answer tokens, consider updating the best_start and best_end\n            if span_concat == answer_concat:\n                best_start, best_end = start, end\n                found_match = True\n                break  # Stop further checking if the exact match is found\n        if found_match:\n            break  # Stop further checking if the exact match is found\n\n    return best_start, best_end", "CodeBleu_score": 29.261924174480214}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\ndef _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):\n    \"\"\"The function improves the answer span to better match the annotated answer text.\n    It tokenizes the original answer text and then iterates through possible new start and end positions within the input span.\n    For each possible span, it checks if the text matches the tokenized answer text.\n    If a match is found, it returns the new start and end positions.\n    If no match is found, it returns the original input start and end positions.\"\"\"\n\n    # Tokenize the original answer text\n    tokenized_answer = tokenizer.tokenize(orig_answer_text)\n\n    # Get the length of the tokenized answer text\n    len_answer_tokens = len(tokenized_answer)\n\n    # Iterate over possible start positions within the input span\n    for new_start in range(input_start, input_end + 1):\n        # Set the new end based on the length of the answer tokens\n        new_end = new_start + len_answer_tokens - 1\n\n        # Check if the new end is within the bounds of the input end\n        if new_end > input_end:\n            continue\n\n        # Extract the text corresponding to the new start and end positions\n        span_tokens = doc_tokens[new_start:new_end + 1]\n\n        # Compare the extracted tokens with the tokenized answer text\n        if span_tokens == tokenized_answer:\n            return new_start, new_end\n\n    # If no matching span is found, return the original positions\n    return input_start, input_end\n\n# Example usage:\n# tokenizer = YourTokenizer (Initialize your tokenizer here depending on the library/model you are using)\n# doc_tokens = [\"This\", \"is\", \"an\", \"example\", \"of\", \"the\", \"answer\", \"span\"]\n# orig_answer_text = \"answer span\"\n# input_start = 5\n# input_end = 7\n# improved_start, improved_end = _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text)\n# print(improved_start, improved_end)  # Expected output: 6, 7 (better alignment with 'answer span')", "CodeBleu_score": 32.08589879941762}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position):\n    \"\"\"\n    The function checks if a given span has the maximum context for a token position within document spans.\n    It iterates over the document spans to find spans that include the token position.\n    For each span, it calculates a score based on the token's left and right context and the span's length.\n    It tracks the span with the highest score and compares its index to the current span index.\n    The function returns True if the current span index is the one with the highest score, indicating it has the maximum context.\n    \n    Args:\n    doc_spans (list of dicts): Each dictionary contains 'start', 'length' keys indicating span start and span length respectively.\n    cur_span_index (int): Index of the current span being considered.\n    position (int): Token position in the overall document for which max context is to be determined.\n    \n    Returns:\n    bool: True if the current span provides the maximum context for the token position.\n    \"\"\"\n\n    max_score = -1\n    best_span_index = -1\n\n    # Iterate through each span to calculate context scores\n    for i, span in enumerate(doc_spans):\n        span_start = span['start']\n        span_length = span['length']\n        span_end = span_start + span_length\n\n        if span_start <= position < span_end:\n            # Calculate left and right context sizes\n            left_context_size = position - span_start\n            right_context_size = span_end - position - 1\n\n            # Score could be decided based on the size of the context or any other sophisticated method\n            # Here, let's assume we prioritize larger contexts\n            context_score = left_context_size + right_context_size\n            \n            # Update the best score and best span index found so far\n            if context_score > max_score:\n                max_score = context_score\n                best_span_index = i\n    \n    # Check if the current span index has the maximum score\n    return best_span_index == cur_span_index", "CodeBleu_score": 30.5847568274007}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position):\n    \"\"\"\n    The function checks if a given span has the maximum context for a token position within document spans.\n    It iterates over the document spans to find spans that include the token position.\n    For each span, it calculates a score based on the token's left and right context and the span's length.\n    It tracks the span with the highest score and compares its index to the current span index.\n    The function returns True if the current span index is the one with the highest score, \n    indicating it has the maximum context.\n    \n    Args:\n        doc_spans (list of tuples): List where each tuple represents a span (start, end).\n        cur_span_index (int): Index of the current span under consideration.\n        position (int): Token position in the overall document for which context is checked.\n\n    Returns:\n        bool: True if the current span provides the maximum context for the token, else False.\n    \"\"\"\n    max_score = -1\n    max_index = -1\n    \n    # Iterate through all spans to check the context size\n    for idx, (start, end) in enumerate(doc_spans):\n        if start <= position < end:\n            left_context_size = position - start\n            right_context_size = end - position\n            # Typically, the score could consider both left and right context\n            score = left_context_size + right_context_size\n            \n            # Update if this span has a better score\n            if score > max_score:\n                max_score = score\n                max_index = idx\n\n    # Check if the current span index has the maximum context size score\n    return cur_span_index == max_index", "CodeBleu_score": 30.720722169523953}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position):\n    \"\"\"The function checks if a given span has the maximum context for a token position within document spans.\n    It iterates over the document spans to find spans that include the token position.\n    For each span, it calculates a score based on the token's left and right context and the span's length.\n    It tracks the span with the highest score and compares its index to the current span index.\n    The function returns True if the current span index is the one with the highest score, indicating it has the maximum context.\n    \"\"\"\n    \n    max_score = -1\n    max_idx = None\n    \n    # Iterate through all spans to find the one with the maximum context for the given position\n    for idx, span in enumerate(doc_spans):\n        start = span['start']\n        length = span['length']\n        end = start + length\n        \n        # Check if the position is inside this span\n        if start <= position < end:\n            # Calculate score for this span (we use 'end - position' for right context, 'position - start' for left context\n            left_context = position - start\n            right_context = end - position\n            score = left_context + right_context\n            \n            # Keep track of the span with the maximum score\n            if score > max_score:\n                max_score = score\n                max_idx = idx\n    \n    # Return True if the current span index has the maximum score\n    return max_idx == cur_span_index", "CodeBleu_score": 32.96881233372355}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "from collections import OrderedDict\n\ndef load_vocab(vocab_file):\n    \"\"\"\n    The function loads a vocabulary file into an ordered dictionary.\n    It initializes an ordered dictionary and an index counter.\n    It reads the vocabulary file line by line, converts each line to Unicode, strips any whitespace, \n    and adds the token to the dictionary with its corresponding index.\n    The function increments the index for each token and returns the populated dictionary.\n    \n    Parameters:\n        vocab_file (str): The path to the vocabulary file.\n    \n    Returns:\n        OrderedDict: An ordered dictionary where keys are tokens from the file and values are indices.\n    \"\"\"\n    vocab_dict = OrderedDict()\n    index = 0\n    with open(vocab_file, 'r', encoding='utf-8') as file:\n        for line in file:\n            token = line.strip()\n            vocab_dict[token] = index\n            index += 1\n    return vocab_dict", "CodeBleu_score": 41.65169379195892}, {"code": "from collections import OrderedDict\n\ndef load_vocab(vocab_file):\n    \"\"\"\n    The function loads a vocabulary file into an ordered dictionary.\n    It initializes an ordered dictionary and an index counter.\n    It reads the vocabulary file line by line, converts each line to Unicode, strips any whitespace, \n    and adds the token to the dictionary with its corresponding index.\n    The function increments the index for each token and returns the populated dictionary.\n\n    Args:\n    vocab_file (str): The path to the vocabulary file.\n\n    Returns:\n    OrderedDict: An ordered dictionary with each line from the vocab file as keys and their\n                 corresponding line number as values.\n    \"\"\"\n    vocab_dict = OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            # Convert line to Unicode, strip whitespace\n            token = line.strip()\n            # Add token to the dictionary with the current index\n            vocab_dict[token] = index\n            index += 1  # Increment the index for the next token\n    return vocab_dict", "CodeBleu_score": 40.06725941914928}, {"code": "def load_vocab(vocab_file)\nfrom collections import OrderedDict\n\ndef load_vocab(vocab_file):\n    \"\"\"\n    The function loads a vocabulary file into an ordered dictionary.\n    It initializes an ordered dictionary and an index counter.\n    It reads the vocabulary file line by line, converts each line to a string if needed,\n    strips any whitespace, and adds the token to the dictionary with its corresponding index.\n    The function increments the index for each token and returns the populated dictionary.\n\n    Args:\n    vocab_file (str): The path to the vocabulary file.\n\n    Returns:\n    OrderedDict: An ordered dictionary with tokens as keys and their corresponding indices as values.\n    \"\"\"\n    # Initialize the ordered dictionary and the index counter\n    vocab_dict = OrderedDict()\n    index = 0\n    \n    # Open and read the vocab file\n    with open(vocab_file, 'r', encoding='utf-8') as file:\n        for line in file:\n            # Convert line to Unicode, strip whitespace\n            token = line.strip()\n            # Add the token to the dictionary with the index\n            vocab_dict[token] = index\n            # Increment the index for the next token\n            index += 1\n\n    # Return the populated dictionary\n    return vocab_dict\n\n# Example usage:\n# vocab = load_vocab('path_to_vocab.txt')", "CodeBleu_score": 43.93356685155448}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items):\n    \"\"\"\n    The function converts a sequence of tokens or IDs using a given vocabulary.\n    It iterates over each item in the input sequence and appends the corresponding vocabulary value to the output list.\n    The function returns the converted sequence as a list.\n\n    Parameters:\n    - vocab (dict): A dictionary mapping from tokens or IDs (string or integer) to their desired transformation.\n    - items (list): A list containing the tokens or IDs to be converted.\n\n    Returns:\n    - list: A list containing the converted values of the input items according to the vocabulary.\n    \"\"\"\n    # Initialize an empty list to store the converted items\n    converted_items = []\n\n    # Iterate over each item in the input sequence\n    for item in items:\n        # Append the corresponding vocabulary value to the converted list\n        # Check if the item exists in the vocabulary to avoid KeyError\n        if item in vocab:\n            converted_items.append(vocab[item])\n        else:\n            # Handle the case where the item is not found in the vocabulary\n            # An alternative might be to throw an error or append a placeholder\n            converted_items.append(None)  # Or `converted_items.append(item)` to keep original item\n\n    return converted_items", "CodeBleu_score": 62.23249929158323}, {"code": "def convert_by_vocab(vocab, items):\n    \"\"\"The function converts a sequence of tokens or IDs using a given vocabulary. \n    It iterates over each item in the input sequence and appends the corresponding vocabulary value \n    to the output list. The function returns the converted sequence as a list.\n    \n    Args:\n    vocab (dict): A dictionary mapping tokens or IDs to their corresponding values.\n    items (list): A list of tokens or IDs to be converted using the vocabulary.\n\n    Returns:\n    list: A list of values corresponding to the input tokens or IDs, as mapped by the vocabulary.\n    \"\"\"\n    converted_items = []\n    for item in items:\n        # Retrieve the item from vocab and append to converted_items, handle missing items if needed\n        if item in vocab:\n            converted_items.append(vocab[item])\n        else:\n            # Optionally handle the case where the item is not found in the vocabulary\n            # For example, append None or an error sentinel value, or just continue\n            converted_items.append(None)\n    return converted_items", "CodeBleu_score": 63.95130440720167}, {"code": "def convert_by_vocab(vocab, items):\n    \"\"\"The function converts a sequence of tokens or IDs using a given vocabulary.\n    It iterates over each item in the input sequence and appends the corresponding \n    vocabulary value to the output list.\n    The function returns the converted sequence as a list.\n\n    Args:\n    vocab (dict): A dictionary mapping tokens or IDs to other tokens, IDs, or values.\n    items (list): A list of tokens or IDs to be converted using `vocab`.\n\n    Returns:\n    list: A list of converted values corresponding to each item in the input based on the `vocab`.\n    \"\"\"\n    output_list = []\n    for item in items:\n        if item in vocab:\n            output_list.append(vocab[item])\n        else:\n            # Optionally handle or log cases where the item is not found in the vocabulary\n            # For this placeholder, I append `None` or you might just skip it\n            output_list.append(None)\n    return output_list", "CodeBleu_score": 66.85799023914004}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "class Tokenizer:\n    def __init__(self):\n        # Simple simulation of a wordpiece tokenizer with predefined subwords.\n        self.wordpiece_vocab = {\n            'un': 'un', 'happy': 'happy', 'ness': 'ness', \n            'fortu': 'fortu', 'nate': 'nate', 'ly': 'ly',\n            'pre': 'pre', 'fix': 'fix', 'es': 'es'\n        }\n    \n    def basic_tokenize(self, text):\n        \"\"\"Tokenize the input text into a list of tokens based on whitespace.\"\"\"\n        return text.split()\n\n    def wordpiece_tokenize(self, token):\n        \"\"\"Further tokenize each token using a simulated wordpiece tokenizer.\"\"\"\n        sub_tokens = []\n        while len(token) > 0:\n            matched = False\n            # Check longer substrings first to match larger subwords\n            for subword_length in range(len(token), 0, -1):\n                subword = token[:subword_length]\n                if subword in self.wordpiece_vocab:\n                    sub_tokens.append(self.wordpiece_vocab[subword])\n                    token = token[subword_length:]\n                    matched = True\n                    break\n            # If no subword found, we use an 'unknown' token for the remaining part (or indicate a better handling)\n            if not matched:\n                sub_tokens.append('[UNK]')\n                break\n        return sub_tokens\n\n    def tokenize(self, text):\n        \"\"\"Tokenize the text using basic tokenizer, then each token using a wordpiece tokenizer.\"\"\"\n        tokens = self.basic_tokenize(text)\n        sub_tokens = []\n        for token in tokens:\n            word_pieces = self.wordpiece_tokenize(token)\n            sub_tokens.extend(word_pieces)\n        return sub_tokens\n\n# Example usage\ntokenizer = Tokenizer()\ntext = \"unhappiness unfortunately prefix\"\nprint(tokenizer.tokenize(text))", "CodeBleu_score": 33.82443579679412}, {"code": "from transformers import BertTokenizer\n\nclass TextTokenizer:\n    def __init__(self):\n        # Initialize a BERT tokenizer which uses WordPiece internally\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n    def tokenize(self, text):\n        \"\"\"The function tokenizes a given text by first using a basic tokenizer to split the text into tokens.\n        It then further tokenizes each token using a wordpiece tokenizer, appending the resulting sub-tokens to a list.\n        Finally, it returns the list of all sub-tokens.\n        \n        Args:\n        text (str): The text to be tokenized.\n\n        Returns:\n        list: A list containing the sub-tokens.\n        \"\"\"\n        # Using the BERT tokenizer to tokenize and then wordpiece tokenize\n        tokens = self.tokenizer.tokenize(text)\n        return tokens\n\n# Example usage:\nif __name__ == \"__main__\":\n    text = \"Example sentence for tokenization.\"\n    tokenizer = TextTokenizer()\n    sub_tokens = tokenizer.tokenize(text)\n    print(sub_tokens)", "CodeBleu_score": 30.325840202221304}, {"code": "class TextTokenizer:\n    def __init__(self):\n        # Typically you would load a WordPiece model or vocabulary here\n        self.vocab = {\n            'hello': ['hel', '##lo'],\n            'world': ['wor', '##ld'],\n            # You can add more known tokens and their sub-token forms\n            # For unknown forms, we can handle it in the tokenize method\n        }\n        \n    def basic_tokenize(self, text):\n        \"\"\" Simple whitespace tokenizer.\"\"\"\n        return text.strip().split()\n\n    def wordpiece_tokenize(self, token):\n        \"\"\" Simulate a wordpiece tokenizer based on a predefined vocabulary.\"\"\"\n        if token in self.vocab:\n            return self.vocab[token]\n        else:\n            # Example of a naive token splitting for unknown words\n            return [token[:len(token)//2], '##'+token[len(token)//2:]] if len(token) > 2 else [token]\n\n    def tokenize(self, text):\n        \"\"\"\n        The function tokenizes a given text by first using a basic tokenizer to split the text into tokens.\n        It then further tokenizes each token using a wordpiece tokenizer, appending the resulting sub-tokens to a list.\n        Finally, it returns the list of all sub-tokens.\n        \"\"\"\n        tokens = self.basic_tokenize(text)\n        sub_tokens = []\n        for token in tokens:\n            sub_tokens.extend(self.wordpiece_tokenize(token))\n        return sub_tokens\n\n# Example usage:\ntokenizer = TextTokenizer()\ntext = \"hello world this is a test\"\nprint(tokenizer.tokenize(text))", "CodeBleu_score": 37.582527910548535}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "import librosa\nimport numpy as np\nimport logging\n\ndef safe_load(path, offset, duration, sample_rate, dtype='float32'):\n    \"\"\"\n    The function safely loads audio data from a specified file path within a given time range.\n    It logs the loading process and attempts to load the audio data using provided parameters.\n    If successful, it returns the audio data and a False flag indicating no error.\n    If an exception occurs, it logs the error and returns a default value of -1.0 with a True flag indicating an error.\n    \n    Parameters:\n        path (str): File path of the audio file.\n        offset (float): Start time in seconds from which to start loading the audio.\n        duration (float): Duration in seconds for which to load the audio.\n        sample_rate (int): The target sample rate for loading the audio.\n        dtype (str): The data type of the loaded audio array, 'float32' by default.\n    \n    Returns:\n        tuple:\n            np.ndarray: The loaded audio data or -1.0 if an error occurred.\n            bool: False if loading succeeded, True otherwise.\n    \"\"\"\n    \n    logging.basicConfig(level=logging.INFO)\n    \n    try:\n        # Load the audio file with the specified parameters\n        audio_data, _ = librosa.load(path, sr=sample_rate, dtype=dtype, offset=offset, duration=duration)\n        \n        logging.info(f\"Audio loaded successfully from {path} (offset={offset}s, duration={duration}s, sample_rate={sample_rate}Hz)\")\n        \n        return audio_data, False\n    except Exception as e:\n        logging.error(f\"Failed to load audio from {path}: {str(e)}\")\n        \n        # Return a default error of -1.0 and True to indicate failure\n        return np.array([-1.0], dtype=dtype), True", "CodeBleu_score": 35.93361302496103}, {"code": "bash\npip install librosa", "CodeBleu_score": 0.0}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\nimport logging\nimport soundfile as sf\n\ndef safe_load(path, offset, duration, sample_rate, dtype):\n    \"\"\"\n    The function safely loads audio data from a specified file path within a given time range.\n    It logs the loading process and attempts to load the audio data using provided parameters.\n    If successful, it returns the audio data and a False flag indicating no error.\n    If an exception occurs, it logs the error and returns a default value of -1.0 with a True flag indicating an error.\n\n    Parameters:\n        path (str): Path to the audio file.\n        offset (float): Starting position in seconds from where the audio file is to be read.\n        duration (float): Duration in seconds to read from the audio file.\n        sample_rate (int): Sample rate to which the audio needs to be resampled.\n        dtype (str): Data type of the returned audio array.\n\n    Returns:\n        tuple (numpy.ndarray, bool): (audio_data, error_flag)\n    \"\"\"\n    logging.basicConfig(level=logging.INFO)\n\n    try:\n        # Load the data\n        data, sr = sf.read(path, start=int(offset * sr), frames=int(duration * sr), dtype=dtype)\n\n        # Resample data if needed\n        if sr != sample_rate:\n            import resampy\n            data = resampy.resample(data, sr, sample_rate, filter='kaiser_fast')\n\n        logging.info(f\"Audio data loaded successfully: {path}\")\n        return data, False\n    except Exception as e:\n        logging.error(f\"Failed to load audio from {path}. Error: {str(e)}\")\n        return -1.0, True", "CodeBleu_score": 37.27964179919634}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "import tensorflow as tf\n\ndef to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor:\n    \"\"\"\n    The function adjusts the number of channels in a waveform tensor to the specified number of channels.\n    If the waveform has more or equal channels, it trims to the desired number.\n    If it has fewer channels, it tiles the waveform to increase the channels and then trims it.\n    The function returns the adjusted waveform tensor.\n    \n    Args:\n    waveform (tf.Tensor): A tensor representing the audio waveform with shape [batch_size, ..., samples, channels].\n    n_channels (int): The desired number of output channels.\n\n    Returns:\n    tf.Tensor: The waveform tensor adjusted to the specified number of channels.\n    \"\"\"\n\n    current_channels = waveform.shape[-1]\n\n    if current_channels < n_channels:\n        # Tile the channels to meet or exceed the desired number.\n        tile_multiples = [1] * (len(waveform.shape) - 1) + [n_channels // current_channels + (n_channels % current_channels > 0)]\n        waveform = tf.tile(waveform, tile_multiples)\n\n        # Trim extra tiled channels if necessary.\n        waveform = waveform[..., :n_channels]\n    elif current_channels > n_channels:\n        # Trim the channels to meet the desired count.\n        waveform = waveform[..., :n_channels]\n\n    return waveform", "CodeBleu_score": 35.0185930352857}, {"code": "import tensorflow as tf\n\ndef to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor:\n    \"\"\"\n    The function adjusts the number of channels in a waveform tensor to the specified number of channels.\n    If the waveform has more or equal channels, it trims to the desired number.\n    If it has fewer channels, it tiles the waveform to increase the channels and then trims it.\n    The function returns the adjusted waveform tensor.\n    \n    Parameters:\n    - waveform: tf.Tensor - Input waveform tensor of shape [batch_size, num_samples, num_channels]\n    - n_channels: int - Desired number of channels\n    \n    Returns:\n    - tf.Tensor - Waveform tensor of shape [batch_size, num_samples, n_channels]\n    \"\"\"\n    \n    # Get the current number of channels\n    current_channels = waveform.shape[-1]\n    \n    if current_channels == n_channels:\n        # The current number of channels matches the desired number of channels\n        return waveform\n    elif current_channels > n_channels:\n        # Reduce the number of channels by slicing\n        return waveform[..., :n_channels]\n    else:\n        # Increase the number of channels by tiling\n        replication_factor = -(-n_channels // current_channels)  # calculate the ceil of n_channels / current_channels\n        tiled_waveform = tf.tile(waveform, [1, 1, replication_factor])\n        # Trim to the desired number of channels\n        return tiled_waveform[..., :n_channels]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Creating a dummy waveform with 1 channel\n    sample_waveform = tf.random.normal([10, 100, 1])\n    \n    # Adjusting to 4 channels\n    adjusted_waveform = to_n_channels(sample_waveform, 4)\n    print(\"Shape adjusted wavefrom (should be [10, 100, 4]):\", adjusted_waveform.shape)", "CodeBleu_score": 32.95937520644413}, {"code": "import tensorflow as tf\n\ndef to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor:\n    \"\"\"\n    The function adjusts the number of channels in a waveform tensor to the specified number of channels.\n    If the waveform has more or equal channels, it trims to the desired number.\n    If it has fewer channels, it tiles the waveform to increase the channels and then trims it.\n    The function returns the adjusted waveform tensor.\n    \n    Args:\n    waveform (tf.Tensor): A tensor representing the waveform, assumed to be of shape [batch_size, samples, channels].\n    n_channels (int): The number of channels desired for the output tensor.\n    \n    Returns:\n    tf.Tensor: The waveform tensor adjusted to have exactly n_channels. \n    \"\"\"\n    \n    current_channels = waveform.shape[-1]\n    \n    if current_channels > n_channels:\n        # If current number of channels is more than required, trim the excess channels\n        return waveform[..., :n_channels]\n    elif current_channels < n_channels:\n        # Need more channels, thus replicate the existing channels\n        replication_factor = n_channels // current_channels\n        extra_needed = n_channels % current_channels\n        \n        # Tile the waveform to have at least as many channels as needed by duplicating current channels\n        tiled_waveform = tf.tile(waveform, [1, 1, replication_factor])\n\n        # If the division isn't perfect, extra channels will be needed; take some from the current waveform\n        if extra_needed > 0:\n            extra_waveform = waveform[..., :extra_needed]\n            # Concatenate the tiled waveform and extra waveform to get the exact number of channels\n            adjusted_waveform = tf.concat([tiled_waveform, extra_waveform], axis=-1)\n        else:\n            adjusted_waveform = tiled_waveform\n\n        return adjusted_waveform[..., :n_channels]\n    else:\n        # The number of channels is exactly what we need\n        return waveform", "CodeBleu_score": 34.86564672439199}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "import tensorflow as tf\n\ndef spectrogram_to_db_uint(spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs) -> tf.Tensor:\n    \"\"\"The function converts a spectrogram to a dB-scaled uint8 format.\n    It first converts the spectrogram to dB using gain_to_db.\n    Then, it finds the maximum dB value and clips the dB spectrogram to a specified range below this maximum.\n    Finally, it converts the clipped dB spectrogram from float32 to uint8 format and returns it.\"\"\"\n    \n    # Convert the spectrogram to decibels using the existing TensorFlow function.\n    db_spectrogram = tf.audio.gain_to_db(spectrogram, **kwargs)\n    \n    # Find the maximum dB value in the spectrogram\n    max_db = tf.reduce_max(db_spectrogram)\n    \n    # Clip the dB values to be within the specified dB range below the max value.\n    min_db = max_db - db_range\n    db_spectrogram_clipped = tf.clip_by_value(db_spectrogram, min_db, max_db)\n    \n    # Scale the data to 0-255\n    db_spectrogram_clipped_shifted = db_spectrogram_clipped - min_db\n    db_spectrogram_clipped_scaled = (db_spectrogram_clipped_shifted / db_range) * 255.0\n    \n    # Convert from float32 to uint8.\n    spectrogram_uint8 = tf.cast(db_spectrogram_clipped_scaled, tf.uint8)\n    \n    return spectrogram_uint8", "CodeBleu_score": 44.51329354000103}, {"code": "# db_spectrogram = 20 * log10(max(spectrogram / reference_value, 1e-12))\n#", "CodeBleu_score": 0.0}, {"code": "import tensorflow as tf\n\ndef spectrogram_to_db_uint(spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs) -> tf.Tensor:\n    \"\"\"\n    The function converts a spectrogram to a dB-scaled uint8 format.\n    It first converts the spectrogram to dB using gain_to_db.\n    Then, it finds the maximum dB value and clips the dB spectrogram to a specified range below this maximum.\n    Finally, it converts the clipped dB spectrogram from float32 to uint8 format and returns it.\n    \"\"\"\n    # Ensure the spectrogram has no zero values (since log(0) is undefined)\n    spectrogram = tf.clip_by_value(spectrogram, tf.keras.backend.epsilon(), spectrogram.max())\n\n    # Convert the magnitude spectrogram to dB scale.\n    # Reference is the max value within the spectrogram\n    ref = tf.reduce_max(spectrogram)\n    # Minimum value based on the db_range to consider\n    min_val = ref / tf.exp(db_range / 10.0)\n    spectrogram_db = tf.math.log(spectrogram / min_val) * (10. / tf.math.log(10.))\n\n    # Find the maximum value in the dB spectrogram\n    max_db = tf.reduce_max(spectrogram_db)\n    # Clip the values to the range [max_db - db_range, max_db]\n    spectrogram_db_clipped = tf.clip_by_value(spectrogram_db, max_db - db_range, max_db)\n\n    # Scale the clipped dB values to the range [0, 255] for uint8\n    spectrogram_db_scaled = 255 * (spectrogram_db_clipped - (max_db - db_range)) / db_range\n    # Cast to uint8\n    spectrogram_uint8 = tf.cast(spectrogram_db_scaled, tf.uint8)\n\n    return spectrogram_uint8", "CodeBleu_score": 41.30434468502461}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "import tensorflow as tf\n\ndef compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor:\n    \"\"\"The function computes the spectrogram of a given waveform tensor using Short-Time Fourier Transform (STFT).\n    It applies a Hann window function raised to a specified exponent and performs the STFT with defined frame length and step.\n    The resulting tensor is transposed and the absolute value is raised to a specified exponent.\n    The function returns the computed spectrogram tensor.\n    \n    Args:\n        waveform (tf.Tensor): The input waveform, a 1-D tensor.\n        frame_length (int): The frame length for STFT. Defaults to 2048.\n        frame_step (int): The step between successive frames for STFT. Defaults to 512.\n        spec_exponent (float): The exponent to apply to the magnitude spectrogram. Defaults to 1.0.\n        window_exponent (float): The exponent to apply to the Hann window function. Defaults to 1.0.\n        \n    Returns:\n        tf.Tensor: The spectrogram tensor.\n    \"\"\"\n    # Compute Hann window and raise it to the specified window exponent.\n    hann_window = tf.signal.hann_window(frame_length) ** window_exponent\n\n    # Apply STFT\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n        window_fn=lambda frame_length, dtype: hann_window\n    )\n    \n    # Compute the magnitude of the complex numbers and raise them to the spec_exponent\n    magnitude_spectrogram = tf.abs(stft) ** spec_exponent\n    \n    # Since we often need the complex layout as [Frames, Bins], make sure to organize dimensions as needed:\n    # Default output of stft is [Frames, Bins]; transpose if any different format is desired.\n\n    return magnitude_spectrogram", "CodeBleu_score": 49.076379400979576}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\nimport tensorflow as tf\n\ndef compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor:\n    \"\"\"\n    The function computes the spectrogram of a given waveform tensor using Short-Time Fourier Transform (STFT).\n    It applies a Hann window function raised to a specified exponent and performs the STFT with defined frame length and step.\n    The resulting tensor is transposed and the absolute value is raised to a specified exponent.\n    The function returns the computed spectrogram tensor.\n\n    Parameters:\n    waveform (tf.Tensor): A tensor containing the waveform data.\n    frame_length (int, optional): The frame length parameter for the STFT. Defaults to 2048.\n    frame_step (int, optional): The step between successive frames for the STFT. Defaults to 512.\n    spec_exponent (float, optional): The exponent to which the spectrogram magnitudes are raised. Defaults to 1.0.\n    window_exponent (float, optional): The exponent to which the Hann window is raised. Defaults to 1.0.\n\n    Returns:\n    tf.Tensor: A tensor containing the computed spectrogram.\n    \"\"\"\n    # Apply the Hann window function raised to the given exponent\n    window_fn = tf.signal.hann_window(frame_length) ** window_exponent\n\n    # Compute the STFT of the waveform using the custom window function\n    stft = tf.signal.stft(\n        waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        window_fn=lambda x: window_fn\n    )\n\n    # Compute the magnitude of the complex STFT values\n    spectrogram = tf.abs(stft)\n\n    # Raise the magnitudes to the specified exponent\n    if spec_exponent != 1.0:\n        spectrogram = spectrogram ** spec_exponent\n\n    # Return the transposed spectrogram to align the time axis horizontally\n    return tf.transpose(spectrogram)", "CodeBleu_score": 36.726178225684706}, {"code": "import tensorflow as tf\n\ndef compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor:\n    \"\"\"\n    The function computes the spectrogram of a given waveform tensor using Short-Time Fourier Transform (STFT).\n    It applies a Hann window function raised to a specified exponent and performs the STFT with defined frame length and step.\n    The resulting tensor is transposed and the absolute value is raised to a specified exponent.\n    The function returns the computed spectrogram tensor.\n    \"\"\"\n\n    # Create a Hann window and raise it to the specified window exponent\n    window = tf.signal.hann_window(frame_length) ** window_exponent\n\n    # Compute the STFT of the waveform using the modified Hann window\n    stft = tf.signal.stft(waveform, frame_length, frame_step, fft_length=frame_length, window_fn=lambda _: window)\n\n    # Take the magnitude of the complex numbers obtained from STFT\n    magnitude = tf.abs(stft)\n\n    # Raise the magnitude to the spec exponent\n    if spec_exponent != 1.0:\n        magnitude = magnitude ** spec_exponent\n\n    # Return the transposed spectrogram for better layout in typical applications\n    spectrogram = tf.transpose(magnitude)\n\n    return spectrogram", "CodeBleu_score": 46.37031551507401}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "import tensorflow as tf\n\ndef time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor:\n    \"\"\"\n    The function time-stretches a spectrogram by a given factor.\n    It calculates the new time dimension by scaling the original time dimension by the factor.\n    The spectrogram is resized using the specified interpolation method.\n    Finally, the resized spectrogram is adjusted to the original dimensions by cropping or padding, and the resulting tensor is returned.\n    \"\"\"\n    # Calculate the new time dimension after stretching\n    original_shape = tf.shape(spectrogram)\n    new_time_size = tf.cast(tf.math.ceil(original_shape[1] * factor), tf.int32)\n    \n    # Resize the spectrogram using the specified method\n    resized_spectrogram = tf.image.resize(\n        spectrogram, \n        [original_shape[0], new_time_size], \n        method=method\n    )\n    \n    # Adjust the size of the spectrogram to match the original dimensions\n    if new_time_size > original_shape[1]:\n        # Crop if the new spectrogram is larger than the original\n        output = resized_spectrogram[:, :original_shape[1]]\n    else:\n        # Pad if the new spectrogram is smaller than the original\n        padding = [[0, 0], [0, original_shape[1] - new_time_size]]\n        output = tf.pad(resized_spectrogram, padding)\n    \n    return output", "CodeBleu_score": 40.24510354881061}, {"code": "import tensorflow as tf\n\ndef time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor:\n    \"\"\"\n    The function time-stretches a spectrogram by a given factor.\n    It calculates the new time dimension by scaling the original time dimension by the factor.\n    The spectrogram is resized using the specified interpolation method.\n    Finally, the resized spectrogram is adjusted to the original dimensions by cropping or padding,\n    and the resulting tensor is returned.\n    \n    Args:\n    spectrogram (tf.Tensor): Input spectrogram tensor, expected shape is [batch_size, height, width, channels].\n    factor (float): Stretching factor. Values greater than 1.0 elongate the time dimension, \n                    while values less than 1.0 shorten it.\n    method (tf.image.ResizeMethod): Interpolation method to use for resizing.\n    \n    Returns:\n    tf.Tensor: The time-stretched spectrogram tensor.\n    \"\"\"\n\n    # Original dimensions\n    original_height, original_width = spectrogram.shape[1:3]\n\n    # New width calculation\n    new_width = int(original_width * factor)\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(spectrogram, [original_height, new_width], method=method)\n\n    # Adjust to original dimensions by cropping or padding\n    if new_width > original_width:\n        # Crop to original width\n        start_width = (new_width - original_width) // 2\n        resized_spectrogram = resized_spectrogram[:, :, start_width:start_width + original_width, :]\n    elif new_width < original_width:\n        # Pad to original width\n        padding_width = (original_width - new_width) // 2\n        padding = tf.constant([[0, 0], [0, 0], [padding_width, original_width - new_width - padding_width], [0, 0]])\n        resized_spectrogram = tf.pad(resized_spectrogram, padding)\n    \n    return resized_spectrogram", "CodeBleu_score": 34.45715374954783}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\nimport tensorflow as tf\n\ndef time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor:\n    \"\"\"\n    The function time-stretches a spectrogram by a given factor.\n    It calculates the new time dimension by scaling the original time dimension by the factor.\n    The spectrogram is resized using the specified interpolation method.\n    Finally, the resized spectrogram is adjusted to the original dimensions by cropping or padding, and the resulting tensor is returned.\n\n    Parameters:\n    - spectrogram: a 2D or 3D tf.Tensor representing the spectrogram where the time dimension is assumed to be the last one.\n    - factor: a float that indicates the stretching factor (factor > 1 stretches the spectrogram, factor < 1 compresses it).\n    - method: resize method from tf.image.ResizeMethod, which includes BILINEAR, NEAREST_NEIGHBOR, etc.\n\n    Returns:\n    - A tf.Tensor of the same shape as the input spectrogram but with the time dimension stretched or compressed.\n    \"\"\"\n    \n    # Validate input tensor dimensions\n    if len(spectrogram.shape) not in [2, 3]:\n        raise ValueError(\"Spectrogram tensor should be either 2D or 3D.\")\n\n    # Extract dimensions\n    if len(spectrogram.shape) == 2:  # For a 2D spectrogram\n        freq_bins, time_steps = spectrogram.shape\n    else:  # For a 3D spectrogram [batch_size, freq_bins, time_steps]\n        batch_size, freq_bins, time_steps = spectrogram.shape\n\n    new_time_steps = int(time_steps * factor)  # Calculate new time dimension size\n\n    # Reshape in case of a 2D tensor for compatibility with tf.image.resize\n    if len(spectrogram.shape) == 2:\n        spectrogram = tf.expand_dims(spectrogram, axis=0)  # Add a batch dimension\n    \n    # Resize the spectrogram\n    resized = tf.image.resize(spectrogram, [freq_bins, new_time_steps], method=method)\n    \n    if len(spectrogram.shape) == 2:  # Remove the batch dimension for 2D tensors\n        resized = tf.squeeze(resized, axis=0)\n    \n    # Adjust dimensions to match the original time steps\n    if new_time_steps > time_steps:  # If the time dimension expanded\n        resized = resized[..., :time_steps]  # Crop to original time dimension\n    elif new_time_steps < time_steps:  # If the time dimension shrank\n        # Pad along the time dimension to match the original size\n        padding = [[0, 0]] * len(resized.shape)\n        padding[-1] = [0, time_steps - new_time_steps]\n        resized = tf.pad(resized, paddings=padding, mode='CONSTANT', constant_values=0)\n    \n    return resized\n\n# Example usage:\n# spectro = tf.random.uniform((32, 100), 0, 1)  # Example spectrogram (2D)\n# stretched_spectro = time_stretch(spectro, factor=1.5)\n# print(stretched_spectro.shape)  # Expected shape: (32, 100) due to cropping or padding to original size", "CodeBleu_score": 32.149852524871754}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "import tensorflow as tf\n\ndef pitch_shift(spectrogram: tf.Tensor, semitone_shift: float = 0.0,\n                method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR) -> tf.Tensor:\n    \"\"\"\n    Performs pitch shifting on a given spectrogram by resizing it based on a semitone shift factor.\n    It calculates the resizing factor and resizes the spectrogram along the frequency axis using\n    the specified method. It then pads the resized spectrogram to maintain the original dimensions\n    and returns the pitch-shifted spectrogram.\n\n    Arguments:\n    - spectrogram: tf.Tensor, a 3D tensor where the dimensions are (time, frequency, channels).\n    - semitone_shift: float, the number of semitones to shift the pitch of the spectrogram.\n    - method: tf.image.ResizeMethod, the TensorFlow resizing method.\n\n    Returns:\n    - Tensor: A 3D tensor of the same shape as the input, representing the pitch-shifted spectrogram.\n    \"\"\"\n    # Calculate the resizing factor\n    resize_factor = 2 ** (semitone_shift / 12)\n\n    # Get current shape\n    original_shape = tf.shape(spectrogram)\n    time_steps, frequency_bins, channels = original_shape[0], original_shape[1], original_shape[2]\n    \n    # Calculate new frequency dimension\n    new_frequency_bins = tf.cast(tf.math.round(tf.cast(frequency_bins, tf.float32) * resize_factor), tf.int32)\n    \n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(spectrogram, [time_steps, new_frequency_bins], method=method)\n    \n    # Pad or crop to the original frequency dimension\n    if new_frequency_bins < frequency_bins:\n        # Pad the spectrogram if new frequency bins are fewer than original\n        pad_before = (frequency_bins - new_frequency_bins) // 2\n        pad_after = frequency_bins - new_frequency_bins - pad_before\n        padded_spectrogram = tf.pad(resized_spectrogram, [[0, 0], [pad_before, pad_after], [0, 0]], mode='CONSTANT')\n    else:\n        # Crop the spectrogram if new frequency bins are more than original\n        crop_start = (new_frequency_bins - frequency_bins) // 2\n        padded_spectrogram = resized_spectrogram[:, crop_start:crop_start + frequency_bins, :]\n    \n    return padded_spectrogram", "CodeBleu_score": 38.21637485992886}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\nimport tensorflow as tf\n\ndef pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor:\n    \"\"\"\n    The function performs pitch shifting on a given spectrogram by resizing it based on a semitone shift factor.\n    It calculates the resizing factor and resizes the spectrogram along the frequency axis using the specified method.\n    It then pads the resized spectrogram to maintain the original dimensions and returns the pitch-shifted spectrogram.\n\n    Parameters:\n    - spectrogram: tf.Tensor - a Tensor representing the spectrogram to be modified.\n    - semitone_shift: float - the number of semitones to shift the pitch, which can be positive or negative.\n    - method: tf.image.ResizeMethod - the method to use for resizing. Default is BILINEAR.\n\n    Returns:\n    - tf.Tensor - a Tensor representing the pitch-shifted spectrogram.\n    \"\"\"\n    # Calculate the pitch shifting factor\n    # Frequency scaling factor for a semitone change\n    pitch_shift_factor = 2 ** (semitone_shift / 12.0)\n\n    # Get spectrogram shape\n    spect_shape = spectrogram.shape\n    new_frequency_dim = int(spect_shape[1] * pitch_shift_factor)\n\n    # Resize the spectrogram along the frequency dimension\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=[new_frequency_dim, spect_shape[2]],\n        method=method\n    )\n\n    # Pad or crop to original shape\n    if new_frequency_dim < spect_shape[1]:\n        # Pad if the new frequency dimension is smaller\n        paddings = tf.constant([[0, 0], [0, spect_shape[1] - new_frequency_dim], [0, 0]])\n        pitch_shifted_spectrogram = tf.pad(resized_spectrogram, paddings)\n    else:\n        # Crop if the new frequency dimension is larger\n        pitch_shifted_spectrogram = resized_spectrogram[:, :spect_shape[1], :]\n\n    return pitch_shifted_spectrogram\n\n# Example of how to use the function is typically not provided as part of docstring\n# but could be considered in testing or demonstration scripts.\n# For example:\n# spectrogram_tensor = tf.random.normal([1, 256, 512])  # Fake spectrogram tensor\n# shifted_spectrogram = pitch_shift(spectrogram_tensor, 3)  # Example use case", "CodeBleu_score": 36.151360640132346}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\nimport tensorflow as tf\n\ndef pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor:\n    \"\"\"The function performs pitch shifting on a given spectrogram by resizing it based on a semitone shift factor.\n    It calculates the resizing factor and resizes the spectrogram along the frequency axis using the specified method.\n    It then pads the resized spectrogram to maintain the original dimensions and returns the pitch-shifted spectrogram.\n\n    Args:\n        spectrogram (tf.Tensor): A 2D or 3D tensor (frequency x time [x batch]) representing the spectrogram.\n        semitone_shift (float): The amount of pitch shifting in semitones.\n        method (tf.image.ResizeMethod): The resizing method to use from TensorFlow.\n\n    Returns:\n        tf.Tensor: The pitch-shifted spectrogram, with the same shape as the input.\n    \"\"\"\n    if semitone_shift == 0.0:\n        return spectrogram\n    \n    # Calculate the pitch shift factor as a multiplicative factor using the formula 2^(semitone_shift / 12).\n    pitch_shift_factor = 2 ** (semitone_shift / 12)\n    \n    # Extract original dimensions\n    original_shape = tf.shape(spectrogram)\n    frequency_dim, time_dim = original_shape[0], original_shape[1]\n    \n    # New frequency dimension after the pitch shift\n    new_frequency_dim = tf.cast(tf.math.round(tf.cast(frequency_dim, dtype=tf.float32) * pitch_shift_factor), dtype=tf.int32)\n    \n    # Reshape spectrogram for resizing: add batch dimension if necessary\n    if len(spectrogram.shape) == 2:\n        spectrogram = tf.expand_dims(spectrogram, axis=0)\n\n    # Resize along the frequency axis\n    resized = tf.image.resize(spectrogram, [new_frequency_dim, time_dim], method=method)\n\n    # If downsampling, pad the spectrogram to keep the same dimension;\n    # if upsampling, crop the spectrogram back to original dimension.\n    if new_frequency_dim < frequency_dim:\n        # Padding size\n        pad_size = frequency_dim - new_frequency_dim\n        padding = [[0, pad_size], [0, 0]] if spectrogram.dtype.is_complex else [[0, pad_size], [0, 0], [0, 0]]\n        padded_spectrogram = tf.pad(resized, padding, mode='CONSTANT')\n    else:\n        padded_spectrogram = resized[:, :frequency_dim, :] if spectrogram.dtype.is_complex else resized[:, :frequency_dim, :]\n    \n    # Remove batch dimension, if it was originally not there.\n    if len(original_shape) == 2:\n        padded_spectrogram = tf.squeeze(padded_spectrogram, axis=0)\n    \n    return padded_spectrogram\n\n# Example usage:\n# Create a dummy spectrogram tensor to simulate pitch shifting\n# spectrogram = tf.random.normal([256, 100, 1])  # example dimensions (frequency bins x time steps x batch)\n# shifted_spectrogram = pitch_shift(spectrogram, semitone_shift=4.0)", "CodeBleu_score": 36.065105656440224}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "from typing import Dict, Any\nimport torch.nn as nn\n\ndef _get_conv_activation_layer(params: Dict) -> Any:\n    \"\"\"\n    The function returns a convolutional activation layer based on provided parameters.\n    It checks the \"conv_activation\" parameter and returns the corresponding activation layer:\n    ReLU, ELU, or LeakyReLU with a negative slope of 0.2 as the default.\n\n    Arguments:\n    params -- A dictionary containing the key 'conv_activation' which can be 'relu', 'elu' or 'leaky_relu'\n\n    Returns:\n    nn.Module -- An instance of one of PyTorch's activation layers (nn.ReLU, nn.ELU, nn.LeakyReLU)\n    \"\"\"\n\n    # Default activation function if 'conv_activation' is not specified or does not match the expected values.\n    default_activation = nn.LeakyReLU(negative_slope=0.2)\n\n    if 'conv_activation' not in params:\n        return default_activation\n\n    activation_type = params['conv_activation'].lower()\n    if activation_type == 'relu':\n        return nn.ReLU()\n    elif activation_type == 'elu':\n        return nn.ELU()\n    elif activation_type == 'leaky_relu':\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        return default_activation\n\n# Example usage:\nparams = {'conv_activation': 'relu'}\nactivation_layer = _get_conv_activation_layer(params)\nprint(activation_layer)\n\n# This should print an object/instance of nn.ReLU", "CodeBleu_score": 39.61018548479623}, {"code": "import torch.nn as nn\n\ndef _get_conv_activation_layer(params: Dict) -> Any:\n    \"\"\"\n    The function returns a convolutional activation layer based on provided parameters.\n    It checks the \"conv_activation\" parameter and returns the corresponding activation layer:\n    ReLU, ELU, or LeakyReLU with a negative slope of 0.2 as the default.\n\n    Args:\n        params (Dict): Dictionary containing the key \"conv_activation\" which should be one of \n                       [\"relu\", \"elu\", \"leaky_relu\"].\n\n    Returns:\n        Any: PyTorch activation layer object.\n    \"\"\"\n    activation_type = params.get(\"conv_activation\", \"leaky_relu\").lower()\n\n    if activation_type == \"relu\":\n        return nn.ReLU()\n    elif activation_type == \"elu\":\n        return nn.ELU()\n    elif activation_type == \"leaky_relu\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\"Unsupported activation type. Supported types are 'relu', 'elu', and 'leaky_relu'.\")", "CodeBleu_score": 43.4387949908124}, {"code": "import torch.nn as nn\nfrom typing import Dict, Any\n\ndef _get_conv_activation_layer(params: Dict) -> Any:\n    \"\"\"\n    The function returns a convolutional activation layer based on provided parameters.\n    It checks the \"conv_activation\" parameter and returns the corresponding activation layer:\n    ReLU, ELU, or LeakyReLU with a negative slope of 0.2 as the default.\n    \n    Args:\n    params (Dict): Dictionary containing the parameter 'conv_activation' which should be one\n                   of ['relu', 'elu', 'leaky_relu'].\n    \n    Returns:\n    Any: Returns the activation layer object from PyTorch's nn module.\n    \"\"\"\n    conv_activation = params.get('conv_activation', 'leaky_relu').lower()\n\n    if conv_activation == 'relu':\n        return nn.ReLU()\n    elif conv_activation == 'elu':\n        return nn.ELU()\n    elif conv_activation == 'leaky_relu':\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"Unsupported activation type: {conv_activation}\")", "CodeBleu_score": 45.64965537479168}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "import tensorflow as tf\nfrom typing import Dict, Iterable\n\ndef softmax_unet(input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}) -> Dict:\n    \"\"\"\n    The function applies a U-Net model to an input tensor for each specified instrument to generate spectrograms.\n    It iterates over the instruments, applies the U-Net model to the input tensor for each instrument, and collects the logit masks.\n    The logit masks are combined using a softmax function along a specified axis.\n    The function then creates an output dictionary where each entry is a spectrogram for an instrument, obtained by multiplying the corresponding mask with the input tensor.\n    Finally, the function returns the dictionary of instrument spectrograms.\n    \"\"\"\n\n    # Parameters\n    axis = params.get(\"axis\", -1)\n    \n    # Create a list to store model outputs (logit masks)\n    logits_list = []\n    \n    # Assuming a U-Net model has been predefined or to be dynamically imported\n    unet_model = load_unet_model(params)\n    \n    # Applying U-Net model for every instrument and collecting logit masks.\n    for instrument in instruments:\n        print(f\"Processing {instrument}\")\n        logits = unet_model(input_tensor)\n        logits_list.append(logits)\n    \n    # Concatenate logits along a new axis so softmax can be applied.\n    combined_logits = tf.stack(logits_list, axis=axis)\n    \n    # Applying softmax along the specified axis\n    softmaxed_masks = tf.nn.softmax(combined_logits, axis=axis)\n    \n    instrument_spectrograms = {}\n    \n    # Construct spectrograms for each instrument by multiplying the mask with input tensor\n    for index, instrument in enumerate(instruments):\n        mask = softmaxed_masks[index]\n        \n        # Avoiding broadcasting issues by ensuring mask shapes matches input_tensor shape\n        mask = tf.expand_dims(mask, axis=-1)\n        spectrogram = mask * input_tensor\n        \n        # Storing the spectrogram for the current instrument\n        instrument_spectrograms[instrument] = spectrogram\n    \n    return instrument_spectrograms\n\ndef load_unet_model(params):\n    # Placeholder for loading a U-Net model, depends on your architecture or method of defining/importing U-Net.\n    # This should be replaced with actual model loading mechanism.\n    return tf.keras.models.load_model(params.get(\"model_path\", \"path_to_unet_model\"))", "CodeBleu_score": 35.53165757452136}, {"code": "import tensorflow as tf\nfrom typing import Iterable, Dict\n\ndef softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict:\n    \"\"\"\n    Applies a U-Net model to an input tensor for each specified instrument to generate spectrograms.\n    Iterates over the instruments, applies the U-Net model to the input tensor for each instrument, \n    and collects the logit masks. The logit masks are combined using a softmax function along \n    a specified axis. The function then creates an output dictionary where each entry is a \n    spectrogram for an instrument, obtained by multiplying the corresponding mask with the input tensor.\n    Finally, the function returns the dictionary of instrument spectrograms.\n    \"\"\"\n    \n    # Dictionary to store the spectrograms for each instrument\n    output = {}\n\n    # List to store logits from each instrument model for softmax calculation\n    logits_list = []\n    \n    # Load and apply the U-Net models for each instrument\n    for instrument in instruments:\n        model = get_unet_model(instrument, params)\n        logits = model(input_tensor, training=False)  # Predict logits\n        logits_list.append(logits)\n    \n    # Stack the logits [num_instruments, batch, height, width, channels]\n    stacked_logits = tf.stack(logits_list, axis=0)\n    \n    # Apply softmax across the instruments axis\n    softmax_masks = tf.nn.softmax(stacked_logits, axis=0)\n    \n    # Generate the masked output for each instrument\n    for i, instrument in enumerate(instruments):\n        mask = softmax_masks[i]  # this gets the mask for the current instrument\n        spectrogram = mask * input_tensor  # element-wise multiplication\n        output[instrument] = spectrogram\n    \n    return output\n\ndef get_unet_model(instrument_name, params):\n    # Placeholder function to instantiate U-Net model for a given instrument\n    # To be implemented or replaced with actual model loading code.\n    return tf.keras.Model()  # returns a dummy model", "CodeBleu_score": 43.65675977263938}, {"code": "import tensorflow as tf\nfrom typing import Iterable, Dict", "CodeBleu_score": 0.6172839506172839}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "import hashlib\n\ndef compute_file_checksum(path):\n    \"\"\"\n    The function computes the SHA-256 checksum of a file.\n    It initializes a SHA-256 hash object and reads the file in chunks, updating the hash with each chunk.\n    Finally, it returns the hexadecimal representation of the computed hash.\n    \"\"\"\n    # Create a SHA-256 hash object\n    sha256_hash = hashlib.sha256()\n    \n    # Open the file in binary mode\n    with open(path, \"rb\") as file:\n        # Read and update hash in chunks of 4K (You can adjust the chunk size as per your need)\n        for byte_block in iter(lambda: file.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    \n    # Return the hex digest of the hash\n    return sha256_hash.hexdigest()", "CodeBleu_score": 70.12139945847368}, {"code": "import hashlib\n\ndef compute_file_checksum(path):\n    \"\"\"The function computes the SHA-256 checksum of a file.\n    It initializes a SHA-256 hash object and reads the file in chunks, updating the hash with each chunk.\n    Finally, it returns the hexadecimal representation of the computed hash.\n    \n    Args:\n    path (str): The path to the file for which to compute the checksum.\n    \n    Returns:\n    str: The hexadecimal string of the SHA-256 checksum.\n    \"\"\"\n    # Initialize a SHA-256 hasher\n    hasher = hashlib.sha256()\n    \n    # Open the file in binary mode\n    with open(path, \"rb\") as file:\n        # Read the file in chunks to manage memory usage on large files\n        chunk = file.read(4096)\n        while chunk:\n            # Update the hash with the contents of the chunk\n            hasher.update(chunk)\n            chunk = file.read(4096)\n    \n    # Return the hexadecimal checksum of the file\n    return hasher.hexdigest()", "CodeBleu_score": 53.3849222066404}, {"code": "import hashlib\n\ndef compute_file_checksum(path):\n    \"\"\"The function computes the SHA-256 checksum of a file.\n    It initializes a SHA-256 hash object and reads the file in chunks, updating the hash with each chunk.\n    Finally, it returns the hexadecimal representation of the computed hash.\n    \n    Args:\n    path (str): The path to the file for which the checksum is to be computed.\n    \n    Returns:\n    str: The hexadecimal SHA-256 checksum of the file.\n    \"\"\"\n    # Create a new SHA-256 hash object\n    sha256 = hashlib.sha256()\n\n    # Define the size of each read chunk (you can adjust the size as needed)\n    chunk_size = 8192\n\n    try:\n        # Open the file in binary mode\n        with open(path, \"rb\") as file:\n            # Read the file in the specified chunk size\n            chunk = file.read(chunk_size)\n            while chunk:\n                # Update the hash object with the chunk\n                sha256.update(chunk)\n                # Read the next chunk\n                chunk = file.read(chunk_size)\n\n        # Get the hexadecimal digest of the hash\n        checksum = sha256.hexdigest()\n        return checksum\n    except IOError as e:\n        print(f\"An error occurred trying to read the file: {e}\")\n        return None", "CodeBleu_score": 42.62656514264703}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "import requests\n\nclass ModelManager:\n    def __init__(self, host, repository, release_path):\n        self.host = host\n        self.repository = repository\n        self.release_path = release_path\n\n    def checksum(self, name: str) -> str:\n        \"\"\"\n        The function retrieves the checksum for a specified model name.\n        It constructs a URL using host, repository, release path, and checksum index.\n        It sends an HTTP GET request to this URL and checks for a successful response.\n        It parses the response as JSON and checks if the model name is in the index.\n        If the model name is not found, it raises a ValueError.\n        The function returns the checksum for the specified model name.\n        \"\"\"\n        # Construct the URL\n        url = f\"{self.host}/{self.repository}/{self.release_path}/checksum_index.json\"\n\n        # Send an HTTP GET request\n        response = requests.get(url)\n        if response.status_code != 200:\n            response.raise_for_status()\n\n        # Parse the response as JSON\n        checksum_data = response.json()\n\n        # Check if model name is in the checksum index\n        if name not in checksum_data:\n            raise ValueError(f\"Checksum for model '{name}' not found in index.\")\n\n        # Return the checksum for the specified model name\n        return checksum_data[name]\n\n# Example usage\n# Assuming the class has been properly initialized with correct host, repository, and release path\n# model_manager = ModelManager(\"http://example.com\", \"models\", \"v1\")\n# try:\n#    print(model_manager.checksum(\"my_model\"))\n# except Exception as e:\n#    print(str(e))", "CodeBleu_score": 44.050267388655264}, {"code": "import requests\n\ndef checksum(self, name: str) -> str:\n    \"\"\"\n    The function retrieves the checksum for a specified model name.\n    It constructs a URL using host, repository, release path, and checksum index.\n    It sends an HTTP GET request to this URL and checks for a successful response.\n    It parses the response as JSON and checks if the model name is in the index.\n    If the model name is not found, it raises a ValueError.\n    The function returns the checksum for the specified model name.\n    \"\"\"\n    \n    # Assume host, repository, and release path are attributes of the class containing this function\n    base_url = f\"http://{self.host}/{self.repository}/{self.release_path}\"\n    checksum_index_url = f\"{base_url}/checksums.json\"  # Assuming checksums are stored in a JSON at this path\n    \n    # Send a GET request to the checksum index URL\n    response = requests.get(checksum_index_url)\n    \n    # Check for a successful response\n    if response.status_code == 200:\n        # Parse the JSON response\n        checksums = response.json()\n        \n        # Check if the model name is in the index\n        if name in checksums:\n            return checksums[name]\n        else:\n            raise ValueError(f\"Checksum for model '{name}' not found.\")\n    else:\n        # Handle the case where the HTTP request failed\n        response.raise_for_status()\n\n# Example of how the class could look like:\nclass ModelChecksum:\n    def __init__(self, host, repository, release_path):\n        self.host = host\n        self.repository = repository\n        self.release_path = release_path", "CodeBleu_score": 40.500612164188276}, {"code": "import requests\nimport json\n\nclass ModelManager:\n    def __init__(self):\n        self.host = \"http://example.com\"\n        self.repository = \"/models\"\n        self.release_path = \"/releases\"\n        self.checksum_index = \"/checksums.json\"\n\n    def checksum(self, name: str) -> str:\n        \"\"\"The function retrieves the checksum for a specified model name.\n        It constructs a URL using host, repository, release path, and checksum index.\n        It sends an HTTP GET request to this URL and checks for a successful response.\n        It parses the response as JSON and checks if the model name is in the index.\n        If the model name is not found, it raises a ValueError.\n        The function returns the checksum for the specified model name.\"\"\"\n\n        # Construct the URL to access the checksum index\n        url = f\"{self.host}{self.repository}{self.release_path}{self.checksum_index}\"\n        \n        # Send the GET request\n        response = requests.get(url)\n        \n        # Check for a successful response\n        if response.status_code != 200:\n            raise ConnectionError(f\"Failed to retrieve data from server, status code {response.status_code}\")\n        \n        # Parse the response data\n        checksums = response.json()\n        \n        # Check if the model name is in the index\n        if name not in checksums:\n            raise ValueError(f\"Checksum for model name '{name}' not found in the index.\")\n        \n        # Return the checksum\n        return checksums[name]\n\n# Example usage:\nmanager = ModelManager()\ntry:\n    model_checksum = manager.checksum(\"my_model\")\n    print(\"Checksum:\", model_checksum)\nexcept ValueError as e:\n    print(e)\nexcept ConnectionError as e:\n    print(e)", "CodeBleu_score": 40.10298469733402}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "import os\nimport requests\nimport hashlib\nimport tarfile\nimport zipfile\nimport logging\n\nclass ModelDownloader:\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n        logging.basicConfig(level=logging.INFO)\n\n    def download(self, name: str, path: str) -> None:\n        \"\"\"The function downloads a model archive from a specified URL, validates its checksum,\n        and extracts its contents to a given path.\n        It constructs the download URL, logs the download initiation, and streams the file using HTTP.\n        After downloading, it checks the file's integrity by comparing its checksum with the expected value.\n        If the checksum is valid, it extracts the archive to the specified path and deletes the temporary file.\n        Finally, it logs the extraction completion.\"\"\"\n        \n        download_url = f\"{self.base_url}/{name}\"\n        local_filename = f\"{name}.tar.gz\"  # Assuming the file is a tar.gz archive\n        expected_checksum = \"your_expected_checksum_here\"  # You must replace with actual value\n        \n        logging.info(f\"Starting to download: {download_url}\")\n        response = requests.get(download_url, stream=True)\n        \n        hasher = hashlib.sha256()\n        with open(local_filename, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    file.write(chunk)\n                    hasher.update(chunk)\n        \n        actual_checksum = hasher.hexdigest()\n        if actual_checksum != expected_checksum:\n            logging.error(\"Checksum validation failed.\")\n            os.remove(local_filename)\n            return\n        \n        logging.info(\"Checksum valid. Extracting contents...\")\n\n        if tarfile.is_tarfile(local_filename):\n            with tarfile.open(local_filename, 'r:gz') as tar:\n                tar.extractall(path=path)\n        elif zipfile.is_zipfile(local_filename):\n            with zipfile.ZipFile(local_filename, 'r') as zip_ref:\n                zip_ref.extractall(path=path)\n        else:\n            logging.error('Unrecognized archive format')\n            os.remove(local_filename)\n            return\n        \n        os.remove(local_filename)\n        logging.info(f\"Successfully extracted archive to {path}.\")", "CodeBleu_score": 39.585598707837875}, {"code": "import requests\nimport hashlib\nimport tarfile\nimport os\n\nclass ModelDownloader:\n    def download(self, name: str, path: str) -> None:\n        \"\"\"\n        The function downloads a model archive from a specified URL, validates its checksum,\n        and extracts its contents to a given path. It constructs the download URL, logs the \n        download initiation, and streams the file using HTTP/2. After downloading, it checks \n        the file's integrity by comparing its checksum with the expected value. If the checksum \n        is valid, it extracts the archive to the specified path and deletes the temporary file.\n        Finally, it logs the extraction completion.\n        \"\"\"\n        base_url = f\"https://example.com/models/{name}.tar.gz\"\n        expected_checksum = \"expected_checksum_value\"  # This should be the actual expected checksum\n        temp_path = os.path.join(\"/tmp\", f\"{name}.tar.gz\")\n\n        # Log download initiation\n        print(f\"Starting download of {name} from {base_url}\")\n\n        # Download the file using HTTP/2\n        with requests.get(base_url, stream=True) as response:\n            response.raise_for_status()\n            with open(temp_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n\n        # Verify checksum\n        with open(temp_path, \"rb\") as f:\n            file_hash = hashlib.sha256()\n            while chunk := f.read(4096):\n                file_hash.update(chunk)\n            downloaded_checksum = file_hash.hexdigest()\n\n        if downloaded_checksum != expected_checksum:\n            os.remove(temp_path)\n            raise ValueError(\"Downloaded file checksum does not match expected checksum\")\n\n        # Extract the archive\n        with tarfile.open(temp_path, \"r:gz\") as tar:\n            tar.extractall(path)\n\n        # Cleanup temporary file\n        os.remove(temp_path)\n\n        # Log extraction completion\n        print(f\"Successfully extracted {name} to {path}\")\n\n# Usage example:\n# downloader = ModelDownloader()\n# downloader.download(\"model_name\", \"/desired/path\")", "CodeBleu_score": 40.07465597028717}, {"code": "import requests\nimport hashlib\nimport tarfile\nimport os\nimport logging\n\nclass ModelDownloader:\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n\n    def download(self, name: str, path: str) -> None:\n        \"\"\"\n        The function downloads a model archive from a specified URL, validates its checksum, and extracts its contents\n        to a given path. It constructs the download URL, logs the download initiation, and streams the file using HTTP/2.\n        After downloading, it checks the file's integrity by comparing its checksum with the expected value.\n        If the checksum is valid, it extracts the archive to the specified path, and deletes the temporary file.\n        Finally, it logs the extraction completion.\n        \"\"\"\n        # Construct download URL\n        download_url = f\"{self.base_url}/{name}.tar.gz\"\n\n        # Setting the headers for HTTP/2\n        headers = {\"Accept-Encoding\": \"gzip\"}\n\n        # Logging the download initiation\n        logging.info(f\"Starting to download the model from {download_url}\")\n\n        # Perform the download\n        response = requests.get(download_url, headers=headers, stream=True)\n        if response.status_code == 200:\n            # Define temporary download path\n            download_path = f\"{path}/{name}.tar.gz\"\n\n            # Save the downloaded content to the path\n            with open(download_path, \"wb\") as file:\n                for chunk in response.iter_content(chunk_size=8192):\n                    file.write(chunk)\n\n            # Verify checksum\n            expected_checksum = self.get_expected_checksum(name)\n            if self.validate_checksum(download_path, expected_checksum):\n                # Extract archive\n                logging.info(f\"Extracting the archive {name}.tar.gz\")\n                with tarfile.open(download_path, 'r:gz') as tar:\n                    tar.extractall(path=path)\n                # Remove the temporary file\n                os.remove(download_path)\n                logging.info(f\"Successfully extracted the model to {path}\")\n            else:\n                logging.error(\"Checksum does not match. File may be corrupted.\")\n                os.remove(download_path) # Cleanup corrupted file\n        else:\n            logging.error(f\"Failed to download the file. Status code: {response.status_code}\")\n\n    def get_expected_checksum(self, name: str) -> str:\n        \"\"\"Retrieves the expected checksum of the file based on some external or predefined information.\"\"\"\n        # Here as an example. In practice, you should replace this with actual checksum retrieval logic.\n        return \"abc123\"\n\n    def validate_checksum(self, file_path: str, expected_checksum: str) -> bool:\n        \"\"\"Calculates the checksum of the file at file_path and compares with expected_checksum.\"\"\"\n        sha256_hash = hashlib.sha256()\n        with open(file_path, \"rb\") as file:\n            for byte_block in iter(lambda: file.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        calculated_checksum = sha256_hash.hexdigest()\n        return calculated_checksum == expected_checksum\n\n# Usage\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    downloader = ModelDownloader(base_url=\"http://yourbaseurl.com/models\")\n    downloader.download(name=\"your_model_name\", path=\"/desired/path\")", "CodeBleu_score": 37.23580819293437}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\nimport os\nimport json\nfrom typing import Dict\n\ndef load_configuration(descriptor: str) -> Dict:\n    \"\"\"\n    The function loads a configuration based on the provided descriptor string.\n    If the descriptor indicates an embedded configuration, it extracts the name and\n    reads the corresponding JSON resource.\n    If the descriptor is a standard file path, it checks for file existence and reads\n    the JSON content from the file.\n    The function raises an error if the embedded configuration or file is not found,\n    and returns the loaded configuration as a dictionary.\n\n    Args:\n    descriptor (str): A string descriptor which can be a path to a json file or an\n                      identifier for an embedded configuration resource.\n\n    Returns:\n    Dict: The loaded JSON configuration as a dictionary.\n\n    Raises:\n    FileNotFoundError: If the specified file or resource does not exist.\n    \"\"\"\n    \n    # Check if the descriptor is likely a path or an embedded resource\n    # This assumes embedded resources use a naming scheme without typical path separators\n    if \"/\" not in descriptor and \"\\\\\" not in descriptor:\n        # Treat as embedded resource\n        # Here we need to define where embedded resources are stored, example use JSON module\n        \n        file_path = os.path.join(\"resources\", descriptor + \".json\")\n\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"Embedded configuration file '{file_path}' does not exist.\")\n\n        with open(file_path, 'r') as file:\n            configuration = json.load(file)\n    else:\n        # Treat as file path\n        if not os.path.isfile(descriptor):\n            raise FileNotFoundError(f\"Configuration file '{descriptor}' does not exist.\")\n        \n        with open(descriptor, 'r') as file:\n            configuration = json.load(file)\n\n    return configuration\n\n# Example usage:\n# config = load_configuration(\"config_settings\")\n# config_from_file = load_configuration(\"/path/to/config.json\")", "CodeBleu_score": 40.84913829647709}, {"code": "import json\nimport os\nfrom typing import Dict\n\ndef load_configuration(descriptor: str) -> Dict:\n    \"\"\"\n    The function loads a configuration based on the provided descriptor string.\n    If the descriptor indicates an embedded configuration, it extracts the name and reads the corresponding JSON resource.\n    If the descriptor is a standard file path, it checks for file existence and reads the JSON content from the file.\n    The function raises an error if the embedded configuration or file is not found, and returns the loaded configuration as a dictionary.\n    \n    :param descriptor: A string that could either be a path to a JSON file or an identifier for an embedded configuration.\n    :return: A dictionary containing the configuration.\n    :raises FileNotFoundError: If the descriptor does not lead to a valid file or configuration.\n    \"\"\"\n    # Loading from an embedded configuration if descriptor contains some specific markers\n    if descriptor.startswith(\"embedded:\"):\n        # Extract the name after \"embedded:\"\n        config_name = descriptor.split(\"embedded:\")[1]\n        try:\n            # Load from an imaginary module/resource named 'embedded_configurations'\n            import embedded_configurations  # assuming there exists a module for embedded configurations\n            config = getattr(embedded_configurations, config_name)\n        except (ImportError, AttributeError):\n            raise FileNotFoundError(f\"Embedded configuration '{config_name}' not found.\")\n\n    # If not an embedded configuration, assume it's a file path\n    else:\n        if not os.path.exists(descriptor):\n            raise FileNotFoundError(f\"Configuration file '{descriptor}' not found.\")\n\n        with open(descriptor, 'r') as file:\n            config = json.load(file)\n    \n    return config", "CodeBleu_score": 42.517932006410206}, {"code": "import json\nimport os\nfrom typing import Dict\n\ndef load_configuration(descriptor: str) -> Dict:\n    \"\"\"The function loads a configuration based on the provided descriptor string.\n    If the descriptor indicates an embedded configuration, it extracts the name and reads the corresponding JSON resource.\n    If the descriptor is a standard file path, it checks for file existence and reads the JSON content from the file.\n    The function raises an error if the embedded configuration or file is not found, and returns the loaded configuration as a dictionary.\n    \"\"\"\n    # Example Embedded: \"embedded:config1.json\"\n    if descriptor.startswith(\"embedded:\"):\n        # Removing the 'embedded:' prefix\n        filename = descriptor[9:]\n        # Path to the file within a 'config' directory, for example purposes\n        file_path = os.path.join(\"configs\", filename)\n    else:\n        # Standard file path\n        file_path = descriptor\n\n    # Check if file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"No configuration file found at {file_path}\")\n    \n    # Read the JSON file\n    with open(file_path, 'r') as file:\n        configuration = json.load(file)\n    \n    return configuration", "CodeBleu_score": 38.63680415473325}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\nimport tensorflow as tf\nfrom typing import Dict, Callable\n\ndef sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]:\n    \"\"\"\n    The function applies a given function to concatenated tensors from a dictionary.\n    It checks if the concatenation axis is either 0 or 1, raising an error if not.\n    It concatenates the tensors along the specified axis, applies the function to the concatenated tensor,\n    and then splits the processed tensor back into the original dictionary format based on the concatenation axis.\n    The function returns the dictionary of processed tensors.\n\n    Args:\n    tensor_dict (Dict[str, tf.Tensor]): Dictionary containing tensor data.\n    func (Callable): Function to apply to the concatenated tensor.\n    concat_axis (int): Axis along which to concatenate tensors, 0 or 1.\n\n    Returns:\n    Dict[str, tf.Tensor]: Dictionary with tensors having the function applied.\n    \"\"\"\n    \n    # Ensure the concatenation axis is either 0 or 1\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n\n    # Get the list of tensors and concatenate them along the specified axis\n    tensors_list = list(tensor_dict.values())\n    concatenated_tensor = tf.concat(tensors_list, axis=concat_axis)\n\n    # Apply the function to the concatenated tensor\n    processed_tensor = func(concatenated_tensor)\n\n    # Split the tensor back into the original parts\n    sizes = [t.shape[concat_axis] for t in tensors_list]  # List of sizes per tensor along concat_axis\n    split_tensors = tf.split(processed_tensor, sizes, axis=concat_axis)\n\n    # Create a new dictionary with the same keys and processed tensors as values\n    processed_tensor_dict = {key: tensor for key, tensor in zip(tensor_dict.keys(), split_tensors)}\n\n    return processed_tensor_dict\n\n# Example usage of the sync_apply function:\n# Define a simple function to apply\ndef increase_by_one(x):\n    return x + 1\n\n# Create a sample dictionary of tensors\ntensor_dict_example = {\n    'a': tf.constant([[1, 2], [3, 4]]),\n    'b': tf.constant([[5, 6], [7, 8]])\n}\n\n# Apply the function\nprocessed_tensors = sync_apply(tensor_dict_example, increase_by_one, concat_axis=1)\nprint({key: tensor.numpy() for key, tensor in processed_tensors.items()})  # Output the processed tensors as numpy arrays", "CodeBleu_score": 36.942425705022494}, {"code": "import tensorflow as tf\nfrom typing import Callable, Dict\n\ndef sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]:\n    \"\"\"\n    The function applies a given function to concatenated tensors from a dictionary.\n    It checks if the concatenation axis is either 0 or 1, raising an error if not.\n    It concatenates the tensors along the specified axis, applies the function to the concatenated tensor, \n    and then splits the processed tensor back into the original dictionary format based on the concatenation axis.\n    The function returns the dictionary of processed tensors.\n    \n    Args:\n    tensor_dict (Dict[str, tf.Tensor]): Dictionary containing the tensor objects.\n    func (Callable): Function to apply to the concatenated tensor.\n    concat_axis (int): Axis along which to concatenate the tensors. Must be 0 or 1.\n\n    Returns:\n    Dict[str, tf.Tensor]: Dictionary containing the processed tensors.\n    \"\"\"\n\n    # Check if the concatenation axis is valid\n    if concat_axis not in (0, 1):\n        raise ValueError('concat_axis must be either 0 or 1.')\n    \n    # Concatenate the tensors\n    concatenated_tensor = tf.concat(list(tensor_dict.values()), axis=concat_axis)\n    \n    # Apply the function\n    processed_tensor = func(concatenated_tensor)\n    \n    # Split the tensor back into the original dictionary format\n    # Determine split sizes based on the original tensor shapes in the specified dimension\n    original_sizes = [tensor.shape[concat_axis] for tensor in tensor_dict.values()]\n    split_tensors = tf.split(processed_tensor, original_sizes, axis=concat_axis)\n    \n    # Recreate the dictionary with the processed tensors\n    processed_tensor_dict = {key: tensor for key, tensor in zip(tensor_dict.keys(), split_tensors)}\n    \n    return processed_tensor_dict", "CodeBleu_score": 40.73473770236918}, {"code": "import tensorflow as tf\n\ndef sync_apply(tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1) -> Dict[str, tf.Tensor]:\n    \"\"\"\n    Applies a given function to concatenated tensors from a dictionary.\n    It checks if the concatenation axis is either 0 or 1, raising an error if not.\n    It concatenates the tensors along the specified axis, applies the function to the concatenated tensor, \n    and then splits the processed tensor back into the original dictionary format based on the concatenation axis.\n    The function returns the dictionary of processed tensors.\n\n    Args:\n        tensor_dict (Dict[str, tf.Tensor]): A dictionary containing string keys and tensor values.\n        func (Callable): A function to apply to the concatenated tensor.\n        concat_axis (int, optional): Axis along which to concatenate tensors. Defaults to 1.\n\n    Returns:\n        Dict[str, tf.Tensor]: A dictionary of processed tensors, maintaining the original keys.\n\n    Raises:\n        ValueError: If `concat_axis` is not 0 or 1.\n    \"\"\"\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"concat_axis must be 0 or 1\")\n\n    # Extract and concatenate tensor values along the specified axis\n    tensors = list(tensor_dict.values())\n    concatenated_tensor = tf.concat(tensors, axis=concat_axis)\n\n    # Apply the function to the concatenated tensor\n    processed_tensor = func(concatenated_tensor)\n\n    # Calculate new split sizes based on the original tensor shapes along the concat axis\n    split_sizes = [tensor.shape[concat_axis] for tensor in tensors]\n\n    # Split the processed tensor back into the components\n    split_tensors = tf.split(processed_tensor, split_sizes, axis=concat_axis)\n\n    # Repackage the split tensors into the original dictionary format using the original keys\n    output_dict = {key: tensor for key, tensor in zip(tensor_dict.keys(), split_tensors)}\n\n    return output_dict", "CodeBleu_score": 39.018601666546935}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\nimport tensorflow as tf\n\ndef from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> dict:\n    \"\"\"\n    The function normalizes a float32 tensor to the uint8 range (0-255).\n    It computes the minimum and maximum values of the tensor and scales the tensor values to this range.\n    It returns a dictionary containing the scaled uint8 tensor, the original minimum value, and the original maximum value.\n\n    Args:\n    tensor (tf.Tensor): Input tensor with dtype tf.float32.\n    tensor_key (str): Key for the tensor in the returned dictionary.\n    min_key (str): Key for the minimum value in the returned dictionary.\n    max_key (str): Key for the maximum value in the returned dictionary.\n\n    Returns:\n    dict: A dictionary with keys defined by `tensor_key`, `min_key`, and `max_key` corresponding to the scaled tensor,\n          the minimum value and the maximum value of the original tensor respectively.\n    \"\"\"\n    # Compute the minimum and maximum values of the tensor\n    min_val = tf.reduce_min(tensor)\n    max_val = tf.reduce_max(tensor)\n\n    # Normalize the tensor to the range 0-255\n    normalized_tensor = (tensor - min_val) / (max_val - min_val) * 255\n\n    # Cast the tensor to uint8\n    tensor_uint8 = tf.cast(normalized_tensor, tf.uint8)\n\n    # Return the dictionary containing the tensor and the min/max values\n    return {\n        tensor_key: tensor_uint8,\n        min_key: min_val,\n        max_key: max_val\n    }", "CodeBleu_score": 44.08936742397184}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\nimport tensorflow as tf\n\ndef from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> dict:\n    \"\"\"\n    The function normalizes a float32 tensor to the uint8 range (0-255).\n    It computes the minimum and maximum values of the tensor and scales the tensor \n    values to this range.\n    \n    Args:\n        tensor (tf.Tensor): A Tensor of type float32.\n        tensor_key (str, optional): Key name for the tensor in the returned dictionary.\n            Defaults to \"tensor\".\n        min_key (str, optional): Key name for the minimum value in the returned dictionary.\n            Defaults to \"min\".\n        max_key (str, optional): Key name for the maximum value in the returned dictionary.\n            Defaults to \"max\".\n    \n    Returns:\n        dict: A dictionary containing the scaled uint8 tensor and \n              the original minimum and maximum values.\n    \"\"\"\n    # Calculate the min and max values of the tensor\n    min_val = tf.reduce_min(tensor)\n    max_val = tf.reduce_max(tensor)\n\n    # Normalize the tensor to a 0-1 range\n    normalized_tensor = (tensor - min_val) / (max_val - min_val)\n    \n    # Scale up to 0-255 and convert to uint8\n    scaled_tensor = tf.cast(normalized_tensor * 255, tf.uint8)\n    \n    # Build the result dictionary\n    result = {\n        tensor_key: scaled_tensor,\n        min_key: min_val,\n        max_key: max_val\n    }\n    return result\n\n# Example usage:\n# Assuming TensorFlow has been installed and the required libraries are imported.\n# tensor = tf.constant([[0.0, 1.2], [-1.0, 0.4]], dtype=tf.float32)\n# result = from_float32_to_uint8(tensor)\n# print(result)\n# Output: {'tensor': <tf.Tensor: shape=(2, 2), dtype=uint8, numpy=array([[  0, 255], [  0, 171]], dtype=uint8)>,\n#          'min': <tf.Tensor: shape=(), dtype=float32, numpy=-1.0>,\n#          'max': <tf.Tensor: shape=(), dtype=float32, numpy=1.2>}", "CodeBleu_score": 43.348158920711946}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\nimport tensorflow as tf\n\ndef from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> dict:\n    \"\"\"\n    The function normalizes a float32 tensor to the uint8 range (0-255).\n    It computes the minimum and maximum values of the tensor and scales the tensor values to this range.\n    It returns a dictionary containing the scaled uint8 tensor, the original minimum value, and the original maximum value.\n    \n    Parameters:\n        tensor (tf.Tensor): Input tensor of type float32.\n        tensor_key (str): Key for the normalized tensor in the output dictionary.\n        min_key (str): Key for the minimum value in the output dictionary.\n        max_key (str): Key for the maximum value in the output dictionary.\n        \n    Returns:\n        dict: A dictionary containing the scaled uint8 tensor, original minimum value, and original maximum value.\n    \"\"\"\n    # Ensure the input tensor is in float32 format\n    tensor = tf.cast(tensor, tf.float32)\n    \n    # Compute min and max values of the tensor\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    \n    # Normalize the tensor to [0, 1]\n    normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n    \n    # Scale to [0, 255] and convert to uint8\n    tensor_scaled = normalized_tensor * 255\n    tensor_uint8 = tf.cast(tensor_scaled, tf.uint8)\n    \n    # Return the dictionary with required information\n    result = {\n        tensor_key: tensor_uint8,\n        min_key: tensor_min,\n        max_key: tensor_max\n    }\n    return result", "CodeBleu_score": 52.224277835093304}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "import tensorflow as tf\n\ndef pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor:\n    \"\"\"\n    The function pads and partitions a tensor into segments of a specified length.\n    It calculates the padding needed to make the tensor length a multiple of the segment length,\n    pads the tensor, and then reshapes it into a new tensor with the specified segment length.\n    The function returns the reshaped tensor, where the first dimension represents the number of segments.\n    \n    Args:\n    tensor (tf.Tensor): The input 1D tensor.\n    segment_len (int): The length of each segment after partitioning.\n\n    Returns:\n    tf.Tensor: A reshaped tensor after padding and partitioning.\n    \"\"\"\n    # Calculate the length of the tensor\n    original_length = tf.size(tensor)\n    \n    # Calculate how much padding is required to make the tensor length a multiple of the segment length\n    padding_needed = (-original_length % segment_len)\n    \n    # Pad the tensor\n    padded_tensor = tf.pad(tensor, [[0, padding_needed]], constant_values=0)\n    \n    # Calculate number of segments\n    num_segments = (original_length + padding_needed) // segment_len\n    \n    # Reshape the tensor into segments\n    reshaped_tensor = tf.reshape(padded_tensor, (num_segments, segment_len))\n    \n    return reshaped_tensor", "CodeBleu_score": 36.655175921629876}, {"code": "import tensorflow as tf\n\ndef pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor:\n    \"\"\"\n    The function pads and partitions a tensor into segments of a specified length.\n    It calculates the padding needed to make the tensor length a multiple of the\n    segment length, pads the tensor, and then reshapes it into a new tensor with\n    the specified segment length. The function returns the reshaped tensor,\n    where the first dimension represents the number of segments.\n\n    Args:\n    tensor (tf.Tensor): Input tensor to be padded and partitioned.\n    segment_len (int): Length of each segment after partitioning.\n\n    Returns:\n    tf.Tensor: A new tensor where the first dimension is the number of segments\n               and each segment is of length `segment_len`.\n    \"\"\"\n    # Get the total length of the tensor\n    tensor_length = tf.size(tensor)\n\n    # Calculate the number of zeros to pad at the end of the tensor. Padding is\n    # needed to make the length of the tensor a multiple of segment_len.\n    padding_size = (-tensor_length % segment_len)\n\n    # Pad the tensor with zeros at the end\n    padded_tensor = tf.pad(tensor, [[0, padding_size]], mode='CONSTANT')\n\n    # Compute the new shape after partitioning into segments\n    new_shape = [-1, segment_len]\n\n    # Reshape the padded tensor into the new shape\n    partitioned_tensor = tf.reshape(padded_tensor, new_shape)\n\n    return partitioned_tensor\n\n# Example usage:\n# Suppose we have a tensor of length 10, and we want to partition it into segments of length 3\nsample_tensor = tf.constant([1,2,3,4,5,6,7,8,9,10], dtype=tf.int32)\nsegment_length = 3\nresult = pad_and_partition(sample_tensor, segment_length)\nprint(result)", "CodeBleu_score": 36.11225002700915}, {"code": "import tensorflow as tf\n\ndef pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor:\n    \"\"\"\n    The function pads and partitions a tensor into segments of a specified length.\n    It calculates the padding needed to make the tensor length a multiple of the segment length, pads the tensor,\n    and then reshapes it into a new tensor with the specified segment length.\n    The function returns the reshaped tensor, where the first dimension represents the number of segments.\n    \n    Args:\n    tensor (tf.Tensor): Input tensor.\n    segment_len (int): The desired length of each segment after partitioning.\n\n    Returns:\n    tf.Tensor: A reshaped tensor segmented into the specified length.\n    \"\"\"\n    # Calculate the total length of the tensor\n    total_length = tf.shape(tensor)[0]\n    \n    # Calculate the number of segments needed\n    num_segments = tf.math.ceil(total_length / segment_len)\n    \n    # Calculate the total padded length required to make the tensor divisible by segment_len\n    total_padded_length = num_segments * segment_len\n    \n    # Calculate the padding size needed\n    padding_size = total_padded_length - total_length\n    \n    # Pad the tensor\n    padded_tensor = tf.pad(tensor, [[0, padding_size]], constant_values=0)\n    \n    # Reshape the tensor to have shape [num_segments, segment_len]\n    reshaped_tensor = tf.reshape(padded_tensor, [num_segments, segment_len])\n    \n    return reshaped_tensor", "CodeBleu_score": 41.41041477396906}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "import numpy as np\n\ndef pad_and_reshape(instr_spec, frame_length, F) -> np.ndarray:\n    \"\"\"\n    The function pads and reshapes an input spectrogram tensor.\n    It calculates the number of extra rows needed for padding and creates an extension of zeros.\n    The function concatenates this extension to the original spectrogram along the frequency axis.\n    It then reshapes the extended spectrogram by merging the first two dimensions and returns the processed spectrogram.\n\n    Parameters:\n        instr_spec (np.ndarray): 2D array representing the spectrogram [times x frequency bins].\n        frame_length (int): The desired number of frequency bins after padding.\n        F (int): The size of the new reshaped dimension.\n\n    Returns:\n        np.ndarray: The processed spectrogram after padding and reshaping.\n    \"\"\"\n    # Determine current shape\n    current_shape = instr_spec.shape\n\n    # Compute how many rows of zeros are needed\n    if frame_length > current_shape[1]:\n        zero_padding = np.zeros((current_shape[0], frame_length - current_shape[1]))\n        # Concatenate zeros to the original spectrogram along the frequency axis\n        padded_spec = np.concatenate((instr_spec, zero_padding), axis=1)\n    else:\n        padded_spec = instr_spec  # No padding needed if frame_length is less than or equal to current frequency bins\n\n    # Reshape the spectrogram into the new dimension\n    # Calculate the reshape dimensions assuming the number of elements is conserved and making the second last dimension F\n    reshaped_specs = padded_spec.reshape(-1, F, padded_spec.shape[1] // F)\n\n    return reshaped_specs", "CodeBleu_score": 27.8236389139366}, {"code": "import numpy as np\n\ndef pad_and_reshape(instr_spec, frame_length, F):\n    \"\"\"\n    The function pads and reshapes an input spectrogram tensor.\n    It calculates the number of extra rows needed for padding and creates an extension of zeros.\n    The function concatenates this extension to the original spectrogram along the frequency axis.\n    It then reshapes the extended spectrogram by merging the first two dimensions and returns the processed spectrogram.\n    \n    Args:\n    instr_spec (numpy.ndarray): The input spectrogram tensor, expected shape [n_frames, current_freq_bins]\n    frame_length (int): Target number of frequency bins.\n    F (int): Does not affect code based on the description, but a traditional pad parameter, typically ensures compatibility with some transformations.\n\n    Returns:\n    numpy.ndarray: Reshaped spectrogram tensor.\n    \"\"\"\n    # Calculate the needed padding size\n    current_freq_bins = instr_spec.shape[1]\n    pad_size = frame_length - current_freq_bins\n    \n    if pad_size < 0:\n        raise ValueError(f\"frame_length {frame_length} is less than the current number of frequency bins {current_freq_bins}.\")\n    \n    # Create padding of zero values\n    padding = np.zeros((instr_spec.shape[0], pad_size))\n    \n    # Concatenate original tensor and the padding along frequency axis\n    padded_spec = np.concatenate([instr_spec, padding], axis=1)\n    \n    # Reshape by multiplying the two first dimensions\n    reshaped_spec = padded_spec.reshape(-1, padded_spec.shape[2])\n    \n    return reshaped_spec\n\n# Depending on usage, this may be modified accordingly, and perhaps a sample call of this function or testing could follow.", "CodeBleu_score": 25.191135691390652}, {"code": "import numpy as np\n\ndef pad_and_reshape(instr_spec, frame_length, F):\n    \"\"\"\n    Pads and reshapes an input spectrogram tensor.\n    \n    Args:\n    instr_spec (numpy.ndarray): Input spectrogram tensor, expected shape (num_frames, num_frequencies).\n    frame_length (int): The desired number of frequency bins in the output spectrogram.\n    F (int): The factor by which to merge the frames and frequency bins.\n    \n    Returns:\n    numpy.ndarray: The reshaped and padded spectrogram tensor.\n    \"\"\"\n    # Check the current number of frequencies in the spectrogram\n    num_frames, num_frequencies = instr_spec.shape\n    \n    # Calculate how many rows of zeros are needed if the current number of frequencies is less than frame_length\n    if num_frequencies < frame_length:\n        padding = frame_length - num_frequencies\n        # Create a pad of zeros with the necessary number of rows\n        zero_pad = np.zeros((num_frames, padding))\n        # Concatenate the zero pad with the original spectrogram along the frequency axis (axis=1)\n        padded_spec = np.concatenate((instr_spec, zero_pad), axis=1)\n    else:\n        padded_spec = instr_spec\n    \n    # Now we need to reshape the spectrogram\n    # We are assuming the first dimension is approximately divisible by F after padding\n    new_num_frames = num_frames * (padded_spec.shape[1] // F)\n    if new_num_frames % F != 0:\n        raise ValueError(\"The total number of elements is not evenly divisible by F, adjust your parameters.\")\n    \n    # Reshape the tensor to merge frames and frequency bins\n    reshaped_spec = padded_spec.reshape(new_num_frames, F)\n    \n    return reshaped_spec", "CodeBleu_score": 24.05789855380377}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "import pandas as pd\nimport tensorflow as tf\nfrom typing import Any\n\ndef dataset_from_csv(csv_path: str, **kwargs) -> Any:\n    \"\"\"\n    The function creates a TensorFlow dataset from a CSV file.\n    It reads the CSV file into a pandas DataFrame, then converts the DataFrame columns into a TensorFlow dataset \n    by slicing the tensor values for each column.\n    The function returns the created TensorFlow dataset.\n    \n    Parameters:\n    csv_path (str): The file path to the CSV file.\n    **kwargs: Additional keyword arguments to be passed to pd.read_csv method.\n    \n    Returns:\n    tf.data.Dataset: A TensorFlow dataset containing the data from the csv file.\n    \"\"\"\n    # Read the CSV file into a Pandas DataFrame\n    df = pd.read_csv(csv_path, **kwargs)\n    \n    # Convert the DataFrame to a TensorFlow dataset\n    target_dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    \n    return target_dataset", "CodeBleu_score": 51.02238381367958}, {"code": "import pandas as pd\nimport tensorflow as tf\n\ndef dataset_from_csv(csv_path: str, **kwargs) -> tf.data.Dataset:\n    \"\"\"\n    The function creates a TensorFlow dataset from a CSV file.\n    It reads the CSV file into a pandas DataFrame, then converts the DataFrame columns\n    into a TensorFlow dataset by slicing the tensor values for each column.\n    The function returns the created TensorFlow dataset.\n    \n    Parameters:\n    - csv_path (str): The file path to the CSV file.\n    - **kwargs: Additional keyword arguments passed to `pandas.read_csv()` function.\n\n    Returns:\n    - tf.data.Dataset: A TensorFlow Dataset object created from the CSV file.\n    \"\"\"\n    # Read the CSV file into a Pandas DataFrame\n    df = pd.read_csv(csv_path, **kwargs)\n    \n    # Convert the pandas DataFrame into a TensorFlow dataset\n    tf_dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n    \n    return tf_dataset", "CodeBleu_score": 50.155265913781136}, {"code": "import pandas as pd\nimport tensorflow as tf\n\ndef dataset_from_csv(csv_path: str, **kwargs) -> tf.data.Dataset:\n    \"\"\"The function creates a TensorFlow dataset from a CSV file.\n    It reads the CSV file into a pandas DataFrame, then converts the DataFrame columns into a TensorFlow dataset by slicing the tensor values for each column.\n    The function returns the created TensorFlow dataset.\n    \n    Arguments:\n    csv_path : str\n        The path to the CSV file.\n    **kwargs\n        Additional keyword arguments to pass to `pandas.read_csv()` function.\n    \n    Returns:\n    tf.data.Dataset\n        A TensorFlow dataset representing the data in the CSV file.\n    \"\"\"\n    # Read the data using pandas\n    df = pd.read_csv(csv_path, **kwargs)\n    \n    # Convert the pandas DataFrame to a TensorFlow Dataset.\n    # Each row will be sliced as a tuple of tensor values for columns.\n    target = df.pop('target_column') if 'target_column' in kwargs else None  # e.g., you may want to separate target variables\n    dataset = tf.data.Dataset.from_tensor_slices((dict(df), target)) if target is not None else tf.data.Dataset.from_tensor_slices(dict(df))\n    \n    return dataset", "CodeBleu_score": 47.18464921558235}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "import tensorflow as tf\n\ndef check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool:\n    \"\"\"\n    The function checks if a TensorFlow tensor matches a specified target shape.\n    It initializes a boolean result as True and iterates over the target shape dimensions.\n    For each dimension with a specified length, it updates the result by logically AND-ing it \n    with the comparison between the tensor's corresponding dimension and the target length.\n    The function returns the final boolean result indicating if the tensor matches the target shape.\n    \"\"\"\n    # Initialize the result as True\n    result = True\n\n    # Convert the tensor's shape to a list for easier indexing\n    tensor_shape = tensor_tf.shape.as_list()\n\n    # Iterate over the target shape dimensions\n    for i, dim in enumerate(target_shape):\n        # Check if dimension length is specified (not None)\n        if dim is not None:\n            # Update the result by logically AND-ing with the comparison of exact dimensions\n            if i >= len(tensor_shape) or tensor_shape[i] != dim:\n                result = False\n                break\n\n    return result", "CodeBleu_score": 37.591048162772935}, {"code": "import tensorflow as tf\nfrom typing import Any\n\ndef check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool:\n    \"\"\"The function checks if a TensorFlow tensor matches a specified target shape.\n    It initializes a boolean result as True and iterates over the target shape dimensions.\n    For each dimension with a specified length, it updates the result by logically AND-ing it with the comparison between the tensor's corresponding dimension and the target length.\n    The function returns the final boolean result indicating if the tensor matches the target shape.\n\n    Args:\n    tensor_tf (tf.Tensor): TensorFlow tensor whose shape is to be verified.\n    target_shape (Any): The target shape to check the tensor against. This can be a list, tuple, or tf.TensorShape.\n\n    Returns:\n    bool: True if the tensor shape matches the target shape, False otherwise.\n    \"\"\"\n    # Convert target_shape to tuple if it is a tf.TensorShape\n    if isinstance(target_shape, tf.TensorShape):\n        target_shape = tuple(target_shape.as_list())\n\n    tensor_shape = tuple(tensor_tf.shape.as_list())\n    \n    # Initialize result as True\n    result = True\n    \n    # Ensure lengths match\n    if len(tensor_shape) != len(target_shape):\n        return False\n    \n    # Iterate over each dimension in the target shape\n    for dim_index, target_dim_size in enumerate(target_shape):\n        if target_dim_size is not None:  # Skip comparison if dimension size is None (meaning any size is acceptable)\n            # Compare each dimension size from tensor_shape with target_shape\n            result = result and (tensor_shape[dim_index] == target_dim_size)\n            # If result becomes False, no need to continue checking\n            if not result:\n                break\n            \n    return result", "CodeBleu_score": 33.092479890672024}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\nimport tensorflow as tf\n\ndef check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool:\n    \"\"\"The function checks if a TensorFlow tensor matches a specified target shape.\n    It initializes a boolean result as True and iterates over the target shape dimensions.\n    For each dimension with a specified length, it updates the result by logically AND-ing it with the comparison between the tensor's corresponding dimension and the target length.\n    The function returns the final boolean result indicating if the tensor matches the target shape.\n    \n    Args:\n    tensor_tf (tf.Tensor): The TensorFlow tensor whose shape is to be verified.\n    target_shape (Any): The target shape to match. Elements can be integers or None (where None means any size is acceptable for that dimension).\n    \n    Returns:\n    bool: True if the tensor shape matches the target shape, False otherwise.\n    \"\"\"\n    \n    # Define the initial result as True\n    result = True\n  \n    # Get the actual shape of the tensor as a list of integers\n    actual_shape = tensor_tf.shape.as_list()\n    \n    # Check to ensure lengths of shapes match, respecting dimensions specified as None\n    if len(actual_shape) != len(target_shape):\n        return False\n    \n    for i, dim in enumerate(target_shape):\n        # Only compare dimensions that are specified (i.e., not None)\n        if dim is not None:\n            # Update result with the condition that the actual dimension matches the target dimension\n            result = result and (actual_shape[i] == dim)\n        \n        # Early exit if result becomes False\n        if not result:\n            break\n    \n    return result\n\n# Sample usage and testing the function\nif __name__ == '__main__':\n    tensor_shape_1 = tf.random.normal([10, 64, 64, 3]).shape\n    target_1 = [10, 64, None, 3]\n    print(\"Match status:\", check_tensor_shape(tf.random.normal([10, 64, 64, 3]), target_1))  # Expected True\n\n    tensor_shape_2 = tf.random.normal([8, 32, 32]).shape\n    target_2 = [None, None, 64]\n    print(\"Match status:\", check_tensor_shape(tf.random.normal([8, 32, 32]), target_2))  # Expected False\n", "CodeBleu_score": 31.298062328452087}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "from typing import Dict, Any\n\n# Placeholder classes for demonstration purposes\nclass AudioAdapter:\n    def load(self, audio_path: str, sr: int, mono: bool) -> Any:\n        # Actual implementation of this method would load and return the audio track\n        pass\n\nclass DatasetBuilder:\n    def __init__(self, audio_params: Dict, audio_adapter: AudioAdapter):\n        self.audio_params = audio_params\n        self.audio_adapter = audio_adapter\n        \n    def build(self, train_csv: str, cache_dir: str, batch_size: int, chunks_per_song: int, augment: bool, use_cache: bool) -> Any:\n        # Actual implementation required here to build the dataset\n        pass\n\ndef get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any:\n    \"\"\"\n    The function creates a training dataset using provided audio parameters, an audio adapter, and an audio path.\n    It initializes a DatasetBuilder with these parameters, specifying chunk duration and random seed.\n    The function then calls the build method of the builder with additional parameters like train_csv path, cache directory, \n    batch size, number of chunks per song, and options for data augmentation and caching.\n    Finally, it returns the built dataset.\n    \"\"\"\n\n    # Initialising the DatasetBuilder with audio parameters and the adapter\n    builder = DatasetBuilder(audio_params, audio_adapter)\n\n    # Assuming these are predefined or should be passed into the function or globally defined\n    train_csv_path = \"path/to/training_data.csv\"\n    cache_directory = \"path/to/cache\"\n    batch_size = 32\n    chunks_per_song = 10\n    enable_augmentation = True\n    use_cache = True\n\n    # Building the dataset\n    dataset = builder.build(train_csv=train_csv_path,\n                            cache_dir=cache_directory,\n                            batch_size=batch_size,\n                            chunks_per_song=chunks_per_song,\n                            augment=enable_augmentation,\n                            use_cache=use_cache)\n\n    return dataset", "CodeBleu_score": 36.21500700052079}, {"code": "from typing import Dict, Any\nfrom abc import ABC, abstractmethod\n\n# Assuming the existence and behavior of an AudioAdapter and DatasetBuilder\n\nclass AudioAdapter(ABC):\n    @abstractmethod\n    def load_audio(self, file_path: str):\n        pass\n\nclass DatasetBuilder:\n    def __init__(self, audio_params: Dict, audio_data):\n        self.audio_params = audio_params\n        self.audio_data = audio_data\n\n    def build(self, train_csv: str, cache_dir: str, batch_size: int, num_chunks_per_song: int, augment: bool, cache: bool):\n        # example of processing the dataset build\n        print(\"Building dataset using parameters:\")\n        print(f\"Train CSV: {train_csv}, Cache Directory: {cache_dir}, Batch Size: {batch_size}, Num Chunks/Song: {num_chunks_per_song}, Augment: {augment}, Cache: {cache}\")\n        # A more complex implementation would involve data preprocessing and augmentations here\n        return \"dataset\"\n\ndef get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any:\n    \"\"\"The function creates a training dataset using provided audio parameters, an audio adapter, and an audio path.\n    It initializes a DatasetBuilder with these parameters, specifying chunk duration and random seed.\n    The function then calls the build method of the builder with additional parameters like train_csv path, cache directory, \n    batch size, number of chunks per song, and options for data augmentation and caching.\n    Finally, it returns the built dataset.\"\"\"\n\n    # Load audio data using the adapter\n    audio_data = audio_adapter.load_audio(audio_path)\n\n    # Initialize the dataset builder\n    builder = DatasetBuilder(audio_params, audio_data)\n\n    # Example values for build parameters\n    train_csv = \"path/to/train.csv\"\n    cache_dir = \"path/to/cache\"\n    batch_size = 16\n    num_chunks_per_song = 5\n    augment = True\n    cache = True\n\n    # Build the dataset\n    dataset = builder.build(train_csv, cache_dir, batch_size, num_chunks_per_song, augment, cache)\n\n    return dataset", "CodeBleu_score": 35.872862517068214}, {"code": "from typing import Dict, Any\nfrom some_library import AudioAdapter, DatasetBuilder # Import your AudioAdapter and DatasetBuilder\n\ndef get_training_dataset(\n    audio_params: Dict, \n    audio_adapter: AudioAdapter, \n    audio_path: str\n) -> Any:\n    \"\"\"\n    The function creates a training dataset using provided audio parameters, an audio adapter, and an audio path.\n    It initializes a DatasetBuilder with these parameters, specifying chunk duration and random seed.\n    The function then calls the build method of the builder with additional parameters like train_csv path, \n    cache directory, batch size, number of chunks per song, and options for data augmentation and caching.\n    Finally, it returns the built dataset.\n    \n    Args:\n    audio_params (Dict): A dictionary containing parameters for audio processing such as 'chunk_duration' and 'random_seed'.\n    audio_adapter (AudioAdapter): An object capable of handling audio input/output operations.\n    audio_path (str): The path to the directory or file where the audio data is stored.\n\n    Returns:\n    Any: The training dataset built by the DatasetBuilder.\n    \"\"\"\n\n    # Extract necessary parameters from audio_params, or set defaults\n    chunk_duration = audio_params.get('chunk_duration', 5)  # in seconds\n    random_seed = audio_params.get('random_seed', 42)\n    train_csv_path = audio_params.get('train_csv_path', 'path/to/train.csv')\n    cache_directory = audio_params.get('cache_directory', 'path/to/cache')\n    batch_size = audio_params.get('batch_size', 32)\n    num_chunks_per_song = audio_params.get('num_chunks_per_song', 10)\n    augment = audio_params.get('augment', False)\n    enable_caching = audio_params.get('enable_caching', True)\n\n    # Initialize the DatasetBuilder with the extracted parameters\n    builder = DatasetBuilder(\n        audio_adapter=audio_adapter,\n        chunk_duration=chunk_duration,\n        random_seed=random_seed,\n        audio_path=audio_path\n    )\n\n    # Build the dataset with additional parameters\n    dataset = builder.build(\n        train_csv=train_csv_path,\n        cache_dir=cache_directory,\n        batch_size=batch_size,\n        num_chunks_per_song=num_chunks_per_song,\n        augment=augment,\n        enable_caching=enable_caching\n    )\n\n    # Return the constructed dataset\n    return dataset", "CodeBleu_score": 41.09811274616775}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "class InstrumentDatasetBuilder:\n    def __init__(self, name):\n        self.name = name\n    \n    def build_dataset(self):\n        print(f\"Building dataset for {self.name}\")\n\nclass YourClass:\n    def __init__(self):\n        self.instruments = [\"Guitar\", \"Piano\", \"Drums\"]\n        self.instrument_builders = {}\n    \n    def instruments(self) -> Any:\n        \"\"\"\n        The function initializes instrument dataset builders if they are not already created.\n        It iterates over a list of instruments, creating and storing an InstrumentDatasetBuilder for each.\n        The function then yields each builder from the list of instrument builders.\n        \"\"\"\n        for instrument in self.instruments:\n            if instrument not in self.instrument_builders:\n                # Initialize and store the new builder\n                builder = InstrumentDatasetBuilder(instrument)\n                self.instrument_builders[instrument] = builder\n            else:\n                # Retrieve existing builder\n                builder = self.instrument_builders[instrument]\n            \n            yield builder\n\n# Example usage\nyc = YourClass()\nfor builder in yc.instruments():\n    builder.build_dataset()", "CodeBleu_score": 27.47181230138414}, {"code": "from typing import Iterator, List\n\nclass InstrumentDatasetBuilder:\n    def __init__(self, instrument_name: str):\n        self.instrument_name = instrument_name\n        print(f\"Initialized builder for {instrument_name}\")\n\nclass InstrumentManager:\n    def __init__(self):\n        self.instrument_builders = {}\n        self.instruments_list = [\"piano\", \"violin\", \"guitar\", \"flute\"]  # Sample list of instruments\n\n    def instruments(self) -> Iterator[InstrumentDatasetBuilder]:\n        \"\"\"\n        Iterates over a list of instruments, creating and storing an InstrumentDatasetBuilder for each.\n        Yields each builder from the list of instrument builders.\n        \"\"\"\n        for instrument in self.instruments_list:\n            if instrument not in self.instrument_builders:\n                # Create a new builder if not exists\n                self.instrument_builders[instrument] = InstrumentDatasetBuilder(instrument)\n            yield self.instrument_builders[instrument]\n\n# Example usage:\nmanager = InstrumentManager()\nfor builder in manager.instruments():\n    print(f\"Processing {builder.instrument_name}\")", "CodeBleu_score": 28.524333671296326}, {"code": "from typing import Any, Generator\n\nclass InstrumentDatasetBuilder:\n    \"\"\"Placeholder class for building instrument datasets.\"\"\"\n    def __init__(self, instrument):\n        self.instrument = instrument\n        # Presuming more initialization code could be here.\n        \nclass InstrumentManager:\n    def __init__(self):\n        self.instruments = ['Guitar', 'Piano', 'Violin']  # Example list of instruments\n        self.instrument_builders = {}\n    \n    def instruments(self) -> Generator[InstrumentDatasetBuilder, None, None]:\n        \"\"\"\n        The function initializes instrument dataset builders if they are not already created.\n        It iterates over a list of instruments, creating and storing an InstrumentDatasetBuilder\n        for each. The function then yields each builder from the list of instrument builders.\n        \"\"\"\n        for instrument in self.instruments:\n            if instrument not in self.instrument_builders:\n                # Create and store a new builder for this instrument\n                self.instrument_builders[instrument] = InstrumentDatasetBuilder(instrument)\n                \n            # Yield the builder for this instrument\n            yield self.instrument_builders[instrument]", "CodeBleu_score": 27.25404440544903}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "import os\nimport time\nfrom typing import Any, Optional\n\ndef cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any:\n    \"\"\"The function caches a dataset if a cache path is provided.\n    If wait is True, it repeatedly checks for the existence of the cache index file, logging a message and waiting for a specified period if not found.\n    It ensures the cache directory exists, then caches the dataset to the specified path.\n    If no cache path is provided, it returns the original dataset.\n\n    Args:\n        dataset (Any): The dataset to cache.\n        cache (Optional[str]): The path to cache the dataset, or None if no caching is desired.\n        wait (bool): If True, waits for an index file to appear in the cache directory.\n\n    Returns:\n        Any: The cached or original dataset depending on whether a cache path was provided.\n    \"\"\"\n    if cache is None:\n        return dataset\n\n    # Ensure the cache directory exists\n    os.makedirs(cache, exist_ok=True)\n\n    # For illustration, defining how the dataset could be cached (e.g., saved to a file)\n    cache_path = os.path.join(cache, 'dataset.pkl')\n\n    # Simulate saving the dataset to a file (in reality, use actual I/O operations such as with pickle or similar)\n    with open(cache_path, 'wb') as f:\n        import pickle\n        pickle.dump(dataset, f)\n\n    # If wait is True, check for the existence of an index file periodically\n    if wait:\n        index_file_path = os.path.join(cache, 'index.json')\n        wait_duration = 5  # in seconds\n\n        while not os.path.exists(index_file_path):\n            print(f\"Waiting for index file at {index_file_path}...\")\n            time.sleep(wait_duration)\n\n        print(\"Index file found, proceeding.\")\n    \n    return dataset", "CodeBleu_score": 46.50453896655689}, {"code": "import os\nimport time\nimport logging\n\ndef cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any:\n    \"\"\"The function caches a dataset if a cache path is provided.\n    If wait is True, it repeatedly checks for the existence of the cache index file, logging a message and waiting for a specified period if not found.\n    It ensures the cache directory exists, then caches the dataset to the specified path.\n    If no cache path is provided, it returns the original dataset.\n    \n    Args:\n    - dataset: The dataset to cache.\n    - cache: The path to cache the dataset to.\n    - wait: Whether to wait (polling) until a cache completion index file is found.\n    \n    Returns:\n    - The cached dataset or the original dataset if no cache path is provided.\n    \"\"\"\n    if cache is None:\n        return dataset\n\n    # Ensure the cache directory exists\n    cache_dir = os.path.dirname(cache)\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Cache the dataset - This would depend on the nature of `dataset`; pseudocode follows\n    cache_path = os.path.join(cache_dir, 'cached_data.pkl')\n    with open(cache_path, 'wb') as f:\n        pickle.dump(dataset, f)\n\n    # If wait is True, repeatedly check for the existence of the cache index file\n    if wait:\n        cache_index_path = os.path.join(cache_dir, 'cache_index')\n        while not os.path.exists(cache_index_path):\n            logging.info(f\"Waiting for the cache index file at {cache_index_path}...\")\n            time.sleep(5)  # Wait for 5 seconds before checking again\n\n    return dataset  # Returning either the updated dataset or a marker to indicate cached data.", "CodeBleu_score": 44.018222985623076}, {"code": "import os\nimport time\nimport logging\nfrom typing import Any, Optional\n\ndef cache(self, dataset: Any, cache_path: Optional[str], wait: bool) -> Any:\n    \"\"\"\n    The function caches a dataset if a cache path is provided.\n    If wait is True, it repeatedly checks for the existence of the cache index file, logging a message and \n    waiting for a specified period if not found.\n    It ensures the cache directory exists, then caches the dataset to the specified path.\n    If no cache path is provided, it returns the original dataset.\n    \"\"\"\n    if not cache_path:\n        return dataset\n\n    # Ensure the cache directory exists\n    os.makedirs(os.cache_path, exist_ok=True)\n\n    # Cache the dataset to the specified path\n    cache_file_path = os.path.join(cache_path, \"dataset_cache.pkl\")\n    with open(cache_file_path, \"wb\") as f:\n        pickle.dump(dataset, f)\n\n    if wait:\n        # Check for the existence of a cache index file\n        index_file_path = os.path.join(cache_path, \"index.txt\")\n        while not os.path.exists(index_file_path):\n            # Log message and sleep if index file doesn't exist\n            logging.info(f\"Waiting for index file at {index_file_path}...\")\n            time.sleep(5)  # wait for 5 seconds before checking again\n\n    return dataset", "CodeBleu_score": 43.02470557225371}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "import tensorflow as tf\n\ndef create_estimator(params: dict, MWF: bool) -> tf.estimator.Estimator:\n    \"\"\"\n    The function creates a TensorFlow estimator using provided parameters and a Model Weight File (MWF) flag.\n    It loads the model directory using a ModelProvider and updates the parameters.\n    It sets up the session configuration to limit GPU memory usage and creates a RunConfig with this session configuration.\n    Finally, it initializes and returns a TensorFlow estimator using the specified model function, model directory, parameters, and configuration.\n    \"\"\"\n    # Control GPU memory usage by the TensorFlow session\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n    \n    # Prepare the model directory and the model function\n    model_directory = \"/tmp/model\"  # Example model directory\n    if MWF:\n        model_directory += \"_mwf\"  # Differentiate directory based on Model Weight File usage\n    \n    # A simple example model function using TensorFlow\n    def model_fn(features, labels, mode, params):\n        # Example Linear Classifier\n        net = tf.feature_column.input_layer(features, params['feature_columns'])\n        logits = tf.keras.layers.Dense(1)(net)\n        \n        predicted_classes = tf.argmax(logits, 1)\n        if mode == tf.estimator.ModeKeys.PREDICT:\n            predictions = {\n                'class_ids': predicted_classes[:, tf.newaxis],\n                'probabilities': tf.nn.softmax(logits),\n                'logits': logits,\n            }\n            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n        \n        # Compute loss.\n        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(labels, logits)\n        \n        # Compute evaluation metrics.\n        accuracy = tf.metrics.accuracy(labels=labels, predictions=predicted_classes, name='acc_op')\n        metrics = {'accuracy': accuracy}\n        tf.summary.scalar('accuracy', accuracy[1])\n        \n        if mode == tf.estimator.ModeKeys.EVAL:\n            return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n        \n        # Create training op.\n        assert mode == tf.estimator.ModeKeys.TRAIN\n        optimizer = tf.optimizers.Adam(learning_rate=0.01)\n        train_op = optimizer.minimize(loss, global_step=tf.compat.v1.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    # Setup the RunConfig for the estimator\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n\n    # Create the estimator\n    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_directory, config=run_config, params=params)\n    \n    return estimator\n\n# Example use\nparams = {\n    'feature_columns': [tf.feature_column.numeric_column(\"x\", shape=(1,))]\n}\nestimator = create_estimator(params, MWF=True)", "CodeBleu_score": 40.86402398461164}, {"code": "import tensorflow as tf\n\ndef create_estimator(params: dict, MWF: bool) -> tf.estimator.Estimator:\n    \"\"\"\n    The function creates a TensorFlow estimator using provided parameters and a Model Weight File (MWF) flag.\n    It loads the model directory using a ModelProvider and updates the parameters.\n    It sets up the session configuration to limit GPU memory usage and creates a RunConfig with this session configuration.\n    Finally, it initializes and returns a TensorFlow estimator using the specified model function, model directory, parameters, and configuration.\n    \"\"\"\n\n    # Placeholder assumption: ModelProvider.get_model_dir() - This would be an external function\n    # Since ModelProvider doesn't actually exist in TensorFlow, this part is hypothetical\n    model_directory = \"path/to/model/directory\"\n    if MWF:\n        model_directory = \"path/to/mwf/specific/model/directory\"  # Updating the model directory based on MWF flag\n    \n    # Update parameters (example of modifying parameters)\n    params['batch_size'] = 32  # Just a sample update, in reality, this needs to be contextually relevant\n\n    # TensorFlow session configuration to limit GPU memory usage\n    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.75)\n    session_config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n\n    # Creating a RunConfig with the session configuration\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    \n    # Assume a simple model function for the estimator\n    def model_fn(features, labels, mode):\n        # Here, you should define the logic for your model\n        logits = tf.layers.dense(features['x'], 10, activation=None)\n        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n        train_op = tf.train.AdamOptimizer().minimize(loss, global_step=tf.train.get_or_create_global_step())\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n\n    # Initializing and returning TensorFlow Estimator\n    estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_directory, config=run_config, params=params)\n    return estimator", "CodeBleu_score": 41.0731936435078}, {"code": "import tensorflow as tf\nfrom typing import Dict\n\ndef create_estimator(params: Dict, MWF: bool) -> tf.estimator.Estimator:\n    \"\"\"\n    Creates a TensorFlow estimator using provided parameters and a Model Weight File (MWF) flag.\n    It loads the model directory using a ModelProvider and updates the parameters.\n    It sets up the session configuration to limit GPU memory usage and creates a RunConfig,\n    with this session configuration.\n    Finally, it initializes and returns a TensorFlow estimator using the specified model function,\n    model directory, parameters, and configuration.\n    \"\"\"\n    \n    # Example model function (you should replace this with the actual model function)\n    def model_fn(features, labels, mode, params):\n        # Implement your machine learning model here\n        pass\n\n    # Prepare the model directory\n    model_directory = \"path_to_models/\" + (\"use_mwf\" if MWF else \"do_not_use_mwf\")\n    \n    # Optionally, update parameters (customize based on actual requirements)\n    # params[\"new_key\"] = \"new_value\"\n    \n    # Configure GPU options to limit memory usage\n    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.5)\n    session_config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n    \n    # Use the session configuration in a RunConfig\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n\n    # Initialize the TensorFlow Estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_directory,\n        params=params,\n        config=run_config\n    )\n    \n    return estimator", "CodeBleu_score": 49.87445583845304}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "import numpy as np\nfrom typing import Dict\n\nclass AudioDescriptor:\n    # Assuming this class has been defined elsewhere in the code\n    pass\n\nclass AudioSeparator:\n    def __init__(self):\n        # Instantiate your TensorFlow model or setup here\n        self.model = self.initialize_model()\n\n    def initialize_model(self):\n        # Load or define your TensorFlow model\n        import tensorflow as tf\n        # Placeholder for model definition\n        model = tf.keras.models.load_model('path_to_your_model')\n        return model\n\n    def _ensure_stereo(self, waveform: np.ndarray) -> np.ndarray:\n        if waveform.ndim == 1:\n            # If mono, duplicate the channel\n            waveform = np.stack((waveform, waveform), axis=-1)\n        elif waveform.shape[1] == 1:\n            # If the waveform is two-dimensional but with one channel\n            waveform = np.concatenate([waveform, waveform], axis=1)\n        return waveform\n\n    def _separate_tensorflow(self, waveform: np.ndarray, audio_descriptor: AudioDescriptor) -> Dict:\n        # Ensure the waveform is stereo\n        waveform = self._ensure_stereo(waveform)\n\n        # Assuming a model and data preparation method already exists\n        data = waveform  # Placeholder for actual data prep\n        prediction = self.model.predict(data)\n\n        # Assuming the result is a dictionary and has an 'audio_id' key\n        prediction.pop('audio_id', None)  # Safe removal of 'audio_id' key if it exists\n        \n        return prediction\n\n# Usage\n# audio_descriptor = AudioDescriptor(...)\n# separator = AudioSeparator()\n# waveform = np.random.randn(10000, 1)  # Random mono waveform for example\n# result = separator._separate_tensorflow(waveform, audio_descriptor)\n# print(result)", "CodeBleu_score": 37.596651201622045}, {"code": "import numpy as np\nfrom typing import Dict\n\nclass AudioSeparator:\n    def __init__(self, model):\n        self.model = model\n    \n    def _separate_tensorflow(\n            self, waveform: np.ndarray, audio_descriptor\n        ) -> Dict:\n        \"\"\"\n        The function performs audio source separation using TensorFlow.\n        It ensures the input waveform is stereo, then obtains a prediction generator.\n        The data generator is updated with the waveform and audio descriptor.\n        A prediction is generated, and the \"audio_id\" key is removed from the prediction before returning it.\n\n        Args:\n            waveform (np.ndarray): The input waveform, expected to be a stereo (2-channel) waveform.\n            audio_descriptor (AudioDescriptor): An object containing metadata or additional information for processing.\n        \n        Returns:\n            Dict: Resulting predictions from the model without the 'audio_id'.\n        \"\"\"\n        # Ensure waveform is stereo\n        if waveform.ndim != 2 or waveform.shape[1] != 2:\n            raise ValueError(\"Input waveform must be a stereo (2-channel) audio.\")\n\n        # Preparing the input for the model, this could be specific to how your model expects input\n        waveform_input = self._process_waveform_for_model(waveform, audio_descriptor)\n\n        # Get predictions\n        predictions = self.model.predict(waveform_input)\n\n        # Assume predictions is a dictionary and remove 'audio_id'\n        predictions.pop('audio_id', None)\n\n        return predictions\n\n    def _process_waveform_for_model(self, waveform, audio_descriptor):\n        # This function should transform the audio as required by your TensorFlow model. E.g., normalizing, shaping, etc.\n        # Placeholder for real processing\n        return np.expand_dims(waveform, axis=0)  # Example if model expects (batch, time, channels)\n\n# Example use (assuming you have implemented all needed parts):\n# audio_descriptor = AudioDescriptor(...) # Your implementation of AudioDescriptor.\n# model = load_your_tensorflow_model_function()\n# separator = AudioSeparator(model)\n# waveform = np.random.rand(100000, 2)  # Example stereo waveform\n# result = separator._separate_tensorflow(waveform, audio_descriptor)\n# print(result)", "CodeBleu_score": 35.84247605038248}, {"code": "import numpy as np\nimport tensorflow as tf\nfrom typing import Dict\n\nclass AudioDescriptor:\n    # Just a placeholder for the actual class definition\n    pass\n\nclass AudioSeparator:\n    def __init__(self, model):\n        self.model = model\n\n    def _separate_tensorflow(self, waveform: np.ndarray, audio_descriptor: AudioDescriptor) -> Dict:\n        \"\"\"The function performs audio source separation using TensorFlow.\n        It ensures the input waveform is stereo, then obtains a prediction generator.\n        The data generator is updated with the waveform and audio descriptor.\n        A prediction is generated, and the \"audio_id\" key is removed from the prediction before returning it.\n        \"\"\"\n        # Ensure the waveform is stereo (2 channels)\n        if waveform.ndim == 1 or waveform.shape[0] != 2:\n            raise ValueError(\"Input waveform is not stereo.\")\n        \n        # We assume the audio descriptor might contain additional pre-processing details\n        # This can be used if our model.generators require such a specification\n        # Suppose we have a method in our model or elsewhere to create a data generator for predictions\n        generator = self.create_generator(waveform, audio_descriptor)\n        \n        # Predict using the TensorFlow model\n        predictions = self.model.predict(generator)\n        \n        # Assume predictions include an 'audio_id' that we do not need to return\n        if 'audio_id' in predictions:\n            del predictions['audio_id']\n        \n        return predictions\n\n    def create_generator(self, waveform, audio_descriptor):\n        # This should be tailored based on how the model expects input data\n        # For the sake of illustration, assuming the generator just needs the waveform reshaped\n        return waveform.reshape((1, -1, 2))  # Example: reshape to (batch_size, samples, channels)\n\n# Dummy example usage, assuming model exists\n# model = tf.keras.models.load_model('path_to_saved_model')\n# audio_separator = AudioSeparator(model)\n# waveform = np.random.randn(2, 32000)  # Example stereo waveform\n# audio_descriptor = AudioDescriptor()  # Assuming some attributes are set that model uses\n# separated_audio = audio_separator._separate_tensorflow(waveform, audio_descriptor)\n# print(separated_audio)", "CodeBleu_score": 38.47703254256267}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape):\n    \"\"\"\n    The function computes the shapes of feature maps at different stages of a backbone network given an image shape and configuration.\n    If the backbone is a callable function, it uses the configuration's COMPUTE_BACKBONE_SHAPE method to get the shapes.\n    Otherwise, it asserts that the backbone is either \"resnet50\" or \"resnet101\" and calculates the shapes based on the BACKBONE_STRIDES configuration using the image dimensions divided by each stride.\n    \n    Args:\n    config (object): A configuration object which must contain either a method to compute the backbone shapes or attributes specifying the backbone type and strides.\n    image_shape (tuple): The shape of the input image in (height, width, channels) format.\n    \n    Returns:\n    list: A list of tuples representing the shape of the feature maps at each stage of the backbone.\n    \"\"\"\n\n    # Check if there's a callable backbone definition in the configuration\n    if hasattr(config, \"COMPUTE_BACKBONE_SHAPE\") and callable(config.COMPUTE_BACKBONE_SHAPE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    # Otherwise, expect a static backbone type and use pre-defined stride settings to compute feature map shapes\n    backbone = getattr(config, \"BACKBONE\", None)\n    assert backbone in [\"resnet50\", \"resnet101\"], \"Unsupported backbone type\"\n\n    backbone_strides = getattr(config, \"BACKBONE_STRIDES\", None)\n    assert backbone_strides is not None, \"BACKBONE_STRIDES must be defined in the config\"\n\n    # Assuming image_shape in the form (height, width, channels)\n    height, width, _ = image_shape\n\n    # Compute feature map shapes for each backbone stride\n    feature_shapes = []\n    for stride in backbone_strides:\n        feature_height = (height + stride - 1) // stride  # computes the ceiling of the division\n        feature_width = (width + stride - 1) // stride  # computes the ceiling of the division\n        feature_shapes.append((feature_height, feature_width))\n\n    return feature_shapes", "CodeBleu_score": 37.561296221889705}, {"code": "def compute_backbone_shapes(config, image_shape):\n    \"\"\"\n    The function computes the shapes of feature maps at different stages of a backbone network given an image shape and configuration.\n    If the backbone is a callable function, it uses the configuration's COMPUTE_BACKBONE_SHAPE method to get the shapes.\n    Otherwise, it asserts that the backbone is either \"resnet50\" or \"resnet101\" and calculates the shapes based on the BACKBONE_STRIDES configuration using the image dimensions divided by each stride.\n    \n    Parameters:\n    - config: An object containing the network configuration. It must have attributes .backbone (str or callable) and .BACKBONE_STRIDES (list of ints) and optionally .COMPUTE_BACKBONE_SHAPE (method) if .backbone is callable.\n    - image_shape: tuple of (height, width) representing the size of the input image.\n    \n    Returns:\n    - List of tuples, where each tuple represents the shape (height, width) of the feature map at each stage of the backbone.\n    \"\"\"\n    \n    if callable(config.backbone):\n        # If the backbone is a callable, use the provided method to compute the shapes\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        # Backbone must be either resnet50 or resnet101 for predefined computation\n        assert config.backbone in [\"resnet50\", \"resnet101\"], \"Backbone must be either 'resnet50' or 'resnet101'\"\n        # Compute the shape of the feature maps across the network stages specified by strides\n        feature_shapes = []\n        for stride in config.BACKBONE_STRIDES:\n            # Calculate the output shapes by dividing the input dimensions by the stride and applying floor operation\n            feature_shape = (image_shape[0] // stride, image_shape[1] // stride)\n            feature_shapes.append(feature_shape)\n        return feature_shapes", "CodeBleu_score": 43.98788734718543}, {"code": "def compute_backbone_shapes(config, image_shape):\n    \"\"\"\n    The function computes the shapes of feature maps at different stages of a backbone network given an image shape and configuration.\n    If the backbone is a callable function, it uses the configuration's COMPUTE_BACKBONE_SHAPE method to get the shapes.\n    Otherwise, it asserts that the backbone is either \"resnet50\" or \"resnet101\" and calculates the shapes based on the BACKBONE_STRIDES configuration using the image dimensions divided by each stride.\n\n    Parameters:\n        config (object): Configuration object which must have either a COMPUTE_BACKBONE_SHAPE method or a backbone attribute (str) \n                         and BACKBONE_STRIDES list.\n        image_shape (tuple): A tuple of integers representing the shape of the input image (height, width).\n\n    Returns:\n        List of tuples: Each tuple represents the shape of the feature map at each stage of the backbone.\n    \"\"\"\n    if callable(getattr(config, 'COMPUTE_BACKBONE_SHAPE', None)):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    else:\n        assert config.backbone in [\"resnet50\", \"resnet101\"], f\"Unsupported backbone type {config.backbone}\"\n        # Generally, backbone strides are defined for different stages in the backbone\n        # For a typical ResNet, they could be [4, 8, 16, 32] corresponding to different blocks or layers\n        strides = config.BACKBONE_STRIDES\n        return [(image_shape[0] // stride, image_shape[1] // stride) for stride in strides]", "CodeBleu_score": 46.774892114684754}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "import tensorflow as tf\n\ndef clip_boxes_graph(boxes, window):\n    \"\"\"\n    The function clips bounding boxes to fit within a given window.\n    It splits the window and box coordinates into individual components.\n    Each coordinate is clipped to lie within the window boundaries using TensorFlow operations.\n    The clipped coordinates are concatenated back into a tensor and returned with a fixed shape.\n\n    Arguments:\n    boxes : Tensor of shape (num_boxes, 4) containing [y1, x1, y2, x2]\n    window : Tensor of shape (4,) containing [y1, x1, y2, x2] which represents the valid area\n    \n    Returns:\n    Tensor of shape (num_boxes, 4)\n    \"\"\"\n    \n    # Ensure window constraints are specified correctly\n    win_y1, win_x1, win_y2, win_x2 = tf.split(window, 4)\n    \n    # Unpack box coordinates\n    boxes_y1, boxes_x1, boxes_y2, boxes_x2 = tf.split(boxes, 4, axis=1)\n\n    # Clip the coordinates\n    clipped_y1 = tf.maximum(boxes_y1, win_y1)\n    clipped_x1 = tf.maximum(boxes_x1, win_x1)\n    clipped_y2 = tf.minimum(boxes_y2, win_y2)\n    clipped_x2 = tf.minimum(boxes_x2, win_x2)\n\n    # If use tf.concat, ensure dimensions match\n    clipped_boxes = tf.concat([clipped_y1, clipped_x1, clipped_y2, clipped_x2], axis=1)\n\n    return clipped_boxes", "CodeBleu_score": 45.88230074374415}, {"code": "import tensorflow as tf\n\ndef clip_boxes_graph(boxes, window):\n    \"\"\"\n    The function clips bounding boxes to fit within a given window.\n    It splits the window and box coordinates into individual components.\n    Each coordinate is clipped to lie within the window boundaries using TensorFlow operations.\n    The clipped coordinates are concatenated back into a tensor and returned with a fixed shape.\n\n    Parameters:\n    boxes (tf.Tensor): A tensor of shape (num_boxes, 4) containing the coordinates of the boxes as [y1, x1, y2, x2].\n    window (tf.Tensor): A tensor of shape (4,) containing the coordinates of the window as [win_y1, win_x1, win_y2, win_x2].\n\n    Returns:\n    tf.Tensor: A tensor of clipped box coordinates of shape (num_boxes, 4).\n    \"\"\"\n    \n    # Extract the coordinates of the boxes\n    y1, x1, y2, x2 = tf.split(boxes, num_or_size_splits=4, axis=1)\n    \n    # Extract the window boundaries\n    win_y1, win_x1, win_y2, win_x2 = tf.unstack(window)\n    \n    # Clip the coordinates of the boxes\n    y1_clipped = tf.maximum(y1, win_y1)\n    x1_clipped = tf.maximum(x1, win_x1)\n    y2_clipped = tf.minimum(y2, win_y2)\n    x2_clipped = tf.minimum(x2, win_x2)\n    \n    # Concatenate the clipped coordinates back together\n    clipped_boxes = tf.concat([y1_clipped, x1_clipped, y2_clipped, x2_clipped], axis=1)\n    \n    return clipped_boxes", "CodeBleu_score": 38.87292861127066}, {"code": "import tensorflow as tf\n\ndef clip_boxes_graph(boxes, window):\n    \"\"\"\n    The function clips bounding boxes to fit within a given window.\n    It splits the window and box coordinates into individual components.\n    Each coordinate is clipped to lie within the window boundaries using TensorFlow operations.\n    The clipped coordinates are concatenated back into a tensor and returned with a fixed shape.\n\n    Arguments:\n    boxes -- A tensor of shape (num_boxes, 4) representing bounding boxes. Each box is \n             represented as [y_min, x_min, y_max, x_max].\n    window -- A tensor of shape (4,) representing the window [y_min, x_min, y_max, x_max] \n              within which all boxes need to be clipped.\n\n    Returns:\n    clipped_boxes -- A tensor of shape (num_boxes, 4) containing the clipped boxes.\n    \"\"\"\n\n    # Extract the minimums and maximums from the provided window tensor\n    win_y_min, win_x_min, win_y_max, win_x_max = tf.unstack(window)\n    \n    # Unpack the boxes coordinates\n    boxes_y_min, boxes_x_min, boxes_y_max, boxes_x_max = tf.unstack(boxes, axis=1)\n    \n    # Clip the coordinates of the boxes to be within the window's coordinates\n    clipped_y_min = tf.maximum(boxes_y_min, win_y_min)\n    clipped_x_min = tf.maximum(boxes_x_min, win_x_min)\n    clipped_y_max = tf.minimum(boxes_y_max, win_y_max)\n    clipped_x_max = tf.minimum(boxes_x_max, win_x_max)\n\n    # Re-pack the coordinates into the final tensor for clipped boxes\n    clipped_boxes = tf.stack([clipped_y_min, clipped_x_min, clipped_y_max, clipped_x_max], axis=1)\n    \n    return clipped_boxes", "CodeBleu_score": 29.3077007128974}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "import tensorflow as tf\n\ndef rpn_graph(feature_map, anchors_per_location, anchor_stride):\n    \"\"\"\n    The function defines the Region Proposal Network (RPN) graph for object detection.\n    It applies a shared convolutional layer to the feature map, followed by two separate convolutional layers\n    for class scores and bounding box predictions.\n    The class scores are reshaped and passed through a softmax activation to obtain class probabilities.\n    The bounding box predictions are also reshaped.\n    The function returns the class logits, class probabilities, and bounding box predictions.\n    \n    Parameters:\n    - feature_map: A tensor representing the input feature map.\n    - anchors_per_location: An integer, the number of anchors per spatial location.\n    - anchor_stride: An integer, the stride of anchors on the feature map.\n    \n    Returns:\n    - class_logits: Raw scores for each class.\n    - class_prob: Softmax probability scores for each class.\n    - bbox_predictions: Bounding box proposals for each class.\n    \"\"\"\n\n    # Shared convolutional base\n    shared_map = tf.keras.layers.Conv2D(\n        filters=512, kernel_size=(3, 3), padding='same', strides=anchor_stride,\n        activation='relu', name='rpn_conv_shared')(feature_map)\n\n    # Class score layer\n    x_class = tf.keras.layers.Conv2D(\n        filters=2 * anchors_per_location, kernel_size=(1, 1), padding='valid',\n        activation='linear', name='rpn_class_raw')(shared_map)\n    class_logits = tf.reshape(x_class, [tf.shape(x_class)[0], -1, 2])\n    class_prob = tf.nn.softmax(class_logits, axis=-1)\n\n    # Bounding box regression layer\n    x_bbox = tf.keras.layers.Conv2D(\n        filters=4 * anchors_per_location, kernel_size=(1, 1), padding='valid',\n        activation='linear', name='rpn_bbox_pred')(shared_map)\n    bbox_predictions = tf.reshape(x_bbox, [tf.shape(x_bbox)[0], -1, 4])\n\n    return class_logits, class_prob, bbox_predictions", "CodeBleu_score": 49.93791841650497}, {"code": "import tensorflow as tf\n\ndef rpn_graph(feature_map, anchors_per_location, anchor_stride):\n    \"\"\"\n    The function defines the Region Proposal Network (RPN) graph for object detection.\n    It applies a shared convolutional layer to the feature map, followed by two separate convolutional layers\n    for class scores and bounding box predictions.\n    The class scores are reshaped and passed through a softmax activation to obtain class probabilities.\n    The bounding box predictions are also reshaped.\n    The function returns the class logits, class probabilities, and bounding box predictions.\n    \n    Args:\n    - feature_map (tensor): A feature map from a backbone network.\n    - anchors_per_location (int): Number of anchors per location (e.g., per pixel in the feature map).\n    - anchor_stride (int): Stride of anchors on the feature map.\n    \n    Returns:\n    - class_logits (tensor): The raw scores for each anchor before applying softmax.\n    - class_probs (tensor): The probability scores for each anchor after applying softmax.\n    - box_preds (tensor): The coordinates for each anchor box.\n    \"\"\"\n\n    # Shared convolutional layer\n    shared_layer = tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), padding='same',\n                                          strides=(anchor_stride, anchor_stride),\n                                          name='rpn_conv_shared')(feature_map)\n    \n    # Class predictor layer\n    class_logits = tf.keras.layers.Conv2D(filters=2 * anchors_per_location, kernel_size=(1, 1), \n                                          padding='valid', activation=None, \n                                          name='rpn_class_raw')(shared_layer)\n    # Reshape class predictions to [batch, anchors, 2]\n    class_logits = tf.reshape(class_logits, [tf.shape(feature_map)[0], -1, 2])\n\n    # Apply softmax to get class probabilities\n    class_probs = tf.nn.softmax(class_logits, axis=-1)\n    \n    # Bounding box prediction layer\n    box_preds = tf.keras.layers.Conv2D(filters=4 * anchors_per_location, kernel_size=(1, 1),\n                                       padding='valid', activation=None,\n                                       name='rpn_bbox_pred')(shared_layer)\n    # Reshape bounding box predictions to [batch, anchors, 4]\n    box_preds = tf.reshape(box_preds, [tf.shape(feature_map)[0], -1, 4])\n    \n    return class_logits, class_probs, box_preds", "CodeBleu_score": 43.9519105905197}, {"code": "import tensorflow as tf\n\ndef rpn_graph(feature_map, anchors_per_location, anchor_stride):\n    \"\"\"\n    The function defines the Region Proposal Network (RPN) graph for object detection.\n    It applies a shared convolutional layer to the feature map, followed by two separate convolutional layers for class scores and bounding box predictions.\n    The class scores are reshaped and passed through a softmax activation to obtain class probabilities.\n    The bounding box predictions are also reshaped.\n    The function returns the class logits, class probabilities, and bounding box predictions.\n\n    Args:\n    - feature_map (tensor): A tensor of shape (batch_size, height, width, depth) representing the feature map from the base network.\n    - anchors_per_location (int): The number of anchors per feature map location.\n    - anchor_stride (int): The stride of anchors on the feature map.\n\n    Returns:\n    - rpn_class_logits (tensor): Raw class predictions before softmax of shape (batch_size, num_anchors, 2)\n    - rpn_probs (tensor): Softmax probabilities for foreground/background.\n    - rpn_bbox (tensor): Bounding box regressions of shape (batch_size, num_anchors, 4)\n    \"\"\"\n    # Shared convolutional base of the RPN\n    shared = tf.keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map)\n\n    # Define the anchor classifier layer\n    x = tf.keras.layers.Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared)\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = tf.reshape(x, [tf.shape(x)[0], -1, 2])\n\n    # Apply softmax to obtain probabilities\n    rpn_probs = tf.keras.layers.Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n\n    # Define the bounding box regressor layer\n    x = tf.keras.layers.Conv2D(4 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_bbox_pred')(shared)\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = tf.reshape(x, [tf.shape(x)[0], -1, 4])\n    \n    return rpn_class_logits, rpn_probs, rpn_bbox", "CodeBleu_score": 60.14141192351384}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred):\n    \"\"\"\n    The function computes the Smooth L1 loss between true and predicted values.\n    It calculates the absolute difference and creates a mask for values less than one.\n    The loss is computed using a combination of squared differences for small values\n    and linear differences for large values.\n    The function returns the calculated loss.\n\n    Parameters:\n    y_true (numpy array or similar): Array of true values.\n    y_pred (numpy array or similar): Array of predicted values.\n\n    Returns:\n    float: The calculated Smooth L1 loss.\n    \"\"\"\n    import numpy as np\n    \n    # Calculate absolute difference\n    diff = np.abs(y_true - y_pred)\n    \n    # Applying the mask where the difference is small or large\n    # Using the formula:\n    # 0.5 * x^2                  if |x| < 1\n    # |x| - 0.5                  if |x| >= 1\n    loss = np.where(diff < 1, 0.5 * diff ** 2, diff - 0.5)\n    \n    # Return mean of loss to average across all elements\n    return np.mean(loss)", "CodeBleu_score": 33.54500242045251}, {"code": "def smooth_l1_loss(y_true, y_pred)\ndef smooth_l1_loss(y_true, y_pred):\n    \"\"\"\n    The function computes the Smooth L1 loss between true and predicted values.\n    It calculates the absolute difference and creates a mask for values less than one.\n    The loss is computed using a combination of squared differences for small values and linear differences for large values.\n    The function returns the calculated loss.\n    \n    Parameters:\n        y_true (float or array of floats): True values.\n        y_pred (float or array of floats): Predicted values.\n\n    Returns:\n        float or array of floats: The computed Smooth L1 loss.\n    \"\"\"\n    import numpy as np\n    \n    # Calculate the absolute difference\n    diff = np.abs(y_true - y_pred)\n\n    # Create a mask for values less than 1\n    less_than_one = diff < 1\n\n    # Calculate the Smooth L1 loss\n    loss = np.where(less_than_one, 0.5 * (diff ** 2), diff - 0.5)\n    return np.sum(loss)  # Summing up the individual losses to get one scalar loss value\n", "CodeBleu_score": 35.74753446074207}, {"code": "def smooth_l1_loss(y_true, y_pred)\ndef smooth_l1_loss(y_true, y_pred):\n    \"\"\"\n    The function computes the Smooth L1 loss between true (y_true) and predicted (y_pred) values.\n    It calculates the absolute difference and creates a mask for values less than one.\n    The loss is computed using a combination of squared differences for small values and linear differences for large values.\n    The function returns the calculated loss.\n\n    Parameters:\n        y_true (numpy.ndarray): The ground truth values.\n        y_pred (numpy.ndarray): The predicted values.\n\n    Returns:\n        numpy.ndarray: The Smooth L1 Loss computed for each element in y_true and y_pred.\n    \"\"\"\n    import numpy as np\n    \n    # Calculate absolute difference\n    abs_diff = np.abs(y_true - y_pred)\n    \n    # Determine which elements are less than 1 (small differences)\n    small_diff = abs_diff < 1\n    \n    # Calculate loss for small differences\n    # For differences < 1, use 0.5 * (difference)^2\n    loss_small = 0.5 * (abs_diff ** 2)\n    \n    # Calculate loss for large differences\n    # For differences >= 1, use (difference - 0.5)\n    loss_large = abs_diff - 0.5\n    \n    # Combine losses using the mask\n    total_loss = np.where(small_diff, loss_small, loss_large)\n    \n    return total_loss", "CodeBleu_score": 30.035474539597974}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation\n\ndef build_model(x_train, num_classes):\n    \"\"\"\n    The function builds a convolutional neural network model for image classification.\n    It starts by resetting the TensorFlow graph and then defines the input layer with the shape of the training data.\n    The model consists of two convolutional layers with ReLU activation, followed by a max-pooling layer.\n    The output is flattened and passed through a dense layer with ReLU activation, followed by a dense layer with softmax activation for classification.\n    The function returns the compiled model.\n    \"\"\"\n\n    # Resetting the TensorFlow graph (useful in a Jupyter notebook-like environment)\n    tf.keras.backend.clear_session()\n    \n    # Define the model\n    model = Sequential()\n    # Input/problem-specific shape definition (assuming x_train is not empty and has shape attribute, e.g., x_train.shape[1:])\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]))\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())  # Flatten the output to feed it into dense layers\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes))\n    model.add(Activation('softmax'))  # Softmax layer for classification\n\n    # Compile the model\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model", "CodeBleu_score": 28.758895866745178}, {"code": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\ndef build_model(x_train, num_classes):\n    \"\"\"\n    The function builds a convolutional neural network model for image classification.\n    It starts by resetting the TensorFlow graph and then defines the input layer with the shape of the training data.\n    The model consists of two convolutional layers with ReLU activation, followed by a max-pooling layer.\n    The output is flattened and passed through a dense layer with ReLU activation, followed by a dense layer with softmax activation for classification.\n    The function returns the compiled model.\n    \"\"\"\n    # Reset Keras Session\n    tf.keras.backend.clear_session()\n\n    # Define the input shape, assuming x_train is a 4D tensor [batch_size, height, width, channels]\n    input_shape = x_train.shape[1:]  # Exclude the batch dimension\n\n    # Build the Sequential model\n    model = Sequential()\n    # Convolutional layer 1\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    # Convolutional layer 2\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    # Max-pooling layer\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    # Flatten the output of the pooling layer\n    model.add(Flatten())\n    # Dense layer with ReLU activation\n    model.add(Dense(128, activation='relu'))\n    # Output layer with softmax activation to classify the inputs into 'num_classes' categories\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model with a suitable loss function, optimizer, and metrics\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model", "CodeBleu_score": 29.27277617931891}, {"code": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n\ndef build_model(x_train, num_classes):\n    \"\"\"\n    The function builds a convolutional neural network model for image classification.\n    It starts by resetting the TensorFlow graph and then defines the input layer with the shape of \n    the training data. The model consists of two convolutional layers with ReLU activation, followed by \n    a max-pooling layer. The output is flattened and passed through a dense layer with ReLU activation, \n    followed by a dense layer with softmax activation for classification.\n    The function returns the compiled model.\n    \n    Parameters:\n        x_train (numpy.ndarray): Training data used to define the input shape of the model.\n        num_classes (int): Total number of classes for classification.\n    \n    Returns:\n        tf.keras.Model: A compiled convolutional neural network model.\n    \"\"\"\n    # Reset the TensorFlow graph (important for models being initialized in a loop or script)\n    tf.keras.backend.clear_session()\n\n    # Define the model using the Sequential API\n    model = Sequential([\n        Input(shape=x_train.shape[1:]),  # input shape is all dimensions except for the batch size\n        Conv2D(32, kernel_size=(3, 3), activation='relu'),\n        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(num_classes, activation='softmax')\n    ])\n\n    # Compile the model\n    model.compile(optimizer='adam', \n                  loss='categorical_crossentropy', \n                  metrics=['accuracy'])\n\n    return model", "CodeBleu_score": 36.012975963417624}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "import numpy as np\n\ndef extract_bboxes(mask):\n    \"\"\"\n    The function extracts bounding boxes from a mask tensor.\n    It initializes an array to store bounding boxes for each mask layer.\n    For each mask layer, it calculates the horizontal and vertical indices where the mask is present.\n    It then determines the coordinates of the bounding box based on these indices and stores them in the array.\n    If no mask is present, it sets the bounding box to zeros.\n    The function returns the array of bounding boxes as integers.\n    \n    Parameters:\n        mask (numpy.ndarray): 3D numpy array where each layer represents a mask. Shape should be (layers, height, width).\n    \n    Returns:\n        numpy.ndarray: 2D numpy array containing bounding boxes. Shape (layers, 4), where each row contains [y1, x1, y2, x2].\n    \"\"\"\n    num_masks = mask.shape[0]\n    bboxes = np.zeros((num_masks, 4), dtype=int)\n\n    for i in range(num_masks):\n        pos = np.where(mask[i] > 0)\n        if len(pos[0]) == 0 or len(pos[1]) == 0:\n            # If no mask is present, store a bbox of zeros (already initialized)\n            continue\n        y_min = np.min(pos[0])\n        y_max = np.max(pos[0])\n        x_min = np.min(pos[1])\n        x_max = np.max(pos[1])\n        bboxes[i] = [y_min, x_min, y_max, x_max]\n\n    return bboxes", "CodeBleu_score": 23.567211696759323}, {"code": "import numpy as np\n\ndef extract_bboxes(mask):\n    \"\"\"\n    The function extracts bounding boxes from a mask tensor.\n    It initializes an array to store bounding boxes for each mask layer.\n    For each mask layer, it calculates the horizontal and vertical indices where the mask is present.\n    It then determines the coordinates of the bounding box based on these indices and stores them in the array.\n    If no mask is present, it sets the bounding box to zeros.\n    The function returns the array of bounding boxes as integers.\n    \n    Args:\n    mask (numpy.ndarray): A 3D numpy array where each layer [num_masks, height, width] is a separate mask.\n    \n    Returns:\n    numpy.ndarray: An array of bounding boxes with the shape [num_masks, 4]. Each bounding box is [min_y, min_x, max_y, max_x].\n    \"\"\"\n    num_masks = mask.shape[0]\n    bboxes = np.zeros((num_masks, 4), dtype=np.int32)\n    \n    for i in range(num_masks):\n        rows = np.any(mask[i], axis=1)\n        cols = np.any(mask[i], axis=0)\n        if np.any(rows) and np.any(cols):\n            min_row = np.where(rows)[0][0]\n            max_row = np.where(rows)[0][-1]\n            min_col = np.where(cols)[0][0]\n            max_col = np.where(cols)[0][-1]\n            bboxes[i] = [min_row, min_col, max_row, max_col]\n        else:\n            bboxes[i] = [0, 0, 0, 0]  # If no mask is present, set the bbox to zeros\n    \n    return bboxes", "CodeBleu_score": 32.91233151652298}, {"code": "import numpy as np\n\ndef extract_bboxes(mask):\n    \"\"\"\n    The function extracts bounding boxes from a mask tensor.\n    It initializes an array to store bounding boxes for each mask layer.\n    For each mask layer, it calculates the horizontal and vertical indices where the mask is present.\n    It then determines the coordinates of the bounding box based on these indices and stores them in the array.\n    If no mask is present, it sets the bounding box to zeros.\n    The function returns the array of bounding boxes as integers.\n    \n    :param mask: numpy array of shape (n_masks, height, width), where each layer represents a mask.\n    :return: numpy array of bounding boxes with shape (n_masks, 4), where each box is [y1, x1, y2, x2].\n    \"\"\"\n    n_masks, height, width = mask.shape\n    bboxes = np.zeros((n_masks, 4), dtype=int)\n    \n    for i in range(n_masks):\n        # Find where the mask is\n        rows = np.any(mask[i], axis=1)\n        cols = np.any(mask[i], axis=0)\n\n        if rows.any() and cols.any():\n            y1, y2 = np.where(rows)[0][0], np.where(rows)[0][-1]\n            x1, x2 = np.where(cols)[0][0], np.where(cols)[0][-1]\n            bboxes[i] = [y1, x1, y2, x2]\n        else:\n            # If no mask present, leave bbox as [0, 0, 0, 0]\n            bboxes[i] = [0, 0, 0, 0]\n    \n    return bboxes", "CodeBleu_score": 31.09245754129999}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "import numpy as np\n\ndef compute_iou(box, boxes, box_area, boxes_area):\n    \"\"\"\n    The function computes the Intersection over Union (IoU) between a given box\n    and multiple boxes.\n    It calculates the intersection areas between the box and each box in the\n    array, then computes the union areas using the provided box areas and the\n    intersection areas.\n    The IoU is obtained by dividing the intersection areas by the union areas,\n    and the function returns the IoU values.\n    \n    Parameters:\n    -----------\n    box : ndarray\n        1D array (x1, y1, x2, y2) representing the coordinates of a single box.\n    boxes : ndarray\n        2D array Nx4 where each row is (x1, y1, x2, y2) representing the coordinates of a box.\n    box_area : float\n        The area of `box`.\n    boxes_area : ndarray\n        1D array, with each element being the area of the corresponding box in `boxes`.\n\n    Returns:\n    --------\n    ious : ndarray\n        1D array containing the IoU values between `box` and each box in `boxes`.\n    \"\"\"\n    \n    # Calculate intersection coordinates\n    x1_inter = np.maximum(box[0], boxes[:, 0])\n    y1_inter = np.maximum(box[1], boxes[:, 1])\n    x2_inter = np.minimum(box[2], boxes[:, 2])\n    y2_inter = np.minimum(box[3], boxes[:, 3])\n    \n    # Calculate intersection areas\n    w_inter = np.maximum(0, x2_inter - x1_inter)\n    h_inter = np.maximum(0, y2_inter - y1_inter)\n    intersection = w_inter * h_inter\n    \n    # Calculate union areas\n    union_areas = box_area + boxes_area - intersection\n    \n    # Prevent division by zero\n    union_areas = np.where(union_areas > 0, union_areas, 1e-9)\n    \n    # Calculate IoU\n    ious = intersection / union_areas\n    \n    return ious", "CodeBleu_score": 59.04281968129247}, {"code": "import numpy as np\n\ndef compute_iou(box, boxes, box_area, boxes_area):\n    \"\"\"\n    The function computes the Intersection over Union (IoU) between a given box and multiple boxes.\n    It calculates the intersection areas between the box and each box in the array, then computes the union areas \n    using the provided box areas and the intersection areas. The IoU is obtained by dividing the intersection areas\n    by the union areas, and the function returns the IoU values.\n    \n    Args:\n    - box (np.array): The reference box, a numpy array of shape (4,) with elements [x1, y1, x2, y2]\n    - boxes (np.array): An array of boxes, each in format [x1, y1, x2, y2], shape (N, 4)\n    - box_area (float): The area of the reference box\n    - boxes_area (np.array): A numpy array containing the areas of each of the N boxes in 'boxes', shape (N,)\n\n    Returns:\n    - ious (np.array): An array of IoU values computed between 'box' and each box in 'boxes', shape (N,)\n    \"\"\"\n    \n    # Calculate intersection coordinates\n    x1_int = np.maximum(box[0], boxes[:, 0])\n    y1_int = np.maximum(box[1], boxes[:, 1])\n    x2_int = np.minimum(box[2], boxes[:, 2])\n    y2_int = np.minimum(box[3], boxes[:, 3])\n    \n    # Compute width and height of intersecting area\n    int_width = np.maximum(0, x2_int - x1_int)\n    int_height = np.maximum(0, y2_int - y1_int)\n    \n    # Compute the area of the intersecting region\n    intersection_area = int_width * int_height\n\n    # Compute union area\n    union_area = box_area + boxes_area - intersection_area\n    \n    # Compute IoU\n    ious = intersection_area / union_area\n    \n    return ious", "CodeBleu_score": 59.01722267208681}, {"code": "import numpy as np\n\ndef compute_iou(box, boxes, box_area, boxes_area):\n    \"\"\"The function computes the Intersection over Union (IoU) between a given box and multiple boxes.\n    It calculates the intersection areas between the box and each box in the array, then computes the union areas\n    using the provided box areas and the intersection areas.\n    The IoU is obtained by dividing the intersection areas by the union areas, and the function returns the IoU values.\"\"\"\n    \n    # Extract coordinates for easier reference\n    x1, y1, x2, y2 = box\n    \n    # Extract coordinates for the boxes\n    boxes_x1 = boxes[:, 0]\n    boxes_y1 = boxes[:, 1]\n    boxes_x2 = boxes[:, 2]\n    boxes_y2 = boxes[:, 3]\n\n    # Calculate intersection coordinates\n    inter_x1 = np.maximum(x1, boxes_x1)\n    inter_y1 = np.maximum(y1, boxes_y1)\n    inter_x2 = np.minimum(x2, boxes_x2)\n    inter_y2 = np.minimum(y2, boxes_y2)\n\n    # Calculate intersection width and height (masking negative values where intersection doesn't exist)\n    inter_w = np.maximum(0, inter_x2 - inter_x1)\n    inter_h = np.maximum(0, inter_y2 - inter_y1)\n\n    # Calculate intersection area\n    intersection_area = inter_w * inter_h\n\n    # Calculate union area\n    union_area = box_area + boxes_area - intersection_area\n\n    # Calculate IoU\n    iou = intersection_area / union_area\n\n    # Protect against division by zero and return IoU values\n    return np.where(union_area > 0, iou, 0.0)", "CodeBleu_score": 31.32032775636533}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2):\n    \"\"\"The function computes overlap areas between two sets of bounding boxes.\n    It first calculates the area of each box in both sets.\n    It then initializes an overlap matrix and iterates through each box in the second set,\n    computing the Intersection over Union (IoU) with all boxes in the first set.\n    The function returns the matrix of overlaps.\n\n    Args:\n    - boxes1 (list of [x1, y1, x2, y2]): List of bounding boxes.\n    - boxes2 (list of [x1, y1, x2, y2]): List of bounding boxes.\n\n    Returns:\n    - List[List[float]]: Matrix of IoU values.\n    \"\"\"\n    \n    def calculate_area(box):\n        \"\"\"Calculate area of a bounding box\"\"\"\n        x1, y1, x2, y2 = box\n        return max(0, x2 - x1) * max(0, y2 - y1)\n    \n    def calculate_iou(box1, box2):\n        \"\"\"Calculate the Intersection over Union (IoU) of two bounding boxes.\"\"\"\n        x1_inter = max(box1[0], box2[0])\n        y1_inter = max(box1[1], box2[1])\n        x2_inter = min(box1[2], box2[2])\n        y2_inter = min(box1[3], box2[3])\n        width_inter = max(0, x2_inter - x1_inter)\n        height_inter = max(0, y2_inter - y1_inter)\n        area_inter = width_inter * height_inter\n\n        area_box1 = calculate_area(box1)\n        area_box2 = calculate_area(box2)\n        \n        area_union = area_box1 + area_box2 - area_inter\n        \n        return area_inter / area_union if area_union != 0 else 0\n    \n    num_boxes1 = len(boxes1)\n    num_boxes2 = len(boxes2)\n    overlap_matrix = [[0.0]*num_boxes1 for _ in range(num_boxes2)]\n    \n    for j in range(num_boxes2):\n        for i in range(num_boxes1):\n            overlap_matrix[j][i] = calculate_iou(boxes1[i], boxes2[j])\n    \n    return overlap_matrix", "CodeBleu_score": 17.987745560088392}, {"code": "def compute_overlaps(boxes1, boxes2):\n    \"\"\"\n    The function computes overlap areas between two sets of bounding boxes.\n    It first calculates the area of each box in both sets.\n    It then initializes an overlap matrix and iterates through each box in the second set,\n    computing the Intersection over Union (IoU) with all boxes in the first set.\n    The function returns the matrix of overlaps.\n\n    Parameters:\n    - boxes1 (ndarray): An array of bounding boxes of shape (N, 4), each with [y1, x1, y2, x2]\n    - boxes2 (ndarray): An array of bounding boxes of shape (M, 4), each with [y1, x1, y2, x2]\n\n    Returns:\n    - ndarray: A matrix of shape (N, M) containing IoU values.\n    \"\"\"\n    import numpy as np\n\n    N = boxes1.shape[0]\n    M = boxes2.shape[0]\n    overlaps = np.zeros((N, M))\n\n    for i in range(N):\n        for j in range(M):\n            # Unpack the coordinates of the boxes\n            y1A, x1A, y2A, x2A = boxes1[i]\n            y1B, x1B, y2B, x2B = boxes2[j]\n\n            # Calculate intersection coordinates\n            x_left = max(x1A, x1B)\n            y_top = max(y1A, y1B)\n            x_right = min(x2A, x2B)\n            y_bottom = min(y2A, y2B)\n\n            # Check if there is any intersection\n            if x_right > x_left and y_bottom > y_top:\n                intersection_area = (x_right - x_left) * (y_bottom - y_top)\n            else:\n                intersection_area = 0\n\n            # Calculate union area\n            areaA = (x2A - x1A) * (y2A - y1A)\n            areaB = (x2B - x1B) * (y2B - y1B)\n            union_area = areaA + areaB - intersection_area\n            \n            # Compute IoU\n            if union_area != 0:\n                iou = intersection_area / union_area\n            else:\n                iou = 0\n            \n            # Store IoU in overlaps matrix\n            overlaps[i][j] = iou\n\n    return overlaps", "CodeBleu_score": 18.946144147073536}, {"code": "import numpy as np\n\ndef compute_overlaps(boxes1, boxes2):\n    \"\"\"\n    The function computes overlap areas between two sets of bounding boxes.\n    It first calculates the area of each box in both sets.\n    It then initializes an overlap matrix and iterates through each box in the second set,\n    computing the Intersection over Union (IoU) with all boxes in the first set.\n    The function returns the matrix of overlaps.\n    \n    Parameters:\n    - boxes1 (array-like): A list of bounds [x1, y1, x2, y2] representing each box in the first set.\n    - boxes2 (array-like): A list of bounds [x1, y1, x2, y2] representing each box in the second set.\n\n    Returns:\n    - np.ndarray: A matrix where element (i, j) is the IoU between boxes1[i] and boxes2[j].\n    \"\"\"\n    num_boxes1 = len(boxes1)\n    num_boxes2 = len(boxes2)\n    \n    # Calculate the area of each box in both sets\n    area1 = [(box[2] - box[0]) * (box[3] - box[1]) for box in boxes1]\n    area2 = [(box[2] - box[0]) * (box[3] - box[1]) for box in boxes2]\n    \n    # Initialize the matrix to hold IoU values\n    overlaps = np.zeros((num_boxes1, num_boxes2), dtype=np.float32)\n    \n    # Iterate through each box in both sets to compute the IoU\n    for i in range(num_boxes1):\n        for j in range(num_boxes2):\n            # Find the intersection boundaries\n            x1 = max(boxes1[i][0], boxes2[j][0])\n            y1 = max(boxes1[i][1], boxes2[j][1])\n            x2 = min(boxes1[i][2], boxes2[j][2])\n            y2 = min(boxes1[i][3], boxes2[j][3])\n            \n            # Compute the area of the intersection rectangle\n            if x2 > x1 and y2 > y1:\n                intersection_area = (x2 - x1) * (y2 - y1)\n                # Calculate union area using inclusion-exclusion principle\n                union_area = area1[i] + area2[j] - intersection_area\n                # Calculate IoU by dividing the area of the intersection by the union\n                overlaps[i][j] = intersection_area / union_area\n            else:\n                overlaps[i][j] = 0\n    \n    return overlaps", "CodeBleu_score": 21.882296932572558}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "from PIL import Image\n\ndef load_image(self, image_id):\n    \"\"\"\n    The function loads an image using its ID from the image information dictionary.\n    It reads the image from the specified path and converts it to RGB if it is not already in RGB format.\n    If the image has four channels, it removes the alpha channel, returning only the first three channels.\n    The function then returns the processed image.\n    \"\"\"\n    # Get image path from image dictionary using the image_id\n    image_path = self.image_info[image_id]['path']\n    \n    # Open the image file using PIL\n    image = Image.open(image_path)\n    \n    # Convert image to RGB, discarding alpha channel if present\n    if image.mode in ['RGBA', 'LA'] or (image.mode == 'P' and 'transparency' in image.info):\n        image = image.convert('RGB')\n    \n    return image", "CodeBleu_score": 39.18116369739546}, {"code": "from PIL import Image\n\nclass ImageHandler:\n    def __init__(self):\n        # Sample image information dictionary\n        self.image_info = {\n            '1': 'path/to/image1.png',\n            '2': 'path/to/image2.jpg',\n            '3': 'path/to/image3.png',\n            # Add other image IDs and paths as needed\n        }\n\n    def load_image(self, image_id):\n        \"\"\"\n        The function loads an image using its ID from the image information dictionary.\n        It reads the image from the specified path and converts it into RGB if it is not already in RGB format.\n        If the image has four channels (i.e., RGBA), it removes the alpha channel, returning only the first three channels.\n        The function then returns the processed image as a PIL Image object.\n        \n        Args:\n            image_id (str): The identifier for the image to load.\n\n        Returns:\n            Image: The processed PIL Image object in RGB format without an alpha channel.\n        \"\"\"\n        # Retrieve the image path from the dictionary using the provided image_id\n        image_path = self.image_info.get(image_id)\n        if not image_path:\n            raise ValueError(f\"No image found for ID {image_id}\")\n        \n        # Load the image with PIL\n        image = Image.open(image_path)\n        \n        # Ensure the image is in RGB\n        if image.mode not in ['RGB', 'RGBA']:\n            image = image.convert('RGB')\n        elif image.mode == 'RGBA':\n            # Drop Alpha channel\n            image = image.convert('RGB')\n        \n        return image\n\n# Example usage:\n# image_handler = ImageHandler()\n# image = image_handler.load_image('1')\n# image.show()  # This would display the image, for demonstration purposes", "CodeBleu_score": 33.61412829115235}, {"code": "from PIL import Image\n\ndef load_image(self, image_id):\n    \"\"\"\n    The function loads an image using its ID from the image information dictionary.\n    It reads the image from the specified path and converts it to RGB if it is not\n    already in RGB format. If the image has four channels, it removes the alpha\n    channel, returning only the first three channels. The function then returns\n    the processed image.\n    \n    Args:\n    image_id (str): The unique identifier for the image to load.\n    \n    Returns:\n    Image.Image: The image object in RGB format with three channels.\n    \"\"\"\n    # Assuming there is a dictionary called `self.image_info` that contains image paths indexed by image_ids\n    image_path = self.image_info.get(image_id)\n    if image_path is None:\n        raise ValueError(\"Image ID not found in the image information.\")\n\n    # Open the image file using PIL\n    with Image.open(image_path) as img:\n        # Ensure the image is in RGB format\n        if img.mode not in ['RGB', 'RGBA']:\n            img = img.convert('RGB')\n        elif img.mode == 'RGBA':\n            # Drop the alpha channel if present\n            img = img.convert('RGB')\n    \n    return img", "CodeBleu_score": 30.73174876535087}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\nimport matplotlib.pyplot as plt\n\ndef display_images(images, titles=None, cols=4, cmap=None, norm=None, interpolation=None):\n    \"\"\"\n    The function displays a list of images with optional titles in a grid format.\n    It sets default titles if none are provided and calculates the number of rows needed based on the number of images and columns.\n    It creates a figure and iterates over the images and titles, displaying each image in a subplot with its corresponding title.\n    The function sets display parameters like colormap, normalization, and interpolation before showing the figure.\n    \n    Parameters:\n    - images (list): List of image matrices compatible with matplotlib.imshow (typically numpy arrays).\n    - titles (list, optional): A list of strings to use as titles for each image. If None, default titles are set.\n    - cols (int, optional): Number of columns in the image grid display.\n    - cmap (str or Colormap, optional): Colormap to be used for displaying the images. If None, default colormap is used.\n    - norm (Normalize, optional): An instance of matplotlib.colors.Normalize to normalize the luminance data. If None, default normalization is applied.\n    - interpolation (str, optional): Interpolation used in plt.imshow. If None, no interpolation is applied.\n    \"\"\"\n    num_images = len(images)\n    num_rows = (num_images + cols - 1) // cols  # Calculate the sufficient number of rows to hold all images\n    \n    if titles is None:\n        titles = [f\"Image {i+1}\" for i in range(num_images)]\n    \n    fig, axes = plt.subplots(nrows=num_rows, ncols=cols, figsize=(cols * 3, num_rows * 3))\n    \n    if num_rows == 0:\n        axes = []\n    elif num_rows == 1:\n        axes = [axes]  # ensure axes is iterable\n\n    for ax, image, title in zip(axes.flat, images, titles):\n        ax.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        ax.set_title(title)\n        ax.axis('off')  # Hide axis\n\n    # Turn off empty subplots\n    for ax in axes.flat[num_images:]:\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()  # Display the figure\n\n# Example usage in practice after defining the function:\n# display_images([image1, image2, image3], titles=['First', 'Second', 'Third'], cols=2)", "CodeBleu_score": 27.48727006775859}, {"code": "import matplotlib.pyplot as plt\nimport math\n\ndef display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None):\n    \"\"\"\n    The function displays a list of images with optional titles in a grid format.\n    It sets default titles if none are provided and calculates the number of rows\n    needed based on the number of images and columns.\n    It creates a figure and iterates over the images and titles, displaying each\n    image in a subplot with its corresponding title.\n    The function sets display parameters like colormap, normalization, and interpolation\n    before showing the figure.\n\n    Args:\n    images (list of np.ndarray): List of images to display.\n    titles (list of str, optional): Titles for each image. Defaults to None.\n    cols (int, optional): Number of columns in the image grid. Defaults to 4.\n    cmap (str, optional): Colormap used for the images. Defaults to None.\n    norm (Normalize, optional): Normalization used for the images. Defaults to None.\n    interpolation (str, optional): Interpolation used in imshow. Defaults to None.\n\n    Returns:\n    None\n    \"\"\"\n    # Determine number of rows needed\n    num_images = len(images)\n    rows = math.ceil(num_images / cols)\n    \n    if titles is None:\n        titles = ['Image {}'.format(i + 1) for i in range(num_images)]\n    \n    # Create a figure with subplots\n    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n    axes = axes.ravel()  # Flatten the array of axes\n    \n    # Loop over all provided images and respective titles\n    for idx, (img, title) in enumerate(zip(images, titles)):\n        ax = axes[idx]\n        ax.imshow(img, cmap=cmap, norm=norm, interpolation=interpolation)\n        ax.set_title(title)\n        ax.axis('off')  # Turn off axis\n        \n    # Turn off axes for any remaining unused subplots\n    for ax in axes[num_images:]:\n        ax.axis('off')\n        \n    plt.tight_layout()\n    plt.show()", "CodeBleu_score": 37.281613221696716}, {"code": "import matplotlib.pyplot as plt\n\ndef display_images(images, titles=None, cols=4, cmap=None, norm=None, interpolation=None):\n    \"\"\"\n    The function displays a list of images with optional titles in a grid format.\n    It sets default titles if none are provided and calculates the number of rows needed based on the number of images and columns.\n    It creates a figure and iterates over the images and titles, displaying each image in a subplot with its corresponding title.\n    The function sets display parameters like colormap, normalization, and interpolation before showing the figure.\n    \n    Parameters:\n        images (list of np.array): The images to be displayed.\n        titles (list of str, optional): Titles for each image. Defaults to None.\n        cols (int, optional): Number of columns in the image grid. Defaults to 4.\n        cmap (str, optional): Colormap of the images. None means using the default colormap. Defaults to None.\n        norm (matplotlib.colors.Normalize, optional): Normalization to apply to the images. Defaults to None.\n        interpolation (str, optional): The interpolation method used to display images. Defaults to None.\n    \"\"\"\n    \n    num_images = len(images)\n    if titles is None:\n        titles = [f'Image {i+1}' for i in range(num_images)]\n    elif len(titles) != num_images:\n        raise ValueError('The length of titles must match the number of images.')\n    \n    rows = (num_images + cols - 1) // cols # Calculate the minimum number of rows needed\n    \n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3), squeeze=False)\n    for ax, img, title in zip(axes.flat, images, titles):\n        ax.imshow(img, cmap=cmap, norm=norm, interpolation=interpolation)\n        ax.set_title(title)\n        ax.axis('off')\n    \n    # Handle case where number of images is not a perfect fill of the grid\n    for idx in range(len(images), rows * cols):\n        axes.flat[idx].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# Example usage (images would be substituted with actual image data):\n# images = [np.random.rand(10, 10) for _ in range(6)]  # Example random images\n# display_images(images, titles=['First', 'Second', 'Third', 'Fourth', 'Fifth', 'Sixth'], cmap='gray')", "CodeBleu_score": 28.489288976995837}]}
