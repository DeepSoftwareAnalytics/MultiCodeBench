{"instance_id": "58936b2a621b", "generation_result": [{"code": "from sklearn.linear_model import LogisticRegression\nimport numpy as np\n\ndef set_model_params(model: LogisticRegression, params: np.ndarray) -> LogisticRegression:\n    \"\"\"\n    This function sets the parameters of a scikit-learn LogisticRegression model.\n    It assigns the model's coefficients from the first element of the provided parameters.\n    If the model is configured to fit an intercept, it also assigns the intercept from the second element of the parameters.\n    The function returns the updated LogisticRegression model.\n    \n    Args:\n    model (LogisticRegression): The logistic regression model to be updated.\n    params (np.ndarray): Array where the first element (params[0]) are the coefficients of the model (2D numpy array), \n                         and the second element (params[1]), if present, is the intercept (1D numpy array).\n    \n    Returns:\n    LogisticRegression: The updated logistic regression model with new coefficients and intercept.\n    \"\"\"\n    # Assign coefficients\n    model.coef_ = params[0]\n\n    # Check if the model is setup to use an intercept and if the params include an intercept\n    if model.fit_intercept:\n        # Assign intercept\n        if len(params) > 1:\n            model.intercept_ = params[1]\n        else:\n            raise ValueError(\"Model requires an intercept but none provided in parameters.\")\n\n    return model\n\n# Example Usage:\n# Assuming your model is a logistic regression and you have the coefficients and intercept\n# model = LogisticRegression()\n# params = [np.array([[0.1, 0.2, 0.3]]), np.array([0.01])]\n# updated_model = set_model_params(model, params)", "CodeBleu_score": 53.57165481985783}, {"code": "from sklearn.linear_model import LogisticRegression\nimport numpy as np\n\ndef set_model_params(model: LogisticRegression, params: np.ndarray) -> LogisticRegression:\n    \"\"\"\n    This function sets the parameters of a scikit-learn LogisticRegression model.\n    It assigns the model's coefficients from the first element of the provided parameters.\n    If the model is configured to fit an intercept, it also assigns the intercept from the second element of the parameters.\n    The function returns the updated LogisticRegression model.\n\n    Args:\n    model (LogisticRegression): The LogisticRegression model to update.\n    params (numpy.ndarray): Array containing the parameters to set. The first element must be an array of coefficients,\n                            and if fitting an intercept is required, the second element must be an array containing the intercept.\n\n    Returns:\n    LogisticRegression: The LogisticRegression model with updated parameters.\n    \"\"\"\n\n    # Set the coefficients\n    model.coef_ = params[0]\n\n    # Set the intercept if necessary\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n\n    return model", "CodeBleu_score": 73.11877324882681}, {"code": "from numpy import ndarray\nfrom sklearn.linear_model import LogisticRegression\n\ndef set_model_params(model: LogisticRegression, params: tuple[ndarray, ndarray]) -> LogisticRegression:\n    \"\"\"\n    This function sets the parameters of a scikit-learn LogisticRegression model.\n    It assigns the model's coefficients from the first element of the provided parameters.\n    If the model is configured to fit an intercept, it also assigns the intercept from the second element of the parameters.\n    The function returns the updated LogisticRegression model.\n    \n    Parameters:\n    - model : LogisticRegression\n        The LogisticRegression model instance whose parameters need to be updated.\n    - params : tuple(NDArrays)\n        A tuple of numpy arrays where the first element should correspond to the model's coefficients \n        and the second element (if present) to the model's intercept.\n        \n    Returns:\n    - model : LogisticRegression\n        The updated LogisticRegression model with new coefficients and intercept.\n    \"\"\"\n\n    # Sets the coefficient matrix. Typically the shape needs to match exactly.\n    model.coef_ = params[0]\n\n    # Check if the model was configured to use an intercept and if an intercept is provided.\n    if model.fit_intercept:\n        if len(params) > 1:\n            model.intercept_ = params[1]\n        else:\n            raise ValueError(\"Model expects an intercept but none provided in parameters\")\n\n    return model", "CodeBleu_score": 54.26618215528588}]}
{"instance_id": "83b142d9d251", "generation_result": [{"code": "from sklearn.model_selection import train_test_split\nfrom typing import List, Tuple\n\ndef support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]:\n    \"\"\"\n    This function splits data and labels into training and testing sets based on a specified support ratio.\n    It uses the train_test_split function with stratification to ensure the label distribution is preserved across both sets.\n    A random state of 42 is used for reproducibility.\n    The function returns four lists: x_train, x_test, y_train, and y_test.\n    \"\"\"\n    # Splitting dataset into training and testing based on the given support ratio\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, train_size=support_ratio, stratify=label, random_state=42)\n\n    return x_train, x_test, y_train, y_test", "CodeBleu_score": 77.88221664612898}, {"code": "from sklearn.model_selection import train_test_split\nfrom typing import List, Tuple\n\ndef support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]:\n    \"\"\"\n    This function splits data and labels into training (support) and testing (query) sets based on a specified support ratio.\n    It uses the train_test_split function with stratification to ensure the label distribution is preserved across both sets.\n    A random state of 42 is used for reproducibility.\n    The function returns four lists: x_train, x_test, y_train, and y_test.\n\n    Parameters:\n    data (List): The input data to be split.\n    label (List): The labels corresponding to the data.\n    support_ratio (float): The fraction of the dataset to include in the support set (training set).\n\n    Returns:\n    Tuple[List, List, List, List]: A tuple containing the split data:\n        - x_train: Support features\n        - x_test: Query features\n        - y_train: Support labels\n        - y_test: Query labels\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, \n        train_size=support_ratio, \n        random_state=42, \n        stratify=label  # This ensures the proportion of classes is the same in both splits\n    )\n\n    return x_train, x_test, y_train, y_test", "CodeBleu_score": 65.13588177970726}, {"code": "from typing import List, Tuple\nfrom sklearn.model_selection import train_test_split\n\ndef support_query_split(\n    data,\n    label,\n    support_ratio: float\n) -> Tuple[List, List, List, List]:\n    \"\"\"\n    This function splits data and labels into training and testing sets based on a specified support ratio.\n    It uses the `train_test_split` function with stratification to ensure the label distribution is preserved across both sets.\n    A random state of 42 is used for reproducibility.\n    The function returns four lists: x_train, x_test, y_train, and y_test.\n    \n    Parameters:\n        data: The feature dataset which needs to be split.\n        label: The labels or target data corresponding to the dataset.\n        support_ratio: The proportion of the dataset to include in the train split.\n    \n    Returns:\n        Tuple[List, List, List, List]: A tuple containing the lists x_train, x_test, y_train, y_test respectively.\n    \"\"\"\n    # Using train_test_split from sklearn to split the data\n    x_train, x_test, y_train, y_test = train_test_split(\n        data,\n        label,\n        test_size=1 - support_ratio,  # test_size is the complement of the train size\n        stratify=label,  # Stratify split based on labels\n        random_state=42  # For reproducibility\n    )\n\n    return x_train, x_test, y_train, y_test", "CodeBleu_score": 64.85218808007444}]}
{"instance_id": "5fd02b540f61", "generation_result": [{"code": "import numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom typing import List, Tuple\n\ndef load_data() -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n    \"\"\"This function loads a regression dataset and splits it into training and testing sets.\n    It generates the dataset using make_regression with 3 features and a fixed random state for reproducibility.\n    The data is then split into training and testing sets using train_test_split.\n    The function returns four lists: X (training data), y (training labels), X_test (testing data), and y_test (testing labels).\"\"\"\n    \n    # Generate dataset with 3 features\n    X, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)\n    \n    # Split the dataset into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # The expected return type is List[np.ndarray], so we convert arrays to lists\n    return X_train.tolist(), y_train.tolist(), X_test.tolist(), y_test.tolist()\n\n# Example usage:\n# X_train, y_train, X_test, y_test = load_data()", "CodeBleu_score": 30.957904530752252}, {"code": "from typing import List, Tuple\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\ndef load_data() -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n    \"\"\"\n    This function loads a regression dataset and splits it into training and testing sets.\n    It generates the dataset using make_regression with 3 features and a fixed random state for reproducibility.\n    The data is then split into training and testing sets using train_test_split.\n    The function returns four lists: X_train (training data), y_train (training labels), X_test (testing data), and y_test (testing labels).\n    \"\"\"\n    \n    # Generate synthetic regression data\n    X, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)\n    \n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n    \n    # Converting arrays to list\n    X_train_list, y_train_list = list(X_train), list(y_train)\n    X_test_list, y_test_list = list(X_test), list(y_test)\n    \n    # Return the data as four lists of np.ndarray\n    return X_train_list, y_train_list, X_test_list, y_test_list", "CodeBleu_score": 31.889730538637494}, {"code": "import numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom typing import List, Tuple\n\ndef load_data() -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]:\n    \"\"\"\n    This function loads a regression dataset and splits it into training and testing sets.\n    It generates the dataset using make_regression with 3 features and a fixed random state for reproducibility.\n    The data is then split into training and testing sets using train_test_split.\n    The function returns four lists: X (training data), y (training labels), X_test (testing data), and y_test (testing labels).\n    \"\"\"\n\n    # Generate the dataset using make_regression\n    X, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)\n\n    # Split the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n    # Return the training and testing sets\n    return X_train.tolist(), y_train.tolist(), X_test.tolist(), y_test.tolist()", "CodeBleu_score": 31.829191565704356}]}
{"instance_id": "ad958cb31a57", "generation_result": [{"code": "import numpy as np\n\nclass GMMParameters:\n    def __init__(self, label, means, weights, covariances, num_samples):\n        self.label = label\n        self.means = means\n        self.weights = weights\n        self.covariances = covariances\n        self.num_samples = num_samples\n\ndef ndarrays_to_gmmparam(ndarrays: np.ndarray) -> GMMParameters:\n    \"\"\"\n    Converts a NumPy ndarray to a GMMParameters object.\n    It takes an array of ndarrays as input and maps its elements to the corresponding attributes of the GMMParameters class.\n    The attributes include label, means, weights, covariances, and num_samples.\n    The function returns the constructed GMMParameters object.\n    \n    Args:\n        ndarrays (np.ndarray): A structured numpy array containing the respective data for GMM initialization.\n\n    Returns:\n        GMMParameters: An instance of GMMParameters initialized with the data from ndarrays.\n    \"\"\"\n    # Assuming the ndarrays are passed structurally correct\n    label = ndarrays[0]\n    means = ndarrays[1]\n    weights = ndarrays[2]\n    covariances = ndarrays[3]\n    num_samples = ndarrays[4]\n\n    # Create the GMMParameters instance\n    gmm_parameters = GMMParameters(label, means, weights, covariances, num_samples)\n\n    return gmm_parameters", "CodeBleu_score": 39.66170594304462}, {"code": "import numpy as np\n\nclass GMMParameters:\n    def __init__(self, labels, means, weights, covariances, num_samples):\n        self.labels = labels\n        self.means = means\n        self.weights = weights\n        self.covariances = covariances\n        self.num_samples = num_samples\n\ndef ndarrays_to_gmmparam(ndarrays: np.ndarray) -> GMMParameters:\n    \"\"\"\n    Converts a NumPy ndarray to a GMMParameters object.\n    \n    Parameters:\n        ndarrays (np.ndarray): An array where each element is expected to be a structured array with fields:\n                               'label', 'mean', 'weight', 'covariance', and 'num_samples'.\n    \n    Returns:\n        GMMParameters: An object of GMMParameters with attributes mapped from ndarrays.\n    \"\"\"\n    # Assuming that ndarrays come in a structured format as described:\n    # [array(label1, mean1, weight1, covariance1, num_samples1),\n    #  array(label2, mean2, weight2, covariance2, num_samples2), ...]\n    \n    labels = ndarrays['label']\n    means = ndarrays['mean']\n    weights = ndarrays['weight']\n    covariances = ndarrays['covariance']\n    num_samples = ndarrays['num_samples']\n\n    return GMMParameters(labels=labels, means=means, weights=weights, covariances=covariances, num_samples=num_samples)", "CodeBleu_score": 30.255939406405297}, {"code": "class GMMParameters:\n       def __init__(self, label, means, weights, covariances, num_samples):\n           self.label = label\n           self.means = means\n           self.weights = weights\n           self.covariances = covariances\n           self.num_samples = num_samples", "CodeBleu_score": 28.252048492165894}]}
{"instance_id": "061d5a5b9d56", "generation_result": [{"code": "from sklearn.datasets import load_svmlight_file\nimport numpy as np\n\ndef datafiles_fusion(data_paths):\n    \"\"\"\n    This function fuses multiple data files into a single dataset. It begins by loading the first data file\n    using load_svmlight_file, converting the features to a dense array and storing the labels. For each \n    subsequent data file, it loads the data ensuring the number of features matches the first file, then \n    concatenates the new features and labels to the existing arrays. The function returns the combined \n    feature array X and label array Y.\n\n    :param data_paths: List of paths to the data files.\n    :return: A tuple (X, Y) where X is the feature matrix and Y is the label vector.\n    \"\"\"\n    # Initialize the variables for storing features and labels\n    X = None\n    Y = None\n    \n    # Iterate over each file in the data_paths\n    for idx, path in enumerate(data_paths):\n        # Load the data file\n        X_loaded, y_loaded = load_svmlight_file(path)\n        \n        # Convert the features loaded to a dense array\n        X_dense = X_loaded.toarray()\n        \n        # For the first file, initialize X and Y\n        if idx == 0:\n            X = X_dense\n            Y = y_loaded\n        else:\n            # Ensure the number of features matches\n            if X.shape[1] != X_dense.shape[1]:\n                raise ValueError(f\"Feature mismatch: the file at {path} has {X_dense.shape[1]} features, expected {X.shape[1]}\")\n            \n            # Concatenate the new features and labels to the existing arrays\n            X = np.vstack((X, X_dense))\n            Y = np.concatenate((Y, y_loaded))\n    \n    # Return the combined arrays\n    return X, Y", "CodeBleu_score": 22.48289273949292}, {"code": "from sklearn.datasets import load_svmlight_file\nimport numpy as np\n\ndef datafiles_fusion(data_paths):\n    \"\"\"\n    This function fuses multiple data files into a single dataset.\n    It begins by loading the first data file using load_svmlight_file, converting the features to a dense array and storing the labels.\n    For each subsequent data file, it loads the data ensuring the number of features matches the first file, then concatenates the new features and labels to the existing arrays.\n    The function returns the combined feature array X and label array Y.\n    \"\"\"\n\n    # Initialize lists to store the features and labels\n    X_list = []\n    Y_list = []\n\n    # Define a variable to check the number of features\n    feature_size = None\n\n    # Loop through each file path provided in data_paths\n    for i, path in enumerate(data_paths):\n        # Load data from the file\n        X, Y = load_svmlight_file(path)\n\n        # Convert to dense format\n        X = X.toarray()\n\n        # For the first file, set the feature_size\n        if i == 0:\n            feature_size = X.shape[1]\n        else:\n            # Ensure subsequent files match the feature size of the first file\n            if X.shape[1] != feature_size:\n                raise ValueError(f\"Error: Feature mismatch. Expected {feature_size} features, but got {X.shape[1]}.\")\n\n        # Add the loaded features and labels to the lists\n        X_list.append(X)\n        Y_list.append(Y)\n\n    # Concatenate all features and labels from the list to form final arrays\n    X_concatenated = np.concatenate(X_list, axis=0)\n    Y_concatenated = np.concatenate(Y_list, axis=0)\n\n    # Return the concatenated feature and label arrays\n    return X_concatenated, Y_concatenated", "CodeBleu_score": 23.55557219969206}, {"code": "from sklearn.datasets import load_svmlight_file\nimport numpy as np\n\ndef datafiles_fusion(data_paths):\n    \"\"\"\n    Fuses multiple SVMLight/libsvm format data files into a single dataset.\n\n    Parameters:\n    - data_paths: list of strings, where each string is a path to a data file\n\n    Returns:\n    - X: numpy array, where rows are samples and columns are features\n    - Y: numpy array, where each element is a label corresponding to each sample\n    \"\"\"\n    X, Y = None, None\n\n    for i, path in enumerate(data_paths):\n        # Load the data from file\n        data_X, data_y = load_svmlight_file(path)\n        data_X = data_X.toarray()  # Convert sparse matrix to dense\n        \n        if i == 0:\n            # Initialize X and Y with the data from the first file\n            X, Y = data_X, data_y\n        else:\n            # Ensure the number of features is consistent\n            if data_X.shape[1] != X.shape[1]:\n                raise ValueError(f\"Feature mismatch: expected {X.shape[1]} features, got {data_X.shape[1]} in file {path}\")\n            \n            # Concatenate the arrays\n            X = np.concatenate((X, data_X), axis=0)\n            Y = np.concatenate((Y, data_y))\n    \n    return X, Y", "CodeBleu_score": 26.956085917088412}]}
{"instance_id": "977d3bd5d5a3", "generation_result": [{"code": "def eval_learning(y_test, y_pred)\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\ndef eval_learning(y_test, y_pred):\n    \"\"\"\n    This function evaluates the performance of a learning model by calculating four metrics: \n    accuracy, recall, precision, and F1 score. It takes the true labels (y_test) and the \n    predicted labels (y_pred) as input. The recall, precision, and F1 score are calculated \n    with the \"micro\" average to handle multi-class classification. The function returns the \n    calculated accuracy, recall, precision, and F1 score.\n\n    Parameters:\n    y_test (array-like): True labels.\n    y_pred (array-like): Predicted labels by the model.\n\n    Returns:\n    tuple: Tuple containing the metrics accuracy, recall, precision, and F1 score.\n    \"\"\"\n\n    # Calculate the accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n\n    # Calculate recall\n    recall = recall_score(y_test, y_pred, average='micro')\n\n    # Calculate precision\n    precision = precision_score(y_test, y_pred, average='micro')\n\n    # Calculate F1 score\n    f1 = f1_score(y_test, y_pred, average='micro')\n\n    # Return the calculated metrics\n    return accuracy, recall, precision, f1\n\n# Example usage:\n# y_test = [0, 1, 2, 1, 0]\n# y_pred = [0, 2, 2, 1, 0]\n# metrics = eval_learning(y_test, y_pred)\n# print(\"Accuracy:\", metrics[0])\n# print(\"Recall:\", metrics[1])\n# print(\"Precision:\", metrics[2])\n# print(\"F1 Score:\", metrics[3])", "CodeBleu_score": 55.85614583767751}, {"code": "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\ndef eval_learning(y_test, y_pred):\n    \"\"\"\n    Evaluates the performance of a learning model by calculating four metrics: \n    accuracy, recall, precision, and F1 score.\n    It takes the true labels (y_test) and the predicted labels (y_pred) as input.\n    The recall, precision, and F1 score are calculated with the \"micro\" average \n    to handle multi-class classification.\n    The function returns the calculated accuracy, recall, precision, and F1 score.\n\n    Parameters:\n    y_test (array-like): True labels of the data.\n    y_pred (array-like): Predicted labels by the model.\n\n    Returns:\n    tuple: A tuple containing accuracy, recall, precision, and F1 score.\n    \"\"\"\n\n    # Calculating Accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    # Calculating Recall with micro average\n    recall = recall_score(y_test, y_pred, average='micro')\n    \n    # Calculating Precision with micro average\n    precision = precision_score(y_test, y_pred, average='micro')\n    \n    # Calculating F1 Score with micro average\n    f1 = f1_score(y_test, y_pred, average='micro')\n    \n    return accuracy, recall, precision, f1", "CodeBleu_score": 55.344424529100664}, {"code": "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\ndef eval_learning(y_test, y_pred):\n    \"\"\"\n    This function evaluates the performance of a learning model by calculating four metrics: accuracy, recall, precision, and F1 score.\n    It takes the true labels (y_test) and the predicted labels (y_pred) as input.\n    The recall, precision, and F1 score are calculated with the \"micro\" average to handle multi-class classification.\n    The function returns the calculated accuracy, recall, precision, and F1 score.\n    \n    Parameters:\n    - y_test (array-like): True labels.\n    - y_pred (array-like): Predicted labels by the model.\n    \n    Returns:\n    - dict: A dictionary containing 'accuracy', 'recall', 'precision', and 'f1_score'.\n    \"\"\"\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average='micro')\n    precision = precision_score(y_test, y_pred, average='micro')\n    f1Score = f1_score(y_test, y_pred, average='micro')\n    \n    return {\n        'accuracy': accuracy,\n        'recall': recall,\n        'precision': precision,\n        'f1_score': f1Score\n    }", "CodeBleu_score": 47.69712371611961}]}
{"instance_id": "f4c838f1f092", "generation_result": [{"code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef train(model, train_loader, num_epochs=1):\n    \"\"\"\n    This function trains a given model using a specified data loader for a set number of epochs.\n    It uses the binary cross-entropy loss function and the Adam optimizer with a learning rate of 0.001.\n    The model is set to training mode, and for each epoch, it iterates over batches of data from the train_loader.\n    For each batch, it performs a forward pass to compute the outputs, calculates the loss, performs backpropagation, \n    and updates the model parameters using the optimizer.\n    \"\"\"\n    \n    # Set the model to training mode\n    model.train()\n\n    # Define the loss function and the optimizer\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        for batch_idx, (data, targets) in enumerate(train_loader):\n            # Reset the gradients accumulated in the optimizer\n            optimizer.zero_grad()\n            \n            # Perform the forward pass\n            outputs = model(data)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Perform backpropagation\n            loss.backward()\n            \n            # Update model parameters\n            optimizer.step()\n            \n            # Optionally print the loss\n            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item()}')", "CodeBleu_score": 60.53433289431701}, {"code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef train(model, train_loader, num_epochs=1):\n    \"\"\"\n    This function trains a given model using a specified data loader for a set number of epochs.\n    It uses the binary cross-entropy loss function and the Adam optimizer with a learning rate of 0.001.\n    The model is set to training mode, and for each epoch, it iterates over batches of data from the train_loader.\n    For each batch, it performs a forward pass to compute the outputs, calculates the loss, performs backpropagation, \n    and updates the model parameters using the optimizer.\n    \"\"\"\n\n    # This assumes the model outputs logits, for outputs in the form of probabilities you may use nn.BCELoss()\n    criterion = nn.BCEWithLogitsLoss()\n\n    # Setting up the optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Put the model in training mode\n    model.train()\n\n    # Iterate over the number of epochs\n    for epoch in range(num_epochs):\n        # Loop over each batch from the data loader\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            # Reset the gradients accumulated from the previous iteration\n            optimizer.zero_grad()\n            \n            # Perform the forward pass\n            outputs = model(inputs)\n            \n            # Compute the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n\n            # Update the model parameters\n            optimizer.step()\n\n            # Print loss every 100 batches\n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item()}')\n\n    # Optionally, you may want to return the model, but it's usually modified in place", "CodeBleu_score": 58.32903637601642}, {"code": "import torch\nfrom torch import nn\nfrom torch.optim import Adam\n\ndef train(model, train_loader, num_epochs=1):\n    \"\"\"\n    This function trains a given model using a specified data loader for a set number of epochs.\n    It uses the binary cross-entropy loss function and the Adam optimizer with a learning rate of 0.001.\n    The model is set to training mode, and for each epoch, it iterates over batches of data from the train_loader.\n    For each batch, it performs a forward pass to compute the outputs, calculates the loss, performs backpropagation,\n    and updates the model parameters using the optimizer.\n    \"\"\"\n    \n    # Setting the device for training\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    # Defining the loss function and the optimizer\n    criterion = nn.BCELoss()\n    optimizer = Adam(model.parameters(), lr=0.001)\n    \n    # Putting the model into training mode\n    model.train()\n    \n    # Looping over the number of epochs\n    for epoch in range(num_epochs):\n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # Resetting the gradients of the model parameters\n            optimizer.zero_grad()\n            \n            # Forward pass: compute the predictions\n            outputs = model(inputs)\n            \n            # Calculating the loss\n            loss = criterion(outputs, labels)\n            \n            # Backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            \n            # Updating the model parameters\n            optimizer.step()\n            \n            # Printing information every few batches\n            if batch_idx % 100 == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n# Note that the model, train_loader must be appropriately defined and provided when calling this function.\n# Labels and outputs are assumed to be suitable for BCELoss, meaning they should be within the range [0, 1].", "CodeBleu_score": 53.53567101444827}]}
{"instance_id": "35068c779c02", "generation_result": [{"code": "from typing import Dict\nfrom sklearn.datasets import load_svmlight_file\nfrom torch.utils.data import TensorDataset, Dataset\nimport numpy as np\n\ndef _load_libsvm_dataset(cfg: Dict) -> Dataset:\n    \"\"\"\n    This function loads a LIBSVM dataset based on the configuration provided in cfg.\n    It first ensures that the dataset type is LIBSVM and retrieves the path and name of the dataset from the configuration.\n    The data and labels are loaded using the load_svmlight_file function and converted to a dense array of type float32.\n    The function prints the unique labels with their counts and the shape of the features.\n    For the MUSHROOMS dataset, labels are remapped so that label 1 becomes 0 and all other labels become 1.\n    If the dataset name is not MUSHROOMS, a RuntimeError is raised.\n    Finally, the function creates and returns a TensorDataset containing the data and labels.\n    \"\"\"\n    assert cfg[\"type\"] == \"LIBSVM\", \"Dataset type must be 'LIBSVM'\"\n    file_path = cfg[\"path\"]\n    dataset_name = cfg[\"name\"]\n\n    data, labels = load_svmlight_file(file_path)\n    data = data.toarray().astype(np.float32)\n    print(f\"Loaded dataset shape: {data.shape}\")\n    labels, counts = np.unique(labels, return_counts=True)\n    print(f\"Labels and their counts: {list(zip(labels, counts))}\")\n\n    if dataset_name == \"MUSHROOMS\":\n        # Labels remapping according to the doc requirement\n        labels = np.where(labels == 1, 0, 1)\n    else:\n        raise RuntimeError(\"This function only supports the MUSHROOMS dataset\")\n\n    tensor_data = torch.Tensor(data)\n    tensor_labels = torch.LongTensor(labels)\n\n    dataset = TensorDataset(tensor_data, tensor_labels)\n    return dataset", "CodeBleu_score": 44.72308987893774}, {"code": "from typing import Dict\nfrom sklearn.datasets import load_svmlight_file\nfrom torch.utils.data import TensorDataset, Dataset\nimport numpy as np\nimport torch\n\ndef _load_libsvm_dataset(cfg: Dict) -> Dataset:\n    \"\"\"\n    This function loads a LIBSVM dataset based on the configuration provided in `cfg`.\n      - It first ensures that the dataset type is 'LIBSVM' and retrieves the path and name of the dataset from the configuration.\n      - The data and labels are loaded using the `load_svmlight_file` function and are converted to a dense array of type float32.\n      - The function prints the unique labels with their counts and the shape of the features.\n      - For the MUSHROOMS dataset, labels are remapped so that the label 1 becomes 0 and all other labels become 1.\n      - If the dataset name is not 'MUSHROOMS', a RuntimeError is raised.\n      - Finally, the function creates and returns a `TensorDataset` containing the data and labels.\n\n    Args:\n    - cfg (Dict): The configuration dictionary containing keys like 'type', 'path', 'name'.\n\n    Returns:\n    - Dataset: A TensorDataset containing the tuples of data and labels.\n    \"\"\"\n\n    if cfg['type'].lower() != 'libsvm':\n        raise ValueError(\"Wrong dataset type. Expected 'LIBSVM'.\")\n\n    dataset_name = cfg['name']\n    dataset_path = cfg['path']\n    \n    # Load dataset\n    data, labels = load_svmlight_file(dataset_path)\n    data = data.toarray()\n    data = data.astype(np.float32)\n\n    # Print the shape of features and labels information\n    print(f\"Shape of features: {data.shape}\")\n    unique_labels, counts = np.unique(labels, return_counts=True)\n    print(\"Labels and their counts:\", list(zip(unique_labels, counts)))\n\n    # Handling specific dataset name 'MUSHROOMS'\n    if dataset_name.upper() == 'MUSHROOMS':\n        # Remapping labels: label 1 becomes 0, all other labels become 1\n        labels = np.where(labels == 1, 0, 1)\n    else:\n        raise RuntimeError(f\"The dataset name {dataset_name} is not supported. Only 'MUSHROOMS' dataset is currently supported.\")\n\n    # Converting arrays into tensors\n    tensor_data = torch.tensor(data, dtype=torch.float32)\n    tensor_labels = torch.tensor(labels, dtype=torch.long)\n\n    # Create TensorDataset\n    tensor_dataset = TensorDataset(tensor_data, tensor_labels)\n\n    return tensor_dataset", "CodeBleu_score": 41.91300742723782}, {"code": "from typing import Dict\nfrom omegaconf import DictConfig\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nfrom torch.utils.data import TensorDataset\nimport torch\n\ndef _load_libsvm_dataset(cfg: DictConfig) -> TensorDataset:\n    \"\"\"\n    This function loads a LIBSVM dataset based on the configuration provided in cfg.\n    It first ensures that the dataset type is LIBSVM and retrieves the path and name\n    of the dataset from the configuration. The data and labels are loaded using the\n    load_svmlight_file function and converted to a dense array of type float32.\n    The function prints the unique labels with their counts and the shape of the features.\n    For the MUSHROOMS dataset, labels are remapped so that label 1 becomes 0 and all\n    other labels become 1. If the dataset name is not MUSHROOMS, a RuntimeError is raised.\n    Finally, the function creates and returns a TensorDataset containing the data and labels.\n    \"\"\"\n\n    # Ensure the dataset is of type LIBSVM\n    if cfg.dataset.type != 'LIBSVM':\n        raise ValueError(\"Provided dataset type is not LIBSVM\")\n\n    # Get the dataset path and name from configuration\n    dataset_name = cfg.dataset.name\n    data_path = cfg.dataset.path\n\n    # Load the dataset\n    data, labels = load_svmlight_file(data_path)\n\n    # Convert to dense Numpy array and float32 dtype\n    data = data.toarray().astype(np.float32)\n\n    # Print unique labels and their counts\n    unique_labels, counts = np.unique(labels, return_counts=True)\n    print(\"Unique labels:\", dict(zip(unique_labels, counts)))\n    print(\"Shape of features:\", data.shape)\n\n    # Dataset-specific label processing\n    if dataset_name.upper() == \"MUSHROOMS\":\n        # Remap labels: 1 becomes 0, all others become 1\n        labels = (labels != 1).astype(np.int64)\n    else:\n        raise RuntimeError(\"This function is implemented to handle only the MUSHROOMS dataset.\")\n\n    # Create tensors from data\n    tensor_data = torch.tensor(data, dtype=torch.float32)\n    tensor_labels = torch.tensor(labels, dtype=torch.int64)\n\n    # Create a TensorDataset\n    dataset = TensorDataset(tensor_data, tensor_labels)\n\n    return dataset", "CodeBleu_score": 44.32996537213354}]}
{"instance_id": "d602a96e571e", "generation_result": [{"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params):\n    \"\"\"\n    Apply a transformation to data using a specified transformer.\n    It first checks if the transformer has a fit_transform method and uses it to fit and transform the data X and y with optional fit parameters.\n    If fit_transform is not available, it separately fits the transformer and then transforms the data.\n    If a weight is provided, the transformed result is multiplied by this weight.\n    The function returns the transformed data and the fitted transformer.\n\n    Parameters:\n        transformer: A transformer instance which is assumed to have 'fit', 'transform', or 'fit_transform' methods.\n        X: Feature array to be transformed.\n        y: Optional target vector corresponding to X.\n        weight: An optional weight to scale the transformed array.\n        fit_params: Additional keyword parameters to be passed to the fit method of the transformer.\n\n    Returns:\n        tuple: A tuple containing transformed features and the fitted transformer instance.\n    \"\"\"\n    \n    has_fit_transform = hasattr(transformer, 'fit_transform')\n    \n    if has_fit_transform:\n        # Use fit_transform if it's available\n        transformed = transformer.fit_transform(X, y, **fit_params)\n    else:\n        # Fit and then transform\n        transformer.fit(X, y, **fit_params)\n        transformed = transformer.transform(X)\n    \n    # Apply weight if provided\n    if weight is not None:\n        transformed = transformed * weight\n    \n    return transformed, transformer", "CodeBleu_score": 56.19690278339752}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params)\ndef _fit_transform_one(transformer, X, y, weight, **fit_params):\n    \"\"\"\n    This function applies a transformation to data using a specified transformer.\n    It first checks if the transformer has a fit_transform method and uses it to fit and transform the data X and y with optional fit parameters.\n    If fit_transform is not available, it separately fits the transformer and then transforms the data.\n    If a weight is provided, the transformed result is multiplied by this weight.\n    The function returns the transformed data and the fitted transformer.\n    \n    Args:\n    transformer: fitting and transforming object with fit(), transform(), or fit_transform() methods.\n    X: array-like, feature dataset to transform.\n    y: array-like, target dataset to use for fitting if necessary.\n    weight: numeric, factor to weight the transformed data.\n    **fit_params: dict of additional kwargs to pass to the fit and fit_transform methods.\n\n    Returns:\n    X_transformed: The transformed dataset, optionally scaled by 'weight'.\n    transformer: The fitted transformer.\n    \"\"\"\n\n    # Check if the transformer has a 'fit_transform' method\n    if hasattr(transformer, 'fit_transform'):\n        # Using fit_transform if available\n        X_transformed = transformer.fit_transform(X, y, **fit_params)\n    else:\n        # Otherwise, use fit and transform sequentially\n        transformer.fit(X, y, **fit_params)\n        X_transformed = transformer.transform(X)\n\n    # Apply weight if provided\n    if weight is not None:\n        X_transformed *= weight\n\n    return X_transformed, transformer ", "CodeBleu_score": 54.42862645178095}, {"code": "def _fit_transform_one(transformer, X, y, weight, **fit_params):\n    \"\"\"\n    Applies a transformation to data using a specified transformer.\n    It first checks if the transformer has a fit_transform method and uses it to fit and transform the data X and y with optional fit parameters.\n    If fit_transform is not available, it separately fits the transformer and then transforms the data.\n    If a weight is provided, the transformed result is multiplied by this weight.\n    The function returns the transformed data and the fitted transformer.\n\n    Args:\n    transformer (transformer object): An object that has fit and transform capabilities, or fit_transform.\n    X (array-like): The data to transform.\n    y (array-like, optional): The target variable corresponding to X.\n    weight (float): A weighting factor to apply to the transformed output before it is returned.\n    fit_params (dict): Additional parameters to pass to the fit method of the transformer.\n\n    Returns:\n    (array, transformer object): The transformed data and the fitted transformer.\n    \"\"\"\n    if hasattr(transformer, 'fit_transform'):\n        # If the transformer has a `fit_transform` method, use it.\n        result = transformer.fit_transform(X, y, **fit_params)\n    else:\n        # If no `fit_transform`, but `fit` and `transform` are available\n        transformer.fit(X, y, **fit_params)\n        result = transformer.transform(X)\n\n    # If weight is provided, multiply the result by weight\n    if weight is not None:\n        result *= weight\n\n    # Return the transformed output and the fitted transformer\n    return result, transformer", "CodeBleu_score": 62.04412526089194}]}
{"instance_id": "fcf88a773fd8", "generation_result": [{"code": "import numpy as np\nfrom scipy.sparse import issparse, csc_matrix\n\ndef _transform_selected(X, transform, selected=\"all\", copy=True):\n    \"\"\"\n    This function applies a transformation to selected features of an array X.\n    It first checks and ensures that X is a valid array, accepting sparse 'csc' format if specified.\n    If all features are to be transformed, it applies the transformation to the entire array.\n    If no features are selected, it returns X unchanged.\n    For specific selected features, it identifies the selected and non-selected features,\n    applies the transformation to the selected features, and then combines the transformed \n    and non-transformed features.\n    The function handles both sparse and dense arrays, returning the combined result appropriately.\n    \n    Parameters:\n    - X: numpy array or scipy sparse csc_matrix, 2D array-like structure containing features to be transformed.\n    - transform: callable, a function to apply to the selected features in X.\n    - selected: list of column indices, 'all' to select all columns, or None, specifies which features to transform.\n    - copy: boolean, whether to copy X before applying the transformation.\n    \n    Returns:\n    - Transformed array where the selected features have been modified by 'transform'.\n    \"\"\"\n    \n    # Check if input array needs to be copied\n    if copy:\n        if issparse(X):\n            X = X.copy()\n        else:\n            X = X.copy()\n    \n    # Handle when no transformation is required\n    if selected is None:\n        return X\n    \n    # Apply transformation to all features\n    if selected == 'all':\n        if issparse(X):\n            X = X.tocsc()  # Ensure matrix is in csc format for column slicing\n            for i in range(X.shape[1]):\n                X[:, i] = transform(X[:, i])\n        else:\n            X = np.apply_along_axis(transform, axis=0, arr=X)\n    else:\n        # Handle transforming selected features only\n        if issparse(X):\n            X = X.tocsc()  # Ensure matrix is in csc format for column slicing\n            transformed_parts = []\n            for i in range(X.shape[1]):\n                if i in selected:\n                    transformed_parts.append(transform(X[:, i]))\n                else:\n                    transformed_parts.append(X[:, i])\n            # Concatenate the parts back together\n            X = csc_matrix(np.hstack(transformed_parts))\n        else:\n            for i in selected:\n                X[:, i] = transform(X[:, i])\n    \n    return X", "CodeBleu_score": 32.61757605470613}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\nimport numpy as np\nfrom scipy.sparse import issparse, csc_matrix\n\ndef _transform_selected(X, transform, selected=\"all\", copy=True):\n    \"\"\"\n    Applies a transformation to selected features of an array X.\n    \n    Parameters:\n        X (np.ndarray or scipy.sparse.csc_matrix): Input array.\n        transform (callable): A function to apply to the selected features.\n        selected (list of int or str): Indices or 'all' indicating which features to transform.\n        copy (bool): If True, create a copy of X before transforming.\n\n    Returns:\n        np.ndarray or scipy.sparse.csc_matrix: Array with transformed selected features.\n    \"\"\"\n    \n    # Check if X is a sparse CSC matrix or a dense matrix and handle correspondingly\n    if issparse(X) and not isinstance(X, csc_matrix):\n        raise ValueError(\"Sparse input X must be in CSC format\")\n    \n    # Clone X if copying is requested\n    if copy:\n        X = X.copy()\n\n    # Check if all features are selected\n    if selected == \"all\":\n        transformed_values = transform(X)\n    elif selected is None or len(selected) == 0:\n        return X\n    else:\n        if isinstance(selected, list) and all(isinstance(idx, int) for idx in selected):\n            # Apply transformation only to selected features\n            if issparse(X):\n                transformed_columns = transform(X[:, selected])\n                # For sparse matrices, create a new matrix combining unchanged and transformed parts\n                transformed_values = csc_matrix(X.shape, dtype=transformed_columns.dtype)\n                for i, col in enumerate(range(X.shape[1])):\n                    if col in selected:\n                        transformed_values[:, col] = transformed_columns[:, selected.index(col)]\n                    else:\n                        transformed_values[:, col] = X[:, col]\n            else:\n                # For dense matrices, prepare the resulting matrix with transformed selected columns\n                transformed_values = np.array(X, dtype=X.dtype)\n                transformed_values[:, selected] = transform(X[:, selected])\n        else:\n            raise ValueError(\"Selected indices must be a list of integers\")\n\n    return transformed_values\n\n# The function handles selection and non-selection of features, applies given transformations properly, and\n# manages both sparse and dense input arrays while handling copying and CSC format requirements efficiently.", "CodeBleu_score": 34.435795982664224}, {"code": "def _transform_selected(X, transform, selected=\"all\", copy=True)\ndef _transform_selected(X, transform, selected=\"all\", copy=True):\n    import numpy as np\n    from scipy.sparse import issparse, csc_matrix\n    \n    # Validate and optionally copy the input matrix X\n    if issparse(X):\n        if X.format != 'csc':\n            raise ValueError(\"Sparse input must be in csc format.\")\n        if copy:\n            X = X.copy()\n    else:\n        X = np.array(X, copy=copy)\n    \n    n_features = X.shape[1]\n\n    # Apply transformation to all features\n    if selected == \"all\":\n        if issparse(X):\n            return transform(X)\n        else:\n            return np.apply_along_axis(transform, 0, X)\n    \n    # Return X unchanged if no features are selected\n    elif selected is None or (isinstance(selected, (list, np.ndarray)) and len(selected) == 0):\n        return X\n    \n    # Apply transformation to selected features only\n    else:\n        if isinstance(selected, (list, np.ndarray)):\n            # Create mask for selected features\n            mask = np.zeros(n_features, dtype=bool)\n            mask[selected] = True\n        elif isinstance(selected, slice):\n            mask = np.zeros(n_features, dtype=bool)\n            mask[selected] = True\n        else:\n            raise ValueError(\"Selected must be a list, ndarray, slice, or 'all'.\")\n\n        if issparse(X):\n            selected_features = X[:, mask]\n            non_selected_features = X[:, ~mask]\n            \n            transformed_selected_features = transform(selected_features)\n            \n            # Ensure transformed features remain in sparse format if they convert to dense\n            if not issparse(transformed_selected_features):\n                transformed_selected_features = csc_matrix(transformed_selected_features)\n\n            # Combine selected and non-selected features back into a single sparse matrix\n            return csc_matrix((transformed_selected_features, non_selected_features), shape=X.shape)\n        else:\n            selected_features = X[:, mask]\n            non_selected_features = X[:, ~mask]\n            \n            transformed_selected_features = np.apply_along_axis(transform, 0, selected_features)\n            \n            # Combine the transformed and non-transformed features\n            result = np.zeros_like(X)\n            result[:, mask] = transformed_selected_features\n            result[:, ~mask] = non_selected_features\n            \n            return result\n\n# This implementation should work assuming the provided 'transform' function can process either sparse or dense matrix columns as needed. It includes handling of sparse matrices, selections by list, array or slice, and returns a combined matrix.", "CodeBleu_score": 37.36588344900624}]}
{"instance_id": "c9dfb00047d1", "generation_result": [{"code": "import numpy as np\nfrom scipy.sparse import csr_matrix\n\ndef _transform_new(self, X):\n    \"\"\"\n    Transforms a categorical input array X into a sparse matrix format, suitable for encoding categorical features.\n    \n    Processes:\n    - Checks the input array X and adjusts its data type.\n    - Retrieves the number of samples and features in X.\n    - Transforms the input array into integer-encoded values and generates a mask for valid entries.\n    - Calculates the number of categories for each feature and computes feature indices.\n    - Constructs indices and indptr arrays for creating a sparse matrix.\n    - Returns a sparse CSR matrix with binary data. Can be converted to a dense array if self.sparse is False.\n    \n    Parameters:\n        X (array-like): Categorical data array to transform into CSR matrix.\n        \n    Returns:\n        CSR matrix (scipy.sparse.csr_matrix): Sparse matrix representation of X.\n    \"\"\"\n    \n    # Check and adjust data type of X\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    \n    # Get the number of samples and features\n    n_samples, n_features = X.shape\n    \n    # Integer encode the data\n    # Here we need some method `self._integer_encode_data` to encode data and make a valid mask.\n    # This placeholder assumes existence of such a function:\n    encoded_data, valid_mask = self._integer_encode_data(X)\n    \n    # Calculate number of categories for each feature and compute feature indices\n    categories_per_feature = [len(np.unique(encoded_data[:, i])) for i in range(n_features)]\n    feature_indices = np.cumsum([0] + categories_per_feature)\n    \n    # Create indices and indptr for sparse matrix\n    indices = encoded_data.ravel()[valid_mask.ravel()]\n    indptr = np.arange(0, (n_samples + 1) * n_features, n_features)\n    \n    # Flatten valid_mask and use it to filter indices\n    indices = indices[valid_mask.ravel()]\n    \n    # Data for CSR matrix (ones where valid indices exist)\n    data = np.ones(len(indices), dtype=int)\n    \n    # Creating a sparse matrix\n    csr_matrix_result = csr_matrix((data, indices, indptr), shape=(n_samples, feature_indices[-1]))\n    \n    if not self.sparse:\n        # Convert to dense matrix if required\n        return csr_matrix_result.toarray()\n    else:\n        return csr_matrix_result", "CodeBleu_score": 39.434986113902276}, {"code": "def _transform_new(self, X)\ndef _transform_new(self, X):\n    \"\"\"\n    This function transforms a categorical input array X into a sparse matrix format, suitable for encoding categorical features.\n    It first checks the input array X and adjusts its data type if necessary.\n    It then retrieves the number of samples and features in X.\n    Using a helper function, it transforms the input array into integer-encoded values and generates a mask for valid entries.\n    The function calculates the number of categories for each feature and computes feature indices.\n    It constructs indices and indptr arrays for creating a sparse matrix.\n    The output is a sparse CSR matrix with binary data, which can be converted to a dense array if the sparse attribute is set to False.\n    The function returns the transformed data.\n    \"\"\"\n    import numpy as np\n    from scipy.sparse import csr_matrix\n\n    # Convert the input array X to np.ndarray format with dtype 'object' if it's not already\n    if not isinstance(X, np.ndarray) or X.dtype != object:\n        X = np.asarray(X, dtype=object)\n\n    # Get the number of samples (n_samples) and features (n_features) from X\n    n_samples, n_features = X.shape\n\n    # The helper function to encode categorical data to integers and create masks for valid data\n    def encode_features(X):\n        from sklearn.preprocessing import LabelEncoder\n        label_encoders = {}\n        encoded_data = np.empty(X.shape, dtype=int)\n        valid_mask = np.ones(X.shape, dtype=bool)\n        \n        for i in range(X.shape[1]):\n            le = LabelEncoder()\n            feature = X[:, i]\n            valid_mask[:, i] = pd.notnull(feature)\n            feature[~valid_mask[:, i]] = le.fit(feature[valid_mask[:, i]])\n            encoded_data[:, i] = le.transform(feature)\n            label_encoders[i] = le\n            \n        return encoded_data, valid_mask\n\n    # Transform input array into integer-encoded values and generate a mask for valid entries\n    encoded_data, valid_mask = encode_features(X)\n\n    # Calculate the number of categories for each feature\n    n_categories = [len(np.unique(encoded_data[:, i])) for i in range(n_features)]\n\n    # Construct the indices and indptr arrays for csr matrix\n    indices = encoded_data[valid_mask]\n    indptr = np.zeros(n_samples + 1, dtype=int)\n    np.cumsum(np.sum(valid_mask, axis=1), out=indptr[1:])\n\n    # Create data array for csr matrix, which marks presence of a category by 1\n    data = np.ones(len(indices), dtype=int)\n\n    # Create the CSR matrix\n    X_transformed = csr_matrix((data, indices, indptr), shape=(n_samples, sum(n_categories)))\n\n    # If not kept sparse, convert to dense matrix (assume self.sparse is an attribute to determine this)\n    if not self.sparse:\n        X_transformed = X_transformed.toarray()\n\n    return X_transformed\n", "CodeBleu_score": 34.80517141354946}, {"code": "from scipy.sparse import csr_matrix\nimport numpy as np\n\ndef _transform_new(self, X):\n    \"\"\"\n    This function transforms a categorical input array X into a sparse matrix format, suitable for encoding categorical features.\n    It first checks the input array X and adjusts its data type if necessary.\n    It then retrieves the number of samples and features in X.\n    Using a helper function, it transforms the input array into integer-encoded values and generates a mask for valid entries.\n    The function calculates the number of categories for each feature and computes feature indices.\n    It constructs indices and indptr arrays for creating a sparse matrix.\n    The output is a sparse CSR matrix with binary data, which can be converted to a dense array if the sparse attribute is set to False.\n    The function returns the transformed data.\n    \"\"\"\n  \n    # Ensure the input is a numpy array\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    \n    # Convert any non-numeric data types to strings if not already\n    if X.dtype.kind not in 'fi':  # float or int\n        X = X.astype(str)\n    \n    # Retrieve the number of samples (n_samples) and features (n_features)\n    n_samples, n_features = X.shape\n    \n    # Convert X to integer-encoded using the hypothetical helper function `encode_features`\n    # A helper function `encode_features` should return integer encoding and a valid mask\n    encoded_X, valid_mask = self.encode_features(X)\n    \n    # Calculate the number of categories for each feature\n    n_categories = np.array([np.unique(encoded_X[:, i]).size for i in range(n_features)])\n    \n    # Compute the feature indices for CSR matrix construction\n    feature_indices = np.add.accumulate(n_categories, dtype=np.int32)\n    \n    # Create arrays for sparse matrix construction\n    indices = []\n    indptr = [0]\n    data = []\n    \n    # Build indices and indptr\n    for i in range(n_samples):\n        for j in range(n_features):\n            if valid_mask[i, j]:\n                category_index = encoded_X[i, j] + (feature_indices[j - 1] if j > 0 else 0)\n                indices.append(category_index)\n                data.append(1)\n        indptr.append(len(indices))\n    \n    # Create CSR matrix\n    X_transformed = csr_matrix((data, indices, indptr), shape=(n_samples, feature_indices[-1]))\n    \n    return X_transformed", "CodeBleu_score": 37.71147741602872}]}
{"instance_id": "fcde391a8972", "generation_result": [{"code": "import numpy as np\nfrom scipy.stats import uniform, loguniform\n\nclass DistributionSampler:\n    def __init__(self, lower_bound, upper_bound, log=False):\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n        self.log = log\n\n    def convert_to_sklearn(self):\n        \"\"\"\n        This function converts the current object into a scikit-learn compatible sampler.\n        It imports the necessary distributions from scipy.stats.\n        If the log attribute is True, it creates a log-uniform sampler with the specified lower and upper bounds.\n        If log is False, it creates a uniform sampler with the specified range.\n        The function returns the created sampler.\n        \"\"\"\n        if self.log:\n            # Create a log-uniform distribution using loguniform from scipy.stats\n            return loguniform(a=self.lower_bound, b=self.upper_bound)\n        else:\n            # Create a uniform distribution using uniform from scipy.stats\n            # Notice the scale is calculated as the difference between the upper and lower bounds\n            return uniform(loc=self.lower_bound, scale=self.upper_bound - self.lower_bound)\n\n# Example usage:\nsampler = DistributionSampler(1, 10, log=True)\nsklearn_sampler = sampler.convert_to_sklearn()\nprint(sklearn_sampler)  # Should print a loguniform distribution object from scipy.stats", "CodeBleu_score": 30.157508401387535}, {"code": "from scipy.stats import uniform, loguniform\n\nclass Sampler:\n    def __init__(self, log, lower_bound, upper_bound):\n        \"\"\"\n        Initializes the Sampler object with required attributes.\n        :param log: Boolean, whether to use logarithmic scaling.\n        :param lower_bound: The lower bound of the sampler.\n        :param upper_bound: The upper bound of the sampler.\n        \"\"\"\n        self.log = log\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n\n    def convert_to_sklearn(self):\n        \"\"\"\n        Converts the current object into a scikit-learn compatible sampler.\n        It imports the necessary distributions from scipy.stats.\n        If the log attribute is True, it creates a log-uniform sampler with the specified lower and upper bounds.\n        If log is False, it creates a uniform sampler with the specified range.\n        The function returns the created sampler.\n        \"\"\"\n        if self.log:\n            # Create a log-uniform sampler suitable for parameters that are better sampled on a logarithmic scale\n            # loguniform takes as parameters `a` and `b` where they are the exponents of the base (10 by default)\n            return loguniform(a=10**self.lower_bound, b=10**self.upper_bound)\n        else:\n            # Create a uniform sampler with the specified range\n            # uniform takes `loc` (start of the interval) and `scale` (width of the interval)\n            return uniform(loc=self.lower_bound, scale=self.upper_bound - self.lower_bound)\n\n# Example usage:\nsampler = Sampler(log=True, lower_bound=0.001, upper_bound=1)\nsklearn_sampler = sampler.convert_to_sklearn()\nprint(sklearn_sampler)", "CodeBleu_score": 29.70748617622918}, {"code": "bash\npip install scipy", "CodeBleu_score": 0.0}]}
{"instance_id": "5d5413adcca6", "generation_result": [{"code": "import matplotlib.pyplot as plt\nfrom sklearn.inspection import PartialDependenceDisplay\n\nclass AnalysisState:\n    # Placeholder for AnalysisState class which could include model, data, configuration, etc.\n    pass\n\ndef _render(self, state: AnalysisState) -> None:\n    \"\"\"\n    This function generates and displays Partial Dependence Plots (PDP) based on the provided analysis state.\n    It retrieves additional arguments, figure arguments, and features to be plotted.\n    If headers are specified, it renders the header for the PDP section.\n    A figure and axes are created using plt.subplots with the specified figure arguments.\n    The function prepares keyword arguments by merging additional arguments and predefined keyword arguments.\n    It filters the data for non-null values if two-way interaction plots are enabled.\n    PartialDependenceDisplay.from_estimator is used to create PDPs from the model and data, plotting them on the specified axes.\n    The layout is adjusted with tight_layout for better spacing, and the plots are displayed using plt.show().\n    \"\"\"\n    # Retrieve necessary components from state\n    model = state.model\n    features = state.features\n    X = state.data[state.features]\n    additional_args = state.additional_args if hasattr(state, 'additional_args') else {}\n    figure_args = state.figure_args if hasattr(state, 'figure_args') else {}\n    \n    # Render header if specified\n    if hasattr(state, 'header') and state.header:\n        print(state.header)\n    \n    # Prepare figure and axes\n    n_cols = figure_args.get(\"ncols\", 1)\n    n_rows = (len(features) + n_cols - 1) // n_cols  # Calculate number of rows needed\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figure_args.get(\"figsize\", (10, 5)))\n    \n    # Adjust for the case when there's only one axis\n    if len(features) == 1:\n        axes = [axes]  # Wrap it in a list to make it iterable\n    \n    # Prepare keyword arguments for plotting\n    pdp_kwargs = {'n_jobs': -1, **additional_args}\n    \n    # Handling non-null data filtering for interaction plots\n    if pdp_kwargs.get('kind', None) == 'both':\n        X = X.dropna()\n    \n    # Generate and plot PDPs\n    for i, feature in enumerate(features):\n        ax = axes[i // n_cols, i % n_cols] if n_rows > 1 else axes[i]\n        # Display the partial dependence plot\n        display = PartialDependenceDisplay.from_estimator(model, X, [feature], ax=ax, **pdp_kwargs)\n    \n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()", "CodeBleu_score": 36.68145828273804}, {"code": "import matplotlib.pyplot as plt\nfrom sklearn.inspection import PartialDependenceDisplay\n\nclass AnalysisState:\n    def __init__(self, model, data, features, additional_args=None, fig_args=None, headers=None, enable_two_way_interaction=False):\n        self.model = model\n        self.data = data\n        self.features = features\n        self.additional_args = additional_args if additional_args is not None else {}\n        self.fig_args = fig_args if fig_args is not None else {}\n        self.headers = headers\n        self.enable_two_way_interaction = enable_two_way_interaction\n\nclass Plotter:\n    def __init__(self, analysis_state):\n        self.state = analysis_state\n        \n    def _render(self) -> None:\n        \"\"\" Generate and display Partial Dependence Plots (PDP) based on the provided analysis state. \"\"\"\n        # Headers rendering (if specified)\n        if self.state.headers:\n            print(self.state.headers)  # Assuming console header or similar simple mechanism\n        \n        # Figure settings\n        nrows, ncols = len(self.state.features), 2 if self.state.enable_two_way_interaction else 1\n        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, **self.state.fig_args)\n        \n        # Handle preparing keyword arguments for PDP plot\n        pdp_kwargs = {'ax': axes, 'features': self.state.features}\n        pdp_kwargs.update(self.state.additional_args)\n\n        # Optionally adjust data if two-way interactions are to be considered\n        if self.state.enable_two_way_interaction:\n            # Filter data to non-null essential if considering interactions\n            data_non_null = self.state.data.dropna(subset=self.state.features)\n        else:\n            data_non_null = self.state.data\n        \n        # Generate PDP\n        display = PartialDependenceDisplay.from_estimator(\n            self.state.model,\n            data_non_null,\n            self.state.features,\n            **pdp_kwargs\n        )\n        \n        # Adjust layout\n        plt.tight_layout()\n        # Displaying plots\n        plt.show()\n\n# Example usage:\nmodel = ...   # Some type of fitted model (e.g., from sklearn)\ndata = ...    # Some pandas DataFrame\nfeatures = ['feature1', 'feature2']\n\nanalysis_state = AnalysisState(model=model, data=data, features=features)\nplotter = Plotter(analysis_state)\nplotter._render()", "CodeBleu_score": 29.076356817133085}, {"code": "import matplotlib.pyplot as plt\nfrom sklearn.inspection import PartialDependenceDisplay\n\nclass AnalysisState:\n    def __init__(self, model, data, features, additional_args=None, figure_args=None, headers=None):\n        self.model = model\n        self.data = data\n        self.features = features\n        self.additional_args = additional_args or {}\n        self.figure_args = figure_args or {}\n        self.headers = headers\n\ndef _render(self, state: AnalysisState) -> None:\n    # Check if headers are to be displayed and render them\n    if state.headers:\n        print(state.headers)  # Assuming simple printing, but could be more complex depending on requirements\n\n    # Setup plotting arguments based on default and given additional arguments\n    default_plot_kwargs = {'n_cols': 1}\n    plot_kwargs = {**default_plot_kwargs, **state.additional_args}\n    \n    # Determine the number of plots and setup subplots accordingly\n    if isinstance(state.features, list) and all(isinstance(f, list) for f in state.features):\n        # Interaction features\n        n_plots = len(state.features)\n    else:\n        # Single feature plots\n        n_plots = len(state.features) if isinstance(state.features, list) else 1\n\n    # Create a figure and axes object with specified figure arguments\n    fig, axs = plt.subplots(nrows=n_plots, **state.figure_args)\n    \n    # Adjust axes for easier indexing\n    if n_plots == 1:\n        axs = [axs]  # Make it iterable\n    \n    # Filter data for non-null values in features if necessary (specially important for interactions)\n    if any(isinstance(f, list) and len(f) == 2 for f in state.features):\n        filtered_data = state.data.dropna(subset=[f for sublist in state.features if isinstance(sublist, list) for f in sublist])\n    else:\n        filtered_data = state.data\n    \n    # Create PDP and plot it\n    for ax, feature in zip(axs, state.features):\n        display = PartialDependenceDisplay.from_estimator(\n            state.model, filtered_data, features=feature, ax=ax, **plot_kwargs)\n    \n    # Optimize layout\n    plt.tight_layout()\n\n    # Show the plot\n    plt.show()", "CodeBleu_score": 35.098829954964934}]}
{"instance_id": "db8aac29180b", "generation_result": [{"code": "import numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\nclass LabelEncoderFeatureGenerator:\n    def __init__(self, verbosity=0):\n        self.verbosity = verbosity\n        self.label_encoders = {}\n        self.feature_indices = None\n\n    def fit(self, X):\n        # Identify categorical features (as an example)\n        self.feature_indices = [i for i, col in enumerate(X.columns) if X[col].dtype == 'object']\n        for feature in self.feature_indices:\n            le = LabelEncoder()\n            le.fit(X.iloc[:, feature])\n            self.label_encoders[feature] = le\n\n    def transform(self, X):\n        X_trans = X.copy()\n        for feature, encoder in self.label_encoders.items():\n            X_trans.iloc[:, feature] = encoder.transform(X_trans.iloc[:, feature])\n        return X_trans\n\nclass YourSuperClass:\n    def _preprocess(self, X, **kwargs):\n        # The specifics of the superclass preprocessing are assumed to be a placeholder.\n        return X\n\nclass YourClass(YourSuperClass):\n    feature_generator = None\n\n    def _preprocess(self, X, **kwargs):\n        # Call superclass's _preprocess method\n        X_preprocessed = super()._preprocess(X, **kwargs)\n        \n        # Initialize and fit the feature generator if it is not already created\n        if self.feature_generator is None:\n            self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n            self.feature_generator.fit(X_preprocessed)\n        \n        # If there are identified features, apply the transformations\n        if self.feature_generator.feature_indices:\n            X_preprocessed = self.feature_generator.transform(X_preprocessed)\n        \n        # Fill missing values with 0\n        X_preprocessed = X_preprocessed.fillna(0)\n\n        # Convert data to float32 NumPy array\n        X_preprocessed = np.array(X_preprocessed, dtype=np.float32)\n        \n        return X_preprocessed", "CodeBleu_score": 35.25783693120337}, {"code": "import numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom pandas import DataFrame\n\nclass YourSuperClass:\n    def _preprocess(self, X, **kwargs):\n        # Superclass preprocessing - assumed to be implemented\n        pass\n\nclass LabelEncoderFeatureGenerator:\n    def __init__(self, verbosity=0):\n        self.verbosity = verbosity\n        self.label_encoders = {}\n    \n    def fit(self, X):\n        # Assuming X is a DataFrame for simplicity in example\n        for column in X.columns:\n            if X[column].dtype == 'object':  # Assuming only categorical encodings\n                le = LabelEncoder()\n                X[column] = le.fit_transform(X[column].fillna(''))\n                self.label_encoders[column] = le\n    \n    def transform(self, X):\n        for column, encoder in self.label_encoders.items():\n            if column in X.columns:\n                X[column] = encoder.transform(X[column].fillna(''))\n        return X\n\nclass YourClass(YourSuperClass):\n    def __init__(self):\n        self.feature_generator = None\n    \n    def _preprocess(self, X, **kwargs):\n        # Call super's preprocess first\n        super()._preprocess(X, **kwargs)\n        \n        # Conditionally initializing feature generator if not already done\n        if self.feature_generator is None:\n            self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n            self.feature_generator.fit(X)\n        \n        # Apply feature generator transformation if applicable\n        if hasattr(self.feature_generator, 'label_encoders') and self.feature_generator.label_encoders:\n            X = X.copy()\n            X = self.feature_generator.transform(X)\n        \n        # Fill missing values with 0\n        X.fillna(0, inplace=True)\n        \n        # Convert the DataFrame to a numpy array with dtype float32\n        X = X.astype(np.float32)\n        \n        return X", "CodeBleu_score": 33.05601588076487}, {"code": "import numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass LabelEncoderFeatureGenerator(BaseEstimator, TransformerMixin):\n    def __init__(self, verbosity=0):\n        self.verbosity = verbosity\n    \n    def fit(self, X, y=None):\n        # Assuming that all columns in X need to be fitted with a LabelEncoder\n        self.encoders = {}\n        for col in X.columns:\n            le = LabelEncoder()\n            le.fit(X[col])\n            self.encoders[col] = le\n        return self\n    \n    def transform(self, X):\n        X_transformed = X.copy()\n        for col, le in self.encoders.items():\n            X_transformed[col] = le.transform(X[col])\n        return X_transformed\n\nclass YourModelClass(SuperClass):  # Assuming SuperClass exists and has a _preprocess method\n    def __init__(self):\n        self.feature_generator = None\n\n    def _preprocess(self, X, **kwargs):\n        # Call the superclass's _preprocess method:\n        X = super()._preprocess(X)\n\n        # Check if the feature generator is initialized and fit it if not\n        if self.feature_generator is None:\n            self.feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n            self.feature_generator.fit(X)\n\n        # If the feature generator has identified features, apply transformations\n        if hasattr(self.feature_generator, 'encoders') and self.feature_generator.encoders:\n            X = self.feature_generator.transform(X)\n        \n        # Fill missing values and convert data to a specified type\n        X.fillna(0, inplace=True)\n        return X.astype(np.float32)\n\n# Example usage context:\n# model_instance = YourModelClass()\n# preprocessed_X = model_instance._preprocess(your_data_df)", "CodeBleu_score": 30.603055026931557}]}
{"instance_id": "d4eb45e6d013", "generation_result": [{"code": "def _estimate_memory_usage(self, X, **kwargs)\ndef _estimate_memory_usage(self, X, **kwargs):\n    \"\"\"\n    This function estimates the minimum memory usage required for a model based on the input data X.\n    It retrieves model parameters and determines the final number of estimators.\n    A minimum of 40 estimators is set if the final number is less than 40 or if a search space is defined.\n    It calculates the number of trees per estimator and estimates the bytes used per estimator based on the size of X,\n    with an adjustment factor.\n    The expected minimum memory usage is calculated by multiplying the bytes per estimator by the minimum number of estimators.\n    The function returns the estimated minimum memory usage.\n    \n    Args:\n        X (np.array or pd.DataFrame): The input data used for the model.\n        **kwargs: Additional keyword arguments that can contain model settings like number of trees per estimator.\n    \n    Returns:\n        int or float: The estimated minimum memory usage in bytes.\n    \"\"\"\n    import numpy as np\n\n    # Retrieve model parameters\n    num_trees_per_estimator = kwargs.get('num_trees_per_estimator', 10)\n    final_number_of_estimators = kwargs.get('final_number_of_estimators', 100)\n    search_space_defined = kwargs.get('search_space_defined', False)\n\n    # Ensure there are at least 40 estimators\n    if final_number_of_estimators < 40 or search_space_defined:\n        final_number_of_estimators = 40\n\n    # Data size in bytes\n    data_bytes = X.nbytes\n\n    # Calculate bytes used per estimator\n    # Use a simple adjustment factor, e.g., 0.5 to account for extra overhead by the model parameters\n    adjustment_factor = 0.5\n    bytes_per_estimator = data_bytes * num_trees_per_estimator * adjustment_factor\n\n    # Calculating expected minimum memory usage\n    minimum_memory_usage = bytes_per_estimator * final_number_of_estimators\n\n    return minimum_memory_usage", "CodeBleu_score": 35.290786238287126}, {"code": "import numpy as np\nimport pandas as pd\n\ndef _estimate_memory_usage(self, X, **kwargs):\n    \"\"\"\n    This function estimates the minimum memory usage required for a model based on the input data X.\n    It retrieves model parameters and determines the final number of estimators.\n    A minimum of 40 estimators is set if the final number is less than 40 or if a search space is defined.\n    It calculates the number of trees per estimator and estimates the bytes used per estimator based on the size of X, with an adjustment factor.\n    The expected minimum memory usage is calculated by multiplying the bytes per estimator by the minimum number of estimators.\n    The function returns the estimated minimum memory usage.\n    \"\"\"\n    \n    # Determine the data size of X\n    if isinstance(X, np.ndarray):\n        data_size = X.nbytes\n    elif isinstance(X, pd.DataFrame):\n        data_size = X.memory_usage(index=True).sum()\n    else:\n        raise TypeError(\"Unsupported data type for X. X must be a numpy array or pandas DataFrame.\")\n    \n    # Get model parameters from kwargs\n    num_trees_per_estimator = kwargs.get('num_trees_per_estimator', 10)  # Default to 10 if not provided\n    adjustment_factor = kwargs.get('adjustment_factor', 1.5)  # Default adjustment factor\n    \n    # Determine the number of estimators\n    num_estimators = kwargs.get('num_estimators', 100)  # Default to 100 if not provided\n    if num_estimators < 40:\n        num_estimators = 40  # Ensure there are at least 40 estimators\n    \n    # Estimate memory usage per estimator\n    bytes_per_estimator = data_size * num_trees_per_estimator * adjustment_factor\n    \n    # Calculate total minimum memory usage\n    total_memory_usage = bytes_per_estimator * num_estimators\n    \n    return total_memory_usage", "CodeBleu_score": 36.57731244509528}, {"code": "def _estimate_memory_usage(self, X, **kwargs):\n    \"\"\"\n    Estimates the minimum memory usage required for a model based on the input data X.\n    It retrieves model parameters and determines the final number of estimators.\n    A minimum of 40 estimators is set if the final number is less than 40 or if a search space is defined.\n    It calculates the number of trees per estimator and estimates the bytes used per estimator based on the size of X,\n    with an adjustment factor.\n    The expected minimum memory usage is calculated by multiplying the bytes per estimator by the minimum\n    number of estimators.\n    The function returns the estimated minimum memory usage.\n    \n    Parameters:\n        X (np.array): The input data used to train the model.\n        **kwargs: Additional keyword arguments that may include flags to modify behavior or provide additional information.\n    \n    Returns:\n        int: The estimated minimum memory usage in bytes.\n    \"\"\"\n\n    # Number of features in X\n    num_features = X.shape[1]\n    \n    # Default values (these should ideally be fetched/derived from the actual model configuration)\n    base_memory_per_estimator = 512  # Assume each estimator uses 512 bytes base\n    default_trees_per_estimator = 100  # Default number of trees per estimator\n    \n    # Parameterized values from kwargs or defaults\n    trees_per_estimator = kwargs.get('trees_per_estimator', default_trees_per_estimator)\n\n    # Calculate the memory used per estimator\n    bytes_per_estimator = base_memory_per_estimator * trees_per_estimator * num_features\n    \n    # Adjust factor for specific model configurations (this is a simplistic adjustment factor)\n    adjustment_factor = 1.2\n    bytes_per_estimator = int(bytes_per_estimator * adjustment_factor)\n\n    # Retrieve the number of estimators if provided\n    num_estimators = kwargs.get('num_estimators', 10)  # Default to 10 if not specified\n    \n    # Ensuring a minimum of 40 estimators are used\n    min_estimators = 40\n    final_min_estimators = max(min_estimators, num_estimators)\n\n    # Calculate minimum memory usage\n    min_memory_usage = bytes_per_estimator * final_min_estimators\n    \n    return min_memory_usage", "CodeBleu_score": 34.66083563594034}]}
{"instance_id": "6923cb007f4c", "generation_result": [{"code": "from collections import Counter\n\ndef downscale_vectorizer(vectorizer, ngram_freq, vocab_size):\n    \"\"\"\n    Downsizes the vocabulary of a vectorizer based on n-gram frequencies and a specified vocabulary size.\n    It creates a Counter object from the n-gram frequency data and identifies the top n most common n-grams\n    according to the specified vocabulary size. The names of these top n n-grams are sorted and used to\n    create a new vocabulary dictionary, mapping each n-gram to a unique index. The vectorizer's vocabulary\n    is then updated with this new, downsized vocabulary.\n\n    Args:\n    vectorizer: The vectorizer instance whose vocabulary needs to be updated.\n    ngram_freq: A dictionary with n-grams as keys and their frequencies as values.\n    vocab_size: The maximum size of the new vocabulary.\n\n    Returns:\n    The vectorizer with its vocabulary updated.\n    \"\"\"\n    # Create a Counter object from the ngram frequency dictionary\n    ngram_counter = Counter(ngram_freq)\n    \n    # Extract the most common n-grams according to the specified vocabulary size\n    most_common_ngrams = ngram_counter.most_common(vocab_size)\n    \n    # Create a new vocabulary dictionary mapping each n-gram to a unique index\n    new_vocabulary = {ngram[0]: idx for idx, ngram in enumerate(most_common_ngrams)}\n    \n    # Update the vectorizer's vocabulary with the new vocabulary\n    if hasattr(vectorizer, 'vocabulary_'):\n        vectorizer.vocabulary_ = new_vocabulary\n    else:\n        raise AttributeError(\"Vectorizer does not have a 'vocabulary_' attribute to update.\")\n\n    # Return the updated vectorizer\n    return vectorizer", "CodeBleu_score": 36.040303970669186}, {"code": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)\nfrom collections import Counter\n\ndef downscale_vectorizer(vectorizer, ngram_freq, vocab_size):\n    \"\"\"\n    This function downsizes the vocabulary of a vectorizer based on n-gram frequencies and a specified vocabulary size.\n    It creates a Counter object from the n-gram frequency data and identifies the top n most common n-grams according to the specified vocabulary size.\n    The names of these top n n-grams are sorted and used to create a new vocabulary dictionary, mapping each n-gram to a unique index.\n    The vectorizer's vocabulary is then updated with this new, downsized vocabulary.\n    \n    Args:\n    vectorizer: An instance of a vectorizer (such as CountVectorizer or TfidfVectorizer from sklearn) with an existing vocabulary.\n    ngram_freq: A dictionary or counter mapping n-grams to their respective frequency counts.\n    vocab_size: An integer specifying the new maximum size of the vocabulary.\n\n    Returns:\n    None: It modifies the vectorizer in place.\n    \"\"\"\n    \n    # Create a counter from the ngram frequency data if it's not already a counter\n    if not isinstance(ngram_freq, Counter):\n        ngram_freq = Counter(ngram_freq)\n    \n    # Select the top `vocab_size` n-grams based on frequency counts\n    most_common_ngrams = ngram_freq.most_common(vocab_size)\n    \n    # Extract and sort n-gram names from the most common n-grams\n    sorted_ngrams = sorted(ngram for ngram, _ in most_common_ngrams)\n    \n    # Create new vocabulary mapping n-grams to indices\n    new_vocabulary = {ngram: index for index, ngram in enumerate(sorted_ngrams)}\n    \n    # Update the vectorizer's vocabulary\n    vectorizer.vocabulary_ = new_vocabulary", "CodeBleu_score": 43.03426371790627}, {"code": "from collections import Counter\n\ndef downscale_vectorizer(vectorizer, ngram_freq, vocab_size):\n    \"\"\"\n    Downsizes the vocabulary of a vectorizer based on n-gram frequencies and a specified vocabulary size.\n    \n    Parameters:\n    - vectorizer: The vectorizer object whose vocabulary needs to be downsized.\n    - ngram_freq: A dictionary mapping n-grams to their frequencies.\n    - vocab_size: The maximum size of the new vocabulary.\n    \n    Returns:\n    - The updated vectorizer with a downsized vocabulary.\n    \"\"\"\n    # Create a Counter object from the n-gram frequency data\n    ngram_counter = Counter(ngram_freq)\n    \n    # Get the most common n-grams based on the specified vocabulary size\n    most_common_ngrams = ngram_counter.most_common(vocab_size)\n    \n    # Sort the n-grams by the n-gram names (not by frequency)\n    sorted_ngrams = sorted([ngram for ngram, _ in most_common_ngrams])\n    \n    # Create the new vocabulary: map each n-gram to a unique index\n    new_vocabulary = {ngram: index for index, ngram in enumerate(sorted_ngrams)}\n    \n    # Update the vectorizer's vocabulary\n    vectorizer.vocabulary_ = new_vocabulary\n    \n    return vectorizer", "CodeBleu_score": 61.492252456000585}]}
{"instance_id": "0e837b2d8088", "generation_result": [{"code": "import pandas as pd\n\nclass FeatureSelector:\n    def __init__(self, skewness_threshold=0.5):\n        self.skewness_threshold = skewness_threshold\n    \n    def _select_continuous(self, df, features):\n        \"\"\"\n        Categorizes features in a DataFrame as either continuous or skewed based on their skewness.\n        It initializes a dictionary to store these feature types and retrieves the skewness threshold from the parameters.\n        For each feature, it calculates the skewness and compares it to the threshold.\n        Features with skewness exceeding the threshold are classified as \"skewed\", while others are classified as \"continuous\".\n        The function returns a dictionary with lists of continuous and skewed features.\n\n        Args:\n        df (pd.DataFrame): DataFrame containing the features to analyze.\n        features (list of str): List of feature names in the dataframe to analyze for skewness.\n\n        Returns:\n        dict: A dictionary with keys 'continuous' and 'skewed', containing the respective lists of feature names.\n        \"\"\"\n        feature_types = {\n            'continuous': [],\n            'skewed': []\n        }\n        \n        # Compute skewness for each feature\n        for feature in features:\n            skewness = df[feature].skew()\n            if abs(skewness) > self.skewness_threshold:\n                feature_types['skewed'].append(feature)\n            else:\n                feature_types['continuous'].append(feature)\n        \n        return feature_types", "CodeBleu_score": 43.03226699627738}, {"code": "import pandas as pd\n\nclass DataProcessor:\n    def __init__(self, skewness_threshold=0.5):\n        self.skewness_threshold = skewness_threshold\n\n    def _select_continuous(self, df, features):\n        \"\"\"\n        This function categorizes features in a DataFrame as either continuous or skewed based on their skewness.\n        It initializes a dictionary to store these feature types and retrieves the skewness threshold from the parameters.\n        For each feature, it calculates the skewness and compares it to the threshold.\n        Features with skewness exceeding the threshold are classified as \"skewed\", while others are classified as \"continuous\".\n        The function returns a dictionary with lists of continuous and skewed features.\n\n        Args:\n        df (pd.DataFrame): The DataFrame containing the data.\n        features (list of str): The list of feature names to evaluate.\n\n        Returns:\n        dict: A dictionary with keys 'continuous' and 'skewed', each containing lists of feature names.\n        \"\"\"\n        # Initialize the dictionary to hold the classified features\n        feature_types = {'continuous': [], 'skewed': []}\n\n        # Retrieve skewness of each feature and classify based on the threshold\n        for feature in features:\n            if feature in df.columns:\n                skewness = df[feature].skew()\n                if abs(skewness) > self.skewness_threshold:\n                    feature_types['skewed'].append(feature)\n                else:\n                    feature_types['continuous'].append(feature)\n\n        return feature_types\n\n# Example usage:\n# Assuming we have a DataFrame 'data' and a list of features 'feature_list'\n# data_processor = DataProcessor(skewness_threshold=1.0)\n# classified_features = data_processor._select_continuous(data, feature_list)\n# print(classified_features)", "CodeBleu_score": 45.62760863584375}, {"code": "import pandas as pd\n\nclass DataAnalyzer:\n    def __init__(self, skewness_threshold=0.5):\n        \"\"\"\n        Initializes the DataAnalyzer with a specified skewness threshold.\n        \"\"\"\n        self.skewness_threshold = skewness_threshold\n\n    def _select_continuous(self, df, features):\n        \"\"\"\n        This function categorizes features in a DataFrame as either continuous or \n        skewed based on their skewness. It initializes a dictionary to store these \n        feature types and retrieves the skewness threshold from the parameters.\n        For each feature, it calculates the skewness and compares it to the threshold.\n        Features with skewness exceeding the threshold are classified as \"skewed\", \n        while others are classified as \"continuous\".\n        The function returns a dictionary with lists of continuous and skewed features.\n        \n        Args:\n            df (pd.DataFrame): The DataFrame containing the data.\n            features (list of str): The list of features to evaluate.\n        \n        Returns:\n            dict: A dictionary with keys 'continuous' and 'skewed'.\n        \"\"\"\n        # Initialize the dictionary\n        feature_types = {'continuous': [], 'skewed': []}\n\n        # Iterate through the list of features\n        for feature in features:\n            if feature in df.columns:\n                # Calculate the skewness for each feature\n                skewness = df[feature].skew()\n                # Compare the skewness against the threshold\n                if abs(skewness) > self.skewness_threshold:\n                    feature_types['skewed'].append(feature)\n                else:\n                    feature_types['continuous'].append(feature)\n\n        return feature_types\n\n# Example usage:\n# df = pd.DataFrame({\n#    'A': [1, 2, 3, 4, 5],\n#    'B': [2, 2, 2, 2, 2],\n#    'C': [1, 1, 2, 2, 5]\n# })\n# analyzer = DataAnalyzer(skewness_threshold=0.75)\n# print(analyzer._select_continuous(df, ['A', 'B', 'C']))", "CodeBleu_score": 45.06798555964884}]}
{"instance_id": "bb569c10f975", "generation_result": [{"code": "import numpy as np\n\ndef _get_weights(dist, weights):\n    \"\"\"\n    This function calculates weights based on distances and a specified weights parameter.\n    If the weights parameter is None or \"uniform\", it returns None, indicating no weights.\n    If the weights parameter is \"distance\", it computes the inverse of the distances, \n    handling any infinite values by masking and adjusting the rows accordingly.\n    If the weights parameter is a callable function, it applies this function to the distances and returns the result.\n    If the weights parameter is not recognized, it raises a ValueError indicating that the weights should be 'uniform', \n    'distance', or a callable function.\n\n    Args:\n        dist (np.ndarray): An array of distances between samples.\n        weights (str or callable, optional): Specifies the type of weight calculation method to use. Should be \"uniform\",\n            \"distance\", a callable function, or None.\n\n    Returns:\n        np.ndarray or None: An array of weights or None if weights are 'uniform' or None.\n\n    Raises:\n        ValueError: If the weights parameter is unrecognized.\n    \"\"\"\n\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        with np.errstate(divide='ignore', invalid='ignore'):\n            weight_values = 1 / dist\n            # Replace any infinities with 0\n            weight_values[np.isinf(weight_values)] = 0\n        return weight_values\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights parameter should be 'uniform', 'distance', or a callable function\")", "CodeBleu_score": 45.64621921479115}, {"code": "import numpy as np\n\ndef _get_weights(dist, weights):\n    \"\"\"\n    This function calculates weights based on distances and a specified weights parameter.\n    If the weights parameter is None or \"uniform\", it returns None, indicating no weights.\n    If the weights parameter is \"distance\", it computes the inverse of the distances, handling any infinite \n    values by masking and adjusting the rows accordingly.\n    If the weights parameter is a callable function, it applies this function to the distances and returns \n    the result.\n    If the weights parameter is not recognized, it raises a ValueError indicating that the weights should be\n    'uniform', 'distance', or a callable function.\n    \n    :param dist: A numpy array of distances.\n    :param weights: A method of weighting (None, 'uniform', 'distance', or a callable function).\n    :return: A numpy array of weights or None.\n    \"\"\"\n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        if np.any(dist == 0):\n            # Create mask for zero distances to avoid division by zero error\n            mask = (dist == 0)\n            dist_weighted = np.where(mask, 0, 1/dist)\n            # Handle rows where all distances might be zero by assigning a large number\n            all_zeros_rows = np.all(mask, axis=1)\n            dist_weighted[all_zeros_rows] = 0  # Or some specific rule based on requirements\n        else:\n            dist_weighted = 1 / dist\n        return dist_weighted\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"Invalid weight specification. Choose 'uniform', 'distance' or a callable function.\")", "CodeBleu_score": 50.610808438182886}, {"code": "import numpy as np\n\ndef _get_weights(dist, weights):\n    \"\"\"\n    This function calculates weights based on distances and a specified weights parameter.\n    If the weights parameter is None or 'uniform', it returns None, indicating no weights.\n    If the weights parameter is 'distance', it computes the inverse of the distances, \n    handling any infinite values by masking and adjusting the rows accordingly.\n    If the weights parameter is a callable function, it applies this function to the distances and returns the result.\n    If the weights parameter is not recognized, it raises a ValueError indicating that the weights \n    should be 'uniform', 'distance', or a callable function.\n    \n    Args:\n        dist (np.array): The array containing distances.\n        weights (str or callable): Describes how to transform distances into weights.\n                                   Can be 'uniform', 'distance', callable function, or None.\n    \n    Returns:\n        np.array or None: An array of weights, or None if weights are uniform.\n    \"\"\"\n    \n    if weights is None or weights == \"uniform\":\n        return None\n    elif weights == \"distance\":\n        if np.any(dist == 0):\n            # Avoid division by zero\n            with np.errstate(divide='ignore', invalid='ignore'):\n                # Calculate inverse and suppress infinite values issue\n                weight_values = 1.0 / dist\n                weight_values[dist == 0] = np.inf\n        else:\n            weight_values = 1.0 / dist\n\n        weight_values[np.isinf(weight_values)] = 0  # Mask infinite values to zero if necessary\n        return weight_values\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights should be 'uniform', 'distance', or a callable function\")", "CodeBleu_score": 46.0676762430012}]}
{"instance_id": "12aaa9c78402", "generation_result": [{"code": "import numpy as np\nfrom sklearn.neighbors import NearestNeighbors\n\nclass KNNPredictor:\n    def __init__(self, X, y, k, weights='uniform'):\n        self._X = X\n        self._y = np.array(y)\n        self.k = k\n        self.weights = weights\n    \n    def _get_weights(self, distances):\n        if self.weights == 'uniform':\n            return np.ones_like(distances)\n        elif self.weights == 'distance':\n            # Prevent division by zero; small epsilon\n            with np.errstate(divide='ignore'):\n                weights = 1 / distances\n                weights[distances == 0] = 0\n            return weights\n        else:\n            raise ValueError(\"Unsupported weight schema\")\n    \n    def predict_loo(self):\n        n_samples = self._X.shape[0]\n        predictions = np.zeros(n_samples)\n        \n        # Nearest Neighbors model that considers all points (n_neighbors-1 because it includes the sample itself)\n        nn = NearestNeighbors(n_neighbors=self.k + 1) \n        nn.fit(self._X)\n        \n        # Get distances and indices of the k-nearest neighbors for each sample\n        distances, indices = nn.kneighbors(self._X)\n\n        # Exclude the first point since LOO - it is the point itself\n        distances = distances[:, 1:]  \n        indices = indices[:, 1:]\n        \n        for i in range(n_samples):\n            target_neighbors = self._y[indices[i]]", "CodeBleu_score": 24.909330875271532}, {"code": "import numpy as np\nfrom sklearn.neighbors import NearestNeighbors\n\nclass LOOPredictor:\n    def __init__(self, X, y, k, metric='euclidean', weights='uniform'):\n        self.X = X\n        self._y = y\n        self.k = k\n        self.metric = metric\n        self.weights = weights\n        self.nbrs = NearestNeighbors(n_neighbors=k+1, metric=metric).fit(X)\n    \n    def _get_weights(self, distances):\n        if self.weights == 'uniform':\n            return np.ones_like(distances)\n        elif self.weights == 'distance':\n            with np.errstate(divide='ignore', invalid='ignore'):\n                weights = 1. / distances\n                weights[np.isinf(weights)] = 0  # in case any distances are 0\n            return weights\n        else:\n            raise ValueError(\"Unknown weight type: {}\".format(self.weights))\n    \n    def predict_loo(self):\n        n_samples = self.X.shape[0]\n        predictions = np.zeros(n_samples)\n        \n        # Perform LOO\n        for i in range(n_samples):\n            X_train = np.delete(self.X, i, axis=0)\n            y_train = np.delete(self._y, i)\n            sample = self.X[i:i+1]\n\n            # Recompute neighbors after removing the instance\n            nbrs = NearestNeighbors(n_neighbors=self.k, metric=self.metric).fit(X_train)\n            distances, indices = nbrs.kneighbors(sample)\n\n            # Get weights\n            weights = self._get_weights(distances[0])\n\n            # Calculate predictions (weighted or unweighted)\n            if self.weights == 'uniform':\n                prediction = np.mean(y_train[indices[0]], axis=0)\n            else:\n                weighted_sum = np.sum(y_train[indices[0]] * weights[:, np.newaxis], axis=0)\n                weights_sum = np.sum(weights)\n                prediction = weighted_sum / weights_sum if weights_sum != 0 else np.mean(y_train[indices[0]], axis=0)\n\n            predictions[i] = prediction\n        \n        return predictions.flatten()\n\n# Example usage\nif __name__ == \"__main__\":\n    import numpy as np\n    X = np.random.rand(100, 5)  # 100 samples, 5 features\n    y = np.random.rand(100)\n    predictor = LOOPredictor(X, y, k=3, weights='distance')\n    loo_predictions = predictor.predict_loo()\n    print(loo_predictions)", "CodeBleu_score": 24.871143512273893}, {"code": "def predict_loo(self)\ndef predict_loo(self):\n    \"\"\"\n    This function performs leave-one-out (LOO) prediction using k-nearest neighbors.\n    It first computes the distances and indices of the k-nearest neighbors using the kneighbors method.\n    Weights for the neighbors are obtained using the _get_weights function.\n    If the target variable _y is one-dimensional, it is reshaped to a two-dimensional array.\n    Predictions are calculated by averaging the neighbor target values, either unweighted or weighted, depending \n    on the weights parameter.\n    The weighted prediction involves summing the product of neighbor target values and their corresponding weights, \n    divided by the sum of the weights.\n    If the original target variable _y was one-dimensional, the predictions are flattened back to a one-dimensional\n    array.\n    The function returns the leave-one-out predictions.\n    \"\"\"\n    import numpy as np\n\n    # Ensure _y is two-dimensional\n    if self._y.ndim == 1:\n        original_1d = True\n        y = self._y.reshape(-1, 1)\n    else:\n        original_1d = False\n        y = self._y\n    \n    # Preallocate predictions array\n    predictions = np.zeros_like(y)\n\n    # Perform LOO predictions\n    for i in range(len(self._X)):\n        # Exclude current sample\n        X_train = np.delete(self._X, i, axis=0)\n        y_train = np.delete(y, i, axis=0)\n        \n        # Re-train the model ignoring the current instance\n        self._knn.fit(X_train, y_train.ravel())  # Ensure y is suitable for fitting.\n\n        # k-nearest neighbors\n        dists, indices = self._knn.kneighbors(self._X[i].reshape(1, -1), return_distance=True)\n\n        # Get weights for neighbors\n        weights = self._get_weights(dists)\n\n        # Prediction for the left-out sample\n        neighbor_targets = y_train[indices.flatten()]\n        if weights is None:\n            prediction = np.mean(neighbor_targets)\n        else:\n            # Weighted prediction\n            prediction = np.sum(neighbor_targets * weights) / np.sum(weights)\n        \n        predictions[i] = prediction\n\n    # Reshape predictions to original shape if needed\n    if original_1d:\n        predictions = predictions.ravel()\n\n    return predictions\n\n# Note: In this implementation, several placeholder members and methods are referenced (\n# such as `self._knn` and `self._get_weights`) assuming that they are defined elsewhere\n# in the class containing this function. You'll need to replace these or adapt the method to\n# fit actual code context. Also, ensure that `self._X` and `self._y` are appropriately preprocessed \n# and available.", "CodeBleu_score": 34.18062429365932}]}
{"instance_id": "b316687d9e9b", "generation_result": [{"code": "import numpy as np\nfrom sklearn.isotonic import IsotonicRegression\n\ndef isotonic(input_data, quantile_list):\n    \"\"\"\n    This function applies isotonic regression to each row of the input data based on the specified quantile list.\n    It first reshapes the quantile list into a one-dimensional array and determines the batch size from the input data's first dimension.\n    For each row in the input data, it fits and transforms the data using IsotonicRegression with the given quantile list.\n    The transformed data for each row is collected and stacked into a new array, which is then returned.\n    \"\"\"\n    # Ensure quantile_list is a numpy array for operations\n    quantile_list = np.array(quantile_list).flatten()\n\n    # Get the number of rows in the input data\n    n_rows = input_data.shape[0]\n    \n    # Empty list to collect transformed rows\n    transformed_data = []\n    \n    # Apply isotonic regression to each row\n    for i in range(n_rows):\n        # Create an IsotonicRegression model\n        ir = IsotonicRegression(increasing=True)\n        \n        # Fit the model to the data of the current row with the corresponding quantiles\n        y_ = ir.fit_transform(quantile_list, input_data[i])\n        \n        # Append the transformed data to the list\n        transformed_data.append(y_)\n    \n    # Stack the list of transformed data into a new numpy array\n    return np.vstack(transformed_data)\n\n# Example usage:\n# Assuming some dummy data, modifying as needed for actual situations\ninput_data = np.array([[0.1, 0.4, 0.3, 0.8], [0.5, 0.2, 0.9, 0.6]])\nquantile_list = [0, 0.33, 0.66, 1]  # Example quantiles\noutput = isotonic(input_data, quantile_list)\nprint(output)", "CodeBleu_score": 44.09690092368289}, {"code": "import numpy as np\nfrom sklearn.isotonic import IsotonicRegression\n\ndef isotonic(input_data, quantile_list):\n    \"\"\"\n    This function applies isotonic regression to each row of the input data based on the specified quantile list.\n    It first reshapes the quantile list into a one-dimensional array and determines the batch size from the input\n    data's first dimension. For each row in the input data, it fits and transforms the data using IsotonicRegression\n    with the given quantile list. The transformed data for each row is collected and stacked into a new array, which\n    is then returned.\n\n    Parameters:\n        input_data (numpy.array): A 2D numpy array where each row represents a sequence of measurements.\n        quantile_list (list): A list of quantiles that will be used as weights for the isotonic regression.\n\n    Returns:\n        numpy.array: A 2D numpy array with the isotonic regression applied to each row of the input data.\n    \"\"\"\n    # Convert quantile_list into a numpy array\n    quantile_array = np.array(quantile_list).flatten()\n\n    # Determine the number of rows in the input dataset\n    num_rows = input_data.shape[0]\n\n    # Prepare storage for the transformed data\n    transformed_data = np.zeros_like(input_data)\n\n    # Process each row with Isotonic Regression\n    for i in range(num_rows):\n        # Initialize the IsotonicRegression model with the specified quantile_array\n        model = IsotonicRegression()\n        y = input_data[i, :]\n\n        # Check if the input row has the same size as the quantile_list\n        if len(quantile_array) != len(y):\n            raise ValueError(\"The input data row and the quantile array must have the same length.\")\n\n        # Fit the model and transform the data\n        transformed_data[i, :] = model.fit_transform(quantile_array, y)\n\n    return transformed_data", "CodeBleu_score": 37.49620255603489}, {"code": "import numpy as np\nfrom sklearn.isotonic import IsotonicRegression\n\ndef isotonic(input_data, quantile_list):\n    \"\"\"\n    This function applies isotonic regression to each row of the input data based on the specified quantile list.\n    It first reshapes the quantile list into a one-dimensional array and determines the batch size from the input data's first dimension.\n    For each row in the input data, it fits and transforms the data using IsotonicRegression with the given quantile list.\n    The transformed data for each row is collected and stacked into a new array, which is then returned.\n\n    Parameters:\n    - input_data (np.array): A 2D numpy array where rows represent samples and columns represent features.\n    - quantile_list (list): A list of target quantile values.\n\n    Returns:\n    - np.array: A 2D numpy array with isotonic regression applied to each row, transforming the values to fit the quantiles.\n    \"\"\"\n    # Reshape the quantile list into a one-dimensional array\n    quantile_array = np.array(quantile_list).flatten()\n\n    # Create an empty list to collect transformed rows\n    transformed_data = []\n\n    # Create a IsotonicRegression instance\n    iso_reg = IsotonicRegression()\n\n    # Process each row in the input data\n    for row in input_data:\n        # Fit and transform isotonic regression\n        # It's important to note that fitting needs x (features) and y (target), here we assume both are the same\n        x = np.linspace(0, 1, len(row))  # Generate a series of equally spaced numbers for x\n        fitted_row = iso_reg.fit_transform(x, row)\n\n        # Append the transformed data\n        transformed_data.append(fitted_row)\n\n    # Stack transformed data into a numpy array and return\n    return np.vstack(transformed_data)\n\n# Example usage:\ninput_data = np.array([[1, 2, 2, 3], \n                       [4, 3, 2, 1]])\nquantile_list = [0.1, 0.5, 0.9, 1.0]\noutput = isotonic(input_data, quantile_list)\nprint(output)", "CodeBleu_score": 33.49593629027486}]}
{"instance_id": "17566e088b4e", "generation_result": [{"code": "def _train_vectorizer(text_data: list, vectorizer):\n    \"\"\"\n    Trains a vectorizer on the provided text data. It fits the vectorizer to the text data and transforms it into\n    a matrix representation. To reduce the object size significantly, it sets the stop_words_ attribute of the\n    vectorizer to None, which does not affect usability. The function returns the trained vectorizer and the \n    transformed matrix.\n    \n    Args:\n    text_data (list): A list of textual data to train the vectorizer on.\n    vectorizer: An instance of a vectorizer (e.g., from scikit-learn: CountVectorizer, TfidfVectorizer).\n\n    Returns:\n    tuple: A tuple containing the trained vectorizer and the transformed matrix.\n    \"\"\"\n    # Fit the vectorizer on the provided text data\n    vectorizer.fit(text_data)\n    \n    # Transform the text data with the fitted vectorizer\n    transformed_data = vectorizer.transform(text_data)\n    \n    # Assign None to the stop_words_ attribute to reduce memory usage\n    if hasattr(vectorizer, 'stop_words_'):\n        vectorizer.stop_words_ = None\n    \n    # Return the vectorizer and the transformed data as a tuple\n    return vectorizer, transformed_data", "CodeBleu_score": 70.21377441993269}, {"code": "def _train_vectorizer(text_data: list, vectorizer)\ndef _train_vectorizer(text_data: list, vectorizer):\n    \"\"\"\n    This function trains a vectorizer on the provided text data.\n    It fits the vectorizer to the text data and transforms it into a matrix representation.\n    To reduce the object size significantly, it sets the stop_words_ attribute of the vectorizer to None, which does not affect usability.\n    The function returns the trained vectorizer and the transformed matrix.\n    \n    Args:\n    text_data (list): A list of documents (each document is a string) to vectorize.\n    vectorizer: A sklearn-like vectorizer instance (e.g., CountVectorizer, TfidfVectorizer).\n\n    Returns:\n    tuple: A tuple containing the vectorizer and the transformed data matrix.\n    \"\"\"\n    # Fit the vectorizer to the text data\n    trained_vectorizer = vectorizer.fit(text_data)\n    \n    # Transform text data to matrix format\n    transformed_matrix = trained_vectorizer.transform(text_data)\n    \n    # To reduce the memory footprint, set stop_words_ to None if it exists\n    if hasattr(trained_vectorizer, 'stop_words_'):\n        trained_vectorizer.stop_words_ = None\n    \n    return trained_vectorizer, transformed_matrix", "CodeBleu_score": 49.8675950259304}, {"code": "def _train_vectorizer(text_data: list, vectorizer):\n    \"\"\"\n    Trains a vectorizer on the provided text data.\n    It fits the vectorizer to the text data and transforms it into a matrix representation.\n    To reduce the object size significantly, it sets the stop_words_ attribute of the vectorizer to None, which does not affect usability.\n    The function returns the trained vectorizer and the transformed matrix.\n    \n    Parameters:\n        text_data (list): A list of text documents to be vectorized.\n        vectorizer: An instance of a text vectorizer (e.g., from sklearn.feature_extraction.text).\n\n    Returns:\n        vectorizer: The vectorizer fitted to the text data.\n        X: The document-term matrix obtained from text_data using the fitted vectorizer.\n    \"\"\"\n    # Fit the vectorizer to the text data\n    X = vectorizer.fit_transform(text_data)\n    \n    # To reduce memory usage, remove the stop_words_ if present\n    if hasattr(vectorizer, 'stop_words_'):\n        vectorizer.stop_words_ = None\n    \n    # The transformer and X matrix are returned\n    return vectorizer, X", "CodeBleu_score": 69.89123912971735}]}
{"instance_id": "be23c0afc2d6", "generation_result": [{"code": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef preprocess(df, test_size, shuffle):\n    \"\"\"\n    This function preprocesses a dataframe by cleaning and splitting it into training and testing sets.\n    It first removes rows with missing entries marked by '--'.\n    Columns with unique values less than or equal to one or containing 'id' in their name are dropped.\n    Shortcut columns like 'days_to_death' and 'year_of_death' are also removed.\n    Finally, the dataframe is split into training and testing sets using train_test_split with specified test_size and shuffle parameters.\n    The function returns the training and testing dataframes.\n    \"\"\"\n    # Remove rows with '--' as missing entries\n    df = df.replace('--', pd.NA).dropna()\n\n    # Define columns to drop: those with a unique count <= 1 or containing 'id'\n    drop_columns = [col for col in df.columns if (df[col].nunique() <= 1) or ('id' in col)]\n    \n    # Additional columns to drop as mentioned in the docstring\n    drop_columns += ['days_to_death', 'year_of_death']\n    \n    # Remove duplicates that might have been added into drop_columns\n    drop_columns = list(set(drop_columns))\n    \n    # Drop the unnecessary columns from the dataframe\n    df = df.drop(columns=drop_columns, errors='ignore')\n\n    # Splitting the dataframe into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=test_size, shuffle=shuffle)\n\n    return train_df, test_df", "CodeBleu_score": 37.4458255060112}, {"code": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef preprocess(df, test_size, shuffle):\n    \"\"\"\n    This function preprocesses a dataframe by cleaning and splitting it into training and testing sets.\n    It first removes rows with missing entries marked by '--'.\n    Columns with unique values less than or equal to one or containing \"id\" in their name are dropped.\n    Shortcut columns like \"days_to_death\" and \"year_of_death\" are also removed.\n    Finally, the dataframe is split into training and testing sets using train_test_split with specified test_size and shuffle parameters.\n    The function returns the training and testing dataframes.\n    \"\"\"\n    \n    # Cleaning Data:\n    # Remove rows with any '--' entries\n    df.replace('--', pd.NA, inplace=True)\n    df.dropna(inplace=True)\n    \n    # Drop columns based on certain conditions\n    columns_to_drop = []\n    for column in df.columns:\n        if 'id' in column.lower():\n            columns_to_drop.append(column)\n        elif df[column].nunique() <= 1:\n            columns_to_drop.append(column)\n    \n    # Additional specific columns to drop\n    specific_columns_to_drop = ['days_to_death', 'year_of_death']\n    columns_to_drop.extend([col for col in specific_columns_to_drop if col in df.columns])\n    \n    # Drop the selected columns\n    df.drop(columns=columns_to_drop, inplace=True)\n    \n    # Splitting the data\n    train_df, test_df = train_test_split(df, test_size=test_size, shuffle=shuffle)\n    \n    return train_df, test_df", "CodeBleu_score": 35.76878270530886}, {"code": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef preprocess(df, test_size=0.2, shuffle=True):\n    \"\"\"\n    Preprocess a dataframe by cleaning and splitting it into training and testing sets.\n    - It first removes rows with missing entries marked by \"--\".\n    - Columns with unique values less than or equal to one or containing \"id\" in their name are dropped.\n    - Shortcut columns like \"days_to_death\" and \"year_of_death\" are also removed.\n    - Finally, the dataframe is split into training and testing sets using train_test_split with the specified test_size and shuffle parameters.\n    Parameters:\n        df (pd.DataFrame): The input dataframe.\n        test_size (float): The proportion of the dataset to include in the test split.\n        shuffle (bool): Whether or not to shuffle the data before splitting.\n    Returns:\n        tuple: Tuple containing training and testing dataframes (train_df, test_df).\n    \"\"\"\n\n    # Remove rows where any cell has '--'\n    df = df.replace('--', pd.NA)\n    df = df.dropna()\n\n    # Drop columns with unique values less than or equal to one or 'id' in the column name\n    cols_to_drop = [col for col in df.columns if df[col].nunique() <= 1 or 'id' in col.lower()]\n    df = df.drop(columns=cols_to_drop)\n    \n    # Drop shortcut columns\n    shortcut_columns = ['days_to_death', 'year_of_death']\n    df = df.drop(columns=[col for col in shortcut_columns if col in df.columns])\n\n    # Split the data into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=test_size, shuffle=shuffle)\n\n    return train_df, test_df", "CodeBleu_score": 40.60168749332029}]}
{"instance_id": "6a82997a8c59", "generation_result": [{"code": "from sklearn.datasets import load_iris, fetch_20newsgroups\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.utils import shuffle\nfrom scipy.sparse import csr_matrix\nimport numpy as np\n\ndef _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False):\n    \"\"\"\n    Test classifier's iterative fitting process on a specified dataset, defaults to iris.\n    \n    Parameters:\n    - classifier: a sklearn-like classifier with `fit`, `predict`, and possibly `partial_fit` methods.\n    - dataset: str, the name of the dataset to use, defaults to 'iris'. Can be 'iris' or other names for extension.\n    - sparse: bool, whether to convert data into a sparse format.\n    \n    Returns:\n    - predictions: the predicted labels on the test data.\n    - true_labels: the true labels of the test data.\n    - fitted_classifier: the classifier after it has been iteratively fitted.\n    \"\"\"\n    if dataset == \"iris\":\n        data = load_iris()\n        X, y = data.data, data.target\n    else:\n        raise ValueError(\"Dataset not supported\")\n\n    if sparse:\n        X = csr_matrix(X)\n\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    # Dense data might need scaling when converted to sparse\n    if sparse:\n        scaler = MaxAbsScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_test = scaler.transform(X_test)\n\n    # Assume the classifier can accept 'partial_fit' and has 'classes_' attribute\n    classes = np.unique(y)\n    fitted_classifier = classifier(random_state=42)\n    \n    # Initial iterative fit\n    num_iterations = 2\n\n    for _ in range(num_iterations):\n        fitted_classifier.partial_fit(X_train, y_train, classes=classes)\n\n    # Continue fitting, doubling iterations until convergence or a specified condition\n    while not hasattr(fitted_classifier, 'converged') or not fitted_classifier.converged:\n        num_iterations *= 2\n        for _ in range(num_iterations):\n            fitted_classifier.partial_fit(X_train, y_train, classes=classes)\n            if hasattr(fitted_classifier, 'converged') and fitted_classifier.converged:\n                break\n\n    # Make predictions on the test data\n    predictions = fitted_classifier.predict(X_test)\n\n    return predictions, y_test, fitted_classifier", "CodeBleu_score": 32.51956654052769}, {"code": "from sklearn.datasets import load_iris\nfrom scipy import sparse\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import clone\n\ndef _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False):\n    \"\"\"\n    Tests a classifier's iterative fitting process on a specified dataset, which defaults to the iris dataset.\n    It first loads the training and testing data, with an option to make the data sparse.\n    It retrieves the classifier's hyperparameter search space and obtains the default configuration.\n    The classifier is initialized with this default configuration and a random state, then fitted iteratively to the training data for an initial two iterations with refitting enabled.\n    The function continues iterative fitting, doubling the number of iterations each time, until the classifier is fully fitted.\n    Finally, it makes predictions on the test data and returns the predictions, true test labels, and the fitted classifier.\n    \n    Args:\n    - classifier: The machine learning classifier to test, preferably with support for `set_params` and `partial_fit`.\n    - dataset (str, optional): The name of the dataset to use. Currently only supports \"iris\".\n    - sparse (bool, optional): If True, converts the feature matrix to a sparse matrix.\n    \n    Returns:\n    - tuple: A tuple containing:\n        - predictions: An array of predictions from the fitted classifier.\n        - y_test: The true labels of the test set.\n        - classifier: The fitted classifier.\n    \"\"\"\n    \n    # Load dataset\n    if dataset == \"iris\":\n        data = load_iris()\n        X, y = data.data, data.target\n\n    # Optionally convert to sparse matrix\n    if sparse:\n        X = sparse.csr_matrix(X)\n\n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    # Initialize classifier with default configuration and random state\n    default_params = classifier.get_params() # Assuming `classifier` has `get_params`\n    classifier.set_params(**default_params, random_state=42)\n\n    # Start iterative fitting\n    num_iterations = 2\n    while True:\n        try:\n            classifier.partial_fit(X_train, y_train, classes=np.unique(y))\n        except Exception as e:\n            break\n        if hasattr(classifier, 'converged_'):\n            if classifier.converged_:\n                break\n        num_iterations *= 2\n\n        if num_iterations > len(X_train):  # Check if number of iterations exceeds number of samples\n            break\n\n    # Make predictions on the test data\n    predictions = classifier.predict(X_test)\n\n    # Returns predictions, true test labels, and the fitted classifier\n    return predictions, y_test, classifier", "CodeBleu_score": 36.952394074195055}, {"code": "import numpy as np\nfrom sklearn.datasets import load_iris, make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom scipy.sparse import csr_matrix\n\ndef _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False):\n    \"\"\"\n    Tests a classifier's iterative fitting process on a specified dataset, which defaults to the iris dataset.\n    It first loads the training and testing data, with an option to make the data sparse.\n    It retrieves the classifier's hyperparameter search space and obtains the default configuration.\n    The classifier is initialized with this default configuration and a random state, then fitted iteratively to the \n    training data for an initial two iterations with refitting enabled.\n    The function continues iterative fitting, doubling the number of iterations each time, until the classifier is fully fitted.\n    Finally, it makes predictions on the test data and returns the predictions, true test labels, and the fitted classifier.\n    \n    Args:\n    - classifier (sklearn estimator): A classifier that supports partial fit or a similar incremental learning method.\n    - dataset (str, optional): The dataset to load for testing. Defaults to \"iris\".\n    - sparse (bool, optional): If set to True, converts X to sparse matrix. Defaults to False.\n\n    Returns:\n    - predictions (ndarray): Predicted labels for the test set.\n    - y_test (ndarray): True labels for the test set.\n    - classifier (sklearn estimator): The fitted classifier object.\n    \"\"\"\n    \n    # Load dataset\n    if dataset == \"iris\":\n        data = load_iris()\n    else:\n        # Generates a synthetic dataset as placeholder for other dataset choices\n        data = make_classification(n_samples=1000, n_features=20, n_informative=2, \n                                   n_classes=3, n_clusters_per_class=1, random_state=42)\n    X, y = data.data, data.target\n    \n    # Optionally make the data sparse\n    if sparse:\n        X = csr_matrix(X)\n    \n    # Retrieve the default configuration if classifier supports default_params attribute or method\n    if hasattr(classifier, 'default_params'):\n        default_config = classifier.default_params()\n        classifier.set_params(**default_config)\n    \n    # Set a random state for reproducibility if applicable\n    if 'random_state' in classifier.get_params().keys():\n        classifier.set_params(random_state=42)\n    \n    # Split the data into training and testing\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n    \n    # This assumes existence of an iterative fitting method like 'partial_fit' or similar in classifier\n    if hasattr(classifier, 'partial_fit'):\n        classes = np.unique(y)\n        max_iters = 2\n        while not hasattr(classifier, \"fully_fitted\") or not classifier.fully_fitted:\n            current_iters = min(max_iters, y_train.size)\n            for _ in range(current_iters):\n                classifier.partial_fit(X_train, y_train, classes)\n            max_iters *= 2  # Double the iterations for the next round\n    \n    # Prediction\n    predictions = classifier.predict(X_test)\n    \n    return predictions, y_test, classifier", "CodeBleu_score": 37.29706700535657}]}
{"instance_id": "d0d8450c4c17", "generation_result": [{"code": "def _load_models(self):\n    \"\"\"\n    This function loads models based on the specified ensemble class and resampling strategy.\n    If an ensemble class is defined, it loads the ensemble using the backend and a seed.\n    If the ensemble is not loaded and certain evaluator output conditions and resampling strategies are met, it loads the best individual model instead.\n    If the ensemble is successfully loaded, it retrieves model identifiers and loads the corresponding models using the backend.\n    For specific resampling strategies, it also loads cross-validated models; otherwise, it sets the cross-validated models to None.\n    If no ensemble is loaded, it initializes empty lists for models and cross-validated models.\n    \"\"\"\n    try:\n        # Try loading the ensemble if ensemble class is defined and seed is available\n        if hasattr(self, 'ensemble_class') and hasattr(self, 'seed'):\n            self.ensemble = self.backend.load_ensemble(self.ensemble_class, self.seed)\n\n        if self.ensemble is not None:\n            # Load models based on ensemble identifiers\n            self.models = [self.backend.load_model(model_id) for model_id in self.ensemble.get_model_ids()]\n            \n            # Load or set cross-validated models based on resampling strategy\n            if self.resampling_strategy in {'cv', 'bootstrap'}:\n                self.cross_validated_models = [self.backend.load_cv_model(model_id) for model_id in self.ensemble.get_model_ids()]\n            else:\n                self.cross_validated_models = None\n        else:\n            # Conditions when ensemble might not load and an alternative model loading is needed\n            if any(condition for condition in ['condition1', 'condition2']):  # Placeholder condition checks\n                best_model_id = self.evaluator.get_best_model_id()\n                self.models = [self.backend.load_model(best_model_id)]\n                self.cross_validated_models = self.backend.load_cv_model(best_model_id) if self.resampling_strategy in {'cv', 'bootstrap'} else None\n            else:\n                # Initialize empty values if no models are loaded\n                self.models = []\n                self.cross_validated_models = []\n\n    except Exception as e:\n        print(f\"Error loading models: {str(e)}\")\n        # Optionally, handle specific exceptions if necessary\n        self.models = []\n        self.cross_validated_models = []", "CodeBleu_score": 39.811078925614176}, {"code": "def _load_models(self):\n    ensemble_loaded = False\n    self.models = []\n    self.cross_validated_models = None\n\n    # If an ensemble class is defined, try loading the ensemble\n    if self.ensemble_class is not None:\n        try:\n            self.ensemble = self.backend.load_ensemble(self.ensemble_class, self.seed)\n            ensemble_loaded = True\n        except Exception as e:\n            print(f\"Failed to load ensemble due to: {e}\")\n            ensemble_loaded = False\n\n    # If no ensemble is loaded and certain conditions are met, load the best individual model\n    if not ensemble_loaded:\n        if self._evaluator_output_conditions_met() and self.resampling_strategy in ['specific_strategy_1', 'specific_strategy_2']:\n            try:\n                self.models = [self.backend.load_best_individual_model()]\n            except Exception as e:\n                print(f\"Failed to load best individual model due to: {e}\")\n    else:\n        # Retrieves model identifiers and loads corresponding models\n        model_identifiers = self.backend.get_model_identifiers(self.ensemble)\n        try:\n            self.models = [self.backend.load_model(mid) for mid in model_identifiers]\n            if self.resampling_strategy in ['cv', 'another_cv_strategy']:  # Example CV strategies\n                self.cross_validated_models = [self.backend.load_cv_model(mid) for mid in model_identifiers]\n            else:\n                self.cross_validated_models = None\n        except Exception as e:\n            print(f\"Failed to load models or cross-validated models due to: {e}\")\n\ndef _evaluator_output_conditions_met(self):\n    \"\"\"\n    Checks if specific conditions from the evaluator's output are met.\n    \"\"\"\n    # Placeholder for actual condition checks related to evaluator output. Assuming return True for simplicity.\n    return True", "CodeBleu_score": 38.63765991596755}, {"code": "def _load_models(self)\ndef _load_models(self):\n    \"\"\"\n    This function loads models based on the specified ensemble class and resampling strategy.\n    If an ensemble class is defined, it loads the ensemble using the backend and a seed.\n    If the ensemble is not loaded and certain evaluator output conditions and resampling strategies are met, it loads the best individual model instead.\n    If the ensemble is successfully loaded, it retrieves model identifiers and loads the corresponding models using the backend.\n    For specific resampling strategies, it also loads cross-validated models; otherwise, it sets the cross-validated models to None.\n    If no ensemble is loaded, it initializes empty lists for models and cross-validated models.\n    \"\"\"\n    \n    # Try to load ensemble if the class is specified\n    if self.ensemble_class:\n        try:\n            self.ensemble = self.backend.load_ensemble(self.seed)\n            models_to_load = self.ensemble.get_model_identifiers()\n        except Exception as e:\n            print(f\"Failed to load ensemble: {e}\")\n            self.ensemble = None\n\n    # If no ensemble loaded due to error or none specified, load the best model meeting conditions\n    if self.ensemble is None:\n        if self.meets_output_conditions():\n            self.models = [self.backend.load_best_model(resampling_strategy=self.resampling_strategy)]\n        else:\n            self.models = []\n\n        self.cv_models = None\n\n    # If ensemble was loaded, load each model in the ensemble\n    if self.ensemble:\n        self.models = []\n        for model_id in models_to_load:\n            try:\n                model = self.backend.load_model_by_id(model_id)\n                self.models.append(model)\n            except Exception as e:\n                print(f\"Error loading model {model_id}: {e}\")\n\n        # Load cross-validated models based on resampling strategy if needed\n        if self.should_load_cv_models():\n            self.cv_models = [self.backend.load_cv_model_by_id(model_id) for model_id in models_to_load]\n        else:\n            self.cv_models = None\n\n    # Function to determine if evaluator's output conditions are met\n    def meets_output_conditions(self):\n        # Add logic to determine if evaluator's output conditions and resampling strategy requirements are met\n        return True\n\n    # Function to determine if cross-validated models should be loaded\n    def should_load_cv_models(self):\n        # Add logic to determine based on resampling strategy\n        return True\n\n# Note: This is the structure and logic based on the specifications from the docstring. Actual implementation of methods `load_ensemble`, `load_best_model`, `load_model_by_id`, and `load_cv_model_by_id` along with the methods `meets_output_conditions` and `should_load_cv_models` will depend on your system and backend specifics.", "CodeBleu_score": 38.67964110167251}]}
{"instance_id": "ef71b1cddb50", "generation_result": [{"code": "import openml\n\ndef load_task(task_id):\n    \"\"\"\n    This function loads data for a given task ID using the OpenML library.\n    It retrieves the task and extracts features (X) and labels (y).\n    It obtains train-test split indices from the task and uses them to create training and testing sets for both features and labels.\n    The function fetches the dataset associated with the task to determine the categorical or numerical nature of each feature.\n    It maps unique label values to integer indices for both training and testing labels.\n    The function returns the training features, training labels, testing features, testing labels, and a list indicating whether each feature is categorical or numerical.\n    \n    Args:\n    task_id (int): The OpenML task ID for which data needs to be fetched\n\n    Returns:\n    tuple: A tuple containing the following elements:\n        - train_features (np.ndarray): The training features matrix\n        - train_labels (np.ndarray): The training labels vector\n        - test_features (np.ndarray): The testing features matrix\n        - test_labels (np.ndarray): The testing labels vector\n        - feature_types (list): A list of 'categorical' or 'numerical' strings indicating the type of each feature\n    \"\"\"\n    # Load the task from OpenML\n    task = openml.tasks.get_task(task_id)\n    \n    # Get the dataset object\n    dataset = task.get_dataset()\n    \n    # Get the numpy format data from the dataset\n    X, y, _, attribute_names = dataset.get_data(target=dataset.default_target_attribute)\n    \n    # Determine the types of the attributes\n    feature_types = ['categorical' if isinstance(attribute[1], openml.datasets.data_attribute.CategoricalAttribute)\n                     else 'numerical' for attribute in dataset.features.values()]\n    \n    # Get the split indices from the task\n    train_indices, test_indices = task.get_train_test_split_indices(repeat=0, fold=0, sample=0) \n    \n    # Prepare the train and test sets\n    train_features = X[train_indices]\n    train_labels = y[train_indices]\n    test_features = X[test_indices]\n    test_labels = y[test_indices]\n    \n    # Convert labels into integer indices\n    from sklearn.preprocessing import LabelEncoder\n    le = LabelEncoder()\n    train_labels_enc = le.fit_transform(train_labels)\n    test_labels_enc = le.transform(test_labels)\n    \n    return (train_features, train_labels_enc, test_features, test_labels_enc, feature_types)", "CodeBleu_score": 41.42210715492279}, {"code": "import openml\n\ndef load_task(task_id):\n    \"\"\"\n    This function loads data for a given task ID using the OpenML library.\n    It retrieves the task and extracts features (X) and labels (y).\n    It obtains train-test split indices from the task and uses them to create training and testing sets for both features and labels.\n    The function fetches the dataset associated with the task to determine the categorical or numerical nature of each feature.\n    It maps unique label values to integer indices for both training and testing labels.\n    The function returns the training features, training labels, testing features, testing labels, and a list indicating whether each feature is categorical or numerical.\n    \n    Args:\n    task_id (int): The ID of the task to be loaded.\n\n    Returns:\n    tuple: A tuple containing:\n        - X_train (numpy array): Training features.\n        - y_train (numpy array): Training labels.\n        - X_test (numpy array): Testing features.\n        - y_test (numpy array): Testing labels.\n        - feature_types (list): List indicating if features are 'categorical' or 'numerical'.\n    \"\"\"\n    \n    # Fetch the task using its ID\n    task = openml.tasks.get_task(task_id)\n    \n    # Get the dataset object\n    dataset = task.get_dataset()\n    \n    # Download the data and get features & labels\n    X, y, categorical_indicator, attribute_names = dataset.get_data(\n        target=dataset.default_target_attribute,\n        dataset_format='array'\n    )\n    \n    # Get train and test indices\n    train_indices, test_indices = task.get_train_test_split_indices(repeat=0, fold=0, sample=0)\n    \n    # Prepare train & test data sets\n    X_train, y_train = X[train_indices], y[train_indices]\n    X_test, y_test = X[test_indices], y[test_indices]\n    \n    # Determine if features are categorical or numerical\n    feature_types = ['categorical' if cat else 'numerical' for cat in categorical_indicator]\n    \n    # Convert labels y to continuous integers if they are not numeric\n    from sklearn.preprocessing import LabelEncoder\n    le = LabelEncoder()\n    y_train = le.fit_transform(y_train)\n    y_test = le.transform(y_test)\n    \n    return X_train, y_train, X_test, y_test, feature_types", "CodeBleu_score": 43.519122097014694}, {"code": "import openml\nfrom sklearn.model_selection import train_test_split\n\ndef load_task(task_id):\n    \"\"\"\n    This function loads data for a given task ID using the OpenML library.\n    It retrieves the task and extracts features (X) and labels (y).\n    It obtains train-test split indices from the task and uses them to create training and testing sets for both features and labels.\n    The function fetches the dataset associated with the task to determine the categorical or numerical nature of each feature.\n    It maps unique label values to integer indices for both training and testing labels.\n    The function returns the training features, training labels, testing features, testing labels, and a list indicating whether each feature is categorical or numerical.\n    \"\"\"\n    \n    # Retrieve the task\n    task = openml.tasks.get_task(task_id)\n    \n    # Get the associated dataset\n    dataset = task.get_dataset()\n    \n    # Download the data (features X and labels y)\n    X, y, _, attribute_names = dataset.get_data(target=dataset.default_target_attribute, dataset_format='array')\n    \n    # Obtain the split from the task, e.g., for cross-validation, only first repetion/ fold is considered for simplicity\n    train_indices, test_indices = task.get_train_test_split_indices(repeat=0, fold=0)\n    \n    # Create train and test data from indices\n    X_train, y_train = X[train_indices], y[train_indices]\n    X_test, y_test = X[test_indices], y[test_indices]\n    \n    # Check each feature for being categorical or numerical\n    nominal_indices = dataset.get_features_by_type('nominal', [dataset.default_target_attribute])\n    feature_types = ['categorical' if i in nominal_indices else 'numerical' for i in range(X.shape[1])]\n    \n    # Map labels to integer indices\n    # Typically, label encoding is needed if labels are not in integer form, skipped here\n    # Assuming the labels are already appropriate for model training (this could check and be adjusted if real application)\n\n    return X_train, y_train, X_test, y_test, feature_types\n\n# Call the function with a specific task_id, demonstration only, valid task_id needed\n# task_id = 31  # Example task ID (needs to be valid and exist in OpenML)\n# X_train, y_train, X_test, y_test, feature_types = load_task(task_id)\n# print(\"Feature Types:\", feature_types)", "CodeBleu_score": 39.77058036571659}]}
{"instance_id": "90bd6edbcc9e", "generation_result": [{"code": "import numpy as np\nimport pandas as pd\nfrom scipy.sparse import isspmatrix, spmatrix\nfrom typing import Union, List\n\ndef convert_if_sparse(\n    y: Union[np.ndarray, spmatrix, List, pd.DataFrame, pd.Series]\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]:\n    \"\"\"\n    Converts a sparse matrix input into a dense format.\n    If the input `y` is a sparse matrix, it is converted to a dense array using the `toarray` method.\n    For one-dimensional sparse data, the resulting array is flattened before returning.\n    If the input `y` is not a sparse matrix, it is returned unchanged.\n    The function supports returning the data as a NumPy array, list, pandas DataFrame, or pandas Series.\n    \n    Parameters:\n    y : Union[np.ndarray, spmatrix, List, pd.DataFrame, pd.Series]\n        The input data which may be a sparse matrix or other supported data type.\n\n    Returns:\n    Union[np.ndarray, List, pd.DataFrame, pd.Series]\n        The data in a possibly converted, dense format.\n    \"\"\"\n    # Check if input is a sparse matrix\n    if isspmatrix(y):\n        # Convert sparse matrix to a dense NumPy array\n        y_dense = y.toarray()\n        \n        # Flatten the array if it is one-dimensional\n        if y_dense.ndim == 1:\n            y_dense = y_dense.flatten()\n        \n        return y_dense\n    \n    # Return the input unchanged if it is not a sparse matrix\n    return y", "CodeBleu_score": 47.86443844722344}, {"code": "from scipy import sparse\nimport numpy as np\nimport pandas as pd\nfrom typing import Union, List\n\ndef convert_if_sparse(\n    y: Union[sparse.spmatrix, np.ndarray, List, pd.DataFrame, pd.Series]\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]:\n    \"\"\"\n    This function converts a sparse matrix input into a dense format.\n    If the input y is a sparse matrix, it is converted to a dense array using the toarray method.\n    For one-dimensional sparse data, the resulting array is flattened before returning.\n    If the input y is not a sparse matrix, it is returned unchanged.\n    The function supports returning the data as a NumPy array, list, pandas DataFrame, or pandas Series.\n    \"\"\"\n    # Check if the input is a sparse matrix\n    if isinstance(y, sparse.spmatrix):\n        # Convert the sparse matrix to a dense numpy array\n        y_dense = y.toarray()\n        # Flatten the array if it is one-dimensional\n        if y_dense.ndim == 1 or (y_dense.ndim == 2 and y_dense.shape[0] == 1):\n            y_dense = y_dense.flatten()\n        return y_dense\n\n    # If the input is not a sparse matrix, return it as is\n    return y", "CodeBleu_score": 45.9359961788848}, {"code": "import numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom typing import Union, List\n\ndef convert_if_sparse(\n    y: Union[np.ndarray, sparse.spmatrix, List, pd.DataFrame, pd.Series]\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]:\n    \"\"\"\n    This function converts a sparse matrix input into a dense format.\n    If the input y is a sparse matrix, it is converted to a dense array using the toarray method.\n    For one-dimensional sparse data, the resulting array is flattened before returning.\n    If the input y is not a sparse matrix, it is returned unchanged.\n    The function supports returning the data as a NumPy array, list, pandas DataFrame, or pandas Series.\n    \"\"\"\n    if sparse.issparse(y):  # Check if the input is a sparse matrix\n        y_dense = y.toarray()\n        if y_dense.ndim == 1:  # Check for one-dimensional data\n            y_dense = y_dense.ravel()\n        return y_dense\n    else:\n        return y  # Return as is if not a sparse matrix\n\n# Example usage:\nsparse_matrix = sparse.csr_matrix([[1, 0, 0], [0, 0, 3]])\ndense_output = convert_if_sparse(sparse_matrix)\nprint(\"Converted dense output:\", dense_output)", "CodeBleu_score": 42.33141618212255}]}
{"instance_id": "5728e3d4f914", "generation_result": [{"code": "def _calculate_weights(self) -> None:\n    import numpy as np\n\n    # Total number of models might infer from the maximum index in ensemble_members\n    number_of_models = max(self.ensemble_members) + 1\n\n    # Initialize weights array with zeros\n    weights = np.zeros(number_of_models)\n\n    # Count occurrences of each member\n    for member in self.ensemble_members:\n        weights[member] += 1\n\n    # Ensemble size\n    ensemble_size = len(self.ensemble_members)\n\n    # Calculate weights: frequency divided by ensemble size\n    weights /= ensemble_size\n\n    # Normalize weights if their sum is less than 1 (due to numerical issues or others)\n    weight_sum = np.sum(weights)\n    if weight_sum < 1:\n        weights /= weight_sum\n\n    # Store the calculated weights\n    self.weights_ = weights", "CodeBleu_score": 34.73979823395153}, {"code": "def _calculate_weights(self) -> None:\n    \"\"\"\n    This function calculates the weights for ensemble members based on their occurrence frequency in the ensemble.\n    It counts the occurrences of each ensemble member index and initializes a weights array with zeros, having a length equal to the number of input models.\n    For each ensemble member, it calculates the weight as the frequency divided by the ensemble size and assigns it to the corresponding index in the weights array.\n    If the sum of weights is less than 1, the weights are normalized to ensure their sum equals 1.\n    The calculated weights are then stored in the instance variable self.weights_.\n    \"\"\"\n    from collections import Counter\n    import numpy as np\n\n    # Assuming self.ensemble_indices_ holds the indices of ensemble members\n    if not hasattr(self, 'ensemble_indices_'):\n        raise AttributeError(\"The attribute 'ensemble_indices_' is not found in the instance.\")\n\n    # Counting occurrence of each ensemble member index\n    index_counter = Counter(self.ensemble_indices_)\n\n    # Initialize weights with zeros, with length equal to the number of unique models\n    number_of_models = len(set(self.ensemble_indices_))\n    weights = np.zeros(number_of_models)\n\n    # Calculate weight for each index\n    ensemble_size = len(self.ensemble_indices_)\n    for index, count in index_counter.items():\n        weights[index] = count / ensemble_size\n\n    # Normalizing weights if their sum is not 1\n    total_weight = np.sum(weights)\n    if total_weight < 1:\n        weights = weights / total_weight\n\n    # Assigning the calculated weights to instance variable\n    self.weights_ = weights", "CodeBleu_score": 36.49024490687394}, {"code": "def _calculate_weights(self):\n    \"\"\"\n    This function calculates the weights for ensemble members based on their occurrence frequency in the ensemble.\n    It counts the occurrences of each ensemble member index and initializes a weights array with zeros, having a length equal to the number of input models.\n    For each ensemble member, it calculates the weight as the frequency divided by the ensemble size and assigns it to the corresponding index in the weights array.\n    If the sum of weights is less than 1, the weights are normalized to ensure their sum equals 1.\n    The calculated weights are then stored in the instance variable self.weights_.\n    \"\"\"\n    from collections import Counter\n    import numpy as np\n\n    # Example ensemble indices (this should be replaced with actual data)\n    ensemble_indices = self.ensemble_indices  # Assuming this is a list of indices of ensemble members\n\n    # Count occurrences of each index\n    index_counts = Counter(ensemble_indices)\n\n    # Initialize weights array\n    num_models = len(set(ensemble_indices))  # Assuming number of models is the number of unique indices\n    weights = np.zeros(num_models)\n\n    # Calculate weights\n    ensemble_size = len(ensemble_indices)\n    for index, count in index_counts.items():\n        weights[index] = count / ensemble_size\n\n    # Normalize weights if their sum is less than 1\n    if np.sum(weights) < 1:\n        weights /= np.sum(weights)\n\n    # Store the weights in the instance variable\n    self.weights_ = weights", "CodeBleu_score": 35.76959535083624}]}
{"instance_id": "4f9c4799fd8f", "generation_result": [{"code": "def predict_proba(self, X, batch_size=None, n_jobs=1):\n    \"\"\"\n    This function predicts class probabilities for input data X, optionally using specified batch size and number of jobs.\n    It calls the superclass's predict_proba method to obtain the predicted probabilities.\n    If the target type is not \"multilabel-indicator\", it asserts that the sum of probabilities for each instance is 1.\n    It also asserts that all probability values lie between 0 and 1.\n    The function returns the predicted probabilities.\n    \n    Parameters:\n        X: array-like or sparse matrix of shape (n_samples, n_features)\n            Input features.\n        batch_size: int, optional (default=None)\n            The number of samples to be processed at once.\n        n_jobs: int, optional (default=1)\n            The number of CPU cores to use when parallelizing the predictions.\n    \n    Returns:\n        numpy array of shape (n_samples, n_classes)\n            The predicted class probabilities of the input samples.\n    \"\"\"\n\n    # Call the superclass method to get the predicted probabilities\n    # This assumes that 'self' is an instance of a classifier that inherits from another\n    # classifier with a predict_proba method, or has it defined in the current class.\n    probabilities = super().predict_proba(X)\n\n    # Checks if probabilities are valid\n    if self._estimator_type != 'multilabel-indicator':\n        # For each predicted probability array, check if the sum is approximately 1\n        assert all(np.isclose(np.sum(probs), 1) for probs in probabilities), \"The sum of probabilities should be approximately 1 for each instance.\"\n\n    # Check that all probability values are between 0 and 1\n    assert np.all((probabilities >= 0) & (probabilities <= 1)), \"All probability values should be between 0 and 1.\"\n\n    return probabilities", "CodeBleu_score": 37.33890450201998}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1):\n    \"\"\"\n    This function predicts class probabilities for the input data X, optionally using specified batch size and number of jobs.\n    It calls the superclass's predict_proba method to obtain the predicted probabilities.\n    If the target type is not \"multilabel-indicator\", it asserts that the sum of probabilities for each instance is 1.\n    It also asserts that all probability values lie between 0 and 1.\n    The function returns the predicted probabilities.\n    \n    Parameters:\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n        batch_size : int, optional\n            Size of the batches of data (default is None, for no batching).\n        n_jobs : int, optional\n            Number of jobs to run in parallel (default is 1, serial execution).\n            \n    Returns:\n        predicted_probabilities : array, shape (n_samples, n_classes)\n            The class probabilities of the input samples.\n    \"\"\"\n    \n    # Call the superclass method to predict probabilities\n    predicted_probabilities = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n\n    # Check probability values and sums\n    assert all(0 <= prob <= 1 for probs in predicted_probabilities for prob in probs), \\\n        \"All probability values should be between 0 and 1\"\n\n    # Check if the target type needs sum of probabilities to be 1\n    if self.target_type != \"multilabel-indicator\":\n        assert all(abs(sum(probs) - 1) < 1e-6 for probs in predicted_probabilities), \\\n            \"The sum of probabilities for each instance should be 1\"\n\n    return predicted_probabilities", "CodeBleu_score": 52.34631230334574}, {"code": "def predict_proba(self, X, batch_size=None, n_jobs=1):\n    \"\"\"\n    This function predicts class probabilities for input data X, optionally using specified batch size and number of jobs.\n    It calls the superclass's predict_proba method to obtain the predicted probabilities.\n    If the target type is not \"multilabel-indicator\", it asserts that the sum of probabilities for each instance is 1.\n    It also asserts that all probability values lie between 0 and 1.\n    The function returns the predicted probabilities.\n    \n    Args:\n    - X (array-like): input data to predict.\n    - batch_size (int, optional): size of batches for prediction.\n    - n_jobs (int, optional): number of parallel jobs for prediction.\n\n    Returns:\n    - numpy.ndarray: an array of predicted probabilities.\n    \"\"\"\n    # Assuming superclass has a predict_proba method we can call\n    probabilities = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n\n    # Check if probabilities are valid:\n    for prob in probabilities:\n        # Assert all probabilities are between 0 and 1\n        assert all(0 <= p <= 1 for p in prob), \"Probabilities must be between 0 and 1\"\n\n    # Finalizing the probabilities sum check unless it's a multilabel indicator\n    if getattr(self, 'target_type', '') != \"multilabel-indicator\":\n        for prob in probabilities:\n            # Assert the sum of probabilities for each instance is close to 1\n            assert abs(sum(prob) - 1.0) < 1e-6, \"The sum of probabilities for each instance should be 1\"\n\n    # Return the calculated probabilities\n    return probabilities", "CodeBleu_score": 39.91462736556306}]}
{"instance_id": "a9a13150aebe", "generation_result": [{"code": "import numpy as np\n\ndef convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This function converts a binary matrix Ybin to a numeric array.\n    If Ybin is not a one-dimensional array, it calculates the dot product of Ybin \n    with a range of integers corresponding to its second dimension's size,\n    effectively converting binary class indicators to numeric class labels.\n    The function returns the resulting numeric array.\n    \"\"\"\n    if Ybin.ndim > 1:\n        # Create an array of integers that represent each column's index (class label)\n        columns_indices = np.arange(Ybin.shape[1])\n        # Multiply each row in Ybin by these columns indices and sum across rows\n        return np.dot(Ybin, columns_indices)\n    else:\n        # Direct conversion if Ybin is already a one-dimensional array\n        return Ybin", "CodeBleu_score": 36.9202013662337}, {"code": "import numpy as np\n\ndef convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Converts a binary matrix Ybin to a numeric array.\n    \n    If Ybin is not a one-dimensional array, it calculates the dot product of Ybin with a range of integers \n    corresponding to its second dimension's size, effectively converting binary class indicators to numeric \n    class labels.\n    \n    Parameters:\n        Ybin (np.ndarray): A binary matrix of shape (n_samples, n_classes),\n                           where each row represents a sample and each column represents a class.\n                           A value of 1 in a column of a row indicates that the sample belongs to that class.\n    \n    Returns:\n        np.ndarray: A numpy array of shape (n_samples,) containing numeric class labels.\n    \"\"\"\n    if Ybin.ndim == 1:\n        # If Ybin is already a 1-dimensional array, return it directly\n        return Ybin\n    else:\n        # Assuming the second dimension represents the classes and is in one-hot encoded format\n        classes = np.arange(Ybin.shape[1])\n        # Convert binary class indicators to numeric class labels\n        labels = np.dot(Ybin, classes)\n        return labels\n\n# Example usage:\nYbin = np.array([[0, 1], [1, 0], [0, 1]])\nlabels = convert_to_num(Ybin)\nprint(\"Numeric labels:\", labels)", "CodeBleu_score": 39.04946122716028}, {"code": "import numpy as np\n\ndef convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    \"\"\"\n    This function converts a binary matrix Ybin to a numeric array.\n    If Ybin is not a one-dimensional array, it calculates the dot product of Ybin with a \n    range of integers corresponding to its second dimension's size, effectively \n    converting binary class indicators to numeric class labels.\n    The function returns the resulting numeric array.\n    \n    Args:\n    Ybin (np.ndarray): A 2D numpy array where each row represents a one-hot encoded binary vector.\n\n    Returns:\n    np.ndarray: A 1D numpy array with numeric class labels.\n    \"\"\"\n\n    # Establish the number of classes based on the shape of Ybin\n    num_classes = Ybin.shape[1]\n    \n    # Create an array of integers corresponding to the indices of the classes\n    indices = np.arange(num_classes)\n    \n    # Calculate the numeric class labels via the dot product\n    # This assumes that each row of Ybin contains exactly one '1', indicating the class\n    numeric_labels = np.dot(Ybin, indices)\n    \n    return numeric_labels", "CodeBleu_score": 43.81032501007839}]}
{"instance_id": "e97d38e62f81", "generation_result": [{"code": "def fit_transformer(self, X, y, fit_params=None):\n    \"\"\"\n    Fits a transformer to the data X and labels y, applying any provided fit parameters.\n    \n    Args:\n    - X: Data to be transformed.\n    - y: Corresponding labels for the data.\n    - fit_params (dict, optional): Additional parameters for fitting the transformer. Defaults to None.\n    \n    Returns:\n    - tuple: Transformed data and updated fit parameters.\n    \"\"\"\n    # Initialize fit_params if None\n    if fit_params is None:\n        fit_params = {}\n    \n    # Check if balancing strategy is \"weighting\" in the configuration\n    if self.config.get('balancing_strategy') == 'weighting':\n        # Create a Balancing object and obtain weights\n        balancer = Balancing()\n        weights = balancer.get_weights(self.classifier_choice, self.feature_preprocessor_choice)\n        \n        # Update 'sample_weight' in fit_params\n        fit_params['sample_weight'] = weights\n\n    # Update initialization parameters\n    init_params = self.get_initialization_parameters()\n    self.update_hyperparameters(init_params)\n    \n    # Merge any additional fit parameters that may have been fetched\n    additional_fit_params = self.get_additional_fit_params()\n    fit_params.update(additional_fit_params)\n    \n    # Call superclass's fit_transform method with updated parameters\n    transformed_X = super().fit_transformer(X, y, fit_params)\n    \n    # Return transformed data and updated fit parameters\n    return transformed_X, fit_params", "CodeBleu_score": 45.707164734577475}, {"code": "def fit_transformer(self, X, y, fit_params=None):\n    \"\"\"\n    This function fits a transformer to the data X and labels y, applying any provided fit parameters.\n    If no fit parameters are provided, it initializes an empty dictionary.\n    If the balancing strategy in the configuration is set to \"weighting\", it creates a Balancing object and \n    obtains weights for the classifier and feature preprocessor choices.\n    It updates the initialization parameters and sets the hyperparameters with the updated configuration \n    and initialization parameters.\n    Any additional fit parameters obtained are merged with the existing fit parameters.\n    The superclass's fit_transformer method is called with the data, labels, and fit parameters.\n    The function returns the transformed data and the fit parameters.\n    \"\"\"\n    \n    # Check if fit_params is provided, otherwise use an empty dictionary\n    if fit_params is None:\n        fit_params = {}\n        \n    # Perform balancing if configured to use \"weighting\"\n    if self.config.get('balancing_strategy', '') == 'weighting':\n        balancer = Balancing()\n        weights = balancer.calculate_weights(X, y, self.config['classifier'], self.config['feature_preprocessor'])\n        fit_params['sample_weight'] = weights\n    \n    # Update initialization parameters based on updated configuration\n    self.init_params.update(self.config)\n    \n    # Set the hyperparameters based on updated configuration and initialization parameters\n    self.hyperparameters.set_params(**self.init_params)\n    \n    # Merge any additional parameters (simulating this step for the purpose of the example)\n    fit_params.update({'additional_params': 'values'})  # Example additional parameters\n    \n    # Calling the superclass method\n    transformed_data = super().fit_transform(X, y, **fit_params)\n    \n    return transformed_data, fit_params", "CodeBleu_score": 48.23839164851557}, {"code": "class DataTransformer:\n    def __init__(self, config):\n        self.config = config\n        self.transformer = None\n\n    def fit_transformer(self, X, y, fit_params=None):\n        \"\"\"\n        This function fits a transformer to the data X and labels y, applying any provided fit parameters.\n        If no fit parameters are provided, it initializes an empty dictionary.\n        If the balancing strategy in the configuration is set to \"weighting\", it creates a Balancing object and obtains weights for the classifier and feature preprocessor choices.\n        It updates the initialization parameters and sets the hyperparameters with the updated configuration and initialization parameters.\n        Any additional fit parameters obtained are merged with the existing fit parameters.\n        The superclass's fit_transformer method is called with the data, labels, and fit parameters.\n        The function returns the transformed data and the fit parameters.\n        \"\"\"\n        if fit_params is None:\n            fit_params = {}\n\n        # Handling weighting if specified in the configuration\n        if self.config.get('balancing_strategy') == 'weighting':\n            # Assuming Balancing is a class that takes the config and adjusts weights\n            balancer = Balancing(self.config)\n            weights = balancer.calculate_weights(X, y)\n            fit_params['weights'] = weights\n\n        # Update hyperparameters if any additional parameters are provided\n        if 'hyperparameters' in self.config:\n            self.transformer.set_params(**self.config['hyperparameters'])\n\n        # Assume the superclass or transformer itself has a method fit_transformer\n        transformed_X = self.transformer.fit_transform(X, y, **fit_params)\n\n        return transformed_X, fit_params\n\n# Assuming a basic Balancing class structure\nclass Balancing:\n    def __init__(self, config):\n        self.config = config\n    \n    def calculate_weights(self, X, y):\n        # Placeholder logic as no details are given for balancing computation\n        return {'sample_weight': [1]*len(y)}", "CodeBleu_score": 43.43811759038823}]}
{"instance_id": "b1aaad80a9c4", "generation_result": [{"code": "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Tweedie\n\ndef test_glm_identity_link_func():\n    \"\"\"\n    This function tests the Generalized Linear Model (GLM) with an identity link function.\n    It wraps the GLM from the Statsmodels library using a custom wrapper and sets it up\n    with a Tweedie family and Power(1) link (identity link).\n    The model is fitted with a small dataset.\n    An assembler is used to convert the fitted model into an expression representation.\n    The test compares the assembled expression to an expected expression using an assertion to ensure they match.\n    \"\"\"\n    # Create a simple dataset\n    data = {\n        'X': [1, 2, 3, 4, 5],\n        'Y': [2, 4, 5, 4, 5]\n    }\n    df = pd.DataFrame(data)\n\n    # Define the predictor and response variables\n    X = df[['X']]\n    y = df['Y']\n\n    # Add a constant to the predictor variable\n    X = sm.add_constant(X)\n\n    # Define the GLM model with Tweedie family and Identity Link\n    model = sm.GLM(y, X, family=Tweedie(var_power=1, link=sm.families.links.identity()))\n\n    # Fit the model\n    results = model.fit()\n\n    # Assemble the expression in the form 'intercept + coef*X'\n    # Here we assume there are only two coefficients, intercept and one for X\n    intercept, coef = results.params\n    expression = f\"{intercept:.3f} + {coef:.3f}*X\"\n\n    # Expected expression based on the coefficients learned from the data (for ease of illustration)\n    # Ideally, one would have specific values expected based on theoretical calculations or previous run.\n    expected_expression = \"0.200 + 0.800*X\"\n\n    # Assertion to check if the assembled expression matches the expected expression\n    assert expression == expected_expression, f\"Expected: {expected_expression}, Got: {expression}\"\n\n    # In practice, you may want to use more sophisticated assertion, possibly checking\n    # the closeness of the coefficients individually within certain epsilon due to numerical errors in fit.", "CodeBleu_score": 34.72920997566803}, {"code": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Tweedie\n\ndef test_glm_identity_link_func():\n    \"\"\"\n    This function tests the Generalized Linear Model (GLM) with an identity link function.\n    It wraps the GLM from the Statsmodels library using a custom wrapper and sets it up with a Tweedie family and Power(1) link.\n    The model is fitted with a small dataset.\n    An assembler is used to convert the fitted model into an expression representation.\n    The test compares the assembled expression to an expected expression using an assertion to ensure they match.\n    \"\"\"\n    # Create a simple dataset\n    data = pd.DataFrame({\n        'x1': np.random.normal(size=100),\n        'x2': np.random.normal(size=100),\n        'y': np.random.normal(size=100)\n    })\n    \n    # Define the model with Tweedie family (variance power=1 --> Normal Distribution simulating identity link)\n    family = Tweedie(var_power=1)  # Power parameter 1 implies identity link\n\n    # Fit the GLM model\n    glm_model = sm.GLM(data['y'], sm.add_constant(data[['x1', 'x2']]), family=family)\n    glm_results = glm_model.fit()\n\n    # Assemble the model expression (simplified version)\n    coeffs = glm_results.params\n    expression = f\"y = {coeffs['const']:.3f} + {coeffs['x1']:.3f}*x1 + {coeffs['x2']:.3f}*x2\"\n\n    # Define expected expression (assuming some known model output for simplicity)\n    expected_expression = \"y = 0.000 + 1.000*x1 + 1.000*x2\"  # This should be replaced with actual known quantities if available\n\n    # Assert to check if the assembled expression matches the expected one\n    assert expression == expected_expression, f\"Expected: {expected_expression}, Got: {expression}\"\n\n# The function doesn't actually run in this setting\u2014it needs to be called or used in a test suite.", "CodeBleu_score": 30.12348585338478}, {"code": "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.genmod.families import Tweedie\nfrom statsmodels.genmod.families.links import Power\n\ndef test_glm_identity_link_func():\n    \"\"\"\n    This function tests the Generalized Linear Model (GLM) with an identity link function.\n    It wraps the GLM from the Statsmodels library using a custom wrapper and sets \n    it up with a Tweedie family and Power(1) link.\n    The model is fitted with a small dataset.\n    An assembler is used to convert the fitted model into an expression representation.\n    The test compares the assembled expression to an expected expression using an assertion to ensure they match.\n    \"\"\"\n\n    # Creating a small dataset\n    data = {\n        'X1': [1, 2, 3, 4, 5],\n        'X2': [2, 3, 4, 5, 6],\n        'Y': [3, 4, 5, 6, 7]\n    }\n    df = pd.DataFrame(data)\n    \n    # Specify model components\n    family = Tweedie(var_power=1) # Tweedie with power = 1 (variance proportional to mean)\n    link = Power(power=1) # Identity link (power=1)\n    \n    # Fit GLM model\n    model = sm.GLM(df['Y'], df[['X1', 'X2']], family=family, link=link)\n    results = model.fit()\n\n    # Assemble the model into an expression\n    # Assuming a simple string representation would work here. In reality, you might have\n    # a more complex way of representing the model.\n    expected_expression = \"Y = Intercept + X1_coef*X1 + X2_coef*X2\"\n    fitted_expression = f\"Y = {results.params['Intercept']:0.3f} + {results.params['X1']:0.3f}*X1 + {results.params['X2']:0.3f}*X2\"\n\n    # Assert that the fitted expression matches the expected pattern\n    assert expected_expression in fitted_expression, \"Assembled expression does not match expected structure\"\n    \n    print(\"Test passed, assembled expressions match.\")\n\n# To run the test\ntest_glm_identity_link_func()", "CodeBleu_score": 31.762590597778285}]}
{"instance_id": "bcab63e592d0", "generation_result": [{"code": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series\nimport pandas as pd\nfrom typing import Callable, Tuple, Union, Any\n\ndef _compute_aggregation(\n        self, col: str, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series:\n    \"\"\"\n    Computes an aggregation for a specified column in a grouped DataFrame.\n    It handles named aggregation to avoid conflicts with column names when resetting the index.\n    If the aggregation is provided as a tuple, it extracts the name and function; otherwise, it determines\n    the name from the string or function name.\n    The function constructs a new column name based on the original column name and the aggregation name.\n    It returns the result of applying the aggregation to the grouped DataFrame, with the new column name.\n\n    Args:\n    - col (str): The name of the column in the DataFrame to which the aggregation will be applied.\n    - agg (Union[str, Callable, Tuple[str, Union[str, Callable]]]): The aggregation operation as a string name,\n      a callable function, or a tuple containing a custom name and either a string or a callable.\n\n    Returns:\n    - pd.Series: The result of the aggregation applied to the column, including a custom named Series.\n    \"\"\"\n\n    if isinstance(agg, tuple):\n        # Aggregation provided as a tuple with custom name and function\n        agg_name, agg_func = agg\n    else:\n        # Only an aggregation function or string is provided\n        agg_func = agg\n        # Determine the aggregation name from the function or string directly\n        if callable(agg):\n            agg_name = agg.__name__\n        else:\n            agg_name = agg\n\n    # Construct the new column name as \"original_colName_aggName\"\n    new_col_name = f\"{col}_{agg_name}\"\n\n    # Apply the aggregation\n    if callable(agg_func):\n        result = self.grouped[col].apply(agg_func).rename(new_col_name)\n    else:\n        result = self.grouped[col].agg(agg_func).rename(new_col_name)\n\n    return result\n\n# Please note this function implies it's part of a class with a `self.grouped` attribute\n# which represents a grouped pandas DataFrame. The grouped object should be defined elsewhere in the class.\n# Usage would be in a context something like this:\n\n# class DataProcessor:\n#     def __init__(self, dataframe):\n#         self.dataframe = dataframe\n\n#     def process(self):\n#         self.grouped = self.dataframe.groupby(some_keys)\n#         # Eventually, this class would define self.grouped and use _compute_aggregation\n\n# If this function is standalone and isn't part of a class, remove \"self\" and pass the grouped DataFrame or modify accordingly.", "CodeBleu_score": 37.58733508095226}, {"code": "import pandas as pd\nfrom typing import Union, Callable, Tuple\n\nclass DataFrameAggregator:\n    def __init__(self, df: pd.DataFrame):\n        self.grouped_df = df.groupby(...)  # Modify according to how df has to be grouped\n        \n    def _compute_aggregation(self, col: str, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]) -> pd.Series:\n        \"\"\"\n        This function computes an aggregation for a specified column in a grouped DataFrame.\n        It handles named aggregation to avoid conflicts with column names when resetting the index.\n        If the aggregation is provided as a tuple, it extracts the name and function; otherwise,\n        it determines the name from the string or function name.\n        The function constructs a new column name based on the original column name and the aggregation name.\n        It returns the result of applying the aggregation to the grouped DataFrame, with the new column name.\n        \n        Params:\n            col (str): Column name on which aggregation is applied.\n            agg (Union[str, Callable, Tuple[str, Union[str, Callable]]]): Aggregation method.\n        \n        Returns:\n            pd.Series: Aggregated Series with the new column name.\n        \"\"\"\n        if isinstance(agg, tuple):  # Check if agg is a tuple (name, function)\n            agg_name, agg_func = agg\n        else:\n            agg_func = agg\n            if callable(agg):  # if agg is a function\n                agg_name = agg.__name__\n            else:  # if agg is a string\n                agg_name = agg\n                \n        new_col_name = f\"{col}_{agg_name}\"\n        \n        # Perform aggregation\n        if callable(agg_func):\n            result = self.grouped_df[col].agg(agg_func)\n        else:\n            result = self.grouped_df[col].agg(agg_name)\n        \n        # Renaming the result to avoid conflicts\n        result.name = new_col_name\n        \n        return result\n\n# This is a basic illustrative example. Usage of this class and method may require actual DataFrame and more setup.", "CodeBleu_score": 37.79622102673157}, {"code": "import pandas as pd\nfrom typing import Union, Callable, Tuple\n\nclass DataProcessor:\n    def __init__(self, grouped_data: pd.core.groupby.generic.DataFrameGroupBy):\n        self.grouped_data = grouped_data\n\n    def _compute_aggregation(\n        self, col: str, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series:\n        \"\"\"\n        This function computes an aggregation for a specified column in a grouped DataFrame.\n        It handles named aggregation to avoid conflicts with column names when resetting the index.\n        If the aggregation is provided as a tuple, it extracts the name and function; otherwise,\n        it determines the name from the string or function name.\n        The function constructs a new column name based on the original column name and the aggregation name.\n        It returns the result of applying the aggregation to the grouped DataFrame, with the new column name.\n\n        Parameters:\n            col (str): The column to perform the aggregation on.\n            agg (Union[str, Callable, Tuple[str, Union[str, Callable]]]):\n                The aggregation function or a tuple containing the name of the aggregation and the function.\n\n        Returns:\n            pd.Series: The aggregated data with the new column name.\n        \"\"\"\n        \n        if isinstance(agg, tuple):\n            agg_name, agg_func = agg\n        else:\n            agg_func = agg\n            if isinstance(agg, str):\n                agg_name = agg\n            else:  # Callable\n                agg_name = agg.__name__\n\n        new_col_name = f\"{col}_{agg_name}\"\n        \n        # Apply given aggregation function and rename the column\n        if isinstance(agg_func, str):\n            result = self.grouped_data[col].agg(agg_func).rename(new_col_name)\n        else:  # Callable\n            result = self.grouped_data[col].apply(agg_func).rename(new_col_name)\n            \n        return result\n\n# Example Usage\n# Assuming you have a DataFrame 'df' and it has been grouped by some column(s)\n# grouped_df = df.groupby(['grouping_column'])\n# processor = DataProcessor(grouped_df)\n# aggregated_series = processor._compute_aggregation('data_column', ('custom_name', np.mean))\n# print(aggregated_series)", "CodeBleu_score": 41.22070259192331}]}
{"instance_id": "114d623dae7b", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import pearsonr\n\ndef _corrfunc(self, x, y, **kws):\n    \"\"\"\n    This function calculates and annotates the Pearson correlation coefficient between two variables, x and y.\n    It handles optional hue groups, excluding rows where x or y is NaN.\n    It computes the correlation coefficient and adjusts the font size based on the number of hue groups and the correlation value.\n    It retrieves the appropriate axis and annotates it with the correlation coefficient, adjusting the position based on the hue group index.\n    The function is designed to work with different versions of seaborn, handling both old and new keyword arguments for axes.\n    \"\"\"\n    # Filter out NaNs \n    valid_indices = ~np.isnan(x) & ~np.isnan(y)\n    x = x[valid_indices]\n    y = y[valid_indices]\n\n    # Compute the Pearson correlation coefficient\n    corr_coef, _ = pearsonr(x, y)\n    corr_text = f'\u03c1 = {corr_coef:.2f}'\n\n    # Default arguments\n    hue_index = kws.pop('hue_index', 0)\n    num_hue_levels = kws.pop('num_hue_levels', 1)\n\n    # Adjust font size based on hue levels and correlation magnitude\n    font_size = np.clip(15 / np.sqrt(num_hue_levels), 9, 15) * np.clip(np.abs(corr_coef) + 0.5, 1, 1.5)\n    \n    # Retrieve the axis and add annotation\n    ax = kws.pop('ax', plt.gca())\n    xy = (0.05, 0.95 - 0.05 * hue_index)  # Adjust position based on the hue index\n    ax.annotate(corr_text, xy=xy, xycoords='axes fraction', fontsize=font_size,\n                verticalalignment='top', horizontalalignment='left')\n\n# Usage within a class\nclass DataAnalysis:\n    def __init__(self, data):\n        self.data = data\n        \n    _corrfunc = _corrfunc\n    \n# Example usage\nanalysis = DataAnalysis(None)\nanalysis._corrfunc(np.random.rand(100), np.random.rand(100) * 2, hue_index=0, num_hue_levels=3)\nplt.show()", "CodeBleu_score": 32.78005326345753}, {"code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass DataPlotter:\n    def __init__(self):\n        pass\n\n    def _corrfunc(self, x, y, **kws):\n        \"\"\"Calculate and annotate the Pearson correlation coefficient between two variables.\n\n        Handles optional hue groups, excludes NaN, computes and adjusts font size based on correlation value and hue groups.\n        Retrieves the correct axis for annotating the correlation value, placing the annotation based on hue group index.\n\n        Args:\n            x (array-like): The first variable data.\n            y (array-like): The second variable data.\n            **kws: Keyword arguments that can contain:\n                - ax: matplotlib axis object to plot on.\n                - hue_level: int or None, level of the hue group if any.\n        \"\"\"\n        # Handle where to plot the correlation value\n        ax = kws.pop('ax', plt.gca())  # Get current active axis, default to creating new if none is specifically provided.\n        hue_level = kws.get('hue_level', None)\n\n        # First, handle NaN values\n        df = pd.DataFrame({'x': x, 'y': y})\n        df = df.dropna()  # Remove rows where x or y is NaN\n\n        # Compute correlation coefficient\n        correlation_coefficient = df['x'].corr(df['y'])  # Default method is Pearson\n\n        # Prepare annotation text\n        corr_text = f'Corr: {correlation_coefficient:.2f}'\n\n        # Decide annotation position based on presence of hue groups\n        x_position = df['x'].max()  # Typically, annotations are put near the significant value area\n        y_position = df['y'].max()  # Alter this as per requirement or based on hue_level\n\n        # Adjust font size based on correlation magnitude\n        font_size = 12  # Default size\n        if np.abs(correlation_coefficient) > 0.5:\n            font_size += 4  # Larger text for higher correlation\n        \n        # Annotate the correlation coefficient in the plot\n        ax.annotate(corr_text, (x_position, y_position), fontsize=font_size, color='red', ha='right')\n\n        return ax\n\n# Example usage:\n# Plotter = DataPlotter()\n# Plotter._corrfunc(x_data, y_data, ax=some_matplotlib_axis, hue_level=some_level)", "CodeBleu_score": 31.384000762044632}, {"code": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef _corrfunc(self, x, y, **kws):\n    \"\"\"\n    This function calculates and annotates the Pearson correlation coefficient between two variables, x and y.\n    It handles optional hue groups, excluding rows where x or y is NaN.\n    It computes the correlation coefficient and adjusts the font size based on the number of hue groups and the correlation value.\n    It retrieves the appropriate axis and annotates it with the correlation coefficient, adjusting the position based on the hue group index.\n    The function is designed to work with different versions of seaborn, handling both old and new keyword arguments for axes.\n\n    Parameters:\n        x (pd.Series): The x variable data.\n        y (pd.Series): y variable data.\n        **kws: Keyword arguments that can contain metadata such as the label and position of the hue group.\n\n    Returns:\n        None: This function applies annotations directly to the plot.\n    \"\"\"\n    # Narrow down the data to exclude NaNs.\n    not_null = pd.notnull(x) & pd.notnull(y)\n    x, y = x[not_null], y[not_null]\n\n    # Compute the Pearson correlation coefficient.\n    from scipy.stats import pearsonr\n    r, _ = pearsonr(x, y)\n    \n    # Determine the number of hue groups using 'hue' keyword, if present.\n    hue = kws.get('hue', None)\n    hue_order = kws.get('hue_order', None)\n    if hue is not None and hue_order:\n        num_groups = len(hue_order)\n    else:\n        num_groups = 1\n\n    # Adjust font size based on the number of hue groups and the magnitude of the correlation coefficient.\n    font_size = np.clip(12 / np.sqrt(num_groups) * np.abs(r), 8, 15)\n    \n    # Get the axis to plot on, handling different keyword arguments depending on seaborn version.\n    ax = kws.get('ax', plt.gca())\n    \n    # Determine position for the text annotation based on optional arguments.\n    hue_index = kws.get('hue_index', 0)\n    text_x = 0.05 + 0.15 * hue_index\n    text_y = 0.95\n    \n    # Annotate the Pearson correlation coefficient on the plot.\n    ax.annotate(f'r = {r:.2f}', xy=(text_x, text_y), xycoords='axes fraction',\n                ha='left', va='top', fontsize=font_size, color='black')", "CodeBleu_score": 32.849259838251}]}
{"instance_id": "4c1ade308953", "generation_result": [{"code": "import decimal\n\ndef _round_digits(src: float, rounddigit: int = None, method='decimal'):\n    \"\"\"\n    This function rounds a given floating-point number src to a specified number of digits using one of three methods.\n    If the method is 'decimal', it uses Python's built-in round function.\n    If the method is 'sig', it uses the decimal module to set the precision and create a decimal representation of the number.\n    If the method is 'format', it formats the number as a string with a specified number of significant digits.\n    The function returns the rounded number according to the chosen method.\n    \"\"\"\n    if method == 'decimal':\n        if rounddigit is None:\n            raise ValueError(\"rounddigit must be specified for 'decimal' method\")\n        return round(src, rounddigit)\n    elif method == 'sig':\n        if rounddigit is None:\n            raise ValueError(\"rounddigit must be specified for 'sig' method\")\n        with decimal.localcontext() as ctx:\n            ctx.prec = rounddigit\n            d = decimal.Decimal(src)\n            return float(d.quantize(decimal.Decimal('1.' + ('0' * rounddigit))))\n    elif method == 'format':\n        if rounddigit is None:\n            raise ValueError(\"rounddigit must be specified for 'format' method\")\n        # Return as a float after formatting it to a string with a specified number of significant digits\n        return float(f\"{src:.{rounddigit}g}\")\n    else:\n        raise ValueError(\"Invalid rounding method. Choose 'decimal', 'sig', or 'format'\")\n\n# Example usage:\nprint(_round_digits(1234.5678, 5, 'decimal'))  # Example for 'decimal' method\nprint(_round_digits(1234.5678, 5, 'sig'))     # Example for 'sig' method\nprint(_round_digits(1234.5678, 5, 'format'))  # Example for 'format' method", "CodeBleu_score": 57.623801359253044}, {"code": "import decimal\n\ndef _round_digits(src: float, rounddigit: int = None, method='decimal'):\n    \"\"\"\n    This function rounds a given floating-point number src to a specified number of digits using one of three methods.\n    If the method is 'decimal', it uses Python's built-in round function.\n    If the method is 'sig', it uses the decimal module to set the precision and create a decimal representation of the number.\n    If the method is 'format', it formats the number as a string with a specified number of significant digits.\n    The function returns the rounded number according to the chosen method.\n    \n    Args:\n    - src (float): The source number to round.\n    - rounddigit (int, optional): The number of digits to round to.\n    - method (str): Method to use for rounding ('decimal', 'sig', 'format').\n\n    Returns:\n    - Rounded number, in its appropriate format (float or str depending on the method).\n    \"\"\"\n\n    if rounddigit is None:\n        return src  # If no rounding digit specified, return the original number.\n\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig': \n        with decimal.localcontext() as ctx:\n            ctx.prec = rounddigit  # Set precision for rounding\n            d = decimal.Decimal(src)\n            return float(d)  # Convert back to float after rounding\n    elif method == 'format':\n        # This creates a format string like '{:.3f}' for rounddigit=3\n        format_str = '{:.' + str(rounddigit) + 'f}'\n        formatted_number = format_str.format(src)\n        return float(formatted_number)  # Convert the formatted string back to float\n    else:\n        raise ValueError(\"Invalid method. Use 'decimal', 'sig', or 'format'.\")\n\n# Example usage\nresult = _round_digits(3.141592653589793, 3, 'decimal')\nprint(result)  # Should print 3.142\n\nresult = _round_digits(3.141592653589793, 3, 'sig')\nprint(result)  # Should print 3.14\n\nresult = _round_digits(3.141592653589793, 3, 'format')\nprint(result)  # Should print 3.142", "CodeBleu_score": 56.88266190967944}, {"code": "import decimal\n\ndef _round_digits(src: float, rounddigit: int = None, method='decimal'):\n    \"\"\"\n    This function rounds a given floating-point number src to a specified number of digits using one of three methods.\n    If the method is 'decimal', it uses Python's built-in round function.\n    If the method is 'sig', it uses the decimal module to set the precision and create a decimal representation of the number.\n    If the method is 'format', it formats the number as a string with a specified number of significant digits.\n    The function returns the rounded number according to the chosen method.\n    \n    Args:\n    src (float): The number to be rounded.\n    rounddigit (int, optional): The number of digits to round to. Defaults to None.\n    method (str, optional): The method of rounding ('decimal', 'sig', or 'format'). Defaults to 'decimal'.\n\n    Returns:\n    float or str: The rounded number, as a float or string based on the method.\n    \"\"\"\n    if method == 'decimal':\n        if rounddigit is not None:\n            rounded_number = round(src, rounddigit)\n        else:\n            rounded_number = round(src)\n        return rounded_number\n    \n    elif method == 'sig':\n        if rounddigit is not None:\n            with decimal.localcontext() as ctx:\n                ctx.prec = rounddigit\n                d = decimal.Decimal(src)\n                rounded_number = d.quantize(decimal.Decimal('1.' + '0' * (rounddigit - 1)), rounding=decimal.ROUND_HALF_UP)\n        else:\n            rounded_number = float(decimal.Decimal(src))\n        return float(rounded_number)\n    \n    elif method == 'format':\n        if rounddigit is not None:\n            rounded_number = format(src, '.' + str(rounddigit) + 'g')\n        else:\n            rounded_number = format(src, 'g')\n        return rounded_number\n    \n    else:\n        raise ValueError(\"Invalid rounding method specified. Choose 'decimal', 'sig', or 'format'.\")\n\n# Example usage:\nprint(_round_digits(3.14159, 4, 'decimal'))  # 3.1416\nprint(_round_digits(3.14159, 4, 'sig'))      # 3.142\nprint(_round_digits(3.14159, 4, 'format'))   # 3.142", "CodeBleu_score": 49.11351331870976}]}
{"instance_id": "a9923e678320", "generation_result": [{"code": "from typing import Dict, Union, Any\n\ndef _round_digits(value: float, digits: int, method: str) -> float:\n    if method == 'decimal':\n        return round(value, digits)\n    elif method == 'sig':\n        # Round to `digits` significant figures\n        if value != 0:\n            from math import log10, floor\n            return round(value, digits - int(floor(log10(abs(value)))) - 1)\n        return 0.0\n    elif method == 'format':\n        # This is a less precise rounding typically seen in financial context (string formatting)\n        format_str = f'{{:.{digits}f}}'\n        return float(format_str.format(value))\n    else:\n        raise ValueError(\"Invalid rounding method specified\")\n\ndef _round_dict_digits(cls, srcdict: Dict[str, Any], rounddigit: int = None, method: str = 'decimal') -> Dict[str, Any]:\n    \"\"\"\n    This function rounds the values in a dictionary to a specified number of digits using a chosen method.\n    It iterates through the input dictionary srcdict, and for each floating-point value, it rounds the value using the _round_digits\n    method if rounddigit is provided.\n    The method can be 'decimal', 'sig', or 'format'.\n    If the value is not a float or no rounding is specified, the original value is retained.\n    The function returns a new dictionary with the rounded values.\n    \"\"\"\n    if rounddigit is None:\n        return srcdict  # No rounding specified\n\n    rounded_dict = {}\n    for key, value in srcdict.items():\n        if isinstance(value, float):\n            rounded_value = _round_digits(value, rounddigit, method)\n            rounded_dict[key] = rounded_value\n        else:\n            # If not a float, return as is\n            rounded_dict[key] = value\n    return rounded_dict", "CodeBleu_score": 35.506527744961815}, {"code": "from decimal import Decimal, ROUND_HALF_UP\nfrom typing import Dict\n\ndef _round_digits(value: float, ndigits: int, method: str) -> float:\n    \"\"\"Rounds a numerical value to a specified number of digits using different rounding methods.\"\"\"\n    if method == 'decimal':\n        return round(value, ndigits)\n    elif method == 'sig':\n        return round(value, ndigits - len(str(int(value))) - 1)\n    elif method == 'format':\n        return float(\"{:.{}f}\".format(value, ndigits))\n    else:\n        raise ValueError(\"Invalid rounding method provided. Choose 'decimal', 'sig', or 'format'.\")\n\ndef _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal'):\n    \"\"\"This function rounds the values in a dictionary to a specified number of digits using a chosen method.\n    It iterates through the input dictionary srcdict, and for each floating-point value, it rounds the value using\n    the _round_digits method if rounddigit is provided. The method can be 'decimal', 'sig', or 'format'.\n    If the value is not a float or no rounding is specified, the original value is retained.\n    The function returns a new dictionary with the rounded values.\"\"\"\n    result_dict = {}\n    for key, value in srcdict.items():\n        if isinstance(value, float) and rounddigit is not None:\n            rounded_value = _round_digits(value, rounddigit, method)\n            result_dict[key] = rounded_value\n        else:\n            result_dict[key] = value\n    return result_dict\n\n# Example Usage\ndata = {\n    \"value1\": 10.12345,\n    \"value2\": 200.3546,\n    \"value3\": \"Not a number\",\n    \"value4\": 5000.0\n}\nrounded_data = _round_dict_digits(None, data, 2, 'decimal')\nprint(rounded_data)", "CodeBleu_score": 36.57692067579964}, {"code": "from typing import Dict, Union, Optional\n\ndef _round_digits(value: float, digits: int, method: str) -> float:\n    \"\"\"\n    Rounds a float value to the specified number of digits using a specified method.\n    \n    Parameters:\n        - value (float): The value to be rounded.\n        - digits (int): The number of digits to round to.\n        - method (str): Method of rounding, can be 'decimal', 'sig' (significant digits), or 'format'.\n    \n    Returns:\n        float: The rounded value.\n    \"\"\"\n    if method == 'decimal':\n        return round(value, digits)\n    elif method == 'sig':\n        # Significant figures rounding, requires a different approach\n        # Example with decimal module for signal figures:\n        import math\n        if value == 0:\n            return 0\n        else:\n            scale = math.floor(math.log10(abs(value))) + 1 - digits\n            return round(value, -scale)\n    elif method == 'format':\n        format_string = \"{0:.\" + str(digits) + \"f}\"\n        return float(format_string.format(value))\n\ndef _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: Optional[int] = None, method: str = 'decimal') -> Dict[str, Union[float, int]]:\n    \"\"\"\n    Rounds the values in a dictionary to a specified number of digits using a chosen method.\n    \n    Parameters:\n        - cls (object): Typically the Class object, if method inside a class.\n        - srcdict (Dict[str, float]): The source dictionary whose values are to be rounded.\n        - rounddigit (int, optional): The number of digits to round to. If None, defaults to no rounding.\n        - method (str): The method of rounding to use: 'decimal', 'sig', or 'format'.\n    \n    Returns:\n        Dict[str, Union[float, int]]: A new dictionary with rounded or original values.\n    \"\"\"\n    new_dict = {}\n    for key, value in srcdict.items():\n        if rounddigit is not None and isinstance(value, float):\n            rounded_value = _round_digits(value, rounddigit, method)\n            new_dict[key] = rounded_value\n        else:\n            new_dict[key] = value\n    return new_dict", "CodeBleu_score": 40.94087769060572}]}
{"instance_id": "b6106e079dc0", "generation_result": [{"code": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws):\n    \"\"\"\n    This function creates a scatter plot from numpy arrays using Seaborn.\n    It first combines the x and y values into a DataFrame with specified column names.\n    If hue_data is provided, it adds a hue field to the DataFrame for color-coding the points.\n    The function then plots the scatter plot using Seaborn's scatterplot function, \n    applying any additional keyword arguments for the plot and legend.\n    If no title is specified for the legend, it sets the title to the hue field name.\n\n    Parameters:\n        cls (class): The class to which this function belongs (used in a classmethod context).\n        x (numpy.ndarray): 1D numpy array of x values.\n        x_name (str): Column name to use for x values in the data DataFrame.\n        y (numpy.ndarray): 1D numpy array of y values.\n        y_name (str): Column name to use for y values in the data DataFrame.\n        hue_data (numpy.ndarray, optional): 1D numpy array of hue values for color-coding.\n        hue_name (str, optional): Column name to use for hue in the data DataFrame.\n        ax (matplotlib.axes._subplots.AxesSubplot): Matplotlib Axes object on which to draw.\n        scatter_kws (dict): Keyword arguments to pass to seaborn's scatterplot function.\n        legend_kws (dict): Keyword arguments for the legend; if 'title' not provided, hue_name is used.\n\n    Returns:\n        ax: The matplotlib Axes with the scatter plot.\n    \"\"\"\n    # Create a DataFrame with x and y data\n    data_dict = {x_name: x, y_name: y}\n    if hue_data is not None:\n        data_dict[hue_name] = hue_data\n    \n    data = pd.DataFrame(data_dict)\n\n    # Plot using seaborn's scatterplot\n    scatter_plot = sns.scatterplot(data=data, x=x_name, y=y_name, hue=hue_name if hue_data is not None else None, ax=ax, **scatter_kws)\n\n    # Configure legend\n    if 'title' not in legend_kws and hue_name:\n        legend_kws['title'] = hue_name\n    ax.legend(**legend_kws)\n\n    return ax\n\n# Example usage within a class setting (Since cls is not used here, the method can be static or class method.)\nclass Plotter:\n    @classmethod\n    def scatterplot(cls, x, x_name, y, y_name, hue_data=None, hue_name=None, ax=None, scatter_kws=None, legend_kws=None):\n        if ax is None:\n            ax = plt.gca()\n        if scatter_kws is None:\n            scatter_kws = {}\n        if legend_kws is None:\n            legend_kws = {}\n        return _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)", "CodeBleu_score": 48.6715073468441}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\nimport pandas as pd\nimport seaborn as sns\n\ndef _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws):\n    \"\"\"This function creates a scatter plot from numpy arrays using Seaborn.\n    It first combines the x and y values into a DataFrame with specified column names.\n    If hue_data is provided, it adds a hue field to the DataFrame for color-coding the points.\n    The function then plots the scatter plot using Seaborn's scatterplot function, applying any additional keyword arguments for the plot and legend.\n    If no title is specified for the legend, it sets the title to the hue field name.\n\n    Args:\n        cls (type): The class on which this method is possibly a class method.\n        x (np.ndarray): Array of x-values.\n        x_name (str): Name for the x-axis values.\n        y (np.ndarray): Array of y-values.\n        y_name (str): Name for the y-axis values.\n        hue_data (np.ndarray): Array of hue parameter.\n        hue_name (str): Name of the hue parameter.\n        ax (matplotlib.axes.Axes): Matplotlib axes object to plot on.\n        scatter_kws (dict): Additional keyword arguments for the seaborn scatter plot.\n        legend_kws (dict): Additional keyword arguments for the legend.\n    \"\"\"\n    # Create DataFrame using x and y data\n    data = pd.DataFrame({\n        x_name: x,\n        y_name: y\n    })\n    \n    # If hue data is provided, add it to the DataFrame\n    if hue_data is not None and hue_name is not None:\n        data[hue_name] = hue_data\n    \n    # Use seaborn to plot the data\n    sns.scatterplot(data=data, x=x_name, y=y_name, hue=hue_name, ax=ax, **scatter_kws)\n    \n    # Handle legend settings\n    if legend_kws is not None:\n        if 'title' not in legend_kws:\n            legend_kws['title'] = hue_name\n        ax.legend(**legend_kws)", "CodeBleu_score": 47.207851783541884}, {"code": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)\nimport pandas as pd\nimport seaborn as sns\n\ndef _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws):\n    \"\"\"\n    This function creates a scatter plot from numpy arrays using Seaborn.\n    It first combines the x and y values into a DataFrame with specified column names.\n    If hue_data is provided, it adds a hue field to the DataFrame for color-coding the points.\n    The function then plots the scatter plot using Seaborn's scatterplot function, applying any additional keyword arguments for the plot and legend.\n    If no title is specified for the legend, it sets the title to the hue field name.\n\n    Parameters:\n        cls: The class (used when called as a class method).\n        x (np.ndarray): Array of x values.\n        x_name (str): Name for the x values, used as a column name.\n        y (np.ndarray): Array of y values.\n        y_name (str): Name for the y values, used as a column name.\n        hue_data (np.ndarray, optional): Array of hue values for color coding.\n        hue_name (str, optional): Name for the hue data, used as a column name.\n        ax (matplotlib.axes.Axes): The axes object where the plot will be drawn.\n        scatter_kws (dict): Additional keyword arguments to pass to seaborn.scatterplot.\n        legend_kws (dict): Additional keyword arguments to pass to the legend.\n\n    Returns:\n        matplotlib.axes.Axes: The axes object with the plot.\n    \"\"\"\n    \n    # Construct DataFrame from x and y arrays\n    data = pd.DataFrame({x_name: x, y_name: y})\n    \n    # Add hue data if provided\n    if hue_data is not None:\n        data[hue_name] = hue_data\n    \n    # Create scatter plot\n    sns.scatterplot(x=x_name, y=y_name, hue=hue_name, data=data, ax=ax, **scatter_kws)\n    \n    # Configure legend\n    if legend_kws is not None:\n        if 'title' not in legend_kws:\n            legend_kws['title'] = hue_name  # Set legend title to the hue field name if not specified\n        ax.legend(**legend_kws)\n    \n    return ax", "CodeBleu_score": 51.17324970553422}]}
{"instance_id": "257e0aa5dd4d", "generation_result": [{"code": "import warnings\n\nclass TimeSeriesModel:\n    def __init__(self):\n        self.model = None\n        self.endog_data = None  # Placeholder for endogenous data\n        self.exog_data = None  # Placeholder for exogenous data\n\n    def fit(self, **fit_args) -> \"TimeSeriesModel\":\n        \"\"\"\n        This function fits a time series model using the data associated with the current node.\n        It retrieves the main time series data and checks for any exogenous variables specified in the node.\n        Warnings related to user actions and convergence are temporarily suppressed during the fitting process.\n        The model is fitted using the endogenous and exogenous data, along with any additional fitting arguments provided.\n        The function returns the fitted time series model object.\n        \"\"\"\n        # Suppress warnings related to convergence and user actions\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            \n            # Assume some model setup process exists, e.g., self.setup_model()\n            self.setup_model()\n            \n            if self.exog_data is not None:\n                # Fit the model with both exogenous and endogenous data\n                self.model = self.actual_fit_model(self.endog_data, self.exog_data, **fit_args)\n            else:\n                # Fit the model with only endogenous data\n                self.model = self.actual_fit_model(self.endog_data, **fit_args)\n        \n        return self.model\n\n    def setup_model(self):\n        pass\n\n    def actual_fit_model(self, endog, exog=None, **fit_args):\n        \"\"\"\n        A placeholder function that represents the actual model fitting process, perhaps using a statistical or ML library.\n        \"\"\"\n        pass\n\n# How you might use this class:\n# ts_model = TimeSeriesModel()\n# ts_model.endog_data = your_endogenous_data\n# ts_model.exog_data = your_exogenous_data  # Optional\n# fitted_model = ts_model.fit(optimizer='SGD', epochs=100)", "CodeBleu_score": 27.266497990764822}, {"code": "import warnings\nimport statsmodels.api as sm\n\nclass TimeSeriesModel:\n    def __init__(self, data, exog=None):\n        \"\"\"\n        Initialize the TimeSeriesModel with data and optionally exogenous variables.\n        :param data: Main time series data (pandas DataFrame or Series)\n        :param exog: Optional exogenous variables (pandas DataFrame)\n        \"\"\"\n        self.data = data\n        self.exog = exog\n        self.model_fitted = None\n\n    def fit(self, **fit_args) -> \"TimeSeriesModel\":\n        \"\"\"\n        Fit a time series model using the data associated with the current node.\n        It retrieves the main time series data and checks for any exogenous variables specified in the node.\n        Warnings related to user actions and convergence are temporarily suppressed during the fitting process.\n        The model is fitted using the endogenous and exogenous data, along with any additional fitting arguments provided.\n        The function returns the fitted time series model object.\n        \"\"\"\n        # Suppress warnings during the fitting process\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            \n            # Check if exogenous data is provided and fit the model accordingly\n            if self.exog is not None:\n                self.model = sm.tsa.SARIMAX(self.data, exog=self.exog, **fit_args)\n            else:\n                self.model = sm.tsa.SARIMAX(self.data, **fit_args)\n            \n            # Fit the model\n            self.model_fitted = self.model.fit()\n\n        return self\n\n# Example of how to use this class: \n# Assuming you have DataFrame 'df' for the main data and DataFrame 'exog' for exogenous variables\n# ts_model = TimeSeriesModel(df, exog)\n# fitted_model = ts_model.fit(order=(1, 1, 1))", "CodeBleu_score": 34.809398951487026}, {"code": "import warnings\n\nclass TimeSeriesModel:\n    def __init__(self, endog, exog=None, model=None):\n        self.endog = endog\n        self.exog = exog\n        self.model = model\n        self.fitted_model = None\n\n    def fit(self, **fit_args) -> \"TimeSeriesModel\":\n        \"\"\"\n        Fits a time series model using the data associated with the current node.\n        It retrieves the main time series data and checks for any exogenous variables specified in the node.\n        Warnings related to user actions and convergence are temporarily suppressed during the fitting process.\n        The model is fitted using the endogenous and exogenous data, along with any additional fitting arguments provided.\n        The function returns the fitted time series model object.\n        \"\"\"\n        # Temporarily suppress warnings\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            \n            # Check if a model has been specified\n            if self.model is None:\n                raise ValueError(\"No model specified for fitting.\")\n            \n            # Fit the model using the endogenous and, if provided, exogenous data\n            if self.exog is not None:\n                self.fitted_model = self.model(self.endog, exog=self.exog, **fit_args).fit()\n            else:\n                self.fitted_model = self.model(self.endog, **fit_args).fit()\n        \n        # Return the current instance with the fitted model\n        return self\n\n# Example usage if part of a larger system where class attributes are correctly set\n# Assuming usage of an ARIMA model as a placeholder here\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Sample endogenous data\nendog_data = [1, 2, 3, 4, 5]\n# No exogenous data in this example\ntime_series_model = TimeSeriesModel(endog=endog_data, model=ARIMA)\n\n# Fit the model without exogenous variables\nfitted_model_instance = time_series_model.fit()\n\n# Optionally now use fitted_model_instance.fitted_model for predictions, etc.", "CodeBleu_score": 32.15642111923802}]}
{"instance_id": "6446de2223e8", "generation_result": [{"code": "import logging\n\ndef create_model(self, model_type, **kwargs):\n    \"\"\"\n    This function creates a time series model based on the specified model type.\n    If the model type is 'holt_winters', it retrieves the transformed data and creates an ExponentialSmoothing model.\n    If the model type is 'auto_arima', it attempts to import AutoARIMA from pmdarima and create an AutoARIMA model with the given arguments.\n    If pmdarima is not installed, it logs an error and exits.\n    If the model type is 'sarimax', it retrieves the transformed data, checks for exogenous variables, and creates a SARIMAX model.\n    If the model type is not recognized, it raises an exception.\n    The function returns the created model.\n\n    Parameters:\n        model_type (str): The type of model to create ('holt_winters', 'auto_arima', 'sarimax').\n        **kwargs: Variable keyword arguments relevant for the specific model type.\n\n    Returns:\n        model: The created time series model object.\n\n    Raises:\n        ImportError: If pmdarima library is not installed when trying to create 'auto_arima' model.\n        ValueError: If the specified model_type is not recognized.\n    \"\"\"\n    if model_type == 'holt_winters':\n        data = self.get_transformed_data()\n        from statsmodels.tsa.holtwinters import ExponentialSmoothing\n        model = ExponentialSmoothing(data, **kwargs)\n        model_fit = model.fit()\n        return model_fit\n\n    elif model_type == 'auto_arima':\n        try:\n            from pmdarima import AutoARIMA\n            data = self.get_transformed_data()\n            model = AutoARIMA(**kwargs)\n            model_fit = model.fit(data)\n            return model_fit\n        except ImportError:\n            logging.error(\"The pmdarima package needs to be installed to use the auto_arima model type.\")\n            raise ImportError(\"The pmdarima package needs to be installed to use the auto_arima model type.\")\n\n    elif model_type == 'sarimax':\n        data = self.get_transformed_data()\n        exogenous = kwargs.pop('exogenous', None)\n        from statsmodels.tsa.statespace.sarimax import SARIMAX\n        model = SARIMAX(data, exog=exogenous, **kwargs)\n        model_fit = model.fit()\n        return model_fit\n\n    else:\n        raise ValueError(f\"Unrecognized model type '{model_type}'. Valid options are 'holt_winters', 'auto_arima', 'sarimax'.\")", "CodeBleu_score": 39.55660030116182}, {"code": "def create_model(self, **kwargs):\n    \"\"\"\n    Creates a time series model based on the specified model type.\n\n    Args:\n        model_type (str): The type of the model to create, options are 'holt_winters', 'auto_arima', 'sarimax'.\n        **kwargs: Additional keyword arguments for model creation.\n    \n    Returns:\n        A time series prediction model.\n    \n    Raises:\n        ModuleNotFoundError: If pmdarima is not installed and 'auto_arima' model type is chosen.\n        ValueError: If the provided model type is not recognized.\n    \"\"\"\n    model_type = kwargs.get('model_type', '').lower()\n    transformed_data = self.get_transformed_data()  # Assuming a method to get transformed data\n    \n    if model_type == 'holt_winters':\n        from statsmodels.tsa.holtwinters import ExponentialSmoothing\n        model = ExponentialSmoothing(transformed_data, **kwargs)\n        return model.fit()\n    \n    elif model_type == 'auto_arima':\n        try:\n            from pmdarima import auto_arima\n            return auto_arima(transformed_data, **kwargs)\n        except ImportError:\n            print('pmdarima module is not installed. Please install it to use the AutoARIMA model.')\n            raise ModuleNotFoundError('pmdarima must be installed to use auto_arima model type')\n    \n    elif model_type == 'sarimax':\n        from statsmodels.tsa.statespace.sarimax import SARIMAX\n        exog_vars = kwargs.get('exog', None)  # Exogenous variables if any\n        model = SARIMAX(transformed_data, exog=exog_vars, **kwargs)\n        return model.fit()\n    \n    else:\n        raise ValueError(f\"Unrecognized model type '{model_type}'. Please choose one of 'holt_winters', 'auto_arima', or 'sarimax'.\")", "CodeBleu_score": 35.788790932429414}, {"code": "def create_model(self, **kwargs)\ndef create_model(self, **kwargs):\n    # Import needed libraries for modeling\n    import logging\n    from statsmodels.tsa.holtwinters import ExponentialSmoothing\n    from statsmodels.tsa.statespace.sarimax import SARIMAX\n\n    # Getting the model type from kwargs\n    model_type = kwargs.get('model_type')\n\n    if not model_type:\n        raise ValueError(\"No model type specified.\")\n\n    # Process each type of model\n    if model_type == 'holt_winters':\n        # Assume transformed_data is provided in kwargs\n        transformed_data = kwargs.get('transformed_data')\n        if transformed_data is None:\n            raise ValueError(\"Transformed data is required for Holt-Winters model.\")\n\n        # Create Holt-Winters model\n        model = ExponentialSmoothing(\n            transformed_data,\n            seasonal_periods=kwargs.get('seasonal_periods', 12),\n            trend=kwargs.get('trend', 'add'),\n            seasonal=kwargs.get('seasonal', 'add'),\n            damped_trend=kwargs.get('damped_trend', False)\n        ).fit()\n\n    elif model_type == 'auto_arima':\n        # Try to import AutoARIMA\n        try:\n            from pmdarima.arima import AutoARIMA\n        except ImportError:\n            logging.error(\"pmdarima is not installed. Please install it to use AutoARIMA models.\")\n            raise\n\n        # Assume that data is provided in kwargs\n        data = kwargs.get('data')\n        if data is None:\n            raise ValueError(\"Data is required for Auto ARIMA model.\")\n\n        # Create AutoARIMA model\n        model = AutoARIMA(**kwargs)\n        model.fit(data)\n    \n    elif model_type == 'sarimax':\n        # Assume transformed_data and possibly exogenous data are in kwargs\n        transformed_data = kwargs.get('transformed_data')\n        if transformed_data is None:\n            raise ValueError(\"Transformed data is required for SARIMAX model.\")\n        \n        exog = kwargs.get('exog', None)\n\n        # Create SARIMAX model\n        model = SARIMAX(\n            transformed_data,\n            order=kwargs.get('order', (1, 1, 1)),\n            seasonal_order=kwargs.get('seasonal_order', (0, 0, 0, 0)),\n            exog=exog,\n            enforce_stationarity=kwargs.get('enforce_stationarity', True),\n            enforce_invertibility=kwargs.get('enforce_invertibility', True)\n        ).fit()\n\n    else:\n        raise ValueError(f\"Unsupported model type: '{model_type}'\")\n\n    return model\n\n# Note: Invoking the actual fitting process might require some extra parameters which depend on the specific use case and should be supplied accordingly.\n", "CodeBleu_score": 35.89327312638033}]}
{"instance_id": "b2f67820e27f", "generation_result": [{"code": "import numpy as np\nimport chainerx\n\ndef _to_chx(array):\n    \"\"\"\n    This function converts a given array to a chainerx.ndarray.\n    If the input array is a numpy.ndarray, it converts it using chainerx.array directly.\n    If the input array is a cupy.ndarray and the cupy module is available, it converts \n    it using chainerx.array but potentially could use a specific approach if needed.\n    For objects with other types, the function returns them unchanged.\n    \"\"\"\n    # Check if array is a numpy array\n    if isinstance(array, np.ndarray):\n        return chainerx.array(array)\n    \n    # Check if array is a cupy array\n    try:\n        import cupy as cp\n        if isinstance(array, cp.ndarray):\n            return chainerx.array(array)\n    except ImportError:\n        pass  # Cupy not available, do nothing\n\n    # If it's any other type of object, return it unchanged\n    return array", "CodeBleu_score": 48.28702024892799}, {"code": "import numpy as np\nimport chainerx\n\ndef _to_chx(array):\n    \"\"\"\n    This function converts a given array to a chainerx.ndarray.\n    If the input array is a numpy.ndarray, it converts it using the _from_numpy function.\n    If the input array is a cupy.ndarray and the cupy module is available, it converts it using the _from_cupy function.\n    For objects with other types, the function returns them unchanged.\n\n    Args:\n    array (numpy.ndarray or cupy.ndarray or any): Input array to be converted to chainerx.ndarray.\n\n    Returns:\n    chainerx.ndarray or type of `array`: Converted chainerx.ndarray if input is numpy or cupy array, otherwise returns the input unchanged.\n    \"\"\"\n    if isinstance(array, np.ndarray):\n        return chainerx.array(array)\n    try:\n        import cupy\n        if isinstance(array, cupy.ndarray):\n            return chainerx.array(array)\n    except ImportError:\n        pass\n    return array", "CodeBleu_score": 51.21600580290262}, {"code": "import chainerx\nimport numpy as np\n\ndef _to_chx(array):\n    \"\"\"\n    This function converts a given array into a chainerx.ndarray.\n    If the input array is a numpy.ndarray, it converts it using chainerx.from_numpy function.\n    If the input array is a cupy.ndarray and the cupy module is available, it converts it using chainerx.from_dlpack function.\n    For objects with other types, the function returns them unchanged.\n    \"\"\"\n\n    if isinstance(array, np.ndarray):\n        # If the array is a numpy array, convert it to a chainerx array using from_numpy\n        return chainerx.from_numpy(array)\n    \n    try:\n        import cupy as cp\n        if isinstance(array, cp.ndarray):\n            # If the array is a cupy array, firstly convert it to a DLPack and then to a chainerx array using from_dlpack\n            dlpack = array.toDlpack()\n            return chainerx.from_dlpack(dlpack)\n    except ImportError:\n        pass  # cupy is not available\n    \n    # If the type of the array is neither numpy nor cupy, return it unchanged:\n    return array", "CodeBleu_score": 39.513233180772524}]}
{"instance_id": "b96f7d9bcc95", "generation_result": [{"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\nimport numpy as np\n\ndef _fix_numpy_split(ys, x, indices_or_sections, axis):\n    \"\"\"This function ensures compatibility of the output of np.split with numpy versions >= 1.11.\n    It checks if all arrays in the input list ys have the same number of dimensions as the array x.\n    If not, it calculates the expected shapes for the split segments using a temporary split on an empty array with the same shape as x along the specified axis.\n    For each segment, if the dimensions do not match, it reshapes the segment to the expected shape based on the calculated split sizes.\n    The function returns the modified list of split arrays.\n    \n    Args:\n    ys (list of np.array): A list containing the segments resulting from splitting an array.\n    x (np.array): The original array that was split.\n    indices_or_sections (int or 1-D array): This parameter can either be an integer, indicating the number of equal splits, or a 1-D array of sorted integers, indicating the indices at which to split the array.\n    axis (int): The axis along which the split was done.\n\n    Returns:\n    list of np.array: The potentially reshaped list of split arrays, ensuring all split segments adhere to the expected shapes.\n    \"\"\"\n    # Check parts shape compatibility by creating a mock split on an empty array\n    expected_shapes = np.split(np.empty_like(x), indices_or_sections, axis=axis)\n    \n    # Must ensure the shapes, especially dimensions, match\n    fixed_ys = []\n    for part, shape in zip(ys, expected_shapes):\n        if part.shape != shape.shape:\n            # Reshape the part if dimensions mismatch\n            part = part.reshape(*shape.shape)\n        fixed_ys.append(part)\n\n    return fixed_ys\n\n# Example usage:\n# import numpy as np\n# x = np.random.rand(10, 2, 4)\n# ys = np.split(x, 5, axis=0)\n# fixed_ys = _fix_numpy_split(ys, x, 5, 0)\n# Check the shapes\n# for array in fixed_ys:\n#     print(array.shape)\n# This should print (2, 2, 4) for each array in the list, conforming to the original split expectations.", "CodeBleu_score": 29.642376455526325}, {"code": "def _fix_numpy_split(ys, x, indices_or_sections, axis)\nimport numpy as np\n\ndef _fix_numpy_split(ys, x, indices_or_sections, axis):\n    \"\"\"\n    This function ensures compatibility of the output of np.split with numpy versions >= 1.11.\n    It checks if all arrays in the input list ys have the same number of dimensions as the array x.\n    If not, it calculates the expected shapes for the split segments using a temporary split on\n    an empty array with the same shape as x along the specified axis.\n    For each segment, if the dimensions do not match, it reshapes the segment to the expected shape\n    based on the calculated split sizes.\n    The function returns the modified list of split arrays.\n    \n    Args:\n    ys (list of np.ndarray): List of arrays resulting from a split.\n    x (np.ndarray): The original array which was split.\n    indices_or_sections (int or array-like): This can either be an integer indicating the number of\n                                             even splits to make, or an array of indices at which\n                                             to split the array.\n    axis (int): The axis along which to split the array.\n    \n    Returns:\n    list of np.ndarray: List of arrays post potentially necessary reshaping.\n    \"\"\"\n    \n    # Determine the shapes of the segments as they should be using a dtype with minimal memory footprint\n    template = np.empty_like(x, shape=0)  # Create a minimal memory footprint based on x's shape\n    try:\n        # Split the minimal template array to get the shapes\n        temp_splits = np.split(template, indices_or_sections, axis=axis)\n    except ValueError:\n        # Handle any exceptions that might occur when attempting split, typically:\n        # \"array split does not result in an equal division\"\n        raise ValueError(\"Invalid split size for the given indices_or_sections and array shape along the specified axis.\")\n    \n    # Calculate the target shapes of the outputs to enforce match\n    target_shapes = [arr.shape for arr in temp_splits]\n    \n    # Adjust the shapes of ys to match the target shapes if required\n    corrected_ys = []\n    for i, seg in enumerate(ys):\n        if seg.shape != target_shapes[i]:  # Only reshape if necessary\n            try:\n                # reshape using the target shape for this segment while retaining all data (assuming data is contiguous).\n                corrected_seg = seg.reshape(target_shapes[i])\n            except ValueError:\n                # This case handles reshaping errors where the number of elements must match exactly.\n                raise ValueError(\"Cannot reshape segment with shape {} to target shape {}\".format(seg.shape, target_shapes[i]))\n            \n            corrected_ys.append(corrected_seg)\n        else:\n            corrected_ys.append(seg)  # Correct shape; append as is\n    \n    return corrected_ys", "CodeBleu_score": 30.272247726392532}, {"code": "import numpy as np\n\ndef _fix_numpy_split(ys, x, indices_or_sections, axis):\n    \"\"\"\n    Ensures compatibility of the output of np.split with numpy versions >= 1.11.\n    It checks if all arrays in the input list ys have the same number of dimensions as the array x.\n    If not, it calculates the expected shapes for the split segments using a temporary split on an empty array with the same shape as x along the specified axis.\n    For each segment, if the dimensions do not match, it reshapes the segment to the expected shape based on the calculated split sizes.\n    The function returns the modified list of split arrays.\n\n    Args:\n    - ys (list of np.ndarray): The list of arrays to verify.\n    - x (np.ndarray): The original array before splitting.\n    - indices_or_sections (int or 1-D array): The number of equal partitions to split into, or the exact indices where the splits should occur.\n    - axis (int): The axis along which to split.\n\n    Returns:\n    - list of np.ndarray: The list of corrected shape arrays.\n    \"\"\"\n\n    # Validate dimensions\n    expected_dims = x.ndim\n\n    # Create an empty array with the same shape as x\n    empty_x = np.empty_like(x)\n\n    # Calculate the shapes of segments that would result from splitting this empty array\n    try:\n        # Using np.split on an empty array to determine shapes\n        expected_shapes = [s.shape for s in np.split(empty_x, indices_or_sections, axis)]\n    except ValueError:\n        # np.split can raise value error if indices are not compatible with dimensions\n        raise ValueError(\"Invalid indices or sections for the shape and axis specified.\")\n\n    # Reshape the elements in ys if their current shape does not match expected shape\n    corrected_ys = []\n    for segment, expected_shape in zip(ys, expected_shapes):\n        if segment.shape != expected_shape:\n            # Reshape segment to match the expected shape\n            segment = segment.reshape(expected_shape)\n        corrected_ys.append(segment)\n\n    return corrected_ys", "CodeBleu_score": 28.535511659041802}]}
{"instance_id": "cf39fb123842", "generation_result": [{"code": "def to_device_batch(batch):\nimport numpy as np\nimport chainer\n\ndef to_device_batch(batch, device=None):\n    \"\"\"\n    This function transfers a batch of data to a specified device.\n    If the device is None, it returns the batch unchanged.\n    If the device is a negative number, it transfers each element in the batch to the specified device using chainer.dataset.to_device.\n    For positive device values, it determines the array module (NumPy or CuPy) for the batch, concatenates the batch along the first axis,\n    and calculates the split sections.\n    The concatenated batch is then transferred to the specified device and split back into the original batch sections using CuPy.\n    The function returns the batch transferred to the specified device.\n    \n    Parameters:\n        batch (list of ndarray): The batch of data to be transferred.\n        device (int, optional): The target device identifier. Default is None.\n        \n    Returns:\n        list of ndarray: The batch of data transferred to the specified device.\n    \"\"\"\n    if device is None:\n        return batch\n    \n    if device < 0:\n        return [chainer.dataset.to_device(device, x) for x in batch]\n    \n    # Determine the array module (numpy for CPU and cupy for GPU)\n    try:\n        import cupy as cp\n        xp = cp.get_array_module(batch[0]) if device >= 0 else np\n    except ImportError:\n        xp = np\n    \n    # Concatenate the batch along the first axis\n    concatenated = xp.concatenate(batch, axis=0)\n    \n    # Calculate the split sections to maintain the structure of the original batch\n    sections = np.cumsum([x.shape[0] for x in batch[:-1]])\n    \n    # Transfer concatenated batch to the specified device\n    if device >= 0 and xp is cp:\n        concatenated = chainer.dataset.to_device(device, concatenated)\n    \n    # Split the concatenated batch using sections calculated\n    split_batch = xp.split(concatenated, sections) if sections.size > 0 else [concatenated]\n    \n    return list(split_batch) if isinstance(split_batch, list) else [split_batch]", "CodeBleu_score": 60.662898917152845}, {"code": "import numpy as np\nimport chainer\n\ndef to_device_batch(batch, device):\n    \"\"\"\n    This function transfers a batch of data to a specified device.\n    If the device is None, it returns the batch unchanged.\n    If the device is a negative number, it transfers each element in the batch to the specified device using chainer.dataset.to_device.\n    For positive device values, it determines the array module (NumPy or CuPy) for the batch, concatenates the batch along the first axis, and calculates the split sections.\n    The concatenated batch is then transferred to the specified device and split back into the original batch sections using CuPy.\n    The function returns the batch transferred to the specified device.\n    \"\"\"\n\n    if device is None:\n        return batch\n    \n    if isinstance(batch, (list, tuple)):\n        if device < 0:\n            return [chainer.dataset.to_device(device, x) for x in batch]\n        else:\n            try:\n                import cupy as cp\n                # Determine array module, concatenate, and transfer\n                if isinstance(batch[0], np.ndarray):\n                    concatenated = np.concatenate(batch, axis=0)\n                else:  # Assuming CuPy or similar GPU-compatible types\n                    concatenated = cp.concatenate(batch, axis=0)\n                    \n                concatenated = chainer.dataset.to_device(device, concatenated)\n                \n                # Calculate the split indices to recreate the original batch structure\n                split_sections = np.cumsum([x.shape[0] for x in batch[:-1]])\n                if isinstance(concatenated, np.ndarray):\n                    split_batch = np.split(concatenated, split_sections)\n                else:\n                    split_batch = cp.split(concatenated, split_sections)\n                \n                return split_batch\n\n            except ImportError:\n                raise ImportError(\"Cupy is not installed. Please install it to handle GPU arrays.\")\n\n    raise TypeError(\"Batch should be a list or a tuple of numpy or CuPy arrays.\")", "CodeBleu_score": 52.278326030606834}, {"code": "def to_device_batch(batch):\ndef to_device_batch(batch, device=None):\n    \"\"\"\n    This function transfers a batch of data to a specified device.\n    If the device is None, it returns the batch unchanged.\n    If the device is a negative number, it transfers each element in the batch to the specified device using chainer.dataset.to_device.\n    For positive device values, it determines the array module (NumPy or CuPy) for the batch, concatenates the batch along the first axis, and calculates the split sections.\n    The concatenated batch is then transferred to the specified device and split back into the original batch sections using CuPy.\n    The function returns the batch transferred to the specified device.\n    \n    Args:\n    - batch (list or tuple): List or tuple of arrays to be transferred.\n    - device (int, optional): Device identifier. None for CPU, negative for GPU with Chainer, positive for GPU with manual control.\n\n    Returns:\n    - list or tuple: Batch of arrays on the specified device.\n    \"\"\"\n    \n    import numpy\n    import chainer\n\n    if device is None:\n        # Device is None, return unchanged\n        return batch\n\n    elif device < 0:\n        # Negative device means using chainer for transfer\n        return [chainer.cuda.to_gpu(x, device=device) for x in batch]\n\n    else:\n        # Positive device value, choose between NumPy and CuPy based on the location of the data\n        try:\n            import cupy as cp\n            using_cupy = True\n        except ImportError:\n            using_cupy = False\n            cp = numpy\n        \n        if using_cupy:\n            # When CuPy is available and device is positive, perform operations using CuPy\n    \n            # Concatenating the batch along the first axis\n            concatenated_array = cp.concatenate(batch, axis=0)\n            # To transfer, we need split sections where to split the concatenated array back to batches\n            sections = numpy.cumsum([x.shape[0] for x in batch[:-1]])\n            # Transfer the concatenated array to the specific device\n            concatenated_array_device = cp.asarray(concatenated_array, device=device)\n            # Split the array back into its original batches\n            transferred_batch = cp.split(concatenated_array_device, sections)\n            return transferred_batch\n        \n        else:\n            # When CuPy is not available or when using CPU, use NumPy for operations\n            concatenated_array = numpy.concatenate(batch, axis=0)\n            sections = numpy.cumsum([x.shape[0] for x in batch[:-1]])\n            # No actual device transfer operations in numpy, just mimic the operation\n            transferred_batch = numpy.split(concatenated_array, sections)\n            return transferred_batch\n\n# Note: The function implementation assumes certain conditions and simplifications to illustrate potential approaches. Adjustments might be needed based on the practical specifics and configurations of your execution environment.", "CodeBleu_score": 48.04590896773752}]}
{"instance_id": "845c744d897c", "generation_result": [{"code": "import torch\nimport torchvision\nimport os\nimport numpy as np\n\ndef make_image(trainer):\n    \"\"\"\n    This function generates and saves an image based on the current state of a trainer.\n    It sets a random seed and determines the number of images to generate.\n    It creates a latent variable z, and generates images using a generator model with the training mode turned off.\n    The generated images are moved to the CPU, scaled to a [0, 255] range, and converted to uint8 type.\n    The images are reshaped and transposed to form a grid of images.\n    The function creates a directory for saving the preview image if it doesn't exist and saves the generated image as a PNG file named according to the current training iteration.\n    \"\"\"\n    \n    # Set random seed for reproducibility of the images generated\n    torch.manual_seed(0)\n    \n    # Generate a batch of latent variables to feed into the generator\n    z = torch.randn(trainer.batch_size, trainer.latent_dim, 1, 1, device=trainer.device)\n    \n    # Turn off gradients as we are in inference mode\n    with torch.no_grad():\n        trainer.generator.eval()  # Set the generator to evaluation mode\n        # Generate images from the latent variable\n        images = trainer.generator(z)\n    \n    # Move images to CPU and scale them to [0,255]\n    images = images.cpu().numpy() * 127.5 + 127.5\n    images = images.astype(np.uint8)\n    \n    # Reshape and transpose images to make them viewable\n    images = images.transpose((0, 2, 3, 1))  # Change from (B, C, H, W) to (B, H, W, C)\n   \n    # Create a grid of images\n    grid = torchvision.utils.make_grid(torch.from_numpy(images), nrow=int(np.sqrt(trainer.batch_size)))\n    \n    # Create the output directory if it does not exist\n    output_dir = \"generated_images\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Save the image in the output directory\n    filepath = os.path.join(output_dir, f\"image_at_iteration_{trainer.iterations}.png\")\n    torchvision.utils.save_image(grid, filepath)\n\n    print(f\"Saved image at {filepath}\")", "CodeBleu_score": 35.09304952685704}, {"code": "import torch\nimport numpy as np\nimport os\nfrom torchvision.utils import save_image, make_grid\n\ndef make_image(trainer):\n    \"\"\"\n    This function generates and saves an image based on the current state of a trainer.\n    It sets a random seed and determines the number of images to generate.\n    It creates a latent variable z, and generates images using a generator model with the training mode turned off.\n    \"\"\"\n    torch.manual_seed(0)  # Set seed for reproducibility\n    \n    num_images = 64  # Define the number of images to generate\n    z_dim = trainer.model.z_dim  # Latent dimension size from the trainer's model\n    \n    # Generate random latent variables\n    z = torch.randn(num_images, z_dim, device=trainer.device)\n    \n    # Turn off gradients and set the model to evaluation mode\n    with torch.no_grad():\n        trainer.model.eval()\n        # Generate images using the trained generator model\n        generated_images = trainer.model.generator(z)\n    \n    # Move images to CPU and scale to [0, 255]\n    generated_images = generated_images.cpu().numpy()  # move to CPU and convert to numpy array\n    generated_images = (generated_images * 0.5 + 0.5) * 255.0  # Scale to [0, 255]\n    generated_images = generated_images.astype(np.uint8)  # Cast pixel values to integers\n    \n    # Reshape and transpose images to form a grid\n    generated_image_grid = make_grid(torch.from_numpy(generated_images), nrow=8, normalize=True)\n\n    # Ensure a directory exists for saving the images\n    image_dir = 'generated_images'\n    if not os.path.exists(image_dir):\n        os.makedirs(image_dir)\n    \n    # Save the image as a PNG file\n    image_path = os.path.join(image_dir, f\"image_at_iteration_{trainer.current_iteration}.png\")\n    save_image(generated_image_grid, image_path)\n\n    print(f\"Image saved to {image_path}\")", "CodeBleu_score": 30.461292276390793}, {"code": "import os\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport torchvision.utils as vutils\n\ndef make_image(trainer):\n    # Set random seed for reproducibility\n    seed = 999\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    # Parameters for generating images\n    num_images = 64\n    image_size = (64, 64)  # Assuming the generator model uses this image size.\n\n    # Prepare a latent vector z for generating images\n    z_dim = 100  # Assuming the size of the latent vector the generator expects.\n    z = torch.randn(num_images, z_dim, device=trainer.device)\n    \n    # Use the generator model to generate images\n    trainer.generator.eval()  # Turn on evaluation mode for the generator\n    with torch.no_grad():\n        images = trainer.generator(z)\n    \n    # Move images to CPU and scale to [0, 255]\n    images = images.to('cpu')  \n    images = ((images + 1) / 2 * 255).clamp(0, 255).to(torch.uint8)\n    \n    # Reshape and transpose to form a grid of images\n    grid = vutils.make_grid(images, padding=2, normalize=False)\n\n    # Create directory for saving images if it doesn't exist\n    dir_name = \"generated_images\"\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n    \n    # Save the images\n    image_path = os.path.join(dir_name, f\"iter_{trainer.iteration}.png\")\n    grid_image = Image.fromarray(grid.permute(1, 2, 0).numpy())\n    grid_image.save(image_path)\n\n    print(f\"Image saved at {image_path}\")", "CodeBleu_score": 35.164477187094064}]}
{"instance_id": "fc2162d43aee", "generation_result": [{"code": "import numpy as np\nimport numpy.typing as npt\n\ndef _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool:\n    \"\"\"\n    This function checks if a data type conversion is possible on CUDA-enabled devices.\n    It converts the input data types from_dtype and to_dtype to their numpy dtype equivalents.\n    It then compares the type characters of the data types using their positions in a predefined string _typechars.\n    The function returns True if the from_dtype can be cast to the to_dtype based on their positions in _typechars; otherwise, it returns False.\n    \"\"\"\n    # Define the order of type characters from least to most capable\n    _typechars = 'bhilqBHILQefdg'\n\n    # Get numpy dtypes from the given input\n    from_dt = np.dtype(from_dtype)\n    to_dt = np.dtype(to_dtype)\n\n    # Find the index or position of type characters in the _typechars string\n    from_index = _typechars.find(from_dt.char)\n    to_index = _typechars.find(to_dt.char)\n\n    # Check the casting possibility\n    return from_index <= to_index\n\n# Example usage:\nprint(_cuda_can_cast('int32', 'float64'))  # True\nprint(_cuda_can_cast('float64', 'int32'))  # False", "CodeBleu_score": 47.03082798229972}, {"code": "import numpy as np\nimport numpy.typing as npt\n\ndef _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool:\n    \"\"\"\n    This function checks if a data type conversion is possible on CUDA-enabled devices.\n    It converts the input data types `from_dtype` and `to_dtype` to their numpy dtype equivalents.\n    It then compares the type characters of the data types using their positions in a predefined string `_typechars`.\n    The function returns True if the `from_dtype` can be cast to the `to_dtype` based on their positions in `_typechars`; otherwise, it returns False.\n    \"\"\"\n    _typechars = 'buifc'\n    \n    # Convert input dtypes to numpy dtype objects\n    np_from_dtype = np.dtype(from_dtype)\n    np_to_dtype = np.dtype(to_dtype)\n    \n    # Get the type character for each dtype\n    from_typechar = np_from_dtype.kind\n    to_typechar = np_to_dtype.kind\n    \n    # Get the positions in the _typechars string\n    from_index = _typechars.find(from_typechar)\n    to_index = _typechars.find(to_typechar)\n    \n    # Check if casting is possible based on positions\n    return from_index <= to_index\n\n# Example Usage:\n# _cuda_can_cast(np.int32, np.float32) -> True\n# _cuda_can_cast(np.float32, np.int32) -> False", "CodeBleu_score": 42.726585873527334}, {"code": "import numpy as np\nimport numpy.typing as npt\n\ndef _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool:\n    \"\"\"\n    This function checks if a data type conversion is possible on CUDA-enabled devices.\n    It converts the input data types from_dtype and to_dtype to their numpy dtype equivalents.\n    It then compares the type characters of the data types using their positions in a predefined \n    string _typechars.\n    The function returns True if the from_dtype can be cast to the to_dtype based on their \n    positions in _typechars; otherwise, it returns False.\n    \"\"\"\n    # Define the ordered string of type characters based on casting rules\n    _typechars = 'bBhHiIlLqQfde'  # Ordered from smallest to largest permissible casts\n                                  # 'bBhHiIlLqQfde': \n                                  # (b=byte, B=unsigned byte, h=short, H=unsigned short,\n                                  # i=int, I=unsigned int, l=long, L=unsigned long,\n                                  # q=longlong, Q=unsigned longlong,\n                                  # f=float, d=double precision, e=half precision float)\n    \n    # Convert the input data types to numpy dtypes\n    np_from_dtype = np.dtype(from_dtype)\n    np_to_dtype = np.dtype(to_dtype)\n    \n    # Get the type characters from numpy dtype objects \n    from_type_char = np_from_dtype.char\n    to_type_char = np_to_dtype.char\n    \n    # Find the index positions of each type character in the typechars string\n    from_index = _typechars.find(from_type_char)\n    to_index = _typechars.find(to_type_char)\n    \n    # Return True if the index of from_dtype is <= index of to_dtype, allowing a safe cast\n    # This means the from_dtype is smaller or equal in type size or precision to the to_dtype\n    return from_index <= to_index\n\n# Example usage\n# print(_cuda_can_cast('i4', 'f4'))  # Int32 to Float32 should return True\n# print(_cuda_can_cast('f8', 'i2'))  # Double to Int16 should return False", "CodeBleu_score": 41.1910545070093}]}
{"instance_id": "5b8e2d5e6e12", "generation_result": [{"code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\ndef manual_legend(g, labels, colors, **legend_kwargs):\n    \"\"\"\n    This function manually creates a legend for a given plot using specified labels and colors.\n    It first obtains the matplotlib Axes object from the input, which can be a Visualizer, None,\n    or an Axes object. The function checks that the lengths of the labels and colors lists match,\n    raising an error if they do not. It creates legend handles by pairing each label with its\n    corresponding color using patches.Patch. Finally, it returns the legend object created with \n    the specified legend handles and additional keyword arguments.\n\n    Args:\n    g: matplotlib axis object, visualizer, or None.\n       If None, uses the current axis.\n    labels (list): List of labels for each handle in the legend.\n    colors (list): List of colors for each handle in the legend.\n    **legend_kwargs: Additional keyword arguments to pass to the `legend()` function.\n\n    Returns:\n    legend: A matplotlib legend object.\n    \"\"\"\n    \n    # Obtain the Axes object from the input\n    if g is None:\n        ax = plt.gca()\n    else:\n        ax = g\n    \n    # Validate the lengths of labels and colors are the same\n    if len(labels) != len(colors):\n        raise ValueError(\"The lengths of labels and colors must match\")\n    \n    # Create the list of legend handles\n    handles = [mpatches.Patch(color=color, label=label) for label, color in zip(labels, colors)]\n    \n    # Create and return the legend\n    legend = ax.legend(handles=handles, **legend_kwargs)\n    return legend", "CodeBleu_score": 55.252846266131684}, {"code": "def manual_legend(g, labels, colors, **legend_kwargs)\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\n\ndef manual_legend(g, labels, colors, **legend_kwargs):\n    \"\"\"\n    This function manually creates a legend for a given plot using specified labels and colors.\n    It first obtains the matplotlib Axes object from the input, which can be a Visualizer, None, or an Axes object.\n    The function checks that the lengths of the labels and colors lists match, raising an error if they do not.\n    It creates legend handles by pairing each label with its corresponding color using patches.Patch.\n    Finally, it returns the legend object created with the specified legend handles and additional keyword arguments.\n\n    Args:\n    g : matplotlib Axes, Visualizer, or None\n        The graphical object where the legend will be placed.\n    labels : list of str\n        The labels for each legend entry.\n    colors : list of color representations\n        The colors associated with each label.\n    **legend_kwargs : dict, optional\n        Additional keyword arguments to be passed to the legend function.\n\n    Returns:\n    legend : matplotlib Legend\n        The legend object added to the plot.\n    \"\"\"\n    \n    if len(labels) != len(colors):\n        raise ValueError(\"The lengths of labels and colors must match.\")\n    \n    # Get the Axes object where the legend will be placed.\n    if g is None:\n        ax = plt.gca()\n    else:\n        ax = g if isinstance(g, plt.Axes) else g.ax\n    \n    # Create legend handles\n    handles = [patches.Patch(color=color, label=label) for label, color in zip(labels, colors)]\n    \n    # Create the legend and add it to the Axes\n    legend = ax.legend(handles=handles, **legend_kwargs)\n    \n    return legend", "CodeBleu_score": 54.89395722064154}, {"code": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef manual_legend(g, labels, colors, **legend_kwargs):\n    \"\"\"\n    This function manually creates a legend for a given plot using specified labels and colors.\n    It first obtains the matplotlib Axes object from the input, which can be a Visualizer, None, or an Axes object.\n    The function checks that the lengths of the labels and colors lists match, raising an error if they do not.\n    It creates legend handles by pairing each label with its corresponding color using patches.Patch.\n    Finally, it returns the legend object created with the specified legend handles and additional keyword arguments.\n    \n    Parameters:\n        g (matplotlib.axes.Axes or None): Axes object on which to place the legend. If None, plt.gca() is used.\n        labels (list of str): The labels for each legend entry.\n        colors (list of str): A list of colors for each legend entry. \n        **legend_kwargs: Additional keyword arguments to pass to the legend function.\n\n    Returns:\n        matplotlib.legend.Legend: The created legend object.\n    \"\"\"\n    # Check for matching lengths between labels and colors\n    if len(labels) != len(colors):\n        raise ValueError(\"The lengths of labels and colors must match.\")\n\n    # Automatically obtain the current axes if none provided\n    if g is None:\n        g = plt.gca()\n    \n    # Create a list of patches as handles for the legend\n    handles = [patches.Patch(color=color, label=label) for label, color in zip(labels, colors)]\n    \n    # Create the legend on the provided axes object\n    legend = g.legend(handles=handles, **legend_kwargs)\n\n    return legend", "CodeBleu_score": 57.62464678590273}]}
{"instance_id": "494e7f719eb6", "generation_result": [{"code": "import time\nimport matplotlib.pyplot as plt\nfrom yellowbrick.features import ParallelCoordinates\n\ndef pcoords_time(X, y, fast=True):\n    \"\"\"\n    This function measures the time taken to fit and transform data using the ParallelCoordinates visualizer.\n    It creates a plot axis and initializes the ParallelCoordinates visualizer with the specified speed setting.\n    The function records the start time, fits and transforms the input data X and y with the visualizer,\n    and calculates the elapsed time.\n    It then clears and closes the current plot to free up resources.\n    The function returns the elapsed time for the fit and transform process.\n\n    Parameters:\n    - X : array-like, shape (n_samples, n_features)\n        Training instances to plot. There could be either a numpy.array or a pandas dataframe.\n    - y : array-like, shape (n_samples,)\n        Target values for instances. There could be either a numpy.array or a pandas series.\n    - fast : boolean, optional (default=True)\n        If True, reduces the fidelity of the visualizer to increase rendering speed.\n\n    Returns:\n    - elapsed_time : float\n        Time taken for the fitting and transforming steps in seconds.\n    \"\"\"\n    \n    # Initialize visualizer with the speed setting\n    vis = ParallelCoordinates(fast=fast)\n    \n    # Create plot axes\n    plt.figure()\n    \n    # Start the timer\n    start_time = time.time()\n    \n    # Fit the visualizer and transform data\n    vis.fit_transform(X, y)\n    \n    # Calculate elapsed time\n    elapsed_time = time.time() - start_time\n    \n    # Clear the figure to free up memory\n    plt.clf()\n    plt.close()\n    \n    return elapsed_time", "CodeBleu_score": 42.895851756148915}, {"code": "import time\nimport matplotlib.pyplot as plt\nfrom yellowbrick.features import ParallelCoordinates\n\ndef pcoords_time(X, y, fast=True):\n    \"\"\"\n    This function measures the time taken to fit and transform data using the ParallelCoordinates visualizer.\n    It creates a plot axis and initializes the ParallelCoordinates visualizer with the specified speed setting.\n    The function records the start time, fits and transforms the input data X and y with the visualizer, and calculates the elapsed time.\n    It then clears and closes the current plot to free up resources.\n    The function returns the elapsed time for the fit and transform process.\n    \n    Args:\n    X (array-like): Input features, should be a shape of [n_samples, n_features].\n    y (array-like): Target values, should be a shape of [n_samples].\n    fast (bool): Setting for the visualizer indicating whether to use a faster (approximate) method.\n    \n    Returns:\n    float: Elapsed time in seconds for completing the fit and transform using the ParallelCoordinates.\n    \"\"\"\n    \n    # Create a plot and initialize the ParallelCoordinates visualizer\n    fig, ax = plt.subplots()\n    visualizer = ParallelCoordinates(ax=ax, sample=0.1 if fast else 1.0)\n\n    # Record start time\n    start_time = time.time()\n\n    # Fit and transform data\n    visualizer.fit_transform(X, y)\n    \n    # Calculate elapsed time\n    elapsed_time = time.time() - start_time\n    \n    # Clear and close the plot\n    plt.clf()\n    plt.close(fig)\n    \n    # Return the elapsed time\n    return elapsed_time\n\n# Usage example (assuming you have already the datasets X and y):\n# elapsed = pcoords_time(X, y, fast=True)\n# print(f\"The process took {elapsed} seconds.\")", "CodeBleu_score": 53.57044745717604}, {"code": "import time\nfrom yellowbrick.features import ParallelCoordinates\nimport matplotlib.pyplot as plt\n\ndef pcoords_time(X, y, fast=True):\n    \"\"\"\n    This function measures the time taken to fit and transform data using the ParallelCoordinates visualizer.\n    It creates a plot axis and initializes the ParallelCoordinates visualizer with the specified speed setting.\n    The function records the start time, fits and transforms the input data X and y with the visualizer, and calculates the elapsed time.\n    It then clears and closes the current plot to free up resources.\n    The function returns the elapsed time for the fit and transform process.\n    \n    Parameters:\n        X (array-like): The input features to be visualized.\n        y (array-like): The target variable.\n        fast (bool): Whether to use a fast (but less accurate) rendering mode.\n        \n    Returns:\n        float: The elapsed time in seconds to fit and transform the data using ParallelCoordinates.\n    \"\"\"\n    \n    # Create a matplotlib figure.\n    plt.figure()\n    \n    # Initialize the ParallelCoordinates visualizer with or without fast mode.\n    visualizer = ParallelCoordinates(sample=0.1 if fast else 1.0)\n    \n    # Record the start time.\n    start_time = time.time()\n    \n    # Fit and transform the data.\n    visualizer.fit_transform(X, y)\n    \n    # Calculate the elapsed time.\n    elapsed_time = time.time() - start_time\n    \n    # Clean up the plot to free resources.\n    plt.clf()\n    plt.close()\n    \n    # Return the elapsed time.\n    return elapsed_time", "CodeBleu_score": 41.61341708285723}]}
{"instance_id": "db4143bc029f", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\n\nclass DataPlotter:\n    def __init__(self, alpha=0.25):\n        self.alpha = alpha  # Default alpha\n    \n    def draw_instances(self, X, y, **kwargs):\n        \"\"\"\n        This function draws instances of data on a plot using provided features X and labels y.\n        It retrieves the alpha transparency value from the object's attributes or defaults to 0.25.\n        For each instance, it extracts the feature vector and corresponding label, determines the color based on the label,\n        and plots the feature vector against a predefined increment sequence on the axis.\n        The function returns the axis object with the plotted instances.\n        \"\"\"\n        # Check if a matplotlib Axis is provided in kwargs, otherwise create a new one\n        if 'ax' in kwargs:\n            ax = kwargs['ax']\n        else:\n            fig, ax = plt.subplots()\n        \n        # Prepare a color map and unique labels for different y-values\n        unique_labels = np.unique(y)\n        colors = plt.cm.get_cmap('viridis', len(unique_labels))\n\n        # Plot each instance\n        for idx, (features, label) in enumerate(zip(X, y)):\n            # Find the appropriate color for each label\n            color_index = np.where(unique_labels == label)[0][0]\n            color = colors(color_index)\n            \n            # Assuming features are plottable directly\n            ax.scatter(range(len(features)), features, color=color, alpha=self.alpha, **kwargs)\n        \n        # Return the axis with the plot\n        return ax\n\n# Example usage:\n# Assuming you have data setup as follows\n# X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n# y = [0, 1, 0]\n# plotter = DataPlotter()\n# ax = plotter.draw_instances(X, y)\n# plt.show()", "CodeBleu_score": 28.8410212747795}, {"code": "import matplotlib.pyplot as plt\n\nclass DataPlotter:\n    def __init__(self, alpha=0.25):\n        self.alpha = alpha\n\n    def draw_instances(self, X, y):\n        \"\"\"\n        This function draws instances of data on a plot using provided features X and labels y.\n        It retrieves the alpha transparency value from the object's attributes or defaults to 0.25.\n        For each instance, it extracts the feature vector and corresponding label, determines the color based on the label,\n        and plots the feature vector against a predefined increment sequence on the axis.\n        The function returns the axis object with the plotted instances.\n\n        Args:\n        X (array): An array of shape (n_samples, n_features) where each row represents a feature vector.\n        y (array): An array or list of shape (n_samples,) where each element is a label corresponding to the row of X.\n\n        Returns:\n        matplotlib.axes.Axes: The Axes object with the plotted instances.\n        \"\"\"\n        # Initializing the plot\n        fig, ax = plt.subplots()\n\n        # Generating colors and ensuring there's enough variation\n        unique_labels = list(set(y))\n        colors = plt.cm.get_cmap('viridis', len(unique_labels))\n\n        # Mapping each label to a color\n        color_map = {label: colors(idx) for idx, label in enumerate(unique_labels)}\n\n        # Plot each instance\n        for idx, (features, label) in enumerate(zip(X, y)):\n            ax.scatter(features[0], features[1], color=color_map[label], alpha=self.alpha)\n\n        ax.set_xlabel(\"Feature 1\")\n        ax.set_ylabel(\"Feature 2\")\n        ax.set_title(\"Instance Plotting\")\n        plt.show()\n\n        return ax\n\n# Example usage\n# Assuming `X` is a 2D numpy array of shape (n_samples, 2) and `y` is the corresponding label array.\nplotter = DataPlotter(alpha=0.25)\nax = plotter.draw_instances(X, y)", "CodeBleu_score": 24.608314287817947}, {"code": "import matplotlib.pyplot as plt\nimport numpy as np\n\nclass DataVisualizer:\n    def __init__(self):\n        # Default alpha value\n        self.alpha = 0.25\n        # Predefined colors\n        self.colors = {\n            0: 'red',  # Example label-color mapping\n            1: 'blue',\n            2: 'green',\n            3: 'yellow',\n            4: 'purple'\n        }\n\n    def draw_instances(self, X, y):\n        \"\"\"This function draws instances of data on a plot using provided features X and labels y.\n        It retrieves the alpha transparency value from the object's attributes or defaults to 0.25.\n        For each instance, it extracts the feature vector and corresponding label, determines the color\n        based on the label, and plots the feature vector against a predefined increment sequence on the axis.\n        The function returns the axis object with the plotted instances.\n\n        Args:\n            X (np.ndarray): The feature vectors as a numpy array, where each row corresponds to an instance.\n            y (np.array): The labels corresponding to each instance in X.\n\n        Returns:\n            matplotlib.axes._subplots.AxesSubplot: The axis object with plotted data.\n        \"\"\"\n        # Ensure alpha is defined\n        alpha = getattr(self, 'alpha', 0.25)\n\n        # Check if the shape of X is two-dimensional\n        if X.ndim != 2:\n            raise ValueError('Feature vectors matrix X should be 2-dimensional')\n\n        # Create a plot if not already configured\n        fig, ax = plt.subplots()\n\n        # Determine number of features\n        num_features = X.shape[1]\n\n        # Plot each instance\n        for i, features in enumerate(X):\n            label = y[i]\n            color = self.colors.get(label, 'black')  # Default color black if label not in dict\n\n            # Creating index for features (assuming a simple sequential index for each feature)\n            indices = np.arange(num_features)\n            \n            ax.scatter(indices, features, color=color, alpha=alpha, label=f'Label {label}')\n        \n        # Optional: Handle legend intelligently\n        handles, labels = plt.gca().get_legend_handles_labels()\n        by_label = dict(zip(labels, handles))\n        ax.legend(by_label.values(), by_label.keys())\n\n        # Return the axis object\n        return ax\n\n# Example usage:\n# visualizer = DataVisualizer()\n# X_example = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n# y_example = np.array([0, 1, 2, 3])\n# ax = visualizer.draw_instances(X_example, y_example)\n# plt.show()", "CodeBleu_score": 24.89225002095876}]}
{"instance_id": "676433312ffe", "generation_result": [{"code": "def draw_classes(self, X, y, **kwargs):\ndef draw_classes(self, X, y, **kwargs):\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    # Set default alpha value\n    alpha = self.alpha if hasattr(self, 'alpha') else 0.5\n    \n    # Create axis if not provided in kwargs\n    ax = kwargs.get('ax', plt.gca())\n    \n    # Identify unique class labels\n    classes = np.unique(y)\n    \n    # Color cycle from axis or default one\n    color_cycle = ax._get_lines.prop_cycler\n\n    # Adding column of ones to visually separate instances\n    X_augmented = np.hstack([X, np.ones((X.shape[0], 1))])\n\n    for i, cls in enumerate(classes):\n        # Get color for this class\n        color = next(color_cycle)['color']\n        \n        # Select instances of this class\n        class_mask = (y == cls)\n        X_class = X_augmented[class_mask]\n\n        # Append None to create breaks between instances\n        # We use NaNs to create breaks in the plot\n        X_class_separated = []\n        for instance in X_class:\n            X_class_separated.extend(instance)\n            X_class_separated.extend([np.nan]*(X_class.shape[1]))  # NaNs for separation\n\n        # Plot\n        ax.plot(X_class_separated[:-X_class.shape[1]], color=color, alpha=alpha)\n    \n    return ax", "CodeBleu_score": 43.480070564358655}, {"code": "import matplotlib.pyplot as plt\nimport numpy as np\n\nclass DataVisualizer:\n    def __init__(self, alpha=0.5):\n        self.alpha = alpha\n    \n    def draw_classes(self, X, y, **kwargs):\n        \"\"\"\n        This function draws class-specific line plots for the provided data X and labels y on a given axis.\n        It sets the alpha transparency value from the object's attributes or defaults to 0.5.\n        It adds a column of ones to X to separate the instances visually and appends None to the increments list to create breaks between instances.\n        The function identifies unique class labels in y and iterates over them.\n        For each class, it determines the color, selects the instances of X belonging to that class, and plots them as a single line plot with the specified color and alpha transparency.\n        The function returns the axis object with the plotted classes.\n        \"\"\"\n        \n        # Handling Matplotlib axis from kwargs or create a new one\n        ax = kwargs.get('ax', plt.gca())\n        \n        # Set the alpha transparency\n        alpha = self.alpha\n        \n        # Inc paperwork\n        X = np.c_[X, np.ones(len(X))]  # Add a column of ones to X\n        unique_classes = np.unique(y)\n        \n        # Color map for plotting different classes\n        color_map = plt.cm.get_cmap('viridis', len(unique_classes))\n        \n        # Iterate over each class\n        for i, cls in enumerate(unique_classes):\n            # Find indices where the respective class labels are\n            class_indices = np.where(y == cls)[0]\n            # Extract the data points corresponding to the current class\n            class_data = X[class_indices]\n            \n            # Append None to create a break between instances\n            separated_data = []\n            for row in class_data:\n                separated_data.append(row)\n                separated_data.append(np.array([None] * X.shape[1]))\n            \n            # Set class-specific color\n            color = color_map(i)\n            \n            # Plotting the class data as a line plot\n            ax.plot(np.array(separated_data)[:, 0], np.array(separated_data)[:, 1], \n                    label=f'Class {cls}', color=color, alpha=alpha)\n        \n        # Legend configuration\n        ax.legend(loc='best')\n        \n        # Return the axis object\n        return ax\n\n# Example Usage\nif __name__ == '__main__':\n    X = np.random.rand(100, 2)  # Random data\n    y = np.random.choice(['A', 'B', 'C', 'D'], 100)  # Random class labels\n    visualizer = DataVisualizer(alpha=0.7)\n    ax = visualizer.draw_classes(X, y)\n    plt.show()", "CodeBleu_score": 34.88305049111066}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\n\nclass DataVisualizer:\n    def __init__(self, alpha=0.5):\n        self.alpha = alpha\n\n    def draw_classes(self, X, y, **kwargs):\n        \"\"\"\n        This function draws class-specific line plots for the provided data X and labels y on a given axis.\n        It sets the alpha transparency value from the object's attributes or defaults to 0.5.\n        It adds a column of ones to X to separate the instances visually and appends None to the increments list to create breaks between instances.\n        The function identifies unique class labels in y and iterates over them.\n        For each class, it determines the color, selects the instances of X belonging to that class, and plots them as a single line plot with the specified color and alpha transparency.\n        The function returns the axis object with the plotted classes.\n\n        Parameters:\n            X (np.ndarray): The data points, assumed to be a 2D array.\n            y (np.ndarray): The class labels, assumed to match rows in X.\n            **kwargs: Additional keyword arguments for the plot function, such as color map settings.\n\n        Returns:\n            matplotlib.axes._subplots.AxesSubplot: The axis object with the plotted data.\n        \"\"\"\n        # Unpack color map from kwargs or set default\n        color_map = kwargs.get('cmap', plt.cm.get_cmap('viridis'))\n\n        # Make sure X is an array with an extra column of ones for visual separation\n        if X.shape[1] == 2:\n            X = np.hstack([X, np.ones((X.shape[0], 1))])\n\n        # Get unique classes\n        classes = np.unique(y)\n\n        # Prepare figure and axis if not provided\n        ax = kwargs.get('ax', plt.gca())\n\n        # Iterate over classes to plot them separately\n        for cl in classes:\n            # Filter instances by class\n            class_mask = (y == cl)\n            class_data = X[class_mask]\n\n            # Append None for breaks in the line plot\n            extended_class_data = np.vstack([class_data, [None, None, None]])\n\n            # Determine color from the color map\n            color = color_map(cl / len(classes))  # Normalize index for color map\n\n            # Plotting each class with designated settings\n            ax.plot(extended_class_data[:, 0], extended_class_data[:, 1], label=f'Class {cl}', \n                    color=color, alpha=self.alpha)\n\n        # Legends, labels etc.\n        ax.set_xlabel('Feature 1')\n        ax.set_ylabel('Feature 2')\n        ax.legend()\n\n        return ax\n\n# Example usage:\n# visualizer = DataVisualizer(alpha=0.5)\n# X = np.random.randn(100, 2)\n# y = np.array([0, 1] * 50)\n# visualizer.draw_classes(X, y)", "CodeBleu_score": 40.469862554708534}]}
{"instance_id": "9d297b12a673", "generation_result": [{"code": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom yellowbrick.features import RadViz, Rank2D\n\ndef feature_analysis(fname=\"feature_analysis.png\"):\n    \"\"\"\n    This function creates and saves a feature analysis figure consisting of two subplots. \n    It sets up a side-by-side axes grid with a specified figure size.\n    On the left subplot, it performs RadViz analysis using occupancy data, fitting and finalizing the visualization.\n    On the right subplot, it performs Rank2D analysis using concrete data, fitting, transforming, and finalizing the visualization.\n    Finally, it saves the resulting figure to a specified file path.\n    \"\"\"\n\n    # Load example datasets\n    # Note: Replace these example datasets with your actual datasets\n    occupancy_data = pd.read_csv('path_to_occupancy_data.csv')\n    concrete_data = pd.read_csv('path_to_concrete_data.csv')\n\n    # Setup the matplotlib figure and axes\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n    # RadViz visualization for occupancy data\n    features = occupancy_data.columns[:-1]  # Assuming the last column is the target\n    target = occupancy_data.columns[-1]\n    radviz = RadViz(classes=['occupied', 'not occupied'], features=features, ax=ax1)\n    radviz.fit(occupancy_data[features], occupancy_data[target])\n    radviz.transform(occupancy_data[features])\n\n    # Rank2D visualization for concrete data\n    features = concrete_data.columns[:-1]  # Assuming the last column is the target\n    target = concrete_data.columns[-1]\n    rank2d = Rank2D(features=features, ax=ax2, algorithm='pearson')\n    rank2d.fit(concrete_data[features], concrete_data[target])\n    rank2d.transform(concrete_data[features])\n\n    # Finalizing and saving the figure\n    plt.tight_layout()\n    plt.savefig(fname)\n    plt.show()\n\n# Adjust the paths to the data files accordingly.", "CodeBleu_score": 32.431870724881925}, {"code": "import pandas as pd\nfrom yellowbrick.features import RadViz, Rank2D\nimport matplotlib.pyplot as plt\n\ndef feature_analysis(fname=\"feature_analysis.png\"):\n    \"\"\" \n    This function creates and saves a feature analysis figure consisting of two subplots.\n    It sets up a side-by-side axes grid with a specified figure size.\n    On the left subplot, it performs RadViz analysis using occupancy data, fitting and finalizing the visualization.\n    On the right subplot, it performs Rank2D analysis using concrete data, fitting, transforming, and finalizing the visualization.\n    Finally, it saves the resulting figure to a specified file path.\n    \"\"\"\n    \n    # Assuming `occupancy.csv` and `concrete.csv` are the file paths for the respective datasets.\n    # These would ideally be function parameters or defined globally if otherwise required.\n    occupancy_data = pd.read_csv(\"occupancy.csv\")\n    concrete_data = pd.read_csv(\"concrete.csv\")\n\n    # Extract features and target (assuming last column is target) from datasets\n    features_occupancy = occupancy_data.columns[:-1]\n    target_occupancy = occupancy_data.columns[-1]\n\n    features_concrete = concrete_data.columns[:-1]\n    target_concrete = concrete_data.columns[-1]\n\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n\n    # RadViz visualization on the left\n    rv = RadViz(ax=axs[0], features=features_occupancy, classes=['Not Occupied', 'Occupied'])\n    rv.fit_transform(occupancy_data[features_occupancy], occupancy_data[target_occupancy])\n    rv.finalize()\n\n    # Rank2D visualization on the right\n    rank2d = Rank2D(ax=axs[1], features=features_concrete, algorithm='pearson')\n    rank2d.fit_transform(concrete_data[features_concrete])\n    rank2d.finalize()\n\n    # Adjust layout and save the figure\n    plt.tight_layout()\n    plt.savefig(fname)\n    plt.close()\n\n# Note: Actual data file paths, feature names, and concrete analysis needs to be correctly set to ensure the function works.", "CodeBleu_score": 35.88469483022925}, {"code": "import pandas as pd\nfrom yellowbrick.features import RadViz, Rank2D\nimport matplotlib.pyplot as plt\n\ndef feature_analysis(fname=\"feature_analysis.png\"):\n    \"\"\"\n    This function creates and saves a feature analysis figure consisting of two subplots.\n    It sets up a side-by-side axes grid with a specified figure size.\n    On the left subplot, it performs RadViz analysis using occupancy data, fitting and finalizing the visualization.\n    On the right subplot, it performs Rank2D analysis using concrete data, fitting, transforming, and finalizing the visualization.\n    Finally, it saves the resulting figure to a specified file path.\n    \"\"\"\n    # Load datasets\n    occupancy_data = pd.read_csv('path_to_occupancy_data.csv')\n    concrete_data = pd.read_csv('path_to_concrete_data.csv')\n    \n    # Target features assumed to be 'Occupancy' for occupancy data and some column e.g., 'Strength' for concrete data\n    target_occ = 'Occupancy'  # adjust as per actual dataset\n    target_conc = 'Strength'  # adjust as per actual dataset\n    \n    # Feature names (excluding the target feature)\n    features_occ = [col for col in occupancy_data.columns if col != target_occ]\n    features_conc = [col for col in concrete_data.columns if col != target_conc]\n\n    # Initialize the matplotlib figure\n    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # RadViz for Occupancy data\n    radviz = RadViz(features=features_occ, ax=ax[0], classes=['Unoccupied', 'Occupied'])\n    radviz.fit_transform(occupancy_data[features_occ], occupancy_data[target_occ])\n    radviz.finalize()\n\n    # Rank2D analysis using Pearson correlation for Concrete data\n    rank2d = Rank2D(features=features_conc, ax=ax[1], algorithm='pearson')\n    rank2d.fit_transform(concrete_data[features_conc])\n    rank2d.finalize()\n\n    # Saving the figure\n    plt.savefig(fname)\n    plt.close()\n    \n# Note: Adjust paths to occupancy and concrete datasets, specify target and feature columns per actual data.", "CodeBleu_score": 34.58880278659411}]}
{"instance_id": "fd1b4b9cd7ef", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LassoCV, RidgeCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\n\ndef regression(fname=\"regression.png\"):\n    \"\"\"\n    This function generates and saves regression model evaluation figures.\n    It creates a figure with two subplots of size 18x6 inches and defines a range of alpha values for regularization.\n    The function loads the concrete dataset and splits it into training and testing sets.\n    In the first subplot, it plots the prediction error using LassoCV with the specified alphas, fitting the model on the training data and scoring it on the testing data.\n    In the second subplot, it plots the residuals using RidgeCV with the same alphas, fitting and scoring in a similar manner.\n    Finally, it adjusts the layout, saves the figure to a specified path, and returns the path.\n    \"\"\"\n    # Load concrete dataset\n    data = datasets.load_diabetes()\n    X = data.data\n    y = data.target\n\n    # Splitting the dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Define range of alpha values\n    alphas = np.logspace(-4, -0.5, 30)\n\n    # Set up the LassoCV and RidgeCV models\n    lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42)\n    ridge_cv = RidgeCV(alphas=alphas, cv=5)\n\n    # Fit models\n    lasso_cv.fit(X_train, y_train)\n    ridge_cv.fit(X_train, y_train)\n    \n    # Predict on test data\n    y_pred_lasso = lasso_cv.predict(X_test)\n    y_pred_ridge = ridge_cv.predict(X_test)\n    \n    # Calculate prediction error and residuals\n    prediction_error = y_test - y_pred_lasso\n    residuals = y_test - y_pred_ridge\n\n    # Creating the subplot figure\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n    \n    # Subplot 1: Prediction error of LassoCV\n    ax1.plot(y_test, prediction_error, 'o', label=\"Errors\")\n    ax1.set_title(\"Prediction Error using LassoCV\")\n    ax1.set_xlabel(\"True Values\")\n    ax1.set_ylabel(\"Prediction Error\")\n    \n    # Subplot 2: Residuals of RidgeCV\n    ax2.plot(y_test, residuals, 'o', label=\"Residuals\")\n    ax2.set_title(\"Residuals using RidgeCV\")\n    ax2.set_xlabel(\"True Values\")\n    ax2.set_ylabel(\"Residuals\")\n    \n    # Adjust layout and save figure\n    plt.tight_layout()\n    plt.savefig(fname)\n    \n    # Return path where the figure is saved\n    return fname", "CodeBleu_score": 31.829592787295386}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LassoCV, RidgeCV\nfrom sklearn.metrics import mean_squared_error\n\ndef regression(fname=\"regression.png\"):\n    # Create a figure with two subplots of size 18x6 inches\n    plt.figure(figsize=(18, 6))\n    \n    # Define a range of alpha values for regularization\n    alphas = np.logspace(-6, -1, 30)\n    \n    # Load concrete dataset\n    data = fetch_openml(\"concrete\", version=1, as_frame=True)\n    X = data['data']\n    y = data['target']\n    \n    # Split dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # LassoCV regression\n    lasso = LassoCV(alphas=alphas, cv=5, random_state=42)\n    lasso.fit(X_train, y_train)\n    y_pred_lasso = lasso.predict(X_test)\n\n    # Subplot 1: Plot prediction error\n    plt.subplot(1, 2, 1)\n    plt.plot(lasso.alphas_, np.sqrt(lasso.mse_path_.mean(axis=-1)), marker='o')\n    plt.xlabel('Alpha')\n    plt.ylabel('Root Mean Squared Error')\n    plt.title('LassoCV Prediction Error vs. Alpha')\n    plt.xscale('log')\n    plt.gca().invert_xaxis()\n    \n    # RidgeCV regression\n    ridge = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', cv=5)\n    ridge.fit(X_train, y_train)\n    y_pred_ridge = ridge.predict(X_test)\n    residuals = y_test - y_pred_ridge\n\n    # Subplot 2: Plot residuals\n    plt.subplot(1, 2, 2)\n    plt.scatter(y_test, residuals)\n    plt.xlabel('Actual Values')\n    plt.ylabel('Residuals')\n    plt.title('RidgeCV Residuals')\n    \n    # Adjust layout and save the figure\n    plt.tight_layout()\n    plt.savefig(fname)\n    \n    return fname", "CodeBleu_score": 35.059916359470805}, {"code": "def regression(fname=\"regression.png\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LassoCV, RidgeCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ndef regression(fname=\"regression.png\"):\n    \"\"\"\n    This function generates and saves regression model evaluation figures.\n    It creates a figure with two subplots of size 18x6 inches and defines a range of alpha values for regularization.\n    The function loads the concrete dataset and splits it into training and testing sets.\n    In the first subplot, it plots the prediction error using LassoCV with the specified alphas, fitting the model on \n    the training data and scoring it on the testing data.\n    In the second subplot, it plots the residuals using RidgeCV with the same alphas, fitting and scoring in a similar manner.\n    Finally, it adjusts the layout, saves the figure to a specified path, and returns the path.\n    \"\"\"\n    \n    # Define range of alphas for regularization\n    alphas = np.logspace(-6, 6, 13)\n\n    # Load concrete dataset\n    data = fetch_openml(name='concrete', version=1, as_frame=True)\n    X, y = data['data'], data['target']\n    \n    # Split dataset into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    # Set up figure and subplots\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n\n    # Lasso CV Regression\n    lasso_model = LassoCV(alphas=alphas, cv=5, random_state=42).fit(X_train, y_train)\n    y_pred_lasso = lasso_model.predict(X_test)\n    prediction_error_lasso = mean_squared_error(y_test, y_pred_lasso)\n\n    # Ridge CV Regression\n    ridge_model = RidgeCV(alphas=alphas, cv=5).fit(X_train, y_train)\n    y_pred_ridge = ridge_model.predict(X_test)\n    prediction_error_ridge = mean_squared_error(y_test, y_pred_ridge)\n    \n    # Plotting Lasso prediction errors\n    ax1.plot(lasso_model.alphas_, lasso_model.mse_path_.mean(axis=-1), marker='o', linestyle='-', label='MSE vs. Alpha (Lasso)')\n    ax1.set_title('Lasso Prediction Error')\n    ax1.set_xlabel('Alpha')\n    ax1.set_ylabel('Mean Squared Error')\n    ax1.legend()\n\n    # Plotting Ridge residuals\n    residuals_ridge = y_test - y_pred_ridge\n    ax2.scatter(y_test, residuals_ridge, color='blue', label='Residuals (Ridge)', alpha=0.5)\n    ax2.hlines(0, xmin=min(y_test), xmax=max(y_test), linestyles='dashed', color='black')\n    ax2.set_title('Ridge Residuals')\n    ax2.set_xlabel('True Values')\n    ax2.set_ylabel('Residuals')\n    ax2.legend()\n\n    # Adjust layout and save the figure\n    plt.tight_layout()\n    plt.savefig(fname)\n    \n    return fname\n\n# Example usage:\nfile_path = regression()\nprint(f'Saved regression plots to {file_path}')", "CodeBleu_score": 33.2373288634008}]}
{"instance_id": "627126918f88", "generation_result": [{"code": "import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import classification_report\nfrom yellowbrick.classifier import ClassificationReport, DiscriminationThreshold\n\n\ndef classification(fname=\"classification.png\"):\n    \"\"\"\n    This function generates and saves a classification visualization composed of two plots side-by-side.\n    It creates a grid of two axes with a specified figure size.\n    On the left axis, it loads a dataset with a train-test split, fits a MultinomialNB classifier, \n    and displays a ClassificationReport.\n    On the right axis, it loads the entire dataset, fits a LogisticRegression model, \n    and displays a DiscriminationThreshold plot.\n    Both visualizations are finalized, the layout is adjusted, and the combined figure is saved\n    to a specified file path.\n    \"\"\"\n    # Load dataset\n    data = load_iris()\n    X, y = data.data, data.target\n    \n    # Split data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n    \n    # Create figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n    \n    # Fit and visualize MultinomialNB classifier using ClassificationReport\n    model1 = MultinomialNB()\n    visualizer1 = ClassificationReport(model1, ax=ax1, support=True)\n    visualizer1.fit(X_train, y_train)        # Fit the visualizer and the model\n    visualizer1.score(X_test, y_test)        # Evaluate the model on the test data\n    visualizer1.finalize()\n    \n    # Fit and visualize LogisticRegression model using DiscriminationThreshold\n    model2 = LogisticRegression(max_iter=10000)\n    visualizer2 = DiscriminationThreshold(model2, ax=ax2)\n    visualizer2.fit(X, y)                    # Fit the visualizer and the model on the whole dataset\n    visualizer2.finalize()\n\n    # Adjust layout and save the figure\n    plt.tight_layout()\n    plt.savefig(fname)\n    plt.close()\n\n# Example usage\nclassification(\"classification.png\")", "CodeBleu_score": 33.67650837141043}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_files\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom yellowbrick.classifier import ClassificationReport, DiscriminationThreshold\n\ndef classification(fname=\"classification.png\"):\n    \"\"\"\n    This function generates and saves a classification visualization composed of two plots side-by-side.\n    It creates a grid of two axes with a specified figure size.\n    On the left axis, it loads the spam dataset with a train-test split, fits a MultinomialNB classifier, and displays a ClassificationReport.\n    On the right axis, it loads the entire spam dataset, fits a LogisticRegression model, and displays a DiscriminationThreshold plot.\n    Both visualizations are finalized, the layout is adjusted, and the combined figure is saved to a specified file path.\n    \"\"\"\n\n    # Load spam dataset which contains text files in directories labeled by class\n    data = load_files('path_to_spam_dataset', encoding='utf-8', decode_error='replace')\n    X, y = data.data, data.target\n\n    # Split the dataset into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n\n    # Left Plot: Classification Report using MultinomialNB\n    model1 = make_pipeline(CountVectorizer(), MultinomialNB())\n    viz1 = ClassificationReport(model1, ax=ax1, cmap=\"YlGnBu\")\n    viz1.fit(X_train, y_train)\n    viz1.score(X_test, y_test)\n    viz1.finalize()\n\n    # Right Plot: Discrimination Threshold using LogisticRegression\n    model2 = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000))\n    viz2 = DiscriminationThreshold(model2, ax=ax2)\n    viz2.fit(X, y)\n    viz2.finalize()\n\n    # Layout adjustment and save the figure to file\n    plt.tight_layout()\n    plt.savefig(fname)\n    plt.close()\n\n# Sample call to the function\n# classification(\"classification_output.png\")  # Uncomment to call the function with a specific file name.", "CodeBleu_score": 32.880869090577946}, {"code": "import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_files\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom yellowbrick.classifier import ClassificationReport, DiscriminationThreshold\n\ndef classification(fname=\"classification.png\"):\n    \"\"\"\n    This function generates and saves a classification visualization composed of two plots side-by-side.\n    It creates a grid of two axes with a specified figure size.\n    On the left axis, it loads spam dataset with a train-test split, fits a MultinomialNB classifier, and displays a ClassificationReport.\n    On the right axis, it loads the entire spam dataset, fits a LogisticRegression model, and displays a DiscriminationThreshold plot.\n    Both visualizations are finalized, the layout is adjusted, and the combined figure is saved to a specified file path.\n    \"\"\"\n    # Load the dataset (using a placeholder path for dataset)\n    data = load_files('/path/to/dataset_folder', encoding='utf-8')\n    X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=42)\n\n    # Set up the matplotlib figure and axes\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n\n    # Axis 1: MultinomialNB Classification Report\n    mnb_model = MultinomialNB()\n    viz1 = ClassificationReport(mnb_model, ax=ax1, classes=['ham', 'spam'], support=True)\n    viz1.fit(X_train, y_train)\n    viz1.score(X_test, y_test)\n    viz1.finalize()\n\n    # Axis 2: Logistic Regression Discrimination Threshold\n    lr_model = LogisticRegression(max_iter=1000)\n    X, y = data.data, data.target # use entire dataset\n    viz2 = DiscriminationThreshold(lr_model, ax=ax2)\n    viz2.fit(X, y)\n    viz2.finalize()\n\n    # Adjust layout and save the figure\n    plt.tight_layout()\n    plt.savefig(fname)\n\n# Example usage\nclassification()", "CodeBleu_score": 37.26289728219964}]}
{"instance_id": "318d3a674694", "generation_result": [{"code": "import matplotlib as mpl\nimport matplotlib.pyplot as plt\n\ndef get_color_cycle():\n    \"\"\"\n    This function retrieves the current color cycle from matplotlib.\n    \n    If the matplotlib version is 1.5.0 or greater, it attempts to get the color cycle from the \"axes.prop_cycle\" parameter.\n    If successful, it returns a list of colors from this cycle.\n    \n    If there is a KeyError or if the matplotlib version is below 1.5.0, it falls back to returning the color cycle from the \"axes.color_cycle\" parameter.\n    \"\"\"\n    if mpl.__version__ >= '1.5.0':\n        try:\n            color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n        except KeyError:\n            color_cycle = plt.rcParams.get('axes.color_cycle', [])\n    else:\n        color_cycle = plt.rcParams.get('axes.color_cycle', [])\n  \n    return color_cycle\n\n# Example usage\ncolors = get_color_cycle()\nprint(\"Color Cycle:\", colors)", "CodeBleu_score": 35.399672554031966}, {"code": "import matplotlib\nfrom matplotlib import pyplot as plt\n\ndef get_color_cycle():\n    \"\"\"\n    Retrieves the current color cycle from matplotlib.\n    If the matplotlib version is 1.5.0 or greater, it attempts to get the color cycle from the \"axes.prop_cycle\" parameter.\n    If successful, it returns a list of colors from this cycle.\n    If there is a KeyError or if the matplotlib version is below 1.5.0, it falls back to returning the color cycle from the \"axes.color_cycle\" parameter.\n    \"\"\"\n    # Parse the matplotlib version\n    version = matplotlib.__version__\n    \n    # Compare version number to 1.5.0\n    if version >= \"1.5.0\":\n        try:\n            # Use prop_cycle if possible (Post 1.5.0 version)\n            color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n            return list(color_cycle)\n        except KeyError:\n            # Fallback if 'color' key is not found in 'prop_cycle'\n            pass\n    \n    # Fallback to 'axes.color_cycle' if version below 1.5.0 or KeyError occurred\n    try:\n        # This was the default prior to version 1.5.0 or fallback\n        color_cycle = plt.rcParams['axes.color_cycle']\n        return list(color_cycle)\n    except KeyError:\n        # If 'color_cycle' is also somehow not set, return a default cycle\n        return ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n\n# This function should now correctly retrieve and return the color cycle or a suitable default.", "CodeBleu_score": 36.9001594816872}, {"code": "import matplotlib\nfrom distutils.version import LooseVersion\n\ndef get_color_cycle():\n    \"\"\"\n    This function retrieves the current color cycle from matplotlib.\n    If the matplotlib version is 1.5.0 or greater, it attempts to get the color cycle from the \"axes.prop_cycle\" parameter.\n    If successful, it returns a list of colors from this cycle.\n    If there is a KeyError or if the matplotlib version is below 1.5.0, it falls back to returning the color cycle from the \"axes.color_cycle\" parameter.\n    \"\"\"\n    if LooseVersion(matplotlib.__version__) >= LooseVersion(\"1.5.0\"):\n        # Get color cycle from \"axes.prop_cycle\"\n        try:\n            from matplotlib import rcParams\n            color_cycle = [item['color'] for item in rcParams['axes.prop_cycle']]\n            return color_cycle\n        except KeyError:\n            # Fallback if 'prop_cycle' does not have 'color' information\n            pass\n\n    # For older matplotlib versions or KeyError in newer versions\n    try:\n        from matplotlib import rcParams\n        color_cycle = rcParams.get('axes.color_cycle', None)\n        if color_cycle:\n            return color_cycle\n    except KeyError:\n        pass\n\n    # If all methods fail, return an empty list or a default color cycle\n    return ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n\n# Example usage\nif __name__ == \"__main__\":\n    current_color_cycle = get_color_cycle()\n    print(\"Current colors in cycle:\", current_color_cycle)", "CodeBleu_score": 38.93043925911039}]}
{"instance_id": "f3fac557334c", "generation_result": [{"code": "import matplotlib.pyplot as plt\n\ndef plot_knee_normalized(self):\n    \"\"\"\n    This function plots a normalized curve along with a distance curve and the knee point if it exists.\n    It imports the matplotlib.pyplot module for plotting.\n    The plot is created with a figure size of 8x8 inches.\n    It plots the normalized x and y values, followed by the distance curve in red.\n    The x and y ticks are set at intervals of 0.1 based on the minimum and maximum values of the respective data.\n    A vertical line is drawn at the knee point to highlight it on the plot.\n    \"\"\"\n    # Ensure the required attributes are present\n    if not hasattr(self, 'normalized_x') or not hasattr(self, 'normalized_y') or not hasattr(self, 'distance_curve') or not hasattr(self, 'knee_point'):\n        raise ValueError(\"The required data attributes (normalized_x, normalized_y, distance_curve, knee_point) are missing.\")\n    \n    plt.figure(figsize=(8,8))\n    plt.plot(self.normalized_x, self.normalized_y, label='Normalized Curve')\n    plt.plot(self.normalized_x, self.distance_curve, 'r', label='Distance Curve')\n    \n    if self.knee_point is not None:\n        plt.axvline(x=self.knee_point, color='green', linestyle='--', label='Knee Point')\n    \n    plt.xlabel('Normalized X')\n    plt.ylabel('Normalized Y / Distance')\n    \n    # Setting ticks on the axes\n    xticks = np.arange(min(self.normalized_x), max(self.normalized_x) + 0.1, 0.1)\n    yticks = np.arange(min(min(self.normalized_y), min(self.distance_curve)), max(max(self.normalized_y), max(self.distance_curve)) + 0.1, 0.1)\n    plt.xticks(xticks)\n    plt.yticks(yticks)\n    \n    plt.title('Knee Point Visualization')\n    plt.legend()\n    plt.grid(True)\n    plt.show()", "CodeBleu_score": 43.9158455637747}, {"code": "import matplotlib.pyplot as plt\n\nclass DataAnalyzer:\n    def __init__(self, norm_x, norm_y, distance_curve, knee_point):\n        self.norm_x = norm_x\n        self.norm_y = norm_y\n        self.distance_curve = distance_curve\n        self.knee_point = knee_point\n\n    def plot_knee_normalized(self):\n        # Import Matplotlib for plotting\n        import matplotlib.pyplot as plt\n        \n        # Create plot with specified figure size\n        fig, ax = plt.subplots(figsize=(8, 8))\n        \n        # Plot normalized x and y values\n        ax.plot(self.norm_x, self.norm_y, label='Normalized curve')\n        \n        # Plot distance curve in red\n        ax.plot(self.norm_x, self.distance_curve, color='red', label='Distance curve')\n        \n        # Setting x and y ticks based on the data range with 0.1 intervals\n        x_ticks = np.arange(min(self.norm_x), max(self.norm_x)+0.1, 0.1)\n        y_ticks = np.arange(min(self.norm_y + self.distance_curve), max(self.norm_y + self.distance_curve)+0.1, 0.1)\n        ax.set_xticks(x_ticks)\n        ax.set_yticks(y_ticks)\n        \n        # Drawing a vertical line at the knee point\n        ax.axvline(x=self.knee_point, color='green', linestyle='--', label='Knee point')\n        \n        # Adding labels and title\n        ax.set_xlabel('Normalized X')\n        ax.set_ylabel('Normalized Y / Distance')\n        ax.set_title('Normalized Curve and Distance with Knee Point')\n        ax.legend()\n        \n        # Show plot\n        plt.show()\n\n# Example data (fill these with actual data)\nnorm_x = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nnorm_y = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\ndistance_curve = [0.1, 0.15, 0.20, 0.1, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\nknee_point = 0.5\n\n# Initialize and use the DataAnalyzer class\nanalyzer = DataAnalyzer(norm_x, norm_y, distance_curve, knee_point)\nanalyzer.plot_knee_normalized()", "CodeBleu_score": 36.66911964955519}, {"code": "import matplotlib.pyplot as plt\n\ndef plot_knee_normalized(self):\n    \"\"\"\n    This function plots a normalized curve along with a distance curve and the knee point if it exists.\n    It imports the matplotlib.pyplot module for plotting.\n    The plot is created with a figure size of 8x8 inches.\n    It plots the normalized x and y values, followed by the distance curve in red.\n    The x and y ticks are set at intervals of 0.1 based on the minimum and maximum values of the respective data.\n    A vertical line is drawn at the knee point to highlight it on the plot.\n    \"\"\"\n    # Create the plot with a specified figure size\n    plt.figure(figsize=(8, 8))\n\n    # Plot the normalized data\n    plt.plot(self.x_normalized, self.y_normalized, label='Normalized Curve')\n\n    # Plot the distance curve\n    plt.plot(self.x_normalized, self.y_distance, 'r', label='Distance Curve')  # assuming distance is stored in y_distance\n\n    # Adding x and y ticks\n    plt.xticks(\n        ticks=np.arange(min(self.x_normalized), max(self.x_normalized)+0.1, 0.1),\n        rotation=45\n    )\n    plt.yticks(\n        ticks=np.arange(min(min(self.y_normalized), min(self.y_distance)), \n                        max(max(self.y_normalized), max(self.y_distance))+0.1, 0.1)\n    )\n\n    # Draw a vertical line at the knee point\n    if hasattr(self, 'knee_point'):\n        plt.axvline(x=self.knee_point, color='green', linestyle='--', label='Knee Point')\n\n    # Adding labels, title and legend\n    plt.xlabel('Normalized X')\n    plt.ylabel('Value')\n    plt.title('Normalized Curve and Distance Curve')\n    plt.legend()\n\n    # Show the plot\n    plt.show()", "CodeBleu_score": 46.52707225017414}]}
{"instance_id": "b2d48aae4fce", "generation_result": [{"code": "import matplotlib.pyplot as plt\n\ndef plot_knee(self):\n    \"\"\"\n    This function plots a curve and its knee point, if it exists.\n    It uses matplotlib to create a plot with a specified figure size of 8x8 inches.\n    The function plots the data points defined by self.x and self.y.\n    It then draws a vertical line at the knee point along the y-axis limits.\n    The plot is displayed with the curve and the knee point highlighted.\n    \"\"\"\n    # Create the figure and the axes\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y, label='Data Curve')  # Plotting the curve using data from self.x and self.y\n    \n    # Check if the knee point exists and plot if true\n    if hasattr(self, 'knee_x') and self.knee_x is not None:\n        plt.axvline(x=self.knee_x, color='r', linestyle='--', label='Knee Point')  # Drawing the knee line\n        \n        # Highlighting the knee point\n        knee_index = list(self.x).index(self.knee_x)\n        knee_y = self.y[knee_index]\n        plt.plot(self.knee_x, knee_y, 'ro')  # Marking the knee point with a red dot\n    \n    plt.xlabel('X Axis')\n    plt.ylabel('Y Axis')\n    plt.title('Plot of Data Curve with Knee Point')\n    plt.legend()\n    plt.grid(True)\n    plt.show()", "CodeBleu_score": 45.71630951025798}, {"code": "import matplotlib.pyplot as plt\n\nclass DataPlotter:\n    def __init__(self, x, y, knee_point):\n        self.x = x\n        self.y = y\n        self.knee_point = knee_point\n\n    def plot_knee(self):\n        \"\"\"Plot a curve and its knee point using matplotlib.\n\n        This function uses matplotlib to create a plot with a specified figure size of 8x8 inches.\n        It plots the data points defined by self.x and self.y.\n        It then draws a vertical line at the knee point along the y-axis limits.\n        The plot is displayed with the curve and the knee point highlighted.\n        \"\"\"\n        plt.figure(figsize=(8, 8))\n        plt.plot(self.x, self.y, label='Data Curve')\n        \n        # Drawing the knee point as a vertical line\n        plt.axvline(x=self.knee_point, color='r', linestyle='--', label='Knee Point')\n        plt.ylim(min(self.y), max(self.y))  # Setting y-axis limits based on data range\n        \n        plt.xlabel('X-Axis')\n        plt.ylabel('Y-Axis')\n        plt.title('Plot of Data and Knee Point')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n\n# Example usage:\n# Assuming you have the following data and a knee point\nx_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny_data = [10, 9, 8, 6, 7, 5, 3, 2, 1, 0]\nknee = 4  # Assuming the knee point is at x=4 for demonstration\n\nplotter = DataPlotter(x_data, y_data, knee)\nplotter.plot_knee()", "CodeBleu_score": 39.4176559332945}, {"code": "import matplotlib.pyplot as plt\n\nclass DataPlotter:\n    def __init__(self, x, y, knee_point):\n        self.x = x\n        self.y = y\n        self.knee_point = knee_point\n\n    def plot_knee(self):\n        \"\"\"\n        This function plots a curve and its knee point, if it exists.\n        It uses matplotlib to create a plot with a specified figure size of 8x8 inches.\n        The function plots the data points defined by self.x and self.y.\n        It then draws a vertical line at the knee point along the y-axis limits.\n        The plot is displayed with the curve and the knee point highlighted.\n        \"\"\"\n        # Create a figure with specified figure size\n        plt.figure(figsize=(8, 8))\n        \n        # Plot the data points\n        plt.plot(self.x, self.y, label='Data Curve', marker='o')\n        \n        # Plot the knee point as a vertical line\n        plt.axvline(x=self.knee_point, color='r', linestyle='--', label='Knee Point')\n        \n        # Setting plot labels and title\n        plt.xlabel('X-axis')\n        plt.ylabel('Y-axis')\n        plt.title('Plot of Data and Knee Point')\n        plt.legend()\n        \n        # Show the plot\n        plt.show()\n\n# Example usage:\n# Let's assume some hypothetical data and a knee point\nx = list(range(10))\ny = [xi**2 for xi in x]\nknee_point = 3\n\nplotter = DataPlotter(x, y, knee_point)\nplotter.plot_knee()", "CodeBleu_score": 46.0677438296474}]}
{"instance_id": "4450ba2384c9", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom IPython.display import display\nfrom ipywidgets import interact\n\ndef plot_pca_interactive(data, n_components=6):\n    \"\"\"\n    This function performs PCA on the given data and provides an interactive plot \n    to visualize the PCA components. It imports the necessary PCA module from sklearn \n    and interact from IPython widgets.\n    \n    The function applies PCA to the data with a specified number of components and \n    transforms the data accordingly.\n    \n    An internal function, show_decomp, is defined to plot the original data \n    and its PCA components for a selected index.\n    \n    The interact function creates an interactive widget to select the index, \n    updating the plot to show the PCA decomposition for the chosen data instance.\n    \"\"\"\n    \n    # Step 1: Perform PCA\n    pca = PCA(n_components=n_components)\n    pca_data = pca.fit_transform(data)\n    \n    # Step 2: Define an internal function for plotting\n    def show_decomp(index=0):\n        plt.figure(figsize=(14, 7))\n        \n        # Original data\n        plt.subplot(1, 2, 1)\n        plt.scatter(data[:, 0], data[:, 1], alpha=0.2)\n        plt.scatter(data[index, 0], data[index, 1], color='red') # to highlight selected point\n        plt.title('Original Data')\n        plt.xlabel('Feature 1')\n        plt.ylabel('Feature 2')\n        \n        # PCA transformed data\n        plt.subplot(1, 2, 2)\n        plt.scatter(pca_data[:, 0], pca_data[:, 1], alpha=0.2)\n        plt.scatter(pca_data[index, 0], pca_data[index, 1], color='red') # to highlight selected data point\n        plt.title('PCA Transformed Data')\n        plt.xlabel('Principal Component 1')\n        plt.ylabel('Principal Component 2')\n        \n        plt.show()\n    \n    # Step 4: Create interactive widget\n    interact(show_decomp, index=(0, data.shape[0]-1))", "CodeBleu_score": 54.611552396343384}, {"code": "from sklearn.decomposition import PCA\nfrom ipywidgets import interact\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_pca_interactive(data, n_components=6):\n    \"\"\"\n    This function performs PCA on the given data and provides an interactive plot to visualize the PCA components.\n    It imports the necessary PCA module from sklearn and interact from IPython widgets.\n    The function applies PCA to the data with a specified number of components and transforms the data accordingly.\n    An internal function, show_decomp, is defined to plot the original data and its PCA components for a selected index.\n    The interact function creates an interactive widget to select the index, updating the plot to show the PCA decomposition for the chosen data instance.\n\n    Parameters:\n        data (np.ndarray): Input data matrix where each row is a data instance and each column is a feature.\n        n_components (int): Number of principal components to compute.\n    \"\"\"\n    \n    # Step 1: Fit PCA\n    pca = PCA(n_components=n_components)\n    pca_transformed_data = pca.fit_transform(data)\n    \n    # Step 2: Create internal function to plot the PCA components for a selected index\n    def show_decomp(index):\n        plt.figure(figsize=(12, 6))\n        \n        plt.subplot(1, 2, 1)\n        plt.title('Original Data')\n        plt.scatter(data[:, 0], data[:, 1], color='blue', alpha=0.5)\n        plt.scatter(data[index, 0], data[index, 1], color='red')\n        \n        plt.subplot(1, 2, 2)\n        plt.title('PCA Components')\n        plt.scatter(pca_transformed_data[:, 0], pca_transformed_data[:, 1], color='green', alpha=0.5)\n        plt.scatter(pca_transformed_data[index, 0], pca_transformed_data[index, 1], color='red')\n        \n        plt.show()\n    \n    # Step 3: Create interactive widget\n    interact(show_decomp, index=(0, len(data)-1))", "CodeBleu_score": 44.31973891358209}, {"code": "import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom ipywidgets import interact\nimport numpy as np\n\ndef plot_pca_interactive(data, n_components=6):\n    \"\"\"\n    This function performs PCA on the given data and provides an interactive plot to visualize the PCA components.\n    It imports the necessary PCA module from sklearn and interact from IPython widgets.\n    The function applies PCA to the data with a specified number of components and transforms the data accordingly.\n    An internal function, show_decomp, is defined to plot the original data and its PCA components for a selected index.\n    The interact function creates an interactive widget to select the index, updating the plot to show the PCA decomposition for the chosen data instance.\n    \"\"\"\n\n    # Apply PCA to the dataset\n    pca = PCA(n_components=n_components)\n    pca_transformed_data = pca.fit_transform(data)\n    \n    # Define the internal function to plot the original data and its PCA components\n    def show_decomp(index):\n        plt.figure(figsize=(12, 6))\n        plt.subplot(1, 2, 1)\n        plt.title(\"Original Data\")\n        plt.scatter(data[:, 0], data[:, 1], alpha=0.5)\n        plt.scatter(data[index, 0], data[index, 1], c='red')  # Highlight the selected index\n        \n        plt.subplot(1, 2, 2)\n        plt.title(\"PCA Components\")\n        plt.scatter(pca_transformed_data[:, 0], pca_transformed_data[:, 1], alpha=0.5)\n        plt.scatter(pca_transformed_data[index, 0], pca_transformed_data[index, 1], c='red')  # Highlight the selected index\n        \n        plt.show()\n    \n    # Create an interactive widget to select an index\n    interact(show_decomp, index=(0, len(data) - 1))", "CodeBleu_score": 45.04850793657368}]}
{"instance_id": "b09a163f3ff1", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndef plot_iris_knn():\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # use the first two features: sepal length and sepal width\n    y = iris.target\n    \n    # Create the kNN classifier with 5 neighbors\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X, y)\n\n    # Create a mesh grid over the feature space\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    h = .02  # step size in the mesh\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Predict the class for each point in the grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Define the color plot\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.8)\n\n    # Scatter the training points with labels\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('Decision boundary of kNN classifier on the Iris dataset')\n    \n    # Create a legend\n    plt.legend(handles=scatter.legend_elements()[0], labels=iris.target_names, title=\"Classes\")\n\n    # Show the plot\n    plt.show()\n\nplot_iris_knn()", "CodeBleu_score": 60.10845833015925}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndef plot_iris_knn():\n    \"\"\"\n    This function plots the decision boundaries of a k-Nearest Neighbors (kNN) classifier on the Iris dataset using the first two features.\n    It loads the Iris dataset and extracts the first two features and the target labels.\n    A kNN classifier with 5 neighbors is trained on this data.\n    The function defines a mesh grid over the feature space and predicts the class for each point in the grid using the trained kNN model.\n    The predictions are reshaped and displayed as a color plot representing the decision boundaries.\n    Additionally, the training points are plotted on top of the decision boundary plot with labeled axes for sepal length and sepal width.\n    \"\"\"\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n    # Create a k-NN classifier with 5 neighbors\n    clf = KNeighborsClassifier(n_neighbors=5)\n    clf.fit(X, y)\n\n    # Create a mesh grid\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n\n    # Predict class for each point in the mesh grid\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot the decision boundaries\n    plt.figure()\n    plt.contourf(xx, yy, Z, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='g')\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('Decision boundaries of a k-NN classifier (k=5) on the Iris dataset')\n    plt.show()\n\n# To use the function, simply call it:\n# plot_iris_knn()", "CodeBleu_score": 63.19154832857692}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndef plot_iris_knn():\n    \"\"\"\n    This function plots the decision boundaries of a k-Nearest Neighbors (kNN) classifier on the Iris dataset using the first two features.\n    It loads the Iris dataset and extracts the first two features and the target labels.\n    A kNN classifier with 5 neighbors is trained on this data.\n    The function defines a mesh grid over the feature space and predicts the class for each point in the grid using the trained kNN model.\n    The predictions are reshaped and displayed as a color plot representing the decision boundaries.\n    Additionally, the training points are plotted on top of the decision boundary plot with labeled axes for sepal length and sepal width.\n    \"\"\"\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n    # Create the kNN model\n    knn = KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X, y)\n\n    # Create a mesh grid\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    h = .02  # step size in the mesh\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # Predict the class for each point in the mesh grid\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plot the results\n    plt.figure(figsize=(8, 6))\n    plt.contourf(xx, yy, Z, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='g')\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('kNN decision boundaries with training points')\n    plt.show()\n\n# To use the function, simply call it:\n# plot_iris_knn()", "CodeBleu_score": 68.4734436701616}]}
{"instance_id": "8eaee3207c09", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.datasets import make_blobs\n\ndef plot_sgd_separator():\n    \"\"\"\n    This function plots the decision boundary of an SGD classifier.\n    It creates 50 separable data points using the make_blobs function with two centers and a specified standard deviation.\n    The SGD classifier is instantiated with a hinge loss function, an alpha value of 0.01, 200 iterations, and fit_intercept set to True, then fitted to the generated data.\n    The function prepares a meshgrid of values for plotting the decision boundary and evaluates the decision function at each point in the grid.\n    It uses contour plotting to visualize the decision boundary at levels -1.0, 0.0, and 1.0 with different line styles.\n    Finally, it scatters the original data points on the plot and sets the axis to 'tight'.\n    \"\"\"\n    # Generate 50 separable data points\n    X, y = make_blobs(n_samples=50, centers=2, random_state=6, cluster_std=1.0)\n\n    # Create an SGD classifier\n    clf = SGDClassifier(loss='hinge', alpha=0.01, max_iter=200, fit_intercept=True, random_state=42)\n    clf.fit(X, y)\n\n    # Create a meshgrid for plotting\n    xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 500),\n                         np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 500))\n\n    # Evaluate the decision function on the grid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plotting the decision boundary\n    plt.contour(xx, yy, Z, levels=[-1.0, 0.0, 1.0], linestyles=['dashed', 'solid', 'dashed'], colors='k')\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n\n    # Setting the plot limits\n    plt.axis('tight')\n    plt.show()\n\n# Call the function to plot\nplot_sgd_separator()", "CodeBleu_score": 40.78100875978996}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.datasets import make_blobs\n\ndef plot_sgd_separator():\n    \"\"\"\n    This function plots the decision boundary of an SGD classifier.\n    It creates 50 separable data points using the make_blobs function with two centers and a specified standard deviation.\n    The SGD classifier is instantiated with a hinge loss function, an alpha value of 0.01, 200 iterations, and fit_intercept set to True, then fitted to the generated data.\n    The function prepares a meshgrid of values for plotting the decision boundary and evaluates the decision function at each point in the grid.\n    It uses contour plotting to visualize the decision boundary at levels -1.0, 0.0, and 1.0 with different line styles.\n    Finally, it scatters the original data points on the plot and sets the axis to 'tight'.\n    \"\"\"\n    # Generate 50 separable points\n    X, y = make_blobs(n_samples=50, centers=2, random_state=6, cluster_std=1.0)\n\n    # Create the SGD classifier\n    clf = SGDClassifier(loss='hinge', alpha=0.01, max_iter=200, fit_intercept=True, random_state=42)\n    clf.fit(X, y)\n\n    # Create a meshgrid for plotting\n    xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n                         np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n\n    # Evaluate the decision function on the grid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plotting the decision boundary\n    plt.contour(xx, yy, Z, levels=[-1.0, 0.0, 1.0], linestyles=['dashed', 'solid', 'dashed'], colors='k')\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n\n    # Setting the plot limits\n    plt.axis('tight')\n    plt.show()", "CodeBleu_score": 41.00144656932905}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.datasets import make_blobs\n\ndef plot_sgd_separator():\n    \"\"\"\n    This function plots the decision boundary of an SGD classifier.\n    It creates 50 separable data points using the make_blobs function with two centers and a specified standard deviation.\n    The SGD classifier is instantiated with a hinge loss function, an alpha value of 0.01, 200 iterations, and fit_intercept set to True, then fitted to the generated data.\n    The function prepares a meshgrid of values for plotting the decision boundary and evaluates the decision function at each point in the grid.\n    It uses contour plotting to visualize the decision boundary at levels -1.0, 0.0, and 1.0 with different line styles.\n    Finally, it scatters the original data points on the plot and sets the axis to 'tight'.\n    \"\"\"\n    # Generate 50 separable data points\n    X, y = make_blobs(n_samples=50, centers=2, random_state=6, cluster_std=0.60)\n\n    # Create the SGD classifier\n    clf = SGDClassifier(loss='hinge', alpha=0.01, max_iter=200, fit_intercept=True, random_state=42)\n    clf.fit(X, y)\n\n    # Create a meshgrid for plotting\n    xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 500),\n                         np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 500))\n\n    # Evaluate the decision function on the grid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # Plotting the decision boundary\n    plt.contour(xx, yy, Z, levels=[-1.0, 0.0, 1.0], linestyles=['dashed', 'solid', 'dashed'], colors='k')\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n\n    # Setting the plot limits\n    plt.axis('tight')\n    plt.show()\n\n# Call the function to plot\nplot_sgd_separator()", "CodeBleu_score": 40.98348940069993}]}
{"instance_id": "8b54f999b4c0", "generation_result": [{"code": "import numpy as np\nfrom sklearn.datasets import dump_svmlight_file as skl_dump_svmlight_file\n\ndef dump_svmlight_file(self, file):\n    \"\"\"\n    This function exports data to a file in the SVMLight format.\n    It converts the object's data attribute to a NumPy array, then separates the first two columns as features (X) and the third column as labels (y).\n    It uses the dump_svmlight_file function to save the features and labels to the specified file.\n    \"\"\"\n    # Convert the data attribute to a NumPy array\n    data_array = np.array(self.data)\n    \n    # Separate the first two columns as features and the third column as labels\n    X = data_array[:, :2]\n    y = data_array[:, 2]\n    \n    # Use the dump_svmlight_file function from sklearn to save the features and labels to the specified file\n    skl_dump_svmlight_file(X, y, file)", "CodeBleu_score": 47.94104025044865}, {"code": "import numpy as np\nfrom sklearn.datasets import dump_svmlight_file as skl_dump_svmlight_file\n\ndef dump_svmlight_file(self, file):\n    \"\"\"\n    This function exports data to a file in the SVMLight format.\n    It converts the object's data attribute to a NumPy array, then separates the first two columns as features (X) \n    and the third column as labels (y).\n    It uses the dump_svmlight_file function to save the features and labels to the specified file.\n    \"\"\"\n    # Convert the data attribute to a NumPy array\n    data_array = np.array(self.data)\n    \n    # Separate the first two columns as features (X) and the third column as labels (y)\n    X = data_array[:, :2]\n    y = data_array[:, 2]\n    \n    # Use the dump_svmlight_file function from sklearn to save the features and labels to the specified file\n    skl_dump_svmlight_file(X, y, file)", "CodeBleu_score": 47.94104025044865}, {"code": "import numpy as np\nfrom sklearn.datasets import dump_svmlight_file as skl_dump_svmlight_file\n\ndef dump_svmlight_file(self, file):\n    \"\"\"\n    This function exports data to a file in the SVMLight format.\n    It converts the object's data attribute to a NumPy array, then separates the first two columns as features (X) and the third column as labels (y).\n    It uses the dump_svmlight_file function to save the features and labels to the specified file.\n    \"\"\"\n    # Convert the data attribute to a NumPy array\n    data_array = np.array(self.data)\n    \n    # Separate the first two columns as features and the third column as labels\n    X = data_array[:, :2]\n    y = data_array[:, 2]\n    \n    # Use the dump_svmlight_file function from sklearn to save the features and labels to the specified file\n    skl_dump_svmlight_file(X, y, file)", "CodeBleu_score": 47.94104025044865}]}
{"instance_id": "2419013adc6e", "generation_result": [{"code": "from sklearn.svm import SVC, OneClassSVM\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nclass SVMModel:\n    def __init__(self, data, labels, complexity=1.0, gamma='scale', coef0=0.0, degree=3, kernel='rbf'):\n        self.data = data\n        self.labels = labels\n        self.complexity = complexity\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.degree = degree\n        self.kernel = kernel\n        self.model = None\n        self.surface = None\n        self.surface_type = None\n        self.is_fitted = False\n\n    def fit(self):\n        print(\"Starting the fitting process.\")\n        \n        # Splitting the data into features and labels\n        X, y = self.data, self.labels\n        \n        # Retrieving SVM parameters\n        params = {\n            'C': self.complexity,\n            'gamma': self.gamma,\n            'coef0': self.coef0,\n            'degree': self.degree,\n            'kernel': self.kernel\n        }\n        \n        # Check if all labels are the same\n        if len(set(y)) == 1:\n            self.model = OneClassSVM(**params)\n        else:\n            self.model = SVC(**params)\n        \n        # Fitting the model\n        self.model.fit(X, y)\n        \n        # Checking if the model has a score method and printing the accuracy\n        if hasattr(self.model, 'score'):\n            accuracy = self.model.score(X, y)\n            print(f\"Model accuracy on training data: {accuracy}\")\n        \n        # Calculating decision surface (this is a placeholder, actual implementation may vary)\n        self.surface = \"Decision surface calculated\"  # Placeholder\n        self.surface_type = \"SVM decision surface\"\n        \n        # Setting the model as fitted\n        self.is_fitted = True\n        \n        # Triggering an event (placeholder for actual event handling)\n        print(\"Model's surface has changed.\")", "CodeBleu_score": 27.203975639748524}, {"code": "from sklearn.svm import SVC, OneClassSVM\n\ndef fit(self):\n    print(\"Fitting process has started.\")\n    \n    # Extracting features X and labels y from the training data\n    X = self.data[:, :-1]\n    y = self.data[:, -1]\n    \n    # Retrieving SVM parameters from the object's attributes\n    complexity = self.complexity\n    gamma = self.gamma\n    coef0 = self.coef0\n    degree = self.degree\n    kernel = self.kernel\n    \n    # Initialize the appropriate SVM classifier\n    if len(set(y)) == 1:  # Check if all labels are the same\n        classifier = OneClassSVM(kernel=kernel, gamma=gamma, coef0=coef0, degree=degree)\n    else:\n        classifier = SVC(C=complexity, kernel=kernel, gamma=gamma, coef0=coef0, degree=degree)\n    \n    # Fit the classifier to the data\n    classifier.fit(X, y)\n    \n    # If the classifier has a score method, print the accuracy\n    if hasattr(classifier, 'score'):\n        accuracy = classifier.score(X, y)\n        print(f\"Model accuracy on the training data: {accuracy:.2f}\")\n    \n    # Calculate and store the decision surface\n    self.decision_surface = classifier.decision_function(X)\n    \n    # Update the model's surface type and fitting flag\n    self.surface_type = 'SVM'\n    self.is_fitted = True\n    \n    # Trigger an event indicating that the model's surface has changed\n    self.trigger_event('model_surface_changed')", "CodeBleu_score": 33.00104993465155}, {"code": "from sklearn.svm import SVC, OneClassSVM\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nclass SVMModel:\n    def __init__(self, data, labels, complexity=1.0, gamma='scale', coef0=0.0, degree=3, kernel='rbf'):\n        self.data = data\n        self.labels = labels\n        self.complexity = complexity\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.degree = degree\n        self.kernel = kernel\n        self.model = None\n        self.surface = None\n        self.surface_type = None\n        self.is_fitted = False\n\n    def fit(self):\n        print(\"Fitting the model...\")\n        X, y = self.data, self.labels\n\n        # Check if all labels are the same\n        if len(set(y)) == 1:\n            self.model = OneClassSVM(kernel=self.kernel, degree=self.degree, gamma=self.gamma, coef0=self.coef0)\n        else:\n            self.model = SVC(C=self.complexity, kernel=self.kernel, degree=self.degree, gamma=self.gamma, coef0=self.coef0)\n\n        self.model.fit(X, y)\n\n        if hasattr(self.model, 'score'):\n            accuracy = self.model.score(X, y)\n            print(f\"Model accuracy: {accuracy}\")\n\n        # Assuming decision surface calculation (this is a placeholder)\n        self.surface = \"Decision surface calculated\"  # Placeholder\n        self.surface_type = \"Type of the surface\"  # Placeholder\n        self.is_fitted = True\n\n        print(\"Model fitting completed and the surface has changed.\")", "CodeBleu_score": 28.782765814345396}]}
{"instance_id": "e887fd2a8513", "generation_result": [{"code": "def source_decode(sourcecode, verbose=0):\n    \"\"\"\n    This function attempts to import and decode a given source code string.\n    It splits the source code into its module and object components.\n    If the source code starts with \"tpot.\", it imports the object from the corresponding module, removing the \"tpot.\" prefix.\n    Otherwise, it imports the object from the module directly.\n    If the import fails and the verbose level is higher than 2, it raises an ImportError with the error details.\n    If verbose is not higher than 2, it prints a warning message.\n    The function returns the import string, object string, and the imported object (or None if the import failed).\n    \"\"\"\n    import_string = None\n    object_string = None\n    imported_object = None\n\n    try:\n        # Split the source code into module and object parts\n        parts = sourcecode.split('.')\n        if sourcecode.startswith(\"tpot.\"):\n            import_string = '.'.join(parts[1:-1])\n            object_string = parts[-1]\n        else:\n            import_string = '.'.join(parts[:-1])\n            object_string = parts[-1]\n\n        # Import the module and object\n        module = __import__(import_string, fromlist=[object_string])\n        imported_object = getattr(module, object_string)\n\n    except ImportError as e:\n        if verbose > 2:\n            raise ImportError(f\"Failed to import {object_string} from {import_string}: {str(e)}\")\n        else:\n            print(f\"Warning: Failed to import {object_string} from {import_string}. Error: {str(e)}\")\n\n    return import_string, object_string, imported_object", "CodeBleu_score": 41.73032540499547}, {"code": "def source_decode(sourcecode, verbose=0):\n    \"\"\"\n    This function attempts to import and decode a given source code string.\n    It splits the source code into its module and object components.\n    If the source code starts with \"tpot.\", it imports the object from the corresponding module, removing the \"tpot.\" prefix.\n    Otherwise, it imports the object from the module directly.\n    If the import fails and the verbose level is higher than 2, it raises an ImportError with the error details.\n    If verbose is not higher than 2, it prints a warning message.\n    The function returns the import string, object string, and the imported object (or None if the import failed).\n    \"\"\"\n    import_string = None\n    object_string = None\n    imported_object = None\n\n    try:\n        # Split the source code into module and object parts\n        parts = sourcecode.split('.')\n        if sourcecode.startswith(\"tpot.\"):\n            import_string = '.'.join(parts[1:-1])\n            object_string = parts[-1]\n        else:\n            import_string = '.'.join(parts[:-1])\n            object_string = parts[-1]\n\n        # Import the module and object\n        module = __import__(import_string, fromlist=[object_string])\n        imported_object = getattr(module, object_string)\n\n    except ImportError as e:\n        if verbose > 2:\n            raise ImportError(f\"Failed to import {object_string} from {import_string}: {str(e)}\")\n        else:\n            print(f\"Warning: Failed to import {object_string} from {import_string}. Error: {str(e)}\")\n\n    return import_string, object_string, imported_object", "CodeBleu_score": 41.73032540499547}, {"code": "def source_decode(sourcecode, verbose=0):\n    \"\"\"\n    This function attempts to import and decode a given source code string.\n    It splits the source code into its module and object components.\n    If the source code starts with \"tpot.\", it imports the object from the corresponding module, removing the \"tpot.\" prefix.\n    Otherwise, it imports the object from the module directly.\n    If the import fails and the verbose level is higher than 2, it raises an ImportError with the error details.\n    If verbose is not higher than 2, it prints a warning message.\n    The function returns the import string, object string, and the imported object (or None if the import failed).\n    \"\"\"\n    import_string = None\n    object_string = None\n    imported_object = None\n\n    try:\n        # Split the source code into module and object parts\n        parts = sourcecode.split('.')\n        if sourcecode.startswith(\"tpot.\"):\n            import_string = '.'.join(parts[1:-1])\n            object_string = parts[-1]\n        else:\n            import_string = '.'.join(parts[:-1])\n            object_string = parts[-1]\n\n        # Import the module and object\n        module = __import__(import_string, fromlist=[object_string])\n        imported_object = getattr(module, object_string)\n\n    except ImportError as e:\n        if verbose > 2:\n            raise ImportError(f\"Failed to import {object_string} from {import_string}: {str(e)}\")\n        else:\n            print(f\"Warning: Failed to import {object_string} from {import_string}. Error: {str(e)}\")\n\n    return import_string, object_string, imported_object", "CodeBleu_score": 41.73032540499547}]}
{"instance_id": "5d6f629c3363", "generation_result": [{"code": "def set_sample_weight(pipeline_steps, sample_weight=None):\n    \"\"\"\n    This function sets sample weights for the steps in a machine learning pipeline.\n    It initializes an empty dictionary for sample weights and checks if sample_weight is not None.\n    For each step in the pipeline, it inspects the fit method to see if it accepts a \"sample_weight\" argument.\n    If it does, it constructs a key by combining the step name and \"sample_weight\", and adds it to the dictionary.\n    The function returns the sample weight dictionary if it is not empty, otherwise, it returns None.\n    \"\"\"\n    if sample_weight is None:\n        return None\n\n    sample_weight_dict = {}\n    for name, step in pipeline_steps:\n        if 'sample_weight' in step.fit.__code__.co_varnames:\n            sample_weight_dict[f\"{name}__sample_weight\"] = sample_weight\n\n    return sample_weight_dict if sample_weight_dict else None", "CodeBleu_score": 30.62208347898504}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None):\n    \"\"\"\n    This function sets sample weights for the steps in a machine learning pipeline.\n    It initializes an empty dictionary for sample weights and checks if sample_weight is not None.\n    For each step in the pipeline, it inspects the fit method to see if it accepts a \"sample_weight\" argument.\n    If it does, it constructs a key by combining the step name and \"sample_weight\", and adds it to the dictionary.\n    The function returns the sample weight dictionary if it is not empty, otherwise, it returns None.\n    \"\"\"\n    if sample_weight is None:\n        return None\n\n    sample_weight_dict = {}\n    for name, step in pipeline_steps:\n        if 'sample_weight' in step.fit.__code__.co_varnames:\n            sample_weight_dict[f\"{name}__sample_weight\"] = sample_weight\n\n    return sample_weight_dict if sample_weight_dict else None", "CodeBleu_score": 30.62208347898504}, {"code": "def set_sample_weight(pipeline_steps, sample_weight=None):\n    \"\"\"\n    This function sets sample weights for the steps in a machine learning pipeline.\n    It initializes an empty dictionary for sample weights and checks if sample_weight is not None.\n    For each step in the pipeline, it inspects the fit method to see if it accepts a \"sample_weight\" argument.\n    If it does, it constructs a key by combining the step name and \"sample_weight\", and adds it to the dictionary.\n    The function returns the sample weight dictionary if it is not empty, otherwise, it returns None.\n    \"\"\"\n    if sample_weight is None:\n        return None\n\n    sample_weight_dict = {}\n    for name, step in pipeline_steps:\n        if 'sample_weight' in step.fit.__code__.co_varnames:\n            sample_weight_dict[f\"{name}__sample_weight\"] = sample_weight\n\n    return sample_weight_dict if sample_weight_dict else None", "CodeBleu_score": 30.62208347898504}]}
{"instance_id": "cae17d519818", "generation_result": [{"code": "import warnings\n\ndef _summary_of_best_pipeline(self, features, target):\n    \"\"\"\n    This function provides a summary of the best pipeline identified by TPOT optimization.\n    It first checks if an optimized pipeline exists, raising a RuntimeError with a descriptive message if not.\n    If an optimized pipeline is available, it compiles and fits the pipeline using the provided features and target, while ignoring warnings.\n    If verbosity is set to 1 or 2, it prints the best pipeline string, adding extra spacing if verbosity is 2 or higher.\n    The function also compiles and fits the entire Pareto front of pipelines, storing them as fitted models for convenience.\n    \"\"\"\n    if not hasattr(self, 'fitted_pipeline_'):\n        raise RuntimeError(\"No optimized pipeline is available. Please run the optimization process first.\")\n    \n    # Ignore warnings during pipeline fitting\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        \n        # Fit the best pipeline\n        self.fitted_pipeline_.fit(features, target)\n        \n        # Print the best pipeline if verbosity is set\n        if self.verbosity in [1, 2]:\n            print(\"\nBest pipeline:\")\n            print(self.fitted_pipeline_)\n            if self.verbosity >= 2:\n                print(\"\n\")\n    \n    # Fit and store all pipelines in the Pareto front\n    self.fitted_models_ = []\n    for pipeline in self.pareto_front_fitted_pipelines_:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            pipeline.fit(features, target)\n            self.fitted_models_.append(pipeline)", "CodeBleu_score": 32.86694271114518}, {"code": "import warnings\n\ndef _summary_of_best_pipeline(self, features, target):\n    \"\"\"\n    This function provides a summary of the best pipeline identified by TPOT optimization.\n    It first checks if an optimized pipeline exists, raising a RuntimeError with a descriptive message if not.\n    If an optimized pipeline is available, it compiles and fits the pipeline using the provided features and target, while ignoring warnings.\n    If verbosity is set to 1 or 2, it prints the best pipeline string, adding extra spacing if verbosity is 2 or higher.\n    The function also compiles and fits the entire Pareto front of pipelines, storing them as fitted models for convenience.\n    \"\"\"\n    if not hasattr(self, 'fitted_pipeline_'):\n        raise RuntimeError(\"No optimized pipeline is available. Please run fit() first.\")\n    \n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        self.fitted_pipeline_.fit(features, target)\n    \n    if self.verbosity in [1, 2]:\n        print(\"Best Pipeline:\")\n        print(self.fitted_pipeline_)\n        if self.verbosity >= 2:\n            print(\"\n\")\n    \n    # Assuming 'pareto_front_fitted_pipelines_' is a list of pipelines\n    self.pareto_front_fitted_pipelines_ = []\n    for pipeline in self.pareto_front_:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            pipeline.fit(features, target)\n        self.pareto_front_fitted_pipelines_.append(pipeline)", "CodeBleu_score": 32.84122046302864}, {"code": "import warnings\n\ndef _summary_of_best_pipeline(self, features, target):\n    \"\"\"\n    This function provides a summary of the best pipeline identified by TPOT optimization.\n    It first checks if an optimized pipeline exists, raising a RuntimeError with a descriptive message if not.\n    If an optimized pipeline is available, it compiles and fits the pipeline using the provided features and target, while ignoring warnings.\n    If verbosity is set to 1 or 2, it prints the best pipeline string, adding extra spacing if verbosity is 2 or higher.\n    The function also compiles and fits the entire Pareto front of pipelines, storing them as fitted models for convenience.\n    \"\"\"\n    if not hasattr(self, 'fitted_pipeline_'):\n        raise RuntimeError(\"No optimized pipeline exists. Please run the optimization process first.\")\n    \n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        self.fitted_pipeline_.fit(features, target)\n    \n    if self.verbosity in [1, 2]:\n        print(\"Best Pipeline:\")\n        print(self.fitted_pipeline_)\n        if self.verbosity >= 2:\n            print(\"\n\")\n    \n    # Assuming there's a method to get the Pareto front pipelines\n    pareto_front_pipelines = self.get_pareto_front_pipelines()\n    self.fitted_models_ = []\n    for pipeline in pareto_front_pipelines:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            pipeline.fit(features, target)\n            self.fitted_models_.append(pipeline)", "CodeBleu_score": 34.66791656699248}]}
{"instance_id": "638683d78167", "generation_result": [{"code": "def score(self, testing_features, testing_target):\n    \"\"\"\n    This function evaluates the performance of a fitted pipeline on testing data.\n    It first checks if the pipeline has been optimized and raises an error if not.\n    The testing features and target are validated using a helper method.\n    Depending on the type of scoring function provided, it either retrieves a scorer from the predefined SCORERS dictionary or uses the provided callable scorer.\n    The function then calculates and returns the score by applying the scorer to the fitted pipeline, testing features, and testing target.\n    \"\"\"\n    # Check if the pipeline has been optimized\n    if not self.is_optimized:\n        raise ValueError(\"The pipeline has not been optimized yet.\")\n    \n    # Validate testing features and target\n    self.validate_data(testing_features, testing_target)\n    \n    # Determine the scoring function\n    if isinstance(self.scorer, str):\n        if self.scorer in SCORERS:\n            scorer = SCORERS[self.scorer]\n        else:\n            raise ValueError(f\"Scorer '{self.scorer}' not found in predefined SCORERS dictionary.\")\n    elif callable(self.scorer):\n        scorer = self.scorer\n    else:\n        raise TypeError(\"Scorer must be either a string key to SCORERS dictionary or a callable.\")\n    \n    # Calculate and return the score\n    score = scorer(self.pipeline, testing_features, testing_target)\n    return score", "CodeBleu_score": 42.76627988062276}, {"code": "def score(self, testing_features, testing_target):\n    \"\"\"\n    This function evaluates the performance of a fitted pipeline on testing data.\n    It first checks if the pipeline has been optimized and raises an error if not.\n    The testing features and target are validated using a helper method.\n    Depending on the type of scoring function provided, it either retrieves a scorer from the predefined SCORERS dictionary or uses the provided callable scorer.\n    The function then calculates and returns the score by applying the scorer to the fitted pipeline, testing features, and testing target.\n    \"\"\"\n    # Check if the pipeline has been optimized\n    if not self.is_optimized:\n        raise ValueError(\"The pipeline has not been optimized yet.\")\n    \n    # Validate testing features and target\n    self.validate_data(testing_features, testing_target)\n    \n    # Determine the scoring function\n    if isinstance(self.scorer, str):\n        if self.scorer in SCORERS:\n            scorer = SCORERS[self.scorer]\n        else:\n            raise ValueError(f\"Scorer '{self.scorer}' not recognized.\")\n    elif callable(self.scorer):\n        scorer = self.scorer\n    else:\n        raise ValueError(\"Invalid scorer type provided.\")\n    \n    # Calculate and return the score\n    score = scorer(self.pipeline, testing_features, testing_target)\n    return score", "CodeBleu_score": 41.36427981635752}, {"code": "def score(self, testing_features, testing_target):\n    # Check if the pipeline has been optimized\n    if not self.optimized:\n        raise ValueError(\"The pipeline has not been optimized yet.\")\n    \n    # Validate testing features and target\n    self.validate_features_and_target(testing_features, testing_target)\n    \n    # Retrieve or use the provided scorer\n    if isinstance(self.scorer, str):\n        if self.scorer in SCORERS:\n            scorer = SCORERS[self.scorer]\n        else:\n            raise ValueError(f\"Scorer '{self.scorer}' not found in predefined SCORERS dictionary.\")\n    elif callable(self.scorer):\n        scorer = self.scorer\n    else:\n        raise TypeError(\"Provided scorer is neither a string key nor a callable.\")\n    \n    # Calculate and return the score\n    score = scorer(self.pipeline, testing_features, testing_target)\n    return score", "CodeBleu_score": 42.88977893977266}]}
{"instance_id": "a04382560333", "generation_result": [{"code": "from sklearn.impute import SimpleImputer\n\nclass DataImputer:\n    def __init__(self, verbosity=0):\n        self.verbosity = verbosity\n        self.imputer = None\n\n    def _impute_values(self, features):\n        \"\"\"\n        This function imputes missing values in a feature set.\n        If verbosity is greater than 1, it prints a message indicating the imputation process.\n        If the imputer has not been fitted yet, it creates a SimpleImputer with the \"median\" strategy and fits it to the features.\n        The function then transforms the features using the fitted imputer and returns the imputed feature set.\n        \"\"\"\n        if self.imputer is None:\n            self.imputer = SimpleImputer(strategy='median')\n            self.imputer.fit(features)\n\n        if self.verbosity > 1:\n            print(\"Imputing missing values using the median strategy.\")\n\n        imputed_features = self.imputer.transform(features)\n        return imputed_features", "CodeBleu_score": 53.527203347541196}, {"code": "from sklearn.impute import SimpleImputer\n\nclass DataPreprocessor:\n    def __init__(self, verbosity=0):\n        self.verbosity = verbosity\n        self.imputer = None\n\n    def _impute_values(self, features):\n        \"\"\"\n        This function imputes missing values in a feature set.\n        If verbosity is greater than 1, it prints a message indicating the imputation process.\n        If the imputer has not been fitted yet, it creates a SimpleImputer with the \"median\" strategy and fits it to the features.\n        The function then transforms the features using the fitted imputer and returns the imputed feature set.\n        \"\"\"\n        if self.verbosity > 1:\n            print(\"Starting the imputation process.\")\n\n        if self.imputer is None:\n            self.imputer = SimpleImputer(strategy='median')\n            self.imputer.fit(features)\n\n        imputed_features = self.imputer.transform(features)\n        return imputed_features", "CodeBleu_score": 48.5188470808959}, {"code": "from sklearn.impute import SimpleImputer\n\nclass DataPreprocessor:\n    def __init__(self, verbosity=0):\n        self.verbosity = verbosity\n        self.imputer = None\n\n    def _impute_values(self, features):\n        \"\"\"\n        Imputes missing values in a feature set.\n        If verbosity is greater than 1, it prints a message indicating the imputation process.\n        If the imputer has not been fitted yet, it creates a SimpleImputer with the \"median\" strategy and fits it to the features.\n        The function then transforms the features using the fitted imputer and returns the imputed feature set.\n        \n        Args:\n            features (DataFrame): The feature set with potential missing values.\n        \n        Returns:\n            DataFrame: The feature set with imputed values.\n        \"\"\"\n        if self.verbosity > 1:\n            print(\"Starting the imputation process.\")\n\n        if self.imputer is None:\n            self.imputer = SimpleImputer(strategy='median')\n            self.imputer.fit(features)\n\n        imputed_features = self.imputer.transform(features)\n        \n        if self.verbosity > 1:\n            print(\"Imputation completed.\")\n\n        return imputed_features", "CodeBleu_score": 47.120861971361606}]}
{"instance_id": "0043d411c332", "generation_result": [{"code": "from sklearn.pipeline import Pipeline\nimport numpy as np\n\nclass SomeClass:\n    def __init__(self, operators, random_state=None):\n        self.operators = operators\n        self.random_state = random_state\n\n    def _compile_to_sklearn(self, expr):\n        \"\"\"\n        Converts an expression into a scikit-learn pipeline.\n\n        Parameters:\n            expr (str): The expression to convert.\n\n        Returns:\n            sklearn.pipeline.Pipeline: The configured scikit-learn pipeline.\n        \"\"\"\n        # Generate the pipeline code string from the expression\n        pipeline_code = self._generate_pipeline_code(expr)\n\n        # Evaluate the pipeline code string to create the pipeline object\n        pipeline = eval(pipeline_code, {'__builtins__': None}, self.operators)\n\n        # Set the memory attribute of the pipeline (assuming None for simplicity)\n        pipeline.memory = None\n\n        # Set the random_state for each step in the pipeline if specified\n        if self.random_state is not None:\n            self._set_random_state(pipeline)\n\n        return pipeline\n\n    def _generate_pipeline_code(self, expr):\n        # This is a placeholder for the actual implementation\n        # It should parse the expression and map it to the corresponding sklearn components\n        return \"Pipeline(steps=[('step1', self.operators['operator1']()), ('step2', self.operators['operator2']())])\"\n\n    def _set_random_state(self, pipeline):\n        for name, step in pipeline.steps:\n            if hasattr(step, 'random_state'):\n                step.random_state = self.random_state\n\n# Example usage:\noperators = {\n    'operator1': lambda: np.random.RandomState(42),  # Example operator\n    'operator2': lambda: np.random.RandomState(123)  # Example operator\n}\nsome_class_instance = SomeClass(operators, random_state=42)\nexpression = \"operator1 | operator2\"\npipeline = some_class_instance._compile_to_sklearn(expression)", "CodeBleu_score": 33.32308381907666}, {"code": "from sklearn.pipeline import Pipeline\nimport ast\nimport inspect\n\nclass ModelCompiler:\n    def __init__(self, operators, memory=None, random_state=None):\n        self.operators = operators\n        self.memory = memory\n        self.random_state = random_state\n\n    def _compile_to_sklearn(self, expr):\n        \"\"\"\n        Converts an expression into a scikit-learn pipeline.\n        \n        Parameters:\n            expr (str): A string expression representing the model configuration.\n        \n        Returns:\n            sklearn.pipeline.Pipeline: A configured scikit-learn pipeline.\n        \"\"\"\n        # Generate the pipeline code string from the expression\n        pipeline_code = self._generate_pipeline_code(expr)\n        \n        # Evaluate the pipeline code in the context of predefined operators\n        pipeline = eval(pipeline_code, {'__builtins__': None}, self.operators)\n        \n        # Set the memory attribute of the pipeline\n        pipeline.memory = self.memory\n        \n        # Recursively set the random_state for each step in the pipeline, if specified\n        if self.random_state is not None:\n            self._set_random_state(pipeline)\n        \n        return pipeline\n\n    def _generate_pipeline_code(self, expr):\n        \"\"\"\n        Generates a Python code string for a scikit-learn pipeline based on an expression.\n        \n        Parameters:\n            expr (str): The expression to convert.\n        \n        Returns:\n            str: The Python code as a string for creating a pipeline.\n        \"\"\"\n        # This is a placeholder for the actual implementation\n        return \"Pipeline(steps=[('step1', self.operators['operator1']()), ('step2', self.operators['operator2']())])\"\n\n    def _set_random_state(self, pipeline):\n        \"\"\"\n        Recursively sets the random_state for each step in the pipeline.\n        \n        Parameters:\n            pipeline (sklearn.pipeline.Pipeline): The pipeline to modify.\n        \"\"\"\n        for name, estimator in pipeline.steps:\n            if 'random_state' in inspect.signature(estimator.__class__.__init__).parameters:\n                estimator.set_params(random_state=self.random_state)\n\n# Example usage:\noperators = {\n    'operator1': SomeSklearnEstimatorClass1,\n    'operator2': SomeSklearnEstimatorClass2\n}\ncompiler = ModelCompiler(operators, memory='/path/to/cache', random_state=42)\nexpr = \"some expression defining the model structure\"\npipeline = compiler._compile_to_sklearn(expr)", "CodeBleu_score": 33.81903560258498}, {"code": "from sklearn.pipeline import Pipeline\n\ndef _compile_to_sklearn(self, expr):\n    \"\"\"\n    Converts an expression into a scikit-learn pipeline.\n\n    Parameters:\n    - expr: The expression to convert into a pipeline.\n\n    Returns:\n    - A scikit-learn Pipeline object configured according to the expression.\n    \"\"\"\n    # Generate the pipeline code string from the expression and predefined operators\n    pipeline_code = self._generate_pipeline_code(expr)\n    \n    # Define the context in which the pipeline code will be evaluated\n    context = {'Pipeline': Pipeline, **self._get_sklearn_operators()}\n    \n    # Evaluate the pipeline code string within the defined context\n    pipeline = eval(pipeline_code, context)\n    \n    # Set the memory attribute of the pipeline\n    pipeline.memory = self.pipeline_memory\n    \n    # If a random state is specified, set it recursively for each step\n    if hasattr(self, 'random_state'):\n        self._set_random_state(pipeline, self.random_state)\n    \n    # Return the configured scikit-learn pipeline\n    return pipeline\n\ndef _generate_pipeline_code(self, expr):\n    # This method should generate the pipeline code from the expression\n    # Placeholder implementation\n    return \"Pipeline(steps=[('step1', SomeTransformer()), ('step2', SomeEstimator())])\"\n\ndef _get_sklearn_operators(self):\n    # This method should return a dictionary of available scikit-learn operators\n    # Placeholder implementation\n    return {'SomeTransformer': SomeTransformer, 'SomeEstimator': SomeEstimator}\n\ndef _set_random_state(self, pipeline, random_state):\n    # Recursively set the random_state for each step in the pipeline\n    for name, estimator in pipeline.steps:\n        if hasattr(estimator, 'random_state'):\n            estimator.random_state = random_state", "CodeBleu_score": 34.63102352386971}]}
{"instance_id": "ca0ce462aa76", "generation_result": [{"code": "import random\n\ndef pick_two_individuals_eligible_for_crossover(population):\n    \"\"\"\n    The function pick_two_individuals_eligible_for_crossover(population) selects two individuals from a given population\n    that are eligible for crossover. It first creates sets of primitive nodes for each individual and converts each individual\n    to a string representation. It then identifies pairs of individuals that share at least one primitive and are not identical.\n    Eligible pairs are considered in both possible orders. If no eligible pairs are found, the function returns None for both\n    individuals. Otherwise, it randomly selects one of the eligible pairs and returns the corresponding individuals from the population.\n    \"\"\"\n    eligible_pairs = []\n\n    # Convert each individual to a set of primitives and a string representation\n    individuals_info = [(set(individual), str(individual)) for individual in population]\n\n    # Compare each pair of individuals\n    for i in range(len(individuals_info)):\n        for j in range(i + 1, len(individuals_info)):\n            primitives_i, string_i = individuals_info[i]\n            primitives_j, string_j = individuals_info[j]\n\n            # Check if they share at least one primitive and are not identical\n            if primitives_i & primitives_j and string_i != string_j:\n                eligible_pairs.append((population[i], population[j]))\n                eligible_pairs.append((population[j], population[i]))\n\n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n\n    # Randomly select one of the eligible pairs\n    return random.choice(eligible_pairs)", "CodeBleu_score": 31.436637763997627}, {"code": "import random\n\ndef pick_two_individuals_eligible_for_crossover(population):\n    \"\"\"\n    The function pick_two_individuals_eligible_for_crossover(population) selects two individuals from a given population\n    that are eligible for crossover. It first creates sets of primitive nodes for each individual and converts each individual\n    to a string representation. It then identifies pairs of individuals that share at least one primitive and are not identical.\n    Eligible pairs are considered in both possible orders. If no eligible pairs are found, the function returns None for both\n    individuals. Otherwise, it randomly selects one of the eligible pairs and returns the corresponding individuals from the population.\n    \"\"\"\n    # Create a list to hold tuples of (individual, set of primitives)\n    individual_primitives = []\n    \n    # Convert each individual to a set of primitives and store with the individual\n    for individual in population:\n        primitives = set(str(individual))  # Assuming individual can be converted to a string and split into primitives\n        individual_primitives.append((individual, primitives))\n    \n    # Find all eligible pairs\n    eligible_pairs = []\n    for i in range(len(individual_primitives)):\n        for j in range(i + 1, len(individual_primitives)):\n            individual1, primitives1 = individual_primitives[i]\n            individual2, primitives2 = individual_primitives[j]\n            # Check if they share at least one primitive and are not identical\n            if primitives1 & primitives2 and individual1 != individual2:\n                eligible_pairs.append((individual1, individual2))\n                eligible_pairs.append((individual2, individual1))\n    \n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n    \n    # Randomly select one of the eligible pairs\n    selected_pair = random.choice(eligible_pairs)\n    return selected_pair", "CodeBleu_score": 31.574213070564873}, {"code": "import random\n\ndef pick_two_individuals_eligible_for_crossover(population):\n    \"\"\"\n    Selects two individuals from a given population that are eligible for crossover.\n    It first creates sets of primitive nodes for each individual and converts each individual\n    to a string representation. It then identifies pairs of individuals that share at least\n    one primitive and are not identical. Eligible pairs are considered in both possible orders.\n    If no eligible pairs are found, the function returns None for both individuals.\n    Otherwise, it randomly selects one of the eligible pairs and returns the corresponding\n    individuals from the population.\n    \"\"\"\n    eligible_pairs = []\n    \n    # Convert each individual to a set of primitives and a string representation\n    individuals_info = [(set(individual), str(individual)) for individual in population]\n    \n    # Compare each pair of individuals\n    for i in range(len(individuals_info)):\n        for j in range(i + 1, len(individuals_info)):\n            primitives_i, string_i = individuals_info[i]\n            primitives_j, string_j = individuals_info[j]\n            \n            # Check if they share at least one primitive and are not identical\n            if primitives_i & primitives_j and string_i != string_j:\n                eligible_pairs.append((population[i], population[j]))\n                eligible_pairs.append((population[j], population[i]))\n    \n    # If no eligible pairs are found, return None for both individuals\n    if not eligible_pairs:\n        return None, None\n    \n    # Randomly select one of the eligible pairs\n    return random.choice(eligible_pairs)", "CodeBleu_score": 31.436637763997627}]}
{"instance_id": "4b03c843f506", "generation_result": [{"code": "def generate_export_pipeline_code(pipeline_tree, operators):\n    \"\"\"\n    Generates the Python code for a scikit-learn pipeline from a given pipeline tree and set of operators.\n    It processes the operators to create the steps of the pipeline and determines the number of steps.\n    If the pipeline has more than one step, it formats the steps into a make_pipeline call with indentation.\n    If there is only one step, it formats it directly without make_pipeline.\n    The resulting code as a string is then returned.\n    \"\"\"\n    # Assuming `pipeline_tree` is a list of tuples (step_name, operator_key)\n    # and `operators` is a dictionary mapping operator_key to operator class name.\n    \n    steps = []\n    for step_name, operator_key in pipeline_tree:\n        operator_class = operators.get(operator_key, None)\n        if operator_class:\n            steps.append(f\"('{step_name}', {operator_class}())\")\n    \n    if len(steps) > 1:\n        # Join steps with proper formatting and indentation for make_pipeline\n        steps_code = \",\n    \".join(steps)\n        return f\"make_pipeline(\n    {steps_code}\n)\"\n    elif steps:\n        # Single step, no need for make_pipeline\n        return steps[0]\n    else:\n        return \"# No steps available\"", "CodeBleu_score": 19.58747580874726}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators):\n    \"\"\"\n    Generates the Python code for a scikit-learn pipeline from a given pipeline tree and set of operators.\n    \n    Args:\n    pipeline_tree (dict): A dictionary representing the structure of the pipeline.\n    operators (dict): A dictionary mapping operator names to their corresponding scikit-learn implementation.\n    \n    Returns:\n    str: The generated Python code for the pipeline.\n    \"\"\"\n    # Initialize the list to hold the pipeline steps\n    pipeline_steps = []\n    \n    # Process each node in the pipeline tree\n    for node, op_name in pipeline_tree.items():\n        if op_name in operators:\n            # Append the operator as a tuple (name, operator)\n            pipeline_steps.append((node, operators[op_name]))\n    \n    # Generate the pipeline code\n    if len(pipeline_steps) > 1:\n        # Multiple steps: use make_pipeline with proper formatting\n        steps_code = \",\n    \".join(f\"{step[0]}={step[1]}\" for step in pipeline_steps)\n        return f\"make_pipeline(\n    {steps_code}\n)\"\n    elif pipeline_steps:\n        # Single step: no need for make_pipeline\n        return f\"{pipeline_steps[0][1]}()\"\n    else:\n        # No steps provided\n        return \"# No operators provided for the pipeline\"", "CodeBleu_score": 18.791879335938738}, {"code": "def generate_export_pipeline_code(pipeline_tree, operators):\n    \"\"\"\n    Generates the Python code for a scikit-learn pipeline from a given pipeline tree and set of operators.\n    It processes the operators to create the steps of the pipeline and determines the number of steps.\n    If the pipeline has more than one step, it formats the steps into a make_pipeline call with indentation.\n    If there is only one step, it formats it directly without make_pipeline.\n    The resulting code as a string is then returned.\n    \"\"\"\n    # Assuming `pipeline_tree` is a list of tuples (step_name, operator_key)\n    # and `operators` is a dictionary mapping operator_key to operator class name.\n    \n    steps = []\n    for step_name, operator_key in pipeline_tree:\n        operator_class = operators.get(operator_key, None)\n        if operator_class:\n            steps.append(f\"('{step_name}', {operator_class}())\")\n    \n    if len(steps) > 1:\n        # Join steps with commas and newlines for readability\n        steps_code = \",\n    \".join(steps)\n        return f\"make_pipeline(\n    {steps_code}\n)\"\n    elif steps:\n        # Single step, no need for make_pipeline\n        return steps[0]\n    else:\n        return \"# No valid steps provided\"", "CodeBleu_score": 19.45648394098809}]}
{"instance_id": "e9aec2ee39a4", "generation_result": [{"code": "def _process_operator(operator, operators, depth=0):\n    \"\"\"\n    Recursively processes a given operator to generate a sequence of steps for a pipeline.\n    \n    Parameters:\n        operator (str): The current operator to process.\n        operators (dict): A dictionary of all operators with their details.\n        depth (int): The current depth of recursion, used to determine if a StackingEstimator is needed.\n    \n    Returns:\n        list: A list of steps representing the processed pipeline.\n    \"\"\"\n    steps = []\n    operator_details = operators.get(operator, {})\n    \n    # Check if the operator is \"CombineDFs\"\n    if operator == \"CombineDFs\":\n        # Assuming 'combine_dataframes' is a function that handles the combining logic\n        result = combine_dataframes(operator_details['inputs'])\n        steps.append(result)\n    else:\n        # Retrieve the corresponding TPOT operator\n        tpot_operator = get_tpot_operator(operator)\n        if tpot_operator:\n            # Process input operators recursively if they are not the initial input matrix\n            for input_op in operator_details.get('inputs', []):\n                if input_op in operators:\n                    steps.extend(_process_operator(input_op, operators, depth + 1))\n            \n            # If the operator is a root operator and depth > 0, wrap in a StackingEstimator\n            if depth > 0 and operator_details.get('is_root', False):\n                steps.append(StackingEstimator(tpot_operator))\n            else:\n                steps.append(tpot_operator)\n    \n    return steps", "CodeBleu_score": 42.23311105770624}, {"code": "def _process_operator(operator, operators, depth=0):\n    \"\"\"\n    Recursively processes a given operator to generate a sequence of steps for a pipeline.\n    \n    Parameters:\n        operator (str): The current operator to process.\n        operators (dict): A dictionary of all operators with their details.\n        depth (int): The current depth of recursion, used to determine if an operator should be wrapped in a StackingEstimator.\n    \n    Returns:\n        list: A list of steps representing the processed pipeline.\n    \"\"\"\n    steps = []\n    operator_details = operators.get(operator, {})\n    \n    # Check if the operator is \"CombineDFs\"\n    if operator == \"CombineDFs\":\n        # Assuming 'combine_dataframes' is a function that handles the combining logic\n        result = combine_dataframes(operator_details['inputs'])\n        steps.append(('CombineDFs', result))\n    else:\n        # Retrieve the corresponding TPOT operator\n        tpot_operator = get_tpot_operator(operator)\n        \n        # Process input operators if not the initial input matrix\n        if 'inputs' in operator_details:\n            for input_operator in operator_details['inputs']:\n                if input_operator != 'initial_input_matrix':\n                    steps.extend(_process_operator(input_operator, operators, depth + 1))\n        \n        # If the operator is a root operator and depth > 0, wrap in a StackingEstimator\n        if depth > 0 and operator_details.get('is_root', False):\n            steps.append(('StackingEstimator', tpot_operator))\n        else:\n            steps.append((operator, tpot_operator))\n    \n    return steps", "CodeBleu_score": 42.11812777066838}, {"code": "def _process_operator(operator, operators, depth=0):\n    \"\"\"\n    Recursively processes a given operator to generate a sequence of steps for a pipeline.\n    \n    Args:\n    operator (str): The current operator to process.\n    operators (dict): A dictionary of all operators with their corresponding functions.\n    depth (int): The current depth of recursion, default is 0.\n    \n    Returns:\n    list: A list of steps generated for the pipeline.\n    \"\"\"\n    steps = []\n    if operator == \"CombineDFs\":\n        # Assuming 'combine_dfs' is a function that handles the combining of dataframes\n        result = combine_dfs()  # This function needs to be defined elsewhere\n        steps.append(result)\n    else:\n        # Retrieve the corresponding TPOT operator\n        if operator in operators:\n            current_operator = operators[operator]\n            # Check if the current operator has any input operators that need to be processed\n            if 'input_ops' in current_operator:\n                for input_op in current_operator['input_ops']:\n                    steps += _process_operator(input_op, operators, depth + 1)\n            # If it's a root operator and depth > 0, wrap in a StackingEstimator\n            if depth > 0 and 'is_root' in current_operator and current_operator['is_root']:\n                steps.append(StackingEstimator(current_operator))\n            else:\n                steps.append(current_operator)\n    \n    return steps", "CodeBleu_score": 37.07919487638366}]}
{"instance_id": "0513bf969510", "generation_result": [{"code": "import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef transform(self, X):\n    # Assuming `X` is a pandas DataFrame\n    # Identify categorical features based on a threshold (e.g., unique values count)\n    threshold = 10  # This threshold can be adjusted\n    categorical_features = [col for col in X.columns if X[col].nunique() <= threshold and X[col].dtype == 'object']\n\n    # Check if any categorical features are found\n    if not categorical_features:\n        raise ValueError(\"No categorical features found based on the threshold.\")\n\n    # Apply one-hot encoding to the categorical features\n    encoder = OneHotEncoder(sparse=False)\n    encoded_features = encoder.fit_transform(X[categorical_features])\n\n    # Create a DataFrame from the encoded features, with appropriate column names\n    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n\n    # Drop the original categorical columns and concatenate the new encoded features\n    X_transformed = pd.concat([X.drop(columns=categorical_features), encoded_df], axis=1)\n\n    return X_transformed", "CodeBleu_score": 29.36365959268729}, {"code": "import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef transform(self, X):\n    # Assuming `X` is a pandas DataFrame\n    # Identify categorical features based on a threshold for the number of unique values\n    threshold = 10  # This threshold can be adjusted\n    categorical_features = [col for col in X.columns if X[col].nunique() <= threshold and X[col].dtype == 'object']\n\n    # Check if there are any categorical features\n    if not categorical_features:\n        raise ValueError(\"No categorical features found based on the threshold.\")\n\n    # Apply one-hot encoding\n    encoder = OneHotEncoder(sparse=False)\n    encoded_features = encoder.fit_transform(X[categorical_features])\n\n    # Create a DataFrame from the encoded features, with appropriate column names\n    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names(categorical_features))\n\n    # Drop original categorical columns and concatenate the new one-hot encoded columns\n    X_transformed = X.drop(columns=categorical_features).join(encoded_df)\n\n    return X_transformed", "CodeBleu_score": 30.200343272306025}, {"code": "import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef transform(self, X, threshold=10):\n    \"\"\"This function selects categorical features from the input data `X` based on a threshold.\n    If no categorical features are found, it raises a `ValueError`.\n    If categorical features are selected, it applies one-hot encoding to these features and returns the transformed data.\n    \n    Parameters:\n    X (pd.DataFrame): The input data containing various features.\n    threshold (int): The maximum number of unique values a feature can have to be considered categorical.\n    \n    Returns:\n    pd.DataFrame: The transformed data with one-hot encoded categorical features.\n    \"\"\"\n    # Identify categorical features based on the threshold\n    categorical_features = [col for col in X.columns if X[col].nunique() <= threshold and X[col].dtype == 'object']\n    \n    # Check if there are any categorical features\n    if not categorical_features:\n        raise ValueError(\"No categorical features found based on the provided threshold.\")\n    \n    # Apply one-hot encoding\n    encoder = OneHotEncoder(sparse=False)\n    encoded_features = encoder.fit_transform(X[categorical_features])\n    \n    # Create a DataFrame from the encoded features, with appropriate column names\n    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names(categorical_features))\n    \n    # Drop the original categorical columns and concatenate the new one-hot encoded columns\n    X_transformed = X.drop(columns=categorical_features).join(encoded_df)\n    \n    return X_transformed", "CodeBleu_score": 31.0507468202095}]}
{"instance_id": "5b1bcf42ca9e", "generation_result": [{"code": "import numpy as np\n\ndef balanced_accuracy(y_true, y_pred):\n    \"\"\"\n    Computes the balanced accuracy for a set of true and predicted labels.\n    \n    Parameters:\n        y_true (list or numpy.ndarray): True class labels.\n        y_pred (list or numpy.ndarray): Predicted class labels.\n    \n    Returns:\n        float: The balanced accuracy.\n    \"\"\"\n    \n    # Convert y_true and y_pred to numpy arrays if they aren't already\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Identify all unique classes in the true labels\n    classes = np.unique(y_true)\n    \n    # Initialize lists to hold sensitivity and specificity for each class\n    class_scores = []\n    \n    # Loop over each class to calculate sensitivity and specificity for each class\n    for cls in classes:\n        # True positives (TP): correct predictions that this class occurs\n        TP = np.sum((y_pred == cls) & (y_true == cls))\n        \n        # False negatives (FN): incorrect predictions of class as some other class\n        FN = np.sum((y_pred != cls) & (y_true == cls))\n\n        # True negatives (TN): correct predictions that this class does not occur\n        TN = np.sum((y_pred != cls) & (y_true != cls))\n\n        # False positives (FP): incorrect predictions that this class occurs\n        FP = np.sum((y_pred == cls) & (y_true != cls))\n        \n        # Compute sensitivity (recall or true positive rate)\n        sensitivity = TP / (TP + FN) if TP + FN != 0 else 0\n        \n        # Compute specificity (true negative rate)\n        specificity = TN / (TN + FP) if TN + FP != 0 else 1\n\n        # Calculate class accuracy as the mean of sensitivity and specificity\n        class_accuracy = (sensitivity + specificity) / 2\n        class_scores.append(class_accuracy)\n    \n    # Return the mean of the class accuracies\n    return np.mean(class_scores)", "CodeBleu_score": 27.637201050714253}, {"code": "def balanced_accuracy(y_true, y_pred)\nimport numpy as np\n\ndef balanced_accuracy(y_true, y_pred):\n    \"\"\"\n    Computes the balanced accuracy for a set of true and predicted labels.\n    \n    Parameters:\n        y_true (list or np.array): True labels/classes for the dataset.\n        y_pred (list or np.array): Predicted labels/classes as output by a classifier.\n        \n    Returns:\n        float: The balanced accuracy score.\n    \"\"\"\n    # Convert inputs to numpy arrays if not already\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Get the unique classes from true labels\n    classes = np.unique(y_true)\n    \n    sensitivities = []\n    specificities = []\n    \n    # Iterate over each class to compute sensitivity and specificity\n    for cls in classes:\n        # Sensitivity calculation: TP / (TP + FN)\n        TP = np.sum((y_pred == cls) & (y_true == cls))\n        FN = np.sum((y_pred != cls) & (y_true == cls))\n        if TP + FN > 0:\n            sensitivity = TP / (TP + FN)\n        else:\n            sensitivity = 0\n        \n        # Specificity calculation: TN / (TN + FP)\n        TN = np.sum((y_pred != cls) & (y_true != cls))\n        FP = np.sum((y_pred == cls) & (y_true != cls))\n        if TN + FP > 0:\n            specificity = TN / (TN + FP)\n        else:\n            specificity = 1  # Default to 1, as mentioned in the docstring\n        \n        sensitivities.append(sensitivity)\n        specificities.append(specificity)\n    \n    # Average sensitivity and specificity for each class\n    class_accuracies = [(sensitivity + specificity) / 2 for sensitivity, specificity in zip(sensitivities, specificities)]\n    \n    # Balanced accuracy is the mean of class accuracies\n    balanced_acc = np.mean(class_accuracies)\n    \n    return balanced_acc\n\n# Example usage:\n# y_true = [0, 1, 0, 1, 0, 0, 1, 1]\n# y_pred = [0, 1, 1, 0, 0, 1, 1, 0]\n# print(\"Balanced Accuracy:\", balanced_accuracy(y_true, y_pred))\n# This should print a balanced accuracy measuring how well the model has performed across the given classes", "CodeBleu_score": 33.88232747960282}, {"code": "def balanced_accuracy(y_true, y_pred):\n    \"\"\"\n    Computes the balanced accuracy for given true and predicted labels.\n\n    Args:\n    y_true (list of int): True labels.\n    y_pred (list of int): Predicted labels.\n\n    Returns:\n    float: The balanced accuracy of the predictions.\n    \"\"\"\n    from sklearn.metrics import confusion_matrix\n    from sklearn.utils.multiclass import unique_labels\n    \n    # First, get the unique labels in the data\n    labels = unique_labels(y_true, y_pred)\n    \n    # Initialize accumulators for sensitivity and specificity\n    sensitivity = []\n    specificity = []\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n\n    # Calculate sensitivity and specificity for each class\n    for i, label in enumerate(labels):\n        # True positives: diagonal element of confusion matrix at position (i, i)\n        TP = cm[i, i]\n        # False negatives: sum of elements in row i, excluding TP\n        FN = sum(cm[i, :]) - TP\n        # False positives: sum of elements in column i, excluding TP\n        FP = sum(cm[:, i]) - TP\n        # True negatives: sum of all elements in matrix excluding those for class i\n        TN = sum(cm.flatten()) - (TP + FN + FP)\n        \n        if (TP + FN) != 0:\n            sens = TP / (TP + FN)\n        else:\n            sens = 0\n        \n        if (TN + FP) != 0:\n            spec = TN / (TN + FP)\n        else:\n            spec = 1  # As described in the docstring, specifically set to 1 if only one class present\n        \n        sensitivity.append(sens)\n        specificity.append(spec)\n\n    # Calculate balanced accuracy as mean of average sensitivity and average specificity\n    balanced_accuracy_score = (sum(sensitivity) + sum(specificity)) / (2 * len(labels))\n    \n    return balanced_accuracy_score", "CodeBleu_score": 26.217253281106302}]}
{"instance_id": "19a9ad39de0a", "generation_result": [{"code": "import numpy as np\n\ndef transform(self, X, y=None):\n    \"\"\"\n    The function \"transform\" processes the input array X by first verifying it with check_array.\n    It then calculates the number of non-zero elements per row and the number of zero elements per row.\n    These two new columns are added to the beginning of the transformed array.\n    The function returns the modified array with the additional columns representing\n    non-zero and zero element counts per row.\n    \n    Parameters:\n        X (array-like): The input feature array.\n        y (array-like, optional): Ignored, present for compatibility with common transform function signatures.\n        \n    Returns:\n        numpy.ndarray: The modified input array X with two new columns added at the beginning.\n    \"\"\"\n    \n    # Ensuring X is a numpy array\n    X = np.array(X) if not isinstance(X, np.ndarray) else X\n\n    # Calculate non-zero counts per row\n    non_zero_counts = np.count_nonzero(X, axis=1)\n\n    # Calculate zero counts per row\n    zero_counts = (X == 0).sum(axis=1)\n\n    # Combine the counts and reshape to be added as columns\n    counts = np.column_stack((non_zero_counts, zero_counts))\n\n    # Concatenate the counts as new columns at the beginning of X\n    transformed_X = np.hstack((counts, X))\n\n    return transformed_X", "CodeBleu_score": 36.544321174431296}, {"code": "from sklearn.utils.validation import check_array\nimport numpy as np\n\ndef transform(self, X, y=None):\n    \"\"\"\n    The function \"transform\" processes the input array X by first verifying it with check_array. It then calculates the \n    number of non-zero elements per row and the number of zero elements per row. These two new columns are added to \n    the beginning of the transformed array. The function returns the modified array with the additional columns \n    representing non-zero and zero element counts per row.\n\n    Parameters:\n        X (array-like): Array-like input data to transform.\n        y (None, optional): Unused parameter, included for compatibility with common sklearn interface.\n\n    Returns:\n        numpy.ndarray: The transformed array with two new columns at the beginning,\n                       one for the count of non-zero elements and one for the zero elements per row.\n    \"\"\"\n    # First validate the input array\n    X = check_array(X)\n    \n    # Calculate the number of non-zero and zero elements per row\n    non_zero_counts = np.count_nonzero(X, axis=1)\n    zero_counts = (X == 0).sum(axis=1)\n    \n    # Combine these counts as columns in a new array\n    additional_columns = np.column_stack((non_zero_counts, zero_counts))\n    \n    # Add these new columns to the beginning of the original array\n    transformed_X = np.hstack((additional_columns, X))\n    \n    return transformed_X", "CodeBleu_score": 39.01048993729038}, {"code": "from sklearn.utils import check_array\nimport numpy as np\n\ndef transform(self, X, y=None):\n    \"\"\"\n    The function \"transform\" processes the input array X by first verifying it with check_array. It then\n    calculates the number of non-zero elements per row and the number of zero elements per row. These two\n    new columns are added to the beginning of the transformed array. The function returns the modified array\n    with the additional columns representing non-zero and zero element counts per row.\n    \n    Parameters:\n        X (array-like): The input feature array to transform.\n        y (array-like, optional): Ignored, present here for compatibility with sklearn's fit_transform method.\n\n    Returns:\n        np.ndarray: The transformed array with two new columns at the beginning, one representing the count of\n                    non-zero elements per row and the other for the count of zero elements per row.\n    \"\"\"\n    \n    # Check input array\n    X = check_array(X)\n    \n    # Calculate number of non-zeros per row\n    non_zeros_per_row = np.count_nonzero(X, axis=1)\n    \n    # Calculate number of zeros per row (total elements minus number of non-zeros)\n    zeros_per_row = X.shape[1] - non_zeros_per_row\n    \n    # Add new columns to the original array\n    transformed_X = np.hstack((non_zeros_per_row[:, np.newaxis], zeros_per_row[:, np.newaxis], X))\n    \n    return transformed_X", "CodeBleu_score": 34.04644439652175}]}
{"instance_id": "5e8db0b5d447", "generation_result": [{"code": "from sklearn.model_selection import train_test_split\nimport numpy as np\n\ndef _init_pretest(self, features, target):\n    \"\"\"\n    Initializes a pretest dataset.\n    It calculates the number of unique target classes and ensures the training size is at least this number.\n    Using train_test_split, it splits the features and target into pretest_X and pretest_y with a specified random state.\n    If all target classes are not included in pretest_y, it adjusts pretest_y to include at least one example from each class by indexing unique target values.\n    \"\"\"\n    # Calculate the number of unique classes\n    unique_classes = np.unique(target)\n    num_classes = len(unique_classes)\n\n    # Ensure the split size is at least the number of unique classes\n    split_size = max(num_classes, int(len(target) * 0.2))  # at least 20% of the data or number of classes\n\n    # Split the data ensuring that we stratify to get at least one of each class in the test set\n    pretest_X, pretest_y = train_test_split(features, target, train_size=split_size, stratify=target, random_state=42)\n\n    # Check if all classes are present in pretest_y, if not, adjust\n    pretest_classes = np.unique(pretest_y)\n    missing_classes = np.setdiff1d(unique_classes, pretest_classes)\n\n    if len(missing_classes) > 0:\n        # Find indices of each missing class and add to pretest_y\n        for cls in missing_classes:\n            cls_index = np.where(target == cls)[0][0]  # Get the index of the first occurrence of the class\n            # Add this example to pretest_y\n            pretest_X = np.vstack([pretest_X, features[cls_index]])\n            pretest_y = np.append(pretest_y, target[cls_index])\n\n    return pretest_X, pretest_y", "CodeBleu_score": 34.03893880528223}, {"code": "from sklearn.model_selection import train_test_split\nimport numpy as np\n\ndef _init_pretest(self, features, target):\n    \"\"\"\n    Initializes a pretest dataset.\n    It calculates the number of unique target classes and ensures the training size is at least this number.\n    Using train_test_split, it splits the features and target into pretest_X and pretest_y with a specified random state.\n    If all target classes are not included in pretest_y, it adjusts pretest_y to include at least one example from each class by indexing unique target values.\n    \"\"\"\n    # Calculate the number of unique classes in the target\n    unique_classes = np.unique(target)\n    num_classes = len(unique_classes)\n\n    # Ensure that the split size is at least the number of unique classes\n    # This is important to ensure that we have at least one example of each class\n    split_size = max(num_classes, int(len(target) * 0.2))  # Using at least 20% of data or number of classes\n\n    # Split the data ensuring that the test size is at least the number of unique classes\n    pretest_X, pretest_y = None, None\n    for random_state in range(100):  # Try different random states to find a valid split\n        pretest_X, _, pretest_y, _ = train_test_split(features, target, test_size=split_size, random_state=random_state, stratify=target)\n        if all(np.isin(unique_classes, np.unique(pretest_y))):\n            break\n\n    # Check if all classes are represented in pretest_y\n    if not all(np.isin(unique_classes, np.unique(pretest_y))):\n        raise ValueError(\"Unable to include at least one example of each target class in the pretest split.\")\n\n    return pretest_X, pretest_y", "CodeBleu_score": 30.13798072806318}, {"code": "from sklearn.model_selection import train_test_split\nimport numpy as np\n\ndef _init_pretest(self, features, target):\n    \"\"\"\n    Initializes a pretest dataset.\n    It calculates the number of unique target classes and ensures the training size is at least this number.\n    Using train_test_split, it splits the features and target into pretest_X and pretest_y with a specified random state.\n    If all target classes are not included in pretest_y, it adjusts pretest_y to include at least one example from each class by indexing unique target values.\n    \"\"\"\n    unique_classes = np.unique(target)\n    num_classes = len(unique_classes)\n\n    # Ensure that the split size is at least the number of unique classes\n    split_size = max(num_classes / len(target), 0.1)  # at least 10% to ensure some minimal data in pretest\n\n    # Split the data\n    pretest_X, _, pretest_y, _ = train_test_split(features, target, test_size=split_size, random_state=42, stratify=target)\n\n    # Check if all classes are represented in pretest_y\n    pretest_classes = np.unique(pretest_y)\n    if set(pretest_classes) != set(unique_classes):\n        # If not all classes are represented, adjust the split\n        missing_classes = set(unique_classes) - set(pretest_classes)\n        for missing_class in missing_classes:\n            # Find indices of the missing class in the original target\n            indices = np.where(target == missing_class)[0]\n            # Add at least one example of the missing class to pretest_y\n            selected_index = np.random.choice(indices)\n            pretest_X = np.vstack([pretest_X, features[selected_index]])\n            pretest_y = np.append(pretest_y, target[selected_index])\n\n    return pretest_X, pretest_y", "CodeBleu_score": 31.52563199950959}]}
{"instance_id": "5e3d48667396", "generation_result": [{"code": "import numpy as np\nimport tensorflow as tf\n\ndef _to_numpy_type(dtype):\n    \"\"\"This function converts the given data type to a NumPy data type.\n    If the input is a TensorFlow data type, it returns the corresponding NumPy data type.\n    If the input is already a NumPy data type, it returns it as is.\"\"\"\n    \n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    elif isinstance(dtype, np.dtype):\n        return dtype\n    else:\n        raise TypeError(\"Input must be a TensorFlow or NumPy data type\")", "CodeBleu_score": 36.186160498644696}, {"code": "import numpy as np\nimport tensorflow as tf\n\ndef _to_numpy_type(dtype):\n    \"\"\"This function converts the given data type to a NumPy data type.\n    If the input is a TensorFlow data type, it returns the corresponding NumPy data type.\n    If the input is already a NumPy data type, it returns it as is.\"\"\"\n    \n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    elif isinstance(dtype, np.dtype):\n        return dtype\n    else:\n        raise TypeError(\"Input must be a TensorFlow or NumPy data type\")", "CodeBleu_score": 36.186160498644696}, {"code": "import numpy as np\nimport tensorflow as tf\n\ndef _to_numpy_type(dtype):\n    \"\"\"This function converts the given data type to a NumPy data type.\n    If the input is a TensorFlow data type, it returns the corresponding NumPy data type.\n    If the input is already a NumPy data type, it returns it as is.\"\"\"\n    \n    if isinstance(dtype, tf.DType):\n        return dtype.as_numpy_dtype\n    elif isinstance(dtype, np.dtype):\n        return dtype\n    else:\n        raise TypeError(\"Input must be a TensorFlow or NumPy data type\")", "CodeBleu_score": 36.186160498644696}]}
{"instance_id": "8f1120fdd463", "generation_result": [{"code": "import numpy as np\nimport tensorflow as tf\n\ndef _dataset_as_numpy(ds, batch_size=None):\n    \"\"\"\n    This function converts a TensorFlow dataset to a NumPy format by batching the dataset and iterating over the batches.\n    If no batching is requested or the required batching function is unavailable, it falls back to a default method.\n    The function ensures compatibility with both TensorFlow 1.X and later versions by using a try-except block to handle the presence of the batching function.\n    \"\"\"\n    try:\n        # Check TensorFlow version for compatibility\n        if tf.__version__.startswith('1.'):\n            # TensorFlow 1.X: Ensure dataset operations are compatible\n            if batch_size is not None:\n                ds = ds.batch(batch_size)\n            iterator = ds.make_one_shot_iterator()\n            next_element = iterator.get_next()\n            with tf.Session() as sess:\n                # Iterate over the dataset and collect batches\n                while True:\n                    try:\n                        data = sess.run(next_element)\n                        for item in data:\n                            yield item\n                    except tf.errors.OutOfRangeError:\n                        break\n        else:\n            # TensorFlow 2.X and later: Simplified approach\n            if batch_size is not None:\n                ds = ds.batch(batch_size)\n            # Convert dataset to numpy arrays\n            for batch in ds.as_numpy_iterator():\n                yield batch\n    except AttributeError:\n        # Fallback if the batching function is not available\n        for item in ds:\n            yield item.numpy() if hasattr(item, 'numpy') else item\n\n# Example usage:\n# Assuming `dataset` is a TensorFlow dataset object\n# for numpy_batch in _dataset_as_numpy(dataset, batch_size=32):\n#     print(numpy_batch)", "CodeBleu_score": 30.238534689690205}, {"code": "import tensorflow as tf\nimport numpy as np\n\ndef _dataset_as_numpy(ds, batch_size=None):\n    \"\"\"\n    This function converts a TensorFlow dataset to a NumPy format by batching the dataset and iterating over the batches.\n    If no batching is requested or the required batching function is unavailable, it falls back to a default method.\n    The function ensures compatibility with both TensorFlow 1.X and later versions by using a try-except block to handle the presence of the batching function.\n    \"\"\"\n    try:\n        # TensorFlow 2.X compatibility: using tf.compat.v1 to ensure backward compatibility\n        if batch_size is not None:\n            ds = ds.batch(batch_size)\n        \n        # Iterate over the dataset and convert to numpy\n        if tf.__version__.startswith('2.'):\n            return np.array([item.numpy() for item in ds])\n        else:\n            # TensorFlow 1.X compatibility\n            with tf.compat.v1.Session() as sess:\n                # Initialize all variables\n                sess.run(tf.compat.v1.global_variables_initializer())\n                # Create a one-shot iterator\n                iterator = ds.make_one_shot_iterator()\n                next_element = iterator.get_next()\n                # Iterate through the dataset\n                numpy_data = []\n                try:\n                    while True:\n                        numpy_data.append(sess.run(next_element))\n                except tf.errors.OutOfRangeError:\n                    pass\n                return np.array(numpy_data)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None", "CodeBleu_score": 27.996116075812605}, {"code": "import numpy as np\nimport tensorflow as tf\n\ndef _dataset_as_numpy(ds, batch_size=None):\n    \"\"\"\n    This function converts a TensorFlow dataset to a NumPy format by batching the dataset and iterating over the batches.\n    If no batching is requested or the required batching function is unavailable, it falls back to a default method.\n    The function ensures compatibility with both TensorFlow 1.X and later versions by using a try-except block to handle the presence of the batching function.\n    \"\"\"\n    try:\n        # Check TensorFlow version for compatibility\n        if tf.__version__.startswith('1.'):\n            # TensorFlow 1.X: Ensure dataset is in the form of batches if batch_size is specified\n            if batch_size is not None:\n                ds = ds.batch(batch_size)\n        else:\n            # TensorFlow 2.X and later: Use built-in batching if available\n            if batch_size is not None:\n                ds = ds.batch(batch_size)\n        \n        # Iterate over the dataset and convert to NumPy arrays\n        if hasattr(ds, 'as_numpy_iterator'):\n            # TensorFlow 2.X and later: Use as_numpy_iterator if available\n            return np.array(list(ds.as_numpy_iterator()))\n        else:\n            # TensorFlow 1.X or no as_numpy_iterator: Use a manual method to convert to NumPy\n            return np.array([item.numpy() for item in ds])\n    except Exception as e:\n        # Handle exceptions, possibly from incorrect TensorFlow usage or incompatible operations\n        print(f\"An error occurred: {e}\")\n        return None", "CodeBleu_score": 22.572963603010542}]}
{"instance_id": "282271955871", "generation_result": [{"code": "import jax\nimport jax.numpy as jnp\n\ndef get_prng(seed):\n    \"\"\"This function generates a JAX-compatible pseudo-random number generator (PRNG) key from a given seed.\n    It ensures the seed is a scalar and splits it into two 32-bit unsigned integers.\n    These integers are then concatenated to form the PRNG key.\"\"\"\n    assert isinstance(seed, int), \"Seed must be an integer\"\n    seed = jnp.uint32(seed)  # Ensure the seed is a 32-bit unsigned integer\n    key = jax.random.PRNGKey(seed)  # Generate the PRNG key\n    return key", "CodeBleu_score": 19.82569659119019}, {"code": "import jax\nimport jax.numpy as jnp\n\ndef get_prng(seed):\n    \"\"\"This function generates a JAX-compatible pseudo-random number generator (PRNG) key from a given seed.\n    It ensures the seed is a scalar and splits it into two 32-bit unsigned integers.\n    These integers are then concatenated to form the PRNG key.\"\"\"\n    if not isinstance(seed, int):\n        raise ValueError(\"Seed must be an integer\")\n    seed = jnp.uint32(seed)  # Ensure the seed is a 32-bit unsigned integer\n    return jax.random.PRNGKey(seed)", "CodeBleu_score": 20.737067613486442}, {"code": "import jax\nimport jax.numpy as jnp\n\ndef get_prng(seed):\n    \"\"\"This function generates a JAX-compatible pseudo-random number generator (PRNG) key from a given seed.\n    It ensures the seed is a scalar and splits it into two 32-bit unsigned integers.\n    These integers are then concatenated to form the PRNG key.\"\"\"\n    seed = jnp.uint32(seed)  # Ensure the seed is a 32-bit unsigned integer\n    key = jax.random.PRNGKey(seed)  # Generate the PRNG key\n    return key", "CodeBleu_score": 16.777641614673538}]}
{"instance_id": "42234dccce9a", "generation_result": [{"code": "import pandas as pd\n\ndef convert_Timestamp(cls, s, dtype, pandas_type):\n    \"\"\"\n    Converts a given series `s` to a timestamp with a specific timezone based on its `dtype`.\n    \n    Parameters:\n        cls (type): The class on which this method is defined, typically used in class methods.\n        s (pd.Series): The series to convert.\n        dtype (object): The data type of the series, used to determine conversion method.\n        pandas_type (type): Expected pandas type after conversion, for validation or further processing.\n    \n    Returns:\n        pd.Series: The series converted to datetime with appropriate timezone handling.\n    \"\"\"\n    if isinstance(dtype, pd.DatetimeTZDtype):\n        # Convert timezone using tz_convert if dtype is a timezone-aware datetime type\n        return s.dt.tz_convert(dtype.tz)\n    elif dtype == 'datetime64' or dtype == 'datetime64[ns]':\n        # Localize timezone if dtype is a naive datetime type\n        return s.dt.tz_localize('UTC').dt.tz_convert(pandas_type.tz)\n    elif pd.api.types.is_numeric_dtype(s):\n        # Convert numeric to datetime and localize timezone\n        s = pd.to_datetime(s, unit='s')  # Assuming 's' represents seconds since epoch\n        return s.dt.tz_localize('UTC').dt.tz_convert(pandas_type.tz)\n    else:\n        # Convert to datetime with UTC and then localize the timezone\n        s = pd.to_datetime(s)\n        return s.dt.tz_localize('UTC').dt.tz_convert(pandas_type.tz)\n\n# Example usage:\n# Assuming you have a pandas Series `s` and appropriate dtype and pandas_type\n# result = convert_Timestamp(YourClass, s, dtype, pandas_type)", "CodeBleu_score": 49.8234427147356}, {"code": "import pandas as pd\n\ndef convert_Timestamp(cls, s, dtype, pandas_type):\n    \"\"\"\n    The function convert_Timestamp(cls, s, dtype, pandas_type) converts a given series s to a timestamp with a specific timezone based on its dtype.\n    If s is a pd.DatetimeTZDtype, it converts the timezone using tz_convert.\n    If s is a datetime64 type, it localizes the timezone using tz_localize.\n    If s is numeric, it converts it to datetime and localizes the timezone.\n    Otherwise, it converts s to datetime with UTC and then localizes the timezone.\n    \"\"\"\n    if isinstance(dtype, pd.DatetimeTZDtype):\n        # Convert the timezone using tz_convert\n        return s.dt.tz_convert(pandas_type.tz)\n    elif pd.api.types.is_datetime64_any_dtype(s):\n        # Localize the timezone without changing existing timezone information\n        return s.dt.tz_localize(pandas_type.tz, ambiguous='infer')\n    elif pd.api.types.is_numeric_dtype(s):\n        # Convert numeric to datetime and then localize\n        s = pd.to_datetime(s, unit='ns')\n        return s.dt.tz_localize(pandas_type.tz)\n    else:\n        # Convert to datetime with UTC and then localize\n        s = pd.to_datetime(s, utc=True)\n        return s.dt.tz_convert(pandas_type.tz)", "CodeBleu_score": 58.78120762810205}, {"code": "import pandas as pd\n\ndef convert_Timestamp(cls, s, dtype, pandas_type):\n    \"\"\"\n    Converts a given series 's' to a timestamp with a specific timezone based on its dtype.\n    - If 's' is a pd.DatetimeTZDtype, it converts the timezone using tz_convert.\n    - If 's' is a datetime64 type, it localizes the timezone using tz_localize.\n    - If 's' is numeric, it converts it to datetime and localizes the timezone.\n    - Otherwise, it converts 's' to datetime with UTC and then localizes the timezone.\n    \"\"\"\n    if isinstance(dtype, pd.DatetimeTZDtype):\n        # Convert timezone using tz_convert\n        return s.dt.tz_convert(dtype.tz)\n    elif pandas_type == 'datetime64':\n        # Localize the timezone without changing existing timezone information\n        return s.dt.tz_localize(dtype.tz)\n    elif pandas_type == 'numeric':\n        # Convert numeric to datetime and localize the timezone\n        s = pd.to_datetime(s, unit='ms')  # Assuming milliseconds since epoch\n        return s.dt.tz_localize(dtype.tz)\n    else:\n        # Convert to datetime with UTC and then localize the timezone\n        s = pd.to_datetime(s)\n        s = s.dt.tz_localize('UTC')\n        return s.dt.tz_convert(dtype.tz)", "CodeBleu_score": 53.31854540775191}]}
{"instance_id": "9cee7388879b", "generation_result": [{"code": "import dask.dataframe as dd\nimport pandas as pd\n\ndef visit(cls, op: 'ops.BetweenTime', arg, lower_bound, upper_bound):\n    \"\"\"\n    This function processes a time-based operation on a Dask DataFrame.\n    It checks if the data is timezone-aware and converts it to UTC if necessary.\n    It then creates a boolean indexer to identify rows within the specified time range.\n    The result is a Dask array where the identified rows are marked as True, and this array is converted back to a Dask DataFrame for the final output.\n    \"\"\"\n    # Ensure the DataFrame is timezone-aware and convert to UTC if necessary\n    if arg.dt.tz is None:\n        arg = arg.dt.tz_localize('UTC')\n    else:\n        arg = arg.dt.tz_convert('UTC')\n    \n    # Create a boolean indexer for rows within the specified time range\n    mask = (arg >= pd.to_datetime(lower_bound)) & (arg <= pd.to_datetime(upper_bound))\n    \n    # Filter the DataFrame using the mask\n    result = arg[mask]\n    \n    # Convert the boolean mask to a Dask DataFrame\n    result_df = dd.from_array(result, columns=arg.columns)\n    \n    return result_df", "CodeBleu_score": 35.93967785509097}, {"code": "import dask.dataframe as dd\nimport pandas as pd\n\ndef visit(cls, op: 'ops.BetweenTime', arg, lower_bound, upper_bound):\n    \"\"\"\n    This function processes a time-based operation on a Dask DataFrame.\n    It checks if the data is timezone-aware and converts it to UTC if necessary.\n    It then creates a boolean indexer to identify rows within the specified time range.\n    The result is a Dask array where the identified rows are marked as True, and this array is converted back to a Dask DataFrame for the final output.\n    \"\"\"\n    # Ensure the DataFrame is timezone-aware and convert to UTC if necessary\n    if arg.dt.tz:\n        arg = arg.dt.tz_convert('UTC')\n    else:\n        arg = arg.dt.tz_localize('UTC')\n\n    # Create a boolean indexer for rows within the specified time range\n    mask = (arg >= pd.to_datetime(lower_bound)) & (arg <= pd.to_datetime(upper_bound))\n\n    # Use the mask to filter the DataFrame\n    result = arg[mask]\n\n    # Convert the result to a Dask DataFrame if it's not already\n    if not isinstance(result, dd.DataFrame):\n        result = dd.from_pandas(result, npartitions=1)\n\n    return result", "CodeBleu_score": 35.154259516888395}, {"code": "import dask.dataframe as dd\nimport pandas as pd\n\ndef visit(cls, op: 'ops.BetweenTime', arg, lower_bound, upper_bound):\n    \"\"\"\n    This function processes a time-based operation on a Dask DataFrame.\n    It checks if the data is timezone-aware and converts it to UTC if necessary.\n    It then creates a boolean indexer to identify rows within the specified time range.\n    The result is a Dask array where the identified rows are marked as True, and this array is converted back to a Dask DataFrame for the final output.\n    \"\"\"\n    \n    # Ensure the DataFrame is timezone-aware and convert to UTC if necessary\n    if arg.dt.tz is None:\n        arg = arg.dt.tz_localize('UTC')\n    else:\n        arg = arg.dt.tz_convert('UTC')\n    \n    # Create a boolean indexer for rows within the specified time range\n    mask = (arg >= pd.to_datetime(lower_bound)) & (arg <= pd.to_datetime(upper_bound))\n    \n    # Apply the mask to the DataFrame to filter rows\n    result = arg[mask]\n    \n    # Convert the result to a Dask DataFrame if it's not already\n    if not isinstance(result, dd.DataFrame):\n        result = dd.from_pandas(result, npartitions=1)\n    \n    return result", "CodeBleu_score": 35.267975739486324}]}
{"instance_id": "0ab1cc8ccf71", "generation_result": [{"code": "import pandas as pd\nimport dask.dataframe as dd\n\ndef asseries(cls, value, like=None):\n    \"\"\"\n    The function asseries(cls, value, like=None) ensures that the input value is converted into a pandas Series object,\n    broadcasting it if necessary.\n    If the value is a Dask Series, it returns it directly.\n    If the value is a Dask scalar, it creates a Dask array from the scalar and then a Dask Series from the array.\n    If the value is a pandas Series, it converts it into a Dask Series with one partition.\n    If a 'like' object is provided, it creates a Series based on the structure of the 'like' object, handling tuples,\n    lists, and dictionaries appropriately.\n    If none of these conditions are met, it converts the value into a single-partition Dask Series.\n    \"\"\"\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.core.Scalar):\n        # Assuming value.compute() retrieves the scalar value for conversion\n        array = dd.from_array([value.compute()])\n        return dd.from_array(array, columns=['value'])\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif like is not None:\n        if isinstance(like, (tuple, list)):\n            return dd.from_pandas(pd.Series(list(value)), npartitions=1)\n        elif isinstance(like, dict):\n            return dd.from_pandas(pd.Series(like), npartitions=1)\n        else:\n            raise ValueError(\"Unsupported 'like' type\")\n    else:\n        return dd.from_pandas(pd.Series([value]), npartitions=1)", "CodeBleu_score": 47.572364494461304}, {"code": "import pandas as pd\nimport dask.dataframe as dd\n\ndef asseries(cls, value, like=None):\n    \"\"\"\n    The function asseries(cls, value, like=None) ensures that the input value is converted into a pandas Series object,\n    broadcasting it if necessary.\n    If the value is a Dask Series, it returns it directly.\n    If the value is a Dask scalar, it creates a Dask array from the scalar and then a Dask Series from the array.\n    If the value is a pandas Series, it converts it into a Dask Series with one partition.\n    If a 'like' object is provided, it creates a Series based on the structure of the 'like' object,\n    handling tuples, lists, and dictionaries appropriately.\n    If none of these conditions are met, it converts the value into a single-partition Dask Series.\n    \"\"\"\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.core.Scalar):\n        return dd.from_array(value.to_dask_array()).to_series()\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif like is not None:\n        if isinstance(like, (tuple, list)):\n            return dd.from_pandas(pd.Series(list(value)), npartitions=1)\n        elif isinstance(like, dict):\n            return dd.from_pandas(pd.Series({k: v for k, v in value.items()}), npartitions=1)\n    else:\n        return dd.from_pandas(pd.Series([value]), npartitions=1)", "CodeBleu_score": 43.24520899663192}, {"code": "import pandas as pd\nimport dask.dataframe as dd\n\ndef asseries(cls, value, like=None):\n    \"\"\"\n    The function asseries(cls, value, like=None) ensures that the input value is converted into a pandas Series object,\n    broadcasting it if necessary.\n    If the value is a Dask Series, it returns it directly.\n    If the value is a Dask scalar, it creates a Dask array from the scalar and then a Dask Series from the array.\n    If the value is a pandas Series, it converts it into a Dask Series with one partition.\n    If a 'like' object is provided, it creates a Series based on the structure of the 'like' object,\n    handling tuples, lists, and dictionaries appropriately.\n    If none of these conditions are met, it converts the value into a single-partition Dask Series.\n    \"\"\"\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.core.Scalar):\n        # Convert Dask scalar to Dask array then to Dask Series\n        array = dd.from_array(value.compute(), chunks=(1,))\n        return dd.from_array(array, columns=cls)\n    elif isinstance(value, pd.Series):\n        # Convert pandas Series to Dask Series\n        return dd.from_pandas(value, npartitions=1)\n    elif like is not None:\n        # Create a Series based on the structure of 'like'\n        if isinstance(like, (tuple, list)):\n            return dd.from_pandas(pd.Series(value, index=like), npartitions=1)\n        elif isinstance(like, dict):\n            return dd.from_pandas(pd.Series(value, index=like.keys()), npartitions=1)\n        else:\n            raise ValueError(\"Unsupported 'like' type\")\n    else:\n        # Convert other types to a single-partition Dask Series\n        return dd.from_pandas(pd.Series([value]), npartitions=1)", "CodeBleu_score": 48.683081681367455}]}
{"instance_id": "926220a06a0c", "generation_result": [{"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, pd.DataFrame] | None = None,\n    ) -> None:\n    \"\"\"\n    This function connects to a given dictionary of data frames.\n    If the dictionary is not provided, it initializes an empty dictionary.\n    It validates that each value in the dictionary is either a Dask or Pandas DataFrame.\n    If any value is not a valid DataFrame, it raises a TypeError.\n    Finally, it calls the superclass's connect method with the validated dictionary.\n    \"\"\"\n    import pandas as pd\n    import dask.dataframe as dd\n\n    if dictionary is None:\n        dictionary = {}\n\n    for key, df in dictionary.items():\n        if not isinstance(df, (pd.DataFrame, dd.DataFrame)):\n            raise TypeError(f\"The value associated with {key} is not a Pandas or Dask DataFrame\")\n\n    super().connect(dictionary)", "CodeBleu_score": 63.12626788585418}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, pd.DataFrame] | None = None,\n    ) -> None:\n    \"\"\"This function connects to a given dictionary of data frames.\n    If the dictionary is not provided, it initializes an empty dictionary.\n    It validates that each value in the dictionary is either a Dask or Pandas DataFrame.\n    If any value is not a valid DataFrame, it raises a TypeError.\n    Finally, it calls the superclass's connect method with the validated dictionary.\"\"\"\n\n    import pandas as pd\n    import dask.dataframe as dd\n\n    if dictionary is None:\n        dictionary = {}\n\n    for key, df in dictionary.items():\n        if not isinstance(df, (pd.DataFrame, dd.DataFrame)):\n            raise TypeError(f\"The value associated with {key} is not a Pandas or Dask DataFrame\")\n\n    super().connect(dictionary)", "CodeBleu_score": 63.12626788585418}, {"code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, pd.DataFrame] | None = None,\n    ) -> None:\n    \"\"\"\n    This function connects to a given dictionary of data frames.\n    If the dictionary is not provided, it initializes an empty dictionary.\n    It validates that each value in the dictionary is either a Dask or Pandas DataFrame.\n    If any value is not a valid DataFrame, it raises a TypeError.\n    Finally, it calls the superclass's connect method with the validated dictionary.\n    \"\"\"\n    import pandas as pd\n    import dask.dataframe as dd\n\n    if dictionary is None:\n        dictionary = {}\n\n    for key, df in dictionary.items():\n        if not isinstance(df, (pd.DataFrame, dd.DataFrame)):\n            raise TypeError(f\"The value associated with {key} is not a valid DataFrame.\")\n\n    super().connect(dictionary)", "CodeBleu_score": 63.0698797041762}]}
{"instance_id": "95afc38c9125", "generation_result": [{"code": "import logging\nfrom sklearn.base import BaseEstimator\n\ndef get_module_members(library_name):\n    \"\"\"Simulated function to retrieve module members.\"\"\"\n    # This is a placeholder function. In practice, you would dynamically import and inspect the module.\n    if library_name == \"sklearn.ensemble\":\n        return {\"RandomForestClassifier\": RandomForestClassifier, \"GradientBoostingClassifier\": GradientBoostingClassifier}\n    return {}\n\ndef get_estimator(library_name: str, estimator_name: str):\n    \"\"\"The function get_estimator takes a library name and an estimator name as arguments.\n    It retrieves a map of classes from the specified library module using the get_module_members function.\n    If the estimator name is not found in the classes map, it raises a ValueError indicating the estimator could not be found in the module.\n    If multiple classes match the estimator name, it logs a debug message with the list of matching estimators and a warning message indicating that multiple classes were found and the first one will be used.\n    The function then checks if the selected estimator class is a subclass of sklearn's BaseEstimator and logs an informational message if it is not.\n    Finally, it returns the selected estimator class.\"\"\"\n\n    # Retrieve the map of classes from the specified library\n    classes = get_module_members(library_name)\n\n    # Check if the estimator name is in the classes map\n    matching_classes = [cls for name, cls in classes.items() if estimator_name.lower() in name.lower()]\n\n    if not matching_classes:\n        raise ValueError(f\"Estimator '{estimator_name}' could not be found in the module '{library_name}'.\")\n\n    if len(matching_classes) > 1:\n        logging.debug(f\"Multiple matching estimators found: {matching_classes}\")\n        logging.warning(\"Multiple classes were found; the first one will be used.\")\n\n    # Select the first matching class\n    selected_class = matching_classes[0]\n\n    # Check if the selected class is a subclass of BaseEstimator\n    if not issubclass(selected_class, BaseEstimator):\n        logging.info(f\"The selected class {selected_class.__name__} is not a subclass of sklearn's BaseEstimator.\")\n\n    return selected_class", "CodeBleu_score": 36.06603733074177}, {"code": "import logging\nfrom sklearn.base import BaseEstimator\n\ndef get_module_members(library_name):\n    # This is a placeholder for the actual implementation that retrieves module members.\n    # It should return a dictionary of class names to class objects.\n    pass\n\ndef get_estimator(library_name: str, estimator_name: str):\n    # Retrieve a map of classes from the specified library module\n    classes = get_module_members(library_name)\n    \n    # Find all classes that match the estimator name\n    matching_classes = [cls for name, cls in classes.items() if estimator_name.lower() in name.lower()]\n    \n    if not matching_classes:\n        raise ValueError(f\"Estimator '{estimator_name}' not found in module '{library_name}'.\")\n    \n    if len(matching_classes) > 1:\n        logging.debug(f\"Multiple matching estimators found: {matching_classes}\")\n        logging.warning(\"Multiple classes found; the first one will be used.\")\n    \n    # Select the first matching class\n    selected_class = matching_classes[0]\n    \n    # Check if the selected class is a subclass of sklearn's BaseEstimator\n    if not issubclass(selected_class, BaseEstimator):\n        logging.info(f\"The selected estimator '{selected_class.__name__}' is not a subclass of sklearn's BaseEstimator.\")\n    \n    # Return the selected estimator class\n    return selected_class", "CodeBleu_score": 37.0509234127857}, {"code": "import logging\nfrom sklearn.base import BaseEstimator\n\ndef get_module_members(library_name):\n    \"\"\"Simulated function to get module members, replace with actual implementation.\"\"\"\n    import sys\n    module = sys.modules[library_name]\n    return {name: cls for name, cls in module.__dict__.items() if isinstance(cls, type)}\n\ndef get_estimator(library_name: str, estimator_name: str):\n    # Retrieve a map of classes from the specified library module\n    classes = get_module_members(library_name)\n    \n    # Find all classes that match the estimator name\n    matching_classes = {name: cls for name, cls in classes.items() if estimator_name.lower() in name.lower()}\n    \n    if not matching_classes:\n        raise ValueError(f\"Estimator '{estimator_name}' not found in module '{library_name}'.\")\n    \n    if len(matching_classes) > 1:\n        logging.debug(f\"Multiple matching estimators found: {list(matching_classes.keys())}\")\n        logging.warning(\"Multiple classes were found; the first one will be used.\")\n    \n    # Select the first matching class\n    selected_class = next(iter(matching_classes.values()))\n    \n    # Check if the selected class is a subclass of sklearn's BaseEstimator\n    if not issubclass(selected_class, BaseEstimator):\n        logging.info(f\"The selected class '{selected_class.__name__}' is not a subclass of sklearn's BaseEstimator.\")\n    \n    return selected_class", "CodeBleu_score": 38.49978078463605}]}
{"instance_id": "9562a9cc043b", "generation_result": [{"code": "import json\n\ndef estimator_to_task(estimator_name: str) -> str:\n    \"\"\"\n    This function maps an estimator name to a machine learning task by checking its postfix against a predefined mapping.\n    It reads the mapping from a JSON file and returns the corresponding task if a match is found.\n    If no match is found, it returns \"unknown\".\n    \"\"\"\n    try:\n        # Load the mapping from a JSON file\n        with open('estimator_mapping.json', 'r') as file:\n            mapping = json.load(file)\n        \n        # Check each key in the mapping to see if it is a postfix of the estimator_name\n        for key in mapping:\n            if estimator_name.endswith(key):\n                return mapping[key]\n        \n        # If no match is found, return \"unknown\"\n        return \"unknown\"\n    except FileNotFoundError:\n        # Handle the case where the JSON file is not found\n        return \"File not found. Please ensure the 'estimator_mapping.json' file exists.\"\n\n# Example usage:\n# print(estimator_to_task(\"RandomForestClassifier\"))", "CodeBleu_score": 34.0870646181688}, {"code": "import json\n\ndef estimator_to_task(estimator_name: str) -> str:\n    \"\"\"\n    This function maps an estimator name to a machine learning task by checking its postfix against a predefined mapping.\n    It reads the mapping from a JSON file and returns the corresponding task if a match is found.\n    If no match is found, it returns \"unknown\".\n    \"\"\"\n    try:\n        # Load the mapping from a JSON file\n        with open('estimator_mapping.json', 'r') as file:\n            mapping = json.load(file)\n        \n        # Check each key in the mapping to see if it is a postfix of the estimator_name\n        for key in mapping:\n            if estimator_name.endswith(key):\n                return mapping[key]\n        \n        # If no match is found, return \"unknown\"\n        return \"unknown\"\n    except FileNotFoundError:\n        # Handle the case where the JSON file is not found\n        return \"File not found. Please ensure the 'estimator_mapping.json' file exists.\"", "CodeBleu_score": 34.894302981191075}, {"code": "import json\n\ndef estimator_to_task(estimator_name: str) -> str:\n    \"\"\"\n    This function maps an estimator name to a machine learning task by checking its postfix against a predefined mapping.\n    It reads the mapping from a JSON file and returns the corresponding task if a match is found.\n    If no match is found, it returns \"unknown\".\n    \"\"\"\n    # Load the mapping from a JSON file\n    try:\n        with open('estimator_mapping.json', 'r') as file:\n            mapping = json.load(file)\n    except FileNotFoundError:\n        print(\"Mapping file not found.\")\n        return \"unknown\"\n    except json.JSONDecodeError:\n        print(\"Error decoding JSON.\")\n        return \"unknown\"\n\n    # Check each key in the mapping to see if it is a postfix of the estimator name\n    for key in mapping:\n        if estimator_name.endswith(key):\n            return mapping[key]\n\n    # If no match is found, return \"unknown\"\n    return \"unknown\"", "CodeBleu_score": 34.20301277138218}]}
{"instance_id": "2f50195d043a", "generation_result": [{"code": "import os\nfrom typing import Dict\n\ndef load_file_content(file_path: str) -> any:\n    # This function should be provided, or you need to define it according to your data handling needs.\n    # It loads content from the file and returns it in an appropriate format.\n    with open(file_path, 'r') as file:\n        # Assuming a simple file type like txt:\n        content = file.read()\n    return content\n\ndef load_data_from_cache(data_cache: str, data_name: str) -> Dict:\n    \"\"\"\n    The function load_data_from_cache takes a directory path and a prefix string, retrieves filenames\n    starting with the prefix, and loads data from those files into a dictionary. It skips files with a\n    .json extension and processes other files by extracting the component and file extension from the\n    filename. The function then loads the file content using another function and stores it in the\n    dictionary under the component name as the key. Finally, it returns the dictionary containing\n    the loaded data.\n    \"\"\"\n    loaded_data = {}\n    \n    # Verify if data_cache is a valid directory\n    if not os.path.isdir(data_cache):\n        raise ValueError(f\"The provided data cache path is not a valid directory: {data_cache}\")\n    \n    # List all files in the directory\n    for fname in os.listdir(data_cache):\n        # Check if the file starts with given data_name and is not a json file\n        if fname.startswith(data_name) and not fname.endswith('.json'):\n            # Split on extension to get the component name\n            component = os.path.splitext(fname)[0]\n            # Construct full file path\n            file_path = os.path.join(data_cache, fname)\n            # Load the content of the file using a helper function (assumed already defined)\n            content = load_file_content(file_path)\n            # Add content to dictionary under the component's name\n            loaded_data[component] = content\n\n    return loaded_data", "CodeBleu_score": 33.50901433840545}, {"code": "import os\nimport json\nfrom typing import Dict\n\ndef load_specific_file_format(filename: str) -> any:\n    \"\"\"Custom function for loading specific file formats based on the file extension.\"\"\"\n    extension = filename.split('.')[-1]\n    if extension == 'txt':\n        with open(filename, 'r') as file:\n            return file.read()\n    elif extension == 'csv':\n        import pandas as pd\n        return pd.read_csv(filename)\n    else:\n        raise ValueError(f\"Unsupported file extension: {extension}\")\n\ndef load_data_from_cache(data_cache: str, data_name: str) -> Dict[str, any]:\n    \"\"\"\n    The function load_data_from_cache takes a directory path and a prefix string, retrieves filenames starting with the prefix, and loads data from those files into a dictionary. It skips files with a .json extension and processes other files by extracting the component and file extension from the filename. The function then loads the file content using another function and stores it in the dictionary under the component name as the key. Finally, it returns the dictionary containing the loaded data.\n\n    Parameters:\n    data_cache (str): The directory containing data files.\n    data_name (str): The prefix to filter files by.\n\n    Returns:\n    Dict[str, any]: Dictionary with component names as keys and loaded data as values.\n    \"\"\"\n    data_dictionary = {}\n    \n    # Loop through each file in the directory that has the data_name prefix but is not a .json file\n    for filename in os.listdir(data_cache):\n        if filename.startswith(data_name) and not filename.endswith('.json'):\n            filepath = os.path.join(data_cache, filename)\n            component, ext = os.path.splitext(filename)\n\n            if ext != '.json':\n                data = load_specific_file_format(filepath)\n                data_dictionary[component] = data\n    \n    return data_dictionary", "CodeBleu_score": 33.8038800217453}, {"code": "import os\nimport json\nfrom typing import Dict, Any\n\ndef load_file_content(file_path: str) -> Any:\n    # This is a placeholder for actual function logic that would read and interpret file content\n    # It should handle different kinds of files based on their formats.\n    with open(file_path, 'r') as file:\n        return file.read()\n\ndef load_data_from_cache(data_cache: str, data_name: str) -> Dict[str, Any]:\n    \"\"\"\n    The function load_data_from_cache takes a directory path and a prefix string,\n    retrieves filenames starting with the prefix, and loads data from those files into a dictionary.\n    It skips files with a .json extension and processes other files by extracting the component \n    and file extension from the filename. The function then loads the file content using another\n    function and stores it in the dictionary under the component name as the key. Finally,\n    it returns the dictionary containing the loaded data.\n    \n    Args:\n    data_cache (str): Directory path where the data files are stored.\n    data_name (str): Prefix string of the files to process.\n\n    Returns:\n    Dict[str, Any]: A dictionary with the key as the component name and value as the loaded data.\n    \"\"\"\n    data_dictionary = {}\n    # Iterate over files in the directory\n    for file in os.listdir(data_cache):\n        # Check if the file starts with the data_name prefix and does not end with .json\n        if file.startswith(data_name) and not file.endswith('.json'):\n            file_path = os.path.join(data_cache, file)\n            # Extract component name by splitting the filename and removing the extension\n            component_name = os.path.splitext(file)[0]\n            # Load the content of the file using another function (assumed)\n            content = load_file_content(file_path)\n            # Store the content in the dictionary with the component name as the key\n            data_dictionary[component_name] = content\n    \n    return data_dictionary", "CodeBleu_score": 33.17175020134957}]}
{"instance_id": "e0281206029e", "generation_result": [{"code": "import os\nimport requests\n\ndef retrieve(url: str, filename: str) -> None:\n    \"\"\"\n    The function retrieve takes a URL and a filename as arguments.\n    It checks if the specified file already exists using os.path.isfile; if it does, the function returns immediately.\n    If the URL starts with \"http\", it attempts to download the content from the URL using the requests library.\n    If the response status code is not 200, it raises an AssertionError indicating the download failed and includes the status code.\n    It then reads the total content length from the response headers and sets a block size for reading the data.\n    The function writes the content to the specified file in chunks, updating the written data size.\n    After the download, it verifies if the total downloaded size matches the expected content length, raising an AssertionError if there is a discrepancy.\n    \"\"\"\n    # Check if the file already exists\n    if os.path.isfile(filename):\n        return\n\n    # Check if the URL is valid for HTTP requests\n    if url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        \n        # Check if the download was successful\n        if response.status_code != 200:\n            raise AssertionError(f\"Download failed with status code: {response.status_code}\")\n        \n        # Get the total content length\n        total_length = int(response.headers.get('content-length', 0))\n        \n        # Set a block size for reading data\n        block_size = 1024\n        \n        # Write the content to the file in chunks\n        written_size = 0\n        with open(filename, 'wb') as file:\n            for data in response.iter_content(block_size):\n                file.write(data)\n                written_size += len(data)\n        \n        # Verify the total downloaded size\n        if written_size != total_length:\n            raise AssertionError(\"Downloaded size does not match the expected content length\")", "CodeBleu_score": 65.58975593978651}, {"code": "import os\nimport requests\n\ndef retrieve(url: str, filename: str) -> None:\n    \"\"\"The function retrieve takes a URL and a filename as arguments.\n    It checks if the specified file already exists using os.path.isfile; if it does, the function returns immediately.\n    If the URL starts with \"http\", it attempts to download the content from the URL using the requests library.\n    If the response status code is not 200, it raises an AssertionError indicating the download failed and includes the status code.\n    It then reads the total content length from the response headers and sets a block size for reading the data.\n    The function writes the content to the specified file in chunks, updating the written data size.\n    After the download, it verifies if the total downloaded size matches the expected content length, raising an AssertionError if there is a discrepancy.\"\"\"\n\n    # Check if the file already exists\n    if os.path.isfile(filename):\n        return\n\n    # Check if the URL is valid for HTTP requests\n    if not url.startswith(\"http\"):\n        raise ValueError(\"URL must start with 'http'\")\n\n    # Attempt to download the content\n    response = requests.get(url, stream=True)\n    if response.status_code != 200:\n        raise AssertionError(f\"Download failed with status code: {response.status_code}\")\n\n    # Get the total content length\n    total_length = int(response.headers.get('content-length', 0))\n\n    # Set a block size for reading data\n    block_size = 1024\n\n    # Write the content to the file in chunks\n    written_size = 0\n    with open(filename, 'wb') as file:\n        for data in response.iter_content(block_size):\n            file.write(data)\n            written_size += len(data)\n\n    # Verify the total downloaded size\n    if written_size != total_length:\n        raise AssertionError(\"Downloaded size does not match the expected content length\")", "CodeBleu_score": 64.12266565009074}, {"code": "import os\nimport requests\n\ndef retrieve(url: str, filename: str) -> None:\n    \"\"\"The function retrieve takes a URL and a filename as arguments.\n    It checks if the specified file already exists using os.path.isfile; if it does, the function returns immediately.\n    If the URL starts with \"http\", it attempts to download the content from the URL using the requests library.\n    If the response status code is not 200, it raises an AssertionError indicating the download failed and includes the status code.\n    It then reads the total content length from the response headers and sets a block size for reading the data.\n    The function writes the content to the specified file in chunks, updating the written data size.\n    After the download, it verifies if the total downloaded size matches the expected content length, raising an AssertionError if there is a discrepancy.\"\"\"\n\n    # Check if the file already exists\n    if os.path.isfile(filename):\n        return\n\n    # Check if the URL is valid for HTTP requests\n    if not url.startswith(\"http\"):\n        raise ValueError(\"URL must start with 'http'\")\n\n    # Attempt to download the content\n    response = requests.get(url, stream=True)\n    if response.status_code != 200:\n        raise AssertionError(f\"Download failed with status code: {response.status_code}\")\n\n    # Get the total content length\n    total_length = int(response.headers.get('content-length', 0))\n    block_size = 1024  # 1 Kilobyte\n\n    # Write the content to the file in chunks\n    written_size = 0\n    with open(filename, 'wb') as file:\n        for data in response.iter_content(block_size):\n            file.write(data)\n            written_size += len(data)\n\n    # Verify the total downloaded size\n    if written_size != total_length:\n        raise AssertionError(\"Downloaded size does not match expected content length\")", "CodeBleu_score": 64.46117069740856}]}
{"instance_id": "a69a71cc2fe8", "generation_result": [{"code": "import pandas as pd\nfrom typing import Dict, Tuple\n\ndef load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]:\n    \"\"\"The function load_covtype loads the Covertype dataset, processes the target labels, and returns the data along with a description. It takes the dataset name, cache paths, and parameters as inputs. The function retrieves the dataset features and labels, adjusts the labels to be zero-indexed, and optionally converts them to binary based on a parameter. It then creates a description dictionary with the number of classes and default split parameters. Finally, it returns a tuple containing the dataset and its description.\"\"\"\n\n    # Load the dataset\n    file_path = f\"{raw_data_cache}/{data_name}.csv\"\n    data = pd.read_csv(file_path)\n\n    # Adjust labels to be zero-indexed if they aren't already\n    data['label'] = data['label'] - 1\n\n    # Check if labels should be converted to binary\n    if dataset_params.get('convert_to_binary', False):\n        # Assuming binary classification around a specific class, e.g., class 0\n        data['label'] = (data['label'] == 0).astype(int)\n\n    # Create a description of the dataset\n    description = {\n        'num_classes': len(data['label'].unique()),\n        'default_split': dataset_params.get('default_split', 'train:60%, val:20%, test:20%')\n    }\n\n    # Optionally cache the processed data\n    if dataset_params.get('cache_processed_data', False):\n        processed_file_path = f\"{data_cache}/{data_name}_processed.csv\"\n        data.to_csv(processed_file_path, index=False)\n\n    # Return the dataset and its description\n    return data.to_dict('list'), description", "CodeBleu_score": 33.76728274598189}, {"code": "import os\nimport pickle\nfrom typing import Dict, Tuple\n\ndef load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]:\n    \"\"\"\n    The function load_covtype loads the Covertype dataset, processes the target labels, and returns the data along with a description.\n    It takes the dataset name, cache paths, and parameters as inputs. The function retrieves the dataset features and labels, adjusts the labels\n    to be zero-indexed, and optionally converts them to binary based on a parameter. It then creates a description dictionary with the number of\n    classes and default split parameters. Finally, it returns a tuple containing the dataset and its description.\n    \"\"\"\n    \n    # Check if processed data is cached\n    processed_data_path = os.path.join(data_cache, f\"{data_name}_processed.pkl\")\n    if os.path.exists(processed_data_path):\n        with open(processed_data_path, 'rb') as f:\n            dataset, description = pickle.load(f)\n        return dataset, description\n\n    # Load raw data\n    raw_data_path = os.path.join(raw_data_cache, f\"{data_name}_raw.pkl\")\n    with open(raw_data_path, 'rb') as f:\n        data = pickle.load(f)\n    \n    # Process labels (assuming labels are in the last column)\n    labels = data[:, -1].astype(int) - 1  # Convert labels to zero-indexed\n    if dataset_params.get('binary', False):\n        # Convert to binary classification problem if specified\n        positive_class = dataset_params.get('positive_class', 1)\n        labels = (labels == positive_class).astype(int)\n    \n    # Create dataset dictionary\n    dataset = {\n        'features': data[:, :-1],\n        'labels': labels\n    }\n    \n    # Create description dictionary\n    num_classes = len(set(labels))\n    description = {\n        'num_classes': num_classes,\n        'default_split': dataset_params.get('default_split', 'train-test')\n    }\n    \n    # Cache processed data\n    with open(processed_data_path, 'wb') as f:\n        pickle.dump((dataset, description), f)\n    \n    return dataset, description", "CodeBleu_score": 34.27351176202224}, {"code": "from typing import Dict, Tuple\nimport numpy as np\n\ndef load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]:\n    \"\"\"\n    The function load_covtype loads the Covertype dataset, processes the target labels, and returns the data along with a description.\n    It takes the dataset name, cache paths, and parameters as inputs. The function retrieves the dataset features and labels, adjusts the labels to be zero-indexed,\n    and optionally converts them to binary based on a parameter. It then creates a description dictionary with the number of classes and default split parameters.\n    Finally, it returns a tuple containing the dataset and its description.\n    \"\"\"\n    \n    # Load the dataset (assuming a function exists to load it based on name and cache paths)\n    dataset = load_dataset_from_cache(data_name, data_cache, raw_data_cache)\n    \n    # Adjust labels to be zero-indexed\n    dataset['labels'] -= 1\n    \n    # Check if labels should be converted to binary (assuming 'binary' is a key in dataset_params)\n    if dataset_params.get('binary', False):\n        # Convert labels to binary; assuming binary classification between class 0 and all others\n        dataset['labels'] = (dataset['labels'] == 0).astype(int)\n    \n    # Create a description dictionary\n    description = {\n        'num_classes': np.unique(dataset['labels']).size,\n        'default_split': dataset_params.get('default_split', 'train-test')\n    }\n    \n    return dataset, description\n\ndef load_dataset_from_cache(data_name, data_cache, raw_data_cache):\n    # This is a placeholder function. You would need to replace this with actual data loading logic.\n    # For now, it returns a mock dataset.\n    return {\n        'features': np.random.rand(100, 10),  # 100 samples, 10 features each\n        'labels': np.random.randint(1, 8, size=100)  # Random labels from 1 to 7 (Covertype has 7 classes)\n    }", "CodeBleu_score": 33.40907297558642}]}
{"instance_id": "129cbd2529ef", "generation_result": [{"code": "from typing import List, Any\n\ndef flatten_list(input_list: List[Any], ensure_type_homogeneity: bool = False) -> List[Any]:\n    \"\"\"\n    The function flatten_list takes a list and an optional boolean argument ensure_type_homogeneity.\n    It initializes an empty list to store the flattened values.\n    The function iterates through each element in the input list.\n    If an element is a list, it recursively calls itself to flatten the inner list and appends the flattened values to the output list.\n    If an element is not a list, it directly appends the element to the output list.\n    If ensure_type_homogeneity is True, it calls ensure_list_types_homogeneity on the output list to ensure all elements are of the same type.\n    Finally, it returns the flattened list.\n    \"\"\"\n    def flatten(items):\n        for item in items:\n            if isinstance(item, list):\n                yield from flatten(item)\n            else:\n                yield item\n\n    flattened = list(flatten(input_list))\n\n    if ensure_type_homogeneity:\n        if not all(isinstance(x, type(flattened[0])) for x in flattened):\n            raise TypeError(\"Not all elements are of the same type.\")\n\n    return flattened", "CodeBleu_score": 32.74911034822988}, {"code": "from typing import List, Any\n\ndef flatten_list(input_list: List[Any], ensure_type_homogeneity: bool = False) -> List[Any]:\n    \"\"\"\n    The function flatten_list takes a list and an optional boolean argument ensure_type_homogeneity.\n    It initializes an empty list to store the flattened values.\n    The function iterates through each element in the input list.\n    If an element is a list, it recursively calls itself to flatten the inner list and appends the flattened values to the output list.\n    If an element is not a list, it directly appends the element to the output list.\n    If ensure_type_homogeneity is True, it calls ensure_list_types_homogeneity on the output list to ensure all elements are of the same type.\n    Finally, it returns the flattened list.\n    \"\"\"\n    def flatten(items):\n        for item in items:\n            if isinstance(item, list):\n                yield from flatten(item)\n            else:\n                yield item\n\n    flattened_list = list(flatten(input_list))\n\n    if ensure_type_homogeneity:\n        if not all(isinstance(x, type(flattened_list[0])) for x in flattened_list):\n            raise ValueError(\"All elements in the list must be of the same type.\")\n\n    return flattened_list", "CodeBleu_score": 32.71882325470115}, {"code": "from typing import List\n\ndef flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List:\n    \"\"\"\n    The function flatten_list takes a list and an optional boolean argument ensure_type_homogeneity.\n    It initializes an empty list to store the flattened values.\n    The function iterates through each element in the input list.\n    If an element is a list, it recursively calls itself to flatten the inner list and appends the flattened values to the output list.\n    If an element is not a list, it directly appends the element to the output list.\n    If ensure_type_homogeneity is True, it calls ensure_list_types_homogeneity on the output list to ensure all elements are of the same type.\n    Finally, it returns the flattened list.\n    \"\"\"\n    def flatten(items):\n        for item in items:\n            if isinstance(item, list):\n                yield from flatten(item)\n            else:\n                yield item\n\n    flattened = list(flatten(input_list))\n\n    if ensure_type_homogeneity:\n        if not all(isinstance(x, type(flattened[0])) for x in flattened):\n            raise ValueError(\"All elements in the list must be of the same type.\")\n\n    return flattened", "CodeBleu_score": 33.24486539137412}]}
{"instance_id": "2d38c2c60b59", "generation_result": [{"code": "import numpy as np\nfrom typing import Optional\nfrom typing_extensions import Literal\n\ndef logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n):\n    \"\"\"The function logit computes the logit (log-odds) of input array x. It takes x as a mandatory input, with optional parameters eps for clipping values, complex_mode to specify handling of complex numbers, and out for output array. If eps is None, values outside [0,1] are set to NaN. Otherwise, values are clipped to the range [eps, 1-eps]. The function returns the logit of x, preserving the input data type.\"\"\"\n\n    if eps is not None:\n        x = np.clip(x, eps, 1 - eps)\n    else:\n        # Set values outside [0, 1] to NaN\n        x = np.where((x < 0) | (x > 1), np.nan, x)\n\n    if np.iscomplexobj(x):\n        if complex_mode == \"split\":\n            real_part = np.log(x.real / (1 - x.real))\n            imag_part = np.log(x.imag / (1 - x.imag))\n            result = real_part + 1j * imag_part\n        elif complex_mode == \"magnitude\":\n            magnitude = np.abs(x)\n            magnitude = np.log(magnitude / (1 - magnitude))\n            result = magnitude * np.exp(1j * np.angle(x))\n        elif complex_mode == \"jax\":\n            # Placeholder for JAX-specific complex handling, which might involve differentiable programming\n            result = np.log(x / (1 - x))  # Simplified version\n    else:\n        result = np.log(x / (1 - x))\n\n    if out is not None:\n        out[:] = result\n        return out\n    else:\n        return result", "CodeBleu_score": 49.01037760909569}, {"code": "import numpy as np\nfrom typing import Optional\nfrom typing_extensions import Literal\n\ndef logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"The function logit computes the logit (log-odds) of input array x. It takes x as a mandatory input, with optional parameters eps for clipping values, complex_mode to specify handling of complex numbers, and out for output array. If eps is None, values outside [0,1] are set to NaN. Otherwise, values are clipped to the range [eps, 1-eps]. The function returns the logit of x, preserving the input data type.\"\"\"\n\n    if eps is not None:\n        x = np.clip(x, eps, 1 - eps)\n    else:\n        # Set values outside [0, 1] to NaN\n        x = np.where((x < 0) | (x > 1), np.nan, x)\n\n    if np.iscomplexobj(x):\n        if complex_mode == \"split\":\n            real_part = np.log(x.real / (1 - x.real))\n            imag_part = np.log(x.imag / (1 - x.imag))\n            result = real_part + 1j * imag_part\n        elif complex_mode == \"magnitude\":\n            magnitude = np.abs(x)\n            magnitude = np.log(magnitude / (1 - magnitude))\n            result = magnitude * np.exp(1j * np.angle(x))\n        elif complex_mode == \"jax\":\n            # Placeholder for JAX-specific complex handling, using default numpy handling here\n            result = np.log(x / (1 - x))\n    else:\n        result = np.log(x / (1 - x))\n\n    if out is not None:\n        np.copyto(out, result)\n        return out\n    else:\n        return result", "CodeBleu_score": 49.052484284226985}, {"code": "import numpy as np\nfrom typing import Optional\nfrom typing_extensions import Literal\n\ndef logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n):\n    \"\"\"The function logit computes the logit (log-odds) of input array x. It takes x as a mandatory input, with optional parameters eps for clipping values, complex_mode to specify handling of complex numbers, and out for output array. If eps is None, values outside [0,1] are set to NaN. Otherwise, values are clipped to the range [eps, 1-eps]. The function returns the logit of x, preserving the input data type.\"\"\"\n\n    if eps is not None:\n        x = np.clip(x, eps, 1 - eps)\n    else:\n        # Set values outside [0, 1] to NaN\n        x = np.where((x < 0) | (x > 1), np.nan, x)\n\n    if np.iscomplexobj(x):\n        if complex_mode == \"split\":\n            real_part = np.log(x.real / (1 - x.real))\n            imag_part = np.log(x.imag / (1 - x.imag))\n            result = real_part + 1j * imag_part\n        elif complex_mode == \"magnitude\":\n            magnitude = np.abs(x)\n            angle = np.angle(x)\n            logit_magnitude = np.log(magnitude / (1 - magnitude))\n            result = logit_magnitude * np.exp(1j * angle)\n        elif complex_mode == \"jax\":\n            # This mode might require specific handling similar to a library like JAX, here we use a placeholder\n            result = np.log(x / (1 - x))  # Simplified version\n    else:\n        result = np.log(x / (1 - x))\n\n    if out is not None:\n        out[:] = result\n        return out\n    else:\n        return result", "CodeBleu_score": 48.79316214206229}]}
{"instance_id": "205e296f8621", "generation_result": [{"code": "import numpy as np\nfrom typing import Optional\n\ndef hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray:\n    \"\"\"The function hardshrink takes a NumPy array x, a threshold value lambd, and an optional output array out as arguments.\n    It applies a hard thresholding operation to the input array x, setting values to zero if they are within the range [-lambd, lambd], and keeping the original values otherwise.\n    If an output array out is provided, the function updates it in-place with the result and returns it with the same data type as the input array x.\n    If no output array is provided, it returns the result with the same data type as the input array x.\"\"\"\n    \n    # Apply the hardshrink operation\n    result = np.where((x >= -lambd) & (x <= lambd), 0, x)\n    \n    # If an output array is provided, update it in-place\n    if out is not None:\n        out[:] = result\n        return out\n    \n    return result", "CodeBleu_score": 39.79142625061388}, {"code": "import numpy as np\nfrom typing import Optional\n\ndef hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray:\n    \"\"\"The function hardshrink takes a NumPy array x, a threshold value lambd, and an optional output array out as arguments.\n    It applies a hard thresholding operation to the input array x, setting values to zero if they are within the range [-lambd, lambd], and keeping the original values otherwise.\n    If an output array out is provided, the function updates it in-place with the result and returns it with the same data type as the input array x.\n    If no output array is provided, it returns the result with the same data type as the input array x.\"\"\"\n    \n    # Apply the hardshrink operation\n    result = np.where((x > -lambd) & (x < lambd), 0, x)\n    \n    # If an output array is provided, update it in-place\n    if out is not None:\n        out[:] = result\n        return out\n    else:\n        return result", "CodeBleu_score": 39.39312990179058}, {"code": "import numpy as np\nfrom typing import Optional\n\ndef hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray:\n    \"\"\"The function hardshrink takes a NumPy array x, a threshold value lambd, and an optional output array out as arguments.\n    It applies a hard thresholding operation to the input array x, setting values to zero if they are within the range [-lambd, lambd], and keeping the original values otherwise.\n    If an output array out is provided, the function updates it in-place with the result and returns it with the same data type as the input array x.\n    If no output array is provided, it returns the result with the same data type as the input array x.\"\"\"\n    \n    # Apply the hardshrink operation\n    result = np.where((x > -lambd) & (x < lambd), 0, x)\n    \n    # If an output array is provided, update it in-place\n    if out is not None:\n        out[:] = result\n        return out\n    \n    return result", "CodeBleu_score": 40.129420539874474}]}
{"instance_id": "5b133f6ebe2d", "generation_result": [{"code": "import numpy as np\n\ndef vorbis_window(\n    window_length: int,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None\n) -> np.ndarray:\n    \"\"\"The function vorbis_window generates a Vorbis window for a given length and returns it as a NumPy array. It takes the window length, an optional data type, and an optional output array as inputs. The function calculates the window values using a specific mathematical formula involving sine functions and appends the rounded results to a list. Finally, it converts the list to a NumPy array with the specified data type and returns it.\"\"\"\n\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n\n    for i in range(window_length):\n        x = i / (window_length - 1)\n        out[i] = np.sin(np.pi / 2 * np.sin(np.pi * x) ** 2)\n\n    return out", "CodeBleu_score": 39.0550749466936}, {"code": "import numpy as np\n\ndef vorbis_window(\n    window_length: int,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"The function vorbis_window generates a Vorbis window for a given length and returns it as a NumPy array. It takes the window length, an optional data type, and an optional output array as inputs. The function calculates the window values using a specific mathematical formula involving sine functions and appends the rounded results to a list. Finally, it converts the list to a NumPy array with the specified data type and returns it.\"\"\"\n\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n\n    for i in range(window_length):\n        x = i / (window_length - 1)\n        out[i] = np.sin(np.pi / 2 * np.sin(np.pi * x) ** 2)\n\n    return out", "CodeBleu_score": 38.985691663488794}, {"code": "import numpy as np\n\ndef vorbis_window(\n    window_length: int,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"The function vorbis_window generates a Vorbis window for a given length and returns it as a NumPy array. It takes the window length, an optional data type, and an optional output array as inputs. The function calculates the window values using a specific mathematical formula involving sine functions and appends the rounded results to a list. Finally, it converts the list to a NumPy array with the specified data type and returns it.\"\"\"\n\n    if out is None:\n        out = np.empty(window_length, dtype=dtype)\n\n    for i in range(window_length):\n        x = i / (window_length - 1)\n        out[i] = np.sin(np.pi / 2 * np.sin(np.pi * x) ** 2)\n\n    return out", "CodeBleu_score": 38.985691663488794}]}
{"instance_id": "473296f771a9", "generation_result": [{"code": "import numpy as np\n\ndef polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    The function polyval evaluates a polynomial at specific values given by the coefficients and input array. \n    It ensures precision by setting a precise mode and promoting data types of the coefficients and inputs to a common type. \n    The function uses numpy's polyval to compute the polynomial values and then converts the result to an array with the promoted data type. \n    Finally, it returns the computed array.\n    \"\"\"\n    # Promote data types of coefficients and inputs to a common type\n    common_type = np.promote_types(coeffs.dtype, x.dtype)\n    coeffs = coeffs.astype(common_type)\n    x = x.astype(common_type)\n    \n    # Compute the polynomial values using numpy's polyval\n    result = np.polyval(coeffs, x)\n    \n    # Convert the result to the promoted data type\n    result = result.astype(common_type)\n    \n    return result", "CodeBleu_score": 50.14274230977838}, {"code": "import numpy as np\n\ndef polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    The function polyval evaluates a polynomial at specific values given by the coefficients and input array.\n    It ensures precision by setting a precise mode and promoting data types of the coefficients and inputs to a common type.\n    The function uses numpy's polyval to compute the polynomial values and then converts the result to an array with the promoted data type.\n    Finally, it returns the computed array.\n    \"\"\"\n    # Promote data types of the coefficients and inputs to a common type\n    common_type = np.promote_types(coeffs.dtype, x.dtype)\n    coeffs = coeffs.astype(common_type)\n    x = x.astype(common_type)\n    \n    # Compute the polynomial values using numpy's polyval\n    result = np.polyval(coeffs, x)\n    \n    # Convert the result to the promoted data type\n    result = result.astype(common_type)\n    \n    return result", "CodeBleu_score": 50.14274230977838}, {"code": "import numpy as np\n\ndef polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    The function polyval evaluates a polynomial at specific values given by the coefficients and input array. \n    It ensures precision by setting a precise mode and promoting data types of the coefficients and inputs to a common type. \n    The function uses numpy's polyval to compute the polynomial values and then converts the result to an array with the promoted data type. \n    Finally, it returns the computed array.\n    \"\"\"\n    # Promote data types of coefficients and inputs to a common type\n    common_type = np.promote_types(coeffs.dtype, x.dtype)\n    coeffs = coeffs.astype(common_type)\n    x = x.astype(common_type)\n    \n    # Compute the polynomial values using numpy's polyval\n    result = np.polyval(coeffs, x)\n    \n    # Convert the result to the promoted data type\n    result = result.astype(common_type)\n    \n    return result", "CodeBleu_score": 50.14274230977838}]}
{"instance_id": "b8455221cb2a", "generation_result": [{"code": "import numpy as np\nfrom typing import Optional, Union, Sequence\n\ndef amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"The function amax computes the maximum value of a numpy array x along a specified axis or axes.\n    It accepts parameters for the axis or axes to operate on, whether to keep the dimensions of the original array, and an optional output array to store the result.\n    If the axis parameter is a list, it is converted to a tuple.\n    The function uses np.amax to calculate the maximum value, and ensures that if the result is a scalar, it is returned as a numpy array.\n    Otherwise, the result is returned as is.\"\"\"\n\n    # Convert axis to tuple if it's a list\n    if isinstance(axis, list):\n        axis = tuple(axis)\n\n    # Compute the maximum\n    result = np.amax(x, axis=axis, keepdims=keepdims, out=out)\n\n    # Ensure the result is a numpy array if it's a scalar\n    if np.isscalar(result):\n        result = np.array(result)\n\n    return result", "CodeBleu_score": 54.22700573080091}, {"code": "import numpy as np\nfrom typing import Optional, Union, Sequence\n\ndef amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"The function amax computes the maximum value of a NumPy array x along a specified axis or axes.\n    It accepts parameters for the axis or axes to operate on, whether to keep the dimensions of the original array, and an optional output array to store the result.\n    If the axis parameter is a list, it is converted to a tuple.\n    The function uses np.amax to calculate the maximum value, and ensures that if the result is a scalar, it is returned as a NumPy array.\n    Otherwise, the result is returned as is.\"\"\"\n\n    # Convert axis to tuple if it's a list\n    if isinstance(axis, list):\n        axis = tuple(axis)\n\n    # Compute the maximum\n    result = np.amax(x, axis=axis, keepdims=keepdims, out=out)\n\n    # Ensure the result is a numpy array if it's a scalar\n    if np.isscalar(result):\n        result = np.array(result)\n\n    return result", "CodeBleu_score": 54.22700573080091}, {"code": "import numpy as np\nfrom typing import Optional, Union, Sequence\n\ndef amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"\n    The function amax computes the maximum value of a NumPy array x along a specified axis or axes.\n    It accepts parameters for the axis or axes to operate on, whether to keep the dimensions of the original array,\n    and an optional output array to store the result.\n    If the axis parameter is a list, it is converted to a tuple.\n    The function uses np.amax to calculate the maximum value, and ensures that if the result is a scalar,\n    it is returned as a NumPy array.\n    Otherwise, the result is returned as is.\n    \"\"\"\n    # Convert axis to tuple if it's a list\n    if isinstance(axis, list", "CodeBleu_score": 28.228484520726266}]}
{"instance_id": "f72489f434a3", "generation_result": [{"code": "import numpy as np\nimport numpy.typing as npt\nfrom typing import Optional\n\ndef copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None\n) -> np.ndarray:\n    \"\"\"\n    The function copysign takes two array-like inputs x1 and x2, along with an optional output array out.\n    It promotes the types of x1 and x2 to a common type using promote_types_of_inputs.\n    If x1 is not of a float data type, both x1 and x2 are cast to the default float data type.\n    The function then applies the np.copysign operation, which assigns the sign of each element in x2 to the corresponding element in x1,\n    and returns the result, optionally storing it in the provided output array out.\n    \"\"\"\n    x1_arr = np.asarray(x1)\n    x2_arr = np.asarray(x2)\n\n    # Promote input types to a common type\n    common_type = np.promote_types(x1_arr.dtype, x2_arr.dtype)\n    x1_arr = x1_arr.astype(common_type)\n    x2_arr = x2_arr.astype(common_type)\n    \n    # Ensure both arrays are of float type\n    if not np.issubdtype(common_type, np.floating):\n        float_type = np.float64  # default float data type\n        x1_arr = x1_arr.astype(float_type)\n        x2_arr = x2_arr.astype(float_type)\n\n    # Apply np.copysign\n    result = np.copysign(x1_arr, x2_arr, out=out)\n\n    return result", "CodeBleu_score": 37.45632751261508}, {"code": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray\nimport numpy as np\nimport numpy.typing as npt\nfrom typing import Optional\n\ndef promote_types_of_inputs(a, b):\n    \"\"\"Promote the numpy data types of two arrays to a common type.\"\"\"\n    return np.find_common_type([a.dtype, b.dtype], [])\n\ndef copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"The function copysign takes two array-like inputs x1 and x2, along with an optional output array out.\n    It promotes the types of x1 and x2 to a common type using promote_types_of_inputs.\n    If x1 is not of a float data type, both x1 and x2 are cast to the default float data type.\n    The function then applies the np.copysign operation, which assigns the sign of each element in x2 to the corresponding element in x1, and returns the result, optionally storing it in the provided output array out.\"\"\"\n    \n    # Convert x1 and x2 to numpy arrays\n    x1 = np.asarray(x1)\n    x2 = np.asarray(x2)\n\n    # Promote types of x1 and x2 to a common type\n    common_type = promote_types_of_inputs(x1, x2)\n\n    # Cast x1 and x2 to the promoted type\n    x1 = x1.astype(common_type)\n    x2 = x2.astype(common_type)\n\n    # Ensure both arrays are of float type, if not already\n    if not np.issubdtype(x1.dtype, np.floating):\n        x1 = x1.astype(float)\n        x2 = x2.astype(float)\n\n    # Apply np.copysign operation\n    result = np.copysign(x1, x2, out=out)  # np.copysign also supports 'out' directly\n\n    return result\n\n# Example usage:\nx1 = [1, -2, 3]\nx2 = [-1, -3, -5]\nprint(copysign(x1, x2))\n", "CodeBleu_score": 41.709330547140176}, {"code": "import numpy as np\nfrom numpy.typing import ArrayLike\nfrom typing import Optional\nimport numpy.typing as npt\n\ndef copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"\n    The function copysign takes two array-like inputs x1 and x2, along with an optional output array out.\n    It promotes the types of x1 and x2 to a common type using internal or hypothetical promote_types_of_inputs.\n    If x1 is not of a float data type, both x1 and x2 are cast to the default float data type.\n    The function then applies the np.copysign operation, which assigns the sign of each element in x2 to the\n    corresponding element in x1, and returns the result, optionally storing it in the provided output array out.\n    \"\"\"\n    \n    # Convert inputs to numpy arrays\n    x1_arr = np.asarray(x1)\n    x2_arr = np.asarray(x2)\n    \n    # Determine the appropriate data type, assumed function: promote_types_of_inputs()\n    # Since promote_types_of_inputs is hypothetical, let's determine the dtype manually.\n    result_type = np.promote_types(x1_arr.dtype, x2_arr.dtype)\n    if result_type.kind != 'f': # Check if not floating point type\n        result_type = np.float64  # Default to float64 if result type isn't float\n\n    # Cast to determined type or float\n    x1_arr = x1_arr.astype(result_type)\n    x2_arr = x2_arr.astype(result_type)\n\n    # Perform np.copysign operation\n    result = np.copysign(x1_arr, x2_arr, out=out)\n\n    return result", "CodeBleu_score": 36.45323855325605}]}
{"instance_id": "55c0d77e4dc2", "generation_result": [{"code": "import numpy as np\nfrom typing import Optional, Union, Tuple\n\ndef count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"\n    The function count_nonzero counts the number of non-zero elements in an array along a specified axis, with options to keep dimensions, specify output data type, and use an output array. It converts axis lists to tuples and uses np.count_nonzero for counting. The function handles scalar results by converting them to an array with the specified dtype and returns the result.\n    \"\"\"\n    # Convert axis lists to tuples if necessary\n    if isinstance(axis, list):\n        axis = tuple(axis)\n\n    # Use numpy's count_nonzero function to count non-zero elements\n    result = np.count_nonzero(a, axis=axis, keepdims=keepdims)\n\n    # If dtype is specified, ensure the result is of that dtype\n    if dtype is not None:\n        result = np.array(result, dtype=dtype)\n\n    # If an output array is provided, assign the result to it\n    if out is not None:\n        out[...] = result\n        return out\n\n    return result", "CodeBleu_score": 69.04710343627339}, {"code": "import numpy as np\nfrom typing import Optional, Union, Tuple\n\ndef count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"\n    The function count_nonzero counts the number of non-zero elements in an array along a specified axis, with options to keep dimensions, specify output data type, and use an output array. It converts axis lists to tuples and uses np.count_nonzero for counting. The function handles scalar results by converting them to an array with the specified dtype and returns the result.\n    \"\"\"\n    # Convert axis lists to tuples if necessary\n    if isinstance(axis, list):\n        axis = tuple(axis)\n\n    # Use numpy's count_nonzero function to count non-zero elements\n    result = np.count_nonzero(a, axis=axis, keepdims=keepdims)\n\n    # If dtype is specified, ensure the result is of that dtype\n    if dtype is not None:\n        result = np.array(result, dtype=dtype)\n\n    # If an output array is provided, assign the result to it\n    if out is not None:\n        out[...] = result\n        return out\n\n    return result", "CodeBleu_score": 69.04710343627339}, {"code": "import numpy as np\nfrom typing import Optional, Union, Tuple\n\ndef count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"\n    The function count_nonzero counts the number of non-zero elements in an array along a specified axis, with options to keep dimensions, specify output data type, and use an output array. It converts axis lists to tuples and uses np.count_nonzero for counting. The function handles scalar results by converting them to an array with the specified dtype and returns the result.\n    \"\"\"\n    # Convert axis to tuple if it's a list\n    if isinstance(axis, list):\n        axis = tuple(axis)\n\n    # Use numpy's count_nonzero function to count non-zero elements\n    result = np.count_nonzero(a, axis=axis, keepdims=keepdims)\n\n    # If dtype is specified, ensure the result is of that dtype\n    if dtype is not None:\n        result = np.array(result, dtype=dtype)\n\n    # If an output array is provided, assign the result to it\n    if out is not None:\n        out[...] = result\n        return out\n\n    return result", "CodeBleu_score": 69.04710343627339}]}
{"instance_id": "784bf7d24fc2", "generation_result": [{"code": "import numpy as np\nfrom typing import Union, List, Optional\n\ndef gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]:\n    \"\"\"\n    The function gradient computes the numerical gradient of an input array along specified axes using given spacing and edge order.\n    It accepts an array, an optional spacing parameter which can be an int, list, or tuple, an optional axis specification, and an edge order.\n    If the spacing is a single value, it calls the gradient computation with that spacing. Otherwise, it unpacks and uses the spacing list or tuple.\n    It returns the computed gradient as an array or a list of arrays.\n    \"\"\"\n    if isinstance(spacing, (list, tuple)):\n        if axis is None:\n            # Compute gradient for each dimension with corresponding spacing\n            return [np.gradient(x, *spacing, edge_order=edge_order)]\n        elif isinstance(axis, (list, tuple)):\n            # Compute gradient along each specified axis with corresponding spacing\n            return [np.gradient(x, spacing[i], axis=a, edge_order=edge_order) for i, a in enumerate(axis)]\n        else:\n            # Compute gradient along a single axis with corresponding spacing\n            return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n    else:\n        if axis is None:\n            # Compute gradient along all axes with uniform spacing\n            return np.gradient(x, spacing, edge_order=edge_order)\n        elif isinstance(axis, (list, tuple)):\n            # Compute gradient along each specified axis with uniform spacing\n            return [np.gradient(x, spacing, axis=a, edge_order=edge_order) for a in axis]\n        else:\n            # Compute gradient along a single axis with uniform spacing\n            return np.gradient(x, spacing, axis=axis, edge_order=edge_order)", "CodeBleu_score": 61.6204109467842}, {"code": "import numpy as np\nfrom typing import Union, List, Optional\n\ndef gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]:\n    \"\"\"\n    The function gradient computes the numerical gradient of an input array along specified axes using given spacing and edge order.\n    It accepts an array, an optional spacing parameter which can be an int, list, or tuple, an optional axis specification, and an edge order.\n    If the spacing is a single value, it calls the gradient computation with that spacing. Otherwise, it unpacks and uses the spacing list or tuple.\n    It returns the computed gradient as an array or a list of arrays.\n    \"\"\"\n    # Check if spacing is a single int or a sequence and adjust accordingly\n    if isinstance(spacing, (list, tuple)):\n        if len(spacing) == 1:\n            spacing = spacing[0]\n    # Compute the gradient using numpy's gradient function\n    return np.gradient(x, spacing, axis=axis, edge_order=edge_order)", "CodeBleu_score": 58.70516332253097}, {"code": "import numpy as np\nfrom typing import Union, List, Optional\n\ndef gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]:\n    \"\"\"The function gradient computes the numerical gradient of an input array along specified axes using given spacing and edge order. It accepts an array, an optional spacing parameter which can be an int, list, or tuple, an optional axis specification, and an edge order. If the spacing is a single value, it calls the gradient computation with that spacing. Otherwise, it unpacks and uses the spacing list or tuple. It returns the computed gradient as an array or a list of arrays.\"\"\"\n\n    # Check if spacing is a single value or a list/tuple\n    if isinstance(spacing, (list, tuple)):\n        # If spacing is a list or tuple, ensure it matches the dimensions of the axes specified\n        if axis is not None and isinstance(axis, (list, tuple)):\n            if len(spacing) != len(axis):\n                raise ValueError(\"Length of spacing must match the number of axes.\")\n            return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)\n        else:\n            # If no axis is specified but spacing is a list or tuple, apply spacing to all dimensions\n            return np.gradient(x, *spacing, edge_order=edge_order)\n    else:\n        # If spacing is a single value, apply it uniformly\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)", "CodeBleu_score": 64.5691931975605}]}
{"instance_id": "efa0a51af6bc", "generation_result": [{"code": "import numpy as np\n\ndef xlogy(x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None) -> np.ndarray:\n    \"\"\"\n    The function xlogy takes two NumPy arrays x and y, along with an optional output array out.\n    It promotes the types of x and y to a common type using promote_types_of_inputs.\n    If all elements in x are zero, the function returns 0.0.\n    Otherwise, it computes the element-wise product of x and the natural logarithm of y, returning the result.\n    \"\"\"\n    # Promote types of x and y to a common type\n    dtype = np.promote_types(x.dtype, y.dtype)\n    x = x.astype(dtype)\n    y = y.astype(dtype)\n\n    # Check if all elements in x are zero\n    if np.all(x == 0):\n        return np.zeros_like(x, dtype=dtype) if out is None else np.zeros_like(out, dtype=dtype)\n\n    # Compute element-wise x * log(y)\n    result = x * np.log(y)\n\n    # If an output array is provided, use it to store the result\n    if out is not None:\n        out[:] = result\n        return out\n    else:\n        return result", "CodeBleu_score": 39.44596426119741}, {"code": "import numpy as np\n\ndef xlogy(x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None) -> np.ndarray:\n    \"\"\"The function xlogy takes two NumPy arrays x and y, along with an optional output array out.\n    It promotes the types of x and y to a common type using promote_types_of_inputs.\n    If all elements in x are zero, the function returns 0.0.\n    Otherwise, it computes the element-wise product of x and the natural logarithm of y, returning the result.\"\"\"\n    \n    # Promote types of x and y to a common type\n    dtype = np.promote_types(x.dtype, y.dtype)\n    x = x.astype(dtype)\n    y = y.astype(dtype)\n    \n    # Check if all elements in x are zero\n    if np.all(x == 0):\n        return np.zeros_like(x)\n    \n    # Compute x * log(y) element-wise\n    result = x * np.log(y)\n    \n    # If an output array is provided, store the result in it\n    if out is not None:\n        out[:] = result\n        return out\n    \n    return result", "CodeBleu_score": 40.71747694220935}, {"code": "import numpy as np\n\ndef xlogy(x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None) -> np.ndarray:\n    \"\"\"\n    The function xlogy takes two NumPy arrays x and y, along with an optional output array out.\n    It promotes the types of x and y to a common type using promote_types_of_inputs.\n    If all elements in x are zero, the function returns 0.0.\n    Otherwise, it computes the element-wise product of x and the natural logarithm of y, returning the result.\n    \"\"\"\n    # Promote types of x and y to a common type\n    dtype = np.promote_types(x.dtype, y.dtype)\n    x = x.astype(dtype)\n    y = y.astype(dtype)\n\n    # Check if all elements in x are zero\n    if np.all(x == 0):\n        return np.zeros_like(x, dtype=dtype) if out is None else np.zeros_like(out, dtype=dtype)\n\n    # Compute element-wise x * log(y)\n    result = x * np.log(y)\n\n    # If an output array is provided, use it to store the result\n    if out is not None:\n        out[:] = result\n        return out\n    else:\n        return result", "CodeBleu_score": 39.44596426119741}]}
{"instance_id": "ac1645284260", "generation_result": [{"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channels_last\"):\n    \"\"\"\n    The function _determine_depth_max_pooling adjusts the input tensor for depth-wise max pooling. \n    It calls a helper function to get updated kernel size, strides, and a flag indicating if depth pooling is needed. \n    If depth pooling is required, the input tensor is transposed to reorder its dimensions. \n    The function returns the possibly transposed tensor, updated kernel size, strides, and the depth pooling flag.\n\n    Args:\n    x (tensor): Input tensor.\n    kernel (tuple): The size of the window to take a max over.\n    strides (tuple): The stride of the window.\n    dims (tuple): The dimensions to apply max pooling on.\n    data_format (str): Format of the data, either 'channels_last' or 'channels_first'.\n\n    Returns:\n    tuple: A tuple containing the possibly transposed tensor, updated kernel size, strides, and a depth pooling flag.\n    \"\"\"\n    # Helper function to determine if depth pooling is needed and to update kernel and strides\n    def update_pooling_parameters(kernel, strides, dims):\n        # Example logic for updating parameters\n        depth_pooling_needed = len(dims) > 1\n        updated_kernel = tuple(kernel[d] for d in dims)\n        updated_strides = tuple(strides[d] for d in dims)\n        return updated_kernel, updated_strides, depth_pooling_needed\n\n    # Call the helper function\n    updated_kernel, updated_strides, depth_pooling_needed = update_pooling_parameters(kernel, strides, dims)\n\n    # Transpose tensor if necessary\n    if depth_pooling_needed:\n        if data_format == \"channels_last\":\n            # Example transpose for 'channels_last' to bring depth dimensions to the front\n            new_order = [x.ndim - 1] + list(range(x.ndim - 1))\n            x = x.transpose(new_order)\n        elif data_format == \"channels_first\":\n            # Example transpose for 'channels_first' to bring depth dimensions to the appropriate order\n            new_order = [0] + list(dims) + [d for d in range(1, x.ndim) if d not in dims]\n            x = x.transpose(new_order)\n\n    return x, updated_kernel, updated_strides, depth_pooling_needed", "CodeBleu_score": 28.42620089789626}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channels_last\"):\n    \"\"\"\n    The function _determine_depth_max_pooling adjusts the input tensor for depth-wise max pooling.\n    It calls a helper function to get updated kernel size, strides, and a flag indicating if depth pooling is needed.\n    If depth pooling is required, the input tensor is transposed to reorder its dimensions.\n    The function returns the possibly transposed tensor, updated kernel size, strides, and the depth pooling flag.\n\n    Args:\n    x (tensor): Input tensor.\n    kernel (tuple): The size of the window to take a max over.\n    strides (tuple): The stride of the window.\n    dims (tuple): The dimensions to apply max pooling on.\n    data_format (str): Format of the data, either 'channels_last' or 'channels_first'.\n\n    Returns:\n    tuple: A tuple containing the possibly transposed tensor, updated kernel size, strides, and a depth pooling flag.\n    \"\"\"\n\n    # Helper function to determine if depth pooling is needed and to update kernel and strides\n    def update_pooling_parameters(kernel, strides, dims):\n        # Example logic to update kernel and strides based on dimensions\n        new_kernel = tuple(kernel[d] if d in dims else 1 for d in range(len(kernel)))\n        new_strides = tuple(strides[d] if d in dims else 1 for d in range(len(strides)))\n        need_depth_pooling = len(dims) > 0 and any(d != 0 for d in dims)\n        return new_kernel, new_strides, need_depth_pooling\n\n    # Update pooling parameters\n    updated_kernel, updated_strides, need_depth_pooling = update_pooling_parameters(kernel, strides, dims)\n\n    # Transpose tensor if needed\n    if need_depth_pooling:\n        if data_format == \"channels_last\":\n            # Example transpose for channels_last format\n            x = x.transpose(0, 3, 1, 2)  # Move channels to second dimension\n        elif data_format == \"channels_first\":\n            # Example transpose for channels_first format\n            x = x.transpose(0, 2, 3, 1)  # Move channels to last dimension\n\n    return x, updated_kernel, updated_strides, need_depth_pooling", "CodeBleu_score": 29.28597013149862}, {"code": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channels_last\"):\n    \"\"\"\n    The function _determine_depth_max_pooling adjusts the input tensor for depth-wise max pooling. \n    It calls a helper function to get updated kernel size, strides, and a flag indicating if depth pooling is needed. \n    If depth pooling is required, the input tensor is transposed to reorder its dimensions. \n    The function returns the possibly transposed tensor, updated kernel size, strides, and the depth pooling flag.\n    \n    Args:\n    x (Tensor): The input tensor.\n    kernel (tuple): The size of the pooling window.\n    strides (tuple): The stride of the pooling operation.\n    dims (tuple): The dimensions to pool over.\n    data_format (str): Format of the data, either 'channels_last' or 'channels_first'.\n    \n    Returns:\n    Tensor: The adjusted tensor after applying or preparing for depth-wise max pooling.\n    tuple: Updated kernel size.\n    tuple: Updated strides.\n    bool: Flag indicating if depth pooling was applied.\n    \"\"\"\n    \n    # Helper function to determine if depth pooling is needed and adjust parameters\n    def adjust_pooling_parameters(kernel, strides, dims):\n        # Example logic for adjusting parameters based on some conditions\n        need_depth_pooling = len(dims) > 1\n        if need_depth_pooling:\n            # Adjust kernel and strides for depth pooling\n            kernel = tuple(kernel[d] for d in dims)\n            strides = tuple(strides[d] for d in dims)\n        return kernel, strides, need_depth_pooling\n    \n    # Call the helper function\n    updated_kernel, updated_strides, need_depth_pooling = adjust_pooling_parameters(kernel, strides, dims)\n    \n    # Transpose tensor if needed\n    if need_depth_pooling:\n        if data_format == \"channels_last\":\n            # Example transpose for 'channels_last' to bring depth dimensions to the front\n            new_order = [x.ndim - 1] + list(range(x.ndim - 1))\n            x = x.transpose(new_order)\n        elif data_format == \"channels_first\":\n            # Example transpose for 'channels_first' to bring depth dimensions to the appropriate order\n            new_order = [0] + [d + 1 for d in dims] + [i for i in range(1, x.ndim) if i - 1 not in dims]\n            x = x.transpose(new_order)\n    \n    return x, updated_kernel, updated_strides, need_depth_pooling", "CodeBleu_score": 29.50467684489221}]}
{"instance_id": "18ff0a4f7550", "generation_result": [{"code": "import numpy as np\nfrom numpy.random import binomial\nfrom typing import Optional\n\ndef dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"The function dropout1d applies 1D dropout regularization to a NumPy array x with a given probability prob.\n    If training is True, it determines the shape of x and whether it is batched.\n    For \"NCW\" data format, it transposes x accordingly.\n    A binary mask is generated using a binomial distribution, and x is scaled by 1/(1 - prob) where the mask is true, otherwise set to zero.\n    If data_format is \"NCW\", it transposes the result back.\n    If training is False, the function returns x unchanged.\"\"\"\n\n    if not training:\n        return x\n\n    if data_format == \"NCW\":\n        x = x.transpose(0, 2, 1)  # Transpose to NCW if necessary\n\n    # Generate the dropout mask\n    mask = binomial(n=1, p=1 - prob, size=x.shape)\n\n    # Apply the dropout mask\n    x = x * mask / (1 - prob)\n\n    if data_format == \"NCW\":\n        x = x.transpose(0, 2, 1)  # Transpose back to original format if necessary\n\n    if out is not None:\n        np.copyto(out, x)\n        return out\n\n    return x", "CodeBleu_score": 43.25018827063109}, {"code": "import numpy as np\nfrom numpy.random import binomial\nfrom typing import Optional\n\ndef dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"The function dropout1d applies 1D dropout regularization to a NumPy array x with a given probability prob.\n    If training is True, it determines the shape of x and whether it is batched.\n    For \"NCW\" data format, it transposes x accordingly.\n    A binary mask is generated using a binomial distribution, and x is scaled by 1/(1 - prob) where the mask is true, otherwise set to zero.\n    If data_format is \"NCW\", it transposes the result back.\n    If training is False, the function returns x unchanged.\"\"\"\n\n    if not training:\n        return x\n\n    if data_format == \"NCW\":\n        x = x.transpose(0, 2, 1)  # Transpose to NCW if necessary\n\n    # Generate the dropout mask\n    mask = binomial(n=1, p=1 - prob, size=x.shape)\n\n    # Apply the dropout mask\n    x = x * mask / (1 - prob)\n\n    if data_format == \"NCW\":\n        x = x.transpose(0, 2, 1)  # Transpose back to original format if necessary\n\n    if out is not None:\n        out[:] = x\n        return out\n\n    return x", "CodeBleu_score": 41.78927486710408}, {"code": "import numpy as np\nfrom typing import Optional\n\ndef dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"The function dropout1d applies 1D dropout regularization to a NumPy array x with a given probability prob.\n    If training is True, it determines the shape of x and whether it is batched.\n    For \"NCW\" data format, it transposes x accordingly.\n    A binary mask is generated using a binomial distribution, and x is scaled by 1/(1 - prob) where the mask is true, otherwise set to zero.\n    If data_format is \"NCW\", it transposes the result back.\n    If training is False, the function returns x unchanged.\"\"\"\n\n    if not training:\n        return x\n\n    if data_format == \"NCW\":\n        x = x.transpose(0, 2, 1)  # Transpose from NCW to NWC\n\n    # Generate the dropout mask\n    mask = np.random.binomial(1, 1 - prob, x.shape).astype(x.dtype)\n\n    # Apply the dropout mask\n    x = x * mask / (1 - prob)\n\n    if data_format == \"NCW\":\n        x = x.transpose(0, 2, 1)  # Transpose back from NWC to NCW\n\n    if out is not None:\n        np.copyto(out, x)\n        return out\n\n    return x", "CodeBleu_score": 48.59590178089248}]}
{"instance_id": "0ce45eb7cbd9", "generation_result": [{"code": "import numpy as np\nfrom typing import Optional, Tuple\n\ndef top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"The function top_k retrieves the top k elements along a specified axis of a NumPy array. It accepts parameters to control whether the largest or smallest elements are selected, whether the result should be sorted, and allows for optional output storage. The function calculates the appropriate indices for the top k elements, extracts these elements from the input array, and returns both the values and their indices as a named tuple.\"\"\"\n\n    # Determine the direction for partitioning\n    if largest:\n        # We use negative k because 'argpartition' finds the smallest values by default\n        kth = -k\n    else:\n        kth = k - 1\n\n    # Perform argpartition to get the indices of the top k elements\n    if largest:\n        index_array = np.argpartition(x, kth, axis=axis)\n        top_k_indices = np.take(index_array, -k, axis=axis)\n    else:\n        index_array = np.argpartition(x, kth, axis=axis)\n        top_k_indices = np.take(index_array, range(k), axis=axis)\n\n    # Extract the top k values using the indices\n    top_k_values = np.take_along_axis(x, top_k_indices, axis=axis)\n\n    # Optionally sort the results if required\n    if sorted:\n        if largest:\n            top_k_indices = np.argsort(-top_k_values, axis=axis)\n        else:\n            top_k_indices = np.argsort(top_k_values, axis=axis)\n        top_k_values = np.take_along_axis(top_k_values, top_k_indices, axis=axis)\n        top_k_indices = np.take_along_axis(top_k_indices, top_k_indices, axis=axis)\n\n    # Prepare output\n    if out is not None:\n        out[0][:] = top_k_values\n        out[1][:] = top_k_indices\n        return out\n    else:\n        return top_k_values, top_k_indices", "CodeBleu_score": 39.94540123352192}, {"code": "import numpy as np\nfrom typing import Optional, Tuple\n\ndef top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    The function top_k retrieves the top k elements along a specified axis of a NumPy array. It accepts parameters to control whether the largest or smallest elements are selected, whether the result should be sorted, and allows for optional output storage. The function calculates the appropriate indices for the top k elements, extracts these elements from the input array, and returns both the values and their indices as a named tuple.\n    \"\"\"\n    # Determine the order in which to sort the elements\n    if largest:\n        order = -np.sort(-x, axis=axis)\n    else:\n        order = np.sort(x, axis=axis)\n\n    # Get the indices of the top k elements\n    if largest:\n        indices = np.argpartition(-x, k-1, axis=axis)\n    else:\n        indices = np.argpartition(x, k-1, axis=axis)\n\n    # Slice out the top k elements\n    if axis == -1:\n        top_k_values = order[..., :k]\n        top_k_indices = indices[..., :k]\n    else:\n        index_array = np.ogrid[[slice(x.shape[dim]) if dim != axis else slice(None) for dim in range(x.ndim)]]\n        index_array[axis] = indices.take(range(k), axis=axis)\n        top_k_values = x[tuple(index_array)]\n        top_k_indices = indices[tuple(index_array)]\n\n    # Optionally sort the results if required\n    if sorted:\n        if axis == -1:\n            sort_indices = np.argsort(-top_k_values if largest else top_k_values, axis=axis)\n        else:\n            sort_indices = np.argsort(-top_k_values if largest else top_k_values, axis=axis)\n        top_k_values = np.take_along_axis(top_k_values, sort_indices, axis=axis)\n        top_k_indices = np.take_along_axis(top_k_indices, sort_indices, axis=axis)\n\n    # Handle the optional output\n    if out is not None:\n        out[0][:] = top_k_values\n        out[1][:] = top_k_indices\n        return out\n    else:\n        return top_k_values, top_k_indices", "CodeBleu_score": 41.763310723523425}, {"code": "import numpy as np\nfrom typing import Optional, Tuple\n\ndef top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    The function top_k retrieves the top k elements along a specified axis of a NumPy array. It accepts parameters to control whether the largest or smallest elements are selected, whether the result should be sorted, and allows for optional output storage. The function calculates the appropriate indices for the top k elements, extracts these elements from the input array, and returns both the values and their indices as a named tuple.\n    \"\"\"\n    # Determine the order based on the 'largest' flag\n    if largest:\n        order = np.argpartition(x, -k, axis=axis)[-k:]\n    else:\n        order = np.argpartition(x, k, axis=axis)[:k]\n\n    # Extract the top k elements\n    top_k_values = np.take_along_axis(x, order, axis=axis)\n\n    # Sort if required\n    if sorted:\n        sort_indices = np.argsort(top_k_values, axis=axis)\n        if largest:\n            sort_indices = sort_indices[..., ::-1]\n        top_k_values = np.take_along_axis(top_k_values, sort_indices, axis=axis)\n        order = np.take_along_axis(order, sort_indices, axis=axis)\n\n    # Prepare output\n    if out is not None:\n        out[0][:] = top_k_values\n        out[1][:] = order\n        return out\n    else:\n        return top_k_values, order", "CodeBleu_score": 40.503120260397125}]}
{"instance_id": "1424f9541998", "generation_result": [{"code": "import numpy as np\nfrom typing import Optional\n\ndef l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"\n    The function l1_normalize takes a NumPy array x, an optional axis, and an optional output array out.\n    If axis is None, it calculates the L1 norm by summing the absolute values of x reshaped into a 1D array, \n    then creates a denormalization array of the same shape as x filled with this norm.\n    If axis is specified, it calculates the L1 norm along the given axis and creates a denormalization array \n    by dividing the norm by the absolute values of x plus a small epsilon to prevent division by zero.\n    The function returns the element-wise division of x by the denormalization array.\n    \"\"\"\n    epsilon = 1e-10  # Small epsilon to prevent division by zero\n\n    if axis is None:\n        # Calculate the L1 norm for the entire array as a 1D array\n        norm = np.sum(np.abs(x))\n        # Create a denormalization array filled with the norm\n        denorm_array = np.full_like(x, fill_value=norm)\n    else:\n        # Calculate the L1 norm along the specified axis\n        norm = np.sum(np.abs(x), axis=axis, keepdims=True)\n        # Create a denormalization array\n        denorm_array = norm + epsilon\n\n    # Perform element-wise division\n    normalized_x = x / denorm_array\n\n    # If an output array is provided, store the result there\n    if out is not None:\n        np.copyto(out, normalized_x)\n        return out\n    else:\n        return normalized_x", "CodeBleu_score": 52.28602265205423}, {"code": "import numpy as np\nfrom typing import Optional\n\ndef l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"\n    The function l1_normalize takes a NumPy array x, an optional axis, and an optional output array out.\n    If axis is None, it calculates the L1 norm by summing the absolute values of x reshaped into a 1D array, then creates a denormalization array of the same shape as x filled with this norm.\n    If axis is specified, it calculates the L1 norm along the given axis and creates a denormalization array by dividing the norm by the absolute values of x plus a small epsilon to prevent division by zero.\n    The function returns the element-wise division of x by the denormalization array.\n    \"\"\"\n    epsilon = 1e-10  # Small epsilon to prevent division by zero\n    if axis is None:\n        norm = np.sum(np.abs(x))\n        denorm_array = np.full(x.shape, norm)\n    else:\n        norm = np.sum(np.abs(x), axis=axis, keepdims=True)\n        denorm_array = norm + epsilon\n\n    normalized_x = x / denorm_array\n    if out is not None:\n        out[:] = normalized_x\n        return out\n    return normalized_x", "CodeBleu_score": 50.38249849080268}, {"code": "import numpy as np\nfrom typing import Optional\n\ndef l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"\n    The function l1_normalize takes a NumPy array x, an optional axis, and an optional output array out.\n    If axis is None, it calculates the L1 norm by summing the absolute values of x reshaped into a 1D array, then creates a denormalization array of the same shape as x filled with this norm.\n    If axis is specified, it calculates the L1 norm along the given axis and creates a denormalization array by dividing the norm by the absolute values of x plus a small epsilon to prevent division by zero.\n    The function returns the element-wise division of x by the denormalization array.\n    \"\"\"\n    epsilon = 1e-10  # Small epsilon to prevent division by zero\n    if axis is None:\n        norm = np.sum(np.abs(x))\n        denorm_array = np.full(x.shape, norm)\n    else:\n        norm = np.sum(np.abs(x), axis=axis, keepdims=True)\n        denorm_array = norm + epsilon\n\n    normalized_x = x / denorm_array\n    if out is not None:\n        out[:] = normalized_x\n        return out\n    return normalized_x", "CodeBleu_score": 50.38249849080268}]}
{"instance_id": "4c3a386a1f05", "generation_result": [{"code": "import numpy as np\nfrom typing import Union, Optional, Sequence\n\ndef beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"The function beta generates samples from a beta distribution given the parameters alpha and beta. It optionally accepts shape, device, dtype, seed, and out parameters. The function checks and determines the shape of the output, sets the random seed if provided, and returns an array of samples from the beta distribution with the specified dtype.\"\"\"\n\n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Determine the output shape\n    if shape is None:\n        if isinstance(alpha, np.ndarray):\n            shape = alpha.shape\n        elif isinstance(beta, np.ndarray):\n            shape = beta.shape\n        else:\n            shape = ()\n\n    # Generate the samples\n    samples = np.random.beta(alpha, beta, size=shape)\n\n    # Set the dtype if specified\n    if dtype is not None:\n        samples = samples.astype(dtype)\n\n    # If an output array is provided, fill it with the samples\n    if out is not None:\n        out[:] = samples\n        return out\n\n    return samples", "CodeBleu_score": 55.805452269191505}, {"code": "import numpy as np\nfrom typing import Union, Optional, Sequence\n\ndef beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[int, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"\n    The function beta generates samples from a beta distribution given the parameters alpha and beta.\n    It optionally accepts shape, device, dtype, seed, and out parameters. The function checks and determines\n    the shape of the output, sets the random seed if provided, and returns an array of samples from the beta\n    distribution with the specified dtype.\n\n    Parameters:\n        alpha (Union[float, np.ndarray]): The alpha parameter of the beta distribution.\n        beta (Union[float, np.ndarray]): The beta parameter of the beta distribution.\n        shape (Optional[Union[int, Sequence[int]]]): The shape of the output array. If None, defaults to the shape of alpha or beta.\n        device (Optional[str]): The device on which to generate the samples. This is more relevant in frameworks like PyTorch or TensorFlow.\n        dtype (Optional[np.dtype]): The data type of the output array. If None, defaults to the dtype of alpha or beta.\n        seed (Optional[int]): A seed for the random number generator.\n        out (Optional[np.ndarray]): An optional output array to write the samples into.\n\n    Returns:\n        np.ndarray: An array of samples from the beta distribution.\n    \"\"\"\n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Determine the output dtype\n    if dtype is None:\n        dtype = alpha.dtype if isinstance(alpha, np.ndarray) else np.float32\n\n    # Determine the output shape\n    if shape is None:\n        if isinstance(alpha, np.ndarray):\n            shape = alpha.shape\n        elif isinstance(beta, np.ndarray):\n            shape = beta.shape\n        else:\n            shape = ()\n\n    # Generate the samples\n    rng = np.random.default_rng(seed)\n    samples = rng.beta(alpha, beta, size=shape)\n\n    # Cast to the specified dtype\n    samples = samples.astype(dtype)\n\n    # If an output array is provided, fill it with the samples\n    if out is not None:\n        out[:] = samples\n        return out\n\n    return samples", "CodeBleu_score": 53.14131367209233}, {"code": "import numpy as np\nfrom typing import Union, Optional, Sequence\n\ndef beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    \"\"\"\n    The function beta generates samples from a beta distribution given the parameters alpha and beta.\n    It optionally accepts shape, device, dtype, seed, and out parameters. The function checks and determines\n    the shape of the output, sets the random seed if provided, and returns an array of samples from the beta\n    distribution with the specified dtype.\n    \"\"\"\n    \n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Determine the output shape\n    if shape is None:\n        if isinstance(alpha, np.ndarray):\n            shape = alpha.shape\n        elif isinstance(beta, np.ndarray):\n            shape = beta.shape\n        else:\n            shape = ()\n    \n    # Generate samples from the beta distribution\n    samples = np.random.beta(alpha, beta, size=shape)\n    \n    # Set the dtype if specified\n    if dtype is not None:\n        samples = samples.astype(dtype)\n    \n    # If an output array is provided, fill it with the samples\n    if out is not None:\n        out[:] = samples\n        return out\n    \n    return samples", "CodeBleu_score": 55.805452269191505}]}
{"instance_id": "271200a8b261", "generation_result": [{"code": "import numpy as np\nfrom typing import Union\n\ndef invert_permutation(x: Union[np.ndarray, list, tuple]) -> np.ndarray:\n    \"\"\"The function invert_permutation takes an array-like input x, which can be a NumPy array, list, or tuple.\n    It computes the sorted indices of x and initializes an array inverse with the same shape as the sorted indices, filled with zeros.\n    It then assigns to inverse the indices of the sorted elements of x.\n    Finally, it computes the inverse permutation by sorting the inverse array and returns it.\"\"\"\n    \n    # Ensure x is a numpy array\n    x = np.array(x)\n    \n    # Get the sorted indices of x\n    sorted_indices = np.argsort(x)\n    \n    # Initialize the inverse array with zeros\n    inverse = np.zeros_like(x)\n    \n    # Assign to inverse the indices of the sorted elements of x\n    inverse[sorted_indices] = np.arange(len(x))\n    \n    # Compute the inverse permutation by sorting the inverse array\n    inverse_permutation = np.argsort(inverse)\n    \n    return inverse_permutation", "CodeBleu_score": 75.26396282939386}, {"code": "import numpy as np\nfrom typing import Union\n\ndef invert_permutation(x: Union[np.ndarray, list, tuple]) -> np.ndarray:\n    \"\"\"The function invert_permutation takes an array-like input x, which can be a NumPy array, list, or tuple.\n    It computes the sorted indices of x and initializes an array inverse with the same shape as the sorted indices, filled with zeros.\n    It then assigns to inverse the indices of the sorted elements of x.\n    Finally, it computes the inverse permutation by sorting the inverse array and returns it.\"\"\"\n    \n    # Ensure x is a numpy array\n    x = np.array(x)\n    \n    # Get the sorted indices of x\n    sorted_indices = np.argsort(x)\n    \n    # Initialize the inverse array with zeros\n    inverse = np.zeros_like(x)\n    \n    # Assign to inverse the indices of the sorted elements\n    inverse[sorted_indices] = np.arange(len(x))\n    \n    # Compute the inverse permutation\n    inverse_permutation = np.argsort(inverse)\n    \n    return inverse_permutation", "CodeBleu_score": 75.26396282939386}, {"code": "import numpy as np\nfrom typing import Union\n\ndef invert_permutation(x: Union[np.ndarray, list, tuple]) -> np.ndarray:\n    \"\"\"The function invert_permutation takes an array-like input x, which can be a NumPy array, list, or tuple.\n    It computes the sorted indices of x and initializes an array inverse with the same shape as the sorted indices, filled with zeros.\n    It then assigns to inverse the indices of the sorted elements of x.\n    Finally, it computes the inverse permutation by sorting the inverse array and returns it.\"\"\"\n    \n    # Ensure x is a numpy array\n    x = np.array(x)\n    \n    # Get the sorted indices of x\n    sorted_indices = np.argsort(x)\n    \n    # Initialize the inverse array with zeros\n    inverse = np.zeros_like(x)\n    \n    # Assign to inverse the indices of the sorted elements of x\n    inverse[sorted_indices] = np.arange(len(x))\n    \n    # Compute the inverse permutation by sorting the inverse array\n    inverse_permutation = np.argsort(inverse)\n    \n    return inverse_permutation", "CodeBleu_score": 75.26396282939386}]}
{"instance_id": "040d73b37229", "generation_result": [{"code": "import matplotlib.pyplot as plt\nimport numpy as np  # Assuming data might be in NumPy arrays or similar\n\ndef plot_medical_charts() -> None:\n    \"\"\"\n    The function plot_medical_charts configures the plotting environment and generates a set of medical-related charts.\n    It sets the style, creates a figure with subplots, and plots various medical data on these subplots, including total vomit per month,\n    days between vomit, days in daycare, and doctor visits. Finally, it adjusts the layout of the subplots and exports the figure to a specified file location.\n    \"\"\"\n    # Set the style of plots\n    plt.style.use('seaborn-darkgrid')\n\n    # Create a figure with subplots\n    fig, axs = plt.subplots(2, 2, figsize=(10, 8))  # 2x2 grid of charts\n\n    # Example data (this should be replaced with actual data)\n    months = np.arange(1, 13)  # Months of the year\n    total_vomit_per_month = np.random.randint(0, 10, size=12)  # Random data\n    days_between_vomit = np.random.randint(1, 30, size=12)\n    days_in_daycare = np.random.randint(0, 20, size=12)\n    doctor_visits = np.random.randint(0, 5, size=12)\n\n    # Plotting the data\n    axs[0, 0].plot(months, total_vomit_per_month, marker='o', color='r')\n    axs[0, 0].set_title('Total Vomit per Month')\n    axs[0, 0].set_xlabel('Month')\n    axs[0, 0].set_ylabel('Vomit Count')\n\n    axs[0, 1].plot(months, days_between_vomit, marker='s', color='b')\n    axs[0, 1].set_title('Days Between Vomit')\n    axs[0, 1].set_xlabel('Month')\n    axs[0, 1].set_ylabel('Days')\n\n    axs[1, 0].bar(months, days_in_daycare, color='g')\n    axs[1, 0].set_title('Days in Daycare')\n    axs[1, 0].set_xlabel('Month')\n    axs[1, 0].set_ylabel('Days')\n\n    axs[1, 1].scatter(months, doctor_visits, color='purple')\n    axs[1, 1].set_title('Doctor Visits')\n    axs[1, 1].set_xlabel('Month')\n    axs[1, 1].set_ylabel('Visits')\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save the figure\n    plt.savefig('medical_charts.png')\n\n    # Show the plot\n    plt.show()", "CodeBleu_score": 24.500699428801294}, {"code": "import matplotlib.pyplot as plt\nimport numpy as np  # Assuming data might be in NumPy arrays or similar\n\ndef plot_medical_charts() -> None:\n    \"\"\"\n    The function plot_medical_charts configures the plotting environment and generates a set of medical-related charts.\n    It sets the style, creates a figure with subplots, and plots various medical data on these subplots, including total vomit per month,\n    days between vomit, days in daycare, and doctor visits. Finally, it adjusts the layout of the subplots and exports the figure to a specified file location.\n    \"\"\"\n    # Set the style of plots\n    plt.style.use('seaborn-darkgrid')\n\n    # Create a figure with subplots\n    fig, axs = plt.subplots(2, 2, figsize=(10, 8))  # 2x2 grid of charts\n\n    # Example data (this should be replaced with actual data)\n    months = np.arange(1, 13)  # Months of the year\n    total_vomit_per_month = np.random.randint(0, 10, size=12)  # Random data\n    days_between_vomit = np.random.randint(1, 30, size=12)\n    days_in_daycare = np.random.randint(0, 20, size=12)\n    doctor_visits = np.random.randint(0, 5, size=12)\n\n    # Plotting the data\n    axs[0, 0].plot(months, total_vomit_per_month, marker='o', color='r')\n    axs[0, 0].set_title('Total Vomit per Month')\n    axs[0, 0].set_xlabel('Month')\n    axs[0, 0].set_ylabel('Counts')\n\n    axs[0, 1].plot(months, days_between_vomit, marker='x', linestyle='--', color='g')\n    axs[0, 1].set_title('Days Between Vomit')\n    axs[0, 1].set_xlabel('Month')\n    axs[0, 1].set_ylabel('Days')\n\n    axs[1, 0].bar(months, days_in_daycare, color='b')\n    axs[1, 0].set_title('Days in Daycare')\n    axs[1, 0].set_xlabel('Month')\n    axs[1, 0].set_ylabel('Days')\n\n    axs[1, 1].scatter(months, doctor_visits, color='m')\n    axs[1, 1].set_title('Doctor Visits')\n    axs[1, 1].set_xlabel('Month')\n    axs[1, 1].set_ylabel('Visits')\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save the figure\n    plt.savefig('medical_charts.png')\n\n    # Show the plot\n    plt.show()", "CodeBleu_score": 24.491478199726245}, {"code": "import matplotlib.pyplot as plt\nimport numpy as np  # Assuming data might be in NumPy arrays or similar\n\ndef plot_medical_charts() -> None:\n    \"\"\"\n    The function plot_medical_charts configures the plotting environment and generates a set of medical-related charts.\n    It sets the style, creates a figure with subplots, and plots various medical data on these subplots, including total vomit per month,\n    days between vomit, days in daycare, and doctor visits. Finally, it adjusts the layout of the subplots and exports the figure to a specified file location.\n    \"\"\"\n    # Set the style of plots\n    plt.style.use('ggplot')\n\n    # Create a figure with subplots\n    fig, axs = plt.subplots(2, 2, figsize=(10, 8))  # 2x2 grid of charts\n\n    # Example data (this should be replaced with actual data)\n    months = np.arange(1, 13)  # Months of the year\n    total_vomit_per_month = np.random.randint(0, 10, size=12)  # Random data\n    days_between_vomit = np.random.randint(1, 30, size=12)\n    days_in_daycare = np.random.randint(0, 20, size=12)\n    doctor_visits = np.random.randint(0, 5, size=12)\n\n    # Plotting total vomit per month\n    axs[0, 0].bar(months, total_vomit_per_month, color='blue')\n    axs[0, 0].set_title('Total Vomit per Month')\n    axs[0, 0].set_xlabel('Month')\n    axs[0, 0].set_ylabel('Total Vomit')\n\n    # Plotting days between vomit\n    axs[0, 1].plot(months, days_between_vomit, marker='o', linestyle='-', color='green')\n    axs[0, 1].set_title('Days Between Vomit')\n    axs[0, 1].set_xlabel('Month')\n    axs[0, 1].set_ylabel('Days')\n\n    # Plotting days in daycare\n    axs[1, 0].bar(months, days_in_daycare, color='red')\n    axs[1, 0].set_title('Days in Daycare')\n    axs[1, 0].set_xlabel('Month')\n    axs[1, 0].set_ylabel('Days')\n\n    # Plotting doctor visits\n    axs[1, 1].plot(months, doctor_visits, marker='s', linestyle='--', color='purple')\n    axs[1, 1].set_title('Doctor Visits')\n    axs[1, 1].set_xlabel('Month')\n    axs[1, 1].set_ylabel('Visits')\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save the figure\n    plt.savefig('medical_charts.png')\n\n    # Show the plot\n    plt.show()", "CodeBleu_score": 24.355319731303}]}
{"instance_id": "7061f4d0c3fc", "generation_result": [{"code": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom datetime import datetime, timedelta\n\ndef parse_raw_data():\n    # This is a placeholder function. Replace it with actual data parsing logic.\n    return pd.DataFrame({\n        'type': ['bottle', 'solid', 'bottle', 'solid'],\n        'time': [datetime.now() - timedelta(hours=h) for h in range(0, 100, 25)]\n    })\n\ndef export_figure(fig, filename='feeding_24h_viz.png'):\n    fig.savefig(filename)\n\ndef plot_feeding_24h_viz() -> None:\n    \"\"\"The function plot_feeding_24h_viz visualizes 24-hour feeding data for bottle and solid feedings.\n    It imports and extracts feeding data using parse_raw_data, sets up a plot with a dark grid style, and computes the offset from the birthday based on the last solid feeding date.\n    It then creates a scatter plot of feeding times, using red for bottle feedings and blue for solid feedings, adding a legend to distinguish between the two.\n    The function calculates the end date for the plot, formats the plot to display feeding data over a week in a 24-hour format, and exports the figure using export_figure.\"\"\"\n\n    # Import and extract feeding data\n    data = parse_raw_data()\n\n    # Set up plot with dark grid style\n    sns.set(style=\"darkgrid\")\n\n    # Create a scatter plot of feeding times\n    plt.figure(figsize=(10, 6))\n    for feeding_type, color in zip(['bottle', 'solid'], ['red', 'blue']):\n        subset = data[data['type'] == feeding_type]\n        plt.scatter(subset['time'], [feeding_type] * len(subset), color=color, label=f\"{feeding_type.capitalize()} Feeding\")\n\n    # Adding a legend\n    plt.legend()\n\n    # Calculate the end date for the plot\n    last_solid_date = data[data['type'] == 'solid']['time'].max()\n    start_date = last_solid_date - timedelta(days=7)\n    plt.xlim(start_date, last_solid_date + timedelta(days=1))\n\n    # Format the plot to display feeding data over a week in a 24-hour format\n    plt.gca().xaxis.set_major_locator(plt.MultipleLocator(timedelta(days=1)))\n    plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: pd.to_datetime(x).strftime('%Y-%m-%d %H:%M')))\n\n    # Rotate date labels for better readability\n    plt.gcf().autofmt_xdate()\n\n    # Export the figure\n    export_figure(plt.gcf())\n\n    # Show the plot\n    plt.show()", "CodeBleu_score": 32.73181431921208}, {"code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import timedelta\n\ndef parse_raw_data():\n    # This is a placeholder function. You should replace it with actual data parsing logic.\n    return pd.DataFrame({\n        'type': ['bottle', 'solid', 'bottle', 'solid'],\n        'time': pd.to_datetime(['2023-09-01 12:00', '2023-09-01 13:00', '2023-09-02 14:00', '2023-09-02 18:00'])\n    })\n\ndef export_figure(fig, filename='feeding_24h_viz.png'):\n    # This is a placeholder function. Replace with actual export logic if different.\n    fig.savefig(filename)\n\ndef plot_feeding_24h_viz() -> None:\n    # Import and extract feeding data\n    data = parse_raw_data()\n\n    # Set up plot with a dark grid style\n    plt.style.use('dark_background')\n    fig, ax = plt.subplots()\n\n    # Compute the offset from the birthday based on the last solid feeding date\n    last_solid_date = data[data['type'] == 'solid']['time'].max()\n    start_date = last_solid_date - timedelta(days=6)\n    end_date = last_solid_date + timedelta(days=1)\n\n    # Filter data within the week range\n    week_data = data[(data['time'] >= start_date) & (data['time'] <= end_date)]\n\n    # Create scatter plot of feeding times\n    for feed_type, color in [('bottle', 'red'), ('solid', 'blue')]:\n        times = week_data[week_data['type'] == feed_type]['time']\n        ax.scatter(times, [feed_type] * len(times), color=color, label=f'{feed_type.capitalize()} Feedings')\n\n    # Add a legend\n    ax.legend()\n\n    # Format the plot to display feeding data over a week in a 24-hour format\n    ax.xaxis.set_major_locator(mdates.DayLocator())\n    ax.xaxis.set_minor_locator(mdates.HourLocator(interval=6))\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n    ax.xaxis.set_minor_formatter(mdates.DateFormatter('%H:%M'))\n    plt.xticks(rotation=45)\n    plt.xlim([start_date, end_date])\n    plt.ylim([-1, 2])\n    plt.yticks([0, 1], ['Bottle', 'Solid'])\n\n    # Export the figure\n    export_figure(fig)\n\n# Example usage\nplot_feeding_24h_viz()", "CodeBleu_score": 31.31485697395157}, {"code": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom datetime import timedelta\n\ndef parse_raw_data():\n    # This is a placeholder function. You should replace it with the actual data parsing logic.\n    # It should return a DataFrame with columns 'type', 'datetime', and 'birthday'.\n    return pd.DataFrame({\n        'type': ['bottle', 'solid', 'bottle', 'solid'],\n        'datetime': pd.to_datetime(['2023-09-01 08:00', '2023-09-01 12:00', '2023-09-02 08:00', '2023-09-02 12:00']),\n        'birthday': pd.to_datetime(['2023-08-01'] * 4)\n    })\n\ndef export_figure(fig, filename='feeding_24h_viz.png'):\n    # This is a placeholder function. You should replace it with the actual figure export logic.\n    fig.savefig(filename)\n\ndef plot_feeding_24h_viz() -> None:\n    # Import and extract feeding data\n    data = parse_raw_data()\n\n    # Set up plot with a dark grid style\n    sns.set(style=\"darkgrid\")\n    plt.figure(figsize=(10, 6))\n\n    # Compute the offset from the birthday based on the last solid feeding date\n    last_solid_date = data[data['type'] == 'solid']['datetime'].max()\n    offset_days = (last_solid_date - data['birthday'].iloc[0]).days\n\n    # Create a scatter plot of feeding times\n    for feeding_type, color in zip(['bottle', 'solid'], ['red', 'blue']):\n        feeding_data = data[data['type'] == feeding_type]\n        plt.scatter(feeding_data['datetime'], [offset_days] * len(feeding_data), color=color, label=f'{feeding_type.capitalize()} Feedings')\n\n    # Add a legend\n    plt.legend()\n\n    # Calculate the end date for the plot\n    end_date = last_solid_date + timedelta(days=7)\n\n    # Format the plot to display feeding data over a week in a 24-hour format\n    plt.xlim(last_solid_date, end_date)\n    plt.gca().xaxis.set_major_locator(plt.MultipleLocator(timedelta(days=1)))\n    plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: pd.to_datetime(x).strftime('%Y-%m-%d %H:%M')))\n\n    # Export the figure\n    fig = plt.gcf()\n    export_figure(fig)\n\n    # Show the plot\n    plt.show()\n\n# Example usage\nplot_feeding_24h_viz()", "CodeBleu_score": 34.71869218429228}]}
{"instance_id": "6c1ffa63d122", "generation_result": [{"code": "import seaborn as sns\n\nclass YourClassName:\n    def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n        \"\"\"\n        The function plot creates a bar plot on the provided axis, showing target counts for a specified classification task.\n        It first retrieves the target counts for the given task, then uses seaborn to generate a bar plot with target names on the x-axis\n        and their corresponding counts on the y-axis.\n        \"\"\"\n        # Assuming `get_target_counts` is a method that retrieves target counts based on the task\n        target_counts = self.get_target_counts(task)\n        \n        # Creating the bar plot\n        sns.barplot(x=target_counts.index, y=target_counts.values, ax=ax, **kwargs)\n        ax.set_xlabel('Target Names')\n        ax.set_ylabel('Counts')\n        ax.set_title(f'Target Counts for {task.name}')", "CodeBleu_score": 41.1498950955275}, {"code": "import seaborn as sns\n\nclass DataPlotter:\n    def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n        \"\"\"\n        The function plot creates a bar plot on the provided axis, showing target counts for a specified classification task.\n        It first retrieves the target counts for the given task, then uses seaborn to generate a bar plot with target names on the x-axis\n        and their corresponding counts on the y-axis.\n        \"\"\"\n        # Assuming `get_target_counts` is a method that retrieves the counts based on the task\n        target_counts = self.get_target_counts(task)\n        \n        # Creating the bar plot\n        sns.barplot(x=list(target_counts.keys()), y=list(target_counts.values()), ax=ax, **kwargs)\n        ax.set_xlabel('Target Names')\n        ax.set_ylabel('Counts')\n        ax.set_title(f'Target Counts for {task.name}')", "CodeBleu_score": 41.79694328291628}, {"code": "import seaborn as sns\n\nclass DataPlotter:\n    def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n        \"\"\"\n        The function plot creates a bar plot on the provided axis, showing target counts for a specified classification task.\n        It first retrieves the target counts for the given task, then uses seaborn to generate a bar plot with target names on the x-axis\n        and their corresponding counts on the y-axis.\n        \"\"\"\n        # Assuming `get_target_counts` is a method that retrieves the counts of targets based on the classification task\n        target_counts = self.get_target_counts(task)\n        \n        # Creating the bar plot\n        sns.barplot(x=list(target_counts.keys()), y=list(target_counts.values()), ax=ax, **kwargs)\n        ax.set_xlabel('Target Names')\n        ax.set_ylabel('Counts')\n        ax.set_title(f'Target Counts for {task.name}')", "CodeBleu_score": 41.79694328291628}]}
{"instance_id": "2f81a906bad5", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\n\ndef render_to_rgb(figure):\n    \"\"\"\n    The function render_to_rgb converts a Matplotlib figure to an RGB image.\n    It creates a canvas from the figure using plt_backend_agg.FigureCanvasAgg and draws the figure on it.\n    The RGBA buffer data from the canvas is converted to a NumPy array and reshaped into an image with width and height dimensions and 3 color channels (RGB).\n    The function optionally closes the figure and returns the image in CHW format (channels, height, width).\n    If a list of figures is provided, the function processes each figure and returns a stacked array of images.\n    If a single figure is provided, it returns the corresponding image.\n    \"\"\"\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            images.append(_process_figure_to_rgb(fig))\n        return np.stack(images)\n    else:\n        return _process_figure_to_rgb(figure)\n\ndef _process_figure_to_rgb(fig):\n    canvas = FigureCanvasAgg(fig)\n    canvas.draw()\n    buf = canvas.buffer_rgba()\n    # Convert buffer to numpy array\n    X = np.array(buf)\n    # Drop the Alpha channel, reshape\n    img = X[:, :, :3]\n    # Convert from HWC to CHW format\n    img = np.transpose(img, (2, 0, 1))\n    plt.close(fig)\n    return img", "CodeBleu_score": 40.07466646800396}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\n\ndef render_to_rgb(figure):\n    \"\"\"\n    The function render_to_rgb converts a Matplotlib figure to an RGB image.\n    It creates a canvas from the figure using plt_backend_agg.FigureCanvasAgg and draws the figure on it.\n    The RGBA buffer data from the canvas is converted to a NumPy array and reshaped into an image with width and height dimensions and 3 color channels (RGB).\n    The function optionally closes the figure and returns the image in CHW format (channels, height, width).\n    If a list of figures is provided, the function processes each figure and returns a stacked array of images.\n    If a single figure is provided, it returns the corresponding image.\n    \"\"\"\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            images.append(_process_figure_to_rgb(fig))\n        return np.stack(images)\n    else:\n        return _process_figure_to_rgb(figure)\n\ndef _process_figure_to_rgb(fig):\n    canvas = FigureCanvasAgg(fig)\n    canvas.draw()\n    buf = canvas.buffer_rgba()\n    # Convert buffer to numpy array\n    X = np.array(buf)\n    # Drop the Alpha channel, reshape and reorder dimensions to CHW format\n    img = X[:, :, :3]\n    img = np.moveaxis(img, 2, 0)  # Move channels to first dimension\n    return img", "CodeBleu_score": 37.957161713295214}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\n\ndef render_to_rgb(figure):\n    \"\"\"\n    The function render_to_rgb converts a Matplotlib figure to an RGB image.\n    It creates a canvas from the figure using plt_backend_agg.FigureCanvasAgg and draws the figure on it.\n    The RGBA buffer data from the canvas is converted to a NumPy array and reshaped into an image with width and height dimensions and 3 color channels (RGB).\n    The function optionally closes the figure and returns the image in CHW format (channels, height, width).\n    If a list of figures is provided, the function processes each figure and returns a stacked array of images.\n    If a single figure is provided, it returns the corresponding image.\n    \"\"\"\n    if isinstance(figure, list):\n        images = []\n        for fig in figure:\n            images.append(_process_figure_to_rgb(fig))\n        return np.stack(images)\n    else:\n        return _process_figure_to_rgb(figure)\n\ndef _process_figure_to_rgb(fig):\n    canvas = FigureCanvasAgg(fig)\n    canvas.draw()\n    buf = canvas.buffer_rgba()\n    # Convert buffer to numpy array\n    X = np.array(buf)\n    # Drop the Alpha channel, reshape\n    img = X[:, :, :3]\n    # Convert from HWC to CHW format\n    img = np.transpose(img, (2, 0, 1))\n    plt.close(fig)\n    return img", "CodeBleu_score": 40.07466646800396}]}
{"instance_id": "c3037b01569c", "generation_result": [{"code": "import torch\nimport time\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tensorboard.plugins.beholder import Beholder\n\ndef beholder_pytorch():\n    \"\"\"The function beholder_pytorch repeatedly generates random tensors and updates a Beholder visualization with them. It runs a loop 1000 times, creating two lists of random tensors with associated names in each iteration. A Beholder object is instantiated with a specified log directory, and its update method is called with the generated tensors and a random frame. The function then sleeps briefly and prints the current iteration index.\"\"\"\n\n    # Specify the log directory for Beholder\n    log_dir = '/path/to/log'\n    writer = SummaryWriter(log_dir)\n    beholder = Beholder(log_dir=log_dir)\n\n    for i in range(1000):\n        # Generate random tensors\n        tensors = [torch.randn(10, 10) for _ in range(2)]\n        names = ['tensor1', 'tensor2']\n\n        # Update Beholder visualization\n        beholder.update(\n            frame=tensors,\n            frame_name=names\n        )\n\n        # Sleep briefly to simulate time passing\n        time.sleep(0.1)\n\n        # Print the current iteration index\n        print(f'Iteration {i}')\n\n# Note: Ensure that TensorBoard and the Beholder plugin are properly installed and configured to use this function.", "CodeBleu_score": 35.52715299379453}, {"code": "import torch\nimport time\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tensorboard.plugins.beholder import Beholder\n\ndef beholder_pytorch():\n    \"\"\"The function beholder_pytorch repeatedly generates random tensors and updates a Beholder visualization with them. It runs a loop 1000 times, creating two lists of random tensors with associated names in each iteration. A Beholder object is instantiated with a specified log directory, and its update method is called with the generated tensors and a random frame. The function then sleeps briefly and prints the current iteration index.\"\"\"\n\n    # Specify the log directory for Beholder\n    log_dir = '/path/to/log'\n    writer = SummaryWriter(log_dir)\n    beholder = Beholder(log_dir=log_dir)\n\n    for i in range(1000):\n        # Generate random tensors\n        tensor_list = [torch.randn(10, 10) for _ in range(2)]\n        names = ['tensor1', 'tensor2']\n\n        # Update Beholder visualization\n        beholder.update(\n            frame=writer,\n            arrays=[{'data': tensor.numpy(), 'label': name} for tensor, name in zip(tensor_list, names)]\n        )\n\n        # Sleep briefly\n        time.sleep(0.1)\n\n        # Print the current iteration index\n        print(f'Iteration {i}')", "CodeBleu_score": 35.51954451963203}, {"code": "import torch\nfrom tensorboardX import Beholder\nimport time\n\ndef beholder_pytorch():\n    \"\"\"The function beholder_pytorch repeatedly generates random tensors and updates a Beholder visualization with them. It runs a loop 1000 times, creating two lists of random tensors with associated names in each iteration. A Beholder object is instantiated with a specified log directory, and its update method is called with the generated tensors and a random frame. The function then sleeps briefly and prints the current iteration index.\"\"\"\n\n    log_dir = '/path/to/log'  # Specify the log directory\n    beholder = Beholder(log_dir=log_dir)  # Create a Beholder object\n\n    for i in range(1000):\n        # Generate random tensors\n        tensor_list = [torch.randn(10, 10) for _ in range(5)]\n        names_list = [f'tensor_{j}' for j in range(5)]\n\n        # Update the Beholder visualization\n        beholder.update(\n            arrays=tensor_list,\n            frame=i,\n            arrays_name=names_list\n        )\n\n        # Sleep briefly\n        time.sleep(0.1)\n\n        # Print the current iteration index\n        print(f'Iteration {i}')", "CodeBleu_score": 35.07421435992138}]}
{"instance_id": "cd57b5de8a88", "generation_result": [{"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None):\n    import numpy as np\n\n    # Extract tensor data from the summary\n    tensor_data = np.array(summary)  # Assuming summary is a list-like structure that can be converted to a numpy array\n\n    # Compute thresholds\n    thresholds = np.linspace(0, 1, num_thresholds)\n\n    # Reshape tensor data based on its dimensions\n    # Assuming tensor_data is structured as [TP, FP, TN, FN, precision, recall]\n    # Each metric is assumed to be in a separate row\n    TP = tensor_data[0].flatten()\n    FP = tensor_data[1].flatten()\n    TN = tensor_data[2].flatten()\n    FN = tensor_data[3].flatten()\n    precision = tensor_data[4].flatten()\n    recall = tensor_data[5].flatten()\n\n    # Store the values in a dictionary\n    pr_data = {\n        'tag': tag,\n        'thresholds': thresholds.tolist(),\n        'TP': TP.tolist(),\n        'FP': FP.tolist(),\n        'TN': TN.tolist(),\n        'FN': FN.tolist(),\n        'precision': precision.tolist(),\n        'recall': recall.tolist()\n    }\n\n    # Log the pr_data using the log_asset_data method\n    self.log_asset_data(tag, pr_data, step=step)", "CodeBleu_score": 38.0125530244335}, {"code": "import numpy as np\n\ndef log_pr_data(self, tag, summary, num_thresholds, step=None):\n    # Extract tensor data from the summary\n    tp = summary['tp']  # true positives\n    fp = summary['fp']  # false positives\n    tn = summary['tn']  # true negatives\n    fn = summary['fn']  # false negatives\n    precision = summary['precision']\n    recall = summary['recall']\n\n    # Compute thresholds\n    thresholds = np.linspace(0, 1, num=num_thresholds)\n\n    # Flatten the tensor values\n    tp_flat = tp.flatten().tolist()\n    fp_flat = fp.flatten().tolist()\n    tn_flat = tn.flatten().tolist()\n    fn_flat = fn.flatten().tolist()\n    precision_flat = precision.flatten().tolist()\n    recall_flat = recall.flatten().tolist()\n\n    # Store values in a dictionary\n    pr_data = {\n        'tag': tag,\n        'thresholds': thresholds,\n        'true_positives': tp_flat,\n        'false_positives': fp_flat,\n        'true_negatives': tn_flat,\n        'false_negatives': fn_flat,\n        'precision': precision_flat,\n        'recall': recall_flat\n    }\n\n    # Log the pr_data\n    self.log_asset_data(pr_data, tag, step)", "CodeBleu_score": 34.87228784812997}, {"code": "def log_pr_data(self, tag, summary, num_thresholds, step=None):\n    import numpy as np\n\n    # Extract tensor data from the summary\n    tensor_data = np.array(summary)  # Assuming summary is a list-like object containing tensor data\n\n    # Compute thresholds\n    thresholds = np.linspace(0, 1, num_thresholds)\n\n    # Reshape tensor data based on its dimensions\n    # Assuming tensor_data is structured as [TP, FP, TN, FN, precision, recall]\n    # Each metric is assumed to be in a separate row\n    TP = tensor_data[0].flatten()\n    FP = tensor_data[1].flatten()\n    TN = tensor_data[2].flatten()\n    FN = tensor_data[3].flatten()\n    precision = tensor_data[4].flatten()\n    recall = tensor_data[5].flatten()\n\n    # Store the values in a dictionary\n    pr_data = {\n        'tag': tag,\n        'thresholds': thresholds.tolist(),\n        'TP': TP.tolist(),\n        'FP': FP.tolist(),\n        'TN': TN.tolist(),\n        'FN': FN.tolist(),\n        'precision': precision.tolist(),\n        'recall': recall.tolist()\n    }\n\n    # Log the pr_data using the log_asset_data method\n    self.log_asset_data(tag, pr_data, step=step)", "CodeBleu_score": 38.3801575166583}]}
{"instance_id": "7c4ea9beecc9", "generation_result": [{"code": "import numpy as np\nimport gzip\n\ndef load_mnist(images_path, labels_path, num):\n    \"\"\"The function load_mnist reads a specified number of MNIST images and labels from compressed files and returns them as NumPy arrays. It initializes empty arrays for the data and target labels, then opens the image and label files using gzip. It skips the header bytes in both files and reads the image and label data byte by byte, storing them in the respective arrays. Finally, it returns the data and target arrays.\n\n    Args:\n        images_path (str): The file path to the images file (.gz).\n        labels_path (str): The file path to the labels file (.gz).\n        num (int): The number of images and labels to load.\n\n    Returns:\n        tuple: A tuple containing two numpy arrays, the first being the images and the second being the labels.\n    \"\"\"\n    with gzip.open(images_path, 'rb') as img_file, gzip.open(labels_path, 'rb') as lbl_file:\n        # Skip the header information in the image file\n        img_file.read(16)  # first 16 bytes are header for images\n        lbl_file.read(8)   # first 8 bytes are header for labels\n\n        # Prepare buffer for reading num images x 28x28 pixels, each pixel is 1 byte\n        images = np.frombuffer(img_file.read(num * 28 * 28), dtype=np.uint8).reshape(num, 28, 28)\n        labels = np.frombuffer(lbl_file.read(num), dtype=np.uint8)\n\n    return images, labels", "CodeBleu_score": 37.05602917206441}, {"code": "import numpy as np\nimport gzip\n\ndef load_mnist(images_path, labels_path, num):\n    \"\"\"The function load_mnist reads a specified number of MNIST images and labels from compressed files and returns them as NumPy arrays. It initializes empty arrays for the data and target labels, then opens the image and label files using gzip. It skips the header bytes in both files and reads the image and label data byte by byte, storing them in the respective arrays. Finally, it returns the data and target arrays.\n    \n    Args:\n        images_path (str): The file path to the images file (.gz format).\n        labels_path (str): The file path to the labels file (.gz format).\n        num (int): The number of images and labels to load.\n\n    Returns:\n        tuple: A tuple containing two numpy arrays, the first being the images and the second being the labels.\n    \"\"\"\n    with gzip.open(images_path, 'rb') as img_file, gzip.open(labels_path, 'rb') as lbl_file:\n        # Skip the header information in the image file\n        img_file.read(16)  # first 16 bytes are header for image file\n        lbl_file.read(8)   # first 8 bytes are header for label file\n\n        # Prepare buffer for reading num images and labels\n        image_size = 28 * 28  # MNIST images are 28x28\n        images = np.zeros((num, image_size), dtype=np.uint8)\n        labels = np.zeros(num, dtype=np.uint8)\n\n        # Read num images and labels from the files\n        for i in range(num):\n            images[i] = np.frombuffer(img_file.read(image_size), dtype=np.uint8)\n            labels[i] = np.frombuffer(lbl_file.read(1), dtype=np.uint8)\n\n    return images, labels", "CodeBleu_score": 36.743868314957076}, {"code": "import numpy as np\nimport gzip\n\ndef load_mnist(images, labels, num):\n    \"\"\"The function load_mnist reads a specified number of MNIST images and labels from compressed files and returns them as NumPy arrays. It initializes empty arrays for the data and target labels, then opens the image and label files using gzip. It skips the header bytes in both files and reads the image and label data byte by byte, storing them in the respective arrays. Finally, it returns the data and target arrays.\n    \n    Args:\n        images (str): The filepath to the gzipped file containing the MNIST images.\n        labels (str): The filepath to the gzipped file containing the MNIST labels.\n        num (int): The number of images and labels to load.\n\n    Returns:\n        tuple: A tuple containing two numpy arrays. The first array contains the image data, and the second array contains the labels.\n    \"\"\"\n    with gzip.open(images, 'rb') as img_file, gzip.open(labels, 'rb') as lbl_file:\n        # Skip the header information in the image file\n        img_file.read(16)  # MNIST image header is 16 bytes\n        lbl_file.read(8)   # MNIST label header is 8 bytes\n\n        # Prepare buffer for reading num images and labels\n        img_size = 28 * 28 * num\n        lbl_size = num\n\n        # Read the image and label data\n        img_data = img_file.read(img_size)\n        lbl_data = lbl_file.read(lbl_size)\n\n        # Convert data to numpy arrays and reshape appropriately\n        images = np.frombuffer(img_data, dtype=np.uint8).reshape(num, 28, 28)\n        labels = np.frombuffer(lbl_data, dtype=np.uint8).reshape(num)\n\n    return images, labels", "CodeBleu_score": 40.83745626464166}]}
{"instance_id": "bf4f1c828b25", "generation_result": [{"code": "def compute_stats(self, ldf):\n    \"\"\"\n    The function compute_stats initializes and computes several statistics for a LuxDataFrame (ldf).\n    It sets up dictionaries for unique values, min-max values, and cardinality, and stores the length of the dataframe.\n    For each attribute in the dataframe's columns, it computes unique values and cardinality.\n    If the attribute is a timestamp, its string representation is used as the dictionary key.\n    For numeric attributes, it also computes the minimum and maximum values.\n    If the dataframe's index is not of integer type, it computes unique values and cardinality for the index as well.\n    \"\"\"\n    # Initialize dictionaries\n    unique_values = {}\n    min_max_values = {}\n    cardinality = {}\n\n    # Store the length of the dataframe\n    dataframe_length = len(ldf)\n\n    # Iterate over each column in the dataframe\n    for column in ldf.columns:\n        # Compute unique values and cardinality\n        unique_vals = ldf[column].unique()\n        unique_values[column] = unique_vals\n        cardinality[column] = len(unique_vals)\n\n        # Check if the column is a timestamp\n        if pd.api.types.is_datetime64_any_dtype(ldf[column]):\n            unique_values[str(column)] = unique_vals\n\n        # Check if the column is numeric\n        if pd.api.types.is_numeric_dtype(ldf[column]):\n            min_max_values[column] = (ldf[column].min(), ldf[column].max())\n\n    # Check if the index is not of integer type\n    if not pd.api.types.is_integer_dtype(ldf.index):\n        index_unique_vals = ldf.index.unique()\n        unique_values['index'] = index_unique_vals\n        cardinality['index'] = len(index_unique_vals)\n\n    # Store computed statistics in the LuxDataFrame\n    ldf.metadata = {\n        'length': dataframe_length,\n        'unique_values': unique_values,\n        'min_max_values': min_max_values,\n        'cardinality': cardinality\n    }", "CodeBleu_score": 32.32690320137319}, {"code": "def compute_stats(self, ldf):\n    \"\"\"\n    The function compute_stats initializes and computes several statistics for a LuxDataFrame (ldf).\n    It sets up dictionaries for unique values, min-max values, and cardinality, and stores the length of the dataframe.\n    For each attribute in the dataframe's columns, it computes unique values and cardinality.\n    If the attribute is a timestamp, its string representation is used as the dictionary key.\n    For numeric attributes, it also computes the minimum and maximum values.\n    If the dataframe's index is not of integer type, it computes unique values and cardinality for the index as well.\n    \"\"\"\n    # Initialize dictionaries\n    self.unique_values = {}\n    self.min_max_values = {}\n    self.cardinality = {}\n    \n    # Store the length of the dataframe\n    self.length = len(ldf)\n    \n    # Compute stats for each column\n    for column in ldf.columns:\n        # Check if the column is a timestamp\n        if pd.api.types.is_datetime64_any_dtype(ldf[column]):\n            key = str(column)\n        else:\n            key = column\n        \n        # Compute unique values and cardinality\n        unique_vals = ldf[column].unique()\n        self.unique_values[key] = unique_vals\n        self.cardinality[key] = len(unique_vals)\n        \n        # Compute min and max for numeric columns\n        if pd.api.types.is_numeric_dtype(ldf[column]):\n            self.min_max_values[key] = (ldf[column].min(), ldf[column].max())\n    \n    # Compute stats for the index if it's not of integer type\n    if not pd.api.types.is_integer_dtype(ldf.index):\n        self.unique_values['index'] = ldf.index.unique()\n        self.cardinality['index'] = len(ldf.index.unique())", "CodeBleu_score": 39.0775709023399}, {"code": "def compute_stats(self, ldf):\n    \"\"\"\n    The function compute_stats initializes and computes several statistics for a LuxDataFrame (ldf).\n    It sets up dictionaries for unique values, min-max values, and cardinality, and stores the length of the dataframe.\n    For each attribute in the dataframe's columns, it computes unique values and cardinality.\n    If the attribute is a timestamp, its string representation is used as the dictionary key.\n    For numeric attributes, it also computes the minimum and maximum values.\n    If the dataframe's index is not of integer type, it computes unique values and cardinality for the index as well.\n    \"\"\"\n    # Initialize dictionaries\n    self.unique_values = {}\n    self.min_max_values = {}\n    self.cardinality = {}\n    \n    # Store the length of the dataframe\n    self.length = len(ldf)\n    \n    # Iterate over each column in the dataframe\n    for column in ldf.columns:\n        # Compute unique values and cardinality\n        unique_vals = ldf[column].unique()\n        self.unique_values[column] = unique_vals\n        self.cardinality[column] = len(unique_vals)\n        \n        # Check if the column is a timestamp\n        if pd.api.types.is_datetime64_any_dtype(ldf[column]):\n            self.unique_values[column] = {str(val): val for val in unique_vals}\n        \n        # Check if the column is numeric\n        if pd.api.types.is_numeric_dtype(ldf[column]):\n            self.min_max_values[column] = (ldf[column].min(), ldf[column].max())\n    \n    # Check if the index is not of integer type and compute stats\n    if not pd.api.types.is_integer_dtype(ldf.index):\n        index_unique_vals = ldf.index.unique()\n        self.unique_values['index'] = index_unique_vals\n        self.cardinality['index'] = len(index_unique_vals)", "CodeBleu_score": 37.27523988564444}]}
{"instance_id": "3e6fc3d2f47c", "generation_result": [{"code": "import pandas as pd\nimport sqlite3  # Assuming SQLite for demonstration; adjust based on actual SQL connection used\n\ndef execute_preview(tbl: LuxSQLTable, preview_size=5):\n    \"\"\"\n    The function execute_preview generates a preview of a SQL table using a specified query template and returns the result as a pandas DataFrame.\n    It takes a LuxSQLTable object and an optional preview size parameter. The function formats a preview query with the table name and number of rows,\n    executes the query using a predefined SQL connection, and retrieves the data into a DataFrame, which is then returned.\n    \"\"\"\n    # Construct SQL query to fetch a limited number of rows\n    query = f\"SELECT * FROM {tbl.table_name} LIMIT {preview_size}\"\n    \n    # Execute the query and fetch the data\n    # Assuming there's a method or attribute in LuxSQLTable to get the connection, e.g., tbl.get_connection()\n    # Adjust the connection acquisition as per the actual implementation details of LuxSQLTable\n    connection = tbl.get_connection()  # This method needs to be defined in LuxSQLTable\n    df = pd.read_sql_query(query, connection)\n    \n    # Return the resulting DataFrame\n    return df", "CodeBleu_score": 28.504203908173054}, {"code": "import pandas as pd\n\ndef execute_preview(tbl: LuxSQLTable, preview_size=5) -> pd.DataFrame:\n    \"\"\"\n    The function execute_preview generates a preview of a SQL table using a specified query template and returns the result as a pandas DataFrame.\n    It takes a LuxSQLTable object and an optional preview size parameter. The function formats a preview query with the table name and number of rows,\n    executes the query using a predefined SQL connection, and retrieves the data into a DataFrame, which is then returned.\n    \"\"\"\n    # Assuming LuxSQLTable has a method to get the table name and a static method to execute SQL queries\n    table_name = tbl.get_table_name()\n    query = f\"SELECT * FROM {table_name} LIMIT {preview_size}\"\n    \n    # Assuming LuxSQLTable has a static method to execute SQL queries and return a DataFrame\n    result_df = LuxSQLTable.execute_sql_query(query)\n    \n    return result_df", "CodeBleu_score": 30.631594952859142}, {"code": "import pandas as pd\n\ndef execute_preview(tbl: LuxSQLTable, preview_size=5) -> pd.DataFrame:\n    \"\"\"\n    The function execute_preview generates a preview of a SQL table using a specified query template and returns the result as a pandas DataFrame.\n    It takes a LuxSQLTable object and an optional preview size parameter. The function formats a preview query with the table name and number of rows,\n    executes the query using a predefined SQL connection, and retrieves the data into a DataFrame, which is then returned.\n    \"\"\"\n    # Assuming LuxSQLTable has a method to get the table name\n    table_name = tbl.get_table_name()\n    \n    # Formulating the SQL query to fetch a limited number of rows\n    query = f\"SELECT * FROM {table_name} LIMIT {preview_size}\"\n    \n    # Assuming there is a method to execute the query and fetch results into a DataFrame\n    result_df = tbl.execute_query(query)\n    \n    return result_df", "CodeBleu_score": 30.25272862043673}]}
{"instance_id": "b3dee87be4b8", "generation_result": [{"code": "def execute_sampling(tbl: LuxSQLTable):\n    \"\"\"\n    The function execute_sampling takes a LuxSQLTable object tbl as an argument.\n    It retrieves sampling configuration parameters from lux.config, including SAMPLE_FLAG, SAMPLE_START, SAMPLE_CAP, and SAMPLE_FRAC.\n    It executes a SQL query to determine the length of the table and calculates a limit based on a fraction of the table length.\n    Using this limit, it formats and executes a sample query to retrieve a sample of rows from the table.\n    The sampled rows are then stored in the _sampled attribute of the tbl object.\n    \"\"\"\n    import lux  # Assuming lux is a module that needs to be imported\n\n    # Check if sampling is enabled\n    if not lux.config.SAMPLE_FLAG:\n        return\n\n    # Execute a query to get the total number of rows in the table\n    total_rows_query = f\"SELECT COUNT(*) FROM {tbl.name}\"\n    cursor = tbl.connection.cursor()\n    cursor.execute(total_rows_query)\n    total_rows = cursor.fetchone()[0]\n\n    # Calculate the sample limit\n    sample_limit = int(total_rows * lux.config.SAMPLE_FRAC)\n\n    # Cap the sample limit if it exceeds the SAMPLE_CAP\n    if sample_limit > lux.config.SAMPLE_CAP:\n        sample_limit = lux.config.SAMPLE_CAP\n\n    # Format and execute the sample query\n    sample_query = f\"SELECT * FROM {tbl.name} LIMIT {sample_limit} OFFSET {lux.config.SAMPLE_START}\"\n    cursor.execute(sample_query)\n    sampled_data = cursor.fetchall()\n\n    # Store the sampled rows in the _sampled attribute of the tbl object\n    tbl._sampled = sampled_data", "CodeBleu_score": 29.02017052400095}, {"code": "def execute_sampling(tbl: LuxSQLTable):\n    \"\"\"\n    The function execute_sampling takes a LuxSQLTable object tbl as an argument.\n    It retrieves sampling configuration parameters from lux.config, including SAMPLE_FLAG, SAMPLE_START, SAMPLE_CAP, and SAMPLE_FRAC.\n    It executes a SQL query to determine the length of the table and calculates a limit based on a fraction of the table length.\n    Using this limit, it formats and executes a sample query to retrieve a sample of rows from the table.\n    The sampled rows are then stored in the _sampled attribute of the tbl object.\n    \"\"\"\n    import lux  # Assuming lux is a module that needs to be imported\n\n    # Check if sampling is enabled\n    if not lux.config.SAMPLE_FLAG:\n        return\n\n    # Execute a query to get the total number of rows in the table\n    total_rows_query = f\"SELECT COUNT(*) FROM {tbl.name}\"\n    cursor = tbl.connection.cursor()\n    cursor.execute(total_rows_query)\n    total_rows = cursor.fetchone()[0]\n\n    # Calculate the sample limit\n    sample_limit = int(total_rows * lux.config.SAMPLE_FRAC)\n\n    # Cap the sample limit if it exceeds the SAMPLE_CAP\n    if sample_limit > lux.config.SAMPLE_CAP:\n        sample_limit = lux.config.SAMPLE_CAP\n\n    # Format and execute the sample query\n    sample_query = f\"SELECT * FROM {tbl.name} LIMIT {sample_limit}\"\n    cursor.execute(sample_query)\n    sampled_data = cursor.fetchall()\n\n    # Store the sampled rows in the _sampled attribute of the tbl object\n    tbl._sampled = sampled_data", "CodeBleu_score": 28.87865999647492}, {"code": "import lux  # Assuming the necessary import for configuration handling\n\ndef execute_sampling(tbl: LuxSQLTable):\n    \"\"\"Executes a sample query to retrieve a subset of rows based on configured parameters.\n\n    Args:\n    tbl (LuxSQLTable): The table object from which a sample of rows will be retrieved.\n\n    Description:\n    - Retrieves sampling configuration parameters from lux.config.\n    - Determines the length of the table and calculates a sample size.\n    - Fetches a sample of rows based on the calculated limit and stores them in the _sampled attribute of tbl.\n    \"\"\"\n\n    # Retrieve configuration parameters\n    sample_flag = lux.config.SAMPLE_FLAG\n    sample_start = lux.config.SAMPLE_START\n    sample_cap = lux.config.SAMPLE_CAP\n    sample_frac = lux.config.SAMPLE_FRAC\n    \n    if not sample_flag:\n        # Sampling is not enabled\n        return\n\n    # Get total row count from the table\n    db_cursor = tbl.connection.cursor()\n    count_query = f\"SELECT COUNT(*) FROM {tbl.name}\"\n    db_cursor.execute(count_query)\n    total_rows = db_cursor.fetchone()[0]\n\n    # Calculate the sample limit\n    if total_rows * sample_frac > sample_cap:\n        sample_size = sample_cap\n    else:\n        sample_size = int(total_rows * sample_frac)\n\n    # Limit sample size to start from a specific row if needed\n    sample_size -= sample_start\n\n    # Create and execute the sample query\n    sample_query = f\"SELECT * FROM {tbl.name} LIMIT {sample_size} OFFSET {sample_start}\"\n    db_cursor.execute(sample_query)\n    sampled_data = db_cursor.fetchall()\n\n    # Store the sampled data in the table object\n    tbl._sampled = sampled_data\n\n    # Cleanup: Close the cursor\n    db_cursor.close()", "CodeBleu_score": 30.502754275769185}]}
{"instance_id": "485253b1088a", "generation_result": [{"code": "import matplotlib.pyplot as plt\nfrom shapely.geometry.base import BaseGeometry\nfrom typing import Dict\n\ndef draw_text(params: Dict[str, dict], background: BaseGeometry) -> None:\n    \"\"\"\n    The function draw_text places text on a plot with specified parameters and a background.\n    It first overrides default text settings with provided parameters, extracting the text, x, and y values.\n    The function then retrieves the background's bounds and interpolates the x and y values to fit within these bounds.\n    Finally, it uses the plt.text method to draw the text at the calculated position with the remaining parameters.\n    \"\"\"\n    # Extract the bounds of the background geometry\n    minx, miny, maxx, maxy = background.bounds\n    \n    # Set default text parameters if not all parameters are provided in `params`. You may customize further.\n    default_params = {\n        'x': 0.5,  # default x position (center normalized)\n        'y': 0.5,  # default y position (center normalized)\n        's': '',   # text content\n        'fontsize': 12,  # default font size\n        'color': 'black'  # default text color\n    }\n\n    # Update default parameters with values from params\n    for key, value in params.items():\n        default_params.update(value)\n\n    # Calculate the real position using the bounds and normalized positions\n    real_x = minx + default_params['x'] * (maxx - minx)\n    real_y = miny + default_params['y'] * (maxy - miny)\n\n    # Draw text using plt.text\n    plt.text(real_x, real_y, default_params['s'],\n             fontsize=default_params['fontsize'],\n             color=default_params['color'])\n\n    # Display the plot (assumed necessary as part of the requirement, remove if handled elsewhere)\n    plt.show()", "CodeBleu_score": 26.481375576227062}, {"code": "import matplotlib.pyplot as plt\nfrom shapely.geometry.base import BaseGeometry\nfrom typing import Dict\n\ndef draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None:\n    \"\"\"\n    The function draw_text places text on a plot with specified parameters and a background. It first overrides default text settings\n    with provided parameters, extracting the text, x, and y values. The function then retrieves the background's bounds and interpolates\n    the x and y values to fit within these bounds. Finally, it uses the plt.text method to draw the text at the calculated \n    position with the remaining parameters.\n    \n    Args:\n    params (Dict[str, dict]): A dictionary containing parameters for text drawing, expects keys 'text', 'x', 'y' among others.\n    background (BaseGeometry): A shapely BaseGeometry object which provides bounds for scaling the text positioning.\n    \"\"\"\n    \n    # Extract text parameters, with defaults if essential keys are missing:\n    text = params.get('text', \"Sample Text\")\n    x = params.get('x', 0)\n    y = params.get('y', 0)\n    \n    # Additional styling params can be directly passed to plt.text\n    additional_params = {k: v for k, v in params.items() if k not in ['text', 'x', 'y']}\n\n    # Retrieve background geometry bounds\n    minx, miny, maxx, maxy = background.bounds\n    \n    # Translate and scale x, y coordinates into background bounds\n    # Assuming x, y are normalized to the range [0,1]\n    scaled_x = minx + x * (maxx - minx)\n    scaled_y = miny + y * (maxy - miny)\n\n    # Draw the text onto the plot\n    plt.text(scaled_x, scaled_y, text, **additional_params)\n\n    # Display the plot\n    plt.show()", "CodeBleu_score": 24.890914318325496}, {"code": "import matplotlib.pyplot as plt\nfrom typing import Dict\nfrom shapely.geometry.base import BaseGeometry", "CodeBleu_score": 1.0643025142594143e-15}]}
{"instance_id": "5f70a88b6b72", "generation_result": [{"code": "from datetime import datetime\nfrom typing import List\n\ndef _determine_format_string(dates: List[str], datetime_format: str = None) -> str:\n    \"\"\"The function _determine_format_string takes a list of dates and an optional datetime_format.\n       It calculates the average number of days between consecutive dates in the list.\n       If datetime_format is provided, it returns this format.\n       If the average days between points is less than 0.33, indicating intraday data, it checks if the data spans more than one day \n       and sets the format string to '%b %d, %H:%M' or '%H:%M' accordingly.\n       For daily or less frequent data, it checks if the data spans multiple years \n       and sets the format string to '%Y-%b-%d' or '%b %d' as appropriate.\n       Finally, it returns the determined format string.\n    \"\"\"\n    if datetime_format:\n        return datetime_format\n    \n    # Convert strings to datetime objects assuming a common format exists\n    date_objects = [datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\") for date in dates]\n    \n    # Calculate differences between consecutive dates\n    differences = [(date_objects[i+1] - date_objects[i]).total_seconds() / (60 * 60 * 24)\n                    for i in range(len(date_objects) - 1)]\n    \n    # Calculate average difference in days\n    average_difference = sum(differences) / len(differences)\n    \n    # Determine date range\n    date_span_years = date_objects[-1].year - date_objects[0].year\n    date_span_days = (date_objects[-1] - date_objects[0]).days\n    \n    if average_difference < 0.33:  # Intraday data\n        if date_span_days > 1:\n            format_string = '%b %d, %H:%M'\n        else:\n            format_string = '%H:%M'\n    else:  # Daily or less frequent data\n        if date_span_years > 0:\n            format_string = '%Y-%b-%d'\n        else:\n            format_string = '%b %d'\n    \n    return format_string", "CodeBleu_score": 46.389253856187786}, {"code": "def _determine_format_string( dates, datetime_format=None )\nimport datetime\n\ndef _determine_format_string(dates, datetime_format=None):\n    \"\"\"\n    The function _determine_format_string takes a list of dates and an optional datetime_format.\n    It calculates the average number of days between consecutive dates in the list.\n    If datetime_format is provided, it returns this format.\n    If the average days between points is less than 0.33, indicating intraday data, it checks if\n    the data spans more than one day and sets the format string to '%b %d, %H:%M' or '%H:%M' accordingly.\n    For daily or less frequent data, it checks if the data spans multiple years and sets the format string\n    to '%Y-%b-%d' or '%b %d' as appropriate.\n    Finally, it returns the determined format string.\n    \"\"\"\n    if not dates:\n        return \"\"  # or a sensible default like '%Y-%m-%d'\n    \n    if datetime_format is not None:\n        return datetime_format\n\n    # Convert all dates in list to datetime objects, assuming ISO format if not formatted\n    date_objects = [datetime.datetime.fromisoformat(date) if isinstance(date, str) else date for date in dates]\n    date_objects.sort()  # Ensure dates are sorted\n\n    # Find the average number of days between consecutive dates\n    total_days = 0\n    for i in range(1, len(date_objects)):\n        delta = date_objects[i] - date_objects[i-1]\n        total_days += delta.total_seconds() / (3600 * 24)  # Convert seconds to days\n\n    average_days_between = total_days / (len(date_objects) - 1)\n\n    # Check for intraday versus daily or less frequent\n    if average_days_between < 0.33:  # Intraday data\n        span_dates = date_objects[-1].date() != date_objects[0].date()\n        if span_dates:\n            format_string = \"%b %d, %H:%M\"\n        else:\n            format_string = \"%H:%M\"\n    else:  # Daily or less frequent data\n        span_years = date_objects[-1].year != date_objects[0].year\n        if span_years:\n            format_string = \"%Y-%b-%d\"\n        else:\n            format_string = \"%b %d\"\n\n    return format_string\n\n# Example use:\ndate_list = [\"2023-09-01T12:34\", \"2023-09-01T16:21\", \"2023-09-02T09:18\", \"2023-09-03T14:45\"]\nprint(_determine_format_string(date_list))  # Expected: '%b %d, %H:%M' due to spanning multiple days on intraday data\n", "CodeBleu_score": 41.31014436153425}, {"code": "from datetime import datetime\nfrom typing import List\n\ndef _determine_format_string(dates: List[str], datetime_format: str = None) -> str:\n    \"\"\"The function _determine_format_string takes a list of dates and an optional datetime_format.\n    It calculates the average number of days between consecutive dates in the list.\n    If datetime_format is provided, it returns this format.\n    If the average days between points is less than 0.33, indicating intraday data, it checks\n    if the data spans more than one day and sets the format string to '%b %d, %H:%M' or '%H:%M' accordingly.\n    For daily or less frequent data, it checks if the data spans multiple years and sets the format \n    string to '%Y-%b-%d' or '%b %d' as appropriate.\n    Finally, it returns the determined format string.\n    \"\"\"\n    \n    # If a datetime_format is explicitly provided, return it.\n    if datetime_format:\n        return datetime_format\n    \n    if not dates:\n        raise ValueError(\"The list of dates should not be empty.\")\n    \n    # Parse the date strings into datetime objects\n    date_objects = [datetime.strptime(date, \"%Y-%m-%d\") for date in dates]\n    \n    # Calculate differences in days between consecutive dates\n    differences = [(date_objects[i] - date_objects[i - 1]).total_seconds() / (3600 * 24)\n                   for i in range(1, len(date_objects))]\n    \n    # Calculate the average difference in days\n    average_difference = sum(differences) / len(differences)\n    \n    # Determine the format based on average difference\n    min_date = min(date_objects)\n    max_date = max(date_objects)\n    \n    if average_difference < 0.33:\n        # Intraday data\n        if min_date.date() == max_date.date():\n            # Same day\n            return '%H:%M'\n        else:\n            # Spanning multiple days\n            return '%b %d, %H:%M'\n    else:\n        # Daily or less frequent data\n        if min_date.year == max_date.year:\n            # Same year\n            return '%b %d'\n        else:\n            # Spanning multiple years\n            return '%Y-%b-%d'", "CodeBleu_score": 34.00366056495551}]}
{"instance_id": "f9589bf4666d", "generation_result": [{"code": "def _mpf_to_rgba(c, alpha=None)\nfrom matplotlib.colors import to_rgba\n\ndef _mpf_to_rgba(c, alpha=None):\n    \"\"\"\n    The function _mpf_to_rgba converts a color input to an RGBA tuple. It checks if the color input\n    is in uint8 RGB or RGBA format and normalizes it if any RGB components are greater than 1.\n    The normalization scales the RGB components to the range [0, 1]. If the input has an alpha\n    component, it retains it. Finally, the function converts the processed color input to an RGBA\n    tuple using a color conversion utility, optionally applying a specified alpha value.\n\n    Args:\n    c (str or tuple): A color specified as a name (str), hex code (str), or RGB(A) tuple.\n    alpha (float, optional): A float between 0 and 1 specifying the alpha transparency. If None,\n                             the alpha value in c is used if available or defaults to 1 if not.\n\n    Returns:\n    tuple: An RGBA color tuple, each component as a float scaled between 0 and 1.\n    \"\"\"\n    # convert the color to an RGBA tuple\n    rgba = to_rgba(c)\n    \n    # override alpha if provided\n    if alpha is not None:\n        rgba = rgba[:3] + (alpha,)\n\n    return rgba", "CodeBleu_score": 25.540580809951308}, {"code": "import numpy as np\n\ndef _mpf_to_rgba(c, alpha=None):\n    \"\"\"\n    The function _mpf_to_rgba converts a color input to an RGBA tuple. It checks if the color input is in uint8 RGB or RGBA format and normalizes it if any RGB components are greater than 1. \n    The normalization scales the RGB components to the range [0, 1]. If the input has an alpha component, it retains it. Finally, the function converts the processed color input to an \n    RGBA tuple using a color conversion utility, optionally applying a specified alpha value.\n    \n    Args:\n    c (tuple/list): Color input which can be in RGB or RGBA format, e.g., (255,0,0) or (1.0, 0.0, 0.0, 1.0).\n    alpha (float, optional): An alpha value to override the alpha component in the color input. Value should be in the range [0,1].\n    \n    Returns:\n    tuple: Normalized RGBA color in tuple format.\n    \"\"\"\n    \n    # First, we need to convert the input to a NumPy array for easier manipulation:\n    color = np.array(c)\n\n    # Check if the input color includes alpha information:\n    has_alpha = len(color) == 4\n\n    # Normalize RGB components if necessary (assuming 255 scale if numbers > 1):\n    if np.any(color[:3] > 1):\n        color[:3] = color[:3] / 255.0\n    \n    # If alpha is provided and valid, we set it replacing the input's alpha if exists or appending it if not exists\n    if alpha is not None and 0 <= alpha <= 1:\n        if has_alpha:\n            color[3] = alpha\n        else:\n            color = np.append(color, alpha)\n    elif not has_alpha:\n        # If there is no alpha provided and no alpha in input, append 1 (fully opaque):\n        color = np.append(color, 1.0)\n\n    # Limit color channel values to be between 0 and 1\n    color = np.clip(color, 0, 1)\n    \n    # Convert the array back to a tuple and return\n    return tuple(color)\n\n# Example usage\nprint(_mpf_to_rgba((255, 0, 0))) # Should convert to (1, 0, 0, 1)\nprint(_mpf_to_rgba((0.5, 0.2, 0.2, 0.5), alpha=0.8)) # Should convert to (0.5, 0.2, 0.2, 0.8)", "CodeBleu_score": 27.41419304448891}, {"code": "def _mpf_to_rgba(c, alpha=None):\n    \"\"\"\n    The function _mpf_to_rgba converts a color input to an RGBA tuple. It checks if the color input is in uint8 RGB or RGBA format \n    and normalizes it if any RGB components are greater than 1. The normalization scales the RGB components to the range [0, 1]. \n    If the input has an alpha component, it retains it. \n    Finally, the function converts the processed color input to an RGBA tuple using a color conversion utility, \n    optionally applying a specified alpha value.\n    \n    Args:\n    c (tuple or list): Color input as RGB or RGBA.\n    alpha (float, optional): A float between 0 and 1 representing the alpha transparency. \n                             If None, the existing alpha value in c is used if c is RGBA, otherwise defaults to 1.\n\n    Returns:\n    tuple: An RGBA tuple with components normalized to the range [0, 1].\n\n    \"\"\"\n\n    # Check if we have 3 (RGB) or 4 (RGBA) channels\n    if len(c) == 3:\n        r, g, b = c\n        a = alpha if alpha is not None else 1.0  # Default alpha value if not specified and input is RGB\n    elif len(c) == 4:\n        r, g, b, a = c\n        if alpha is not None:\n            a = alpha  # Override the alpha value if specified\n\n    # Normalize RGB values to [0, 1] if they're in the range 0-255\n    if r > 1.0 or g > 1.0 or b > 1.0:\n        r, g, b = r / 255.0, g / 255.0, b / 255.0\n\n    # Normalize alpha value to [0, 1] if it's in the range 0-255 (unlikely, but good to have a complete function)\n    if a > 1.0:\n        a = a / 255.0\n    \n    return (r, g, b, a)", "CodeBleu_score": 24.385615257905712}]}
{"instance_id": "4202e378ab0b", "generation_result": [{"code": "def subplots(self, *args, **kwargs):\n    \"\"\"\n    The function subplots is a method that creates a set of subplots, handling custom styles.\n    It accepts any number of positional and keyword arguments.\n    If the 'style' keyword argument is provided or the object does not have an attribute 'mpfstyle', it applies the specified style using _check_for_and_apply_style and sets 'mpfstyle' attribute to the applied style.\n    Otherwise, it applies the existing 'mpfstyle'.\n    The function then calls the subplots method of matplotlib's Figure class to create the subplots.\n    It assigns the applied style to each of the created axes objects.\n    If the result is a single Axes object, it assigns the style to it directly.\n    If the result is an ndarray of Axes objects, it assigns the style to each Axes in the array.\n    If the result is of an unexpected type, it raises a TypeError.\n    Finally, it returns the created subplots.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    # Check if 'style' is provided or if there's no existing 'mpfstyle' attribute\n    if 'style' in kwargs or not hasattr(self, 'mpfstyle'):\n        style = kwargs.pop('style', 'default')  # Get style from kwargs or default to 'default'\n        self.mpfstyle = self._check_for_and_apply_style(style)\n    else:\n        # Use the existing style\n        self._check_for_and_apply_style(self.mpfstyle)\n\n    # Create the subplots using matplotlib's functionality\n    fig, ax = plt.subplots(*args, **kwargs)\n    \n    # Ensure 'ax' is properly handled whether it's a single Axes object or an array of Axes\n    if isinstance(ax, np.ndarray):\n        for axis in np.nditer(ax, flags=['refs_ok']):\n            self._assign_style_to_axis(axis.item())\n    elif isinstance(ax, plt.Axes):\n        self._assign_style_to_axis(ax)\n    else:\n        raise TypeError(\"Unexpected type of ax object encountered.\")\n\n    return fig, ax\n\ndef _check_for_and_apply_style(self, style):\n    import matplotlib as mpl\n    # Implementation detail for checking and applying a style.\n    # Assume it sets mpl style using mpl.style.use().\n    mpl.style.use(style)\n    return style\n\ndef _assign_style_to_axis(self, axis):\n    # Stub implementation for assigning style to axes. Real-world use might involve more detail.\n    # Function might customize ticks, labels, colors, etc., based on the 'mpfstyle'.\n    axis.grid(True)", "CodeBleu_score": 42.30253842193813}, {"code": "import matplotlib.pyplot as plt\n\nclass PlotManager:\n    def __init__(self):\n        self.mpfstyle = None\n\n    def _check_for_and_apply_style(self, style):\n        \"\"\"Apply matplotlib style.\"\"\"\n        plt.style.use(style)\n\n    def subplots(self, *args, **kwargs):\n        \"\"\"\n        The function subplots is a method that creates a set of subplots, handling custom styles.\n        It accepts any number of positional and keyword arguments.\n        If the 'style' keyword argument is provided or the object does not have an attribute 'mpfstyle',\n        it applies the specified style using _check_for_and_apply_style and sets 'mpfstyle' attribute to the applied style.\n        Otherwise, it applies the existing 'mpfstyle'.\n        The function then calls the subplots method of matplotlib's Figure class to create the subplots.\n        It assigns the applied style to each of the created axes objects.\n        If the result is a single Axes object, it assigns the style to it directly.\n        If the result is an ndarray of Axes objects, it assigns the style to each Axes in the array.\n        If the result is of an unexpected type, it raises a TypeError.\n        Finally, it returns the created subplots.\n        \"\"\"\n        style = kwargs.pop('style', None)\n\n        if style is not None or self.mpfstyle is None:\n            if style is None:\n                style = 'default'  # Assuming default style as fallback\n            self._check_for_and_apply_style(style)\n            self.mpfstyle = style\n        else:\n            self._check_for_and_apply_style(self.mpfstyle)\n\n        fig, axs = plt.subplots(*args, **kwargs)\n\n        if isinstance(axs, plt.Axes):\n            axs.set_title(f\"Styled with: {self.mpfstyle}\")\n        elif isinstance(axs, (np.ndarray, list)):  # Ndarray for grid plots, lists for irregular grids\n            for ax in np.ravel(axs):  # Ravel handles multi-dimensional arrays easily\n                ax.set_title(f\"Styled with: {self.mpfstyle}\")\n        else:\n            raise TypeError(\"Unexpected type of axs returned by plt.subplots()\")\n\n        return fig, axs", "CodeBleu_score": 37.345050346097906}, {"code": "def subplots(self, *args, **kwargs):\n    \"\"\"\n    The function subplots is a method that creates a set of subplots, handling custom styles.\n    It accepts any number of positional and keyword arguments.\n    If the 'style' keyword argument is provided or the object does not have an attribute 'mpfstyle',\n    it applies the specified style using _check_for_and_apply_style and sets 'mpfstyle' attribute to the applied style.\n    Otherwise, it applies the existing 'mpfstyle'.\n    The function then calls the subplots method of matplotlib's Figure class to create the subplots.\n    It assigns the applied style to each of the created axes objects.\n    If the result is a single Axes object, it assigns the style to it directly.\n    If the result is an ndarray of Axes objects, it assigns the style to each Axes in the array.\n    If the result is of an unexpected type, it raises a TypeError.\n    Finally, it returns the created subplots.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    \n    # Check for 'style' in kwargs or if 'mpfstyle' attribute is missing\n    if 'style' in kwargs or not hasattr(self, 'mpfstyle'):\n        style = kwargs.pop('style', None)\n        self.mpfstyle = self._check_for_and_apply_style(style)\n    else:\n        self._check_for_and_apply_style(self.mpfstyle)\n    \n    # Create subplots using matplotlib\n    fig, ax = plt.subplots(*args, **kwargs)\n    \n    # Assign the 'mpfstyle' to each axis\n    if isinstance(ax, plt.Axes):  # Single axes object\n        ax.set_title(f'Style applied: {self.mpfstyle}')\n    elif isinstance(ax, np.ndarray):  # Array of axes\n        for a in ax.flatten():\n            a.set_title(f'Style applied: {self.mpfstyle}')\n    else:\n        raise TypeError(\"Unexpected type of axes object returned.\")\n    \n    # Return created subplots\n    return fig, ax", "CodeBleu_score": 44.076206333966006}]}
{"instance_id": "3e2b6dfe4753", "generation_result": [{"code": "def _adjust_figsize(fig, config):\n    \"\"\"\n    Adjusts the size of a given figure based on a configuration dictionary.\n\n    Parameters:\n    - fig (matplotlib.figure.Figure): The figure to adjust. If this is None, the function will return immediately.\n    - config (dict): Configuration dictionary that may contain keys 'figsize' and 'figratio'.\n                      'figsize' should be a tuple (width, height) in inches.\n                      'figratio' should be a float representing the width to height ratio of the figure.\n\n    Returns:\n    - None: The function modifies the figure size directly and does not return anything.\n    \"\"\"\n\n    import numpy as np\n\n    if fig is None:\n        return\n\n    if 'figsize' in config:\n        # Directly set the figure size if it's specified in the config.\n        fig.set_size_inches(*config['figsize'])\n    elif 'figratio' in config:\n        # Calculate the size based on the figure ratio if provided.\n        current_size = fig.get_size_inches()\n        width, height = current_size[0], current_size[1]\n        desired_ratio = config['figratio']\n\n        # Adjust height based on the desired ratio while keeping the same width.\n        new_height = width / desired_ratio\n\n        # Ensure that the new height is within a reasonable range by comparing it with the original height.\n        if not (0.5 * height < new_height < 2 * height):\n            new_height = np.clip(new_height, 0.5 * height, 2 * height)\n\n        fig.set_size_inches(width, new_height, forward=True)\n    else:\n        # If no size specifications are present, do not modify the size.\n        pass", "CodeBleu_score": 35.250687468425866}, {"code": "import matplotlib.pyplot as plt\n\ndef _adjust_figsize(fig, config):\n    \"\"\"\n    The function _adjust_figsize adjusts the size of a given figure based on a configuration dictionary.\n    If the figure is not provided, it returns immediately. If the configuration does not specify a\n    figure size, it calculates the size using the figure ratio, ensuring the aspect ratio is within\n    a valid range. It scales the dimensions accordingly. If a figure size is specified in the\n    configuration, it uses that size. Finally, it sets the figure size in inches.\n\n    Parameters:\n        fig (matplotlib.figure.Figure): The figure whose size needs to be adjusted.\n        config (dict): Configuration dictionary potentially containing 'figsize' and/or 'aspect_ratio'.\n\n    Returns:\n        None\n    \"\"\"\n\n    if fig is None:\n        print(\"No figure provided.\")\n        return  # Return immediately if no figure is provided\n\n    # Check if 'figsize' is specified in the configuration\n    if 'figsize' in config:\n        new_size = config['figsize']\n    else:\n        # Default aspect ratio and default width\n        default_aspect_ratio = config.get('aspect_ratio', 4/3)  # 4:3 is a commonly used aspect ratio\n        default_width = 6  # Default width in inches\n\n        # Ensure aspect ratio is within a reasonable range, say between 1/4 and 4.\n        aspect_ratio = max(1/4, min(default_aspect_ratio, 4))\n\n        # Calculate figure height based on the width and aspect ratio\n        height = default_width / aspect_ratio\n\n        new_size = (default_width, height)\n\n    # Set the figure size\n    fig.set_size_inches(new_size, forward=True)\n    print(f\"Figure size adjusted to {new_size} inches.\")", "CodeBleu_score": 33.726882535350285}, {"code": "import matplotlib.pyplot as plt\n\ndef _adjust_figsize(fig, config):\n    \"\"\"\n    The function _adjust_figsize adjusts the size of a given figure based on a configuration dictionary. \n    If the figure is not provided, it returns immediately. If the configuration does not specify a figure size, \n    it calculates the size using the figure ratio, ensuring the aspect ratio is within a valid range. \n    It scales the dimensions accordingly. If a figure size is specified in the configuration, it uses that size. \n    Finally, it sets the figure size in inches.\n\n    Parameters:\n        fig (matplotlib.figure.Figure): The figure to adjust.\n        config (dict): Configuration dictionary that may contain keys like 'figsize' and 'aspect_ratio'.\n\n    Returns:\n        None\n    \"\"\"\n    if fig is None:\n        return\n\n    if 'figsize' in config:\n        # If figsize is specified in the configuration, set it directly\n        new_size = config['figsize']\n    else:\n        # If figsize is not specified, compute it based on the aspect ratio (if given)\n        current_size = fig.get_size_inches()\n        aspect_ratio = config.get('aspect_ratio', current_size[1] / current_size[0])\n\n        # Ensure the aspect ratio is within a valid range\n        if aspect_ratio < 0.2:\n            aspect_ratio = 0.2\n        elif aspect_ratio > 5:\n            aspect_ratio = 5\n\n        # Compute new size keeping width fixed and adjusting height\n        new_height = current_size[0] * aspect_ratio\n        new_size = (current_size[0], new_height)\n\n    # Set the new figure size\n    fig.set_size_inches(new_size, forward=True)\n\n# Example usage with a sample figure and config\nfig, ax = plt.subplots()\nconfig = {\n    'aspect_ratio': 0.5\n}\n_adjust_figsize(fig, config)\nplt.show()", "CodeBleu_score": 36.00803354319349}]}
{"instance_id": "986e5804b941", "generation_result": [{"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import colors as mcolors\n\ndef _plot_mav(ax, config, xdates, prices, apmav=None, apwidth=None):\n    \"\"\"\n    The function _plot_mav plots moving averages on a given axis ax using provided configuration \n    config, date data xdates, and price data prices.\n    It determines the moving averages to plot from the config or apmav parameter.\n    If the moving averages are specified as a dictionary, it extracts the shift values and periods.\n    The function ensures at most 7 moving averages are plotted.\n    For each moving average, it calculates the rolling mean of the prices, applies any shifts, \n    and plots the result on the axis using the specified line width and color cycle from the config.\n    The function returns a list of the calculated moving average prices.\n    \"\"\"\n    \n    if apmav is None:\n        apmav = config.get('mav', {})\n    if apwidth is None:\n        apwidth = config.get('linewidth', 1)\n    \n    # Prepare a color cycle using the default matplotlib color cycle or the one specified in the configuration\n    default_color_cycle = list(mcolors.TABLEAU_COLORS)  # Default color cycle\n    color_cycle = config.get('color_cycle', default_color_cycle)\n    color_iterator = iter(color_cycle)\n    \n    max_num_mav = 7  # enforce a maximum of 7 moving averages\n    mav_results = []  # to store results of moving averages\n    \n    try:\n        sorted_mavs = sorted(apmav.items(), key=lambda x: x[0])[:max_num_mav]  # sort and limit MAV definitions\n    except AttributeError:\n        sorted_mavs = [(period, 0) for period in sorted(apmav)[:max_num_mav]]  # handle cases where apmav is a list/tuple\n    \n    for period, shift in sorted_mavs:\n        # Calculate rolling mean\n        mav_series = pd.Series(prices).rolling(window=period, center=False).mean()\n        \n        if shift != 0:\n            mav_series = mav_series.shift(shift)  # shift if necessary\n        \n        # Store the result\n        mav_results.append(mav_series.tolist())\n        \n        # Plotting\n        ax.plot(xdates, mav_series, linewidth=apwidth, label=f'MAV-{period}', color=next(color_iterator))\n    \n    # Update the legend to reflect added lines\n    ax.legend()\n    \n    return mav_results\n\n# Note: `ax`, `config`, `xdates`, and `prices` must be provided correctly before using _plot_mav.\n# Example usage requires matplotlib's ax (axis) object, configuration dictionary with needed keys,\n# an array or list of dates, and respective prices.", "CodeBleu_score": 32.28360868113419}, {"code": "import numpy as np\nimport pandas as pd\n\ndef _plot_mav(ax, config, xdates, prices, apmav=None, apwidth=None):\n    \"\"\"\n    The function _plot_mav plots moving averages on a given axis ax using provided configuration config,\n    date data xdates, and price data prices.\n    It determines the moving averages to plot from the config or apmav parameter.\n    If the moving averages are specified as a dictionary, it extracts the shift values and periods.\n    The function ensures at most 7 moving averages are plotted.\n    For each moving average, it calculates the rolling mean of the prices, applies any shifts, and plots the result on the axis using the specified line width and color cycle from the config.\n    The function returns a list of the calculated moving average prices.\n    \"\"\"\n    \n    if apmav is None:\n        apmav = config.get('mav', [3, 5, 7, 9, 12, 26, 60])\n    if apwidth is None:\n        apwidth = config.get('linewidth', 1)\n\n    color_cycle = config.get('colors', ['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n    mav_config = config.get('shifts', dict(zip(apmav, [0] * len(apmav))))\n\n    # Limit to at most 7 moving averages\n    num_ma = min(len(apmav), 7)\n    mav_prices = []\n\n    # Calculate and plot each moving average\n    for i in range(num_ma):\n        ma_period = apmav[i]\n        shift_val = mav_config.get(ma_period, 0)\n        \n        # Compute the moving average and apply the shift if necessary\n        ma_prices = prices.rolling(window=ma_period, min_periods=1).mean()\n        if shift_val != 0:\n            ma_prices = ma_prices.shift(shift_val)\n        \n        # Store the computed moving averages for returning\n        mav_prices.append(ma_prices)\n        \n        # Plot each moving average with the defined configurations\n        ax.plot(xdates, ma_prices, label=f'MA({ma_period}) Shift({shift_val})',\n                linewidth=apwidth, color=color_cycle[i % len(color_cycle)])\n\n    ax.legend()\n    return mav_prices", "CodeBleu_score": 30.553503513228485}, {"code": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)\nimport pandas as pd\nimport numpy as np\nfrom itertools import cycle\n\ndef _plot_mav(ax, config, xdates, prices, apmav=None, apwidth=None):\n    \"\"\"\n    The function _plot_mav plots moving averages on a given axis ax using provided configuration config, date data xdates, and price data prices.\n    It determines the moving averages to plot from the config or apmav parameter.\n    If the moving averages are specified as a dictionary, it extracts the shift values and periods.\n    The function ensures at most 7 moving averages are plotted.\n    For each moving average, it calculates the rolling mean of the prices, applies any shifts, and plots the result on the axis using the specified line width and color cycle from the config.\n    The function returns a list of the calculated moving average prices.\n    \n    Parameters:\n    - ax: matplotlib axis on which to plot.\n    - config: dictionary containing plot configuration such as color cycle and line width.\n    - xdates: list or array of dates corresponding to the prices.\n    - prices: list or array of price values.\n    - apmav: Optional; dictionary of moving average configurations, where each key is a label and each value is a tuple (period, shift).\n    - apwidth: Optional; line width for moving average plots.\n\n    Returns:\n    - mav_values: list of arrays containing the calculated moving average values.\n    \"\"\"\n\n    if apmav is None:\n        apmav = config.get('apmav', {})\n\n    period_shift_pairs = list(apmav.values())[:7]  # Ensuring not to plot more than 7 moving averages.\n\n    # Defaults from config\n    color_cycle = cycle(config.get('colors', ['b', 'g', 'r', 'c', 'm', 'y', 'k']))\n\n    line_width = apwidth if apwidth is not None else config.get('line_width', 2)\n\n    mav_values = []\n\n    prices_series = pd.Series(prices, index=pd.to_datetime(xdates))\n    \n    for period, shift in period_shift_pairs:\n        rolling_mean = prices_series.rolling(window=period, center=True).mean()\n        \n        if shift:\n            rolling_mean = rolling_mean.shift(shift)\n\n        mav_values.append(rolling_mean)\n\n        ax.plot(xdates, rolling_mean, label=f'MAV {period} (Shift {shift})',\n                linewidth=line_width, color=next(color_cycle))\n\n    ax.legend()\n    \n    return mav_values", "CodeBleu_score": 32.62378381078656}]}
{"instance_id": "5308e20f3c90", "generation_result": [{"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs):\n    \"\"\"\n    Calculates the number of floating-point operations (FLOPs) required\n    for a convolution operation in a neural network.\n\n    Parameters:\n    - opnode (ConvolutionForward): The convolution operation node containing layer information.\n    - inputs (tuple): Tuple containing the input tensor to the convolution.\n    - outputs (tuple): Tuple containing the output tensor from the convolution.\n\n    Returns:\n    - int: Total number of FLOPs for the convolution operation.\n    \"\"\"\n    # Unpack input and output tensor dimensions:\n    # Assuming input shape (N, C_in, H_in, W_in)\n    # Assuming output shape (N, C_out, H_out, W_out)\n    N, C_in, H_in, W_in = inputs[0].shape\n    _, C_out, H_out, W_out = outputs[0].shape\n\n    # Extract kernel size and stride from the operation node (assuming these attributes exist)\n    kernel_h, kernel_w = opnode.kernel_size\n    stride_h, stride_w = opnode.stride\n\n    # Calculate total number of outputs elements\n    total_output_elements = N * C_out * H_out * W_out\n\n    # Calculating FLOPs:\n    # * Each output element requires multiplies and additions per each input channel\n    # * for each slot in the kernel.\n    flops_per_output_element = C_in * kernel_h * kernel_w * 2  # x2 for multiply and add\n\n    total_flops = total_output_elements * flops_per_output_element\n\n    # Check if there is a bias term to add, assuming a bias exists if the attribute is True\n    if getattr(opnode, 'has_bias', False):\n        total_flops += total_output_elements  # For each output element, one more addition is required\n\n    return total_flops", "CodeBleu_score": 23.385331913144547}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs):\n    \"\"\"\n    Calculates the number of floating-point operations (FLOPs) for a convolution operation.\n    \n    Parameters:\n        opnode (ConvolutionForward): An object containing information about the convolution operation,\n                                     likely including kernel size, padding, stride, dilation,\n                                     and whether or not it uses a bias.\n        inputs (Tensor): A tensor representing the input to the convolution. Shape is typically\n                         (batch_size, num_channels_in, height_in, width_in).\n        outputs (Tensor): A tensor representing the output of the convolution. Shape is typically\n                          (batch_size, num_channels_out, height_out, width_out).\n\n    Returns:\n        int: Total FLOPs for the convolution operation.\n\n    Assuming:\n    - opnode has attributes kernel_size, in_channels, out_channels, and use_bias.\n    - inputs have shape where inputs.shape = [batch_size, num_channels_in, height_in, width_in],\n    - outputs have shape where outputs.shape = [batch_size, num_channels_out, height_out, width_out],\n    - Kernel size is specified as (height, width),\n    - The number of output elements is batch_size * num_channels_out * height_out * width_out.\n    \"\"\"\n    \n    kernel_height, kernel_width = opnode.kernel_size\n    num_input_channels = opnode.in_channels\n    num_output_channels = opnode.out_channels\n    use_bias = opnode.use_bias\n    \n    batch_size, _, height_out, width_out = outputs.shape\n    \n    # Calculate the total number of output elements\n    total_output_elements = batch_size * num_output_channels * height_out * width_out\n\n    # FLOPS per output element\n    flops_per_element = kernel_height * kernel_width * num_input_channels\n\n    # Multiply by the number of output elements\n    total_flops = flops_per_element * total_output_elements\n\n    # If there's a bias, add one operation per output element\n    if use_bias:\n        total_flops += total_output_elements\n\n    return total_flops", "CodeBleu_score": 17.611800530957396}, {"code": "def flops_conv(opnode: ConvolutionForward, inputs, outputs):\n    \"\"\"\n    The function flops_conv calculates the number of floating-point operations (FLOPs) required for \n    a convolution operation in a neural network. It takes as input a convolution operation node, input \n    tensors, and output tensors. The function extracts the kernel height and width from the input tensor's \n    shape, determines the number of input channels, and calculates the product of the output tensor's dimensions. \n    It checks if the operation includes a bias term and computes the FLOPs by multiplying the total number \n    of output elements by the number of input channels and the kernel size, adding the bias if present. It \n    returns the computed FLOPs.\n    \n    Args:\n    - opnode (ConvolutionForward): The convolution operation node that carries attributes like kernel size, \n                                   strides, pads, and whether it includes a bias.\n    - inputs (list): List of input tensors. Typically [input_tensor, weights, (optionally) bias]\n    - outputs (list): List of output tensors. Typically [output_tensor]\n\n    Returns:\n    - int: The computed number of floating-point operations required for the convolution.\n    \"\"\"\n    \n    # Assuming the third optional input is the bias if the length of inputs is 3.\n    has_bias = len(inputs) == 3\n    \n    # Unpack input tensor and weight tensor\n    input_tensor, weight_tensor = inputs[0], inputs[1]\n    \n    # Get the attributes from the opnode\n    kernel_height, kernel_width = opnode.kernel_size\n    stride_height, stride_width = opnode.stride\n    pad_height, pad_width = opnode.pad\n\n    # Input dimensions\n    num_input_channels = input_tensor.shape[1]  # Assuming input tensor shape is (N, C, H, W)\n    input_height, input_width = input_tensor.shape[2], input_tensor.shape[3]\n    \n    # Kernel dimensions\n    num_output_channels = weight_tensor.shape[0]  # Assuming weight tensor shape is (M, C, kH, kW)\n\n    # Compute the dimensions of the output tensor\n    output_height = ((input_height - kernel_height + 2 * pad_height) // stride_height) + 1\n    output_width = ((input_width - kernel_width + 2 * pad_width) // stride_width) + 1\n\n    # Total number of outputs in the batch\n    num_outputs = outputs[0].shape[0] * num_output_channels * output_height * output_width\n    \n    # Compute FLOPs for convolution\n    # FLOPs for each output element: kernel_height * kernel_width * num_input_channels\n    flops_per_output_element = kernel_height * kernel_width * num_input_channels\n    \n    # Total FLOPs\n    total_flops = num_outputs * flops_per_output_element\n    \n    # Adding bias FLOPs (1 multiplication per output channel if bias is present)\n    if has_bias:\n        total_flops += num_outputs\n\n    return total_flops", "CodeBleu_score": 27.302349496427137}]}
{"instance_id": "70811d3cf578", "generation_result": [{"code": "import numpy as np\n\n# Dictionary mapping NumPy dtypes to their corresponding IR type factory functions.\n# Here `_ir_type_factory_***` are placeholders for actual factory function names, which we assume exist.\n# Imagine these functions exist and would actually produce an \"IR type\" for given NumPy dtype.\n_dtype_to_ir_type = {\n    np.float32: _ir_type_factory_float32,\n    np.float64: _ir_type_factory_float64,\n    np.int32: _ir_type_factory_int32,\n    np.int64: _ir_type_factory_int64,\n    # Add further mappings as necessary\n}\n\ndef _ir_type_factory_float32():\n    # Example factory function for np.float32\n    return \"IRType for Float32\"\n\ndef _ir_type_factory_float64():\n    # Example factory function for np.float64\n    return \"IRType for Float64\"\n\ndef _ir_type_factory_int32():\n    # Example factory function for np.int32\n    return \"IRType for Int32\"\n\ndef _ir_type_factory_int64():\n    # Example factory function for np.int64\n    return \"IRType for Int64\"\n\ndef mge_dtype_to_ir_type(mge_dtype):\n    \"\"\"\n    The function mge_dtype_to_ir_type converts a given data type mge_dtype to its\n    corresponding intermediate representation (IR) type.\n\n    It first ensures that mge_dtype is a NumPy data type by converting it using np.dtype.\n    It then asserts that the converted mge_dtype is indeed a NumPy data type,\n    raising an error if it is not.\n    Finally, it retrieves the appropriate IR type factory from the _dtype_to_ir_type\n    dictionary using mge_dtype and returns the result of calling this factory function.\n\n    Args:\n    mge_dtype (data type): Data type to convert, expected to be a NumPy data type or something convertible to it.\n\n    Returns:\n    object: The result of the corresponding IR type factory.\n\n    Raises:\n    ValueError: If mge_dtype is not a valid or supported NumPy data type.\n    \"\"\"\n    np_dtype = np.dtype(mge_dtype)  # Convert mge_dtype to numpy dtype\n    assert np_dtype in _dtype_to_ir_type, \"Unsupported dtype!\"  # Check if the dtype is supported\n    return _dtype_to_ir_type[np_dtype]()  # Call the factory function and return its result\n\n# Note: In the real implementation, ensure you have actual callable and implementing factories or handles\n# for converting NumPy data types to intermediate representation types.", "CodeBleu_score": 33.754768672557404}, {"code": "import numpy as np\n\n# Dictionary to map NumPy data types to hypothetical IR types\n_dtype_to_ir_type = {\n    np.dtype('float32'): lambda: \"IR_Float32\",\n    np.dtype('float64'): lambda: \"IR_Float64\",\n    np.dtype('int32'): lambda: \"IR_Int32\",\n    np.dtype('int64'): lambda: \"IR_Int64\",\n    np.dtype('bool'): lambda: \"IR_Bool\",\n    # Add more mappings here as needed\n}\n\ndef mge_dtype_to_ir_type(mge_dtype):\n    \"\"\"\n    The function mge_dtype_to_ir_type converts a given data type mge_dtype to its \n    corresponding intermediate representation (IR) type.\n    It first ensures that mge_dtype is a NumPy data type by converting it using np.dtype.\n    It then asserts that the converted mge_dtype is indeed a NumPy data type, \n    raising an error if it is not.\n    Finally, it retrieves the appropriate IR type factory from the _dtype_to_ir_type \n    dictionary using mge_dtype and returns the result of calling this factory function.\n    \"\"\"\n    # Convert mge_dtype to a NumPy dtype\n    dtype = np.dtype(mge_dtype)\n\n    # Ensure mge_dtype is a NumPy dtype\n    if not isinstance(dtype, np.dtype):\n        raise ValueError(\"mge_dtype must be a valid NumPy dtype\")\n    \n    # Using get to safely access dictionary with the dtype, but raise error if not found\n    if dtype not in _dtype_to_ir_type:\n        raise ValueError(f\"No IR type defined for dtype {dtype}\")\n\n    # Get the IR type factory function from the dictionary\n    ir_type_factory = _dtype_to_ir_type[dtype]\n\n    # Call factory function to get the IR type\n    ir_type = ir_type_factory()\n    \n    return ir_type", "CodeBleu_score": 43.9763646253117}, {"code": "import numpy as np\n\ndef mge_dtype_to_ir_type(mge_dtype):\n    \"\"\"\n    The function mge_dtype_to_ir_type converts a given data type mge_dtype to its corresponding intermediate \n    representation (IR) type.\n    It first ensures that mge_dtype is a NumPy data type by converting it using np.dtype.\n    It then asserts that the converted mge_dtype is indeed a NumPy data type, raising an error if it is not.\n    Finally, it retrieves the appropriate IR type factory from the _dtype_to_ir_type dictionary using mge_dtype \n    and returns the result of calling this factory function.\n    \"\"\"\n    \n    # Ensure mge_dtype is a NumPy data type\n    np_dtype = np.dtype(mge_dtype)\n\n    # Check if it is indeed a NumPy data type (np.dtype always returns a NumPy dtype, thus redundant, but let's assume a check)\n    if not isinstance(np_dtype, np.dtype):\n        raise ValueError(\"Provided data type is not a recognizable NumPy data type.\")\n    \n    # Mapping of NumPy data types to IR types\n    _dtype_to_ir_type = {\n        np.dtype('float32'): lambda: \"Float32IR\",  # Hypothetical factory function for IR types\n        np.dtype('float64'): lambda: \"Float64IR\",\n        np.dtype('int32'): lambda: \"Int32IR\",\n        np.dtype('int64'): lambda: \"Int64IR\",\n        np.dtype('bool'): lambda: \"BoolIR\",\n    }\n    \n    # Get the factory function from map\n    if np_dtype in _dtype_to_ir_type:\n        ir_type_factory = _dtype_to_ir_type[np_dtype]\n    else:\n        raise TypeError(\"Unsupported data type for conversion to IR type.\")\n    \n    # Return the result of calling the factory function\n    return ir_type_factory()", "CodeBleu_score": 44.35295502365511}]}
{"instance_id": "a108c9ba2c22", "generation_result": [{"code": "import numpy as np\nfrom typing import Sequence, Any\nimport some_ir_tool as ir  # This is a placeholder for whatever tool or library you use for IR operations\n\ndef dtype_to_ir_type(dtype):\n    \"\"\"Convert NumPy dtype to specific IR type format.\"\"\"\n    if dtype == np.float32:\n        return ir.Type.create_float(32)\n    elif dtype == np.int32:\n        return ir.Type.create_integer(32)\n    elif dtype == np.bool_:\n        return ir.Type.create_bool()\n    elif dtype == np.float16:  # Assuming handling of bfloat16 similar to float16 for example purposes\n        return ir.Type.create_float(16)\n    else:\n        raise NotImplementedError(\"Unsupported dtype for conversion to IR type\")\n\ndef _numpy_array_constant(x: np.ndarray, canonicalize_types: bool) -> Sequence[Any]:\n    \"\"\"Convert a NumPy array into an intermediate representation (IR) constant.\n    \n    Optionally canonicalizes the array's data type, converts the dtype to an IR type,\n    and handles special cases for boolean and bfloat16 types. The array is made contiguous\n    in memory, and a dense elements attribute is created from it. Finally, it returns the\n    result of an HLO ConstantOp initialized with this attribute.\n    \"\"\"\n    if canonicalize_types:\n        # example of canonicalizing types, this might actually involve more sophisticated logic\n        x = x.astype('float32' if x.dtype in [np.float64, np.float32] else 'int32')\n\n    x = np.ascontiguousarray(x)  # Ensure the array is contiguous in memory\n    ir_type = dtype_to_ir_type(x.dtype)  # Convert dtype to compatible IR type\n    \n    # Convert array to a list of IR values. Assume we use dense elements representation in the IR\n    dense_elements_attr = ir.DenseAttr.create_from_numpy(x, ir_type)\n    \n    # Create HLO ConstantOp with the dense elements attribute\n    constant_op = ir.HLOConstantOp.create(dense_elements_attr)\n    \n    # Return the IR value (constant_op in our simulation)\n    return [constant_op]\n\n# The above implementation is high-level and assumes existence of specific methods and classes\n# (like ir.DenseAttr.create_from_numpy, ir.HLOConstantOp.create) which need to be adjusted\n# according to your actual IR handling libraries and APIs.", "CodeBleu_score": 32.78108338745038}, {"code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]\nimport numpy as np\nfrom typing import Sequence\nfrom mlir.ir import Value, DenseElementsAttr, IntegerType, FloatType, BoolType, RankedTensorType\n\ndef canon_dtype(dtype: np.dtype):\n    \"\"\"Convert NumPy dtype to canonical MLIR type.\"\"\"\n    if np.issubdtype(dtype, np.integer):\n        return IntegerType.get_signless(dtype.itemsize * 8)\n    elif np.issubdtype(dtype, np.floating):\n        return FloatType.get_f64() if dtype == np.float64 else FloatType.get_f32()\n    elif np.issubdtype(dtype, np.bool_):\n        return BoolType.get()\n    # Extend with more types or custom behaviour as needed\n    else:\n        raise ValueError(f\"Unsupported data type: {dtype}\")\n\ndef _numpy_array_constant(x: np.ndarray, canonicalize_types: bool) -> Sequence[Value]:\n    \"\"\"\n    The function _numpy_array_constant converts a NumPy array into an intermediate \n    representation (IR) constant. It optionally canonicalizes the array's data type, \n    converts the dtype to an IR type, and handles special cases for boolean and bfloat16 types.\n    The array is made contiguous in memory, and a dense elements attribute is created from it.\n    Finally, it returns the result of an HLO ConstantOp initialized with this attribute.\n    \"\"\"\n    # Ensure array is contiguous\n    x = np.ascontiguousarray(x)\n\n    # Canonicalize dtype if required\n    dtype = canon_dtype(x.dtype) if canonicalize_types else x.dtype\n\n    # Create an MLIR dense elements attribute from the numpy array\n    # Memory buffer must be cast correctly based on the dtype\n    if np.issubdtype(x.dtype, np.integer) or np.issubdtype(x.dtype, np.floating):\n        buffer = x.ravel().view(dtype=np.uint8)\n    elif np.issubdtype(x.dtype, np.bool_):\n        buffer = x.ravel().astype(np.int8)\n    else:\n        raise ValueError(\"Unsupported dtype for MLIR dense elements attribute.\")\n\n    # Create the tensor type for the dense attribute\n    tensor_type = RankedTensorType.get(x.shape, dtype)\n    \n    # Create the DenseElementsAttr\n    attr = DenseElementsAttr.get(tensor_type, buffer)\n\n    # Create the MLIR HLO ConstantOp (assuming existence of some global MLIR context and builder)\n    from mlir.dialects import hlo\n    constant_op = hlo.ConstantOp(attr)\n\n    # Return the result which is a sequence of values (even as one item)\n    return [constant_op.result]\n\n# Note: The implementation depends greatly on the specifics of the MLIR Python binding.\n# This example assumes some hypothetical functions and classes like `hlo.ConstantOp` \n# which might not be exactly available. Adjust according to your actual environment.\n", "CodeBleu_score": 37.36083706323856}, {"code": "import numpy as np\nfrom typing import Sequence\nfrom tensorflow.compiler.xla.service import hlo\nfrom tensorflow.compiler.mlir.tensorflow.ir import tf_ops as ir\n\ndef _numpy_array_constant(x: np.ndarray, canonicalize_types: bool) -> ir.Value:\n    \"\"\"\n    The function _numpy_array_constant converts a NumPy array into an intermediate representation (IR) constant.\n    It optionally canonicalizes the array's data type, converts the dtype to an IR type, and handles special cases for boolean and bfloat16 types.\n    The array is made contiguous in memory, and a dense elements attribute is created from it.\n    Finally, it returns the result of an HLO ConstantOp initialized with this attribute.\n    \"\"\"\n    # Ensure the array is contiguous\n    if not x.flags['C_CONTIGUOUS']:\n        x = np.ascontiguousarray(x)\n\n    if canonicalize_types:\n        if x.dtype == np.float16:\n            x = x.astype(np.float32)\n        elif x.dtype == np.bool_:\n            x = x.astype(np.int8)\n\n    # Import and initialize MLIR context and builder if necessary here\n    # Since actual MLIR context and build initialization code might be very complex and environment-specific,\n    # consider the following as illustrative dummy code.\n    # context = ir.Context()\n    # builder = ir.Builder(context)\n\n    # Create Tensor IR type based on numpy dtype\n    ir_dtype = _numpy_dtype_to_ir_type(x.dtype)  # You may need to define this function to convert numpy dtype to TF IR types\n\n    # Define the shape of the tensor\n    shape = list(x.shape)\n\n    # Encode data into the Dense Elements Attribute\n    dense_attr = ir.DenseElementsAttr.get(x, type=ir_dtype)\n\n    # Create an MLIR operation using dummy Op (it is necessary to define the used MLIR module for real usage).\n    constant_op = ir.ConstantOp(dense_attr)\n\n    return constant_op\n\n# Example usage\nx = np.array([1, 2, 3, 4], dtype=np.int32)\nir_value = _numpy_array_constant(x, canonicalize_types=True)\nprint(ir_value)", "CodeBleu_score": 44.81354443839628}]}
{"instance_id": "3345fe4d008b", "generation_result": [{"code": "import numpy as np\nfrom typing import Sequence\nimport numpy.typing as npt\nfrom numba import ir\n\ndef _ndarray_constant_handler(\n    val: npt.ArrayLike, canonicalize_types: bool\n) -> Sequence[ir.Value]:\n    \"\"\"\n    The function _ndarray_constant_handler processes a NumPy array val and a flag canonicalize_types.\n    If the array has any zero strides and is non-empty, it identifies the zero-stride and non-zero-stride axes.\n    It collapses the array along the zero-stride axes and optionally canonicalizes its data type.\n    Then, it creates a broadcast operation for the collapsed array, matching the original shape and non-zero-stride axes,\n    and returns the result.\n    If the array does not have zero strides, it processes the array using _numpy_array_constant with the given \n    canonicalize_types flag.\n    \"\"\"\n\n    # Convert input to a numpy array\n    ndarray = np.asarray(val)\n\n    # Check for zero strides and non-empty array\n    zero_stride_axes = [i for i, stride in enumerate(ndarray.strides) if stride == 0]\n    non_zero_stride_axes = [i for i, stride in enumerate(ndarray.strides) if stride != 0]\n\n    if zero_stride_axes and ndarray.size != 0:\n        # Create a collapsed version of the array by removing the zero-stride dimensions\n        squeeze_axes = tuple(zero_stride_axes)\n        collapsed_array = np.squeeze(ndarray, axis=squeeze_axes)\n\n        # Optionally canonicalize data type\n        if canonicalize_types:\n            collapsed_array = collapsed_array.astype(np.min_scalar_type(collapsed_array))\n\n        # Dummy representation of IR value for the purpose of this example\n        ir_value_collapsed = ir.Const(collapsed_array)\n\n        # Simulating broadcasting operation\n        # Normally, user should provide actual logic or environment capable of such operations\n        # Broadcasting to original array shape by introducing ones in zero_stride places\n        desired_shape = list(ndarray.shape)\n        for axis in zero_stride_axes:\n            desired_shape[axis] = 1\n        \n        # Creating a broadcasted IR value (not actual implementation, just placeholder)\n        broadcasted_array = np.broadcast_to(collapsed_array, shape=tuple(desired_shape))\n        ir_value_broadcasted = ir.Const(broadcasted_array)\n\n        return [ir_value_collapsed, ir_value_broadcasted]\n    else:\n        # Handling when there are no zero strides\n        # Below is a placeholder implementation for `_numpy_array_constant`\n        # Replace `_numpy_array_constant` with the actual processing method if available\n        if canonicalize_types:\n            ndarray = ndarray.astype(np.min_scalar_type(ndarray))\n        \n        # Dummy representation of IR value\n        ir_array_constant = ir.Const(ndarray)\n        return [ir_array_constant]", "CodeBleu_score": 35.38466979465385}, {"code": "import numpy as np\nfrom typing import Sequence\n\ndef _numpy_array_constant(array: np.ndarray, canonicalize: bool) -> np.ndarray:\n    \"\"\"Placeholder for _numpy_array_constant function.\"\"\"\n    if canonicalize:\n        # Presumably convert the dtype to a canonical form, e.g., float64 or similar\n        array = array.astype(np.float64)\n    return array\n\ndef _ndarray_constant_handler(\n    val: np.ndarray, \n    canonicalize_types: bool\n) -> np.ndarray:\n    \"\"\"The function _ndarray_constant_handler processes a NumPy array val and a flag canonicalize_types.\n    If the array has any zero strides and is non-empty, it identifies the zero-stride\n    and non-zero-stride axes. It collapses the array along the zero-stride axes and optionally \n    canonicalizes its data type. Then, it creates a broadcast operation for the collapsed array, \n    matching the original shape and non-zero-stride axes, and returns the result.\n    If the array does not have zero strides, it processes the array using _numpy_array_constant \n    with the given canonicalize_types flag.\n    \"\"\"\n    \n    if val.size == 0:\n        raise ValueError(\"Input array is empty.\")\n    \n    # Check if there are any zero strides\n    zero_strides = np.where(np.array(val.strides) == 0)[0]\n    \n    if len(zero_strides) > 0:\n        # Only consider axes with zero stride, collapse the array along these axes\n        keep_dims = tuple(ax for ax in range(val.ndim) if ax not in zero_strides)\n        collapsed_array = val.sum(axis=keep_dims, keepdims=True)\n        \n        if canonicalize_types:\n            collapsed_array = collapsed_array.astype(np.float64)\n        \n        # Now use np.broadcast_to to match the original array's shape\n        result = np.broadcast_to(collapsed_array, val.shape)\n    else:\n        # Use the existing or hypothetical function to handle this part\n        result = _numpy_array_constant(val, canonicalize_types)\n    \n    return result", "CodeBleu_score": 38.07951379513044}, {"code": "import numpy as np\nfrom typing import Sequence\n\ndef _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types: bool,\n) -> Sequence[np.ndarray]:\n    \"\"\" Handles a numpy array `val` with zero strides, collapses it, and optionally\n    canonicalizes its types. If no zero strides, falls back to the simpler loading method. \n    \n    Args:\n        val (np.ndarray): A NumPy array potentially with zero strides.\n        canonicalize_types (bool): A flag to decide if data type canonicalization is to be performed.\n        \n    Returns:\n        Sequence[np.ndarray]: A sequence containing the processed array, typically for embedding in some IR.\n    \"\"\"\n    \n    # Check for zero strides\n    if any(s == 0 for s in val.strides) and val.size > 0:\n        # Find axes with zero and non-zero strides\n        zero_stride_axes = [i for i, stride in enumerate(val.strides) if stride == 0]\n        non_zero_stride_axes = [i for i, stride in enumerate(val.strides) if stride != 0]\n        \n        # Collapse the array across zero-stride dimensions\n        collapsed_shape = tuple(val.shape[i] for i in non_zero_stride_axes)\n        collapsed_view = val.reshape(collapsed_shape)\n        \n        # Optionally canonicalize types\n        if canonicalize_types:\n            dtype = np.result_type(collapsed_view)\n            collapsed_view = collapsed_view.astype(dtype)\n        \n        # Create a broadcast operation to restore original shape\n        # Note: Using np.broadcast_to to simulate broadcast semantics\n        broadcasted_view = np.broadcast_to(collapsed_view, val.shape)\n        \n        # For the purpose of the example, return as list of arrays\n        return [broadcasted_view]\n    \n    else:\n        # Handle arrays without zero strides using a (not implemented here) function\n        return _numpy_array_constant(val, canonicalize_types)\n\ndef _numpy_array_constant(array: np.ndarray, canonicalize_types: bool):\n    \"\"\" Placeholder for handling NumPy arrays with non-zero strides \"\"\"\n    if canonicalize_types:\n        dtype = np.result_type(array)\n        array = array.astype(dtype)\n    return [array]\n\n# Example usage for an array with zero strides:\na = np.zeros((10, 5))\nb = np.broadcast_to(np.array([1, 2, 3, 4, 5]), (10, 5))\nexample_output = _ndarray_constant_handler(b, canonicalize_types=False)\nprint(example_output[0])", "CodeBleu_score": 43.73245179960103}]}
{"instance_id": "91b298678351", "generation_result": [{"code": "from typing import Sequence\n\nclass DenseElementsAttr:\n    def __init__(self, packed_bits: int, bit_width: int, shape: int):\n        self.packed_bits = packed_bits\n        self.bit_width = bit_width\n        self.shape = shape\n\n    def __repr__(self):\n        return f\"<DenseElementsAttr packed_bits={self.packed_bits}, bit_width={self.bit_width}, shape={self.shape}>\"\n\ndef dense_bool_elements(xs: Sequence[bool]) -> DenseElementsAttr:\n    \"\"\"\n    Converts a sequence of boolean values into a packed bit array and returns\n    a DenseElementsAttr object. It first packs the boolean array into bits\n    using little-endian order. If the sequence has only one element, it adjusts\n    the packed array to be either all zeros or all ones.\n    \"\"\"\n    bit_packed = 0\n    for i, bit in enumerate(xs):\n        if bit:\n            bit_packed |= (1 << i)\n\n    if len(xs) == 1:\n        # Adjust bits if there is only one element\n        if xs[0]:\n            # All bits should be set to 1 (length of the array in bits)\n            bit_packed = (1 << len(xs)) - 1  # Since length is 1, this is just setting all bits to 1\n        else:\n            bit_packed = 0  # Set all bits to 0\n\n    # Create a DenseElementsAttr object\n    return DenseElementsAttr(packed_bits=bit_packed, bit_width=1, shape=len(xs))\n\n# Example usage\nexample_seq = [True, False, True, True]\nprint(dense_bool_elements(example_seq))", "CodeBleu_score": 29.15187659793682}, {"code": "import numpy as np\n\n# Mock implementation or stand-in for the DenseElementsAttr class\nclass DenseElementsAttr:\n    def __init__(self, data, dtype, shape):\n        self.data = data\n        self.dtype = dtype\n        self.shape = shape\n    \n    def __repr__(self):\n        return f\"DenseElementsAttr(data={self.data}, dtype={self.dtype}, shape={self.shape})\"\n\ndef pack_bools_to_bits(bools):\n    \"\"\"Pack a list of boolean values into a bit array using little-endian order.\"\"\"\n    result = 0\n    for i, b in enumerate(bools):\n        if b:\n            result |= (1 << i)\n    return result\n\ndef dense_bool_elements(xs: list) -> DenseElementsAttr:\n    # Convert boolean values to a packed bit format\n    packed_bits = pack_bools_to_bits(xs)\n    \n    # Special rule for a single element boolean list\n    if len(xs) == 1:\n        packed_bits = 0xFFFFFFFFFFFFFFFF if xs[0] else 0x0000000000000000\n    \n    # Since we're packing bits, dtype is set as 1-bit integer, but represented here as an 8-bit integer for simplicity\n    dtype = 'uint8'\n    shape = len(xs)\n\n    return DenseElementsAttr(data=np.array([packed_bits], dtype=dtype), dtype=dtype, shape=(shape,))\n\n# Demonstration\nprint(dense_bool_elements([True, False, True, True, False])) \nprint(dense_bool_elements([True]))  # Special case with single element handling", "CodeBleu_score": 26.706463206154513}, {"code": "import numpy as np\nfrom typing import Sequence\n\nclass DenseElementsAttr:\n    def __init__(self, data, type_descr, shape):\n        self.data = data\n        self.type_descr = type_descr\n        self.shape = shape\n\ndef dense_bool_elements(xs: Sequence[bool]) -> DenseElementsAttr:\n    \"\"\"\n    Converts a sequence of boolean values into a packed bit array and returns \n    a DenseElementsAttr object.\n\n    Args:\n    xs (Sequence[bool]): The input sequence of boolean values.\n\n    Returns:\n    DenseElementsAttr: A class object encapsulating the packed bit array and its description.\n\n    \"\"\"\n    \n    # Number of bits needed\n    num_bits = len(xs)\n    \n    # Convert boolean list to bit-packed bytes\n    packed_bits = 0\n    for index, bit in enumerate(xs):\n        packed_bits |= (bit << index)\n    \n    # Handling special case with one element\n    if num_bits == 1:\n        packed_bits = 0xFFFFFFFFFFFFFFFF if xs[0] else 0\n    \n    # Convert the integer to a bytes object\n    # Calculate necessary bytes to store the bits\n    num_bytes = (num_bits + 7) // 8  # round up division\n    packed_byte_array = packed_bits.to_bytes(num_bytes, byteorder='little')\n    \n    # Create a DenseElementsAttr object\n    attr = DenseElementsAttr(\n        data=np.array(list(packed_byte_array), dtype=np.uint8),\n        type_descr='signless 1-bit integer',\n        shape=(num_bits,)\n    )\n    \n    return attr", "CodeBleu_score": 27.938131348334743}]}
{"instance_id": "06ebabf1e0ff", "generation_result": [{"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]]):\n    \"\"\"The function create_tensor_lower takes a context ctx and a variable number of HLOTensor or sequences of HLOTensor as arguments.\n    It asserts that the length of args, ctx.vars_in, and ctx.vars_out are all one.\n    The function retrieves the input and output variables from the context and checks if the input variable has bound data.\n    If bound data exists, it sets the values of both input and output variables in the module context.\n    The function asserts that the shapes of the input and output variables are the same.\n    It then converts the bound data of the output or input variable to a NumPy array with the output variable's data type.\n    If neither variable has bound data, it raises an assertion error indicating only tensor creation from constants is supported.\n    Finally, it returns an HLOTensor initialized with the created data.\"\"\"\n\n    # Asserting the count of arguments\n    assert len(args) == 1, \"Only one input or sequence of inputs is expected\"\n    assert len(ctx.vars_in) == 1, \"Exactly one input variable expected in context\"\n    assert len(ctx.vars_out) == 1, \"Exactly one output variable expected in context\"\n    \n    # Retrieving input and output tensors from context\n    input_var = ctx.vars_in[0]\n    output_var = ctx.vars_out[0]\n    \n    # Checking if input variable has bound data\n    if input_var.has_bound_data():\n        # Setting values of input and output in context\n        bound_data = input_var.get_bound_data()\n        output_var.set_bound_data(bound_data)\n        \n        # Asserting the shapes of input and output variables are the same\n        assert input_var.shape == output_var.shape, \"Shape of input and output variables must match\"\n\n        # Converting data to NumPy array with specified dtype\n        import numpy as np\n        data = np.array(bound_data, dtype=output_var.dtype)\n        \n    else:\n        # Raising an assertion error as no input data is bound\n        raise AssertionError(\"Only tensor creation from constants is supported\")\n    \n    # Returning the HLOTensor initialized with the created data\n    return HLOTensor(data)\n\n# Assuming `HLOTensor` and the methods `.has_bound_data()`, `.get_bound_data()`, `.set_bound_data()`, `.shape`, `.dtype` are defined somewhere\n# Else, you would need to define or adapt these parts in the actual implementation as necessary.", "CodeBleu_score": 41.31953068345076}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]]):\n    \"\"\"\n    The function create_tensor_lower takes a context ctx and a variable number of HLOTensor or sequences of HLOTensor as arguments.\n    It asserts that the length of args, ctx.vars_in, and ctx.vars_out are all one.\n    The function retrieves the input and output variables from the context and checks if the input variable has bound data.\n    If bound data exists, it sets the values of both input and output variables in the module context.\n    The function asserts that the shapes of the input and output variables are the same.\n    It then converts the bound data of the output or input variable to a NumPy array with the output variable's data type.\n    If neither variable has bound data, it raises an assertion error indicating only tensor creation from constants is supported.\n    Finally, it returns an HLOTensor initialized with the created data.\n    \"\"\"\n    from typing import Union, Sequence\n    import numpy as np\n    \n    assert len(args) == 1, \"Function requires exactly one HLOTensor or a sequence containing one HLOTensor.\"\n    assert len(ctx.vars_in) == 1, \"Context must have exactly one input variable.\"\n    assert len(ctx.vars_out) == 1, \"Context must have exactly one output variable.\"\n\n    input_tensor = ctx.vars_in[0]\n    output_tensor = ctx.vars_out[0]\n\n    # Check for bound data and proceed based on the availability of such data\n    if input_tensor.data is not None:\n        assert input_tensor.shape == output_tensor.shape, \"Input and output tensors must have the same shape.\"\n        \n        # Convert and set data\n        bound_data = np.array(input_tensor.data, dtype=output_tensor.dtype)\n        output_tensor.data = bound_data\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported; input tensor has no bound data.\")\n\n    # Return new HLOTensor initialized with the created data\n    return HLOTensor(data=output_tensor.data, shape=output_tensor.shape, dtype=output_tensor.dtype)", "CodeBleu_score": 38.42512768425301}, {"code": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])\nfrom typing import Sequence, Union\nimport numpy as np\n\nclass HLOTensor:\n    def __init__(self, data):\n        self.data = data\n        self.shape = data.shape\n        self.dtype = data.dtype\n\nclass Context:\n    def __init__(self, vars_in, vars_out):\n        self.vars_in = vars_in\n        self.vars_out = vars_out\n\ndef create_tensor_lower(ctx: Context, *args: Union[HLOTensor, Sequence[HLOTensor]]):\n    \"\"\"\n    The function create_tensor_lower takes a context ctx and a variable number of HLOTensor or sequences of HLOTensor as arguments.\n    It asserts that the length of args, ctx.vars_in, and ctx.vars_out are all one.\n    The function retrieves the input and output variables from the context and checks if the input variable has bound data.\n    If bound data exists, it sets the values of both input and output variables in the module context.\n    The function asserts that the shapes of the input and output variables are the same.\n    It then converts the bound data of the output or input variable to a NumPy array with the output variable's data type.\n    If neither variable has bound data, it raises an assertion error indicating only tensor creation from constants is supported.\n    Finally, it returns an HLOTensor initialized with the created data.\n    \"\"\"\n    # Assert the length of args and context variables\n    assert len(args) == 1, \"Only one argument expected\"\n    assert len(ctx.vars_in) == 1, \"Context must have exactly one input variable\"\n    assert len(ctx.vars_out) == 1, \"Context must have exactly one output variable\"\n\n    # Unpack the input and output variables\n    input_var = ctx.vars_in[0]\n    output_var = ctx.vars_out[0]\n\n    # Check for bound data in the input variable\n    if hasattr(input_var, 'data'):\n        # Set the output variable's data to the input variable's data\n        output_var.data = input_var.data\n    else:\n        raise AssertionError(\"Only tensor creation from constants is supported\")\n\n    # Assert that input and output shapes are the same\n    assert input_var.shape == output_var.shape, \"Input and Output shapes must match\"\n\n    # Convert the data to a NumPy array with the appropriate data type\n    output_numpy_data = np.array(output_var.data, dtype=output_var.dtype)\n\n    # Return an HLOTensor initialized with the converted data\n    return HLOTensor(data=output_numpy_data)", "CodeBleu_score": 35.662688362822216}]}
{"instance_id": "6cd750fa0c8d", "generation_result": [{"code": "import numpy as np\n\ndef _convert_scalar_to_numpy(x, dtype=None):\n    \"\"\"Converts a scalar to a numpy array. Automatically determines dtype if not provided.\"\"\"\n    if dtype is None:\n        return np.array(x)\n    else:\n        return np.array(x, dtype=dtype)\n\ndef _transfer_to_device(array, device):\n    \"\"\"Pretends to transfer a numpy array to the device specified (simulated).\"\"\"\n    return f\"Data transferred to {device}: {array}\"\n\ndef _device_put_scalar(x, device):\n    \"\"\"\n    The function _device_put_scalar transfers a scalar value to a specified device. It first converts\n    the scalar to a numpy array, optionally determining the appropriate data type if not provided,\n    using an internal helper function. The converted numpy array is then transferred to the\n    specified device using another internal function. The function ensures that scalar values are \n    appropriately handled and moved to the desired computational device.\n    \n    Args:\n    - x (scalar): The scalar value to be transferred.\n    - device (str): Device identifier to which the scalar should be transferred.\n    \n    Returns:\n    - str: Message indicating successful transfer.\n    \"\"\"\n    # Convert scalar to numpy array\n    numpy_array = _convert_scalar_to_numpy(x)\n    \n    # Transfer numpy array to the specified device\n    transfer_result = _transfer_to_device(numpy_array, device)\n    \n    return transfer_result\n\n# Example usage:\nresult = _device_put_scalar(42, \"gpu\")\nprint(result)", "CodeBleu_score": 43.27677484237242}, {"code": "import numpy as np\n\ndef _convert_to_numpy_array(scalar):\n    \"\"\"Convert a scalar to a numpy array with appropriate dtype.\"\"\"\n    return np.array(scalar)\n\ndef _transfer_to_device(array, device):\n    \"\"\"Mock function to simulate transferring array to a device.\"\"\"\n    print(f\"Transferring array {array} to device '{device}'\")\n    return array\n\ndef _device_put_scalar(x, device):\n    \"\"\"\n    The function _device_put_scalar transfers a scalar value to a specified device. \n    It first converts the scalar to a numpy array, optionally determining the appropriate data type if not provided, \n    using an internal helper function. The converted numpy array is then transferred to the \n    specified device using another internal function. The function ensures that scalar values \n    are appropriately handled and moved to the desired computational device.\n    \"\"\"\n    # Convert the scalar to a numpy array\n    x_array = _convert_to_numpy_array(x)\n    \n    # Transfer the numpy array to the specified device\n    result = _transfer_to_device(x_array, device)\n    \n    # Return the representation of data on the device\n    return result\n\n# Example usage:\nresult = _device_put_scalar(10, 'GPU')\nprint(result)  # Outputs: array with a scalar value, notionally after being transferred to a device", "CodeBleu_score": 33.39140396578022}, {"code": "import numpy as np\n\ndef _device_put_scalar(x, device):\n    \"\"\"\n    The function _device_put_scalar transfers a scalar value to a specified device. It first converts the scalar to\n    a numpy array, optionally determining the appropriate data type if not provided, using an internal helper function.\n    The converted numpy array is then transferred to the specified device using another internal function. The function\n    ensures that scalar values are appropriately handled and moved to the desired computational device.\n\n    Args:\n    x (scalar): Scalar value to be transferred.\n    device (str): Identifier for the device to transfer the scalar to. Could be 'cpu', 'gpu', 'tpu', etc.\n\n    Returns:\n    numpy.ndarray: A numpy array representing the scalar on the specified device.\n\n    Note:\n    This simplified function assumes a minimal representation and does not interact with actual hardware-level device management.\n    For real-world applications, integration with frameworks like PyTorch, TensorFlow, or JAX is necessary to handle device management.\n    \"\"\"\n    # Assuming the helper functions 'convert_to_numpy' and 'transfer_to_device'\n    \n    def convert_to_numpy(x):\n        \"\"\"Converts scalar x to a numpy array with appropriate data type.\"\"\"\n        if isinstance(x, float):\n            return np.array(x, dtype=np.float32)\n        elif isinstance(x, int):\n            return np.array(x, dtype=np.int32)\n        else:\n            raise TypeError(f\"Unsupported type for conversion to numpy: {type(x)}\")\n        \n    def transfer_to_device(np_array, device):\n        \"\"\"Simulate transferring a numpy array to a specific device.\"\"\"\n        print(f\"Transferring array {np_array} to {device}\")\n        return np_array  # In real scenario, use device-specific libraries\n\n    # Convert scalar to numpy array\n    np_array = convert_to_numpy(x)\n\n    # Transfer the numpy array to the specified device\n    device_array = transfer_to_device(np_array, device)\n\n    return device_array\n\n# Example usage of the function:\nif __name__ == \"__main__\":\n    _device_put_scalar(10, 'cpu')\n    _device_put_scalar(3.14, 'gpu')", "CodeBleu_score": 32.28017951483571}]}
{"instance_id": "140de7c2b6a8", "generation_result": [{"code": "def _shape_equal(lhs_shape, rhs_shape)\ndef _shape_equal(lhs_shape, rhs_shape):\n    \"\"\"\n    The function _shape_equal checks if two shapes, lhs_shape and rhs_shape, are equal.\n    It converts the shapes to lists if they are NumPy arrays.\n    The function asserts that both shapes are either tuples or lists.\n    If both shapes are empty, it returns True.\n    It verifies that the first elements of the shapes are integers if the shapes are not empty.\n    If the shapes have different lengths, it returns False.\n    It iterates through the elements of both shapes, and if any corresponding elements are not equal, it returns False.\n    If all checks pass, it returns True.\n    \"\"\"\n    import numpy as np\n\n    # Convert from numpy array shapes to list if necessary\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n\n    # Assert shapes are either lists or tuples\n    assert isinstance(lhs_shape, (list, tuple)), \"lhs_shape must be a list or tuple\"\n    assert isinstance(rhs_shape, (list, tuple)), \"rhs_shape must be a list or tuple\"\n\n    # If both shapes are empty, return True\n    if not lhs_shape and not rhs_shape:\n        return True\n\n    # If one of them is empty and the other isn't, return False\n    if not lhs_shape or not rhs_shape:\n        return False\n\n    # Check the type of the first element of both shapes if not empty\n    assert isinstance(lhs_shape[0], int), \"First element of lhs_shape must be an integer\"\n    assert isinstance(rhs_shape[0], int), \"First element of rhs_shape must be an integer\"\n\n    # If lengths are different, return False\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n\n    # Iterate through elements and check for equality\n    for l, r in zip(lhs_shape, rhs_shape):\n        if l != r:\n            return False\n    \n    # All checks passed, return True\n    return True", "CodeBleu_score": 53.18914247987292}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\ndef _shape_equal(lhs_shape, rhs_shape):\n    import numpy as np\n\n    # Check for numpy array shapes and convert if necessary to lists\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n\n    # Assertion to ensure both shapes are either tuples or lists\n    assert isinstance(lhs_shape, (tuple, list)), \"lhs_shape must be a tuple or list\"\n    assert isinstance(rhs_shape, (tuple, list)), \"rhs_shape must be a tuple or list\"\n\n    # If both shapes are empty, return True\n    if not lhs_shape and not rhs_shape:\n        return True\n\n    # Ensure first elements of the shapes are integers if shapes are not empty\n    if lhs_shape and rhs_shape:\n        assert isinstance(lhs_shape[0], int), \"First element of lhs_shape must be an integer\"\n        assert isinstance(rhs_shape[0], int), \"First element of rhs_shape must be an integer\"\n\n    # If they have different lengths\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n\n    # Iterating and checking every corresponding element\n    for l_ele, r_ele in zip(lhs_shape, rhs_shape):\n        if l_ele != r_ele:\n            return False\n\n    # If all checks pass\n    return True\n\n# Example usage:\n# _shape_equal((3, 2), [3, 2])  # returns True\n# _shape_equal([1, 2, 3], (1, 2, 4))  # returns False", "CodeBleu_score": 50.364079235889456}, {"code": "def _shape_equal(lhs_shape, rhs_shape)\ndef _shape_equal(lhs_shape, rhs_shape):\n    \"\"\"The function _shape_equal checks if two shapes, lhs_shape and rhs_shape, are equal.\n    It converts the shapes to lists if they are NumPy arrays.\n    The function asserts that both shapes are either tuples or lists.\n    If both shapes are empty, it returns True.\n    It verifies that the first elements of the shapes are integers if the shapes are not empty.\n    If the shapes have different lengths, it returns False.\n    It iterates through the elements of both shapes, and if any corresponding elements are not equal, it returns False.\n    If all checks pass, it returns True.\n    \"\"\"\n    import numpy as np\n\n    # Convert shapes from numpy array to list if they are numpy arrays\n    if isinstance(lhs_shape, np.ndarray):\n        lhs_shape = lhs_shape.tolist()\n    if isinstance(rhs_shape, np.ndarray):\n        rhs_shape = rhs_shape.tolist()\n\n    # Asserting shapes are either tuples or lists\n    assert isinstance(lhs_shape, (tuple, list)), \"lhs_shape must be a tuple or list\"\n    assert isinstance(rhs_shape, (tuple, list)), \"rhs_shape must be a tuple or list\"\n\n    # If both shapes are empty, they are equal\n    if not lhs_shape and not rhs_shape:\n        return True\n\n    # Verifying the first element of the shapes are integers if shapes are not empty\n    if lhs_shape and rhs_shape:\n        assert isinstance(lhs_shape[0], int), \"First element of lhs_shape must be an integer\"\n        assert isinstance(rhs_shape[0], int), \"First element of rhs_shape must be an integer\"\n\n    # Check if the lengths of the shapes are same\n    if len(lhs_shape) != len(rhs_shape):\n        return False\n\n    # Iterating through elements of both shapes\n    for l, r in zip(lhs_shape, rhs_shape):\n        if l != r:\n            return False\n\n    # If all checks pass, the shapes are equal\n    return True", "CodeBleu_score": 54.152236343321206}]}
{"instance_id": "55d0c71a259b", "generation_result": [{"code": "import pandas as pd\n\ndef read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n):\n    \"\"\"\n    Loads training and test datasets along with their metadata from CSV files.\n    It takes filenames for the datasets and their metadata, as well as dictionaries specifying data types for the columns.\n    The function reads the training set, test set, training metadata, and test metadata into pandas DataFrames using\n    the specified data types. It temporarily removes the 'target' column type from the metadata dictionary while loading\n    the test metadata, then restores it. Finally, it returns a tuple of the four DataFrames.\n\n    Parameters:\n        training_set_filename (str): The filename of the training dataset CSV file.\n        test_set_filename (str): The filename of the test dataset CSV file.\n        training_set_metadata_filename (str): The filename of the training metadata CSV file.\n        test_set_metadata_filename (str): The filename of the test metadata CSV file.\n        dtypes (dict): Dictionary specifying the data types for columns in the training and test datasets.\n        meta_dtypes (dict): Dictionary specifying the data types for columns in the metadata files.\n\n    Returns:\n        tuple: A tuple containing four pandas DataFrames in the order:\n               (training_set_df, test_set_df, training_metadata_df, test_metadata_df)\n    \"\"\"\n    \n    # Reading the training set\n    training_set_df = pd.read_csv(training_set_filename, dtype=dtypes)\n    \n    # Reading the test set\n    test_set_df = pd.read_csv(test_set_filename, dtype=dtypes)\n    \n    # Reading the training metadata\n    training_metadata_df = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    \n    # Temporarily removing 'target' column dtype if present for reading test metadata\n    if 'target' in meta_dtypes:\n        meta_dtypes_copy = meta_dtypes.copy()\n        removed_dtype = meta_dtypes_copy.pop('target', None)\n        test_metadata_df = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes_copy)\n        # Restoring the 'target' dtype\n        meta_dtypes['target'] = removed_dtype if removed_dtype else meta_dtypes.get('target', None)\n    else:\n        test_metadata_df = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n\n    return (training_set_df, test_set_df, training_metadata_df, test_metadata_df)", "CodeBleu_score": 48.36348124093238}, {"code": "import pandas as pd\n\ndef read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n):\n    \"\"\"\n    Loads training and test datasets along with their metadata from CSV files. It takes filenames \n    for the datasets and their metadata, as well as dictionaries specifying data types for the \n    columns. The function reads the training set, test set, training metadata, and test metadata \n    into pandas DataFrames using the specified data types. It temporarily removes the 'target' \n    column type from the metadata dictionary while loading the test metadata, then restores it. \n    Finally, it returns a tuple of the four DataFrames.\n\n    Parameters:\n        training_set_filename (str): The filename of the training set CSV file.\n        test_set_filename (str): The filename of the test set CSV file.\n        training_set_metadata_filename (str): The filename of the training set metadata CSV file.\n        test_set_metadata_filename (str): The filename of the test set metadata CSV file.\n        dtypes (dict): Dictionary specifying the data types of columns in the datasets.\n        meta_dtypes (dict): Dictionary specifying the data types of columns in the metadata files.\n\n    Returns:\n        tuple: A tuple containing four pandas DataFrames: training_set, test_set, training_metadata, and test_metadata.\n    \"\"\"\n    # Read training and test sets with specified dtypes\n    training_set = pd.read_csv(training_set_filename, dtype=dtypes)\n    test_set = pd.read_csv(test_set_filename, dtype=dtypes)\n\n    # Temporarily adjust meta_dtypes for test metadata\n    if 'target' in meta_dtypes:\n        temp_meta_dtypes = meta_dtypes.copy()\n        del temp_meta_dtypes['target']\n    else:\n        temp_meta_dtypes = meta_dtypes\n\n    # Read metadata files with specified dtypes\n    training_metadata = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    test_metadata = pd.read_csv(test_set_metadata_filename, dtype=temp_meta_dtypes)\n\n    return training_set, test_set, training_metadata, test_metadata", "CodeBleu_score": 49.5377471768227}, {"code": "import pandas as pd\n\ndef read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n):\n    \"\"\"\n    The function read loads training and test datasets along with their metadata from CSV files. It takes filenames for the datasets and their metadata, as well as dictionaries specifying data types for the columns. The function reads the training set, test set, training metadata, and test metadata into pandas DataFrames using the specified data types. It temporarily removes the 'target' column type from the metadata dictionary while loading the test metadata, then restores it. Finally, it returns a tuple of the four DataFrames.\n    \n    Parameters:\n        training_set_filename (str): Path to the training set CSV file.\n        test_set_filename (str): Path to the test set CSV file.\n        training_set_metadata_filename (str): Path to the training metadata CSV file.\n        test_set_metadata_filename (str): Path to the test metadata CSV file.\n        dtypes (dict): Dictionary specifying data types of the columns in the training and test set.\n        meta_dtypes (dict): Dictionary specifying data types of the columns in the metadata files.\n\n    Returns:\n        tuple: A tuple containing four pandas DataFrames (training set, test set, training metadata, test metadata).\n    \"\"\"\n    \n    # Load the training set\n    train_data = pd.read_csv(training_set_filename, dtype=dtypes)\n    \n    # Load the test set\n    test_data = pd.read_csv(test_set_filename, dtype=dtypes)\n    \n    # Load training metadata\n    train_meta_data = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    \n    # Temporarily modify meta dtypes to exclude target column while reading test metadata\n    meta_dtypes_modified = meta_dtypes.copy()\n    if 'target' in meta_dtypes_modified:\n        meta_dtypes_modified.pop('target')\n    \n    # Load test metadata\n    test_meta_data = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes_modified)\n    \n    return train_data, test_data, train_meta_data, test_meta_data", "CodeBleu_score": 50.16443578427937}]}
{"instance_id": "9e5e02cd73e0", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom seaborn import axes_style\n\ndef _show_cmap(cmap):\n    \"\"\"The function _show_cmap displays a continuous matplotlib colormap.\n    It first imports axes_style from rcmod to avoid a circular import.\n    Using the white style from axes_style, it creates a figure and axis with specific dimensions.\n    It removes the x and y ticks from the axis.\n    Then, it generates a 2D array with values linearly spaced between 0 and 1 and uses ax.pcolormesh to display the colormap.\"\"\"\n    \n    # Using a white axes style for a clear background\n    # (Note: here we assume rcmod has an attribute called axes_style which adjusts the plot's theme)\n    with axes_style('white'):\n        fig, ax = plt.subplots(figsize=(6, 2))  # Dimensions of the plot\n\n        # Removing axis ticks\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        # Creating a 2D array with linearly spaced values\n        gradient = np.linspace(0, 1, 256)\n        gradient = np.vstack((gradient, gradient))\n\n        # Displaying the colormap\n        c = ax.pcolormesh(gradient, cmap=cmap, rasterized=True, vmin=0, vmax=1)\n        ax.set_aspect('auto')\n        \n        # Show the color bar:\n        plt.colorbar(c, ax=ax, orientation='horizontal')\n\n        plt.show()", "CodeBleu_score": 45.55850118061446}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom seaborn import axes_style\n\ndef _show_cmap(cmap):\n    \"\"\"\n    The function _show_cmap displays a continuous matplotlib colormap.\n    It first imports axes_style from rcmod to avoid a circular import.\n    Using the white style from axes_style, it creates a figure and axis with specific dimensions.\n    It removes the x and y ticks from the axis.\n    Then, it generates a 2D array with values linearly spaced between 0 and 1 and uses ax.pcolormesh to display the colormap.\n    \"\"\"\n    \n    # Using the white style for the plot\n    with axes_style(\"white\"):\n        # Create a figure and axis\n        fig, ax = plt.subplots(figsize=(6, 2))\n        \n        # Remove ticks on x and y axis\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        # Generate a 2D array with values spaced between 0 and 1\n        gradient = np.linspace(0, 1, 256)\n        gradient = np.vstack((gradient, gradient))\n\n        # Display the colormap\n        ax.pcolormesh(gradient, cmap=cmap, shading='auto')\n\n        # Display the plot\n        plt.show()\n\n# Example usage:\n_show_cmap('viridis')", "CodeBleu_score": 43.37907781123242}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom seaborn import axes_style\n\ndef _show_cmap(cmap):\n    \"\"\"\n    The function _show_cmap displays a continuous matplotlib colormap.\n    It first imports axes_style from rcmod to avoid a circular import.\n    Using the white style from axes_style, it creates a figure and axis with specific dimensions.\n    It removes the x and y ticks from the axis.\n    Then, it generates a 2D array with values linearly spaced between 0 and 1 and uses ax.pcolormesh to display the colormap.\n    \"\"\"\n    with axes_style(\"white\"):\n        fig, ax = plt.subplots(figsize=(6, 1))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        \n        gradient = np.linspace(0, 1, 256)\n        gradient = np.vstack((gradient, gradient))\n        \n        ax.pcolormesh(gradient, cmap=cmap, shading='auto')\n        plt.show()", "CodeBleu_score": 48.479135025558605}]}
{"instance_id": "ad0781b5cc38", "generation_result": [{"code": "import matplotlib.transforms as transforms\n\ndef transform_to_data_coordinates(obj, xdata, ydata):\n    \"\"\"\n    The function transform_to_data_coordinates converts coordinates to data coordinates for a given object.\n    It takes an object and its x and y coordinates as inputs. If the object's axes are defined and its\n    current transform differs from the data transform, the function combines the object's transform\n    with the inverse data transform to convert the coordinates. It then applies the composite transform\n    to the points and returns the transformed coordinates. If no transformation is needed, it\n    returns the original coordinates.\n\n    Parameters:\n    obj : a matplotlib artist or similar object that has a .get_transform() method and is associated with axes.\n    xdata, ydata : float or array_like\n        The x and y coordinates that need to be transformed.\n\n    Returns:\n    tuple\n        Transformed x and y coordinates.\n    \"\"\"\n\n    # If there is an axis and a transform attached to obj\n    if hasattr(obj, 'ax') and getattr(obj, 'ax', None) is not None:\n        ax = obj.ax\n        \n        # Get the transformation of the object\n        obj_transform = obj.get_transform()\n        \n        # Check if the object transform is different from the data transformation\n        if obj_transform != ax.transData:\n            # Get the inverse of the data coordinate system transformation\n            inv_data_transform = ax.transData.inverted()\n            \n            # Combine the object's transform with the inverse of the data transform\n            combo_transform = obj_transform + inv_data_transform\n            \n            # Transform point (you would usually apply this to arrays of points)\n            x_transformed, y_transformed = combo_transform.transform_point((xdata, ydata))\n            return x_transformed, y_transformed\n        \n    # If no transformation is required, or it's not possible, return the original coordinates\n    return xdata, ydata", "CodeBleu_score": 42.03581835183039}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata):\n    \"\"\"\n    Converts coordinates (xdata, ydata) to data coordinates for a given matplotlib graphical object.\n    \n    :param obj: A matplotlib graphical object (such as Line2D, Text, etc.) which has transform attributes.\n    :param xdata: x-coordinate or a list/array of x-coordinates.\n    :param ydata: y-coordinate or a list/array of y-coordinates.\n    :return: A tuple (x_transformed, y_transformed) containing the transformed coordinates.\n    \"\"\"\n    # Check if the object has the necessary 'axes' and 'get_transform' attributes\n    if hasattr(obj, 'axes') and hasattr(obj, 'get_transform'):\n        # Get the object's transformation\n        obj_transform = obj.get_transform()\n        \n        # Get axes data transformation\n        data_transform = obj.axes.transData\n        \n        # If the object's transform is not the same as the data transform, perform conversion\n        # In matplotlib, if the transform object is the same, no conversion is necessary.\n        if obj_transform != data_transform:\n            # Create an 'inverted' data transform\n            invert_data_transform = data_transform.inverted()\n            \n            # Combine the object's transform with the inverse of the data transform\n            composite_transform = obj_transform + invert_data_transform\n            \n            # Apply the composite transform to the input data\n            x_transformed, y_transformed = composite_transform.transform((xdata, ydata))\n            return x_transformed, y_transformed\n        else:\n            # If no transformation is required, return the original coordinates\n            return xdata, ydata\n    else:\n        # If the object does not meet the requirements, handle it (e.g., raise an exception or simply return input)\n        raise AttributeError(\"The provided object does not have the required 'axes' or 'get_transform' attributes.\")", "CodeBleu_score": 40.64215589148458}, {"code": "def transform_to_data_coordinates(obj, xdata, ydata):\n    \"\"\"\n    The function transform_to_data_coordinates converts coordinates to data coordinates for a given object.\n    It takes an object and its x and y coordinates as inputs. If the object's axes are defined and its current \n    transform differs from the data transform, the function combines the object's transform with the inverse\n    data transform to convert the coordinates. It then applies the composite transform to the points and \n    returns the transformed coordinates. If no transformation is needed, it returns the original coordinates.\n\n    Parameters:\n        obj: An artist object or an object having attribute `axes`, and `get_transform()`.\n        xdata: float or iterable of floats. Original x coordinates.\n        ydata: float or iterable of floats. Original y coordinates.\n\n    Returns:\n        tuple: Transformed (x, y) coordinates.\n    \"\"\"\n    import numpy as np\n\n    if hasattr(obj, 'axes') and obj.axes:\n        ax = obj.axes\n        data_trans = ax.transData\n        obj_trans = obj.get_transform()\n        \n        if obj_trans != data_trans:\n            # Create the transformation from the object's coordinates to data coordinates\n            transform = obj_trans + data_trans.inverted()\n            \n            # Convert (xdata, ydata) to an array and transform it\n            coords = np.column_stack((xdata, ydata))\n            transformed_coords = transform.transform(coords)\n            \n            # Separate the coordinates back into x and y\n            x_transformed, y_transformed = transformed_coords[:, 0], transformed_coords[:, 1]\n            return x_transformed, y_transformed\n    \n    # If the object is not associated with an axes or no transformation is needed, return the input coordinates\n    return xdata, ydata", "CodeBleu_score": 42.1760220911869}]}
{"instance_id": "bf776cadef8d", "generation_result": [{"code": "class DataPlotter:\n    def __init__(self, df, module):\n        self._df = df\n        self._module = module\n\n    def _maybe_target_series(self, data, key):\n        \"\"\"Converts the provided data into a pandas Series format if possible.\"\"\"\n        # This is a placeholder implementation.\n        # Depending on real-world use, you might want to handle cases where data needs\n        # conversion to a pandas.Series, etc.\n        if isinstance(data, list):\n            return pd.Series(data)\n        elif isinstance(data, pd.Series):\n            return data\n        elif data in self._df:\n            return self._df[data]\n        raise ValueError(f\"Invalid data input for key '{key}'\")\n\n    def kdeplot(self, data=None, data2=None, *args, **kwargs):\n        \"\"\"The function kdeplot takes data, an optional second dataset data2, and additional arguments and keyword arguments.\n        It processes the first dataset using the method _maybe_target_series with a key of 'data'.\n        If a second dataset data2 is provided and it is not list-like, it retrieves data2 from the dataframe attribute _df.\n        Finally, it calls the kdeplot method from the _module attribute, passing the processed data, data2, and any additional arguments and keyword arguments.\"\"\"\n        \n        if data is None:\n            raise ValueError(\"Data is required for the KDE plot.\")\n        \n        processed_data = self._maybe_target_series(data, 'data')\n\n        if data2 is not None:\n            if not pd.api.types.is_list_like(data2):\n                # Assuming 'data2' should be a column in the DataFrame if it's not list-like\n                data2 = self._df[data2]\n            else:\n                # If data2 is list-like, it's used as is or processed\n                data2 = self._maybe_target_series(data2, 'data2')\n\n        return self._module.kdeplot(processed_data, data2, *args, **kwargs)", "CodeBleu_score": 49.26023708416659}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs):\n    \"\"\"\n    The function kdeplot takes data, an optional second dataset data2, and additional\n    arguments and keyword arguments.\n    It processes the first dataset using the method _maybe_target_series with a key\n    of 'data'.\n    If a second dataset data2 is provided and it is not list-like, it retrieves data2 \n    from the dataframe attribute _df.\n    Finally, it calls the kdeplot method from the _module attribute, passing the processed\n    data, data2, and any additional arguments and keyword arguments.\n    \"\"\"\n\n    # Process the first dataset\n    processed_data = self._maybe_target_series(data, key='data')\n    \n    # Check and potentially get data2 from _df if it's not list-like\n    if data2 is not None:\n        if not isinstance(data2, (list, tuple, set, frozenset)):\n            data2 = getattr(self._df, data2, None)\n    \n    # Call the kdeplot method from the _module, passing all necessary arguments\n    return self._module.kdeplot(processed_data, data2, *args, **kwargs)", "CodeBleu_score": 65.92581626679956}, {"code": "def kdeplot(self, data=None, data2=None, *args, **kwargs):\n    import pandas as pd\n\n    # Process the first dataset\n    processed_data = self._maybe_target_series(data, key='data')\n\n    # Check if a second dataset is provided and if it's not list-like\n    if data2 is not None and not isinstance(data2, (list, pd.Series, pd.DataFrame)):\n        # Retrieve data2 from the dataframe attribute _df\n        processed_data2 = self._df[data2]\n    else:\n        processed_data2 = data2\n\n    # Call the kdeplot method from the _module attribute, pass the processed data and any additional args, kwargs\n    return self._module.kdeplot(processed_data, processed_data2, *args, **kwargs)", "CodeBleu_score": 62.365381246942}]}
{"instance_id": "fb44964174d9", "generation_result": [{"code": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom nltk.corpus import stopwords\nimport nltk\n\ndef plot_most_used_words(df):\n    \"\"\"\n    The function plot_most_used_words visualizes the most frequent words in a dataframe. \n    It first corrects the word \"programa\" to \"programar\" in the dataframe. It then filters the dataframe \n    to include only alphabetic tokens that are longer than one character and are not stop words. \n    The function counts the occurrences of these words, selects the top 20, and creates a bar plot \n    showing their frequency. The plot is labeled and saved as an image file.\n    \"\"\"\n    \n    # Ensure nltk resources are downloaded (in particular the stop words)\n    nltk.download('stopwords')\n    \n    # Correcting \"programa\" to \"programar\"\n    df['text'] = df['text'].str.replace('programa', 'programar', regex=False)\n    \n    # Create a series of all words in the 'text' column\n    words_series = df['text'].str.split(expand=True).stack()\n\n    # Filter words: remove non-alphabetic tokens, keep words longer than one character, remove stopwords\n    stop_words = set(stopwords.words('english'))  # Assuming English. Modify if needed\n    filtered_words = words_series[(words_series.str.isalpha()) & \n                                  (words_series.str.len() > 1) & \n                                  (~words_series.isin(stop_words))]\n\n    # Count occurrences of each word\n    word_counts = filtered_words.value_counts().nlargest(20)\n    \n    # Plotting\n    plt.figure(figsize=(10, 8))\n    word_counts.plot(kind='bar', color='blue')\n    plt.title('Top 20 Most Frequent Words')\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    \n    # Save the plot to an image file\n    plt.savefig('most_used_words.png')\n    plt.show()", "CodeBleu_score": 32.47108151550401}, {"code": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport re\n\ndef plot_most_used_words(df):\n    \"\"\"\n    The function plot_most_used_words visualizes the most frequent words in a dataframe. It first corrects the word \"programa\" to \"programar\" in the dataframe.\n    It then filters the dataframe to include only alphabetic tokens that are longer than one character and are not stop words.\n    The function counts the occurrences of these words, selects the top 20, and creates a bar plot showing their frequency.\n    The plot is labeled and saved as an image file.\n    \n    Args:\n    df (pandas.DataFrame): DataFrame containing a column 'text' with text data.\n\n    Returns:\n    None\n    \"\"\"\n\n    # Assume the column containing the text data is named 'text'\n    # Correct 'programa' to 'programar'\n    df['text'] = df['text'].str.replace(r'\\bprograma\\b', 'programar')\n\n    # Tokenize, filter non-alphabetical tokens, length less than 2 and stop words\n    stop_words = set(stopwords.words('english'))  # use 'spanish' if texts are in Spanish\n    df['words'] = df['text'].apply(lambda x: [word.lower() for word in re.findall(r'\\b\\w+\\b', x)\n                                              if word.isalpha() and len(word) > 1 and word.lower() not in stop_words])\n    \n    # Flatten list of words and count frequencies\n    all_words = [word for sublist in df['words'] for word in sublist]\n    word_freq = pd.Series(all_words).value_counts().head(20)  # Get top 20 most frequent words\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    word_freq.sort_values().plot(kind='barh', color='skyblue')  # Horizontal bar chart\n    plt.xlabel('Frequency')\n    plt.ylabel('Words')\n    plt.title('Top 20 Most Frequent Words')\n    plt.tight_layout()\n\n    # Save the plot as a file\n    plt.savefig('most_used_words.png')\n    plt.show()\n\n# Note: Make sure nltk's stopwords are downloaded before running this function\n# import nltk\n# nltk.download('stopwords')", "CodeBleu_score": 30.262401431340624}, {"code": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport re\n\ndef plot_most_used_words(df):\n    \"\"\"\n    Visualizes the most frequent words in a dataframe.\n\n    Args:\n    df : pandas.DataFrame\n        DataFrame containing a column with text data from which to analyze frequency of words.\n\n    Assumes the text data is under a column named 'text'.\n    \"\"\"\n    # Ensure NLTK stopwords are downloaded\n    try:\n        stop_words = set(stopwords.words('english'))\n    except LookupError:\n        import nltk\n        nltk.download('stopwords')\n        stop_words = set(stopwords.words('english'))\n\n    # Correcting the word \"programa\" to \"programar\" in dataframe\n    df['text'] = df['text'].str.replace('programa', 'programar', regex=False)\n\n    # Define a function to filter tokens\n    def process_text(text):\n        tokens = re.findall(r'\\b\\w+\\b', text.lower())  # Extract words\n        return [token for token in tokens if token.isalpha() and len(token) > 1 and token not in stop_words]\n\n    # Apply text processing\n    df['filtered_text'] = df['text'].apply(process_text)\n\n    # Flatten list of lists of words into a single list\n    all_words = [word for sublist in df['filtered_text'].tolist() for word in sublist]\n    \n    # Create a word frequency count\n    word_counts = Counter(all_words)\n    \n    # Select the top 20 most common words\n    top_words = word_counts.most_common(20)\n    \n    # Creating the bar plot\n    words, counts = zip(*top_words)  # unpacking the top words and their counts\n    plt.figure(figsize=(10, 8))\n    plt.bar(words, counts, color='blue')\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.title('Top 20 Most Frequent Words')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\n    # Save the plot\n    plt.savefig(\"most_used_words.png\")\n    plt.show()\n\n# Assuming you will run this by passing an appropriate dataframe\n# df = pd.DataFrame({'text': [...]}) # Make sure to create or load your dataframe here\n# plot_most_used_words(df)", "CodeBleu_score": 31.011428433107124}]}
{"instance_id": "13414a7eb4e3", "generation_result": [{"code": "import dask.array as da\nimport numpy as np\n\ndef dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1):\n    \"\"\"\n    The function dask_rolling_wrapper applies bottleneck moving window functions to Dask arrays.\n    It imports Dask array as da and promotes the data type of the input array a to handle edge cases, setting the fill value accordingly.\n    The function calculates the depth of overlap for the specified axis and sets the boundary fill values.\n    It creates an overlapped array ag using da.overlap.overlap.\n    It then applies the moving_func to the overlapped array using da.map_blocks with the specified window size and min_count.\n    Finally, the function trims the overlapped regions using da.overlap.trim_internal and returns the resulting array.\n    \"\"\"\n\n    # Promote data type of input array to handle numerical precision\n    if a.dtype.kind in 'ui':  # Unsigned and signed integers\n        a = a.astype(float)\n    \n    # Set the boundary fill value based on data type to handle edge cases\n    fill_value = np.nan if a.dtype.kind == 'f' else 0\n\n    # Calculate depth of overlap based on the window size for the specified axis\n    depth = {axis: (window - 1, window - 1)} if min_count is not None else {axis: (0, 0)} # Assumes symmetric window\n\n    # Create the overlapped array\n    overlapped_a = da.overlap.overlap(a, depth=depth, boundary={axis: 'reflect'})\n\n    # Define a wrapper function for moving_func to include window and min_count\n    def wrapped_moving_func(block):\n        return moving_func(block, window=window, axis=axis, min_count=min_count)\n    \n    # Apply the moving function using map_blocks\n    result_array = overlapped_a.map_blocks(wrapped_moving_func, dtype=a.dtype)\n\n    # Trim the overlapped regions\n    trimmed_result = da.overlap.trim_internal(result_array, depth=depth)\n\n    return trimmed_result", "CodeBleu_score": 39.07752276691551}, {"code": "import dask.array as da\n\ndef dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1):\n    # Step 1: Import is already done above.\n\n    # Step 2: Ensure data type is able to handle potential calculation edge cases\n    if a.dtype.kind not in 'fi':  # If not already a floating point or integer type.\n        a = a.astype(float)  # Assuming promotion to float for most general case.\n\n    # Step 3: Calculate depth of overlapping\n    depth = {axis: window - 1}\n\n    # Step 4: Create overlapped array\n    boundary = {axis: 'reflect'}  # Using reflect but could employ other strategies.\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n\n    # Step 5: Apply moving function on the overlapped array using map_blocks\n    kwargs = {'window': window, 'min_count': min_count}\n    result = ag.map_blocks(moving_func, **kwargs)\n\n    # Step 6: Trim the overlapped array\n    trimmed_result = da.overlap.trim_internal(result, depth=depth)\n\n    # Step 7: Return the resulting array\n    return trimmed_result", "CodeBleu_score": 36.11113444891504}, {"code": "import dask.array as da\nimport numpy as np\n\ndef dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1):\n    \"\"\"\n    The function dask_rolling_wrapper applies bottleneck moving window functions to Dask arrays.\n    It imports Dask array as da and promotes the data type of the input array a to handle edge cases, setting the \n    fill value accordingly.\n    The function calculates the depth of overlap for the specified axis and sets the boundary fill values.\n    It creates an overlapped array ag using da.overlap.overlap.\n    It then applies the moving_func to the overlapped array using da.map_blocks with the specified window size and \n    min_count.\n    Finally, the function trims the overlapped regions using da.overlap.trim_internal and returns the resulting array.\n    \"\"\"\n    \n    # Ensure the datatype is flexible enough to handle Nan (for float) or masked values\n    if np.issubdtype(a.dtype, np.integer):\n        a = a.astype(float)\n    \n    # Calculate depth of the rolling operation\n    depth = {axis: window - 1}\n    boundary = {axis: 'reflect'}\n    \n    # Applying the overlap\n    a_overlapped = da.overlap.overlap(a, depth=depth, boundary=boundary)\n    \n    # Map the moving function using map_blocks\n    # Ensure that the function moving_func is passed additional arguments like window, min_count\n    result = a_overlapped.map_blocks(\n        lambda block: moving_func(block, window, axis=axis, min_count=min_count),\n        dtype=a.dtype\n    )\n    \n    # Trim the overlapped regions to obtain the final clean output\n    trimmed_result = da.overlap.trim_internal(result, depth)\n    \n    return trimmed_result", "CodeBleu_score": 42.83782913692762}]}
{"instance_id": "68ce25d325d4", "generation_result": [{"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\nimport dask.array as da\nimport numpy as np\n\ndef least_squares(lhs, rhs, rcond=None, skipna=False):\n    \"\"\"The function least_squares solves a linear least squares problem using Dask arrays. It converts the input arrays to Dask arrays and optionally handles missing values by applying a polynomial fit function along the specified axis. If skipna is True, it reshapes the right-hand side array if necessary and applies the polynomial fit, extracting coefficients and residuals from the result. If skipna is False, it directly computes the least squares solution using Dask's linear algebra functions. The function returns the coefficients and residuals of the least squares solution.\n    \n    Args:\n        lhs (array_like or dask.array.Array): Left-hand side matrix of the linear equation.\n        rhs (array_like or dask.array.Array): Right-hand side matrix or vector, which must be conformable for multiplication.\n        rcond (float, optional): Cut-off ratio for small singular values of `lhs`.\n        skipna (bool, optional): If True, missing values (NaNs) in `rhs` are skipped by reshaping `rhs` and using a polynomial fit method to estimate the missing values.\n    \n    Returns:\n        tuple: Returns a tuple (coefs, residuals) where 'coefs' are the solution to the equation `lhs @ coefs = rhs` and 'residuals' are the sum of residuals.\n    \"\"\"\n    \n    # Convert input to Dask arrays if they aren't already\n    lhs = da.asarray(lhs)\n    rhs = da.asarray(rhs)\n    \n    if skipna:\n        # Reshape rhs if it's not 2-dimensional\n        if rhs.ndim == 1:\n            rhs = rhs[:, np.newaxis]\n\n        # Polynomial fit for NaN handling, this requires some custom implementation since dask does not support polyfit directly\n        not_nan_mask = ~da.isnan(rhs).all(axis=0)\n        valid_rhs = rhs[:, not_nan_mask]\n        coefs = da.linalg.lstsq(lhs[:, not_nan_mask], valid_rhs, rcond=rcond)[0]\n    else:\n        # Directly calculate the least squares solution\n        coefs, residuals, rank, s = da.linalg.lstsq(lhs, rhs, rcond=rcond)\n\n        # Since Dask computes residuals in terms of the Frobenius norm, convert it if necessary\n        if rhs.ndim == 1:\n            residuals = residuals.sum()\n    \n    return coefs, residuals", "CodeBleu_score": 30.70467552413233}, {"code": "import dask.array as da\nimport numpy as np\n\ndef least_squares(lhs, rhs, rcond=None, skipna=False):\n    \"\"\"\n    The function least_squares solves a linear least squares problem using Dask arrays. It converts the input arrays \n    to Dask arrays and optionally handles missing values by applying a polynomial fit function along the specified axis. \n    If skipna is True, it reshapes the right-hand side array if necessary and applies the polynomial fit, extracting \n    coefficients and residuals from the result. If skipna is False, it directly computes the least squares solution \n    using Dask's linear algebra functions. The function returns the coefficients and residuals of the least squares solution.\n    \n    Args:\n    lhs (array_like): Left-hand side matrix in Ax = b equation.\n    rhs (array_like): Right-hand side matrix or vector in Ax = b equation.\n    rcond (float, optional): Cut-off ratio for small singular values of `lhs`. Singular values are set to zero if they are smaller\n                             than rcond times the largest singular value of `lhs`.\n    skipna (bool, optional): Indicates whether to ignore NaNs. If True, NaNs are handled by interpolating. Defaults to False.\n\n    Returns:\n    tuple: Returns a tuple of two elements (coefficients, residuals) where:\n        - coefficients are the solution to the least squares problem if the system is consistent.\n        - residuals is the difference between the actual and predicted values from the least squares fit.\n    \"\"\"\n\n    # Convert input arrays to Dask arrays\n    lhs_da = da.from_array(lhs, chunks=lhs.shape[0]//2)  # you may want to optimize chunk sizes based on context\n    rhs_da = da.from_array(rhs, chunks=rhs.shape[0]//2)\n\n    if skipna:\n        # Handle NaNs in rhs when skipna is True        \n        mask = da.isnan(rhs_da)\n        valid_rhs = da.where(mask, da.nanmean(rhs_da), rhs_da)\n        \n        # Fit a polynomial to non-NaN values, assuming the simplest case of a linear fit\n        # for a more complex case, you would perhaps involve more sophisticated methods\n        coefficients, residuals, rank, s = da.linalg.lstsq(lhs_da, valid_rhs, rcond=rcond)\n    else:\n        # Compute the least squares solution directly\n        coefficients, residuals, rank, s = da.linalg.lstsq(lhs_da, rhs_da, rcond=rcond)\n    \n    results = (coefficients.compute(), residuals.compute())\n    return results", "CodeBleu_score": 33.73487710334815}, {"code": "def least_squares(lhs, rhs, rcond=None, skipna=False)\nimport dask.array as da\nimport numpy as np\n\ndef least_squares(lhs, rhs, rcond=None, skipna=False):\n    \"\"\"\n    The function least_squares solves a linear least squares problem using Dask arrays. It converts the input arrays to Dask arrays\n    and optionally handles missing values by applying a polynomial fit function along the specified axis. If skipna is True, it reshapes\n    the right-hand side array if necessary and applies the polynomial fit, extracting coefficients and residuals from the result.\n    If skipna is False, it directly computes the least squares solution using Dask's linear algebra functions.\n    The function returns the coefficients and residuals of the least squares solution.\n    \n    Args:\n    lhs (array_like): Left hand side matrix (design matrix) of the linear equation.\n    rhs (array_like): Right hand side vector or matrix.\n    rcond (float, optional): Cutoff ratio for small singular values; singular values smaller (in modulus) than \n                             rcond times the largest singular value are set to zero.\n    skipna (bool): If True, missing values (NaNs) in `rhs` will be ignored.\n\n    Returns:\n    tuple: A tuple containing:\n        - coefficients (ndarray): The coefficients of the linear equation.\n        - residuals (ndarray): The residuals of the solution.\n    \"\"\"\n    # Convert input arrays to Dask arrays\n    d_lhs = da.from_array(lhs, chunks=lhs.shape)\n    d_rhs = da.from_array(rhs, chunks=rhs.shape)\n    \n    # Handle missing values if requested\n    if skipna:\n        # Replace NaNs in rhs with the average of the non-NaN elements of rhs, axis can be set if needed\n        avg = da.nanmean(d_rhs, axis=0)\n        d_rhs = da.where(da.isnan(d_rhs), avg, d_rhs)\n    \n    # Compute the least squares solution, using Dask's linear algebra function\n    result = da.linalg.lstsq(d_lhs, d_rhs, rcond=rcond)\n    \n    # Extract coefficients and residuals\n    coefficients = result[0].compute()\n    residuals = result[1].compute() if len(result) > 1 else da.zeros(1).compute()  # Residuals may not be present if full rank\n    \n    return coefficients, residuals", "CodeBleu_score": 30.798192546239733}]}
{"instance_id": "c0c2c53b76e0", "generation_result": [{"code": "from dask.array import from_array as da_from_array\nimport numpy as np\n\ndef from_array(self, data: Any, chunks: Union[str, int, tuple], **kwargs: Any) -> Union[DaskArray, Any]:\n    \"\"\"\n    The function from_array is a method that takes data, chunks, and additional keyword arguments.\n    It imports the dask.array module and checks if the data is an instance of ImplicitToExplicitIndexingAdapter.\n    If so, it sets the meta keyword argument to np.ndarray, indicating that lazily loaded backend array\n    classes should use NumPy array operations.\n    The function then calls da.from_array with the provided data, chunks, and additional keyword arguments,\n    returning a DaskArray or another appropriate type.\n    \"\"\"\n\n    # Assuming ImplicitToExplicitIndexingAdapter is some type that requires specific handling,\n    # we need to handle it. This part of the code assumes that:\n    # `ImplicitToExplicitIndexingAdapter` should be a defined class or type; here it is used as a placeholder.\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        kwargs['meta'] = np.ndarray\n\n    # Utilize dask.array's from_array function to handle the conversion\n    return da_from_array(data, chunks=chunks, **kwargs)", "CodeBleu_score": 51.6778601415119}, {"code": "from typing import Any, Union\nimport dask.array as da\nfrom numpy import ndarray as DaskArray\n\n# Assuming a placeholder for the contextual ImplicitToExplicitIndexingAdapter class\n# since it's not defined in widely used libraries or earlier in the conversation\nclass ImplicitToExplicitIndexingAdapter:\n    pass\n\n# Define the method method 'from_array' assuming it belongs to a class\nclass DataHandler:\n    def from_array(self, data: Any, chunks: Union[int, tuple[int], str], **kwargs: Any) -> Union[DaskArray, Any]:\n        \"\"\"\n        The function from_array is a method that takes data, chunks, and additional keyword arguments.\n        It imports the dask.array module and checks if the data is an instance of ImplicitToExplicitIndexingAdapter.\n        If so, it sets the meta keyword argument to np.ndarray, indicating that lazily loaded backend array classes should use NumPy array operations.\n        The function then calls da.from_array with the provided data, chunks, and additional keyword arguments, returning a DaskArray or another appropriate type.\n        \"\"\"\n        \n        # Import necessary libraries\n        import dask.array as da\n        import numpy as np\n\n        # Check if data is an instance of ImplicitToExplicitIndexingAdapter and set `meta` to np.ndarray if necessary\n        if isinstance(data, ImplicitToExplicitIndexingAdapter):\n            kwargs['meta'] = np.ndarray\n        \n        # Create a DaskArray from the data\n        result_array = da.from_array(data, chunks, **kwargs)\n\n        return result_array", "CodeBleu_score": 50.61901482590648}, {"code": "import numpy as np\nimport dask.array as da\nfrom typing import Any, Union\n\nclass YourClass:\n    def from_array(\n            self, data: Any, chunks: Union[int, tuple, str], **kwargs: Any\n    ) -> Union[da.Array, Any]:\n        \"\"\"\n        The function from_array is a method that takes data, chunks, and additional keyword arguments.\n        It imports the dask.array module and checks if the data is an instance of ImplicitToExplicitIndexingAdapter.\n        If so, it sets the meta keyword argument to np.ndarray, indicating that lazily loaded backend array classes\n        should use NumPy array operations.\n        The function then calls da.from_array with the provided data, chunks, and additional keyword arguments, \n        returning a DaskArray or another appropriate type.\n        \"\"\"\n        # Simulating the check for an instance of ImplicitToExplicitIndexingAdapter (not an actual class provided)\n        if isinstance(data, ImplicitToExplicitIndexingAdapter):  # Assuming this class is defined elsewhere\n            kwargs['meta'] = np.ndarray\n        \n        return da.from_array(data, chunks, **kwargs)\n\n# Note: The class ImplicitToExplicitIndexingAdapter is assumed to be defined elsewhere in the user's project.", "CodeBleu_score": 49.95255830080556}]}
{"instance_id": "6559019d09a0", "generation_result": [{"code": "from typing import Any, Callable, Tuple\nfrom dask.highlevelgraph import HighLevelGraph  # Assuming a HighLevelGraph is used as the Graph type.\n\n# Define a type for the PostPersistCallable and include a proper import if this is an actual data type in Dask.\n# For this example, assume this is just any callable that deals with post-persistence logic.\nPostPersistCallable = Callable\n\nclass DaskArray:\n    # Assuming this class represents a Dask array.\n    def __dask_postpersist__(self):\n        # This method should mirror the actual __dask_postpersist__ method of a Dask array.\n        # Return type would be as specified (function, additional_args)\n        def finalize(graph: HighLevelGraph, post_func: PostPersistCallable, *args):\n            # Modify the graph, call post_func or whatever is necessary\n            print(\"Running post-persistence modifications...\")\n            return graph  # Modified graph\n        \n        return finalize, ('arg1', 'arg2')\n\nclass MyClass:\n    def __init__(self, data) -> None:\n        self.data = data\n\n    def __dask_postpersist__(self) -> Tuple[Callable, Tuple[Any, ...]]:\n        \"\"\"\n        Handles the post-persistence process for a Dask array-like object. It checks if the\n        object's 'data' attribute is a Dask array, then calls the data's __dask_postpersist__\n        method to get a function and its arguments, and returns those wrapped in a tuple.\n        If the data is not a Dask array-like, it raises an AttributeError.\n\n        Returns:\n            Tuple[Callable, Tuple[Any, ...]]: A tuple containing a finalize function and arguments.\n\n        Raises:\n            AttributeError: If 'data' does not have a __dask_postpersist__ method.\n        \"\"\"\n        if hasattr(self.data, '__dask_postpersist__'):\n            finalize_func, args = self.data.__dask_postpersist__()\n            return finalize_func, args\n        else:\n            raise AttributeError(f\"The data attribute of {self.__class__.__name__} must be a Dask array-like object with a __dask_postpersist__ method.\")", "CodeBleu_score": 37.34982050594027}, {"code": "from typing import Any, Callable, Tuple, Graph\nfrom dask.delayed import Delayed\n\ndef __dask_postpersist__(\n        self,\n    ) -> Tuple[\n        Callable[\n            [Graph, 'PostPersistCallable', Any, Any],\n            'Self',\n        ],\n        Tuple[Any, ...],\n    ]:\n    \"\"\"\n    The function __dask_postpersist__ handles the post-persistence process for a dask array. \n    It checks if the data is a dask array, then calls the data's __dask_postpersist__ method \n    to get a function and its arguments. It returns a tuple containing a finalize function \n    and the arguments, which include the function and its arguments from the dask array's \n    post-persistence method. If the data is not a dask array, it raises an AttributeError.\n    \"\"\"\n    # Check if self is a Dask object, specifically check for `__dask_postpersist__` attribute\n    if hasattr(self, '__dask_postpersist__'):\n        # Get the finalize function and arguments from the Dask object\n        finalize, args = self.__dask_postpersist__()\n        # Return these as results\n        return finalize, args\n    else:\n        # Raise AttributeError if `self` is not a Dask object compatible for post-persistence\n        raise AttributeError(f\"The object {self} does not support Dask post-persistence operations.\")", "CodeBleu_score": 41.8539540595596}, {"code": "from typing import Callable, Tuple, Any\nfrom dask.delayed import Delayed\nimport dask.array as da\n\ndef __dask_postpersist__(self) -> Tuple[Callable, Tuple[Any, ...]]:\n    \"\"\"\n    Handles the post-persistence process for DataWrapper that wraps a Dask array.\n    It delegates the post-persistence routine of the wrapped Dask array.\n\n    Returns:\n        tuple: Consisting of a Callable and a tuple of arguments necessary to\n               reconstruct the post-persisted object.\n    \n    Raises:\n        AttributeError: If the wrapped object does not support post-persistence.\n    \"\"\"\n    if hasattr(self.data, '__dask_postpersist__'):\n        func, args = self.data.__dask_postpersist__()\n        return func, (self.__class__,) + args\n    else:\n        raise AttributeError(f\"The data attribute of type {type(self.data).__name__} does not support post-persistence.\")", "CodeBleu_score": 38.13014458508167}]}
{"instance_id": "4a2801bb3851", "generation_result": [{"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\nimport numpy as np\n\ndef _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n):\n    \"\"\"\n    The function _dask_or_eager_func creates a new function that dispatches to either a dask module or an eager\n    execution module based on the input types.\n    It takes the name of the function to be dispatched, an eager execution module (defaulting to NumPy),\n    and a dask module (defaulting to \"dask.array\").\n    The inner function f checks if any of the input arguments are dask arrays using is_duck_dask_array.\n    If dask arrays are present, it dynamically imports the specified dask module (if provided as a string)\n    and retrieves the corresponding function by name.\n    Otherwise, it retrieves the function from the eager execution module.\n    The inner function f then calls the retrieved function with the provided arguments and keyword arguments.\n    Finally, the outer function returns the inner function f.\n    \"\"\"\n    import importlib\n\n    def is_duck_dask_array(x):\n        \"\"\" Check if the object is a dask array. \"\"\"\n        try:\n            import dask.array as da\n            return isinstance(x, da.Array)\n        except ImportError:\n            return False\n\n    def f(*args, **kwargs):\n        \"\"\" Dynamic function that dispatches based on input types. \"\"\"\n        # Identify if any input is a dask array\n        if any(is_duck_dask_array(arg) for arg in args):\n            # If input type is linked to dask arrays, import and use dask module\n            if isinstance(dask_module, str):\n                dask_mod = importlib.import_module(dask_module)\n            else:\n                dask_mod = dask_module\n            func = getattr(dask_mod, name)\n        else:\n            # Use numpy or the provided eager module\n            func = getattr(eager_module, name)\n        \n        # Call the function with the provided arguments\n        return func(*args, **kwargs)\n\n    return f\n\n# Example usage:\n# get_sum = _dask_or_eager_func('sum')\n# result = get_sum([1,2,3])  # This will use numpy sum function\n# For dask array, it will use dask 'sum' function when passed a dask array.", "CodeBleu_score": 50.870456999497506}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\nfrom typing import Any, Callable\r\n\r\ndef _dask_or_eager_func(\r\n    name: str,\r\n    eager_module: Callable = np,\r\n    dask_module: str = \"dask.array\",\r\n):\r\n    \"\"\"The function _dask_or_eager_func creates a new function that dispatches to either a dask module or an eager execution module based on the input types.\r\n    It takes the name of the function to be dispatched, an eager execution module (defaulting to NumPy), and a dask module (defaulting to \"dask.array\").\r\n    The inner function f checks if any of the input arguments are dask arrays using is_duck_dask_array.\r\n    If dask arrays are present, it dynamically imports the specified dask module (if provided as a string) and retrieves the corresponding function by name.\r\n    Otherwise, it retrieves the function from the eager execution module.\r\n    The inner function f then calls the retrieved function with the provided arguments and keyword arguments.\r\n    Finally, the outer function returns the inner function f.\"\"\"\r\n\r\n    import importlib\r\n    import dask.array as da\r\n\r\n    def f(*args, **kwargs) -> Any:\r\n        # Determine if any argument is a dask array to decide module dynamism\r\n        is_dask_input = any(isinstance(arg, da.Array) for arg in args)\r\n\r\n        # Determine proper module based on input type and dynamically import if necessary\r\n        if is_dask_input:\r\n            if isinstance(dask_module, str):\r\n                imported_dask_module = importlib.import_module(dask_module)\r\n            else:\r\n                imported_dask_module = dask_module\r\n            target_func = getattr(imported_dask_module, name)\r\n        else:\r\n            target_func = getattr(eager_module, name)\r\n\r\n        return target_func(*args, **kwargs)\r\n\r\n    return f\r\n\r\n# Example usage assuming functions with expected behavior exist in the eager and dask modules\r\n# f_np_add = _dask_or_eager_func('add')  # Returns a function that adds numbers using numpy or dask based on input type.\r\n# result = f_np_add(np.array([1, 2]), np.array([3, 4]))  # Uses numpy because all inputs are numpy arrays\r\n# result_dask = f_np_add(da.from_array([1, 2], chunks=1), da.from_array([3, 4], chunks=1))  # Uses dask because inputs are dask arrays", "CodeBleu_score": 48.75895197532382}, {"code": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)\nimport importlib\n\ndef is_duck_dask_array(arr):\n    \"\"\"Helper function to check if the provided array is a dask array.\"\"\"\n    try:\n        return isinstance(arr, dask.array.core.Array)\n    except ImportError:\n        return False\n\ndef _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n):\n    \"\"\"\n    The function _dask_or_eager_func creates a new function that dispatches to either a dask module or an eager execution module based on the input types.\n    It takes the name of the function to be dispatched, an eager execution module (defaulting to NumPy), and a dask module (defaulting to \"dask.array\").\n    The inner function f checks if any of the input arguments are dask arrays using is_duck_dask_array.\n    If dask arrays are present, it dynamically imports the specified dask module (if provided as a string) and retrieves the corresponding function by name.\n    Otherwise, it retrieves the function from the eager execution module.\n    The inner function f then calls the retrieved function with the provided arguments and keyword arguments.\n    Finally, the outer function returns the inner function f.\n    \"\"\"\n\n    def f(*args, **kwargs):\n        # Check if any arguments are dask arrays\n        use_dask = any(is_duck_dask_array(arg) for arg in args)\n        \n        # Select the appropriate module and function\n        if use_dask:\n            if type(dask_module) == str:\n                # Dynamically import the dask module\n                actual_dask_module = importlib.import_module(dask_module)\n            else:\n                actual_dask_module = dask_module\n            func = getattr(actual_dask_module, name)\n        else:\n            func = getattr(eager_module, name)\n        \n        # Call and return the function\n        return func(*args, **kwargs)\n    \n    return f\n\n# Setting the default values\nimport numpy as np\n\n# Assuming the modules are imported if actual instances are to be used\n# import dask.array\n\n# Example usage\n# func_max = _dask_or_eager_func('amax')\n# print(func_max(np.array([1, 2, 3])))  # Using numpy\n# print(func_max(dask.array.array([1, 2, 3])))  # Using dask.array (this line would be commented if dask is not installed)", "CodeBleu_score": 50.834608601735184}]}
{"instance_id": "eef8d746d01e", "generation_result": [{"code": "import numpy as np\n\ndef lazy_array_equiv(arr1, arr2):\n    \"\"\"\n    The function lazy_array_equiv takes two arrays, arr1 and arr2, as arguments.\n    It first checks if the two arrays are the same object and returns True if they are.\n    It then converts the arrays to NumPy arrays using asarray.\n    If the shapes of the arrays are not the same, it returns False.\n    If Dask is available and both arrays are Dask arrays, it uses the Dask tokenize function to compare the arrays.\n    If the tokenized values are equal, it returns True; otherwise, it returns None.\n    In cases where the arrays are not the same object, not the same shape, or not Dask arrays, it returns None.\n    \"\"\"\n    # Check if both arrays are actually the same object\n    if arr1 is arr2:\n        return True\n\n    # Convert arrays to NumPy arrays (if they aren't already)\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n\n    # Check if the shapes of the arrays are the same\n    if arr1.shape != arr2.shape:\n        return False\n\n    # Check if both arrays are dask arrays using dask's `iscollection` and then compare using `tokenize`\n    try:\n        import dask.array as da\n        from dask.base import tokenize\n        if isinstance(arr1, da.Array) and isinstance(arr2, da.Array):\n            return tokenize(arr1) == tokenize(arr2)\n    except ImportError:\n        pass\n\n    # If arrays are not Dask arrays or Dask is not available, we cannot provide a comparison, hence return None\n    return None", "CodeBleu_score": 63.32313469041222}, {"code": "def lazy_array_equiv(arr1, arr2)\nimport numpy as np\n\ndef lazy_array_equiv(arr1, arr2):\n    \"\"\"The function lazy_array_equiv takes two arrays, arr1 and arr2, as arguments.\n    It first checks if the two arrays are the same object and returns True if they are.\n    It then converts the arrays to NumPy arrays using asarray.\n    If the shapes of the arrays are not the same, it returns False.\n    If Dask is available and both arrays are Dask arrays, it uses the Dask tokenize function to compare the arrays.\n    If the tokenized values are equal, it returns True; otherwise, it returns None.\n    In cases where the arrays are not the same object, not the same shape, or not Dask arrays, it returns None.\"\"\"\n\n    # Check if both arrays are the exact same object\n    if arr1 is arr2:\n        return True\n    \n    # Convert the arrays to NumPy arrays using asarray\n    arr1 = np.asarray(arr1)\n    arr2 = np.asarray(arr2)\n    \n    # Check if the shapes of both arrays are equivalent\n    if arr1.shape != arr2.shape:\n        return False\n    \n    # Check for Dask array equivalence\n    try:\n        import dask.array as da\n        if isinstance(arr1, da.Array) and isinstance(arr2, da.Array):\n            from dask.base import tokenize\n            return tokenize(arr1) == tokenize(arr2)\n    except ImportError:\n        pass\n\n    # Otherwise\n    return None\n\n# Note: This function handles cases for identical objects, shape mismatch, and Dask arrays.\n# If you are testing it and aren't using Dask arrays, it will typically return None since the Dask check will be skipped.", "CodeBleu_score": 57.85522419053437}, {"code": "import numpy as np\n\ndef lazy_array_equiv(arr1, arr2):\n    # If arr1 and arr2 are the same object, return True\n    if arr1 is arr2:\n        return True\n\n    # Convert to NumPy arrays using np.asarray\n    arr1_np = np.asarray(arr1)\n    arr2_np = np.asarray(arr2)\n\n    # Check if the shapes of the arrays are the same\n    if arr1_np.shape != arr2_np.shape:\n        return False\n\n    # Check for the availability of dask and if both are dask arrays\n    try:\n        import dask.array as da\n        from dask.base import tokenize\n        \n        if isinstance(arr1, da.Array) and isinstance(arr2, da.Array):\n            # Use tokenize to compare the arrays\n            if tokenize(arr1) == tokenize(arr2):\n                return True\n            else:\n                return None\n    except ImportError:\n        pass  # Dask is not available\n\n    # For any other cases not covered above\n    return None", "CodeBleu_score": 63.10703148514498}]}
{"instance_id": "15e830ff5fcb", "generation_result": [{"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\nimport numpy as np\nimport dask.array as da\n\ndef randn(shape, frac_nan=None, chunks=None, seed=0):\n    \"\"\"The function randn generates an array of normally distributed random numbers with a specified shape, and optionally includes NaN values. It takes the shape of the array, a fraction of NaNs, chunk size, and a seed for random number generation as inputs. If chunking is specified, it uses Dask to create the array; otherwise, it uses NumPy. If a fraction of NaNs is provided, it randomly assigns NaNs to the specified proportion of the array's elements. The function returns the generated array.\n\n    Args:\n    shape (tuple): Shape of the output array.\n    frac_nan (float, optional): Fraction of the array elements to be set as NaN. Must be between 0 and 1. Default is None, which means no NaN values.\n    chunks (tuple, optional): Shape of the chunks for the Dask array. Only relevant if Dask is used. Default is None, which means NumPy is used.\n    seed (int): Seed for the random number generator.\n\n    Returns:\n    numpy.ndarray or dask.array.Array: A NumPy array if chunks is None, otherwise a Dask array.\n    \"\"\"\n    np.random.seed(seed)\n    \n    # Create the array using Dask or NumPy depending on whether chunks are specified\n    if chunks:\n        array = da.random.normal(size=shape, chunks=chunks)\n    else:\n        array = np.random.randn(*shape)\n\n    # If a fraction of NaNs was specified, randomly assign NaNs to the specified proportion of elements\n    if frac_nan is not None:\n        if not 0 <= frac_nan <= 1:\n            raise ValueError(\"frac_nan must be between 0 and 1.\")\n        \n        # Generate a mask for NaNs\n        if chunks:\n            nan_mask = da.random.random(size=shape, chunks=chunks) < frac_nan\n        else:\n            nan_mask = np.random.random(size=shape) < frac_nan\n\n        # Set the selected elements to NaN\n        array = da.where(nan_mask, np.nan, array) if chunks else np.where(nan_mask, np.nan, array)\n\n    return array\n\n# Test the function by printing an output example (for debug purposes)\nif __name__ == \"__main__\":\n    # Example test with NumPy, small shape, and fraction of NaNs\n    print(\"NumPy with NaNs:\")\n    print(randn((2, 3), frac_nan=0.3))\n    # Example test with Dask, chunking, no NaNs\n    print(\"\\nDask without NaNs, chunks specified:\")\n    print(randn((1000, 1000), chunks=(100, 100)).compute())  # Compute to view the result as this returns a Dask array", "CodeBleu_score": 38.51385637273715}, {"code": "import numpy as np\nimport dask.array as da\n\ndef randn(shape, frac_nan=None, chunks=None, seed=0):\n    \"\"\"\n    The function randn generates an array of normally distributed random numbers with a specified shape,\n    and optionally includes NaN values. It takes the shape of the array, a fraction of NaNs, chunk size,\n    and a seed for random number generation as inputs. If chunking is specified, it uses Dask to create\n    the array; otherwise, it uses NumPy. If a fraction of NaNs is provided, it randomly assigns NaNs to\n    the specified proportion of the array's elements. The function returns the generated array.\n    \n    Parameters:\n        shape (tuple): The shape of the output array.\n        frac_nan (float, optional): The fraction of the array that should be NaN. Values between 0 and 1.\n        chunks (tuple, optional): The shape of chunks for the dask array. If None, numpy is used instead of dask.\n        seed (int, optional): The seed for the RNG, defaults to 0, providing reproducibility.\n        \n    Returns:\n        ndarray or dask.array: Normally distributed random numbers (with optional NaNs).\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Create the random numbers array with optional chunking\n    if chunks:\n        # Using dask for chunked arrays\n        data = da.random.normal(size=shape, chunks=chunks)\n    else:\n        # Using numpy for non-chunked arrays\n        data = np.random.randn(*shape)\n    \n    # If frac_nan is specified, insert NaN values into the array\n    if frac_nan is not None:\n        # Check if 'frac_nan' is within the valid range\n        if not (0 <= frac_nan <= 1):\n            raise ValueError(\"frac_nan must be between 0 and 1.\")\n\n        # Generate a mask for NaNs\n        nan_mask = np.random.random(size=shape) < frac_nan\n\n        if chunks:\n            # Apply the mask using dask if chunking is enabled\n            nan_mask = da.from_array(nan_mask, chunks=chunks)\n            data = da.where(nan_mask, np.nan, data)\n        else:\n            # Apply the mask directly if using numpy\n            data[nan_mask] = np.nan\n\n    return data", "CodeBleu_score": 41.23409371759126}, {"code": "def randn(shape, frac_nan=None, chunks=None, seed=0)\nimport numpy as np\nimport dask.array as da\n\ndef randn(shape, frac_nan=None, chunks=None, seed=0):\n    \"\"\"\n    The function randn generates an array of normally distributed random numbers with a specified shape,\n    and optionally includes NaN values. It takes the shape of the array, a fraction of NaNs, chunk size,\n    and a seed for random number generation as inputs. If chunking is specified, it uses Dask to create the\n    array; otherwise, it uses NumPy. If a fraction of NaNs is provided, it randomly assigns NaNs to the\n    specified proportion of the array's elements. The function returns the generated array.\n\n    Args:\n    shape (tuple of int): The dimensions of the output array.\n    frac_nan (float, optional): The fraction of elements to set as NaN. Must be between 0 and 1. Default is None.\n    chunks (int or tuple, optional): The chunk size for Dask array. If not specified, a NumPy array is created.\n    seed (int, optional): The random seed for reproducibility of results. Default is 0.\n\n    Returns:\n    np.ndarray or da.Array: An array with normally distributed values, optionally with NaNs and using Dask if chunked.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate random numbers array using either NumPy or Dask\n    if chunks is None:\n        # Using NumPy\n        array = np.random.randn(*shape)\n    else:\n        # Using Dask\n        array = da.random.normal(size=shape, chunks=chunks)\n\n    # Insert NaNs if frac_nan is specified\n    if frac_nan is not None:\n        if not 0 <= frac_nan <= 1:\n            raise ValueError(\"frac_nan must be between 0 and 1.\")\n        \n        # Calculate number of NaNs\n        total_elements = np.prod(shape)\n        num_nans = int(frac_nan * total_elements)\n        \n        # Create flat index array and randomly select indices to be NaN\n        indices = np.random.choice(total_elements, num_nans, replace=False)\n        \n        if chunks is None:\n            # Use flat indexing for numpy array\n            flat_array = array.ravel()\n            flat_array[indices] = np.nan\n            array = flat_array.reshape(shape)\n        else:\n            # Use map_blocks for Dask array\n            def set_nans(block, block_indices=None):\n                block_shape = block.shape\n                idx_to_nan = np.isin(np.arange(block.size), indices - block_indices[0]).reshape(block_shape)\n                block[idx_to_nan] = np.nan\n                return block\n            \n            array = array.map_blocks(set_nans, dtype=array.dtype)\n\n    return array\n    # Note: For the Dask array with NaNs, the indices computation and application would need further refinement\n    # to properly partition across blocks. This implementation is an approximation.\n", "CodeBleu_score": 40.662750642570764}]}
{"instance_id": "5b57abbbe38b", "generation_result": [{"code": "import pandas as pd\nfrom yfinance.error import YFNotImplementedError  # Assume YFNotImplementedError can be imported from yfinance.error\n\nclass FinancialDataFetcher:\n    def __init__(self):\n        # Initialize the _analyst_price_target attribute, possibly None initially.\n        self._analyst_price_target = None\n\n    def analyst_price_target(self) -> pd.DataFrame:\n        \"\"\"\n        Returns a DataFrame containing analyst price target data.\n        If the attribute _analyst_price_target is None, it raises a YFNotImplementedError with the message 'analyst_price_target'.\n        \n        Returns:\n            pd.DataFrame: Analyst price target data.\n\n        Raises:\n            YFNotImplementedError: If the _analyst_price_target data is not available.\n        \"\"\"\n        if self._analyst_price_target is None:\n            raise YFNotImplementedError('analyst_price_target')\n        \n        return self._analyst_price_target", "CodeBleu_score": 79.50456027370826}, {"code": "import pandas as pd\n\nclass YFNotImplementedError(Exception):\n    pass\n\nclass YourClassName:\n    def __init__(self):\n        self._analyst_price_target = None  # This would typically be set somewhere else in your class\n\n    def analyst_price_target(self) -> pd.DataFrame:\n        \"\"\"\n        The function analyst_price_target returns a DataFrame containing analyst price target data.\n        If the attribute _analyst_price_target is None, it raises a YFNotImplementedError with the message 'analyst_price_target'.\n        \"\"\"\n        if self._analyst_price_target is None:\n            raise YFNotImplementedError('analyst_price_target')\n        else:\n            return self._analyst_price_target", "CodeBleu_score": 69.58129740084038}, {"code": "import pandas as pd\n\nclass YourClassNameHere:\n    def __init__(self):\n        self._analyst_price_target = None  # This should be set elsewhere if using real data\n\n    def analyst_price_target(self) -> pd.DataFrame:\n        \"\"\"\n        The function analyst_price_target returns a DataFrame containing analyst price target data.\n        If the attribute _analyst_price_target is None, it raises a YFNotImplementedError with the message 'analyst_price_target'.\n        \"\"\"\n        if self._analyst_price_target is None:\n            raise YFNotImplementedError('analyst_price_target')\n        return self._analyst_price_target\n\nclass YFNotImplementedError(Exception):\n    def __init__(self, message):\n        super().__init__(message)", "CodeBleu_score": 78.69505466815214}]}
{"instance_id": "592e414671e9", "generation_result": [{"code": "import pandas as pd\nimport logging\n\nclass FinancialDataFetcher:\n    def __init__(self):\n        self.allowed_names = ['income statement', 'balance sheet', 'cash flow statement']\n        self.allowed_timescales = ['annual', 'quarterly']\n        \n    def _fetch_data_from_source(self, name, timescale):\n        \"\"\"Simulates fetching financial data. This would be replaced by an actual data fetching in a real scenario.\"\"\"\n        # This is a placeholder. In practice, this would interact with an API or database.\n        import random\n        if random.choice([True, False]):\n            return pd.DataFrame({'Date': ['2021', '2022'], 'Revenue': [1000, 2000]})\n        else:\n            raise Exception(\"Data fetch error\")\n\n    def _fetch_time_series(self, name, timescale, proxy=None):\n        \"\"\"The function _fetch_time_series retrieves financial time series data based on the provided name and timescale.\n        It validates the name against allowed financial statements and the timescale against allowed periods.\n        If either is invalid, it raises a ValueError. The function attempts to create a financials table using a helper method,\n        returning the table if successful. If an error occurs, it logs the error and returns an empty DataFrame.\n        \n        Args:\n            name (str): The name of the financial statement (e.g., 'income statement').\n            timescale (str): The timescale for data retrieval (e.g., 'annual', 'quarterly').\n            proxy (Optional[dict]): Proxy configuration (not implemented in this example).\n        \n        Returns:\n            pd.DataFrame: The fetched financial data table or an empty DataFrame if the fetch fails.\n        \"\"\"\n        # Validate inputs\n        if name not in self.allowed_names:\n            raise ValueError(f\"Invalid financial statement name: {name}. Allowed names are: {self.allowed_names}\")\n        if timescale not in self.allowed_timescales:\n            raise ValueError(f\"Invalid timescale: {timescale}. Allowed timescales are: {self.allowed_timescales}\")\n        \n        # Fetch data\n        try:\n            data = self._fetch_data_from_source(name, timescale)\n            return data\n        except Exception as e:\n            logging.error(f\"Error fetching data: {e}\")\n            return pd.DataFrame()\n\n# Example usage:\nfetcher = FinancialDataFetcher()\ntry:\n    df = fetcher._fetch_time_series('income statement', 'annual')\n    print(df)\nexcept ValueError as e:\n    print(e)", "CodeBleu_score": 38.86827749440273}, {"code": "import pandas as pd\nimport logging\n\nclass FinancialDataRetriever:\n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n        self.allowed_names = ['income_statement', 'balance_sheet', 'cash_flow']\n        self.allowed_timescales = ['annual', 'quarterly']\n\n    def _fetch_time_series(self, name, timescale, proxy=None):\n        \"\"\"\n        Retrieves financial time series data based on the provided name and timescale.\n        \n        Parameters:\n        name (str): The name of the financial statement to retrieve.\n        timescale (str): The timescale, either 'annual' or 'quarterly'.\n        proxy (optional): Proxy configuration for external requests.\n        \n        Returns:\n        pd.DataFrame: A DataFrame containing the financial data, or an empty DataFrame if an error occurs.\n        \"\"\"\n        # Validate the 'name'\n        if name not in self.allowed_names:\n            raise ValueError(f\"Invalid name. Expected one of {self.allowed_names}, got {name}.\")\n\n        # Validate the 'timescale'\n        if timescale not in self.allowed_timescales:\n            raise ValueError(f\"Invalid timescale. Expected one of {self.allowed_timescales}, got {timescale}.\")\n\n        try:\n            # Simulating a helper method that creates a financials table.\n            # You need to replace this with the actual method or logic to fetch the data.\n            data = self._create_financials_table(name, timescale, proxy)\n            return data\n        except Exception as e:\n            # Log the error and return an empty DataFrame\n            self.logger.error(\"Failed to fetch time series data\", exc_info=True)\n            return pd.DataFrame()\n\n    def _create_financials_table(self, name, timescale, proxy=None):\n        \"\"\"\n        Simulates the fetching of financial data.\n        This method should be replaced with actual data retrieval logic.\n        \"\"\"\n        # Example data retrieval simulation\n        # Normally, this would involve API calls or database queries.\n        if name == 'income_statement' and timescale == 'annual':\n            return pd.DataFrame({'Year': [2021, 2022], 'Revenue': [100, 200]})\n        elif name == 'income_statement' and timescale == 'quarterly':\n            return pd.DataFrame({'Quarter': ['Q1 2022', 'Q2 2022'], 'Revenue': [150, 250]})\n        # Add other conditions for balance_sheet, cash_flow, etc.\n        else:\n            return pd.DataFrame()\n\n# Usage example:\n# retriever = FinancialDataRetriever()\n# df = retriever._fetch_time_series('income_statement', 'annual')\n# print(df)", "CodeBleu_score": 38.157663348353964}, {"code": "import pandas as pd\nimport logging\n\ndef _fetch_time_series(self, name, timescale, proxy=None):\n    \"\"\"\n    Retrieves financial time series data based on the provided 'name' and 'timescale'.\n    Validates the `name` against allowed financial statements and the `timescale` against allowed periods.\n    If either is invalid, raises a ValueError.\n    Uses a helper method to attempt creation of a financials table, returning the table if successful.\n    If an error occurs, logs the error and returns an empty DataFrame.\n\n    Args:\n    name (str): Name of the financial statement or time series data to fetch.\n    timescale (str): The periodicity of the data to be fetched, e.g., 'monthly', 'quarterly', 'annually'.\n    proxy (dict, optional): Proxy configuration for network requests, defaults to None.\n\n    Returns:\n    pd.DataFrame: The requested time series data as a DataFrame, or an empty DataFrame in case of errors.\n    \"\"\"\n    allowed_names = ['income_statement', 'balance_sheet', 'cash_flow']\n    allowed_timescales = ['monthly', 'quarterly', 'annually']\n\n    # Validate name and timescale\n    if name not in allowed_names:\n        raise ValueError(f\"Invalid name provided. Allowed names are {allowed_names}.\")\n    if timescale not in allowed_timescales:\n        raise ValueError(f\"Invalid timescale provided. Allowed timescales are {allowed_timescales}.\")\n\n    try:\n        # Assuming 'create_financials_table' is a helper method to create financial data table.\n        financials_table = self.create_financials_table(name, timescale, proxy)\n        return financials_table\n    except Exception as e:\n        # Log the error and return an empty DataFrame\n        logging.error(f\"Error fetching time series data for {name} on a {timescale} basis: {str(e)}\")\n        return pd.DataFrame()\n\n# Example usage inside a class could look like this:\n# class FinancialDataFetcher:\n#     def create_financials_table(self, name, timescale, proxy=None):\n#         # Logic to fetch and create financial table, might utilize external API or database\n#         pass\n\n#     _fetch_time_series = _fetch_time_series", "CodeBleu_score": 55.066652870465646}]}
{"instance_id": "af01d9bae091", "generation_result": [{"code": "import pandas as pd\n\nclass FundOwnershipProcessor:\n    def __init__(self):\n        self._mutualfund = None\n\n    def _parse_fund_ownership(self, data):\n        \"\"\"The function _parse_fund_ownership processes fund ownership data from a given input.\n        It extracts the list of ownership details and iterates through each owner's data, parsing raw values and removing the 'maxAge' field.\n        The processed data is converted into a pandas DataFrame.\n        If the DataFrame is not empty, it converts the 'reportDate' to a datetime format and renames specific columns for clarity.\n        The resulting DataFrame is assigned to the instance variable _mutualfund.\"\"\"\n\n        # Assuming the fund ownership details are contained in the 'ownership' key\n        ownership_data = data.get(\"ownership\", [])\n        \n        # Process the ownership data, excluding 'maxAge' field\n        if ownership_data:\n            processed_data = []\n            for owner in ownership_data:\n                owner_data = {key: value for key, value in owner.items() if key != \"maxAge\"}\n                processed_data.append(owner_data)\n            \n            # Convert to DataFrame\n            df = pd.DataFrame(processed_data)\n            \n            if not df.empty:\n                # Convert 'reportDate' to datetime\n                if 'reportDate' in df.columns:\n                    df['reportDate'] = pd.to_datetime(df['reportDate'])\n                \n                # Rename columns if needed\n                rename_columns = {\n                    'ownerName': 'Owner Name',\n                    'ownershipPercentage': 'Ownership Percentage'\n                    # Add more mappings as needed\n                }\n                df.rename(columns=rename_columns, inplace=True)\n            \n            # Assign DataFrame to the instance variable\n            self._mutualfund = df", "CodeBleu_score": 36.07598016963084}, {"code": "import pandas as pd\n\ndef _parse_fund_ownership(self, data):\n    \"\"\"\n    The function _parse_fund_ownership processes fund ownership data from a given input.\n    It extracts the list of ownership details and iterates through each owner's data, parsing raw values and removing the 'maxAge' field.\n    The processed data is converted into a pandas DataFrame.\n    If the DataFrame is not empty, it converts the 'reportDate' to a datetime format and renames specific columns for clarity.\n    The resulting DataFrame is assigned to the instance variable _mutualfund.\n    \"\"\"\n    \n    # Start by extracting the ownership details assuming data is a dictionary or similar structure\n    ownership_details = data.get('ownership', [])\n    \n    # Create a list of dictionaries where each dictionary will have the processed owner data\n    processed_data = []\n    for owner in ownership_details:\n        if 'maxAge' in owner:\n            del owner['maxAge']  # Remove the 'maxAge' field from the data\n        processed_data.append(owner)\n    \n    # Create a DataFrame from the processed data\n    df = pd.DataFrame(processed_data)\n    \n    # Check if DataFrame is not empty\n    if not df.empty:\n        # Convert 'reportDate' to datetime format if it exists in the columns\n        if 'reportDate' in df.columns:\n            df['reportDate'] = pd.to_datetime(df['reportDate'])\n        \n        # Rename specific columns for clarity if necessary, example given below\n        column_renames = {\n            'ownerName': 'Owner Name',\n            'sharesheld': 'Shares Held',\n            'changeQty': 'Change in Quantity'\n        }\n        df.rename(columns=column_renames, inplace=True)\n        \n    # Assign the DataFrame to the instance variable _mutualfund\n    self._mutualfund = df\n\n# Example usage (Outside the scope since it is a class method,\n# this example is just to illustrate the type of data structure expected)\nclass FundOwnershipProcessor:\n    def __init__(self):\n        self._mutualfund = pd.DataFrame()\n    \n    _parse_fund_ownership = _parse_fund_ownership\n\n# Sample data hypothetical structure\nprocessor = FundOwnershipProcessor()\nsample_data = {\n    \"ownership\": [\n        {\"ownerName\": \"John Doe Fund\", \"sharesHeld\": 1500, \"changeQty\": 100, \"maxAge\": 1, \"reportDate\": \"2021-08-01\"},\n        {\"ownerName\": \"Jane Smith Capital\", \"sharesHeld\": 2000, \"changeQty\": 50, \"maxAge\": 1, \"reportDate\": \"2021-08-02\"}\n    ]\n}\nprocessor._parse_fund_ownership(sample_data)\nprint(processor._mutualfund)", "CodeBleu_score": 36.95558674404422}, {"code": "import pandas as pd\n\ndef _parse_fund_ownership(self, data):\n    \"\"\"The function _parse_fund_ownership processes fund ownership data from a given input.\n    It extracts the list of ownership details and iterates through each owner's data, parsing raw values and removing the 'maxAge' field.\n    The processed data is converted into a pandas DataFrame.\n    If the DataFrame is not empty, it converts the 'reportDate' to a datetime format and renames specific columns for clarity.\n    The resulting DataFrame is assigned to the instance variable _mutualfund.\"\"\"\n    \n    # Initialize a list to hold processed ownership records\n    processed_data = []\n\n    # Iterate over each entry in the input data\n    for owner in data:\n        # Exclude the 'maxAge' field\n        filtered_data = {key: value for key, value in owner.items() if key != 'maxAge'}\n        # Add it to the list\n        processed_data.append(filtered_data)\n    \n    # Convert the processed data into a DataFrame\n    ownership_df = pd.DataFrame(processed_data)\n\n    # Process the DataFrame if it's not empty\n    if not ownership_df.empty:\n        # Convert the 'reportDate' column to datetime, if it exists\n        if 'reportDate' in ownership_df.columns:\n            ownership_df['reportDate'] = pd.to_datetime(ownership_df['reportDate'])\n\n        # Example renaming of columns for clarity\n        column_renames = {'startDate': 'Start Date', 'position': 'Position Size'}\n        ownership_df.rename(columns=column_renames, inplace=True)\n    \n    # Assign the processed DataFrame to the instance variable\n    self._mutualfund = ownership_df", "CodeBleu_score": 38.99615407280662}]}
{"instance_id": "2b5c6f1253db", "generation_result": [{"code": "class FinancialTicker:\n    def __init__(self, ticker, proxy=None):\n        self.ticker = ticker\n        self.proxy = proxy\n        self._currency = None\n    \n    def _fetch_metadata(self):\n        # Placeholder for fetching metadata\n        # Assuming it gets metadata from some external data source, potentially using a proxy\n        import requests\n        metadata_url = f\"http://example.com/data/{self.ticker}\"\n        headers = {}\n        if self.proxy:\n            proxy_dict = {\n                \"http\": self.proxy,\n                \"https\": self.proxy\n            }\n            response = requests.get(metadata_url, headers=headers, proxies=proxy_dict)\n        else:\n            response = requests.get(metadata_url, headers=headers)\n        \n        if response.status_code == 200:\n            return response.json()\n        else:\n            return {}\n    \n    def currency(self):\n        \"\"\"Retrieve and return the currency information for the given ticker.\"\"\"\n        if self._currency is not None:\n            return self._currency\n        else:\n            metadata = self._fetch_metadata()\n            self._currency = metadata.get('currency', 'Unknown')  # Defaulting to 'Unknown' if currency isn't found\n            return self._currency\n\n# Example usage:\nticker_info = FinancialTicker(\"AAPL\", proxy=\"http://someproxy.com:8080\")\nprint(ticker_info.currency())", "CodeBleu_score": 37.761966275541155}, {"code": "class Asset:\n    def __init__(self, ticker):\n        self.ticker = ticker\n        self._currency = None  # Use underscore to indicate \"protected\" attribute not to be accessed directly\n\n    def fetch_metadata(self):\n        \"\"\"Simulate fetching metadata from an API or database. Normally this might use requests or a database connection.\"\"\"\n        # Here we just simulate the metadata\n        metadata = {\n            'MSFT': {'currency': 'USD'},\n            'BABA': {'currency': 'CNY'},\n            'BMW.DE': {'currency': 'EUR'}\n        }\n        return metadata.get(self.ticker, {})\n\n    def currency(self):\n        \"\"\"\n        Retrieves and returns the currency information for a given ticker.\n        Checks if the currency attribute is already set; if so, returns this value.\n        If not, it fetches the metadata for the ticker's history, extracts the currency information,\n        assigns it to the currency attribute, and then returns.\n        \"\"\"\n        if self._currency is None:\n            metadata = self.fetch_metadata()\n            self._currency = metadata.get('currency', 'Unknown')  # Default to 'Unknown' if not found\n        return self._currency\n\n# Example Usage\nasset = Asset('MSFT')\nprint(asset.currency())  # Outputs 'USD'\nasset2 = Asset('BMW.DE')\nprint(asset2.currency())  # Outputs 'EUR'", "CodeBleu_score": 29.438280824292}, {"code": "class FinancialInstrument:\n    def __init__(self, ticker):\n        self.ticker = ticker\n        self._currency = None  # initially not set\n\n    def get_ticker_metadata(self, use_proxy=False):\n        # Placeholder for a method that retrieves metadata for a ticker.\n        # This typically would involve API calls which might use a proxy.\n        \n        # Mock data return:\n        metadata = {\n            'currency': 'USD',\n            # other hypothetical metadata fields...\n        }\n        return metadata\n\n    def currency(self):\n        \"\"\"Retrieves and returns the currency information for a given ticker.\n        \n        If the currency attribute is already set, it returns this value.\n        If not, it fetches the metadata for the ticker's history, potentially\n        using a proxy, extracts the currency information from the metadata,\n        assigns it to the currency attribute, and then returns the currency.\n        \"\"\"\n        # Check and return the _currency if it's already set\n        if self._currency is not None:\n            return self._currency\n        \n        # Fetch metadata (using an assumed method). Actual implementation may vary\n        metadata = self.get_ticker_metadata()  # Add use_proxy=True if proxy is needed\n        # Extract and set the currency from the metadata\n        self._currency = metadata.get('currency', 'Unknown Currency')\n        \n        # Return the currency\n        return self._currency\n\n# Example usage\ninstrument = FinancialInstrument('AAPL')\nprint(instrument.currency())  # Prints 'USD', from mock data provided.", "CodeBleu_score": 40.1504162931904}]}
{"instance_id": "16b80be60e3b", "generation_result": [{"code": "import pandas as pd\n\nclass YFDataException(Exception):\n    \"\"\"Custom exception for Yahoo Finance related data errors.\"\"\"\n    pass\n\nclass StockRecommendations:\n\n    def __init__(self):\n        self._recommendations = None\n\n    def recommendations(self) -> pd.DataFrame:\n        \"\"\"The method recommendations returns a DataFrame of stock recommendations.\n        If the attribute _recommendations is None, it fetches data from a specified proxy with the module 'recommendationTrend'.\n        If the fetch result is None, it sets _recommendations to an empty DataFrame.\n        Otherwise, it attempts to extract recommendation trend data from the fetched result and stores it in _recommendations as a DataFrame.\n        If there is an error in parsing the data, it raises a YFDataException with an appropriate error message.\n        The method then returns the _recommendations DataFrame.\"\"\"\n        \n        # Check if _recommendations is already set\n        if self._recommendations is None:\n            # Simulate data fetch from proxy module 'recommendationTrend'\n            try:\n                fetched_data = self.fetch_data_from_proxy()\n            \n            except Exception as e:\n                # Handle failed fetch scenario\n                raise YFDataException(f\"Failed to fetch data: {str(e)}\")\n            \n            # Check if fetched_data is None or empty\n            if fetched_data is None:\n                self._recommendations = pd.DataFrame()\n            else:\n                try:\n                    # Assuming fetched_data is a dict or similar format that can be turned to DataFrame\n                    self._recommendations = pd.DataFrame(fetched_data)\n                except Exception as e:\n                    # Handle any issues during DataFrame creation\n                    raise YFDataException(f\"Failed to parse fetched data into DataFrame: {str(e)}\")\n        \n        return self._recommendations\n        \n    def fetch_data_from_proxy(self):\n        \"\"\"Simulate a data fetching process from a recommendation trend API.\"\"\"\n        # Example implicit assumed data format. Replace with actual proxy fetching logic.\n        return [\n            {'Symbol': 'AAPL', 'Buy': 100, 'Hold': 50, 'Sell': 10},\n            {'Symbol': 'MSFT', 'Buy': 80, 'Hold': 60, 'Sell': 5}\n        ]\n\n\n# Example usage\ntry:\n    stock_rec = StockRecommendations()\n    print(stock_rec.recommendations())\nexcept YFDataException as ex:\n    print(ex)", "CodeBleu_score": 39.67479433865145}, {"code": "import pandas as pd\n\nclass StockAnalyzer:\n    def __init__(self):\n        self._recommendations = None\n\n    def recommendations(self) -> pd.DataFrame:\n        \"\"\"\n        The method recommendations returns a DataFrame of stock recommendations.\n        If the attribute _recommendations is None, it fetches data from a specified proxy with the module 'recommendationTrend'.\n        If the fetch result is None, it sets _recommendations to an empty DataFrame.\n        Otherwise, it attempts to extract recommendation trend data from the fetched result and stores it in _recommendations as a DataFrame.\n        If there is an error in parsing the data, it raises a YFDataException with an appropriate error message.\n        The method then returns the _recommendations DataFrame.\n        \"\"\"\n        if self._recommendations is None:\n            try:\n                # Let's assume we fetch data using a hypothetical 'fetch_data' function that interacts with an API\n                fetched_data = self.fetch_data(\"recommendationTrend\")\n                \n                if fetched_data is None:\n                    self._recommendations = pd.DataFrame()\n                else:\n                    # Hypothetical function to extract recommendation trend data from the response\n                    self._recommendations = self.parse_recommendation_data(fetched_data)\n                    \n            except Exception as e:\n                raise YFDataException(f\"Failed to parse recommendation data: {str(e)}\")\n\n        return self._recommendations\n\n    def fetch_data(self, module_name: str):\n        # This is a placeholder function. You will replace this with the actual API call.\n        # For the purpose of this example, we'll just simulate it returns some data.\n        \n        # Simulated Response\n        return {\n            \"data\": [\n                {\"symbol\": \"AAPL\", \"buy\": 10, \"hold\": 5, \"sell\": 1},\n                {\"symbol\": \"GOOG\", \"buy\": 8, \"hold\": 3, \"sell\": 2}\n            ]\n        }\n\n    def parse_recommendation_data(self, fetched_data):\n        # Extracting and parsing the data into a DataFrame\n        recommendations = [entry for entry in fetched_data.get(\"data\", [])]\n        return pd.DataFrame(recommendations)\n\n# An exception to handle specific error cases\nclass YFDataException(Exception):\n    pass", "CodeBleu_score": 42.91984199033055}, {"code": "import pandas as pd\n\nclass StockAnalysis:\n    def __init__(self):\n        self._recommendations = None\n\n    def recommendations(self) -> pd.DataFrame:\n        \"\"\"The method recommendations returns a DataFrame of stock recommendations.\n        If the attribute _recommendations is None, it fetches data from a specified proxy with the module 'recommendationTrend'.\n        If the fetch result is None, it sets _recommendations to an empty DataFrame.\n        Otherwise, it attempts to extract recommendation trend data from the fetched result and stores it in _recommendations as a DataFrame.\n        If there is an error in parsing the data, it raises a YFDataException with an appropriate error message.\n        The method then returns the _recommendations DataFrame.\"\"\"\n        if self._recommendations is None:\n            # Simulated fetch operation, replace with actual data fetching code\n            fetched_data = self.fetch_data()\n            \n            if fetched_data is None:\n                self._recommendations = pd.DataFrame()  # Set to an empty DataFrame if fetch result is None\n            else:\n                try:\n                    # Simulated operation to parse fetched data. Replace with actual implementation.\n                    self._recommendations = self.parse_fetched_data(fetched_data)\n                except Exception as e:\n                    # Raise custom exception on failure to parse data\n                    raise YFDataException(f\"Failed to parse recommendation data: {str(e)}\")\n\n        return self._recommendations\n\n    def fetch_data(self):\n        \"\"\"Simulate fetching recommendation trend data\"\"\"\n        # Example of fetched proxy data\n        return {\n          'data': [\n            {'date': '2023-01-01', 'recommendation': 'Buy'},\n            {'date': '2023-01-02', 'recommendation': 'Hold'},\n            {'date': '2023-01-03', 'recommendation': 'Sell'}\n          ]\n        }\n\n    def parse_fetched_data(self, fetched_data):\n        \"\"\"Parse the fetched recommendation data into a DataFrame\"\"\"\n        data = fetched_data['data']\n        df = pd.DataFrame(data)\n        return df\n\nclass YFDataException(Exception):\n    pass\n\n# Usage example:\nsa = StockAnalysis()\nprint(sa.recommendations())", "CodeBleu_score": 43.609798254166535}]}
{"instance_id": "abaf8c2a3ecc", "generation_result": [{"code": "import time\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.base import clone\n\ndef plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\"):\n    if not hasattr(clf, 'n_clusters'):\n        raise AttributeError(\"The classifier must have the 'n_clusters' attribute.\")\n    \n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)  # Default range 1 to 10 if none is provided\n    \n    if ax is None:\n        plt.figure(figsize=figsize)\n        ax = plt.gca()\n    \n    sse = []\n    times = []\n    \n    for n_cluster in cluster_ranges:\n        model = clone(clf)\n        model.set_params(n_clusters=n_cluster, n_jobs=n_jobs)\n        \n        start_time = time.time()\n        model.fit(X)\n        elapsed_time = time.time() - start_time\n        \n        labels = model.labels_\n        inertia = model.inertia_\n        \n        sse.append(inertia)\n        times.append(elapsed_time)\n    \n    ax.plot(cluster_ranges, sse, marker='o', color='b', label='SSE')\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of squared errors (SSE)', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    \n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, times, marker='s', color='r', label='Time')\n        ax2.set_ylabel('Time (seconds)', fontsize=text_fontsize)\n        ax2.grid(False)\n    \n    ax.legend(loc='best', fontsize=text_fontsize)\n    if show_cluster_time:\n        ax2.legend(loc='best', fontsize=text_fontsize, bbox_to_anchor=(1.1, 0.5))\n    \n    plt.grid(True)\n    return ax", "CodeBleu_score": 51.648885862597396}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n\ndef plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\"):\n    \"\"\"\n    The function plot_elbow_curve generates an elbow plot to help determine the\n    optimal number of clusters for a given clustering algorithm and dataset. It\n    takes the classifier, data, and various optional parameters including the title,\n    cluster range, number of parallel jobs, whether to show clustering time, axes,\n    figure size, and font sizes. The function checks if the classifier has the\n    attribute 'n_clusters', raises an error if not, and then scores the classifier\n    for each cluster number in the specified range. It plots the sum of squared errors\n    against the number of clusters and optionally plots the clustering duration on a\n    secondary y-axis. The function returns the axis object used for the plot.\n    \"\"\"\n    \n    # Verify classifier has the n_clusters attribute or method\n    if not hasattr(clf, 'n_clusters'):\n        raise ValueError(\"Classifier must have 'n_clusters' attribute.\")\n\n    # Set default cluster_ranges if none provided\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 11)  # Defaulting from 1 to 10 clusters\n    \n    sse = []  # Sum of squared errors\n    times = []  # List to store computation times for each k\n\n    # Creating figure and axis if not provided\n    if ax is None:\n        plt.figure(figsize=figsize)\n        ax = plt.gca()\n\n    for k in cluster_ranges:\n        start_time = time.time()  # Start timing\n        \n        # Set the number of clusters\n        clf.n_clusters = k\n        clf.fit(X)\n        \n        # Calculate sum of squared distances of samples to their closest cluster center\n        sse.append(clf.inertia_)\n        \n        # Elapsed time\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        times.append(elapsed_time)\n    \n    # Plot the Elbow\n    ax.plot(cluster_ranges, sse, 'bo-', label='Sum of squared errors')\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of squared errors (SSE)', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    \n    if show_cluster_time:\n        # Creating secondary y-axis to plot the times\n        ax_time = ax.twinx()\n        ax_time.plot(cluster_ranges, times, 'rs-', label='Clustering time')\n        ax_time.set_ylabel('Time (seconds)', fontsize=text_fontsize)\n        ax_time.grid(False)  # Turn off grid for the secondary axis\n    \n    ax.legend()\n    if show_cluster_time:\n        ax_time.legend(loc='upper right')\n    \n    return ax\n\n# Use this function with any clustering model that has attributes 'n_clusters' and 'inertia_'.", "CodeBleu_score": 41.864316668573544}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\n\ndef plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\"):\n    \"\"\"The function plot_elbow_curve generates an elbow plot to help determine the optimal number of clusters for a given clustering algorithm and dataset. It takes the classifier, data, and various optional parameters including the title, cluster range, number of parallel jobs, whether to show clustering time, axes, figure size, and font sizes. The function checks if the classifier has the attribute 'n_clusters', raises an error if not, and then scores the classifier for each cluster number in the specified range. It plots the sum of squared errors against the number of clusters and optionally plots the clustering duration on a secondary y-axis. The function returns the axis object used for the plot.\n    \"\"\"\n\n    if not hasattr(clf, 'n_clusters'):\n        raise AttributeError(\"Classifier must have 'n_clusters' attribute\")\n\n    if cluster_ranges is None:\n        cluster_ranges = range(2, 11)  # default range from 2 to 10 clusters\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    sse = []\n    times = []\n    \n    for n_clusters in cluster_ranges:\n        setattr(clf, 'n_clusters', n_clusters)\n        start_time = time.time()\n        clusters = clf.fit_predict(X)\n        current_time = time.time() - start_time\n        times.append(current_time)\n        \n        # Calculate inertia (Sum of Squared Errors)\n        centers = clf.cluster_centers_\n        sse.append(np.sum(np.min(cdist(X, centers, 'euclidean'), axis=1) ** 2))\n\n    # Plotting the SSE (elbow curve)\n    ax.plot(cluster_ranges, sse, marker='o', label='SSE')\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of squared errors (SSE)', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n\n    if show_cluster_time:\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, times, marker='x', color='r', label='Time')\n        ax2.set_ylabel('Clustering time (seconds)', fontsize=text_fontsize)\n    \n    ax.legend()\n    if show_cluster_time:\n        ax2.legend(loc='upper right')\n\n    return ax\n\n# Make sure necessary packages such as sklearn and numpy are properly installed and imported. Add import statements for 'cdist' and check the classifier, usually such clf would be like KMeans from sklearn. This code assumes classifier has n_clusters attribute.", "CodeBleu_score": 46.75160573471137}]}
{"instance_id": "8a90815c4ee1", "generation_result": [{"code": "def _clone_and_score_clusterer(clf, X, n_clusters)\nfrom sklearn.base import clone\nfrom sklearn.metrics import silhouette_score\nimport time\n\ndef _clone_and_score_clusterer(clf, X, n_clusters):\n    \"\"\"\n    The function _clone_and_score_clusterer takes a clusterer clf, a dataset X, and the number of clusters n_clusters as arguments.\n    It starts by recording the current time and then clones the given clusterer clf.\n    It sets the number of clusters for the cloned clusterer to n_clusters.\n    The function fits the cloned clusterer to the dataset X and returns the clustering score along with the time taken to perform the operation.\n    \n    Parameters:\n        clf (Clusterer): A scikit-learn compatible clustering algorithm instance which should have a fit method.\n        X (array-like or sparse matrix, shape (n_samples, n_features)): The input samples.\n        n_clusters (int): The number of clusters to form.\n    \n    Returns:\n        tuple (float, float): A tuple containing the silhouette score of the clustering and the time taken to compute it.\n    \"\"\"\n    \n    # Start the timer\n    start_time = time.time()\n    \n    # Clone the clusterer\n    cloned_clf = clone(clf)\n    \n    # Set the number of clusters (some clusterers like KMeans have n_clusters parameter directly)\n    if hasattr(cloned_clf, 'n_clusters'):\n        cloned_clf.n_clusters = n_clusters\n    \n    # Fit the cloned clusterer to the data\n    cloned_clf.fit(X)\n    \n    # Calculate clustering score using, for example, silhouette score\n    score = silhouette_score(X, cloned_clf.labels_)\n\n    # Calculate the time taken\n    time_taken = time.time() - start_time\n    \n    return score, time_taken", "CodeBleu_score": 44.78066155721678}, {"code": "from sklearn.base import clone\nfrom sklearn.metrics import silhouette_score\nimport time\n\ndef _clone_and_score_clusterer(clf, X, n_clusters):\n    \"\"\"\n    The function _clone_and_score_clusterer takes a clusterer clf, a dataset X, and the number of clusters n_clusters as arguments.\n    It starts by recording the current time and then clones the given clusterer clf.\n    It sets the number of clusters for the cloned clusterer to n_clusters.\n    The function fits the cloned clusterer to the dataset X and returns the clustering score along with the time taken to perform the operation.\n\n    Parameters:\n        clf (Clusterer): The initial clustering algorithm instance to be cloned.\n        X (np.array): Data set to cluster.\n        n_clusters (int): Number of clusters to set.\n\n    Returns:\n        tuple: A tuple containing the silhouette score of the clustering and the execution time taken.\n    \"\"\"\n    # Record start time\n    start_time = time.time()\n\n    # Clone the clusterer\n    cloned_clf = clone(clf)\n    \n    # Set the number of clusters\n    if hasattr(cloned_clf, 'n_clusters'):\n        cloned_clf.n_clusters = n_clusters\n    else:\n        raise AttributeError(\"The provided clusterer does not support setting 'n_clusters'\")\n        \n    # Fit the cloned clusterer to the data\n    cloned_clf.fit(X)\n    \n    # Calculate clustering score using silhouette score as an example\n    if hasattr(cloned_clf, 'labels_'):\n        labels = cloned_clf.labels_\n    else:\n        labels = cloned_clf.predict(X)\n    \n    score = silhouette_score(X, labels)\n\n    # Calculate elapsed time\n    elapsed_time = time.time() - start_time\n\n    return score, elapsed_time", "CodeBleu_score": 39.438577109084974}, {"code": "from sklearn.base import clone\nfrom sklearn.metrics import silhouette_score\nimport time\n\ndef _clone_and_score_clusterer(clf, X, n_clusters):\n    \"\"\"\n    The function _clone_and_score_clusterer takes a clusterer clf, a dataset X, and the number of clusters n_clusters as arguments.\n    It starts by recording the current time and then clones the given clusterer clf.\n    It sets the number of clusters for the cloned clusterer to n_clusters.\n    The function fits the cloned clusterer to the dataset X and returns the clustering score along with the time taken to perform the operation.\n    \"\"\"\n    \n    start_time = time.time()  # Start timing\n    \n    # Clone the clusterer\n    cloned_clf = clone(clf)\n    \n    # Set the number of clusters (assuming the clusterer uses this parameter name)\n    cloned_clf.set_params(n_clusters=n_clusters)\n    \n    # Fit the cloned clusterer to the data\n    cloned_clf.fit(X)\n    \n    # Calculate silhouette score to evaluate clustering, assuming this method of scoring is relevant.\n    # The silhouette score needs labels which we get from 'labels_' attribute of the fitted model.\n    score = silhouette_score(X, cloned_clf.labels_)\n    \n    # Calculate the time taken\n    elapsed_time = time.time() - start_time\n    \n    return score, elapsed_time", "CodeBleu_score": 49.2496358520068}]}
{"instance_id": "da40bd5dd81a", "generation_result": [{"code": "def clustering_factory(clf)\ndef clustering_factory(clf):\n    import warnings\n    import matplotlib.pyplot as plt\n    from sklearn.metrics import silhouette_samples, silhouette_score\n    from yellowbrick.cluster import KElbowVisualizer\n\n    # Check if the object has the required methods\n    required_methods = ['fit', 'fit_predict']\n    for method in required_methods:\n        if not hasattr(clf, method):\n            raise TypeError(f\"The clustering object must have a {method} method\")\n\n    # Define additional methods to be added to the clustering object\n    def plot_silhouette(X):\n        labels = clf.fit_predict(X)\n        silhouette_values = silhouette_samples(X, labels)\n\n        plt.figure(figsize=(10, 7))\n        y_lower = 10\n        for i in range(len(set(labels))):\n            ith_cluster_silhouette_values = silhouette_values[labels == i]\n            ith_cluster_silhouette_values.sort()\n            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n            y_upper = y_lower + size_cluster_i\n\n            plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values)\n            plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n            y_lower = y_upper + 10\n\n        plt.title(\"Silhouette plot for the various clusters.\")\n        plt.xlabel(\"Silhouette coefficient values\")\n        plt.ylabel(\"Cluster label\")\n\n        overall_avg_silhouette_score = silhouette_score(X, labels)\n        plt.axvline(x=overall_avg_silhouette_score, color=\"red\", linestyle=\"--\")\n        plt.yticks([])  # Clear the yaxis labels / ticks\n        plt.xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n        plt.show()\n\n    def plot_elbow_curve(X):\n        visualizer = KElbowVisualizer(clf, k=(2, 10))\n        visualizer.fit(X)\n        visualizer.show()\n\n    new_methods = {\n        'plot_silhouette': plot_silhouette,\n        'plot_elbow_curve': plot_elbow_curve\n    }\n\n    # Attach the methods to the clustering object\n    for method_name, method_func in new_methods.items():\n        if hasattr(clf, method_name):\n            warnings.warn(f\"The method {method_name} already exists on the clusterer object and will be overridden.\")\n        setattr(clf, method_name, method_func)\n\n    return clf\n\n# This function modifies clf to add capabilities to visualize via the silhouette and elbow plot methods, \n# by leveraging supplied methods and libraries for clustering utilities and visual evaluation.", "CodeBleu_score": 42.06405619268978}, {"code": "def clustering_factory(clf)\ndef clustering_factory(clf):\n    import warnings\n\n    # Check for necessary methods in clf\n    if not hasattr(clf, 'fit') or not hasattr(clf, 'fit_predict'):\n        raise TypeError(\"Provided object is not a valid clusterer instance: missing 'fit' and/or 'fit_predict' methods\")\n    \n    # Define additional methods\n    def plot_silhouette(X):\n        from sklearn.metrics import silhouette_score\n        from matplotlib import pyplot as plt\n        import numpy as np\n        \n        # Ensure the model has been fitted\n        if hasattr(clf, 'labels_'):\n            labels = clf.labels_\n        else:\n            labels = clf.fit_predict(X)\n        \n        silhouette_avg = silhouette_score(X, labels)\n        print(\"The average silhouette_score is :\", silhouette_avg)\n\n        # compute the silhouette scores for each sample\n        from sklearn.metrics import silhouette_samples\n        sample_silhouette_values = silhouette_samples(X, labels)\n        \n        # Create a plot\n        plt.figure()\n        y_lower = 10\n        n_clusters = len(np.unique(labels))\n        for i in range(n_clusters):\n            ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n            ith_cluster_silhouette_values.sort()\n            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n            y_upper = y_lower + size_cluster_i\n            \n            plt.fill_betweenx(np.arange(y_lower, y_upper),\n                              0, ith_cluster_silhouette_values)\n            y_lower = y_upper + 10  # 10 for the 0 samples\n\n        plt.title('The silhouette plot for the various clusters.')\n        plt.xlabel('The silhouette coefficient values')\n        plt.ylabel('Cluster label')\n        \n        plt.show()\n\n    def plot_elbow_curve(X):\n        from matplotlib import pyplot as plt\n\n        distortions = []\n        K = range(1, 10)\n        for k in K:\n            kmeanModel = clf.__class__(n_clusters=k)\n            kmeanModel.fit(X)\n            distortions.append(kmeanModel.inertia_)\n\n        plt.figure(figsize=(16, 8))\n        plt.plot(K, distortions, 'bx-')\n        plt.xlabel('k')\n        plt.ylabel('Distortion')\n        plt.title('The Elbow Method showing the optimal k')\n        plt.show()\n\n    # Attach additional methods to the clusterer object\n    additional_methods = {'plot_silhouette': plot_silhouette, 'plot_elbow_curve': plot_elbow_curve}\n\n    for method_name, method_func in additional_methods.items():\n        if hasattr(clf, method_name):\n            warnings.warn(f\"The method {method_name} is already present and will be overridden.\")\n        setattr(clf, method_name, method_func.__get__(clf, clf.__class__))\n\n    return clf", "CodeBleu_score": 30.560345410566303}, {"code": "def clustering_factory(clf):\n    # Required methods for valid clustering object\n    required_methods = ['fit', 'fit_predict']\n\n    # Check if the clustering object has the required methods\n    for method in required_methods:\n        if not hasattr(clf, method):\n            raise TypeError(f\"The object is not a valid clusterer instance because it lacks {method} method.\")\n\n    # Method definitions to be attached to the clusterer object\n    def plot_silhouette(X):\n        \"\"\"Plots the silhouette analysis for the given data.\"\"\"\n        from sklearn.metrics import silhouette_samples, silhouette_score\n        import matplotlib.pyplot as plt\n        import numpy as np\n\n        # Fit to data and predict the clusters\n        labels = clf.fit_predict(X)\n\n        # Silhouette score for each sample\n        silhouette_vals = silhouette_samples(X, labels)\n\n        # Compute the mean silhouette coefficient\n        score = silhouette_score(X, labels)\n        print(f\"Mean Silhouette Score: {score}\")\n\n        # Plot silhouette scores for each sample\n        y_lower, y_upper = 0, 0\n        yticks = []\n        for i, cluster in enumerate(np.unique(labels)):\n            cluster_silhouette_vals = silhouette_vals[labels == cluster]\n            cluster_silhouette_vals.sort()\n            y_upper += len(cluster_silhouette_vals)\n            plt.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n            yticks.append((y_lower + y_upper) / 2)\n            y_lower += len(cluster_silhouette_vals)\n\n        plt.yticks(yticks, np.unique(labels) + 1)\n        plt.ylabel('Cluster')\n        plt.xlabel('Silhouette coefficient')\n        plt.title('Silhouette plot for the various clusters')\n        plt.show()\n\n    def plot_elbow_curve(X, K_range):\n        \"\"\"Plots the elbow curve for determining the optimal number of clusters within a range K.\"\"\"\n        from sklearn.metrics import silhouette_score\n        import matplotlib.pyplot as plt\n\n        scores = []\n        for k in K_range:\n            temp_clf = type(clf)(n_clusters=k)\n            temp_clf.fit(X)\n            labels = temp_clf.predict(X)\n            score = silhouette_score(X, labels)\n            scores.append(score)\n\n        plt.figure(figsize=(8, 4))\n        plt.plot(K_range, scores, 'bo-')\n        plt.xlabel('Number of clusters')\n        plt.ylabel('Silhouette Score')\n        plt.title('Elbow Curve')\n        plt.show()\n\n    # Attaching methods to the clustering object\n    additional_methods = {'plot_silhouette': plot_silhouette, 'plot_elbow_curve': plot_elbow_curve}\n\n    for method_name, method_def in additional_methods.items():\n        if hasattr(clf, method_name):\n            import warnings\n            warnings.warn(f\"Overriding existing method of the class: {method_name}\", UserWarning)\n\n        setattr(clf, method_name, method_def.__get__(clf, clf.__class__))\n\n    # Return the modified clustering object\n    return clf", "CodeBleu_score": 39.789804837285935}]}
{"instance_id": "b8d8ba3c0b1b", "generation_result": [{"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\"):\n    \"\"\"\n    The function plot_learning_curve generates a learning curve plot for a given classifier clf using the provided training data X and labels y.\n    It optionally accepts parameters for the plot title, cross-validation strategy, training sizes, number of jobs, scoring method, axes object, figure size, and font sizes for the title and text.\n    If no axes object is provided, it creates one.\n    The function sets the plot title and labels for the x and y axes.\n    It calculates the training and cross-validation scores using the learning_curve function and computes their means and standard deviations.\n    The function then plots the learning curves, including shaded areas representing the standard deviation, and adds a legend.\n    Finally, it returns the axes object with the plot.\n    \"\"\"\n    \n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    \n    if train_sizes is None:\n        train_sizes = np.linspace(0.1, 1.0, 10)\n\n    train_sizes, train_scores, validation_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,\n        scoring=scoring)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    validation_scores_mean = np.mean(validation_scores, axis=1)\n    validation_scores_std = np.std(validation_scores, axis=1)\n\n    ax.grid()\n    \n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    ax.fill_between(train_sizes, validation_scores_mean - validation_scores_std,\n                    validation_scores_mean + validation_scores_std, alpha=0.1,\n                    color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    ax.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n\n    return ax\n", "CodeBleu_score": 69.13400153471045}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=np.linspace(0.1, 1.0, 5), n_jobs=1, scoring=None,\n                        ax=None, figsize=(10, 6), title_fontsize=\"large\",\n                        text_fontsize=\"medium\"):\n    \"\"\"\n    The function plot_learning_curve generates a learning curve plot for a given classifier clf using the provided training data X and labels y.\n    It optionally accepts parameters for the plot title, cross-validation strategy, training sizes, number of jobs, scoring method, axes object, figure size, and font sizes for the title and text.\n    If no axes object is provided, it creates one.\n    The function sets the plot title and labels for the x and y axes.\n    It calculates the training and cross-validation scores using the learning_curve function and computes their means and standard deviations.\n    The function then plots the learning curves, including shaded areas representing the standard deviation, and adds a legend.\n    Finally, it returns the axes object with the plot.\n    \"\"\"\n    \n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    \n    # Compute the learning curve data\n    train_sizes, train_scores, validation_scores = learning_curve(\n        clf, X, y, train_sizes=train_sizes, cv=cv, scoring=scoring, n_jobs=n_jobs)\n    \n    # Calculate mean and standard deviation of training and validation scores\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    validation_scores_mean = np.mean(validation_scores, axis=1)\n    validation_scores_std = np.std(validation_scores, axis=1)\n\n    # Plot the learning curve\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    ax.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    \n    # Create bands for the standard deviation\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    ax.fill_between(train_sizes, validation_scores_mean - validation_scores_std,\n                    validation_scores_mean + validation_scores_std, alpha=0.1, color=\"g\")\n    \n    # Setting labels, title and legend\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Training set size', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n    \n    return ax\n\n# Note: Before using this function make sure the classifier and data are correctly set up \ube44\uad50 Private or Public ContextX clr.", "CodeBleu_score": 61.18589605081106}, {"code": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\"):\n    \"\"\"\n    The function plot_learning_curve generates a learning curve plot for a given classifier clf using the provided training data X and labels y.\n    It optionally accepts parameters for the plot title, cross-validation strategy, training sizes, number of jobs, scoring method, axes object, figure size, and font sizes for the title and text.\n    If no axes object is provided, it creates one.\n    The function sets the plot title and labels for the x and y axes.\n    It calculates the training and cross-validation scores using the learning_curve function and computes their means and standard deviations.\n    The function then plots the learning curves, including shaded areas representing the standard deviation, and adds a legend.\n    Finally, it returns the axes object with the plot.\n    \"\"\"\n    if train_sizes is None:\n        train_sizes = np.linspace(0.1, 1.0, 5)\n\n    # Determine the training and test scores for different training set sizes\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n\n    # Calculate the means and standard deviations for plotting\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    # Plotting\n    if ax is None:\n        plt.figure(figsize=figsize)\n        ax = plt.gca()\n\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n\n    # Titles and labels\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Training examples', fontsize=text_fontsize)\n    ax.set_ylabel('Score', fontsize=text_fontsize)\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return ax\n\n# Example usage:\n# from sklearn.datasets import load_iris\n# from sklearn.linear_model import LogisticRegression\n# X, y = load_iris(return_X_y=True)\n# clf = LogisticRegression()\n# plot_learning_curve(clf, X, y, cv=3, n_jobs=-1, figsize=(10, 6), title=\"Iris Dataset Learning Curve\")", "CodeBleu_score": 70.47978957509034}]}
{"instance_id": "e6e76eb35945", "generation_result": [{"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\"):\n    \"\"\"\n    The function plot_elbow_curve generates an elbow plot to help determine the optimal number of clusters for a clustering algorithm.\n    It accepts a classifier clf, data X, and optional parameters for the plot's appearance.\n    If cluster_ranges is not provided, it defaults to a range from 1 to 11 with a step of 2.\n    The function checks if the classifier has an n_clusters attribute and raises a TypeError if it does not.\n    It then creates a list of classifiers with different cluster numbers, fits them to the data, and scores them.\n    If no axes object ax is provided, it creates a new one.\n    The function plots the number of clusters against the absolute value of the scores, adds grid lines, labels, and sets the title and font sizes.\n    It returns the axes object with the plot.\n    \"\"\"\n\n    # Checking if the classifier has the attribute 'n_clusters'\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier must have an 'n_clusters' attribute.\")\n\n    # Handling default range of clusters if not provided\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n\n    # Preparing the plot\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    scores = []\n    for k in cluster_ranges:\n        # Setting number of clusters for the new classifier\n        clf.n_clusters = k\n        # Fitting the classifier to the data\n        clf.fit(X)\n        # Scoring the model (using inertia)\n        scores.append(-clf.score(X))  # Using negative score to make it lower-better (e.g., inertia)\n\n    # Plotting the elbow curve\n    ax.plot(cluster_ranges, scores, marker='o')\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score (inertia)', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.grid(True)\n\n    return ax\n\n# Example use which would be outside of this function (user should make sure clf has n_clusters and score method):\n# from sklearn.cluster import KMeans\n# X = np.random.rand(100, 2)  # Creating some random data\n# clf = KMeans()  # Creating a KMeans clustering classifier\n# ax = plot_elbow_curve(clf, X)\n# plt.show()  # This line should be executed in the code where function is called", "CodeBleu_score": 49.07721966879955}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\nimport matplotlib.pyplot as plt\n\ndef plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=(8, 5), title_fontsize=\"large\",\n                     text_fontsize=\"medium\"):\n    \"\"\"The function plot_elbow_curve generates an elbow plot to help determine the optimal number of clusters for a clustering algorithm.\n    It accepts a classifier clf, data X, and optional parameters for the plot's appearance.\n    If cluster_ranges is not provided, it defaults to a range from 1 to 11 with a step of 2.\n    The function checks if the classifier has an n_clusters attribute and raises a TypeError if it does not.\n    It then creates a list of classifiers with different cluster numbers, fits them to the data, and scores them.\n    If no axes object ax is provided, it creates a new one.\n    The function plots the number of clusters against the absolute value of the scores, adds grid lines, labels, and sets the title and font sizes.\n    It returns the axes object with the plot.\"\"\"\n\n    # Check if the classifier has the 'n_clusters' attribute\n    if not hasattr(clf, 'n_clusters'):\n        raise TypeError(\"The classifier does not have the 'n_clusters' attribute.\")\n\n    # Set default cluster range if none is provided\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n    \n    # Compute the scores for a range of cluster counts\n    scores = []\n    for n_clusters in cluster_ranges:\n        temp_clf = clf.__class__(n_clusters=n_clusters)  # Create a new instance with a different number of clusters\n        temp_clf.fit(X)\n        if hasattr(temp_clf, 'inertia_'):  # Use inertia if possible which is available for KMeans\n            scores.append(temp_clf.inertia_)\n        else:\n            labels = temp_clf.predict(X)\n            scores.append(-temp_clf.score(X, labels))  # Use negative score for other clustering algorithms without inertia_\n\n    # Prepare the plot\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    ax.plot(cluster_ranges, scores, marker='o', linestyle='-', color='b')\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Score (inertia)', fontsize=text_fontsize)\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.grid(True)\n\n    plt.xticks(cluster_ranges, fontsize=text_fontsize)\n    plt.yticks(fontsize=text_fontsize)\n\n    # Return the axes object containing the plot\n    return ax", "CodeBleu_score": 47.29443284089119}, {"code": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\ndef plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\r\n                     ax=None, figsize=None, title_fontsize=\"large\",\r\n                     text_fontsize=\"medium\"):\r\n    \"\"\"\r\n    The function plot_elbow_curve generates an elbow plot to help determine the optimal number of clusters for a clustering algorithm.\r\n    It accepts a classifier clf, data X, and optional parameters for the plot's appearance.\r\n    If cluster_ranges is not provided, it defaults to a range from 1 to 11 with a step of 2.\r\n    The function checks if the classifier has an n_clusters attribute and raises a TypeError if it does not.\r\n    It then creates a list of classifiers with different cluster numbers, fits them to the data, and scores them.\r\n    If no axes object ax is provided, it creates a new one.\r\n    The function plots the number of clusters against the absolute value of the scores, adds grid lines, labels, and sets the title and font sizes.\r\n    It returns the axes object with the plot.\r\n    \"\"\"\r\n    # Check if the classifier has the necessary attribute\r\n    if not hasattr(clf, 'n_clusters'):\r\n        raise TypeError(\"The classifier must have an 'n_clusters' attribute.\")\r\n    \r\n    # Default range for number of clusters\r\n    if cluster_ranges is None:\r\n        cluster_ranges = range(1, 12, 2)  # Default from 1 to 11 with a step of 2\r\n    \r\n    # Prepare the figure and axis\r\n    if ax is None:\r\n        if figsize is None:\r\n            figsize = (8, 6)\r\n        fig, ax = plt.subplots(figsize=figsize)\r\n\r\n    # Initialize variables to store the results\r\n    scores = []\r\n    \r\n    # Create and fit the classifiers for different values of n_clusters\r\n    for k in cluster_ranges:\r\n        # Create a clone of the classifier with the new number of clusters if possible\r\n        cloned_clf = clf.__class__(n_clusters=k, **{attr: getattr(clf, attr)\r\n                        for attr in clf.get_params() if attr != 'n_clusters'})\r\n        # Fit the classifier\r\n        cloned_clf.fit(X)\r\n        # Compute the score and store it\r\n        # Usually inertia or negative of SSE (Sum of squared errors) is used as a score\r\n        if hasattr(cloned_clf, 'inertia_'):\r\n            scores.append(cloned_clf.inertia_)\r\n        else:\r\n            scores.append(cloned_clf.score(X))\r\n    \r\n    # Plotting the elbow curve\r\n    ax.plot(cluster_ranges, np.abs(scores), marker='o')  # We plot the absolute value of the scores\r\n    \r\n    # Setting plot properties\r\n    ax.set_title(title, fontsize=title_fontsize)\r\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\r\n    ax.set_ylabel('Score (Inertia)', fontsize=text_fontsize)\r\n    ax.grid(True)\r\n    \r\n    return ax", "CodeBleu_score": 45.346338368431944}]}
{"instance_id": "6f4530c8a20f", "generation_result": [{"code": "import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\ndef plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=(8, 6), cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\"):\n    \"\"\"The function plot_pca_2d_projection creates a 2D scatter plot of the PCA-transformed data for visualizing\n    the first two principal components. It takes a classifier, data, labels, and several optional parameters including\n    the title, axis, figure size, colormap, and font sizes. The function applies PCA transformation to the data, sets\n    up the plot, and colors the points according to their labels. It labels the axes with the first and second principal components,\n    adds a legend, and returns the axis object used for the plot.\n\n    Parameters:\n    clf : classifier model, pre-fitted or to be fitted with X and y\n    X : array-like, shape [n_samples, n_features]\n        The input data.\n    y : array-like, shape [n_samples]\n        The target labels.\n    title : str, optional\n        The title of the graph.\n    ax : matplotlib Axes, optional\n        The target axes object to draw the plot on. If None, a new figure and axes will be created.\n    figsize : tuple, optional\n        The size of the figure to create (width, height) in inches.\n    cmap : str or matplotlib Colormap, optional\n        The colormap for the categories.\n    title_fontsize : int or str, optional\n        The fontsize of the title.\n    text_fontsize : int or str, optional\n        The fontsize of the text in the plot.\n\n    Returns:\n    matplotlib Axes\n        The axes object with the plot.\n    \"\"\"\n    \n    # If a fitted model is not provided, assume clf can be fitted here\n    if not hasattr(clf, 'fit'):\n        raise AttributeError('clf must be a fitted model or should have a fit method.')\n\n    if not hasattr(clf, 'predict'):\n        clf.fit(X, y)\n\n    # Check if an axis object is provided, if not create a new one\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    # Apply PCA transformation to reduce dimensions to 2D\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    \n    # Scatter plot\n    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap, edgecolor='k', alpha=0.7)\n    \n    # Adding a colorbar\n    cbar = plt.colorbar(scatter, ax=ax)\n    cbar.set_label('Classes', fontsize=text_fontsize)\n    \n    # Setting title and labels\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Principal Component 1', fontsize=text_fontsize)\n    ax.set_ylabel('Principal Component 2', fontsize=text_fontsize)\n\n    # Adding grid for better visibility\n    ax.grid(True)\n\n    # Return the axes object\n    return ax", "CodeBleu_score": 47.244949991458626}, {"code": "import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\ndef plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=(8, 6), cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\"):\n    \"\"\"\n    The function plot_pca_2d_projection creates a 2D scatter plot of the PCA-transformed \n    data for visualizing the first two principal components. It takes a classifier, \n    data, labels, and several optional parameters including the title, axis, figure size,\n    colormap, and font sizes. The function applies PCA transformation to the data, sets \n    up the plot, and colors the points according to their labels. It labels the axes with\n    the first and second principal components, adds a legend, and returns the axis object \n    used for the plot.\n    \"\"\"\n\n    # Check if an axis object is provided, otherwise create one\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    \n    # Standardize the data\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Perform PCA\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X_scaled)\n    \n    # Scatter plot of the PCA components\n    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cmap, alpha=0.6, edgecolors='w', linewidths=0.5)\n\n    # Set the title and labels\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('Principal Component 1', fontsize=text_fontsize)\n    ax.set_ylabel('Principal Component 2', fontsize=text_fontsize)\n    \n    # Color bar (legend for labels)\n    cbar = plt.colorbar(scatter, ax=ax)\n    cbar.set_label('Class', fontsize=text_fontsize)\n    \n    # Optionally, plotting decision boundaries if a classifier is provided\n    if clf is not None:\n        # Create a grid to evaluate classifier\n        x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n        y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n                             np.linspace(y_min, y_max, 500))\n        \n        # Transform grid using PCA inverse transform\n        grid = np.c_[xx.ravel(), yy.ravel()]\n        grid_original = pca.inverse_transform(grid)\n        \n        # Predict classes\n        Z = clf.predict(grid_original)\n        Z = Z.reshape(xx.shape)\n        \n        # Plot decision boundary\n        ax.contourf(xx, yy, Z, alpha=0.3, cmap=cmap)\n    \n    return ax", "CodeBleu_score": 41.43351520083803}, {"code": "import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\ndef plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=(8, 6), cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\"):\n    \"\"\"\n    The function plot_pca_2d_projection creates a 2D scatter plot of the PCA-transformed data for visualizing\n    the first two principal components. It takes a classifier, data, labels, and several optional parameters including\n    the title, axis, figure size, colormap, and font sizes. The function applies PCA transformation to the data,\n    sets up the plot, and colors the points according to their labels. It labels the axes with the first and second\n    principal components, adds a legend, and returns the axis object used for the plot.\n\n    Parameters:\n        clf: Classifier (not actually used for PCA transformation, might be used for other purposes)\n        X (array-like): The data to be transformed and plotted.\n        y (array-like): The labels for the data points.\n        title (str, optional): The title of the plot.\n        ax (matplotlib.axes._subplots.AxesSubplot, optional): The matplotlib axis on which to draw the plot.\n        figsize (tuple, optional): The size of the figure.\n        cmap (str, optional): The colormap used to color data points.\n        title_fontsize (str or int, optional): The fontsize for the title.\n        text_fontsize (str or int, optional): Fontsize for the text elements.\n\n    Returns:\n        matplotlib.axes._subplots.AxesSubplot: The axis object with the plot.\n    \"\"\"\n\n    # First, we need to apply PCA to reduce the data to two dimensions\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    # Prepare the plot\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    # Map each class label to a color using the colormap\n    classes = np.unique(y)\n    colors = plt.cm.get_cmap(cmap, len(classes))\n\n    # Scatter plot for each class\n    for i, class_name in enumerate(classes):\n        ax.scatter(X_pca[y == class_name, 0], X_pca[y == class_name, 1], color=colors(i),\n                   label=f'Class {class_name}', alpha=0.5, edgecolors='w', linewidth=0.5)\n\n    # Styling the plot\n    ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel('First Principal Component', fontsize=text_fontsize)\n    ax.set_ylabel('Second Principal Component', fontsize=text_fontsize)\n    ax.legend(fontsize=text_fontsize)\n    ax.grid(True)\n\n    return ax", "CodeBleu_score": 55.76923000032532}]}
{"instance_id": "e7c2775d024c", "generation_result": [{"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\ndef clip_path(ax, lower_half=True):\n    \"\"\"Creates a clipping path that is either the lower or upper half of the axis.\"\"\"\n    ymin, ymax = ax.get_ylim()\n    xmin, xmax = ax.get_xlim()\n    if lower_half:\n        rect = Rectangle((xmin, ymin), xmax - xmin, (ymax - ymin) / 2)\n    else:\n        rect = Rectangle((xmin, (ymax + ymin) / 2), xmax - xmin, (ymax - ymin) / 2)\n    clip_path = ax.add_patch(rect)\n    clip_path.set_clip_on(False)\n    return clip_path\n\ndef barplot(ax):\n    \"\"\"The function barplot takes a matplotlib axis object ax as an argument.\n    It first generates two clipping paths using the clip_path function.\n    The function then creates data for 16 bars using NumPy arrays, with values for Y1 and Y2 generated randomly within a specified range.\n    It plots four sets of bars on the provided axis: two sets with different shades of gray, and two sets with shades of red, \n    each clipped to one of the clipping paths.\n    The x-axis limits are set from -1 to 16.\n    The function also adds two text labels below the plot, one labeled 'Bar plot' in black and another labeled 'ax.bar()' \n    in blue monospace font.\"\"\"\n    \n    # Parameters\n    n_bars = 16\n    x = np.arange(n_bars)\n    \n    # Data\n    y1 = np.random.randint(1, 100, n_bars)\n    y2 = np.random.randint(1, 100, n_bars)\n    \n    # Create clipping paths\n    clip_path1 = clip_path(ax, lower_half=True)\n    clip_path2 = clip_path(ax, lower_half=False)\n    \n    # Bar plot with clipping\n    ax.bar(x-0.2, y1, width=0.4, color='gray', label='Y1', clip_path=clip_path1)\n    ax.bar(x+0.2, y2, width=0.4, color='darkgray', label='Y2', clip_path=clip_path2)\n    ax.bar(x-0.2, y1, width=0.4, color='red', label='Y1 red', clip_path=clip_path2)\n    ax.bar(x+0.2, y2, width=0.4, color='darkred', label='Y2 red', clip_path=clip_path1)\n    \n    # Setting axis properties\n    ax.set_xlim(-1, 16)\n    \n    # Adding text\n    ax.text(0, -10, 'Bar plot', color='black', fontsize=12)\n    ax.text(0, -15, 'ax.bar()', color='blue', fontsize=12, fontfamily='monospace')\n    \n    # Show legend\n    ax.legend()\n\n# Create figure and axis\nfig, ax = plt.subplots()\nbarplot(ax)\nplt.show()", "CodeBleu_score": 33.48839410101445}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef barplot(ax):\n    \"\"\"\n    The function barplot takes a matplotlib axis object ax as an argument.\n    It first generates two \"clipping\" paths: regions where data might be represented differently.\n    The function then creates data for 16 bars using NumPy arrays, with values for Y1 and Y2 generated randomly within a specified range.\n    It plots four sets of bars on the provided axis: two sets with different shades of gray, and two sets with shades of red, \n    each \"clipped\" by not extending bars fully (simulated clipping).\n    The x-axis limits are set from -1 to 16.\n    The function also adds two text labels below the plot, one labeled 'Bar plot' in black and another labeled 'ax.bar()' in blue monospace font.\n    \"\"\"\n    # Generating bars data\n    n_bars = 16\n    x = np.arange(n_bars)\n    y1 = np.random.rand(n_bars) * 100\n    y2 = np.random.rand(n_bars) * 100\n\n    # Simulate \"clipping paths\" by limiting height of certain bars\n    clipping_threshold = 50\n    y1_clipped = np.clip(y1, 0, clipping_threshold)\n    y2_clipped = np.clip(y2, 0, clipping_threshold)\n\n    # Plotting bars\n    ax.bar(x-0.2, y1, width=0.4, color='gray', align='center', label='Y1 Full') # Full bars\n    ax.bar(x+0.2, y2, width=0.4, color='lightgray', align='center', label='Y2 Full')\n\n    ax.bar(x-0.2, y1_clipped, width=0.4, color='red', align='center', alpha=0.7, label='Y1 Clipped')\n    ax.bar(x+0.2, y2_clipped, width=0.4, color='darkred', align='center', alpha=0.7, label='Y2 Clipped')\n\n    # Set x-axis limits\n    ax.set_xlim(-1, n_bars)\n\n    # Adding labels/text\n    ax.set_title('Bar plot with Simulated Clipping')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n\n    # Lower textual labels\n    ax.text(-1, -10, 'Bar plot', fontsize=12, color='black')\n    ax.text(n_bars-5, -10, 'ax.bar()', fontsize=12, color='blue', family='monospace')\n\n# Use the function in a matplotlib figure\nfig, ax = plt.subplots()\nbarplot(ax)\nplt.legend()\nplt.show()", "CodeBleu_score": 39.49809277506674}, {"code": "def barplot(ax)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef clip_path(ax, x_range, y_range, rotation=0):\n    \"\"\"Utility function to create a clipping path in the shape of a rectangle.\"\"\"\n    return patches.Rectangle((x_range[0], y_range[0]), np.diff(x_range)[0], np.diff(y_range)[0], angle=rotation, transform=ax.transData)\n\ndef barplot(ax):\n    \"\"\"\n    The function barplot takes a matplotlib axis object ax as an argument.\n    It first generates two clipping paths using the clip_path function.\n    The function then creates data for 16 bars using NumPy arrays, with values for Y1 and Y2 generated randomly within a specified range.\n    It plots four sets of bars on the provided axis: two sets with different shades of gray, and two sets with shades of red, each clipped to one of the clipping paths.\n    The x-axis limits are set from -1 to 16.\n    The function also adds two text labels below the plot, one labeled 'Bar plot' in black and another labeled 'ax.bar()' in blue monospace font.\n    \"\"\"\n\n    # Create random data\n    x = np.arange(16)\n    y1 = np.random.rand(16) * 100  # Random values between 0 and 100\n    y2 = np.random.rand(16) * 100  # Random values between 0 and 100\n\n    # Create two clipping paths\n    clip1 = clip_path(ax, (-1, 8), (0, 50))  # Clipping to the first half and lower values\n    clip2 = clip_path(ax, (7, 16), (50, 100)) # Clipping to the second half and higher values\n\n    # Adding clipping paths to axis\n    ax.add_patch(clip1)\n    ax.add_patch(clip2)\n\n    # Plot bars\n    ax.bar(x-0.2, y1, width=0.4, color='gray', label='Gray bars', clip_path=clip1)\n    ax.bar(x+0.2, y2, width=0.4, color='lightgray', label='Light gray bars', clip_path=clip1)\n    ax.bar(x-0.2, y1, width=0.4, color='red', label='Red bars', clip_path=clip2)\n    ax.bar(x+0.2, y2, width=0.4, color='lightcoral', label='Light red bars', clip_path=clip2)\n\n    # Set x-axis limits\n    ax.set_xlim(-1, 16)\n\n    # Add text labels\n    ax.text(0, -10, 'Bar plot', color='black', fontsize=12)\n    ax.text(0, -15, 'ax.bar()', color='blue', fontsize=12, fontfamily='monospace')\n\n    # Show legend\n    ax.legend()\n\n    plt.show()\n\n# Example of using the function\nfig, ax = plt.subplots()\nbarplot(ax)", "CodeBleu_score": 34.07933746050019}]}
{"instance_id": "0ec1d146a32b", "generation_result": [{"code": "import numpy as np\n\ndef split(n_segment, total_width=100):\n    \"\"\"The function split calculates the starting and ending positions of segments within a fixed total width.\n    \n    Args:\n    n_segment (int): The number of segments to divide the width into.\n    total_width (int, optional): The total width to be segmented. Defaults to 100.\n    \n    Returns:\n    tuple: (start_positions, end_positions). Each is an array of positions for segments.\n    \n    Example:\n    split(4, 100)\n    Output:\n    (array([ 0., 25., 50., 75.]), array([ 25., 50., 75., 100.]))\n    \"\"\"\n\n    if n_segment <= 0:\n        raise ValueError(\"Number of segments must be a positive integer.\")\n\n    # Each segment width assuming uniform distribution\n    segment_width = total_width / n_segment\n\n    # Calculate the start (X0) positions of each segment\n    start_positions = np.array([i * segment_width for i in range(n_segment)])\n    # Calculate the end (X1) positions of each segment\n    end_positions = start_positions + segment_width\n\n    return start_positions, end_positions", "CodeBleu_score": 27.403754165180217}, {"code": "import numpy as np\n\ndef split(n_segment):\n    \"\"\"\n    The function split calculates the starting and ending positions of segments within a fixed width.\n    It takes the number of segments as input and computes the width of each segment.\n    The function then determines the starting (X0) and ending (X1) positions of each segment and\n    returns them as two separate arrays.\n    \n    Args:\n    n_segment (int): Number of segments.\n    \n    Returns:\n    tuple of np.ndarray: Two numpy arrays, first containing the starting positions (X0) and the second containing the ending positions (X1) of each segment.\n    \"\"\"\n    if n_segment <= 0:\n        raise ValueError(\"The number of segments must be a positive integer\")\n\n    # Compute the width of each segment\n    segment_width = 1.0 / n_segment\n\n    # Initialize arrays for the starting and ending positions\n    X0 = np.zeros(n_segment)\n    X1 = np.zeros(n_segment)\n    \n    for i in range(n_segment):\n        X0[i] = i * segment_width\n        X1[i] = (i + 1) * segment_width\n    \n    return (X0, X1)", "CodeBleu_score": 26.478067785200786}, {"code": "def split(n_segment, total_width=100):\n    \"\"\"\n    Calculate the starting (X0) and ending (X1) positions of segments within a fixed width.\n    \n    Parameters:\n    n_segment (int): Number of segments to split the space into.\n    total_width (int): The total fixed width to be split (default is 100 units).\n    \n    Returns:\n    tuple: Two arrays, first array containing the starting positions (X0) \n           of each segment and the second array containing ending positions (X1).\n    \"\"\"\n    if n_segment <= 0:\n        raise ValueError(\"Number of segments must be positive and greater than zero.\")\n    \n    segment_width = total_width / n_segment\n    X0 = []\n    X1 = []\n    \n    for i in range(n_segment):\n        start = i * segment_width\n        end = start + segment_width\n        X0.append(start)\n        X1.append(end)\n    \n    return (X0, X1)\n\n# Example usage:\nn_segments = 4\nstart_positions, end_positions = split(n_segments)\nprint(\"Segment Start Positions:\", start_positions)\nprint(\"Segment End Positions:\", end_positions)", "CodeBleu_score": 26.837414777781127}]}
{"instance_id": "604a3e93ac0d", "generation_result": [{"code": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n\ndef pixelated_text(dpi=100):\n    \"\"\"\n    Generates an image of the character 'a' with specified dpi.\n    \"\"\"\n    # Create a figure with specified dpi\n    fig = plt.figure(figsize=(1, 1), dpi=dpi)\n    canvas = FigureCanvas(fig)\n    ax = fig.add_subplot(111)\n\n    # Draw 'a' at center of canvas\n    ax.text(0.5, 0.5, 'a', fontsize=75, ha='center', va='center')\n    ax.axis('off')  # Hide axes\n\n    # Draw canvas and convert to array\n    canvas.draw()\n    s, (width, height) = canvas.print_to_buffer()\n\n    # Convert buffer to numpy array\n    img = np.frombuffer(s, np.uint8).reshape((height, width, 4))\n\n    # Since the buffer gives ARGB, convert to RGBA if necessary\n    # The following line shifts the channels using np.roll if your array setup requires it\n    img = np.roll(img, -1, axis=-1)  # Shift by -1 to convert ARGB to RGBA\n\n    # Crop the image as per dpi needs if needed, optional (not specifically requested)\n    # img = img[...,:3]  # Optionally convert to RGB only, if needed\n\n    return img\n\n# Example usage\nresulting_image = pixelated_text(dpi=100)\nprint(resulting_image.shape)", "CodeBleu_score": 44.8235582359917}, {"code": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef pixelated_text(dpi=100):\n    \"\"\"\n    Generates an image of a single character 'a' with a specified resolution given by dpi.\n    It creates a figure and a canvas using Matplotlib, places the character 'a' at the center\n    of the figure with a font size of 75, and removes the axis.\n    The canvas is drawn and its content is converted to a NumPy array representing the image in\n    ARGB format. The function reshapes this array to the dimensions specified by dpi and adjusts\n    the color channels by rolling the array along the last axis. Finally, it returns the\n    resulting image array.\n    \"\"\"\n    # Create a figure and an axis to handle text rendering\n    fig, ax = plt.subplots(figsize=(2, 2), dpi=dpi)\n    ax.text(0.5, 0.5, 'a', fontsize=75, ha='center', va='center', fontfamily='serif')\n    ax.axis('off')  # Hide axes\n    \n    # Render the figure canvas\n    fig.canvas.draw()\n    \n    # Extract the image from the matplotlib figure canvas\n    data = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n    \n    # Matplotlib ARGB to standard RGBA\n    data = np.roll(data, -1, axis=-1)\n    \n    # Adjust the output image if necessary\n    reshaped_data = data\n    \n    # Clean up resources\n    plt.close(fig)\n    \n    return reshaped_data\n\n# Example usage\nimage_array = pixelated_text(100)\nprint(image_array.shape) # This should show (200, 200, 4) for a 2x2 inch figure at 100 dpi", "CodeBleu_score": 41.92445570093021}, {"code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n\ndef pixelated_text(dpi=100):\n    \"\"\"\n    The function pixelated_text generates an image of a single character 'a' with a specified resolution given by dpi.\n    It creates a figure and a canvas using Matplotlib, places the character 'a' at the center of the figure with a font\n    size of 75, and removes the axis.\n    The canvas is drawn and its content is converted to a NumPy array representing the image in ARGB format.\n    The function reshapes this array to the dimensions specified by dpi and adjusts the color channels by rolling the\n    array along the last axis.\n    Finally, it returns the resulting image array.\n    \"\"\"\n    # Set the font size and character to draw\n    font_size = 75\n    character = 'a'\n    \n    # Create a figure suited to dpi and use Agg to draw on it\n    fig = plt.figure(figsize=(2, 2), dpi=dpi)\n    canvas = FigureCanvas(fig)\n    ax = fig.add_subplot(111)\n    \n    # Draw the text at the center of fig\n    ax.text(0.5, 0.5, character, fontsize=font_size, ha='center', va='center', family='sans-serif')\n    \n    # Remove axes\n    ax.axis('off')\n\n    # Draw canvas and convert to a numpy array\n    canvas.draw()\n    img_array = np.frombuffer(canvas.tostring_argb(), dtype='uint8')\n    \n    # The default format is ARGB, reshape and remove extra dimension caused by figsize scaling\n    width, height = fig.canvas.get_width_height()\n    img_array = img_array.reshape(height, width, 4)\n    \n    # Since `tostring_argb` gives us ARGB format, we roll the array along last axis to get RGB (if needed)\n    # img_array = np.roll(img_array, shift=-1, axis=2)  # Uncomment this line if RGB format is required\n    \n    # Close figure to free resources\n    plt.close(fig)\n    \n    return img_array\n\n# Example usage\nimage = pixelated_text(dpi=100)\nprint(image.shape)  # Should show the shape of the ndarray, expected (200, 200, 4) due to 2x2 inch figure at 100 DPI", "CodeBleu_score": 45.01634488168121}]}
{"instance_id": "2a7fc7d560ea", "generation_result": [{"code": "import numpy as np\n\ndef curve():\n    \"\"\"\n    The function curve generates a composite curve by summing multiple Gaussian-like components.\n    It randomly determines the number of components, their centers, widths, and scales.\n    The widths and scales are normalized to ensure they sum to specific values.\n    The function initializes an array of zeros and iteratively adds scaled Gaussian components\n    based on the calculated parameters. Finally, it returns the resulting composite curve.\n    \"\"\"\n    # Define the number of data points in the curve\n    num_points = 1000\n    # Initialize the array to store the composite curve\n    composite_curve = np.zeros(num_points)\n    \n    # Randomly determine the number of Gaussian components\n    num_components = np.random.randint(1, 10)\n    \n    # Define the domain of the curve\n    x = np.linspace(0, 100, num_points)\n    \n    # Randomly determine the centers, widths, and scales for the Gaussian components\n    centers = np.random.uniform(low=0, high=100, size=num_components)\n    raw_widths = np.random.rand(num_components)\n    raw_scales = np.random.rand(num_components)\n    \n    # Normalize widths and scales so that they sum to specific values\n    # Normalize widths to sum to 50 and scales to sum to 10\n    widths = raw_widths / raw_widths.sum() * 50\n    scales = raw_scales / raw_scales.sum() * 10\n    \n    # Define a function for the Gaussian components\n    def gaussian(x, center, width, scale):\n        return scale * np.exp(-((x - center) ** 2) / (2 * width ** 2))\n    \n    # Iterate over each component and add it to the composite curve\n    for center, width, scale in zip(centers, widths, scales):\n        component = gaussian(x, center, width, scale)\n        composite_curve += component\n    \n    # Return the composite curve\n    return composite_curve\n\n# Example usage\nresulting_curve = curve()", "CodeBleu_score": 39.269519312897934}, {"code": "import numpy as np\n\ndef curve(num_points=1000, center_limit=1.0, total_width=10.0, total_scale=2.0):\n    \"\"\"The function curve generates a composite curve by summing multiple Gaussian-like components. It\n    randomly determines the number of components, their centers, widths, and scales. The widths and scales\n    are normalized to ensure they sum to specific values. The function initializes an array of zeros and\n    iteratively adds scaled Gaussian components based on the calculated parameters. Finally, it returns\n    the resulting composite curve.\n    \n    Parameters:\n    - num_points (int): The number of data points in the curve.\n    - center_limit (float): The limit for the absolute value of the centers of the Gaussian components.\n    - total_width (float): The total sum of the widths of all Gaussian components which will be normalized.\n    - total_scale (float): The total sum of the scales of all Gaussian components which will be normalized.\n    \n    Returns:\n    - np.ndarray: Array containing the values of the composite curve.\n    \"\"\"\n    \n    # Determine the number of components randomly\n    num_components = np.random.randint(1, 10)\n    \n    # Randomly determine centers, initial widths, and initial scales\n    centers = np.random.uniform(-center_limit, center_limit, num_components)\n    initial_widths = np.random.rand(num_components)\n    initial_scales = np.abs(np.random.randn(num_components))\n    \n    # Normalize widths and scales to sum to specific values\n    widths = total_width * initial_widths / np.sum(initial_widths)\n    scales = total_scale * initial_scales / np.sum(initial_scales)\n    \n    # Initialize the composite curve array\n    x = np.linspace(-center_limit, center_limit, num_points)\n    curve = np.zeros(num_points)\n    \n    # Build the composite curve by adding scaled Gaussian components\n    for center, width, scale in zip(centers, widths, scales):\n        curve += scale * np.exp(-0.5 * ((x - center) / width) ** 2)\n    \n    return curve\n\n# (Optional) Additional code to plot the curve for visualization\nif __name__ == '__main__':\n    import matplotlib.pyplot as plt\n    \n    # Generate the composite curve\n    composite_curve = curve()\n    \n    # Plotting\n    plt.plot(composite_curve)\n    plt.title(\"Composite Gaussian Curve\")\n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Amplitude\")\n    plt.show()", "CodeBleu_score": 35.12031490606046}, {"code": "import numpy as np\nfrom scipy.stats import norm\n\ndef curve(num_points=1000, center_limit=1000, width_sum=100, scale_sum=1):\n    \"\"\"\n    The function curve generates a composite curve by summing multiple Gaussian-like components.\n    It randomly determines the number of components, their centers, widths, and scales.\n    The widths and scales are normalized to ensure they sum to specific values.\n    The function initializes an array of zeros and iteratively adds scaled Gaussian components\n    based on the calculated parameters.\n    Finally, it returns the resulting composite curve.\n\n    Parameters:\n        num_points (int): Number of data points in the curve.\n        center_limit (int): Limit for the random centers of the Gaussian components.\n        width_sum (float): Total sum of the widths (standard deviations) for all Gaussian components.\n        scale_sum (float): Total sum of scales (amplitudes) for all Gaussian components.\n\n    Returns:\n        np.ndarray: An array representing the composite curve.\n    \"\"\"\n    # Randomly determine the number of components (between 1 and 10)\n    num_components = np.random.randint(1, 11)\n\n    # Initialize array for the result\n    x = np.linspace(0, center_limit, num_points)\n    composite_curve = np.zeros(num_points)\n\n    # Generate random centers, widths, and scales\n    centers = np.random.uniform(0, center_limit, num_components)\n    raw_widths = np.random.rand(num_components)\n    raw_scales = np.random.rand(num_components)\n\n    # Normalize widths and scales\n    widths = raw_widths / raw_widths.sum() * width_sum\n    scales = raw_scales / raw_scales.sum() * scale_sum\n\n    # Add each Gaussian to the composite curve\n    for center, width, scale in zip(centers, widths, scales):\n        composite_curve += scale * norm.pdf(x, loc=center, scale=width)\n\n    return composite_curve", "CodeBleu_score": 35.11571568543168}]}
{"instance_id": "be694335ebfc", "generation_result": [{"code": "import numpy as np\nimport math\n\ndef difference(derivative, accuracy=1):\n    \"\"\"\n    Computes finite difference coefficients for numerical differentiation.\n    Parameters:\n    - derivative: int, the order of the derivative.\n    - accuracy: int, the accuracy level.\n\n    Returns:\n    - coeffs: numpy array, coefficients of the finite difference formula.\n    - points: numpy array, array of points used in the difference formula.\n    \"\"\"\n\n    # Increase the derivative order by 1\n    n = derivative + 1\n    \n    # Determine the range of points: use symmetric points around zero\n    m = n + accuracy - 1\n    points = np.arange(-m, m + 1)\n    \n    # Create the Vandermonde matrix\n    vander_matrix = np.vander(points, N = m + 1, increasing=True)\n    \n    # Compute the inverse of the Vandermonde matrix\n    vander_inverse = np.linalg.inv(vander_matrix)\n    \n    # Select the row that contains the coefficients for the derivative of interest\n    coeffs = vander_inverse[n]\n    \n    # Scale coefficients by the factorial of (derivative - 1)\n    scale_factor = math.factorial(derivative - 1)\n    coeffs = coeffs * scale_factor\n    \n    return coeffs, points\n\n# Usage example\ncoefficients, points_used = difference(2, 3)\nprint(\"Coefficients: \", coefficients)\nprint(\"Points used: \", points_used)", "CodeBleu_score": 36.8982526021118}, {"code": "import numpy as np\nimport math\n\ndef difference(derivative, accuracy=1):\n    \"\"\"The function difference computes finite difference coefficients for numerical differentiation.\n    It takes a derivative order and an optional accuracy level as inputs.\n    The derivative order is incremented by one, and the radius is calculated based on the accuracy and derivative order.\n    The function defines a range of points centered around zero and calculates the inverse of the Vandermonde matrix for these points.\n    It returns the coefficients for the specified derivative order, scaled by the factorial of one less than the derivative order,\n    along with the points.\"\"\"\n    \n    # Calculate the radius based on accuracy and derivative order\n    n = derivative + 1\n    radius = accuracy * (n-1)\n    \n    # Generate a set of points centered at zero\n    points = np.arange(-radius, radius + 1)\n    \n    # Create the Vandermonde matrix for these points based on powers up to the length of points - 1\n    v_matrix = np.vander(points, len(points), increasing=True)\n    \n    # Identity vector corresponding to the desired derivative order\n    identity_vector = np", "CodeBleu_score": 40.94797893456813}, {"code": "import numpy as np\nfrom scipy.special import factorial\nfrom numpy.linalg import inv\n\ndef difference(derivative, accuracy=1):\n    \"\"\"\n    The function computes finite difference coefficients for numerical differentiation.\n    It takes a derivative order 'derivative' and an optional accuracy level 'accuracy' as inputs.\n    The points are calculated as '2 * accuracy + 1' points centered around zero.\n    The function returns the coefficients for the specified derivative order, scaled by the factorial\n    of one less than the derivative order, along with the points used for the calculation.\n    \"\"\"\n    \n    # Calculate the number of points based on derivative and accuracy\n    num_points = 2 * accuracy + 1\n    \n    # Generate points symmetrically around 0\n    points = np.arange(-accuracy, accuracy + 1)\n    \n    # Create Vandermonde matrix\n    V = np.array([[p**i for p in points] for i in range(num_points)])\n    \n    # Compute the inverse of the Vandermonde matrix\n    Vinv = inv(V)\n    \n    # Extract the row corresponding to the specified derivative\n    d_row = Vinv[derivative]\n    \n    # Scale by factorial of (derivative-1)\n    coefficients = d_row / factorial(derivative - 1) if derivative > 0 else d_row\n    \n    return coefficients, points\n\n# Example of using the function with 1st derivative and accuracy 1\ncoefficients, points = difference(1, 1)\nprint(\"Coefficients:\", coefficients)\nprint(\"Points:\", points)", "CodeBleu_score": 33.56475578693797}]}
{"instance_id": "fa3fb47f33fa", "generation_result": [{"code": "import numpy as np\n\ndef interpolate(X, Y, T):\n    \"\"\"The function interpolate computes the linear interpolation of given X and Y coordinates based on a target array T. \n    It first calculates the distances between consecutive points, then computes the cumulative distances. \n    The function uses these cumulative distances to interpolate the X and Y coordinates at the positions specified by T. \n    Finally, it returns the interpolated X and Y coordinates along with the total distance covered by the original coordinates.\n    \n    Args:\n        X (list or np.array): The X coordinates.\n        Y (list or np.array): The Y coordinates.\n        T (int): The number of samples for the interpolated curve.\n    \n    Returns:\n        tuple: Tuple containing:\n            interp_X (np.array): The interpolated X coordinates.\n            interp_Y (np.array): The interpolated Y coordinates.\n            total_distance (float): Cumulative distance from the start to the last original coordinate.\n    \"\"\"\n    \n    # Convert to numpy arrays if not already\n    X = np.array(X)\n    Y = np.array(Y)\n    \n    # Calculate distances between consecutive points\n    distances = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n    total_distance = np.sum(distances)\n    \n    # Calculate cumulative distances\n    cumulative_distances = np.zeros(len(X))\n    cumulative_distances[1:] = np.cumsum(distances)\n    \n    # Generate interpolation points in terms of total distance\n    distance_points = np.linspace(0, total_distance, T)\n    \n    # Interpolate X and Y coordinates\n    interp_X = np.interp(distance_points, cumulative_distances, X)\n    interp_Y = np.interp(distance_points, cumulative_distances, Y)\n    \n    return interp_X, interp_Y, total_distance", "CodeBleu_score": 39.58178967935343}, {"code": "import numpy as np\n\ndef interpolate(X, Y, T):\n    \"\"\"\n    The function interpolate computes the linear interpolation of given X and Y coordinates \n    based on a target array T. It first calculates the distances between consecutive points, \n    then computes the cumulative distances. The function uses these cumulative distances \n    to interpolate the X and Y coordinates at the positions specified by T. Finally, \n    it returns the interpolated X and Y coordinates along with the total distance \n    covered by the original coordinates.\n\n    Args:\n    X (np.array): An array of x-coordinates.\n    Y (np.array): An array of y-coordinates.\n    T (np.array): An array of target positions (0 to 1) along the cumulative path.\n\n    Returns:\n    tuple: A tuple containing interpolated X and Y coordinates and the total path distance.\n    \"\"\"\n    \n    # Calculate distances between consecutive X,Y points\n    distances = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n    cumulative_distances = np.insert(np.cumsum(distances), 0, 0)\n    \n    # Total distance\n    total_distance = cumulative_distances[-1]\n    \n    # Target real distances\n    target_distances = T * total_distance\n    \n    # Interpolate X and Y coordinates using the target_distances\n    interpolated_X = np.interp(target_distances, cumulative_distances, X)\n    interpolated_Y = np.interp(target_distances, cumulative_distances, Y)\n\n    return interpolated_X, interpolated_Y, total_distance\n\n# Example usage:\nX = np.array([0, 1, 2, 3])\nY = np.array([0, 1, 0, 1])\nT = np.array([0, 0.5, 1])\ninterpolated_coords = interpolate(X, Y, T)\nprint(interpolated_coords)", "CodeBleu_score": 32.88114096855417}, {"code": "import numpy as np\n\ndef interpolate(X, Y, T):\n    \"\"\"The function interpolate computes the linear interpolation of given X and Y coordinates \n    based on a target array T. It first calculates the distances between consecutive points, \n    then computes the cumulative distances. The function uses these cumulative distances to \n    interpolate the X and Y coordinates at the positions specified by T. Finally, it returns \n    the interpolated X and Y coordinates along with the total distance covered by the original coordinates.\n\n    Parameters:\n        X (list or array): The x-coordinates of points.\n        Y (list or array): The y-coordinates of points.\n        T (list or array): The array of target distances at which to interpolate.\n\n    Returns:\n        interpolated_X (array): The interpolated x-coordinates.\n        interpolated_Y (array): The interpolated y-coordinates.\n        total_distance (float): The total distance covered by the path.\n    \"\"\"\n    # Convert inputs to NumPy arrays for better manipulation\n    X = np.array(X)\n    Y = np.array(Y)\n    T = np.array(T)\n    \n    # Step 1: Compute distances between consecutive points\n    distances = np.sqrt(np.diff(X)**2 + np.diff(Y)**2)\n\n    # Step 2: Compute cumulative distances\n    cumulative_distances = np.zeros(len(X))\n    cumulative_distances[1:] = np.cumsum(distances)\n\n    # Step 3: Interpolation for target distances T\n    interpolated_X = np.interp(T, cumulative_distances, X)\n    interpolated_Y = np.interp(T, cumulative_distances, Y)\n    \n    # Total distance is the last value in the cumulative_distances\n    total_distance = cumulative_distances[-1]\n\n    return interpolated_X, interpolated_Y, total_distance", "CodeBleu_score": 40.61922756081615}]}
{"instance_id": "870a2dad3c66", "generation_result": [{"code": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)\nimport numpy as np\n\ndef mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0):\n    # Create a linearly spaced grid of x and y coordinates\n    X = np.linspace(xmin, xmax, xn)\n    Y = np.linspace(ymin, ymax, yn)\n    \n    # Create a meshgrid from x and y coordinates\n    C = X[:, np.newaxis] + 1j * Y[np.newaxis, :]\n    \n    # Initialize the array to hold the number of iterations for each point\n    N = np.zeros(C.shape, dtype=int)\n    \n    # Initialize Z to zero (start of the mandelbrot formula)\n    Z = np.zeros(C.shape, dtype=complex)\n    \n    # Mask array to keep track of points that have \"escaped\" and should no longer be iterated\n    mask = np.ones(C.shape, dtype=bool)\n    \n    # Perform iterations\n    for n in range(maxiter):\n        # Apply the mandelbrot iteration only where needed (where mask is True)\n        Z[mask] = Z[mask]**2 + C[mask]\n        \n        # Update the mask to False for elements that have escaped the horizon\n        escaped = np.abs(Z) > horizon\n        \n        # Update iteration count array where points have escaped and were previously marked True\n        N[mask & escaped] = n + 1  # Since n starts from 0, iter counts should start from 1\n        \n        # Update mask\n        mask[escaped] = False\n        \n        # Break out if there are no more points to iterate\n        if not mask.any():\n            break\n    \n    # Points that never escaped are set to zero iterations\n    N[N == 0] = maxiter\n    \n    return Z, N\n\n# Example call to function:\nZ, N = mandelbrot_set(-2.0, 1.0, -1.5, 1.5, 1000, 1000, 1000)\nprint(\"Mandelbrot set array N (complexity at each point, maxiter where never escaped):\", N)", "CodeBleu_score": 40.76825871998062}, {"code": "import", "CodeBleu_score": 0.0}, {"code": "import numpy as np\n\ndef mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0):\n    \"\"\"\n    The function mandelbrot_set generates the Mandelbrot set for a given range of complex numbers.\n    It takes the minimum and maximum values for the real and imaginary parts (xmin, xmax, ymin, ymax),\n    the number of points in the x and y directions (xn, yn), the maximum number of iterations (maxiter),\n    and an optional escape radius (horizon).\n    The function creates a grid of complex numbers C from the specified ranges, initializes iteration count array N\n    and complex array Z.\n    It iteratively computes the Mandelbrot sequence, updating N with the iteration count where the magnitude of Z exceeds the horizon.\n    Points that remain bounded after maxiter iterations are set to 0 in N.\n    The function returns the final complex values Z and the iteration counts N.\n    \"\"\"\n    # Create a linearly spaced grid of complex numbers C\n    x = np.linspace(xmin, xmax, xn)\n    y = np.linspace(ymin, ymax, yn)\n    C = x[:, np.newaxis] + 1j * y[np.newaxis, :]\n\n    # Initialize the arrays for the iteration counts and complex values\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, dtype=complex)\n\n    # Perform the iterations\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I]**2 + C[I]\n\n    # Set the points that never escaped to maxiter\n    N[N == 0] = maxiter\n\n    return Z, N", "CodeBleu_score": 60.57974755235966}]}
{"instance_id": "e1a908945acf", "generation_result": [{"code": "import numpy as np\nfrom scipy.ndimage import gaussian_filter\n\ndef smooth2d(A, sigma=3):\n    \"\"\"\n    The function smooth2d applies a Gaussian smoothing operation to a 2D array using a specified sigma value.\n    It smooths the array A with a Gaussian filter defined by the sigma.\n    \n    Parameters:\n    A (numpy.ndarray): The input 2D array to be smoothed.\n    sigma (float, optional): The standard deviation for the Gaussian kernel. Defaults to 3.\n    \n    Returns:\n    numpy.ndarray: The smoothed 2D array.\n    \"\"\"\n\n    # Check if the input is indeed a 2D array\n    if len(A.shape) != 2:\n        raise ValueError(\"Input must be a 2D array.\")\n    \n    # Perform the Gaussian smoothing\n    smoothed_A = gaussian_filter(A, sigma=sigma)\n    \n    return smoothed_A", "CodeBleu_score": 14.036326883073253}, {"code": "import numpy as np\nfrom scipy.ndimage import gaussian_filter\n\ndef smooth2d(A, sigma=3):\n    \"\"\"\n    The function smooth2d applies a smoothing operation to a 2D array using a specified sigma value. \n    It calculates the window length based on sigma, ensuring it is an odd number.\n    The function first smooths each row of the array using a 1D smoothing function, then transposes\n    the result and smooths each row of the transposed array. Finally, it transposes the array again\n    and returns the smoothed 2D array.\n\n    Parameters:\n    - A : ndarray\n        The 2D array to be smoothed.\n    - sigma : float (default: 3)\n        The sigma (standard deviation) of the Gaussian kernel used for smoothing.\n\n    Returns:\n    - ndarray\n        The smoothed 2D array.\n    \"\"\"\n\n    # Apply Gaussian filter to smooth the array. This function internally calculates the window size,\n    # applies Gaussian smoothing per the sigma provided, for both dimensions.\n    smoothed_A = gaussian_filter(A, sigma=sigma)\n\n    return smoothed_A", "CodeBleu_score": 8.216757088329674}, {"code": "import numpy as np\nfrom scipy.ndimage import gaussian_filter1d\n\ndef smooth2d(A, sigma=3):\n    \"\"\"The function smooth2d applies a smoothing operation to a 2D array using a specified sigma value.\n    It calculates the window length based on sigma, ensuring it is an odd number.\n    The function first smooths each row of the array using a 1D smoothing function, then transposes\n    the result and smooths each row of the transposed array. Finally, it transposes the array again\n    and returns the smoothed 2D array.\n\n    Parameters:\n    A (numpy.ndarray): The 2D array to be smoothed.\n    sigma (int, optional): The standard deviation used for Gaussian kernel. Default value is 3.\n\n    Returns:\n    numpy.ndarray: The smoothed 2D array.\n    \"\"\"\n    # Smooth each row\n    A_smoothed = np.array([gaussian_filter1d(row, sigma=sigma) for row in A])\n\n    # Transpose the array\n    A_transposed = A_smoothed.T\n\n    # Smooth each row of the transposed array (which are originally the columns)\n    A_transposed_smoothed = np.array([gaussian_filter1d(row, sigma=sigma) for row in A_transposed])\n\n    # Transpose back to original orientation\n    A_smoothed_final = A_transposed_smoothed.T\n\n    return A_smoothed_final", "CodeBleu_score": 21.75952360328896}]}
{"instance_id": "94563c54b483", "generation_result": [{"code": "import numpy as np\n\nclass ImageProcessor:\n    def __init__(self, ox, oy):\n        self.ox = ox\n        self.oy = oy\n\n    def process_image(self, padded_src, dpi):\n        \"\"\"\n        The function process_image takes a padded source image padded_src and a DPI value dpi as arguments.\n        It retrieves the offsets ox and oy from the instance's attributes.\n        The function then shifts the image horizontally by ox converted to pixels \n        and vertically by oy converted to pixels using numpy's roll function.\n        Finally, it returns the processed image.\n\n        Args:\n        padded_src (numpy.ndarray): The source image to be processed, it should be a numpy array.\n        dpi (int): Dots per inch resolution for translating offset from inches to pixels.\n\n        Returns:\n        numpy.ndarray: The processed image after shifting.\n        \"\"\"\n        # Calculate the pixel offsets from the inches offsets and dpi\n        pixel_offset_x = int(self.ox * dpi)\n        pixel_offset_y = int(self.oy * dpi)\n\n        # Shift the image using numpy's roll function\n        # roll axis 0 for vertical shift, axis 1 for horizontal shift\n        shifted_image = np.roll(padded_src, pixel_offset_y, axis=0)\n        shifted_image = np.roll(shifted_image, pixel_offset_x, axis=1)\n\n        return shifted_image", "CodeBleu_score": 29.740650108591304}, {"code": "import numpy as np\n\nclass ImageProcessor:\n    def __init__(self, ox, oy):\n        self.ox = ox  # offset x in inches\n        self.oy = oy  # offset y in inches\n\n    def process_image(self, padded_src, dpi):\n        \"\"\"The function process_image takes a padded source image padded_src and a DPI value as arguments.\n        It retrieves the offsets ox and oy from the instance's attributes.\n        The function then shifts the image horizontally by ox converted to pixels and vertically by oy converted to pixels using numpy's roll function.\n        Finally, it returns the processed image.\n\n        Args:\n            padded_src (np.array): 2D or 3D numpy array representing the padded source image.\n            dpi (int): Dots Per Inch, the pixel density used to convert offset in inches to pixels.\n\n        Returns:\n            np.array: The image shifted by the calculated pixel offsets.\n        \"\"\"\n        # Convert offsets from inches to pixels\n        offset_x_pixels = int(self.ox * dpi)  # Horizontal offset in pixels\n        offset_y_pixels = int(self.oy * dpi)  # Vertical offset in pixels\n\n        # Roll the image to apply the offsets\n        # numpy.roll shifts the array along each axis.\n        # Shift horizontally (axis 1 for 2D or 3D images)\n        rolled_image = np.roll(padded_src, offset_x_pixels, axis=1)\n        # Shift vertically (axis 0 for 2D or 3D images)\n        rolled_image = np.roll(rolled_image, offset_y_pixels, axis=0)\n\n        return rolled_image\n\n# Example to use this class\n# Create an instance of the processor with specific x, y offsets in inches\nprocessor = ImageProcessor(ox=0.5, oy=0.5)\n# Create a dummy image (e.g., 100x100 pixels image with some random colors or grayscale)\nimage = np.random.rand(100, 100, 3)  # Example of a 3-channel image\n# Process the image with a given DPI\nprocessed_image = processor.process_image(image, 300)  # DPI is 300", "CodeBleu_score": 26.797894672043967}, {"code": "import numpy as np", "CodeBleu_score": 0.0}]}
{"instance_id": "d71d232872fe", "generation_result": [{"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\nimport numpy as np\n\ndef gaussian(shape=(25, 25), width=0.5, center=None):\n    \"\"\"\n    The function generates a multi-dimensional Gaussian distribution array with a specified shape, width, and center.\n    It initializes the shape, width, and center parameters as tuples if they are provided as single values. \n    It then creates a grid for the specified shape and computes the squared distance of each grid point from the center,\n    normalized by the width. Finally, it returns the Gaussian values computed using the exponential function \n    applied to the negative half of the squared distances.\n\n    Parameters:\n        shape (tuple): The shape of the output Gaussian array. Defaults to (25, 25).\n        width (float or tuple): The width (standard deviation) of the Gaussian distribution.\n                               If a float, it assumes the same width in all dimensions.\n                               If a tuple, specifies the width for each dimension.\n        center (tuple): The center of the Gaussian peak. If None, defaults to the center of the array.\n                        If given as a single value, it uses the value for all dimensions.\n\n    Returns:\n        np.ndarray: An array of the Gaussian function evaluated at each point of the specified shape and center.\n    \"\"\"\n\n    # Ensure width and center are in tuple format and match dimensionality of the shape\n    if not isinstance(width, tuple):\n        width = (width,) * len(shape)\n    if center is None:\n        center = tuple(s // 2 for s in shape)\n    elif not isinstance(center, tuple):\n        center = (center,) * len(shape)\n\n    # Generate grid of points and their coordinates\n    coords = [np.linspace(-c, s - 1 - c, s) for s, c in zip(shape, center)]\n    grid = np.meshgrid(*coords, indexing='ij')\n\n    # Calculate squared distances from center\n    dist2 = sum((g / w)**2 for g, w in zip(grid, width))\n\n    # Compute Gaussian function\n    gauss = np.exp(-0.5 * dist2)\n\n    return gauss\n\n# Example usage:\ngauss_array = gaussian(shape=(50, 50), width=1.0, center=(25, 25))\nprint(gauss_array.shape)\nprint(gauss_array)", "CodeBleu_score": 36.88924186398896}, {"code": "def gaussian(shape=(25, 25), width=0.5, center=0.0)\nimport numpy as np\n\ndef gaussian(shape=(25, 25), width=0.5, center=None):\n    \"\"\"\n    The function gaussian generates a multi-dimensional Gaussian distribution array with a specified shape, width, and center.\n    It initializes the shape, width, and center parameters as tuples if they are provided as single values.\n    It then creates a grid for the specified shape and computes the squared distance of each grid point from the center,\n    normalized by the width. Finally, it returns the Gaussian values computed using the exponential function applied\n    to the negative half of the squared distances.\n\n    Args:\n    - shape (tuple of ints): The shape of the Gaussian array.\n    - width (float): The width (standard deviation) of the Gaussian.\n    - center (tuple of floats): The center of the Gaussian distribution.\n\n    Returns:\n    - ndarray: A numpy array representing the Gaussian distribution.\n    \"\"\"\n    \n    # Ensure that center and shape are tuples\n    if center is None:\n        center = tuple((s - 1) / 2 for s in shape)  # Default to the middle of the array\n    elif not isinstance(center, tuple):\n        center = (center,)\n\n    if not isinstance(shape, tuple):\n        shape = (shape,)\n\n    # Generate grid of coordinates (x, y, ...)\n    grids = [np.arange(s) for s in shape]\n\n    # Create a meshgrid (ndim-dimensional grid)\n    grid_mesh = np.meshgrid(*grids, indexing='ij')\n    \n    # Calculate distance of each point in grid from the center\n    squared_distances = sum((grid - c) ** 2 for grid, c in zip(grid_mesh, center))\n\n    # Compute the Gaussian function\n    gaussian_array = np.exp(-0.5 * squared_distances / (width ** 2))\n    \n    return gaussian_array\n\n# This function generates a Gaussian (normal distribution) grid based on the parameters. It's mostly used in fields\n# ranging from image processing to physics simulations where a Gaussian spread is necessary.\n# For testing, you might try various shapes like (10, 10) or (100, 100), different widths, and varying center points.", "CodeBleu_score": 29.262809877775673}, {"code": "import numpy as np\n\ndef gaussian(shape=(25, 25), width=0.5, center=0.0):\n    \"\"\"The function Gaussian generates a multi-dimensional Gaussian distribution array with a specified shape, width, and center.\n    \n    Args:\n        shape (tuple or int): Shape of the output Gaussian array. Can be a single integer or a tuple of two integers.\n        width (float or tuple): Standard deviation of the Gaussian distribution. Can be a single float or a tuple of floats.\n        center (float or tuple): Center of the Gaussian peak. Can be a single float or a tuple of floats.\n    \n    Returns:\n        np.ndarray: 2-D Gaussian array with the specified properties.\n    \"\"\"\n\n    # Handling input parameters for flexibility of single values or tuples\n    if isinstance(shape, int):\n        shape = (shape, shape)\n    if isinstance(width, (int, float)):\n        width = (width, width)\n    if isinstance(center, (int, float)):\n        center = (center, center)\n\n    # Create grid for the Gaussian\n    x = np.linspace(-1, 1, shape[0])\n    y = np.linspace(-1, 1, shape[1])\n    x_grid, y_grid = np.meshgrid(x, y)\n\n    # Compute the Gaussian distribution\n    gaussian_distribution = np.exp(-((x_grid - center[0])**2 / (2 * width[0]**2)\n                                      + (y_grid - center[1])**2 / (2 * width[1]**2)))\n\n    return gaussian_distribution", "CodeBleu_score": 29.558244125665574}]}
{"instance_id": "7bbd08c987bc", "generation_result": [{"code": "import numpy as np\n\ndef stimulus(position, size, intensity):\n    \"\"\"This function calculates a stimulus based on a position, size, and intensity.\n    It converts polar coordinates to Cartesian coordinates and normalizes them.\n    It creates a mesh grid representing the spatial domain, normalized and centered.\n    The function computes the squared Euclidean distance (R) from each point in the grid to the stimulus center.\n    Finally, it returns an exponential decay function of these distances, scaled by the size parameter.\n    \n    Args:\n        position (tuple): The polar coordinate of the stimulus center, (radius, angle in radians).\n        size (float): The size parameter of the stimulus, influencing the extent of the exponential decay.\n        intensity (float): The peak intensity of the stimulus.\n\n    Returns:\n        numpy.ndarray: A meshgrid of values representing the stimulus intensity based on squared distances.\n    \"\"\"\n    # Convert polar to Cartesian coordinates\n    r, theta = position\n    x_center = r * np.cos(theta)\n    y_center = r * np.sin(theta)\n    \n    # Normalize and center the coordinates within the spatial domain of -1 to 1\n    # Assuming a normalized domain where the center is (0, 0) and spans outwards\n    grid_x, grid_y = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n    \n    # Compute the squared Euclidean distances (R) from each point in the meshgrid to the stimulus center\n    R = (grid_x - x_center)**2 + (grid_y - y_center)**2\n    \n    # Calculate the stimulus intensity using an exponential decay function scaled by the size parameter\n    stimulus_intensity = intensity * np.exp(-R / size**2)\n    \n    return stimulus_intensity", "CodeBleu_score": 28.04911404483912}, {"code": "import numpy as np\n\ndef stimulus(position, size, intensity):\n    \"\"\"This function calculates a stimulus based on a position, size, and intensity.\n    It converts polar coordinates to Cartesian coordinates and normalizes them.\n    It creates a mesh grid representing the spatial domain, normalized and centered.\n    The function computes the squared Euclidean distance (R) from each point in the grid to the stimulus center.\n    Finally, it returns an exponential decay function of these distances, scaled by the size parameter.\"\"\"\n\n    # Polar to Cartesian conversion for position\n    theta, radius = position\n    x_center = radius * np.cos(theta)\n    y_center = radius * np.sin(theta)\n\n    # Create a normalized mesh grid centered at (x_center, y_center)\n    grid_size = 100  # defines the size of the simulation grid (100x100)\n    x = np.linspace(-1, 1, grid_size)\n    y = np.linspace(-1, 1, grid_size)\n    X, Y = np.meshgrid(x, y)\n\n    # Centering the grid at the stimulus center\n    X_centered = X - x_center\n    Y_centered = Y - y_center\n\n    # Compute squared Euclidean distances from center\n    R_squared = X_centered**2 + Y_centered**2\n\n    # Exponential decay function scaled by size\n    stimulus_field = intensity * np.exp(-R_squared / (2 * size**2))\n\n    return stimulus_field\n\n# Example Usage:\ntheta = np.pi / 4  # 45 degrees\nradius = 1\nsize = 0.1\nintensity = 1\nresult = stimulus((theta, radius), size, intensity)\nprint(result)", "CodeBleu_score": 19.281155709212623}, {"code": "import numpy as np\n\ndef stimulus(position, size, intensity):\n    \"\"\"\n    This function calculates a stimulus based on a position, size, and intensity.\n    It converts polar coordinates to Cartesian coordinates and normalizes them.\n    It creates a mesh grid representing the spatial domain, normalized and centered.\n    The function computes the squared Euclidean distance (R) from each point in the grid to the stimulus center.\n    Finally, it returns an exponential decay function of these distances, scaled by the size parameter.\n    \"\"\"\n    # Unpack the position tuple which contains polar coordinates.\n    radius, angle = position\n    # Convert polar to Cartesian coordinates.\n    centerX, centerY = radius * np.cos(angle), radius * np.sin(angle)\n    \n    # Define the limits of the grid.\n    # We're using size * 2.5 to ensure coverage.\n    limit = size * 2.5\n    x = np.linspace(-limit, limit, int(size * 50))\n    y = np.linspace(-limit, limit, int(size * 50))\n    \n    # Create mesh grid.\n    X, Y = np.meshgrid(x, y)\n    \n    # Compute squared Euclidean distance from the center.\n    R = (X - centerX) ** 2 + (Y - centerY) ** 2\n    \n    # Apply exponential decay based on the size parameter.\n    # Intensity scales the height of the peak of the function.\n    stimulus_grid = intensity * np.exp(-R / size ** 2)\n    \n    return stimulus_grid", "CodeBleu_score": 32.353273938362456}]}
{"instance_id": "2b5806238d5d", "generation_result": [{"code": "import matplotlib.pyplot as plt\n\ndef plot(ax, text):\n    \"\"\"\n    The function plot configures the axes of a plot and adds text and a title. It sets the x and y limits from 0 to 1,\n    defines the x and y ticks, and labels the axes. The function adds centered text with specified properties\n    and sets the title with a specified font family and weight.\n\n    Args:\n    ax (matplotlib.axes.Axes): The axes object to configure.\n    text (str): The text to display on the plot.\n    \"\"\"\n    # Set x and y limits from 0 to 1\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    \n    # Define x and y ticks\n    ax.set_xticks([0, 0.5, 1])\n    ax.set_yticks([0, 0.5, 1])\n    \n    # Label the axes\n    ax.set_xlabel('X-axis Label')\n    ax.set_ylabel('Y-axis Label')\n    \n    # Add text centered with specified properties\n    ax.text(0.5, 0.5, text, fontsize=12, ha='center', va='center')\n    \n    # Set title with specified font family and weight\n    ax.set_title('Plot Title', fontdict={'family': 'serif', 'weight': 'bold'})\n\n# Usage\nfig, ax = plt.subplots()\nplot(ax, \"Hello, World!\")\nplt.show()", "CodeBleu_score": 46.32712032488144}, {"code": "import matplotlib.pyplot as plt\n\ndef plot(ax, text):\n    \"\"\"\n    The function plot configures the axes of a plot and adds text and a title. \n    It sets the x and y limits from 0 to 1, defines the x and y ticks, \n    and labels the axes. The function adds centered text with specified properties \n    and sets the title with a specified font family and weight.\n    \n    Parameters:\n        ax (", "CodeBleu_score": 27.189637629771347}, {"code": "def plot(ax, text):\n    \"\"\"\n    The function plot configures the axes of a plot and adds text and a title. It sets the x and y limits from 0 to 1,\n    defines the x and y ticks, and labels the axes. The function adds centered text with specified properties and sets\n    the title with a specified font family and weight.\n    \"\"\"\n    # Set x and y limits\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    # Define x and y ticks\n    ax.set_xticks([0, 0.5, 1])\n    ax.set_yticks([0, 0.5, 1])\n\n    # Label the axes\n    ax.set_xlabel('X axis')\n    ax.set_ylabel('Y axis')\n\n    # Add centered text\n    ax.text(0.5, 0.5, text, ha='center', va='center', fontsize=12, color='black')\n\n    # Set the title with specified font properties\n    ax.set_title('Plot Title', fontdict={'family': 'serif', 'weight': 'bold'})", "CodeBleu_score": 48.88837560306598}]}
{"instance_id": "0b42aac61e21", "generation_result": [{"code": "class SurvivalAnalysisModel:\n    def __init__(self, pipeline):\n        \"\"\"\n        Initializes the SurvivalAnalysisModel with a pipeline.\n        \n        Parameters:\n        pipeline (Pipeline): A scikit-learn-style pipeline where the last step is an estimator\n                             and the previous steps are data transformations.\n        \"\"\"\n        self.pipeline = pipeline\n\n    def predict_cumulative_hazard_function(self, X, **kwargs):\n        \"\"\"\n        Predicts the cumulative hazard function for input data X.\n\n        Parameters:\n        X (array-like): The input data for which to predict the cumulative hazard function.\n\n        Returns:\n        array-like: Predicted cumulative hazard function values for each input sample.\n        \"\"\"\n        # Apply all transformations but the last (assumed to be the estimator)\n        transformed_X = X\n        for name, transform in self.pipeline.steps[:-1]:\n            transformed_X = transform.transform(transformed_X)\n\n        # Use the final estimator to make the prediction\n        estimator = self.pipeline.steps[-1][1]\n        prediction = estimator.predict(transformed_X, **kwargs)\n\n        # Return the predicted cumulative hazard function\n        return prediction", "CodeBleu_score": 39.47703590973045}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs):\n    \"\"\"\n    Predicts the cumulative hazard function for input data X by applying a series of transformations \n    followed by a prediction using the final estimator.\n    \n    Params:\n        X (any data type accepted by transformations and final estimator): \n            The input data to predict the cumulative hazard function for.\n        **kwargs: Additional keyword arguments to be passed to the final estimator's predict method.\n        \n    Returns:\n        np.array: The predicted cumulative hazard function.\n    \"\"\"\n    # Transform X using each transformation in the pipeline except the last\n    for transformation in self.transformations[:-1]:  # Assuming transformations except the last are stored in a list\n        X = transformation.transform(X)\n    \n    # Use the final estimator for prediction\n    prediction = self.transformations[-1].predict(X, **kwargs)  # Assuming the last element is the estimator\n    \n    return prediction", "CodeBleu_score": 32.96301876013768}, {"code": "def predict_cumulative_hazard_function(self, X, **kwargs):\n    \"\"\"\n    The function predict_cumulative_hazard_function predicts the cumulative hazard function for input data X. \n    It iteratively applies a series of transformations to X, excluding the final transformation, by calling \n    the transform method of each transformation in the pipeline. After all transformations, it uses the \n    final estimator in the pipeline to predict the cumulative hazard function based on the transformed data \n    and returns the prediction.\n\n    Parameters:\n    - X: Data to be transformed and used for prediction.\n    - kwargs: Additional keyword arguments to be passed to the transform and prediction methods.\n\n    Returns:\n    - numpy array: The predicted cumulative hazard function.\n    \"\"\"\n    # Ensure there is at least one transform plus an estimator in the pipeline\n    if len(self.pipeline) < 2:\n        raise ValueError(\"Pipeline should contain at least one transformer and one estimator.\")\n\n    # Apply each transformation in the pipeline except the final element, which is an estimator\n    transformed_data = X\n    for transformer in self.pipeline[:-1]:\n        transformed_data = transformer.transform(transformed_data, **kwargs)\n    \n    # The final element is assumed to be the estimator\n    estimator = self.pipeline[-1]\n    \n    # Predict the cumulative hazard function using the transformed data\n    prediction = estimator.predict_cumulative_hazard_function(transformed_data, **kwargs)\n    \n    return prediction", "CodeBleu_score": 36.17476404203918}]}
{"instance_id": "c8439e4918ad", "generation_result": [{"code": "class SurvivalModel:\n    def __init__(self, pipeline):\n        self.pipeline = pipeline\n\n    def predict_survival_function(self, X, **kwargs):\n        \"\"\"\n        This function predicts the survival function for input data X.\n        It iteratively applies a series of transformations to X using a pipeline of steps, excluding the final step.\n        After transforming the data, it uses the final step in the pipeline to predict the survival function with the transformed data and any additional keyword arguments.\n        \n        Args:\n        X (pd.DataFrame): Input data to predict survival.\n        **kwargs: Additional keyword arguments for prediction step.\n        \n        Returns:\n        np.array: Predicted survival function values.\n        \"\"\"\n\n        # Transform the data using all but the last transformation in the pipeline\n        transformed_X = X\n        for step in self.pipeline[:-1]:\n            transformed_X = step.transform(transformed_X)\n\n        # Final step in the pipeline for prediction\n        prediction_step = self.pipeline[-1]\n        if hasattr(prediction_step, 'predict_survival_function'):\n            survival_function = prediction_step.predict_survival_function(transformed_X, **kwargs)\n        else:\n            raise ValueError(\"The final step of the pipeline does not support survival function prediction.\")\n\n        return survival_function", "CodeBleu_score": 38.11893813082603}, {"code": "def predict_survival_function(self, X, **kwargs):\n    \"\"\"\n    This function predicts the survival function for input data X.\n    It iteratively applies a series of transformations to X using a pipeline of steps, excluding the final step.\n    After transforming the data, it uses the final step in the pipeline to predict the survival function with the \n    transformed data and any additional keyword arguments.\n    \n    Parameters:\n        X (array-like): The input data for which to predict the survival function.\n        **kwargs: Additional keyword arguments to pass to the prediction method of the final step.\n    \n    Returns:\n        array-like: Predicted survival function values for each instance in X.\n    \"\"\"\n    \n    # Assuming 'self.pipeline' is a list of transformation steps with the final item being a predictor.\n    # Check if pipeline is complete with at least one transformation and a predictor\n    if not hasattr(self, 'pipeline') or len(self.pipeline) < 2:\n        raise ValueError(\"The model's pipeline must contain at least one transformation and one prediction step.\")\n    \n    # Transform the data X through all but the last step in the pipeline\n    transformed_X = X\n    for step in self.pipeline[:-1]:  # Excluding the last step which is the predictor\n        transformed_X = step.transform(transformed_X)\n    \n    # Predict using the last step in the pipeline\n    survival_predictions = self.pipeline[-1].predict(transformed_X, **kwargs)\n    \n    return survival_predictions", "CodeBleu_score": 32.68769086914596}, {"code": "def predict_survival_function(self, X, **kwargs):\n    \"\"\"\n    This function predicts the survival function for input data X.\n    It iteratively applies a series of transformations to X using a pipeline of steps, excluding the final step.\n    After transforming the data, it uses the final step in the pipeline to predict the survival function with \n    the transformed data and any additional keyword arguments.\n    \n    Parameters:\n    - X (array-like): The input data to the model.\n    - **kwargs: Additional keyword arguments to pass to the prediction function of the final estimator in the pipeline.\n\n    Returns:\n    - array-like: The predicted survival functions for the input data.\n    \"\"\"\n\n    # Check if there is a pipeline attribute\n    if not hasattr(self, 'pipeline'):\n        raise AttributeError(\"The 'pipeline' attribute is not found in the current object.\")\n    \n    # Access all steps except the last one for transformation\n    transform_pipeline = self.pipeline.steps[:-1]\n    \n    # Data through transformation steps\n    X_transformed = X\n    for name, transformer in transform_pipeline:\n        X_transformed = transformer.transform(X_transformed)\n    \n    # Final estimator for survival function prediction\n    final_step_name, final_estimator = self.pipeline.steps[-1]\n    \n    # Predict using the final estimator\n    if hasattr(final_estimator, \"predict_survival_function\"):\n        survival_function = final_estimator.predict_survival_function(X_transformed, **kwargs)\n    else:\n        raise AttributeError(f\"The final step of the pipeline, '{final_step_name}', does not support survival function prediction.\")\n    \n    return survival_function", "CodeBleu_score": 34.67323465298778}]}
{"instance_id": "216662ca7db4", "generation_result": [{"code": "def _check_success(self, results):\n    \"\"\"\n    The function _check_success evaluates the exit flag from solver results to determine the outcome of an optimization problem.\n    If the exit flag indicates an optimal solution or an acceptable level of inaccuracy, the function completes without action.\n    If the maximum iterations are reached, it issues a convergence warning.\n    For exit flags indicating primal or dual infeasibility, it raises a RuntimeError.\n    For any other unknown exit flags, it also raises a RuntimeError with the exit status.\n    \n    Args:\n    - results (object with property 'exit_flag'): an object that encapsulates the results of an optimization including an exit flag.\n    \n    Raises:\n    - RuntimeError: If the problem is infeasible (primal or dual) or if an unknown exit flag is encountered.\n    - Warns: If the maximum number of iterations is reached without convergence (warning only).\n    \"\"\"\n    optimal_message = \"Optimization was successful.\"\n    acceptable_message = \"Optimal solution within acceptable inaccuracy.\"\n    convergence_warning = \"Maximum iterations reached without convergence.\"\n    infeasibility_error = \"Problem is infeasible: \"\n    unknown_status_error = \"Optimization ended with an unknown status code: \"\n\n    if results.exit_flag == 0:\n        # Optimal solution found\n        print(optimal_message)\n    elif results.exit_flag == 1:\n        # Acceptable level of inaccuracy\n        print(acceptable_message)\n    elif results.exit_flag == 2:\n        # Warning for convergence\n        print(convergence_warning)\n        import warnings\n        warnings.warn(convergence_warning, category=RuntimeWarning)\n    elif results.exit_flag == 3:\n        # Primal infeasibility\n        raise RuntimeError(infeasibility_error + \"Primal infeasibility.\")\n    elif results.exit_flag == 4:\n        # Dual infeasibility\n        raise RuntimeError(infeasibility_error + \"Dual infeasibility.\")\n    else:\n        # Unknown exit status\n        raise RuntimeError(unknown_status_error + str(results.exit_flag))", "CodeBleu_score": 25.81425302917062}, {"code": "def _check_success(self, results):\n    \"\"\"\n    The function _check_success evaluates the exit flag from solver results to determine the outcome of an \n    optimization problem. If the exit flag indicates an optimal solution or an acceptable level of inaccuracy, \n    the function completes without action. If the maximum iterations are reached, it issues a convergence warning.\n    For exit flags indicating primal or dual infeasibility, it raises a RuntimeError. For any other unknown exit flags,\n    it also raises a RuntimeError with the exit status.\n\n    Args:\n    - results (dict): Dictionary containing 'exit_flag' as a key and its value indicating status of optimization.\n\n    Raises:\n    - RuntimeError: If primal or dual infeasibility is detected, or for unknown exit flags.\n    - Warning: If the maximum iterations are reached.\n    \"\"\"\n    exit_flag = results.get('exit_flag')\n\n    # Check the different conditions based on exit_flag\n    if exit_flag == 0 or exit_flag == 1:\n        return  # No action needed, optimization was successful or acceptable.\n\n    if exit_flag == 2:\n        from warnings import warn\n        warn(\"Maximum iterations reached, convergence may not have been achieved.\", RuntimeWarning)\n\n    elif exit_flag == -1:\n        raise RuntimeError(\"Primal infeasibility detected.\")\n\n    elif exit_flag == -2:\n        raise RuntimeError(\"Dual infeasibility detected.\")\n\n    else:\n        raise RuntimeError(f\"Unknown solver exit status: {exit_flag}\")", "CodeBleu_score": 41.938319239154694}, {"code": "def _check_success(self, results):\n    \"\"\"\n    The function _check_success evaluates the exit flag from solver results to determine the outcome of an optimization problem.\n    If the exit flag indicates an optimal solution or an acceptable level of inaccuracy, the function completes without action.\n    If the maximum iterations are reached, it issues a convergence warning.\n    For exit flags indicating primal or dual infeasibility, it raises a RuntimeError.\n    For any other unknown exit flags, it also raises a RuntimeError with the exit status.\n\n    Args:\n    results (dict): A dictionary containing the output from the optimization solver.\n\n    Raises:\n    RuntimeError: If the solver indicates that the problem is infeasible or if there is an unknown error.\n    Warning: If the maximum iterations are reached without convergence.\n    \"\"\"\n\n    optimal_flags = [\"optimal\", \"nearly_optimal\"]\n    infeasible_flags = [\"infeasible\", \"dual_infeasible\", \"primal_infeasible\"]\n    max_iterations_flag = [\"max_iterations_reached\"]\n\n    status = results.get(\"status\", None)\n\n    if status in optimal_flags:\n        # If solution is optimal or acceptably imprecise, do nothing\n        return\n\n    elif status in max_iterations_flag:\n        # Issue a warning if maximum iterations are reached\n        import warnings\n        warnings.warn(\"Convergence warning: Maximum iterations reached without optimal convergence.\", RuntimeWarning)\n\n    elif status in infeasible_flags:\n        # Raise error if the problem is found to be infeasible\n        raise RuntimeError(f\"Solver reports problem infeasibility: {status}\")\n\n    else:\n        # Raise error if the exit status is unknown or unhandled\n        raise RuntimeError(f\"Solver exited with unknown or unhandled status: '{status}'\")", "CodeBleu_score": 32.01770396143987}]}
{"instance_id": "1fe2e379fca0", "generation_result": [{"code": "import numpy as np\nfrom scipy.linalg import eigh\nimport sys\n\ndef _decompose(self, P):\n    \"\"\"\n    This function performs matrix decomposition on matrix P using its eigenvalues and eigenvectors.\n    It computes the eigenvalues and eigenvectors using the linalg.eigh function from scipy.linalg.\n    It identifies the largest eigenvalue and calculates a condition number based on this eigenvalue, the shape of P, and machine epsilon.\n    The function checks if all eigenvalues are above the negative cutoff and filters out those below the positive cutoff.\n    It normalizes the eigenvalues so that the maximum eigenvalue is set to 1.\n    Finally, it returns the transposed decomposed matrix and the largest eigenvalue.\n    \"\"\"\n    \n    # Computing eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = eigh(P)\n    \n    # Eigenvalues are sorted in an ascending order by eigh\n    largest_eigenvalue = eigenvalues[-1]\n    n = P.shape[0]  # dimension of matrix P\n    \n    # Condition number calculation\n    machine_epsilon = np.finfo(float).eps\n    condition_number = largest_eigenvalue / (machine_epsilon * n)\n    \n    # Filter eigenvalues\n    neg_cutoff = 1e-10\n    pos_cutoff = 1e-10\n    eigenvalues_filtered = eigenvalues[(eigenvalues > -neg_cutoff) & (eigenvalues > pos_cutoff)]\n    \n    # Check if filtering has left any eigenvectors\n    if eigenvalues_filtered.size == 0:\n        raise ValueError(\"All eigenvalues are below the positive cutoff or are negative beyond the allowed cutoff.\")\n    \n    largest_filtered_eigenvalue = eigenvalues_filtered[-1]\n    \n    # Normalizing eigenvalues\n    normalized_eigenvalues = eigenvalues_filtered / largest_filtered_eigenvalue\n    \n    # Rebuilding the matrix using the filtered and normalized eigenvalues\n    D_normalized = np.diag(normalized_eigenvalues)\n    decomposed_matrix = eigenvectors[:, -len(eigenvalues_filtered):] @ D_normalized @ eigenvectors[:, -len(eigenvalues_filtered):].T\n\n    return decomposed_matrix.T, largest_eigenvalue", "CodeBleu_score": 31.316949212275226}, {"code": "import numpy as np\nfrom scipy.linalg import eigh\nfrom numpy import finfo\n\ndef _decompose(self, P):\n    \"\"\"\n    This function performs matrix decomposition on matrix P using its eigenvalues and eigenvectors.\n    It computes the eigenvalues and eigenvectors using the linalg.eigh function from scipy.linalg.\n    It identifies the largest eigenvalue and calculates a condition number based on this eigenvalue, the shape of P, and machine epsilon.\n    The function checks if all eigenvalues are above the negative cutoff and filters out those below the positive cutoff.\n    It normalizes the eigenvalues so that the maximum eigenvalue is set to 1.\n    Finally, it returns the transposed decomposed matrix and the largest eigenvalue.\n    \"\"\"\n    # Compute eigenvalues and corresponding eigenvectors of P\n    eigenvalues, eigenvectors = eigh(P)\n\n    # Largest eigenvalue\n    largest_eigenvalue = np.max(eigenvalues)\n\n    # Condition number calculation (using largest eigenvalue, smallest positive eigenvalue and machine epsilon)\n    epsilon = finfo(float).eps\n    positive_eigenvalues = eigenvalues[eigenvalues > 0]\n    if len(positive_eigenvalues) > 0:\n        smallest_positive_eigenvalue = np.min(positive_eigenvalues)\n    else:\n        smallest_positive_eigenvalue = largest_eigenvalue  # If no positive, use the largest one\n    condition_number = largest_eigenvalue / (smallest_positive_eigenvalue + epsilon)\n\n    # Filter eigenvalues: Ignore those smaller than a tiny threshold above 0\n    positive_cutoff = 1e-10\n    negative_cutoff = -1e-10\n    useful_eigenvalues = eigenvalues[(eigenvalues > positive_cutoff) & (eigenvalues > negative_cutoff)]\n\n    # Check if all eigenvalues are in a safe range\n    if not np.all(useful_eigenvalues):\n        raise ValueError(\"Not all eigenvalues are above the negative cutoff\")\n\n    # Normalize eigenvalues so that the largest becomes 1\n    normalized_eigenvalues = eigenvalues / largest_eigenvalue\n\n    # Prepare the matrix using filtered and normalized eigenvectors and eigenvalues\n    filtered_eigenvectors = eigenvectors[:, (eigenvalues > positive_cutoff) & (eigenvalues > negative_cutoff)]\n    decomposed_matrix = np.dot(filtered_eigenvectors * normalized_eigenvalues[np.newaxis, :], filtered_eigenvectors.T)\n\n    # Return transposed decomposed matrix and the largest eigenvalue\n    return decomposed_matrix.T, largest_eigenvalue", "CodeBleu_score": 28.53047156538161}, {"code": "import numpy as np\nfrom scipy.linalg import eigh\n\ndef _decompose(self, P):\n    \"\"\"\n    This function performs matrix decomposition on matrix P using its eigenvalues and eigenvectors.\n    It computes the eigenvalues and eigenvectors using the linalg.eigh function from scipy.linalg.\n    It identifies the largest eigenvalue and calculates a condition number based on this eigenvalue, the shape of P, and machine epsilon.\n    The function checks if all eigenvalues are above the negative cutoff and filters out those below the positive cutoff.\n    It normalizes the eigenvalues so that the maximum eigenvalue is set to 1.\n    Finally, it returns the transposed decomposed matrix and the largest eigenvalue.\n    \"\"\"\n    \n    # Using scipy.linalg.eigh to compute the eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = eigh(P)\n    largest_eigenvalue = max(eigenvalues)\n\n    # Calculate condition number based on largest eigenvalue\n    machine_epsilon = np.finfo(float).eps\n    condition_number = largest_eigenvalue / (machine_epsilon * np.min([P.shape[0], P.shape[1]]))\n\n    # Filter and normalize eigenvalues\n    negative_cutoff = -1e-10  # define the negative cutoff (this could be adjusted)\n    positive_cutoff = 1e-10  # define the positive cutoff (this could be adjusted)\n    filtered_indices = [i for i, value in enumerate(eigenvalues) if value > negative_cutoff and value > positive_cutoff]\n    filtered_eigenvalues = eigenvalues[filtered_indices]\n    filtered_eigenvectors = eigenvectors[:, filtered_indices]\n\n    # Normalize the eigenvalues so that the maximum eigenvalue is set to 1\n    normalized_eigenvalues = filtered_eigenvalues / largest_eigenvalue\n\n    # Construct new matrix with filtered and normalized eigenvalues on the diagonal\n    D_normalized = np.diag(normalized_eigenvalues)\n    decomposed_matrix = np.dot(filtered_eigenvectors, np.dot(D_normalized, filtered_eigenvectors.T))\n\n    # Provide the decomposed matrix transposed\n    return decomposed_matrix.T, largest_eigenvalue, condition_number", "CodeBleu_score": 31.809124667272464}]}
{"instance_id": "20c3f5d05d79", "generation_result": [{"code": "import numpy as np\n\nclass Model:\n    def __init__(self, initial_coefs):\n        self.coef = np.array(initial_coefs)\n    \n    def _update_coef(self, coef, D):\n        \"\"\"\n        This function updates the model's coefficients by selecting elements of the input\n        coefficient array that are greater than a small threshold and then scaling these\n        selected coefficients by corresponding elements in another array. The resulting\n        product is stored in the model's coefficient attribute.\n\n        Parameters:\n            coef (numpy.array): The input coefficient array.\n            D (numpy.array): The scaling array.\n        \"\"\"\n        if not isinstance(coef, np.ndarray) or not isinstance(D, np.ndarray):\n            raise ValueError(\"Both coef and D should be numpy arrays.\")\n\n        # Check that coef and D are of the same shape\n        if coef.shape != D.shape:\n            raise ValueError(\"coef and D must have the same shape.\")\n\n        # Define a small threshold\n        threshold = 1e-5\n\n        # Select coefficients that are greater than the threshold\n        selection_mask = coef > threshold\n\n        # Calculate the new coefficients\n        new_coefs = np.where(selection_mask, coef * D, self.coef)\n\n        # Update the model's coefficients\n        self.coef = new_coefs\n\n# Example usage:\nmodel = Model([0.0, 0.1, 0.5])\nmodel._update_coef(np.array([0.01, 0.02, 0.6]), np.array([5, 10, 2]))\nprint(model.coef)", "CodeBleu_score": 22.507153318552337}, {"code": "class Model:\n    def __init__(self):\n        self.coefficients = None\n\n    def _update_coef(self, coef, D):\n        \"\"\"\n        The function _update_coef updates the model's coefficients by selecting elements of the input coefficient array\n        that are greater than a small threshold and then scaling these selected coefficients by corresponding elements in\n        another array D. The resulting product is stored in the model's coefficient attribute.\n\n        Parameters:\n            coef (numpy.array): The input array of coefficients.\n            D (numpy.array): The array used for scaling the selected coefficients.\n\n        Returns:\n            None\n        \"\"\"\n        import numpy as np\n        \n        # Define a small threshold\n        threshold = 0.01  # This threshold can be adjusted based on specific case requirements.\n        \n        # Select coefficients larger than the threshold\n        selected_coef = coef[coef > threshold]\n        \n        # Ensure D is appropriate for element-wise multiplication with selected coefficients\n        if selected_coef.shape != D.shape:\n            raise ValueError(\"Shape of D does not match the shape of the filtered coefficients array.\")\n        \n        # Scale the selected coefficients\n        scaled_coef = selected_coef * D[coef > threshold]\n        \n        # Store the result into the model's coefficients attribute\n        self.coefficients = scaled_coef", "CodeBleu_score": 14.887034848862255}, {"code": "def _update_coef(self, coef, D):\n    \"\"\"\n    The function _update_coef updates the model's coefficients by selecting elements of the input\n    coefficient array that are greater than a small threshold and then scaling these selected\n    coefficients by corresponding elements in another array D. The resulting product is stored\n    in the model's coefficient attribute.\n    \n    Args:\n    - coef (np.ndarray): The array of coefficients from which we'll select based on a threshold.\n    - D (np.ndarray): The array used to scale the selected coefficients from coef.\n    \n    This function updates `self.coefficients`.\n    \"\"\"\n    import numpy as np\n    \n    # Define a threshold for coefficient selection (this value can be adjusted)\n    threshold = 0.1\n    \n    # Select coefficients greater than the threshold\n    high_coef_indices = coef > threshold\n    \n    # Filter coefficients from `coef` and corresponding scaling factors from `D`\n    selected_coefs = coef[high_coef_indices]\n    scaling_factors = D[high_coef_indices]\n    \n    # Scale the selected coefficients by the corresponding elements from `D`\n    updated_coefs = selected_coefs * scaling_factors\n    \n    # Assuming this class has an attribute `coefficients` meant to store the updated values\n    self.coefficients = updated_coefs", "CodeBleu_score": 18.47297354181165}]}
{"instance_id": "37c347d00d35", "generation_result": [{"code": "import numpy as np\n\ndef gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs):\n    \"\"\"This function computes the gradient for a Cox proportional hazards model.\n    It takes true event data, raw predictions, and optional sample weights.\n    The function calculates the negative gradient of the Cox PH model using event type and time.\n    If sample weights are provided, it applies them to the computed gradient.\n    Finally, it returns the weighted or unweighted gradient.\"\"\"\n\n    # Convert raw predictions to risk scores\n    risk_scores = np.exp(raw_prediction)\n\n    # Sort by survival times\n    order = np.argsort(y_true[:, 0])\n    risk_scores = risk_scores[order]\n    events = y_true[order, 1].astype(np.bool)\n\n    # Compute the hazard ratio\n    hazard_ratio = np.cumsum(risk_scores[::-1])[::-1]\n\n    # Get the denominator of the gradient formula for each sample\n    if sample_weight is not None:\n        weighted_risk_scores = risk_scores * sample_weight[order]\n        weighted_hazard_ratio = np.cumsum(weighted_risk_scores[::-1])[::-1]\n        risk_set_sum = weighted_hazard_ratio\n    else:\n        risk_set_sum = hazard_ratio\n\n    # Calculate the risk set average\n    risk_set_avg = risk_scores / risk_set_sum\n\n    # Gradient: observed - expected\n    grad = events - risk_set_avg\n\n    # If sample weights are provided, apply them\n    if sample_weight is not None:\n        grad *= sample_weight[order]\n\n    return grad", "CodeBleu_score": 40.979457281337176}, {"code": "import numpy as np\n\ndef gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs):\n    \"\"\"\n    This function computes the gradient for a Cox proportional hazards model.\n    It takes true event data, raw predictions, and optional sample weights.\n    The function calculates the negative gradient of the Cox PH model using event type and time.\n    If sample weights are provided, it applies them to the computed gradient.\n    Finally, it returns the weighted or unweighted gradient.\n    \"\"\"\n    event_times = y_true[:, 0]\n    event_observed = y_true[:, 1]\n    n_samples = len(event_times)\n    \n    exp_risk = np.exp(raw_prediction)  # Calculate exp(beta^T X)\n    gradient = np.zeros(n_samples)\n    \n    for i in range(n_samples):\n        time_i = event_times[i]\n        risk_set = (event_times >= time_i)  # All samples at risk at time t_i\n        risk_sum = np.sum(exp_risk[risk_set])  # Sum of exp risks in the risk set\n        gradient[i] = event_observed[i] - (exp_risk[i] / risk_sum)\n    \n    if sample_weight is not None:\n        gradient *= sample_weight  # Apply weights if provided\n    \n    return -gradient  # Negative gradient", "CodeBleu_score": 42.97066071235324}, {"code": "import numpy as np\n\ndef gradient(self, y_true, raw_predictions, sample_weight=None, **kwargs):\n    \"\"\"\n    This function computes the gradient for a Cox proportional hazards model.\n    It takes true event data, raw predictions, and optional sample weights.\n    The function calculates the negative gradient of the Cox PH model using event type and time.\n    If sample weights are provided, it applies them to the computed gradient.\n    Finally, it returns the weighted or unweighted gradient.\n    \n    :param y_true: A structured array with fields 'T' and 'E', where 'T' represents survival times \n                   and 'E' represents event occurrence (1 if observed, 0 if censored).\n    :param raw_predictions: Array of linear predictors from the Cox model.\n    :param sample_weight: Optional array of weights for the instances.\n    :returns: Array of gradients.\n    \"\"\"\n    \n    # Sort data by survival times, descending\n    order = np.argsort(-y_true['T'])\n    raw_predictions = raw_predictions[order]\n    event_occurred = y_true['E'][order]\n    survival_times = y_true['T'][order]\n    \n    # Form the risk set for each instance\n    risk_set_sizes = np.cumsum(np.flip(np.cumsum(np.flip(event_occurred))))\n    \n    # Compute the exponential of the predictions\n    exp_pred = np.exp(raw_predictions)\n    \n    # Cumulative sums of predictions over the risk set\n    cum_sum_exp_pred = np.cumsum(exp_pred)\n    \n    # Compute the sum of exponentiated predictions for risk sets\n    exp_cum_sum_risk = np.array([cum_sum_exp_pred[-1]] - np.cumsum(exp_pred)[:-1])\n    \n    # Compute denominators for hazard ratios\n    hazard_ratios_denom = exp_cum_sum_risk / risk_set_sizes\n    \n    # Calculate the gradient as the negative of event indicator minus hazard ratios\n    gradient_negative = event_occurred - (exp_pred / hazard_ratios_denom)\n    \n    # If sample weights are specified, apply them\n    if sample_weight is not None:\n        sample_weight = sample_weight[order]\n        gradient_negative *= sample_weight\n    \n    return gradient_negative", "CodeBleu_score": 45.790113598357394}]}
{"instance_id": "6be432d27e52", "generation_result": [{"code": "import numpy as np\n\ndef _compute_missing_values_in_feature_mask(self, X, estimator_name=None):\n    \"\"\"\n    The function _compute_missing_values_in_feature_mask checks for missing values in a feature matrix X.\n    It takes an optional estimator name and sets default parameters. If the dataset does not support missing values,\n    it verifies that all elements are finite and returns None if true. It calculates the overall sum of X while ignoring\n    overflow errors. If the sum is not finite, it raises a ValueError for any infinite elements. If the sum is finite\n    and not NaN, it returns None, indicating no missing values. If there are NaN values, it returns a mask indicating\n    which features have missing values.\n    \n    Parameters:\n        X (numpy array): The feature matrix to check for missing values.\n        estimator_name (str, optional): The name of the estimator for which the parameter check is done.\n\n    Returns:\n        numpy array or None: A mask indicating which features have missing values, or None if no missing values exist.\n    \"\"\"\n    # Handle datatype if required based on estimator specific needs\n    if estimator_name:\n        # Setup default parameters or perform custom checks based on the estimator\n        pass\n\n    # Check if all elements in X are finite (not inf and not NaN)\n    if np.isfinite(X).all():\n        # Check for overflow errors using np.seterr\n        old_settings = np.seterr(over='ignore')  # Ignore overflows\n        sum_X = np.sum(X)  # Sum up all elements in X\n        np.seterr(**old_settings)  # Restore previous setting\n\n        # If the sum is not finite, raise ValueError\n        if not np.isfinite(sum_X):\n            raise ValueError(\"Infinite element detected in dataset when summing.\")\n        else:\n            # If the sum is finite and not NaN, return None indicating no missing values\n            return None\n\n    # If there are NaN values in X\n    if np.isnan(X).any():\n        # Return a mask indicating which features have missing values\n        return np.isnan(X).any(axis=0)  # Check along columns if any NaN\n    \n    # No NaN present but verify if infinite values are present\n    if np.isinf(X).any():\n        raise ValueError(\"Infinite element detected in dataset.\")\n\n    return None  # A catch-all return if none of the conditions triggered a return earlier", "CodeBleu_score": 34.93512485067794}, {"code": "import numpy as np\n\ndef _compute_missing_values_in_feature_mask(self, X, estimator_name=None):\n    \"\"\"\n    The function _compute_missing_values_in_feature_mask checks for missing values in a feature matrix X.\n    It takes an optional estimator name and sets default parameters. If the dataset does not support\n    missing values, it verifies that all elements are finite and returns None if true. It calculates\n    the overall sum of X while ignoring overflow errors. If the sum is not finite, it raises a\n    ValueError for any infinite elements. If the sum is finite and not NaN, it returns None,\n    indicating no missing values. If there are NaN values, it returns a mask indicating which\n    features have missing values.\n    \n    Parameters:\n        X (np.ndarray): The input feature matrix.\n        estimator_name (str, optional): The name of the estimator. Defaults to None.\n\n    Returns:\n        np.ndarray or None: A boolean mask indicating columns with missing values or None if there are no missing values.\n    \"\"\"\n    # Check if the matrix supports missing values (contains NaN or inf)\n    if not np.isfinite(X).all():\n        is_finite_sum = np.isfinite(np.sum(X, where=np.isfinite(X), initial=0, dtype=np.float64))\n        if not is_finite_sum:\n            # If sum is not finite, it means there are infinite or NaN values in the matrix\n            raise ValueError(\"Input contains infinity or NaN values.\")\n\n        # If there are NaN values, create a mask of columns containing NaNs\n        missing_values_mask = np.isnan(X).any(axis=0)\n        if missing_values_mask.any():\n            return missing_values_mask\n        else:\n            return None\n    else:\n        # All elements are finite\n        return None", "CodeBleu_score": 38.99099326227707}, {"code": "import numpy as np\n\ndef _compute_missing_values_in_feature_mask(self, X, estimator_name=None):\n    \"\"\"\n    Checks for missing values in a feature matrix X. It takes an optional estimator name and sets default parameters.\n    If the dataset does not support missing values, it verifies that all elements are finite and returns None if true.\n    It calculates the overall sum of X while ignoring overflow errors. If the sum is not finite, it raises a ValueError \n    for any infinite elements. If the sum is finite and not NaN, it returns None, indicating no missing values. If there\n    are NaN values, it returns a mask indicating which features have missing values.\n\n    Parameters:\n    - X : numpy.ndarray\n        The feature matrix where each row represents a sample and each column represents a feature.\n\n    - estimator_name : str, optional\n        Name of the estimator that is invoking this function. It might be used to set default parameters\n        specifically for that estimator. This is not implemented here, but can be added if necessary.\n\n    Returns:\n    - mask : numpy.ndarray or None\n        If there are NaN values, returns a boolean mask indicating which features (columns) contain NaNs.\n        Otherwise, it returns None.\n    \"\"\"\n    \n    # Check if X is an array and convert it if necessary\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    \n    # Check if there are infinite values and raise an error if found\n    if not np.isfinite(X.sum()):\n        raise ValueError(\"Input contains infinity or a value too large for dtype('float64').\")\n    \n    # Check if there are any NaN values in the dataset\n    if np.isnan(X).any():\n        # Return a mask indicating which features contain NaNs\n        return np.isnan(X).any(axis=0)\n    else:\n        # Return None if no NaN values are found\n        return None", "CodeBleu_score": 22.649989578502524}]}
{"instance_id": "dc5bda895a07", "generation_result": [{"code": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils import check_array\nfrom lifelines import KaplanMeierFitter\nimport numpy as np\n\nclass SurvivalModel:\n    def __init__(self):\n        self.model = DecisionTreeClassifier()  # Hypothetical decision tree model.\n        self.is_fitted = False\n\n    def fit(self, X, y):\n        \"\"\" Fit the decision tree model. \"\"\"\n        self.model.fit(X, y)\n        self.is_fitted = True\n\n    def predict_survival_function(self, X, check_input=True, return_array=False):\n        \"\"\"\n        This function predicts a survival function based on the input data X.\n        It first checks the model's memory and verifies that the model is fitted.\n        The input data X is validated and possibly converted to a sparse matrix.\n        The function then uses a decision tree to make predictions, extracting the second element from the predictions.\n        If return_array is True, it returns this array of predictions directly.\n        Otherwise, it converts the array to a step function based on unique times and returns the step function.\n        \"\"\"\n        if not self.is_fitted:\n            raise Exception(\"Model is not fitted yet.\")\n        \n        if check_input:\n            X = check_array(X)  # Validate input data\n\n        # Make predictions using the decision tree classifier\n        predictions = self.model.predict_proba(X)[:, 1]\n\n        if return_array:\n            return predictions\n\n        # Convert predictions into a step function (survival function using Kaplan-Meier)\n        kmf = KaplanMeierFitter()\n        event_times = np.linspace(min(predictions), max(predictions), 10)  # Example event times\n        censoring = np.ones_like(predictions)  # Assuming everyone has the event\n        kmf.fit(event_times, event_observed=censoring)\n        \n        return kmf.survival_function_\n\n# Example usage:\n# X_train, y_train, and X_test should be defined elsewhere\n# survival_model = SurvivalModel()\n# survival_model.fit(X_train, y_train)\n# survival_pred = survival_model.predict_survival_function(X_test)", "CodeBleu_score": 35.53400513449893}, {"code": "import numpy as np\nfrom sklearn.utils import check_array\nfrom sklearn.tree import DecisionTreeClassifier\n\nclass SurvivalModel:\n    def __init__(self):\n        self.tree = DecisionTreeClassifier()\n        self.is_fitted = False\n        self.unique_times = None\n\n    def fit(self, X, y):\n        X = check_array(X)\n        self.tree.fit(X, y)\n        self.unique_times = np.unique(y)\n        self.is_fitted = True\n\n    def predict_survival_function(self, X, check_input=True, return_array=False):\n        \"\"\"\n        This function predicts a survival function based on the input data X.\n        It first checks the model's memory and verifies that the model is fitted.\n        The input data X is validated and possibly converted to a sparse matrix.\n        The function then uses a decision tree to make predictions, extracting the second element from the predictions.\n        If return_array is True, it returns this array of predictions directly.\n        Otherwise, it converts the array to a step function based on unique times and returns the step function.\n        \"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Model is not fitted yet. Fit the model before prediction.\")\n\n        if check_input:\n            X = check_array(X, accept_sparse='csr')\n\n        probabilities = self.tree.predict_proba(X)\n        prediction_second_element = probabilities[:, 1]  # Assuming binary classification and requiring second element\n\n        if return_array:\n            return prediction_second_element\n\n        # Create step function representation\n        step_function = np.zeros((len(X), len(self.unique_times)))\n        for i, time in enumerate(self.unique_times):\n            step_function[:, i] = (prediction_second_element >= time).astype(int)\n\n        return step_function\n\n# Example usage:\n# model = SurvivalModel()\n# model.fit(X_train, y_train) # where X_train and y_train are appropriate training data arrays\n# survival_function = model.predict_survival_function(X_test) # where X_test is the appropriates test data array", "CodeBleu_score": 36.12997344571628}, {"code": "from sklearn.utils.validation import check_is_fitted, check_array\nfrom scipy.sparse import csr_matrix\nimport numpy as np\n\ndef predict_survival_function(self, X, check_input=True, return_array=False):\n    \"\"\"\n    This function predicts a survival function based on the input data X.\n    It first checks the model's memory and verifies that the model is fitted.\n    The input data X is validated and possibly converted to a sparse matrix.\n    The function then uses a decision tree to make predictions, extracting the second element from the predictions.\n    If return_array is True, it returns this array of predictions directly.\n    Otherwise, it converts the array to a step function based on unique times and returns the step function.\n    \n    Args:\n    - X (array-like, sparse matrix): Input data of shape (n_samples, n_features).\n    - check_input (bool, optional): Whether to check input matrices. Defaults to True.\n    - return_array (bool, optional): Whether to return predictions as a numpy array. Defaults to False.\n    \n    Returns:\n    - array or tuple: An array of predictions if return_array is True, or a step function otherwise.\n    \"\"\"\n    \n    # Check if fitted\n    check_is_fitted(self, 'tree_')\n    \n    # Validate input X\n    if check_input:\n        if sp.issparse(X):\n            X = csr_matrix(X)\n        else:\n            X = check_array(X)\n    \n    # Predict using the trained decision tree\n    predictions = self.tree_.predict_proba(X)\n    \n    # Extract the second element from the predictions\n    survival_chances = predictions[:, 1]\n    \n    if return_array:\n        return survival_chances\n    \n    # Get unique times and transform predictions to a step function\n    time_points = np.unique(self.times_)  # Assuming times_ are learned during fitting\n    step_function = np.zeros((len(X), len(time_points)))\n    \n    for i, time in enumerate(time_points):\n        step_function[:, i] = survival_chances * (self.times_ <= time)\n    \n    return time_points, step_function", "CodeBleu_score": 40.57655593816637}]}
{"instance_id": "84ae915b5933", "generation_result": [{"code": "def _pre_fit(self, X, y, center=True, scale=True):\n    import numpy as np\n\n    # Check if size of X and y is sufficient and they are of proper shape\n    if X.ndim != 2 or y.ndim != 1:\n        raise ValueError(\"Invalid shape for X or y.\")\n    if len(X) != len(y):\n        raise ValueError(\"X and y must be of the same length.\")\n    if len(X) < 5:  # Assuming at least 5 samples are required\n        raise ValueError(\"Insufficient data. There must be at least 5 samples.\")\n\n    # Ensuring that the data is float64 for computational stability\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=[('time', float), ('event', bool)])\n\n    # Centers the data by subtracting the mean\n    if center:\n        feature_means = X.mean(axis=0)\n        X = X - feature_means\n    else:\n        feature_means = np.zeros(X.shape[1])  # Feature means are zero if not centered\n\n    # Scales the feature matrix to unit variance\n    if scale:\n        scales = X.std(axis=0)\n        # Avoid division by zero in case of zero variance\n        scales[scales == 0] = 1.0\n        X /= scales\n    else:\n        scales = np.ones(X.shape[1])  # Scales are one if not scaled\n\n    # Sorting data by survival time in descending order\n    sorted_indices = np.argsort(y['time'])[::-1]  # Descending order\n    X = X[sorted_indices]\n    y = y[sorted_indices]\n\n    # Ensuring that the ndarray is Fortran-contiguous for computational libraries that require it\n    X = np.asfortranarray(X)\n\n    # Return the processed feature matrix, event numbers, survival times, feature means, and scales\n    events = y['event']\n    times = y['time']\n    return X, events, times, feature_means, scales", "CodeBleu_score": 33.19650727794118}, {"code": "import numpy as np\n\nclass SurvivalAnalysis:\n    \n    def __init__(self, normalize=False):\n        self.normalize = normalize\n    \n    def _pre_fit(self, X, y):\n        \"\"\"Preprocessing input data for survival analysis.\n        \n        Parameters:\n            X (np.ndarray): The feature matrix.\n            y (np.ndarray): Array consisting of survival time and event observed status.\n            \n        Returns:\n            tuple: Tuple containing:\n                   - np.ndarray: Processed feature matrix (X)\n                   - np.ndarray: Event indicator (1 if event occurred, 0 otherwise)\n                   - np.ndarray: Survival times\n                   - np.ndarray: Feature means\n                   - np.ndarray: Feature scales (only if normalization is performed)\n        \"\"\"\n        \n        # Ensures there are enough samples and correct data type\n        assert X.shape[0] > 0, \"No samples provided.\"\n        X = X.astype(np.float64)\n        \n        # Separates survival times and events\n        survival_times = y[:, 0]\n        events = y[:, 1].astype(int)\n        \n        # Centers the feature matrix\n        feature_means = X.mean(axis=0)\n        X_centered = X - feature_means\n        \n        if self.normalize:\n            feature_std_dev = X_centered.std(axis=0)\n            feature_scales = np.where(feature_std_dev == 0, 1, feature_std_dev)\n            X_normalized = X_centered / feature_scales\n        else:\n            X_normalized = X_centered\n            feature_scales = None\n        \n        # Sort data by survival times in descending order\n        idx_sorted = np.argsort(survival_times)[::-1]\n        X_sorted = X_normalized[idx_sorted]\n        survival_times_sorted = survival_times[idx_sorted]\n        events_sorted = events[idx_sorted]\n        \n        # Ensure Fortran-contiguous array for computational efficiency in certain libraries\n        X_final = np.asfortranarray(X_sorted)\n        \n        return X_final, events_sorted, survival_times_sorted, feature_means, feature_scales\n\n# Example of usage:\n# X = np.random.rand(100, 10)\n# y = np.column_stack((np.random.exponential(scale=10, size=100), np.random.randint(0, 2, size=100)))\n# sa = SurvivalAnalysis(normalize=True)\n# result = sa._pre_fit(X, y)", "CodeBleu_score": 29.55003715231922}, {"code": "import numpy as np\n\ndef _pre_fit(self, X, y, normalize=False, min_samples=10):\n    \"\"\"\n    Preprocesses input data for survival analysis. \n\n    Parameters:\n        X (np.ndarray): The feature matrix.\n        y (np.ndarray): The array containing survival information,\n                        expected to be a structured array or a 2D array\n                        where the first column is the event indicator (1 if event occurred, 0 otherwise)\n                        and the second column is the survival time.\n        normalize (bool, optional): If True, scales the features to unit variance.\n        min_samples (int, optional): Minimum number of samples required.\n\n    Returns:\n        tuple: Tuple containing:\n            - X_centered (np.ndarray): The centered (and optionally normalized) feature matrix.\n            - events (np.ndarray): Array of event indicators.\n            - times (np.ndarray): Array of survival times.\n            - mean_X (np.ndarray): Mean of each feature in the original feature matrix.\n            - scale_X (np.ndarray or None): Scale of each feature, returned if `normalize` is True.\n    \"\"\"\n    # Check if enough samples are available\n    if X.shape[0] < min_samples:\n        raise ValueError(f\"Number of samples in X should be at least {min_samples}, but got {X.shape[0]}.\")\n\n    # Checking the data type and converting if necessary\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    # Separate the survival times and event indicators\n    if y.ndim == 2 and y.shape[1] >= 2:\n        events = y[:, 0]\n        times = y[:, 1]\n    else:\n        raise ValueError(\"y should be a 2D array with the first column as event indicators and the second as times.\")\n\n    # Center the data\n    mean_X = np.mean(X, axis=0)\n    X_centered = X - mean_X\n\n    scale_X = None\n    if normalize:\n        # Normalize the data\n        std_dev_X = np.std(X_centered, axis=0)\n        std_dev_X[std_dev_X == 0] = 1.0  # Avoid division by zero\n        X_centered = X_centered / std_dev_X\n        scale_X = std_dev_X\n\n    # Sort by survival time in descending order\n    sorted_indices = np.argsort(times)[::-1]\n    X_centered = X_centered[sorted_indices]\n    events = events[sorted_indices]\n    times = times[sorted_indices]\n\n    # Ensure Fortran contiguous array to optimize further processing\n    X_centered = np.asfortranarray(X_centered)\n    \n    return X_centered, events, times, mean_X, scale_X", "CodeBleu_score": 30.58017429385937}]}
{"instance_id": "c70bb013f85f", "generation_result": [{"code": "import numpy as np\n\ndef _check_alphas(self):\n    \"\"\"\n    This function checks and prepares the alpha values used for a calculation.\n    It determines if a new alpha path needs to be created based on whether the alphas attribute is None.\n    If a new path is required, it validates that the number of alphas is a positive integer and initializes an array of the specified length.\n    If an existing path is used, it ensures the alpha values are in a 1D array, are finite, and non-negative.\n    The function returns the alpha values and a boolean indicating if a new path was created.\n    \"\"\"\n    new_path_created = False\n\n    # Check if the alphas attribute is None, indicating that a new path needs to be created\n    if self.alphas is None:\n        # Define default number of alphas if not specified\n        default_num_alphas = 100\n        self.alphas = np.linspace(0.0, 1.0, default_num_alphas)\n        new_path_created = True\n    else:\n        # Ensure self.alphas is a numpy array\n        self.alphas = np.asarray(self.alphas)\n\n        # Check that the array is one-dimensional\n        if self.alphas.ndim != 1:\n            raise ValueError(\"Alphas must be a one-dimensional array\")\n\n        # Check that all alphas are non-negative and finite\n        if not np.all(np.isfinite(self.alphas)) or np.any(self.alphas < 0):\n            raise ValueError(\"All alpha values must be non-negative and finite\")\n    \n    return self.alphas, new_path_created", "CodeBleu_score": 33.88482070456824}, {"code": "import numpy as np\n\nclass AlphaHandler:\n    def __init__(self, alphas=None, num_alphas=None):\n        self.alphas = alphas\n        self.num_alphas = num_alphas\n\n    def _check_alphas(self):\n        \"\"\"This function checks and prepares the alpha values used for a calculation.\n        It determines if a new alpha path needs to be created based on whether the alphas attribute is None.\n        If a new path is required, it validates that the number of alphas is a positive integer and initializes an array of the specified length.\n        If an existing path is used, it ensures the alpha values are in a 1D array, are finite, and non-negative.\n        The function returns the alpha values and a boolean indicating if a new path was created.\n\n        Returns:\n            tuple: Tuple containing:\n            - np.ndarray: The array of alpha values.\n            - bool: True if a new path was created, False otherwise.\n        \"\"\"\n        new_path_created = False\n        \n        if self.alphas is None:\n            if self.num_alphas is None or self.num_alphas <= 0:\n                raise ValueError(\"num_alphas must be a positive integer when alphas is None\")\n            self.alphas = np.linspace(0, 1, self.num_alphas)  # Simple initialization, can be adjusted\n            new_path_created = True\n        else:\n            self.alphas = np.asarray(self.alphas).flatten()\n            if np.any(np.isnan(self.alphas)) or np.any(self.alphas < 0):\n                raise ValueError(\"All alpha values must be non-negative and finite\")\n        \n        return self.alphas, new_path_created", "CodeBleu_score": 37.86095359761983}, {"code": "import numpy as np\n\ndef _check_alphas(self):\n    \"\"\"\n    This function checks and prepares the alpha values used for a calculation.\n    It determines if a new alpha path needs to be created based on whether the alphas attribute is None.\n    If a new path is required, it validates that the number of alphas is a positive integer and initializes an array of the specified length.\n    If an existing path is used, it ensures the alpha values are in a 1D array, are finite, and non-negative.\n    The function returns the alpha values and a boolean indicating if a new path was created.\n    \"\"\"\n    new_path_created = False\n    \n    if self.alphas is None:\n        # A new alpha path is needed. Here for example, we create a default array of alphas\n        # Define default length, could be set as an attribute or parameter\n        default_length = 10  # for example purposes\n        self.alphas = np.linspace(0.1, 1.0, num=default_length)\n        new_path_created = True\n    else:\n        # Existing path is used, ensure it is a 1D array of non-negative and finite values\n        self.alphas = np.asarray(self.alphas)  # Ensure it is an array\n        \n        if self.alphas.ndim != 1:\n            raise ValueError(\"Alphas should be in a 1D array.\")\n        \n        if not np.all(np.isfinite(self.alphas)):\n            raise ValueError(\"All alpha values must be finite.\")\n        \n        if not np.all(self.alphas >= 0):\n            raise ValueError(\"All alpha values must be non-negative.\")\n\n    return self.alphas, new_path_created", "CodeBleu_score": 32.93737794986796}]}
{"instance_id": "7b54a0554ba4", "generation_result": [{"code": "import numpy as np\n\ndef nelson_aalen_estimator(event, time):\n    \"\"\"The function nelson_aalen_estimator computes the Nelson-Aalen estimator for cumulative hazard rates. It takes event indicators and corresponding time values as inputs, validates them, and ensures they have consistent lengths. It calculates unique times, the number of events, and the number of subjects at risk at each time point. The function then computes the cumulative hazard by summing the ratio of events to subjects at risk over time. It returns the unique times and the cumulative hazard values.\n\n    Parameters:\n        event (array-like): An array indicating whether an event occurred (1) or not occurred/censored (0).\n        time (array-like): An array of times at which the events were observed.\n\n    Returns:\n        tuple: A tuple containing:\n            - unique_times (numpy.ndarray): Unique event times.\n            - cumulative_hazard (numpy.ndarray): Cumulative hazard estimated at each unique event time.\n\n    Example:\n        event = np.array([1, 1, 0, 1, 0])\n        time = np.array([1, 3, 2, 5, 2])\n        unique_times, cumulative_hazard = nelson_aalen_estimator(event, time)\n    \"\"\"\n    # Ensure input validity\n    event = np.asarray(event)\n    time = np.asarray(time)\n\n    if len(event) != len(time):\n        raise ValueError(\"Event and time arrays should have the same length.\")\n    \n    # Sort times and corresponding events\n    ordering = np.argsort(time)\n    sorted_times = time[ordering]\n    sorted_events = event[ordering]\n    \n    # Get unique times and the count of events at each time point\n    unique_times, indices, counts = np.unique(sorted_times, return_index=True, return_counts=True)\n    \n    # Accumulate the event occurrences at the unique times\n    event_counts = np.zeros(len(unique_times))\n    last_index = 0\n    for i, index in enumerate(indices):\n        if i == len(indices) - 1:\n            event_counts[i] = np.sum(sorted_events[index:])\n        else:\n            next_index = indices[i + 1]\n            event_counts[i] = np.sum(sorted_events[index:next_index])\n    \n    # Calculate the number of individuals at risk at each unique time\n    n_risk = np.zeros(len(unique_times))\n    n_total = len(time)\n    for i, index in enumerate(indices):\n        n_risk[i] = n_total\n        n_total -= counts[i]  # reduce the risk set by the number of times at each unique point\n    \n    # Calculate cumulative hazard\n    cumulative_hazard = np.cumsum(event_counts / n_risk)\n    \n    return unique_times, cumulative_hazard", "CodeBleu_score": 29.028337640994927}, {"code": "import numpy as np\n\ndef nelson_aalen_estimator(event, time):\n    \"\"\"\n    The function `nelson_aalen_estimator` computes the Nelson-Aalen estimator for cumulative hazard rates. \n    It takes event indicators and corresponding time values as inputs, validates them, and ensures they have consistent lengths. \n    It calculates unique times, the number of events, and the number of subjects at risk at each time point. \n    The function then computes the cumulative hazard by summing the ratio of events to subjects at risk over time. \n    It returns the unique times and the cumulative hazard values.\n    \n    Parameters:\n        event (np.array): An array of binary integers (0 or 1), where 1 indicates the event of interest has occurred.\n        time (np.array): An array of time points corresponding to each event indicator.\n        \n    Returns:\n        unique_times (np.array): The sorted unique event times.\n        cum_hazard (np.array): The cumulative hazard calculated at each unique event time.\n    \"\"\"\n    \n    if len(event) != len(time):\n        raise ValueError(\"The lengths of 'event' and 'time' must be identical.\")\n    \n    # Sort time and event according to time\n    sorted_indices = np.argsort(time)\n    time_sorted = time[sorted_indices]\n    event_sorted = event[sorted_indices]\n    \n    # Calculate unique times and indices to them\n    unique_times, indices, counts = np.unique(time_sorted, return_index=True, return_counts=True)\n    \n    # Calculate the number of events and at risk for each unique time\n    events_at_unique_times = np.zeros_like(unique_times)\n    at_risk = np.zeros_like(unique_times)\n    n = len(time_sorted)\n    last_index = 0\n    \n    for i, index in enumerate(indices):\n        events_at_unique_times[i] = np.sum(event_sorted[last_index:index + counts[i]])\n        at_risk[i] = n - last_index\n        last_index = index + counts[i]\n    \n    # Calculate cumulative hazard\n    hazard_rate = events_at_unique_times / at_risk\n    cum_hazard = np.cumsum(hazard_rate)\n    \n    return unique_times, cum_hazard", "CodeBleu_score": 21.003344840179068}, {"code": "import numpy as np\n\ndef nelson_aalen_estimator(event, time):\n    \"\"\"\n    Computes the Nelson-Aalen estimator for cumulative hazard rates from survival data.\n    \n    Parameters:\n    event (array-like): Array of event indicators (1 if event occurred, 0 if censored).\n    time (array-like): Array of times corresponding to each entry in event.\n    \n    Returns:\n    tuple:\n        unique_times (numpy.ndarray): Sorted array of unique times at which events or censoring occurred.\n        cumulative_hazard (numpy.ndarray): Cumulative hazard estimate at each unique time.\n    \n    Raises:\n    ValueError: If 'event' and 'time' have different lengths.\n    \n    Examples:\n    >>> event = [1, 1, 0, 1]\n    >>> time = [2, 3, 4, 5]\n    >>> nelson_aalen_estimator(event, time)\n    (array([2, 3, 5]), array([0.5       , 0.83333333, 1.33333333]))\n    \"\"\"\n    # Ensure event and time arrays are numpy arrays\n    event = np.asarray(event)\n    time = np.asarray(time)\n    \n    # Validate input sizes\n    if (len(event) != len(time)):\n        raise ValueError(\"The length of 'event' and 'time' arrays must be the same.\")\n    \n    # Prepare the data, and sort by time\n    order = np.argsort(time)\n    sorted_time = time[order]\n    sorted_event = event[order]\n    \n    # Calculate unique times and their indices, counts\n    unique_times, indices, counts = np.unique(sorted_time, return_index=True, return_counts=True)\n\n    # Number of subjects at risk at each time point\n    n_at_risk = np.zeros_like(unique_times, dtype=float)\n    n_at_risk[0] = len(event)\n    n_at_risk[1:] = n_at_risk[:-1] - np.add.accumulate(counts[:-1])\n\n    # Number of events at each unique time point\n    event_counts = np.array([np.sum(sorted_event[indices[i]:indices[i] + counts[i]]) for i in range(len(indices))])\n    \n    # Calculate hazard at each unique time\n    hazard = event_counts / n_at_risk\n    \n    # Cumulative hazard calculation\n    cumulative_hazard = np.cumsum(hazard)\n    \n    return unique_times, cumulative_hazard\n\n# Example usage\nif __name__ == \"__main__\":\n    events = [1, 1, 0, 1, 0, 1]\n    times = [3, 3, 4, 4, 5, 6]\n    result = nelson_aalen_estimator(events, times)\n    print(\"Unique Times:\", result[0])\n    print(\"Cumulative Hazard:\", result[1])", "CodeBleu_score": 26.40955611991761}]}
{"instance_id": "35af806300cc", "generation_result": [{"code": "import numpy as np\nfrom lifelines import KaplanMeierFitter\n\ndef ipc_weights(event, time):\n    \"\"\"\n    This function computes inverse probability of censoring (IPC) weights for survival analysis.\n    It first checks if all events have occurred, returning an array of ones if true.\n    Otherwise, it calculates the Kaplan-Meier estimator for the event and time data in reverse.\n    It then finds the index positions of event times within the unique time values and retrieves the corresponding survival probabilities.\n    The function ensures all retrieved probabilities are positive.\n    Finally, it creates a weights array, assigning inverse probabilities to events and returns the weights.\n    \n    Parameters:\n        event (array-like): Boolean or binary array indicating whether an event occurred (1) or not (0).\n        time (array-like): Array of time-to-event or time-to-censoring data.\n    \n    Returns:\n        array: Array of IPC weights corresponding to each entry in event.\n    \"\"\"\n\n    # Check if all events have occurred\n    if np.all(event == 1):\n        return np.ones_like(event)\n\n    # Prepare the Kaplan-Meier fitting in reverse\n    kmf = KaplanMeierFitter()\n\n    # Reverse the times and events for right-censoring treatment\n    order = np.argsort(-time)\n    sorted_time = time[order]\n    sorted_event = event[order]\n\n    # Fit Kaplan-Meier estimator\n    kmf.fit(durations=sorted_time, event_observed=sorted_event)\n\n    # Retrieve the probabilities of survival\n    survival_probability = kmf.survival_function_.reindex(time, method=\"nearest\")['KM_estimate'].values\n \n    # Handling edge cases where survival probability might strictly be zero by small constant\n    survival_probability = np.maximum(survival_probability, np.finfo(float).eps)\n    \n    # Calculate the weights as inverse probabilities for events\n    weights = np.where(event == 1, 1 / survival_probability, 1)\n\n    return weights", "CodeBleu_score": 32.73752969823198}, {"code": "import numpy as np\nfrom lifelines import KaplanMeierFitter\n\ndef ipc_weights(event, time):\n    \"\"\"\n    This function computes inverse probability of censoring (IPC) weights for survival analysis.\n    It first checks if all events have occurred, returning an array of ones if true.\n    Otherwise, it calculates the Kaplan-Meier estimator for the event and time data in reverse.\n    It then finds the index positions of event times within the unique time values and retrieves the corresponding survival probabilities.\n    The function ensures all retrieved probabilities are positive.\n    Finally, it creates a weights array, assigning inverse probabilities to events and returns the weights.\n\n    Args:\n    event (array-like): An array indicating whether an event has happened.\n    time (array-like): An array of time intervals.\n\n    Returns:\n    np.ndarray: Array of IPC weights.\n    \"\"\"\n    # Check if all events have occurred\n    if np.all(event == 1):\n        return np.ones_like(event)\n\n    # Calculate the Kaplan-Meier estimator\n    kmf = KaplanMeierFitter()\n    time_reversed = np.max(time) - time  # reverse the time for km estimate\n    kmf.fit(durations=time_reversed, event_observed=event, timeline=np.unique(time_reversed))\n    \n    # Retrieve survival probabilities for the original event times\n    survival_probabilities = kmf.survival_function_at_times(time).values.flatten()\n    \n    # Ensure all retrieved probabilities are positive to avoid division by zero\n    survival_probabilities = np.clip(survival_probabilities, a_min=np.finfo(float).eps, a_max=1)\n\n    # Assign inverse probabilities to events\n    weights = np.where(event == 1, 1 / survival_probabilities, 1)\n\n    return weights", "CodeBleu_score": 30.33331373406331}, {"code": "import numpy as np\nfrom lifelines import KaplanMeierFitter\n\ndef ipc_weights(event, time):\n    \"\"\"\n    This function computes inverse probability of censoring (IPC) weights for survival analysis.\n    It first checks if all events have occurred, returning an array of ones if true.\n    Otherwise, it calculates the Kaplan-Meier estimator for the event and time data in reverse.\n    It then finds the index positions of event times within the unique time values and retrieves the corresponding survival probabilities.\n    The function ensures all retrieved probabilities are positive.\n    Finally, it creates a weights array, assigning inverse probabilities to events and returns the weights.\n    \n    Args:\n    event (array-like): Binary events array (1 if event occurred, 0 if censored).\n    time (array-like): The time at which events were observed.\n    \n    Returns:\n    numpy.ndarray: Weights for each observation.\n    \"\"\"\n    \n    # Check if all events have occurred\n    if np.all(event == 1):\n        return np.ones(len(time))\n\n    # Fit Kaplan-Meier estimator\n    kmf = KaplanMeierFitter()\n    kmf.fit(time, event, timeline=np.sort(np.unique(time))[::-1])\n\n    # Get survival probabilities at each unique time\n    survival_probabilities = kmf.survival_function_.loc[np.sort(np.unique(time))].values.flatten()\n\n    # Ensure all probabilities are positive\n    survival_probabilities = np.clip(survival_probabilities, 1e-10, 1)\n\n    # Find index positions in the sorted unique times for each event time and retrieve survival probabilities\n    time_indices = np.searchsorted(np.sort(np.unique(time)), time, side='right') - 1\n    event_survival_probabilities = survival_probabilities[time_indices]\n    \n    # Compute weights for events, inverting the survival probabilities\n    weights = np.where(event == 1, 1 / event_survival_probabilities, 1)\n\n    return weights", "CodeBleu_score": 34.60604259164112}]}
{"instance_id": "72b19985ea11", "generation_result": [{"code": "import pandas as pd\nfrom modin.pandas import DataFrame as ModinDataFrame\nfrom polars import DataFrame as PolarsDataFrame\nfrom typing import Union, List\n\nclass BaseConnector:\n    def __init__(self, dataframe):\n        self.data = dataframe\n\nclass PandasConnector(BaseConnector):\n    pass\n\nclass ModinConnector(BaseConnector):\n    pass\n\nclass PolarsConnector(BaseConnector):\n    pass\n\ndef get_dfs(self, dfs: Union[pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]]):\n    \"\"\"\n    This function converts various types of data inputs into a list of connector objects.\n    It accepts single or multiple dataframes, connectors, or other data structures.\n    If a single dataframe is provided, it converts it into a list.\n    It iterates over the list and checks the type of each item.\n    Depending on the type, it creates appropriate connector objects.\n    It handles pandas, modin, and polars dataframes, as well as custom connectors.\n    If the input type is invalid or cannot be converted, it raises a ValueError.\n    Finally, it returns a list of connector objects.\n    \"\"\"\n    \n    # Normalize input into a list\n    if isinstance(dfs, (pd.DataFrame, ModinDataFrame, PolarsDataFrame, BaseConnector)):\n        dfs = [dfs]\n\n    connector_list = []\n  \n    for df in dfs:\n        if isinstance(df, pd.DataFrame):\n            connector = PandasConnector(df)\n        elif isinstance(df, ModinDataFrame):\n            connector = ModinConnector(df)\n        elif isinstance(df, PolarsDataFrame):\n            connector = PolarsConnector(df)\n        elif isinstance(df, BaseConnector):\n            connector = df  # Assuming df is already a Connector, directly append it.\n        else:\n            raise ValueError(\"Provided data structure type is not supported\")\n        \n        connector_list.append(connector)\n    \n    return connector_list", "CodeBleu_score": 39.73686261649047}, {"code": "from typing import Union, List\nimport pandas as pd\n# Assuming these as hypothetical imports\n# from your_package import BaseConnector, PandasConnector, ModinConnector, PolarsConnector\n\nclass BaseConnector:\n    # Mock base connector class\n    pass\n\nclass DataFrameConnector(BaseConnector):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\nclass YourClass:\n    def get_dfs(self, dfs: Union[pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]]):\n        \"\"\"\n        This function converts various types of data inputs into a list of connector objects.\n        It accepts single or multiple dataframes, connectors, or other data structures.\n        If a single dataframe is provided, it converts it into a list.\n        It iterates over the list and checks the type of each item.\n        Depending on the type, it creates appropriate connector objects.\n        It handles pandas, modin, and polars dataframes, as well as custom connectors.\n        If the input type is invalid or cannot be converted, it raises a ValueError.\n        Finally, it returns a list of connector objects.\n        \"\"\"\n        if not isinstance(dfs, list):\n            dfs = [dfs]\n\n        connector_list = []\n\n        for item in dfs:\n            if isinstance(item, pd.DataFrame):\n                connector_list.append(DataFrameConnector(item))\n            elif isinstance(item, BaseConnector):\n                connector_list.append(item)\n            else:\n                raise ValueError(\"Unsupported type for conversion: {}\".format(type(item)))\n\n        return connector_list", "CodeBleu_score": 30.225316561658538}, {"code": "import pandas as pd\nfrom typing import Union, List\n\nclass MyClass:\n    def get_dfs(\n            self,\n            dfs: Union[pd.DataFrame, 'BaseConnector', List[Union[pd.DataFrame, 'BaseConnector']]]\n        ) -> List['BaseConnector']:\n        if not isinstance(dfs, list):\n            dfs = [dfs]  # Convert single input into list\n\n        connectors = []\n        for item in dfs:\n            if isinstance(item, (pd.DataFrame, 'modin.DataFrame', 'polars.DataFrame')):  # Placeholder types for modin and polars\n                connector = BaseConnector(item)  # Assuming BaseConnector can take these DataFrame types\n                connectors.append(connector)\n            elif isinstance(item, BaseConnector):\n                connectors.append(item)\n            else:\n                raise ValueError(f\"Unsupported type {type(item)}. Input must be a DataFrame or a BaseConnector.\")\n\n        return connectors", "CodeBleu_score": 32.36807385953549}]}
{"instance_id": "cf22fb696e90", "generation_result": [{"code": "import pandas as pd\n\nclass DataFetcher:\n    def __init__(self):\n        self.data = None\n\n    def check_cached_data(self):\n        # This is a placeholder function. Implement checking for cached data.\n        # for example, it might return a file path if data is cached to disk.\n        return None\n\n    def read_cached_data(self, cache_location):\n        # Assuming that cached data is stored in a local file as a CSV.\n        return pd.read_csv(cache_location)\n\n    def fetch_data(self):\n        # Placeholder fetch operation. Should implement actual data fetching logic.\n        # For demonstration, let's assume it returns a DataFrame.\n        return pd.DataFrame({\n            'Column1': [1, 2, 3],\n            'Column2': ['A', 'B', 'C']\n        })\n\n    def execute(self) -> pd.DataFrame:\n        \"\"\" Executes data fetching and caching strategy.\n        \"\"\"\n        # Check for cached data\n        cache_location = self.check_cached_data()\n        if cache_location:\n            self.data = self.read_cached_data(cache_location)\n            return self.data\n\n        # Check if self.data is already a DataFrame\n        if isinstance(self.data, pd.DataFrame):\n            return self.data\n\n        # Fetch new data, cache it (optionally), and set self.data\n        self.data = self.fetch_data()\n        # Optionally: self.cache_data(self.data)\n        return self.data", "CodeBleu_score": 39.125537192870425}, {"code": "import pandas as pd\n\nclass DataFetcher:\n    def __init__(self, cache_file=None):\n        self.data = None\n        self.cache_file = cache_file\n    \n    def read_cache(self):\n        \"\"\"This function reads the cache file using pandas and assigns it to self.data if not empty.\"\"\"\n        if self.cache_file:\n            try:\n                df = pd.read_pickle(self.cache_file)\n                if not df.empty:\n                    self.data = df\n            except (FileNotFoundError, pd.errors.EmptyDataError):\n                pass\n    \n    def fetch_data(self):\n        \"\"\"\n        This is a placeholder for the actual data fetching mechanism.\n        You should implement data fetching according to your own backend/source.\n        \"\"\"\n        # Mock data, replace with actual data fetching logic\n        self.data = pd.DataFrame({\n            'Column1': [1, 2, 3],\n            'Column2': [4, 5, 6]\n        })\n    \n    def execute(self) -> pd.DataFrame:\n        \"\"\"\n        The function execute returns a pandas DataFrame. It first checks if there is a cached version of the data, \n        either with or without additional filters, and if found, reads and returns it as a DataFrame. \n        If the instance variable is already a DataFrame, it returns this instance. \n        Otherwise, it fetches the data, assigns it to the instance variable, and returns the fetched data as a DataFrame.\n        \"\"\"\n        # Check if self.data is already a pandas DataFrame\n        if isinstance(self.data, pd.DataFrame):\n            return self.data\n        \n        # Attempt to load from cache\n        self.read_cache()\n        if isinstance(self.data, pd.DataFrame):\n            return self.data\n        \n        # Fetch data if cache is not found\n        self.fetch_data()\n        return self.data", "CodeBleu_score": 28.06788187037943}, {"code": "import pandas as pd\n\n\nclass DataManager:\n    def __init__(self):\n        self.data_frame = None\n        self.cache_path = 'path/to/cache.csv'\n\n    def is_cached(self) -> bool:\n        \"\"\" Check if the cache file exists. \"\"\"\n        try:\n            if pd.read_csv(self.cache_path) is not None:\n                return True\n        except FileNotFoundError:\n            return False\n\n    def read_data_from_cache(self) -> pd.DataFrame:\n        \"\"\" Read data from a cache file and return as DataFrame. \"\"\"\n        return pd.read_csv(self.cache_path)\n    \n    def fetch_data(self) -> pd.DataFrame:\n        \"\"\" Simulate fetching data which could be from a database or an API. \"\"\"\n        # This method should be replaced by actual data fetching logic.\n        self.data_frame = pd.DataFrame({\n            'Column1': range(5),\n            'Column2': ['A', 'B', 'C', 'D', 'E']\n        })\n        return self.data_frame\n\n    def execute(self) -> pd.DataFrame:\n        \"\"\"\n        The function execute returns a pandas DataFrame. It first checks if there is a cached version of the data,\n        either with or without additional filters, and if found, reads and returns it as a DataFrame. \n        If the instance variable is already a DataFrame, it returns this instance. Otherwise,\n        it fetches the data, assigns it to the instance variable, and returns the fetched data as a DataFrame.\n        \"\"\"\n        # Check if a DataFrame is already loaded in the instance\n        if isinstance(self.data_frame, pd.DataFrame):\n            return self.data_frame\n\n        # Check the cache\n        if self.is_cached():\n            self.data_frame = self.read_data_from_cache()\n            return self.data_frame\n\n        # Fetch, assign to instance variable, and return the data\n        return self.fetch_data()\n\n# Example usage:\ndata_manager = DataManager()\nprint(data_manager.execute())", "CodeBleu_score": 35.960460754946496}]}
{"instance_id": "21df5ecea95f", "generation_result": [{"code": "import hashlib\nimport pandas as pd\n\nclass DataFrameHasher:\n    def __init__(self, dataframe, formula='default_formula'):\n        self.dataframe = dataframe\n        self.formula = formula\n\n    def _get_dataframe(self):\n        \"\"\"Simulate any transformations/checks to ensure we have a DataFrame\"\"\"\n        if isinstance(self.dataframe, pd.DataFrame):\n            return self.dataframe\n        else:\n            # Additional logic to convert self.dataframe to a DataFrame if it isn't already one\n            return pd.DataFrame()  # fallback for this example\n\n    def column_hash(self):\n        \"\"\"\n        This function generates a SHA-256 hash based on the column names of a DataFrame and a formula.\n        It first checks if the instance is a DataFrame, and if not, executes a method to obtain it.\n        Then, it creates a string of the column names separated by \"|\", appends a formula string to it,\n        and returns the SHA-256 hash of this combined string.\n        \"\"\"\n\n        df = self._get_dataframe()  # Ensure we're working with a DataFrame\n        if df.empty:\n            raise ValueError(\"Dataframe is empty or invalid.\")\n\n        # Create a single string from column names separated by \"|\"\n        columns_string = \"|\".join(df.columns)\n        combined_string = f\"{columns_string}|{self.formula}\"\n\n        # Create a SHA-256 hash of the combined string\n        hash_object = hashlib.sha256()\n        hash_object.update(combined_string.encode('utf-8'))  # encode the string to bytes\n        hash_digest = hash_object.hexdigest()\n        return hash_digest\n\n# Example Usage:\n# Assuming 'data' is your data and 'your_formula' is the formula string you wish to use\ndata = pd.DataFrame({\n    'col1': [1, 2],\n    'col2': [3, 4]\n})\nhasher = DataFrameHasher(data, 'sum(col1*col2)')\nprint(hasher.column_hash())", "CodeBleu_score": 33.72259814664481}, {"code": "import pandas as pd\nimport hashlib\n\nclass DataFrameHasher:\n    def __init__(self, data, formula):\n        \"\"\"\n        Initializes the DataFrameHasher class with data which may or may not be a DataFrame,\n        and a formula expressed as a string.\n        \"\"\"\n        self.data = data\n        self.formula = formula\n        \n    def get_dataframe(self):\n        \"\"\" Ensure the self.data is a DataFrame. \"\"\"\n        if not isinstance(self.data, pd.DataFrame):\n            raise ValueError(\"Data is not a pandas DataFrame.\")\n        return self.data\n\n    def column_hash(self):\n        \"\"\"\n        Generates a SHA-256 hash based on the column names of a DataFrame and a formula.\n        It first checks if the instance data is a DataFrame, and if not, raises an error.\n        Then, it creates a string of the column names separated by \"|\", appends a formula string to it,\n        and returns the SHA-256 hash of this combined string.\n        \"\"\"\n        # Check if the data is a Pandas DataFrame and get it\n        df = self.get_dataframe()\n        \n        # Create a string from the column names joined with '|'\n        columns_str = '|'.join(df.columns)\n        \n        # Append the formula string to the columns string\n        full_str = f\"{columns_str}|{self.formula}\"\n        \n        # Encode the full string to prepare for hashing\n        encoded_str = full_str.encode()\n        \n        # Perform SHA-256 hashing\n        hash_object = hashlib.sha256(encoded_str)\n        \n        # Get the hexadecimal digest of the hash\n        hex_digest = hash_object.hexdigest()\n        \n        # Return the hash\n        return hex_digest\n\n# Using the DataFrameHasher class with example usage:\n# df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n# hasher = DataFrameHasher(df, \"x*2 + y\")\n# print(hasher.column_hash())", "CodeBleu_score": 33.46889956854025}, {"code": "import pandas as pd\nimport hashlib\n\nclass DataFrameProcessor:\n    def __init__(self, data):\n        if isinstance(data, pd.DataFrame):\n            self.dataframe = data\n        else:\n            raise ValueError(\"Input must be a pandas DataFrame.\")\n\n    def column_hash(self):\n        \"\"\"\n        This function generates a SHA-256 hash based on the column names of a DataFrame and a formula.\n        It first checks if the instance is a DataFrame, and if not, executes a method to obtain it.\n        Then, it creates a string of the column names separated by \"|\", appends a formula string to it,\n        and returns the SHA-256 hash of this combined string.\n        \"\"\"\n        if not isinstance(self.dataframe, pd.DataFrame):\n            # Assuming get_dataframe method retrieves a DataFrame within the class\n            self.dataframe = self.get_dataframe()\n\n        # Ensure dataframe is correct type before proceeding\n        if not isinstance(self.dataframe, pd.DataFrame):\n            raise ValueError(\"No valid DataFrame available.\")\n        \n        # Create a string of column names joined by '|'\n        columns_string = \"|\".join(self.dataframe.columns)\n        \n        # Append a predefined formula to this string\n        formula = \"some_formula\"  # This could be dynamic or more complex based on your requirements\n        combined_string = columns_string + formula\n\n        # Create a SHA-256 hash object\n        hash_object = hashlib.sha256(combined_string.encode())\n        \n        # Return the hexadecimal representation of the hash\n        return hash_object.hexdigest()\n\n# Example of use:\ndf = pd.DataFrame({\n    'column1': [1, 2, 3],\n    'column2': [4, 5, 6]\n})\nprocessor = DataFrameProcessor(df)\nprint(processor.column_hash())", "CodeBleu_score": 42.223575485131846}]}
{"instance_id": "27ee5f415000", "generation_result": [{"code": "import pandas as pd\nfrom typing import Union\n\ndef _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict]) -> pd.DataFrame:\n    \"\"\"\n    The function _load_df loads data into a pandas DataFrame based on the input type. It \n    converts a pandas Series to a DataFrame, directly assigns a DataFrame, converts a \n    list or dictionary to a DataFrame, and imports data from a file if the input is a \n    string representing a file path. If the input is of an unsupported type or cannot \n    be converted, it raises a ValueError.\n\n    Parameters:\n        df (Union[pd.DataFrame, pd.Series, str, list, dict]): Data or path to load into a DataFrame.\n    \n    Returns:\n        pd.DataFrame: The loaded pandas DataFrame.\n\n    Raises:\n        ValueError: If the input has an unsupported type.\n    \"\"\"\n    if isinstance(df, pd.DataFrame):\n        return df\n    elif isinstance(df, pd.Series):\n        return df.to_frame()\n    elif isinstance(df, list):\n        return pd.DataFrame(df)\n    elif isinstance(df, dict):\n        return pd.DataFrame([df])  # Handle single dictionary as a row or multiple dictionaries in a list separately\n    elif isinstance(df, str):\n        try:\n            return pd.read_csv(df)\n        except Exception as e:  # Potentially handle different types of IO errors\n            raise ValueError(f\"Unable to load data from the file path provided: {df}\") from e\n    else:\n        raise ValueError(\"Unsupported data type provided. Must be DataFrame, Series, list, dict, or file path string.\")", "CodeBleu_score": 39.26000356737744}, {"code": "import pandas as pd\nfrom typing import Union\n\nclass DataFrameHandler:\n    def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict]) -> pd.DataFrame:\n        \"\"\"The function _load_df loads data into a pandas DataFrame based on the input type. It converts a pandas Series to a DataFrame,\n        directly assigns a DataFrame, converts a list or dictionary to a DataFrame, and imports data from a file if the input is a string representing a file path.\n        If the input is of an unsupported type or cannot be converted, it raises a ValueError.\n\n        Args:\n            df (Union[pd.DataFrame, pd.Series, str, list, dict]): The input data to be converted to a DataFrame.\n\n        Returns:\n            pd.DataFrame: The converted data as a pandas DataFrame.\n\n        Raises:\n            ValueError: If the input is of an unsupported type or cannot be handled.\n        \"\"\"\n        if isinstance(df, pd.DataFrame):\n            # Directly return if the input is already a DataFrame\n            return df\n        elif isinstance(df, pd.Series):\n            # Convert pandas Series to DataFrame\n            return df.to_frame()\n        elif isinstance(df, (list, dict)):\n            # Convert list or dictionary to DataFrame\n            return pd.DataFrame(df)\n        elif isinstance(df, str):\n            try:\n                # Attempt to load from a file path\n                return pd.read_csv(df)  # We assume CSV for simplicity, but this could be adjusted\n            except Exception as e:\n                # Reraise exception with meaningful message if loading the file fails\n                raise ValueError(f\"Failed to read file at path {df}. Ensure the file path and format are correct.\")\n        else:\n            # Raise ValueError for unsupported types\n            raise ValueError(\"Input type not supported for DataFrame conversion.\")\n\n# Example usage within the class context\nif __name__ == \"__main__\":\n    handler = DataFrameHandler()\n    print(handler._load_df(pd.Series([1, 2, 3])))\n    print(handler._load_df({\"col1\": [1, 2], \"col2\": [3, 4]}))\n    print(handler._load_df([[\"row1\", \"row2\"], [\"row3\", \"row4\"]]))\n    # Uncomment and provide an existing CSV file path to test\n    # print(handler._load_df(\"path/to/your/datafile.csv\"))", "CodeBleu_score": 40.197700997372074}, {"code": "import pandas as pd\nfrom typing import Union\n\ndef _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict]):\n    \"\"\"\n    The function _load_df loads data into a pandas DataFrame based on the input type. \n    It converts a pandas Series to a DataFrame, directly assigns a DataFrame, \n    converts a list or dictionary to a DataFrame, and imports data from a file \n    if the input is a string representing a file path. If the input is of an unsupported type \n    or cannot be converted, it raises a ValueError.\n    \n    Arguments:\n    df -- pandas DataFrame, pandas Series, string (file path), list, or dictionary.\n\n    Returns:\n    A pandas DataFrame object.\n    \"\"\"\n    if isinstance(df, pd.DataFrame):\n        return df\n    elif isinstance(df, pd.Series):\n        return df.to_frame()\n    elif isinstance(df, str):\n        try:\n            return pd.read_csv(df)  # Assuming the file is a CSV for simplification. Use pd.read_excel for Excel files etc.\n        except Exception as e:\n            raise ValueError(f\"Failed to load DataFrame from the file path provided: {df}. Error: {e}\")\n    elif isinstance(df, list):\n        return pd.DataFrame(df)\n    elif isinstance(df, dict):\n        return pd.DataFrame([df])\n    else:\n        raise ValueError(f\"Unsupported input type: {type(df)}. Input must be DataFrame, Series, file path, list, or dict.\")", "CodeBleu_score": 36.796594790007866}]}
{"instance_id": "38f48a358606", "generation_result": [{"code": "import pandas as pd\nimport numpy as np\n\nclass DataFrameSampler:\n    def __init__(self, data: pd.DataFrame):\n        self.data = data\n\n    def sample(self, n: int = 3) -> pd.DataFrame:\n        \"\"\"\n        This function generates a sample from a DataFrame with a specified number of rows.\n        If the DataFrame has fewer rows than the specified number, it returns a shuffled version of the entire DataFrame.\n        Otherwise, it samples each column individually using a custom sampling method and combines these samples into a new DataFrame.\n        The function then anonymizes the first few rows of the sampled DataFrame before returning it.\n        \n        Parameters:\n            n (int): Number of rows to sample. Default is 3.\n        \n        Returns:\n            pd.DataFrame: The sampled and partially anonymized DataFrame.\n        \"\"\"\n        num_rows = len(self.data)\n        \n        # If the number of rows in the DataFrame is less than the specified `n`, shuffle and return the entire DataFrame.\n        if num_rows <= n:\n            return self.data.sample(frac=1).reset_index(drop=True)\n        \n        # Sample each column separately using custom sampling\n        sampled_data = {column: self.data[column].sample(n, replace=True).values for column in self.data.columns}\n        sampled_df = pd.DataFrame(sampled_data)\n        \n        # Anonymize the data (could be anonymizing specific columns based on needs)\n        # For demonstration, let's anonymize the first column if there are any columns\n        if n > 0 and len(sampled_df.columns) > 0:\n            sampled_df[sampled_df.columns[0]] = ['Anonymized'] * min(n, len(sampled_df))\n        \n        return sampled_df\n\n# Example usage:\n# Assuming 'df' is some pandas DataFrame you already have loaded.\n# sampler = DataFrameSampler(df)\n# sample_df = sampler.sample(5)\n# print(sample_df)", "CodeBleu_score": 36.60517673241993}, {"code": "import pandas as pd\n\nclass DataFrameSampler:\n    def __init__(self, data: pd.DataFrame):\n        self.data = data\n\n    def sample(self, n: int = 3) -> pd.DataFrame:\n        \"\"\"\n        This function generates a sample from a DataFrame with a specified number of rows.\n        If the DataFrame has fewer rows than the specified number, it returns a shuffled version of the entire DataFrame.\n        Otherwise, it samples each column individually using a custom sampling method and combines these samples into a new DataFrame.\n        The function then anonymizes the first few rows of the sampled DataFrame before returning it.\n        \"\"\"\n        # Check if the number of rows in the DataFrame is less than the specified number n\n        if len(self.data) <= n:\n            # If there are not enough rows, shuffle the entire DataFrame\n            sampled_df = self.data.sample(frac=1).reset_index(drop=True)\n        else:\n            # Sample each column individually\n            sampled_df = pd.DataFrame({col: self.data[col].sample(n) for col in self.data})\n        \n        # Anonymize the first few rows\n        anonymize_count = min(sampled_df.shape[0], 2)  # anonymizing first 2 rows or fewer if the DataFrame is small\n        for col in sampled_df.columns:\n            # Assuming we are anonymizing by setting them to a default or null value.\n            # More complex anonymization logic might be required based on the data type and use-case\n            sampled_df.loc[:anonymize_count - 1, col] = None  # or use another anonymizing value like 'Anonymous'\n\n        return sampled_df\n\n# Example of usage:\n# df = pd.DataFrame([...])\n# sampler = DataFrameSampler(df)\n# sampled_output = sampler.sample()", "CodeBleu_score": 39.17129978344988}, {"code": "import pandas as pd\nimport numpy as np\n\nclass DataFrameSampler:\n    def __init__(self, df):\n        self.df = df\n\n    def sample(self, n: int = 3) -> pd.DataFrame:\n        \"\"\"\n        This function generates a sample from a DataFrame with a specified number of rows.\n        If the DataFrame has fewer rows than the specified number, it returns a shuffled version of the entire DataFrame.\n        Otherwise, it samples each column individually using a custom sampling method and combines these samples into a new DataFrame.\n        The function then anonymizes the first few rows of the sampled DataFrame before returning it.\n        \"\"\"\n        # Check if n exceeds the number of rows in the DataFrame\n        num_rows = self.df.shape[0]\n        if num_rows <= n:\n            # Shuffle and return all rows when n is more than or equal to the DataFrame's size\n            sampled_df = self.df.sample(frac=1).reset_index(drop=True)\n        else:\n            # Sample each column individually\n            sampled_data = {column: self.df[column].sample(n) for column in self.df.columns}\n            sampled_df = pd.DataFrame(sampled_data).reset_index(drop=True)\n\n        # Anonymize the first few rows if relevant, assuming anonymization for the first 2 columns\n        for column in self.df.columns[:2]:  # Adjust the slicing based on need\n            if sampled_df[column].dtype in ['object', 'category', 'string']:\n                sampled_df.loc[:min(n-1, num_rows), column] = 'Anonymous'\n            elif sampled_df[column].dtype in ['int', 'float']:\n                sampled_df.loc[:min(n-1, num_rows), column] = 0\n\n        return sampled_df\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Name': ['Alice', 'Bob', 'Cathy', 'David'],\n#     'Age': [25, 30, 22, 45],\n#     'Salary': [50000, 54000, 32000, 75000]\n# })\n# sampler = DataFrameSampler(df)\n# print(sampler.sample(2))", "CodeBleu_score": 35.47602748268649}]}
{"instance_id": "8bddad2e31ca", "generation_result": [{"code": "import pandas as pd\n\ndef convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str:\n    \"\"\"The function convert_df_to_csv converts a pandas DataFrame into a string formatted with XML-like tags, including optional name and description attributes if available. It constructs an opening tag with the DataFrame's name and description, adds details about the DataFrame's dimensions and content in CSV format, and closes the tag. The function uses information from an extras dictionary to include additional details about the DataFrame.\n    \n    Args:\n    - df (pd.DataFrame): DataFrame to be converted into a CSV formatted string.\n    - extras (dict): Dictionary that might contain meta information like 'name' and 'description'.\n    \n    Returns:\n    - str: A string with the DataFrame content in CSV format enclosed in XML-like tags.\n    \"\"\"\n    \n    # XML opening tag with optional attributes\n    name = extras.get('name', 'DataFrame')\n    description = extras.get('description', '')\n    tag_open = f\"<data name=\\\"{name}\\\" description=\\\"{description}\\\">\\n\"\n    \n    # Convert DataFrame to CSV string\n    csv_data = df.to_csv(index=False)\n    \n    # XML closing tag\n    tag_close = \"</data>\"\n    \n    # Construct final XML-like string\n    result = f\"{tag_open}{csv_data}{tag_close}\"\n    return result", "CodeBleu_score": 37.143451450494}, {"code": "import pandas as pd", "CodeBleu_score": 0.0}, {"code": "import pandas as pd\n\ndef convert_df_to_csv(self, df: pd.DataFrame, extras:", "CodeBleu_score": 8.893280632411066}]}
{"instance_id": "5380bb60fd39", "generation_result": [{"code": "import pandas as pd\n\ndef convert_df_sql_connector_to_str(df):\n    \"\"\"\n    This function converts a DataFrame into an HTML table string.\n    It optionally includes a description if it exists in the DataFrame metadata.\n    The function constructs the opening table tag with the table name and optional description.\n    It then adds the CSV representation of the DataFrame's head.\n    Finally, it closes the table tag and returns the complete HTML string.\n    \n    Parameters:\n        df (pandas.DataFrame): The DataFrame to convert to an HTML table string.\n\n    Returns:\n        str: An HTML string representation of the DataFrame.\n    \"\"\"\n    \n    # Check if '_metadata' exists and has 'description'\n    description = df._metadata[0]['description'] if hasattr(df, '_metadata') and 'description' in df._metadata[0] else \"\"\n    \n    # Convert DataFrame to CSV format and keep only the head\n    csv_str = df.head().to_csv(index=False)\n    \n    # Convert newlines in CSV string to HTML breaks for display in HTML context\n    csv_html = csv_str.replace('\\n', '<br>')\n    \n    # Construct full HTML output\n    html_output = f'<table border=\"1\">\\n<caption>{description}</caption>\\n<tbody>\\n{csv_html}\\n</tbody>\\n</table>'\n    \n    return html_output\n\n# Sample DataFrame with Metadata\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 35, 30],\n    'Job': ['Engineer', 'Doctor', 'Artist']\n}\ndf = pd.DataFrame(data)\n\n# Add description to the DataFrame's metadata\nif not hasattr(df, '_metadata'):\n    df._metadata = [{}]\ndf._metadata[0]['description'] = \"This is a sample table of names, ages, and jobs.\"\n\n# Using the function\nhtml_table_str = convert_df_sql_connector_to_str(df)\nprint(html_table_str)", "CodeBleu_score": 35.76169637865673}, {"code": "import pandas as pd\n\ndef convert_df_sql_connector_to_str(df):\n    \"\"\"\n    This function converts a DataFrame into an HTML table string.\n    It optionally includes a description if it exists in the DataFrame metadata.\n    The function constructs the opening table tag with the table name and optional description.\n    It then adds the CSV representation of the DataFrame's head.\n    Finally, it closes the table tag and returns the complete HTML string.\n\n    Args:\n    df (pd.DataFrame): The DataFrame to convert, which should possibly have 'table_name' and 'description' in df.attrs.\n\n    Returns:\n    str: The HTML representation of the DataFrame as a SQL table with metadata.\n    \"\"\"\n    if 'table_name' not in df.attrs:\n        raise ValueError(\"DataFrame must have a 'table_name' attribute in df.attrs\")\n\n    # Start the HTML string with the table name\n    html_str = f\"<table><caption>{df.attrs['table_name']}</caption>\"\n\n    # Include description if present\n    if 'description' in df.attrs:\n        html_str += f\"<p>Description: {df.attrs['description']}</p>\"\n\n    # Convert DataFrame head to CSV and then format it into an HTML table\n    csv_data = df.head().to_csv(index=False)\n    csv_rows = csv_data.split('\\n')\n    html_table = \"<table border='1'>\"\n\n    # Add rows from CSV to HTML\n    for row in csv_rows:\n        html_table += \"<tr><td>\"\n        html_table += \"</td><td>\".join(row.split(','))\n        html_table += \"</td></tr>\"\n    \n    html_table += \"</table>\"\n    html_str += html_table\n\n    # Close main table tag\n    html_str += \"</table>\"\n    \n    return html_str", "CodeBleu_score": 32.80757330690631}, {"code": "import pandas as pd\n\ndef convert_df_sql_connector_to_str(df, description=None):\n    \"\"\"\n    This function converts a DataFrame into an HTML table string.\n    It optionally includes a description if it exists in the DataFrame metadata.\n    The function constructs the opening table tag with the table name and optional description.\n    It then adds the CSV representation of the DataFrame's head.\n    Finally, it closes the table tag and returns the complete HTML string.\n    \n    Parameters:\n        df (pd.DataFrame): The DataFrame to convert.\n        description (str, optional): A description of the table's content.\n        \n    Returns:\n        str: The HTML representation of the DataFrame.\n    \"\"\"\n    # Start constructing the HTML table\n    html_output = '<table>'\n  \n    # Add description as a caption if provided\n    if description:\n        html_output += f'<caption>{description}</caption>'\n    \n    # Convert DataFrame's head to HTML and append to the string\n    html_output += df.head().to_html(index=False, border=1)\n    \n    # Close the HTML table tag\n    html_output += '</table>'\n    \n    return html_output\n\n# Example usage:\n# Creating a sample DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35]}\ndf = pd.DataFrame(data)\n\n# Convert DataFrame to HTML string\nhtml_str = convert_df_sql_connector_to_str(df, \"Sample Table\")\nprint(html_str)", "CodeBleu_score": 34.28323404898862}]}
{"instance_id": "0afd995716c5", "generation_result": [{"code": "import pandas as pd\nimport yaml\n\nclass DataProcessor:\n    def convert_df_to_json(self, df: pd.DataFrame) -> str:\n        \"\"\"Convert DataFrame to a JSON string.\"\"\"\n        return df.to_json()\n\n    def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n        \"\"\"\n        The function convert_df_to_yml converts a DataFrame to a YAML formatted string.\n        It first converts the DataFrame to JSON format using another method and then converts the JSON to a YAML string.\n        If the extras dictionary contains the key \"is_direct_sql\" with a value of True,\n        the function wraps the YAML string in HTML table tags.\n        It returns the resulting YAML string or the wrapped string.\n        \"\"\"\n        # Convert DataFrame to JSON\n        json_str = self.convert_df_to_json(df)\n        \n        # Convert JSON to Python dictionary\n        data = pd.read_json(json_str)\n        \n        # Convert dictionary to YAML formatted string\n        yaml_str = yaml.dump(data, default_flow_style=False)\n        \n        # Check 'is_direct_sql' key in extras\n        if extras.get('is_direct_sql', False):\n            # Wrap YAML in HTML tags if 'is_direct_sql' is True\n            yaml_str = f\"<table>{yaml_str}</table>\"\n        \n        return yaml_str\n\n# Example DataFrame\ndf = pd.DataFrame({\n    'id': [1, 2, 3],\n    'name': ['Alice', 'Bob', 'Charlie']\n})\n\n# Extras dictionary\nextras = {'is_direct_sql': True}\n\n# Instantiate and use the class\nprocessor = DataProcessor()\nresult_yaml = processor.convert_df_to_yml(df, extras)\nprint(result_yaml)", "CodeBleu_score": 43.83812980003968}, {"code": "import pandas as pd\nimport json\nimport yaml", "CodeBleu_score": 0.0}, {"code": "import pandas as pd\nimport yaml\nimport json\n\nclass DataFrameConverter:\n    def __init__(self):\n        pass\n\n    def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n        \"\"\"\n        Convert a pandas DataFrame to a YAML formatted string. It converts the DataFrame to JSON\n        format using Pandas to_json method and then converts the JSON to a YAML string.\n        If the extras dictionary contains the key \"is_direct_sql\" with a value of True,\n        the function wraps the YAML string in HTML table tags. It returns the resulting YAML string\n        or the wrapped string.\n\n        :param df: The pandas DataFrame to convert.\n        :param extras: A dictionary containing additional options.\n        :return: A string in YAML format, optionally wrapped in HTML table tags.\n        \"\"\"\n        # Convert DataFrame to JSON, then parse JSON and convert to YAML\n        json_str = df.to_json(orient=\"records\")\n        parsed_json = json.loads(json_str)\n        yaml_str = yaml.safe_dump(parsed_json, default_flow_style=False, sort_keys=False)\n\n        # Check for 'is_direct_sql' in extras and wrap YAML in HTML if True\n        if extras.get('is_direct_sql', False):\n            yaml_str = f\"<table><tr><td><pre>{yaml_str}</pre></td></tr></table>\"\n\n        return yaml_str", "CodeBleu_score": 40.085260721491814}]}
{"instance_id": "e2b90ebd4e5d", "generation_result": [{"code": "import pandas as pd\nimport os\n\ndef validate_value(self, value, expected_type: str) -> bool:\n    \"\"\"\n    This function validates if the value matches the expected type.\n    If expected_type is empty, it returns True.\n    If expected_type is \"number\", it checks if the value is an integer or float.\n    If expected_type is \"string\", it checks if the value is a string.\n    If expected_type is \"dataframe\", it checks if the value is a pandas DataFrame or Series.\n    If expected_type is \"plot\", it checks if the value is a string matching a file path pattern or a dictionary.\n\n    Args:\n    value (any): The value to be validated.\n    expected_type (str): The type the value is expected to conform to (\"number\", \"string\", \"dataframe\", \"plot\", or empty).\n\n    Returns:\n    bool: True if the value matches the expected type, False otherwise.\n    \"\"\"\n    if not expected_type:\n        return True\n    elif expected_type == \"number\":\n        return isinstance(value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        if isinstance(value, dict):\n            return True\n        elif isinstance(value, str):\n            # Assumption here is that if it's a valid file pattern it should be a plot file path.\n            # Common plot files have extensions like .png, .jpg, .svg, etc.\n            return os.path.isfile(value) and os.path.splitext(value)[-1] in ['.png', '.jpg', '.jpeg', '.svg']\n        else:\n            return False\n    else:\n        return False", "CodeBleu_score": 73.91240132604837}, {"code": "def validate_value(self, expected_type: str) -> bool\ndef validate_value(self, expected_type: str) -> bool:\n    \"\"\"\n    This function validates if the value matches the expected type.\n    If expected_type is empty, it returns True.\n    If expected_type is \"number\", it checks if the value is an integer or float.\n    If expected_type is \"string\", it checks if the value is a string.\n    If expected_type is \"dataframe\", it checks if the value is a pandas DataFrame or Series.\n    If expected_type is \"plot\", it checks if the value is a string matching a file path pattern or a dictionary.\n\n    Args:\n    expected_type (str): The type against which the value is checked.\n\n    Returns:\n    bool: True if the value matches the expected type, False otherwise.\n    \"\"\"\n    import pandas as pd\n    import os\n\n    if expected_type == \"\":\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self.value, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self.value, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self.value, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        is_string_path = isinstance(self.value, str) and os.path.isfile(self.value)\n        is_dict = isinstance(self.value, dict)\n        return is_string_path or is_dict\n    else:\n        return False\n\n# Example usage:\n# Assume `self.value` is defined either through __init__ or another method.\n# validator = YourClass(value=5)\n# print(validator.validate_value(\"number\"))  # Example where `value` was set to 5 in `YourClass`.\n# It should be noted that `self.value` should be an attribute of the class instance.", "CodeBleu_score": 54.164177628344525}, {"code": "import pandas as pd\nimport os\n\nclass Validator:\n    def __init__(self, value):\n        self.value = value\n\n    def validate_value(self, expected_type: str) -> bool:\n        \"\"\"\n        This function validates if the self.value matches the expected type.\n        If expected_type is empty, it returns True.\n        If expected_type is \"number\", it checks if the value is an integer or float.\n        If expected_type is \"string\", it checks if the value is a string.\n        If expected_type is \"dataframe\", it checks if the value is a pandas DataFrame or Series.\n        If expected_type is \"plot\", it checks if the value is a string matching a file path pattern or a dictionary.\n        \"\"\"\n        if not expected_type:\n            return True\n        \n        if expected_type == \"number\":\n            return isinstance(self.value, (int, float))\n        \n        if expected_type == \"string\":\n            return isinstance(self.value, str)\n        \n        if expected_type == \"dataframe\":\n            return isinstance(self.value, (pd.DataFrame, pd.Series))\n        \n        if expected_type == \"plot\":\n            if isinstance(self.value, dict):\n                return True\n            elif isinstance(self.value, str):\n                # Very basic check for a file path pattern\n                return os.path.exists(self.value) and self.value.lower().endswith(('.png', '.jpg', '.jpeg', '.svg'))\n            else:\n                return False\n\n        return False\n\n# Example usage:\n# value = pd.DataFrame({'a': [1, 2]})\n# validator = Validator(value)\n# print(validator.validate_value(\"dataframe\"))  # Should return True", "CodeBleu_score": 43.18945289713042}]}
{"instance_id": "d95828fdcbcb", "generation_result": [{"code": "from inspect import signature\nfrom sktime.distances import pairwise_distance\n\nclass DistanceCalculator:\n    def _distance(self, X, X2=None, distance_metric='euclidean', **kwargs):\n        \"\"\"\n        The function _distance calculates the distance between two datasets, X and X2, using a specified \n        distance metric and its parameters. If no additional parameters are provided, it initializes an empty \n        dictionary. If the distance metric is a string, it uses the pairwise_distance function from sktime.distances. \n        Otherwise, it directly calls the distance function with the given datasets and parameters. If X2 is None,\n        the function checks if the distance function can handle a single dataset by inspecting its signature and \n        calls it accordingly. The function returns the computed distance.\n        \n        Parameters:\n            X (array-like): A dataset.\n            X2 (array-like, optional): A second dataset. If None, distances within X are calculated.\n            distance_metric (callable or str): The distance metric to use. If string, it should be one of\n                                               the supported metrics by sktime.distances.pairwise_distance.\n            **kwargs: Additional parameters to pass to the distance function.\n        \n        Returns:\n            float or array: The computed distances.\n        \"\"\"\n        if isinstance(distance_metric, str):\n            # Handle the case where distance is calculated using a predefined metric\n            if X2 is None:\n                # If no second dataset is provided, check if the distance function can handle a single dataset\n                if \"X2\" in signature(pairwise_distance).parameters:\n                    return pairwise_distance(X, X=X, metric=distance_metric, **kwargs)\n                else:\n                    return pairwise_distance(X, metric=distance_metric, **kwargs)\n            else:\n                return pairwise_distance(X, X2, metric=distance_metric, **kwargs)\n        else:\n            # Handle the case where a custom function is provided\n            if X2 is None:\n                param_count = len(signature(distance_metric).parameters)\n                if param_count == 1:\n                    return distance_metric(X, **kwargs)\n                elif param_count == 2:\n                    return distance_metric(X, X, **kwargs)\n            else:\n                return distance_metric(X, X2, **kwargs)\n\n# Example usage of the DistanceCalculator class\n# You may also import corresponding numpy arrays or other data structures as necessary for X and X2 \n\ndc = DistanceCalculator()\ndist = dc._distance(X)  # X should be your dataset\nprint(dist)", "CodeBleu_score": 38.14423106135242}, {"code": "import numpy as np\nfrom sktime.distances import pairwise_distance\nimport inspect\n\nclass DataDistanceCalculator:\n    def _distance(self, X, X2=None, metric='euclidean', **kwargs):\n        \"\"\"\n        The function _distance calculates the distance between two datasets, X and X2,\n        using the specified distance metric and its parameters. If the distance metric\n        is a string, it uses the pairwise_distance function from sktime.distances.\n        Otherwise, it directly calls the distance function with the given datasets\n        and parameters. If X2 is None, the function checks if the distance function can\n        handle a single dataset by inspecting its signature and calls it accordingly.\n        The function returns the computed distance.\n        \n        :param X: First dataset\n        :param X2: Second dataset, defaults to None\n        :param metric: Distance metric to use, defaults to 'euclidean'\n        :param kwargs: Additional parameters for the distance metric\n        :return: Computed distance\n        \"\"\"\n        if X2 is None:\n            if hasattr(metric, '__call__'):\n                # Check if the function can handle a single dataset:\n                params = inspect.signature(metric).parameters\n                if len(params) == 1 or (len(params) == 2 and 'X2' in params and 'default' in params['X2'].name):\n                    return metric(X, **kwargs)\n                else:\n                    raise ValueError(\"Provided metric function cannot handle a single dataset.\")\n            else:\n                raise ValueError(\"X2 is None but a string metric cannot handle a single dataset implicitly.\")\n        \n        if isinstance(metric, str):\n            # Use pairwise_distance from sktime if metric is a string\n            try:\n                # Ensure the distance function passes any additional kwargs\n                return pairwise_distance(X, X2, metric=metric, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error using pairwise_distance with metric {metric}: {str(e)}\")\n        else:\n            # Assume metric is a callable and use it directly\n            try:\n                return metric(X, X2, **kwargs)\n            except Exception as e:\n                raise ValueError(f\"Error calling metric function: {str(e)}\")\n\n# Assuming proper initialization and providing of datasets X and X2, this class should now handle the described functionality.", "CodeBleu_score": 30.00063139270987}, {"code": "from sktime.distances import pairwise_distance\nfrom inspect import signature\nimport numpy as np\n\ndef _distance(self, X, X2=None, distance_metric='euclidean', **params):\n    \"\"\"\n    The function _distance calculates the distance between two datasets, X and X2, using a specified \n    distance metric and its parameters. If no additional parameters are provided, it initializes \n    an empty dictionary. If the distance metric is a string, it uses the pairwise_distance function \n    from sktime.distances. Otherwise, it directly calls the distance function with the given datasets \n    and parameters. If X2 is None, the function checks if the distance function can handle a single \n    dataset by inspecting its signature and calls it accordingly. The function returns the computed \n    distance.\n    \n    Parameters:\n        X (array-like): First dataset.\n        X2 (array-like, optional): Second dataset. Defaults to None.\n        distance_metric (callable or str): The distance metric to use. Defaults to 'euclidean'.\n        **params: Optional keyword parameters for the distance function.\n    \n    Returns:\n        np.array: Computed distances.\n    \"\"\"\n    # Initialize parameters if not provided\n    if params is None:\n        params = {}\n\n    if isinstance(distance_metric, str):\n        # Using a predefined distance metric from sktime\n        if X2 is None:\n            # Compute pairwise distances within the same dataset\n            distances = pairwise_distance(X, X=X, metric=distance_metric, **params)\n        else:\n            # Compute pairwise distances between two datasets\n            distances = pairwise_distance(X, X2, metric=distance_metric, **params)\n    else:\n        # Using a custom distance function passed directly\n        if X2 is None:\n            # Check if the custom metric can handle a single input\n            sig = signature(distance_metric)\n            if len(sig.parameters) == 1:\n                distances = distance_metric(X, **params)\n            else:\n                # Assume it expects two equal datasets if not explicitly handling one\n                distances = distance_metric(X, X, **params)\n        else:\n            # Compute distance with custom metric\n            distances = distance_metric(X, X2, **params)\n\n    return distances", "CodeBleu_score": 45.03444345131049}]}
{"instance_id": "497d73b298ee", "generation_result": [{"code": "import numpy as np\nimport pandas as pd\n\ndef _convert_X_to_sklearn(self, X):\n    \"\"\"\n    This function converts input data X to a 2D numpy array for use with sklearn.\n    For unequal length series, it resets the index, pivots the data, fills missing values with zeros, \n    converts it to a numpy array, calculates the lengths of individual series, and concatenates \n    these lengths as the first column.\n    For equal length series, it identifies the data type of X and converts it to a flattened numpy array.\n    \"\"\"\n    \n    # Check if the input is a list of numpy arrays\n    if isinstance(X, list) and all(isinstance(x, np.ndarray) for x in X):\n        lengths = np.array([len(x) for x in X])\n        max_length = max(lengths)\n\n        # Create a matrix of zeros with size of [len(X), max_length]\n        uniform_data = np.zeros((len(X), max_length))\n\n        # Fill the matrix with the data from X, variable lengths will be zero-padded at the end\n        for i, x in enumerate(X):\n            uniform_data[i, :len(x)] = x\n\n        # Check if the series are of the same length\n        if np.all(lengths == lengths[0]):\n            # Flatten and convert to numpy array\n            return uniform_data\n        else:\n            # Concatenate lengths as the first column\n            return np.hstack((lengths[:, np.newaxis], uniform_data))\n\n    # If X is a pandas DataFrame\n    elif isinstance(X, pd.DataFrame):\n        if X.shape[1] == 1 or X.applymap(np.isscalar).all().all():  # Single Series or all values are scalar\n            return X.values\n        else:\n            # Reset the index (if needed), pivot if required and fill missing values\n            X.reset_index(drop=True, inplace=True)\n            lengths = X.notna().sum().values  # Calculate lengths before filling NAs\n            X_filled = X.fillna(0).values  # Fill NaNs with zeros\n            # Include lengths information if the series were of unequal length\n            if not np.all(lengths == lengths[0]):\n                return np.hstack((lengths[:, np.newaxis], X_filled))\n\n            return X_filled\n\n    # If X is already a numpy array of uniform data\n    elif isinstance(X, np.ndarray):\n        if X.ndim == 1:  # If it's a flat list, reshape it\n            return X.reshape(1, -1)\n        return X\n\n    else:\n        raise ValueError(\"Unsupported data type for X. Expected list of arrays, DataFrame, or numpy array.\")\n        \n    return X", "CodeBleu_score": 32.39448355638708}, {"code": "import numpy as np\nimport pandas as pd\n\ndef _convert_X_to_sklearn(self, X):\n    \"\"\"This function converts input data X to a 2D numpy array for use with sklearn.\n    For unequal length series, it resets the index, pivots the data, fills missing values with zeros, \n    converts it to a numpy array, calculates the lengths of individual series, and concatenates \n    these lengths as the first column.\n    For equal length series, it identifies the data type of X and converts it to a flattened numpy array.\"\"\"\n\n    # Assume that X could either be a list of pandas Series/DataFrames, a single DataFrame, or a numpy array.\n\n    if isinstance(X, list) and all(isinstance(item, pd.Series) for item in X):\n        # We are dealing with a list of Series, which may have unequal lengths\n        df = pd.DataFrame(index=range(max(len(s) for s in X)))\n        for i, series in enumerate(X):\n            df[i] = series.reset_index(drop=True)\n        X_sklearn = df.fillna(0).T.to_numpy()  # Transpose so that series are rows\n        lengths = np.array([len(s) for s in X]).reshape(-1, 1)\n        X_sklearn = np.hstack((lengths, X_sklearn))  # Concatenate lengths as first column\n\n    elif isinstance(X, pd.DataFrame):\n        # Assume DataFrame rows are series (equal length by definition of being a DataFrame)\n        X_sklearn = X.to_numpy()\n        if not np.isnan(X_sklearn).any():\n            X_sklearn = X_sklearn.reshape(X.shape[0], -1)  # Flatten each row into a single dimension\n\n    elif isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            # Single 1D array (single series)\n            X_sklearn = X.reshape(1, -1)  # Reshape to be two-dimensional\n        else:\n            # Assuming already in the correct shape (2D)\n            X_sklearn = X\n\n    else:\n        raise ValueError(\"Unsupported data type for X. Expected list of pandas.Series, pandas.DataFrame, or numpy.ndarray.\")\n\n    return X_sklearn", "CodeBleu_score": 30.84690328119687}, {"code": "import numpy as np\nimport pandas as pd\n\nclass DataConverter:\n    def _convert_X_to_sklearn(self, X):\n        \"\"\"\n        This function converts input data X to a 2D numpy array for use with sklearn.\n        For unequal length series, it resets the index, pivots the data, fills missing values with zeros, \n        converts it to a numpy array, calculates the lengths of individual series, and concatenates these \n        lengths as the first column. For equal length series, it identifies the data type of X and converts \n        it to a flattened numpy array.\n        \"\"\"\n\n        # Check if the data is a pandas dataframe\n        if isinstance(X, pd.DataFrame):\n            if X.shape[0] == 0:\n                raise ValueError(\"Input is an empty DataFrame\")\n\n            lengths = X.apply(lambda x: x.count(), axis=1)\n            max_length = lengths.max()\n\n            # Check for unequal lengths\n            if lengths.nunique() > 1:\n                # Reset index and pivot if data in DataFrame format (assuming the structure supports it)\n                X_reset = X.reset_index(drop=True)\n                \n                # Create a DataFrame to capture series\n                result_df = pd.DataFrame(index=range(max_length))\n\n                # For each column, check for unequal length and make equal\n                for col in X_reset.columns:\n                    result_df = pd.concat([result_df, X_reset[col]], axis=1)\n\n                result_df.fillna(0, inplace=True)\n                result_flat = result_df.values\n                \n                # Concatenate lengths as the first column\n                results_with_lengths = np.column_stack((lengths.values.reshape(-1, 1), result_flat))\n                \n                return results_with_lengths\n            \n            else:\n                # For equal length data\n                return X.values.reshape(X.shape[0], -1)\n\n        else:\n            # Convert list of arrays or list of lists handling\n            if all(isinstance(elem, list) for elem in X) or all(isinstance(elem, np.ndarray) for elem in X):\n                \n                lengths = np.array([len(x) for x in X])\n                max_length = lengths.max()\n                \n                if lengths.std() == 0:\n                    # Equal length processing\n                    flat_array = np.array(X).reshape(len(X), -1)\n                else:\n                    # Unequal length, padding required\n                    padded_X = np.array([np.pad(x, (0, max_length-len(x)), mode='constant') for x in X])\n                    flat_array = np.column_stack((lengths.reshape(-1, 1), padded_X))\n                \n                return flat_array\n            \n            else:\n                raise TypeError(\"Unsupported type for X. Please provide a DataFrame, list of lists, or list of arrays.\")\n\n# Example usage:\nconverter = DataConverter()\ndata_unequal = pd.DataFrame({\"a\": [1, 2, np.nan], \"b\": [4, 5, 6]})\nresult = converter._convert_X_to_sklearn(data_unequal)\nprint(result)\n\ndata_equal = np.array([[1, 2, 3], [4, 5, 6]])\nresult = converter._convert_X_to_sklearn(data_equal)\nprint(result)", "CodeBleu_score": 29.416243446230467}]}
{"instance_id": "8798260a6f12", "generation_result": [{"code": "from sklearn.base import BaseEstimator\nfrom sktime.base import BaseEstimator as sktimeBaseEstimator\n\ndef is_sklearn_estimator(obj):\n    \"\"\"\n    The function checks if a given object is a scikit-learn estimator. It ensures \n    the input is a class, then verifies if it is a subclass of BaseEstimator\n    from sklearn and not a subclass of BaseEstimator from sktime. The function returns\n    True if both conditions are met, indicating the object is a scikit-learn estimator,\n    otherwise it returns False.\n\n    Parameters:\n        obj (object): The object to check.\n\n    Returns:\n        bool: True if obj is a scikit-learn estimator, False otherwise.\n    \"\"\"\n    if not isinstance(obj, type):\n        return False\n    \n    is_sklearn = issubclass(obj, BaseEstimator)\n    is_sktime = issubclass(obj, sktimeBaseEstimator)\n\n    return is_sklearn and not is_sktime", "CodeBleu_score": 36.57017387996885}, {"code": "from sklearn.base import BaseEstimator\nfrom sktime.base import BaseObject\n\ndef is_sklearn_estimator(obj):\n    \"\"\"\n    Checks if a given object is a scikit-learn estimator. It ensures the input is a class, then verifies if it is a subclass of SklearnBaseEstimator \n    and not a subclass of BaseObject from sktime. The function returns True if both conditions are met, indicating the object is a scikit-learn estimator, otherwise it returns False.\n    \n    Args:\n    obj (object): The object to check.\n    \n    Returns:\n    bool: True if the object is a scikit-learn estimator, False otherwise.\n    \"\"\"\n    # Check if obj is a class\n    if not isinstance(obj, type):\n        return False\n    \n    # Check if obj is a subclass of sklearn's BaseEstimator but not sktime's BaseObject\n    is_base_estimator = issubclass(obj, BaseEstimator)\n    is_base_object = issubclass(obj, BaseObject) # from sktime\n    \n    return is_base_estimator and not is_base_object", "CodeBleu_score": 35.67574950877515}, {"code": "from sklearn.base import BaseEstimator\ntry:\n    from sktime.base import BaseObject\nexcept ImportError:\n    # If sktime isn't installed, we define BaseObject such that no object is a subclass of it.\n    class BaseObject:\n        pass\n\ndef is_sklearn_estimator(obj):\n    \"\"\"\n    The function is_sklearn_estimator checks if a given object is a scikit-learn estimator. It ensures the input is a class, \n    then verifies if it is a subclass of BaseEstimator (from scikit-learn), and not a subclass of BaseObject (from sktime).\n    The function returns True if both conditions are met, indicating that the object is a scikit-learn estimator; otherwise, it returns False.\n    \n    Parameters:\n        obj (Any): The object to be checked if it is a scikit-learn estimator\n    \n    Returns:\n        bool: True if the object is a scikit-learn estimator, otherwise False\n    \"\"\"\n    is_class = isinstance(obj, type)\n    if not is_class:\n        return False\n    \n    is_sklearn_subclass = issubclass(obj, BaseEstimator)\n    is_not_sktime_subclass = not issubclass(obj, BaseObject)\n    \n    return is_sklearn_subclass and is_not_sktime_subclass", "CodeBleu_score": 39.92240693739488}]}
{"instance_id": "2ab2596d8520", "generation_result": [{"code": "class BaseTransformer:\n    \"\"\"\n    Base class for all transformers.\n    \"\"\"\n    def fit_transform(self, X, y=None):\n        raise NotImplementedError\n\nclass TransformerPipeline:\n    \"\"\"\n    A pipeline that chains multiple transformers together into a single process.\n    \"\"\"\n    def __init__(self, transformers):\n        self.transformers = transformers\n    \n    def add_transformer(self, transformer):\n        self.transformers.append(transformer)\n    \n    def transform(self, X):\n        for transformer in self.transformers:\n            X = transformer.fit_transform(X)\n        return X\n\nclass SklearnRegressorPipeline:\n    \"\"\"\n    A pipeline for sklearn that combines transformers and a final regressor.\n    \"\"\"\n    def __init__(self, regressor, transformer_pipeline):\n        self.regressor = regressor\n        self.transformer_pipeline = transformer_pipeline\n    \n    def fit(self, X, y):\n        X_transformed = self.transformer_pipeline.transform(X)\n        self.regressor.fit(X_transformed, y)\n    \n    def predict(self, X):\n        X_transformed = self.transformer_pipeline.transform(X)\n        return self.regressor.predict(X_transformed)\n    \nclass YourClass:\n    def __init__(self, regressor, transformers=None):\n        self.regressor = regressor\n        self.transformers = transformers if transformers else []\n\n    def __rmul__(self, other):\n        \"\"\"\n        this function defines the right multiplication behavior for an object.\n        if the other object is an instance of BaseTransformer, it creates a TransformerPipeline by \n        multiplying the other object with the current object's transformers.\n        it then creates a new SklearnRegressorPipeline with the existing regressor and the expanded \n        transformer pipeline.\n        if the other object is not a BaseTransformer, it returns NotImplemented.\n        \"\"\"\n        if isinstance(other, BaseTransformer):\n            new_transform_pipeline = TransformerPipeline([other] + self.transformers)\n            new_regressor_pipeline = SklearnRegressorPipeline(self.regressor, new_transform_pipeline)\n            return new_regressor_pipeline\n        else:\n            return NotImplemented", "CodeBleu_score": 30.959128103998157}, {"code": "class BaseTransformer:\n    # Simplified transformer base class\n    def fit_transform(self, X, y=None):\n        # Dummy transformation logic\n        pass\n\nclass TransformerPipeline:\n    def __init__(self, transformers=None):\n        self.transformers = transformers if transformers is not None else []\n\n    def add_transformer(self, transformer):\n        self.transformers.append(transformer)\n\nclass SklearnRegressorPipeline:\n    def __init__(self, regressor, transformer_pipeline):\n        self.regressor = regressor\n        self.transformer_pipeline = TransformerPipeline(transformer_pipeline)", "CodeBleu_score": 29.2254156701786}, {"code": "class BaseTransformer:\n    # This class would represent the functionality of a base transformer.\n    # Example functionality is not detailed.\n    pass\n\nclass TransformerPipeline:\n    def __init__(self, transformers=None):\n        self.transformers = transformers or []\n\nclass SklearnRegressorPipeline:\n    def __init__(self, regressor=None, transformer_pipeline=None):\n        self.regressor = regressor\n        self.transformer_pipeline = transformer_pipeline\n\nclass YourCustomClass:\n    def __init__(self, transformers, regressor):\n        self.transformers = transformers  # A transformer or list of transformers\n        self.regressor = regressor\n\n    def __rmul__(self, other):\n        \"\"\"\n        This function defines the right multiplication behavior for an object.\n        If the other object is an instance of BaseTransformer, it creates a TransformerPipeline \n        by multiplying the other object with the current object's transformers.\n        It then creates a new SklearnRegressorPipeline with the existing regressor and the expanded transformer pipeline.\n        If the other object is not a BaseTransformer, it returns NotImplemented.\n        \"\"\"\n        if isinstance(other, BaseTransformer):\n            # Create a new TransformerPipeline by combining the other with self's transformers\n            if isinstance(self.transformers, list):\n                new_transformers = [other] + self.transformers\n            else:\n                new_transformers = [other, self.transformers]\n            \n            expanded_transformer_pipeline = TransformerPipeline(new_transformers)\n            \n            # Create a new SklearnRegressorPipeline using the existing regressor and the newly formed transformer pipeline\n            return SklearnRegressorPipeline(self.regressor, expanded_transformer_pipeline)\n        else:\n            return NotImplemented", "CodeBleu_score": 31.64374974762979}]}
{"instance_id": "5c1c6d14ca93", "generation_result": [{"code": "def prep_skl_df(df, copy_df=False):\n    \"\"\"\n    This function prepares a DataFrame for scikit-learn by ensuring all column names are strings.\n    It takes a DataFrame and an optional boolean to determine whether to copy the DataFrame.\n    It converts the column names to strings and checks if they are already strings.\n    If they are not, and if the copy option is True, it creates a copy of the DataFrame.\n    It then assigns the string-converted column names to the DataFrame and returns it.\n\n    Parameters:\n        df (pd.DataFrame): The input DataFrame that needs to be prepared for scikit-learn.\n        copy_df (bool): Whether to modify the original DataFrame or return a modified copy.\n\n    Returns:\n        pd.DataFrame: The DataFrame with all column names as strings.\n    \"\"\"\n    import pandas as pd\n\n    # Check if a copy of the DataFrame is needed\n    if copy_df:\n        df = df.copy()\n\n    # Convert each column name to string and assign back to DataFrame's columns\n    df.columns = [str(col) for col in df.columns]\n\n    return df", "CodeBleu_score": 43.98114613211054}, {"code": "import pandas as pd\n\ndef prep_skl_df(df, copy_df=False):\n    \"\"\"\n    This function prepares a DataFrame for scikit-learn by ensuring all column names are strings.\n    It takes a DataFrame and an optional boolean to determine whether to copy the DataFrame.\n    It converts the column names to strings and checks if they are already strings.\n    If they are not, and if the copy option is True, it creates a copy of the DataFrame.\n    It then assigns the string-converted column names to the DataFrame and returns it.\n\n    Parameters:\n        df (pd.DataFrame): Original DataFrame to be processed.\n        copy_df (bool): Flag to decide whether to create a copy of the DataFrame or not (default False).\n\n    Returns:\n        pd.DataFrame: The DataFrame with column names converted to strings, either a new copy or the modified original.\n    \"\"\"\n    \n    if copy_df:\n        df = df.copy()\n    \n    # Convert all column names to strings\n    df.columns = [str(col) for col in df.columns]\n    \n    return df", "CodeBleu_score": 43.347430851553895}, {"code": "import pandas as pd\n\ndef prep_skl_df(df, copy_df=False):\n    \"\"\"\n    This function prepares a DataFrame for scikit-learn by ensuring all column names are strings.\n    It takes a DataFrame and an optional boolean to determine whether to copy the DataFrame.\n    It converts the column names to strings and checks if they are already strings.\n    If they are not, and if the copy option is True, it creates a copy of the DataFrame.\n    It then assigns the string-converted column names to the DataFrame and returns it.\n\n    Args:\n    df (pd.DataFrame): The DataFrame to prepare.\n    copy_df (bool, optional): If True, operate on a copy of the DataFrame. Default is False.\n\n    Returns:\n    pd.DataFrame: The DataFrame with string column names.\n    \"\"\"\n    if copy_df:\n        df = df.copy()\n\n    df.columns = [str(col) for col in df.columns]\n    \n    return df", "CodeBleu_score": 43.347430851553895}]}
{"instance_id": "d5e567c20361", "generation_result": [{"code": "import numpy as np\n\nclass Classifier:\n    def __init__(self, random_state=None):\n        self.random_state = np.random.RandomState(random_state)\n        self.classes_ = np.array([0, 1])  # Example class labels: 0 and 1\n\n    def predict(self, X):\n        \"\"\"\n        Generates predictions for the given input data X by first obtaining predicted probabilities, then selecting\n        the indices with the maximum probability, randomly choosing one among them in case of ties,\n        and mapping the choice to the corresponding class label.\n        \n        Parameters:\n        X (array-like): Input data of shape (n_samples, n_features).\n        \n        Returns:\n        np.array: Array of predicted class labels.\n        \"\"\"\n        n_samples = X.shape[0]\n        n_classes = len(self.classes_)\n        predictions = np.empty(n_samples, dtype=self.classes_.dtype)\n        \n        # Assuming a dummy probability generation mechanism for the sake of example\n        for i in range(n_samples):\n            # Generate random probabilities for each class\n            probabilities = self.random_state.rand(n_classes)\n            \n            # Find the indices of the maximum probabilities\n            max_proba_indices = np.where(probabilities == np.max(probabilities))[0]\n            \n            # Select one index randomly among the max indices\n            selected_index = self.random_state.choice(max_proba_indices)\n            \n            # Map selected index to class label\n            predictions[i] = self.classes_[selected_index]\n        \n        return predictions\n\n# Example usage\nclf = Classifier(random_state=42)\nX_dummy = np.random.rand(5, 2)  # Assuming each sample has 2 features\npredictions = clf.predict(X_dummy)\nprint(predictions)", "CodeBleu_score": 33.64296997028601}, {"code": "import numpy as np\n\nclass Classifier:\n    def __init__(self, random_state=None):\n        self.random_state = random_state\n        self.classes_ = None  # Assuming model has been trained and classes are known\n        self.prediction_model = None  # This should be the model used to predict probabilities\n\n    def predict(self, X):\n        \"\"\"\n        The function predict generates predictions for the given input data X. It initializes a random state\n        using the object's random_state attribute. For each sample, it obtains the predicted probabilities,\n        selects the indices of the maximum probability values, and randomly chooses one among them if there\n        is a tie. It then maps the chosen index to the corresponding class label. The function returns an\n        array of predicted class labels.\n        \n        Parameters:\n            X: Array-like of shape (n_samples, n_features)\n                Feature matrix for which predictions are to be made.\n                \n        Returns:\n            predictions: Array of shape (n_samples,)\n                Predicted class labels for each sample in X.\n        \"\"\"\n        np.random.seed(self.random_state)  # Seed the random number generator for reproducibility\n        probabilities = self.prediction_model.predict_proba(X)  # Hypothetical method to get probability predictions\n        predictions = []\n\n        for proba in probabilities:\n            max_proba = np.max(proba)  # Find the maximum probability\n            indices_with_max_proba = np.where(proba == max_proba)[0]  # Find indices where probabilities are max\n            chosen_index = np.random.choice(indices_with_max_proba)  # Randomly choose index in case of tie\n            predicted_label = self.classes_[chosen_index]  # Map index to class label\n            predictions.append(predicted_label)\n\n        return np.array(predictions)", "CodeBleu_score": 22.59788761916258}, {"code": "import numpy as np\n\nclass Classifier:\n    def __init__(self, class_labels, random_state=None):\n        \"\"\"\n        Initializes the classifier with given class labels and an optional random state.\n        \n        Args:\n        class_labels (List): List of all possible class labels.\n        random_state (int, optional): Seed for the random number generator.\n        \"\"\"\n        self.class_labels = class_labels\n        self.random_state = random_state\n        np.random.seed(random_state)\n    \n    def predict_proba(self, X):\n        \"\"\"Generates predicted probabilities for the samples in X.\n        \n        This dummy implementation randomly generates probability for each class\n        per sample in X. It's purely for illustration as per the constraints of the problem.\n        \n        Args:\n        X (np.ndarray): Input features, assumption is that it's numeric.\n        \n        Returns:\n        np.ndarray: Array of probabilities per class for each sample.\n        \"\"\"\n        # Number of samples\n        num_samples = X.shape[0]\n        # Number of classes\n        num_classes = len(self.class_labels)\n        # Random probabilities for each class for each sample\n        probabilities = np.random.rand(num_samples, num_classes)\n        # Normalize probabilities so they sum to 1\n        probabilities /= probabilities.sum(axis=1)[:, None]\n        return probabilities\n    \n    def predict(self, X):\n        \"\"\"Generates predictions for the given input data X. For each sample,\n        it obtains the predicted probabilities, selects the indices of the\n        maximum probability values, and randomly chooses one among them. Then \n        maps the chosen index to the corresponding class label. \n        \n        Args:\n        X (np.ndarray): Input feature matrix.\n        \n        Returns:\n        np.ndarray: Array of predicted class labels.\n        \"\"\"\n        # Obtain the predicted probabilities\n        probas = self.predict_proba(X)\n        \n        # Find the indices of maximum probabilities\n        max_indices = np.argmax(probas, axis=1)\n        \n        # Use indices to find corresponding class labels\n        predictions = [self.class_labels[idx] for idx in max_indices]\n        \n        return np.array(predictions)\n\n# Example Usage:\n# Assuming we have a predefined set of class labels and some dummy data\nclass_labels = ['cat', 'dog', 'bird']\nclassifier = Classifier(class_labels, random_state=42)\nX_dummy = np.random.rand(10, 5)  # 10 samples, 5 features each\npredictions = classifier.predict(X_dummy)\nprint(predictions)", "CodeBleu_score": 28.69189111929209}]}
{"instance_id": "210b60dffd55", "generation_result": [{"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups):\n    \"\"\"\n    The function _predict_proba_for_estimator generates class probabilities for\n    the given input data X using a specified classifier, PCA transformations, \n    and groups of features. It first transforms the input data by applying the\n    corresponding PCA to each group and concatenating the results. Missing values \n    in the transformed data are replaced with zeros. The function then computes \n    class probabilities using the classifier. If the number of predicted classes \n    does not match the expected number of classes, it adjusts the probabilities \n    to match the expected class structure. The function returns the adjusted\n    class probabilities.\n\n    Args:\n        X (pd.DataFrame): The input data, structured as a DataFrame.\n        clf: A classifier with `predict_proba` method.\n        pcas (list): A list of PCA transformers corresponding to each group.\n        groups (list of lists): Each sublist contains column indices representing one group.\n\n    Returns:\n        np.array: The adjusted class probabilities for each sample.\n    \"\"\"\n    import numpy as np\n\n    # Apply PCA transformations and concatenate results\n    transformed_data = []\n\n    for group, pca in zip(groups, pcas):\n        group_data = X.iloc[:, group]\n        # Apply PCA only to the columns specified in the group\n        pca_data = pca.transform(group_data)\n        # Fill missing values with zero in transformed data\n        pca_data = np.nan_to_num(pca_data)\n        transformed_data.append(pca_data)\n    \n    # Concatenate all transformed data horizontally\n    feature_data = np.hstack(transformed_data)\n    \n    # Compute class probabilities\n    probabilities = clf.predict_proba(feature_data)\n\n    # Check and adjust probabilities if needed\n    classes_expected = clf.classes_\n    num_classes_expected = len(classes_expected)\n    \n    if probabilities.shape[1] != num_classes_expected:\n        # Create a new probability matrix with the expected class structure\n        adjusted_probabilities = np.zeros((probabilities.shape[0], num_classes_expected))\n        \n        # Map the existing probabilities to the respective classes\n        class_indices = {cls: idx for idx, cls in enumerate(clf.classes_)}\n        for idx, cls in enumerate(classes_expected):\n            if cls in class_indices:\n                adjusted_probabilities[:, idx] = probabilities[:, class_indices[cls]]\n        \n        probabilities = adjusted_probabilities\n\n    return probabilities", "CodeBleu_score": 34.899928487749555}, {"code": "import numpy as np\n\ndef _predict_proba_for_estimator(self, X, clf, pcas, groups):\n    \"\"\"\n    The function _predict_proba_for_estimator generates class probabilities for the given input data `X`\n    using a specified classifier `clf`, PCA transformations `pcas`, and groups of features `groups`.\n    It first transforms the input data by applying the corresponding PCA to each group and concatenating the results.\n    Missing values in the transformed data are replaced with zeros. The function then computes class probabilities\n    using the classifier.\n    If the number of predicted classes does not match the expected number of classes, it adjusts the probabilities to match\n    the expected class structure.\n    The function returns the adjusted class probabilities.\n\n    Parameters:\n        X (np.array): The data to transform and predict.\n        clf (classifier): Trained classifer which can predict probabilities (e.g., sklearn's classifiers with `predict_proba` method).\n        pcas (list of PCA objects): A list of PCA objects corresponding to each group, used to transform the data.\n        groups (list of lists): Each sublist corresponds to a group of columns in `X` that should be transformed by the corresponding PCA in `pcas`.\n\n    Returns:\n        np.array: An array of class probabilities, adjusted as necessary.\n\n    \"\"\"\n\n    # Check that the number of PCA objects and groups match\n    assert len(pcas) == len(groups), \"The number of PCA objects and number of feature groups must be equal\"\n\n    # Create an empty list to hold transformed subarrays\n    transformed_subarrays = []\n\n    # Iterate over each group and corresponding PCA\n    for idx, group in enumerate(groups):\n        pca = pcas[idx]\n        subarray = X[:, group]\n        # Apply PCA transformation to the data of the current group\n        transformed_data = pca.transform(subarray)\n        # Append transformed data to list\n        transformed_subarrays.append(transformed_data)\n\n    # Concatenate all transformed subarrays horizontally\n    transformed_X = np.concatenate(transformed_subarrays, axis=1)\n\n    # Replace any missing values with zeros\n    transformed_X = np.nan_to_num(transformed_X)\n\n    # Use classifier to predict probabilities\n    predicted_probabilities = clf.predict_proba(transformed_X)\n\n    # Ensure correct class probabilities format (Assuming self.classes_ is set somewhere in the class)\n    num_classes = len(self.classes_)\n    if predicted_probabilities.shape[1] != num_classes:\n        # Initialize a new probabilities array with correct shape and zero fill\n        adjusted_probabilities = np.zeros((predicted_probabilities.shape[0], num_classes))\n        for idx, class_label in enumerate(clf.classes_):\n            class_idx = np.where(self.classes_ == class_label)[0][0]\n            adjusted_probabilities[:, class_idx] = predicted_probabilities[:, idx]\n        predicted_probabilities = adjusted_probabilities\n\n    return predicted_probabilities", "CodeBleu_score": 36.96047603366016}, {"code": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)\ndef _predict_proba_for_estimator(self, X, clf, pcas, groups):\n    \"\"\"\n    The function _predict_proba_for_estimator generates class probabilities for the given input data X using a specified \n    classifier, PCA transformations, and groups of features. It first transforms the input data by applying the corresponding\n    PCA to each group and concatenating the results. Missing values in the transformed data are replaced with zeros. The \n    function then computes class probabilities using the classifier. If the number of predicted classes does not match the \n    expected number of classes, it adjusts the probabilities to match the expected class structure. The function returns the \n    adjusted class probabilities.\n\n    Parameters:\n        X (np.array): The input data.\n        clf (classifier): The classifier to predict probabilities.\n        pcas (list of PCA objects): List of PCA objects corresponding to each group in 'groups'.\n        groups (list of lists): Each sublist contains indices of features in 'X' that belong to one group.\n\n    Returns:\n        np.array: Array of predicted class probabilities, adjusted to the expected structure.\n    \"\"\"\n    import numpy as np\n    \n    # Transformed data by applying PCA to each group and concatenating results\n    transformed_data = []\n    \n    for index, group_indices in enumerate(groups):\n        group_data = X[:, group_indices]  # Extract group features\n        pca = pcas[index]\n        transformed_group_data = pca.transform(group_data)  # Apply PCA\n        transformed_data.append(transformed_group_data)\n    \n    # Concatenate all transformed group data horizontally\n    transformed_X = np.concatenate(transformed_data, axis=1)\n    \n    # Replace missing values with zeros\n    transformed_X[np.isnan(transformed_X)] = 0\n    \n    # Compute the class probabilities using the classifier\n    class_probabilities = clf.predict_proba(transformed_X)\n    \n    # It's assumed that there's a predefined number of classes, `n_classes`\n    n_classes = clf.classes_.shape[0]\n    n_expected_classes = len(self.classes_)  # Assuming self.classes_ was previously defined and populated\n    \n    # Adjust class probabilities if mismatch in number of classes\n    if n_classes != n_expected_classes:\n        # Create a template with zeros for all expected classes\n        adjusted_probabilities = np.zeros((class_probabilities.shape[0], n_expected_classes))\n        \n        # Map existing class probabilities to the correct columns in adjusted probabilities\n        for idx, class_label in enumerate(clf.classes_):\n            if class_label in self.classes_:\n                pos = np.where(self.classes_ == class_label)[0][0]\n                adjusted_probabilities[:, pos] = class_probabilities[:, idx]\n        \n        class_probabilities = adjusted_probabilities\n    \n    return class_probabilities\n", "CodeBleu_score": 34.22759894284398}]}
{"instance_id": "9436ee0860bb", "generation_result": [{"code": "import numpy as np\n\nclass DecisionTreeModel:\n    def __init__(self):\n        self.is_fitted = False\n        self.root_node = None  # Assume some kind of Node class that does the actual predictions\n\n    def fit(self, X, y):\n        # This is a placeholder for the actual fitting logic\n        self.root_node = Node()  # Hypothetical Node class instance\n        self.is_fitted = True\n\n    def predict_proba(self, X):\n        \"\"\"This function predicts class probabilities for the given input data.\n        It first checks if the model is fitted and raises an error if not.\n        For a single class, it returns a probability of 1 for all inputs.\n        It reshapes the input if it is a 3D numpy array with a single feature, or raises an error for invalid input types or shapes.\n        The input data is validated, allowing NaN values.\n        It calculates the probability distributions by iterating over each input sample and using the root node's predict_proba method.\n        Finally, it returns the computed probability distributions.\"\"\"\n        \n        if not self.is_fitted:\n            raise ValueError(\"The model must be fitted before prediction.\")\n\n        # Check the input type and shape (assuming X should be 2D or 3D array for the input)\n        if isinstance(X, np.ndarray):\n            if X.ndim == 3 and X.shape[2] == 1:\n                # Flatten the third dimension if there's only one feature\n                X = X.reshape(X.shape[0], X.shape[1])\n            elif X.ndim not in [2, 3]:\n                raise ValueError(\"Input data must be a 2D or 3D array with only one feature in the third dimension.\")\n        else:\n            raise TypeError(\"Input data must be a numpy array.\")\n\n        # Handling NaN values: let's assume we don't need to remove or impute them here\n\n        # Calculating probabilities using the root node's `predict_proba` method\n        probabilities = np.array([self.root_node.predict_proba(x) for x in X])\n\n        return probabilities\n\nclass Node:\n    def predict_proba(self, x):\n        # As a placeholder, predict 0.5 probability for two classes for all data\n        return [0.5, 0.5]  # Assuming two possible class outputs as an example\n\n# Example usage:\nmodel = DecisionTreeModel()\nmodel.fit(None, None)  # Detail of fit method are abstracted\nX_test = np.array([[[1], [2]], [[3], [4]]])\nprobs = model.predict_proba(X_test)  # Testing input validation, reshape, and probs calc\nprint(probs)", "CodeBleu_score": 33.84179408585675}, {"code": "import numpy as np\n\nclass MyModel:\n    def __init__(self):\n        self.fitted = False\n        self.root_node = None\n        \n    def fit(self, X, y):\n        # Hypothetical fit method that configures a root node (for demo purposes) and marks the model as fitted\n        self.root_node = self._build_tree(X, y)\n        self.fitted = True\n    \n    def _build_tree(self, X, y):\n        # Placeholder for actual tree building logic\n        class Node:\n            def predict_proba(self, x):\n                return np.array([1.0])\n        return Node()\n    \n    def predict_proba(self, X):\n        \"\"\"This function predicts class probabilities for the given input data.\n        It first checks if the model is fitted and raises an error if not.\n        For a single class, it returns a probability of 1 for all inputs.\n        It reshapes the input if it is a 3D numpy array with a single feature, or raises an error for invalid input types or shapes.\n        The input data is validated, allowing NaN values.\n        It calculates the probability distributions by iterating over each input sample and using the root node's predict_proba method.\n        Finally, it returns the computed probability distributions.\n\n        :param X: Input data (can be 2D or 3D numpy array where third dimension is 1)\n        :returns: The predicted class probabilities for each input\n        :raises ValueError: If model is not fitted, input dimensions are invalid, or input is not a numpy array\n        \"\"\"\n        if not self.fitted:\n            raise ValueError(\"The model is not fitted yet. Please fit the model before predicting probabilities.\")\n\n        if not isinstance(X, np.ndarray):\n            raise ValueError(\"Input should be a numpy array.\")\n\n        if X.ndim == 3:\n            if X.shape[2] != 1:\n                raise ValueError(\"3D input must have only one feature along the third dimension.\")\n            X = X.reshape(X.shape[0], X.shape[1])\n        elif X.ndim != 2:\n            raise ValueError(\"Input should either be 2D or 3D with a single feature in the third dimension.\")\n\n        # Initialize an empty list to store the probabilities for each sample\n        probabilities = []\n    \n        for sample in X:\n            if self.root_node is not None:\n                prob = self.root_node.predict_proba(sample)\n            else:\n                prob = np.array([1.0])\n            probabilities.append(prob)\n        \n        return np.array(probabilities)", "CodeBleu_score": 32.4072142684088}, {"code": "import numpy as np\n\nclass DecisionTreeClassifier:\n    def __init__(self):\n        self.is_fitted = False\n        self.root_node = None\n\n    def fit(self, X, y):\n        # Simplified example of fitting, not a real implementation:\n        # Here we would construct the tree and adjust the is_fitted flag and root_node.\n        self.root_node = TreeNode()  # Example node\n        self.is_fitted = True\n\n    def predict_proba(self, X):\n        # Check if the model is fitted\n        if not self.is_fitted:\n            raise Exception(\"The model needs to be fitted before predicting.\")\n\n        # Check valid input type and reshape if necessary\n        if isinstance(X, np.ndarray):\n            if X.ndim == 3 and X.shape[2] == 1:\n                X = X.reshape(X.shape[0], X.shape[1])\n            elif X.ndim != 2:\n                raise ValueError(\"Input data must be a 2D array\")\n        else:\n            raise TypeError(\"Input type not supported. X should be a numpy array.\")\n\n        # Validate input data (allow nan but here we assume the tree can handle it)\n        if np.isnan(X).any():\n            pass  # Assuming model handles NaNs internally or data is pre-imputed before fitting\n\n        # Initialize the probability distribution output\n        num_samples = X.shape[0]\n        num_classes = 2  # Assuming binary classification for simplification\n        probas = np.zeros((num_samples, num_classes))\n\n        # Calculate the probability distributions using the model's root node\n        for i in range(num_samples):\n            probas[i, :] = self.root_node.predict_proba(X[i, :])\n\n        return probas\n\nclass TreeNode:\n    def predict_proba(self, x):\n        # A dummy prediction method just for example\n        return np.array([0.5, 0.5])  # This would be more complex in reality\n\n# Example creation and use:\nclf = DecisionTreeClassifier()\nclf.fit(np.array([[1], [2]]), np.array([0, 1]))  # Dummy fit with minimal data\nprobas = clf.predict_proba(np.array([[1], [2], [3]]))\nprint(probas)", "CodeBleu_score": 33.73571963734602}]}
{"instance_id": "d7cb1cb66840", "generation_result": [{"code": "import numpy as np\n\ndef _convert_X_to_sklearn(self, X):\n    \"\"\"\n    Converts input data X into a 2D numpy array format required by scikit-learn.\n    \n    Determines the scitype of the transformed output from the transformers_ attribute.\n    \n    If the scitype is \"Primitives\", it converts X to a 2D numpy array with Table scitype.\n    If the scitype is \"Series\", it converts X to a flat 2D numpy array with Panel scitype.\n    If the scitype is neither, it raises a TypeError indicating an unexpected output type.\n    \n    Returns:\n        numpy.ndarray: The converted 2D numpy array.\n    \n    Raises:\n        TypeError: If the scitype of output is neither \"Primitives\" nor \"Series\".\n    \"\"\"\n    # Retrieve the scitype from the configured transformers (assuming there's an attribute for it)\n    scitype = self.transformers_.scitype if hasattr(self.transformers_, 'scitype') else None\n\n    if scitype is None:\n        raise AttributeError(\"Transformers do not have a 'scitype' attribute.\")\n\n    if scitype == \"Primitives\":\n        # Convert X to a 2D numpy array with Table scitype (assume X is a pandas DataFrame)\n        result = np.array(X)\n        if result.ndim == 1:\n            result = result.reshape(-1, 1)\n    elif scitype == \"Series\":\n        # Convert X to a flat 2D numpy array with Panel scitype (assume X is a list of pandas Series)\n        result = np.column_stack(X)\n    else:\n        raise TypeError(f\"Unexpected output scitype: {scitype}. Expected 'Primitives' or 'Series'\")\n    \n    return result", "CodeBleu_score": 36.450009657644664}, {"code": "import numpy as np\n\ndef _convert_X_to_sklearn(self, X):\n    \"\"\"The function _convert_X_to_sklearn converts input data X into a 2D numpy array format\n    required by scikit-learn. It determines the scitype of the transformed output from the\n    transformers_ attribute. If the scitype is \"Primitives\", it converts X to a 2D numpy\n    array with Table scitype. If the scitype is \"Series\", it converts X to a flat 2D\n    numpy array with Panel scitype. If the scitype is neither, it raises a TypeError\n    indicating an unexpected output type. The function returns the converted 2D numpy array.\n    \n    Args:\n    X (varied): The input data which might be a list, DataFrame, Series, or ndarray.\n\n    Returns:\n    numpy.ndarray: A 2D numpy array formatted for scikit-learn.\n    \"\"\"\n\n    # Determine the scitype from transformers_\n    try:\n        scitype = self.transformers_['scitype']  # Assuming self.transformers_ holds necessary transformation info\n    except AttributeError:\n        raise AttributeError(\"The object does not have transformers_ attribute.\")\n\n    # Convert X based on scitype\n    if scitype == 'Primitives':\n        # Convert to 2D numpy array for Table scitype. Usually this implies a typical DataFrame structure.\n        if isinstance(X, np.ndarray):\n            if X.ndim == 1:\n                return X.reshape(-1, 1)\n            else:\n                return X\n        elif isinstance(X, list):\n            return np.array(X).reshape(-1, 1)\n        elif hasattr(X, 'values'):  # For pandas DataFrame or Series\n            return X.values.reshape(-1, 1) if X.ndim == 1 else X.values\n        else:\n            raise ValueError(\"Unsupported data type for scitype 'Primitives'.\")\n\n    elif scitype == 'Series':\n        # Convert to flat 2D numpy array for Panel scitype, implies uniform length time series or similar.\n        if isinstance(X, np.ndarray):\n            if X.ndim == 1:\n                return X.reshape(1, -1)\n            elif X.ndim == 2:\n                return X\n            else:\n                raise ValueError(f\"Expected 1D or 2D array, got {X.ndim}D array for scitype 'Series'.\")\n        elif isinstance(X, list):\n            # Assuming list of lists (each sub-list is a series)\n            if all(isinstance(el, list) for el in X):\n                return np.array(X)\n            else:\n                return np.array([X]).T\n        elif hasattr(X, 'values'):  # For pandas DataFrame or Series\n            return X.values.T if X.ndim == 1 else X.values\n        else:\n            raise ValueError(\"Unsupported data type for scitype 'Series'.\")\n\n    else:\n        raise TypeError(f\"Unexpected scitype '{scitype}' encountered. Expected 'Primitives' or 'Series'.\")", "CodeBleu_score": 21.296731378205465}, {"code": "import numpy as np\n\ndef _convert_X_to_sklearn(self, X):\n    \"\"\"\n    The function _convert_X_to_sklearn converts input data X into a 2D numpy array format required by scikit-learn.\n    It determines the scitype of the transformed output from the transformers_ attribute. If the scitype is \"Primitives\",\n    it converts X to a 2D numpy array with Table scitype. If the scitype is \"Series\", it converts X to a flat 2D numpy\n    array with Panel scitype. If the scitype is neither, it raises a TypeError indicating an unexpected output type.\n    The function returns the converted 2D numpy array.\n    \n    Parameters:\n    - X: Input data which can be list, numpy array, pandas DataFrame or Series.\n    \n    Returns:\n    - numpy.ndarray: A 2D numpy array suitable for scikit-learn.\n    \"\"\"\n    if not hasattr(self, \"transformers_\"):\n        raise AttributeError(\"Instance does not have attribute 'transformers_'.\")\n\n    # Assume transformer_ contains scitype information\n    scitype = self.transformers_.scitype if hasattr(self.transformers_, \"scitype\") else None\n\n    if scitype not in [\"Primitives\", \"Series\"]:\n        raise TypeError(f\"Unexpected scitype: {scitype}. Expected 'Primitives' or 'Series'.\")\n\n    if isinstance(X, np.ndarray):\n        X_reshaped = X\n    elif isinstance(X, (list, pd.Series)):\n        X_reshaped = np.array(X)\n    elif isinstance(X, pd.DataFrame):\n        X_reshaped = X.values\n    else:\n        raise TypeError(\"Input X must be a list, numpy ndarray, pandas Series, or DataFrame\")\n\n    # Ensure X is 2D\n    if X_reshaped.ndim == 1:\n        X_reshaped = X_reshaped.reshape(-1, 1)\n\n    if scitype == \"Primitives\":\n        # Table scitype expects 2D data (samples, features)\n        return X_reshaped\n    elif scitype == \"Series\":\n        # Panel scitype expects 2D data but interpreted differently (associate each sample with a sequence)\n        return X_reshaped\n\n    raise ValueError(\"The scitype specified does not result in an appropriate transformation.\")", "CodeBleu_score": 31.78294145950541}]}
{"instance_id": "497cb75f2b4d", "generation_result": [{"code": "def set_random_state(estimator, random_state=0):\n    \"\"\"\n    This function sets the random state for an estimator.\n    It first ensures the random_state parameter is a valid random state object.\n    Then, it initializes an empty dictionary to hold parameters that need updating.\n    It iterates over the estimator's parameters, checking for those related to random state.\n    If such parameters are found, they are assigned a random integer value.\n    Finally, the function updates the estimator with the new random state values if any were found.\n    \n    Parameters:\n        estimator: The estimator whose random state should be set.\n        random_state: An integer or None. The random state value to be set for the estimator.\n\n    Returns:\n        None. The estimator's random state is updated in-place.\n    \"\"\"\n    \n    import numpy as np\n    \n    if not isinstance(random_state, (int, type(None))):\n        raise ValueError(\"Random state must be an integer or None\")\n    \n    # Parameters to update\n    params_to_update = {}\n    \n    # Get estimator parameters\n    params = estimator.get_params()\n    \n    # Iterate over the parameters to find those that include 'random_state'\n    for param_name, param_value in params.items():\n        if 'random_state' in param_name:\n            # Assign the new random state\n            params_to_update[param_name] = random_state\n    \n    # Update the estimator parameters if any random state parameters were found\n    if params_to_update:\n        estimator.set_params(**params_to_update)", "CodeBleu_score": 30.39215398798912}, {"code": "def set_random_state(estimator, random_state=0):\n    \"\"\"\n    This function sets the random state for an estimator.\n    It first ensures the random_state parameter is a valid random state object.\n    Then, it initializes an empty dictionary to hold parameters that need updating.\n    It iterates over the estimator's parameters, checking for those related to random state.\n    If such parameters are found, they are assigned a random integer value.\n    Finally, the function updates the estimator with the new random state values if any were found.\n    \n    Args:\n    estimator: the estimator to update with a random state. It should have a get_params() method.\n    random_state (int or RandomState): can be an integer seed or an np.random.RandomState object.\n    \"\"\"\n    import numpy as np\n    \n    # Ensure random_state is a valid random state object\n    if isinstance(random_state, int):\n        random_state = np.random.RandomState(random_state)\n    elif not isinstance(random_state, np.random.RandomState):\n        raise ValueError(\"random_state must be an integer or np.random.RandomState instance\")\n\n    # Initialize a dictionary for parameters update\n    params_to_update = {}\n\n    # Iterate over estimator's parameters to find random state related ones\n    for param, value in estimator.get_params().items():\n        if 'random_state' in param:\n            params_to_update[param] = random_state.randint(0, 2**32)\n    \n    # Update the estimator with new random state values\n    if params_to_update:\n        estimator.set_params(**params_to_update)", "CodeBleu_score": 32.078028707950764}, {"code": "from sklearn.utils import check_random_state\nimport numpy as np\n\ndef set_random_state(estimator, random_state=0):\n    \"\"\"\n    This function sets the random state for an estimator.\n    It first ensures the random_state parameter is a valid random state object.\n    Then, it initializes an empty dictionary to hold parameters that need updating.\n    It iterates over the estimator's parameters, checking for those related to random state.\n    If such parameters are found, they are assigned a random integer value.\n    Finally, the function updates the estimator with the new random state values if any were found.\n\n    Args:\n    estimator: Estimator instance from scikit-learn that potentially utilizes a random state.\n    random_state: int, RandomState instance or None, default=0\n        Optional integer seed, random state instance, or None for no seed (randomness).\n    \"\"\"\n    random_state_obj = check_random_state(random_state)\n    random_state_val = random_state_obj.randint(0, 4294967296)  # high number to ensure broad range for random state\n    \n    params_to_update = {}\n    \n    # Iterate over all parameters of the estimator\n    if hasattr(estimator, 'get_params'):\n        params = estimator.get_params()\n        for param, value in params.items():\n            if 'random_state' in param:\n                params_to_update[param] = random_state_val\n    \n    # Update the estimator with new random_state values\n    if params_to_update:\n        estimator.set_params(**params_to_update)", "CodeBleu_score": 31.90864053114064}]}
{"instance_id": "59ffd1d596a1", "generation_result": [{"code": "import numpy as np\n\ndef _transform(X, intervals):\n    \"\"\"\n    The function _transform processes the input matrix X using specified intervals. \n    It calculates the number of instances and intervals, then initializes an empty \n    array to store the transformed features. For each interval, it slices the input \n    matrix, computes the mean, standard deviation, and slope of the slice, and stores \n    these values in the transformed array. The function returns the transposed transformed array.\n    \"\"\"\n    n_features = X.shape[0]\n    transformed_features = []\n\n    for start, end in intervals:\n        # Slice the matrix for each interval\n        X_slice = X[:, start:end + 1]\n        \n        # Calculate mean and standard deviation\n        means = np.mean(X_slice, axis=1)\n        stds = np.std(X_slice, axis=1)\n        \n        # Calculate slopes (assuming uniform spacing of points)\n        indices = np.arange(start, end + 1)\n        slopes = []\n        for row in X_slice:\n            # Fit a linear model\n            slope = np.polyfit(indices, row, 1)[0]  # We only need the slope coefficient\n            slopes.append(slope)\n        \n        # Combine the features: mean, std, and slope\n        feature_set = np.vstack((means, stds, slopes)).T\n        transformed_features.append(feature_set)\n\n    # Combine all the interval features\n    transformed_features = np.hstack(transformed_features)\n\n    # Transpose the final feature set to match expected output format\n    return transformed_features.T", "CodeBleu_score": 31.183589844256737}, {"code": "import numpy as np\n\ndef _transform(X, intervals):\n    \"\"\"\n    Processes the input matrix X using specified intervals. It calculates the number of instances and intervals,\n    then initializes an empty array to store the transformed features. For each interval start and end,\n    it slices the input matrix, computes the mean, standard deviation, and slope of the slice,\n    and stores these values in the transformed array. The function returns the transposed transformed array.\n\n    Args:\n    X (np.array): A 2D numpy array where rows correspond to instances and columns to features.\n    intervals (list of tuples): A list of tuples, where each tuple contains (start_index, end_index) defining \n                                an interval on the columns of X.\n\n    Returns:\n    np.array: A 2D numpy array where each row now represents the transformed features for each interval.\n    \"\"\"\n    num_instances, num_features = X.shape\n    transformed_features = []\n    \n    # Iterate over each interval\n    for start_index, end_index in intervals:\n        # Slice the matrix for the current interval\n        X_sliced = X[:, start_index:end_index]\n        \n        # Compute the mean and standard deviation\n        means = np.mean(X_sliced, axis=1)\n        stds = np.std(X_sliced, axis=1)\n        \n        # Compute the slope of each row in the sliced matrix\n        slopes = []\n        for row in X_sliced:\n            x = np.arange(start_index, end_index)\n            y = row\n            if len(x) > 1:  # To safely handle computation of slopes with least squares\n                slopes.append(np.polyfit(x, y, 1)[0])  # Linear fit, get the slope (m in y=mx+c)\n            else:\n                slopes.append(0)  # If the interval has one column, the slope is undefined, assume 0\n\n        # Combine the calculated means, stds, and slopes into a single feature set for the interval\n        interval_features = np.stack([means, stds, slopes], axis=-1).reshape(num_instances, -1)\n        transformed_features.append(interval_features)\n    \n    # Combine all interval features\n    transformed_features = np.hstack(transformed_features)\n    \n    return transformed_features", "CodeBleu_score": 29.529395718557005}, {"code": "import numpy as np\n\ndef _transform(X, intervals):\n    \"\"\"The function _transform processes the input matrix X using specified intervals. It calculates the number of instances and intervals, then initializes an empty array to store the transformed features. For each interval, it slices the input matrix, computes the mean, standard deviation, and slope of the slice, and stores these values in the transformed array. The function returns the transposed transformed array.\n\n    Args:\n    X (numpy.ndarray): A 2D array of shape (n_samples, n_features) representing the input data.\n    intervals (list of tuples): A list of tuples wherein each tuple contains two integers representing the start and end indices (inclusive) of an interval on the feature axis.\n\n    Returns:\n    numpy.ndarray: A transformed 2D array of shape (n_features * len(intervals), n_samples).\n    \"\"\"\n\n    n_samples = X.shape[0]\n    feature_transformed = []  # This will store the transformed features for each interval\n\n    # Looping over each interval to transform features\n    for start, end in intervals:\n        X_slice = X[:, start:end+1]  # slicing the array based on the current interval\n        \n        # Calculate mean, standard deviation, and possibly other features across axis=1\n        mean = np.mean(X_slice, axis=1)\n        std = np.std(X_slice, axis=1)\n        \n        # Calculating slope by linear regression over column indices for each row\n        time_indices = np.arange(start, end+1)\n        slopes = np.array([np.polyfit(time_indices, X_slice[i, :], 1)[0] for i in range(n_samples)])\n\n        # Combine the computed statistics\n        feature_transformed.append(np.vstack((mean, std, slopes)))\n\n    # Stack and transpose the array so the shape becomes (n_features * len(intervals), n_samples)\n    transformed_features = np.hstack(feature_transformed).T  # Transposing to meet the output requirements\n\n    return transformed_features", "CodeBleu_score": 28.19185514010771}]}
{"instance_id": "5a01594eebfb", "generation_result": [{"code": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list], Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list], Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs):\n    \"\"\"\n    This function generates and annotates a plot using Seaborn and various configuration parameters.\n    It initializes annotation and operation parameters if they are not provided.\n    It creates the plot with given parameters and applies pre-plot axis operations.\n    The plot is further configured and annotated based on provided configuration and annotation functions.\n    Post-plot axis operations are then applied.\n    Finally, the function returns the output of the plotting process.\n    \"\"\"\n    \n    # Set default dictionaries if not provided\n    if annotation_params is None:\n        annotation_params = {}\n    if annotate_params is None:\n        annotate_params = {}\n\n    # Apply seaborn theme from configuration\n    sns.set_theme(style=configuration.get(\"style\", \"darkgrid\"))\n    \n    # Create a figure and axis if not using a FacetGrid directly\n    plt.figure(figsize=configuration.get(\"figsize\", (10, 6)))\n    ax = plt.gca()\n\n    # Applying pre-plot axis operations\n    if ax_op_before:\n        for op in ax_op_before:\n            if isinstance(op, dict):\n                method = getattr(ax, op['method'])\n                params = op.get('params', {})\n            elif isinstance(op, str):\n                method = getattr(ax, op)\n                params = {}\n            method(**params)\n    \n    # Create the plot using seaborn\n    plot_func = getattr(sns, plot)\n    g = plot_func(data=plot_params[\"data\"], **{k: v for k, v in plot_params.items() if k != 'data'})\n    \n    # Configuration potential FacetGrid operations\n    if isinstance(g, sns.axisgrid.FacetGrid):\n        g = g.map_dataframe(getattr(sns, annotation_func), **annotation_params)\n\n    # Applying post-plot configuration like labels, limits, etc.\n    if configuration.get(\"xlabel\"):\n        ax.set_xlabel(configuration[\"xlabel\"])\n    if configuration.get(\"ylabel\"):\n        ax.set_ylabel(configuration[\"ylabel\"])\n    if \"xlim\" in configuration:\n        ax.set_xlim(*configuration[\"xlim\"])\n    if \"ylim\" in configuration:\n        ax.set_ylim(*configuration[\"ylim\"])\n    if \"title\" in configuration:\n        ax.set_title(configuration[\"title\"])\n\n    # Applying annotations using a specified function\n    if annotation_func and hasattr(self, annotation_func):\n        annotator = getattr(self, annotation_func)\n        annot_params = annotation_params if annotation_params else {}\n        annotator(g, *args, **annot_params)\n    \n    # Applying post-plot axis operations\n    if ax_op_after:\n        for op in ax_op_after:\n            if isinstance(op, dict):\n                method = getattr(ax, op['method'])\n                params = op.get('params', {})\n            elif isinstance(op, str):\n                method = getattr(ax, op)\n                params = {}\n            method(**params)\n\n    plt.show()\n\n    return g", "CodeBleu_score": 42.26398818295646}, {"code": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list], Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list], Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs):\n    \"\"\"\n    This function generates and annotates a plot using Seaborn and various configuration parameters.\n    It initializes annotation and operation parameters if they are not provided.\n    It creates the plot with given parameters and applies pre-plot axis operations.\n    The plot is further configured and annotated based on provided configuration and annotation functions.\n    Post-plot axis operations are then applied.\n    Finally, the function returns the output of the plotting process.\n    \"\"\"\n    # Initialize annotation parameters if not provided\n    if annotation_params is None:\n        annotation_params = {}\n\n    if ax_op_before is None:\n        ax_op_before = []\n        \n    if ax_op_after is None:\n        ax_op_after = []\n\n    # Create a figure and axis object\n    fig, ax = plt.subplots()\n\n    # Perform any axis operations defined to be done before plotting\n    for op in ax_op_before:\n        if isinstance(op, str):\n            getattr(ax, op)()\n        elif isinstance(op, list):\n            func_name = op[0]\n            args = op[1] if len(op) > 1 else []\n            kwargs = op[2] if len(op) > 2 else {}\n            getattr(ax, func_name)(*args, **kwargs)\n        elif isinstance(op, dict):\n            for func, params in op.items():\n                getattr(ax, func)(**params)\n\n    # Prepare plot parameters and create the plot\n    plot_func = getattr(sns, plot)\n    plot_obj = plot_func(ax=ax, **plot_params)\n\n    # Setting axis labels, titles and other properties\n    ax.set(**configuration)\n\n    # Perform annotation using the specified function\n    if hasattr(self, annotation_func):\n        annotation_callable = getattr(self, annotation_func)\n        annotation_callable(ax, *args, **annotation_params)\n\n    # Perform any axis operations defined to be done after plotting\n    for op in ax_op_after:\n        if isinstance(op, str):\n            getattr(ax, op)()\n        elif isinstance(op, list):\n            func_name = op[0]\n            args = op[1] if len(op) > 1 else []\n            kwargs = op[2] if len(op) > 2 else {}\n            getattr(ax, func_name)(*args, **kwargs)\n        elif isinstance(op, dict):\n            for func, params in op.items():\n                getattr(ax, func)(**params)\n\n    # Return the plot object for further manipulation or display\n    return plot_obj\n\n# Note: Ensure that the object 'self' has the respective annotation function implemented which you're referring to by string `annotation_func`.\n# The use of `getattr(self, annotation_func)` assumes that such a function exists within the object/class where this method resides.", "CodeBleu_score": 42.733159134683}, {"code": "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Union, Optional, Dict, Any, Callable\n\nclass Plotter:\n    def plot_and_annotate_facets(\n        self, plot: str, plot_params: dict, configuration: dict,\n        annotation_func: str, *args, annotation_params: dict = None,\n        ax_op_before: List[Union[str, Optional[list], Optional[dict]]] = None,\n        ax_op_after: List[Union[str, Optional[list], Optional[dict]]] = None,\n        annotate_params: dict = None, **kwargs\n    ):\n        \"\"\"\n        This function generates and annotates a plotting using Seaborn and various configuration parameters.\n        It initializes annotation and operation parameters if they are not provided.\n        It creates the plot with given parameters and applies pre-plot axis operations.\n        The plot is further configured and annotated based on provided configuration and annotation functions.\n        Post-plot axis operations are then applied.\n        Finally, the function returns the output of the plotting process.\n        \"\"\"\n\n        # Initialize plotting facet grid with plot_params\n        facet_grid = getattr(sns, plot)(**plot_params)\n        plt.figure(figsize=configuration.get(\"figsize\", (10, 6)))\n\n        # Execute pre-plot operations\n        if ax_op_before:\n            for op in ax_op_before:\n                # Could be dynamically calling matplotlib axes function (like 'set_ylim', 'set_title', etc.)\n                op_func = getattr(facet_grid.ax, op[0])\n                if callable(op_func):\n                    op_func(*op[1], **op[2])\n\n        # Configure the facet grid using `configuration`\n        facet_grid.set(**configuration)\n\n        # Annotation part\n        if annotation_func:\n            annotation_function = getattr(self, annotation_func)\n            if callable(annotation_function):\n                facet_grid.map_dataframe(annotation_function, *args, **(annotate_params or {}))\n\n        # Execute post-plot operations\n        if ax_op_after:\n            for op in ax_op_after:\n                op_func = getattr(facet_grid.ax, op[0])\n                if callable(op_func):\n                    op_func(*op[1], **op[2])\n\n        plt.tight_layout()\n        plt.show()\n\n        return facet_grid\n\n# Example usage:\nplot_params = {\n    'data': df,\n    'x': 'column1',\n    'y': 'column2',\n    'hue': 'column3'\n}\nconfiguration = {\n    'figsize': (15, 8)\n}\nplotter = Plotter()\ngrid = plotter.plot_and_annotate_facets('scatterplot', plot_params, configuration, 'annotate_points')\n\n# This implementation assumes a DataFrame `df` exists and method `annotate_points` which we use to annotate our plot.", "CodeBleu_score": 45.349217466687946}]}
{"instance_id": "b5b17d312c74", "generation_result": [{"code": "def predict_proba(self, X):\n    \"\"\" \n    Computes class probabilities for the input data X. \n    First, checks if the model has been fitted by verifying the presence of the '_program' attribute,\n    raising an AttributeError if not. \n    The input data is validated and its feature count is checked against the model's expected feature count,\n    raising a ValueError if they do not match. \n    The function then executes the program to obtain scores, transforms these scores into probabilities,\n    and constructs a 2D array with the probabilities for both classes. It returns this array of class probabilities.\n    \n    Args:\n    X (array-like): A 2D feature matrix.\n\n    Returns:\n    numpy.ndarray: A 2D array of shape (n_samples, 2) containing probabilities of the two classes for each input sample.\n    \"\"\"\n\n    import numpy as np\n\n    # Check if model is fitted\n    if not hasattr(self, '_program'):\n        raise AttributeError(\"predict_proba called before model was fitted\")\n    \n    # Validate input data (assuming it is numpy array for simplicity)\n    if not isinstance(X, np.ndarray) or len(X.shape) != 2:\n        raise ValueError(\"Input data X must be a 2D array\")\n    \n    # Check feature count matches\n    expected_feature_count = getattr(self, '_feature_count', X.shape[1])  # assume feature count stored during fitting\n    if X.shape[1] != expected_feature_count:\n        raise ValueError(\"Feature count of input X does not match the model's expected feature count\")\n\n    # Execute the program (Assume this part has to do with the inner mechanics of the model)\n    # Simple placeholder for executing program and getting scores\n    scores = self._program.run(X)\n\n    # Transform scores to probabilities (typically done with softmax for classification models)\n    def softmax(x):\n        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        return e_x / e_x.sum(axis=1, keepdims=True)\n\n    probabilities = softmax(scores)\n\n    # Ensuring probabilities are in the shape (n_samples, 2) even for binary outputs\n    if probabilities.shape[1] == 1:\n        probabilities = np.hstack([1 - probabilities, probabilities])\n\n    return probabilities", "CodeBleu_score": 47.910175286613075}, {"code": "def predict_proba(self, X):\n    \"\"\"\n    The function predict_proba computes class probabilities for the input data X. It first checks if the model has been fitted\n    by verifying the presence of the '_program' attribute, raising an error if not. The input data is validated and its feature \n    count is checked against the model's expected feature count, raising an error if they do not match. The function then executes \n    the program to obtain scores, transforms these scores into probabilities, and constructs a 2D array with the probabilities for \n    both classes. It returns this array of class probabilities.\n    \"\"\"\n    \n    # Check if the model has been fitted\n    if not hasattr(self, '_program'):\n        raise AttributeError(\"This model has not been fitted yet. Please fit the model before predicting probabilities.\")\n    \n    # Validate input data (assuming X is expected to be a NumPy array for simplicity)\n    import numpy as np\n    \n    if not isinstance(X, np.ndarray):\n        raise ValueError(\"Input data X must be a NumPy array.\")\n    \n    # Check if feature count matches\n    n_features_expected = self._program.feature_count\n    if X.shape[1] != n_features_expected:\n        raise ValueError(f\"Expected {n_features_expected} features, but got {X.shape[1]} features.\")\n\n    # Calculate scores using the fitted model's program (dummy implementation)\n    scores = self._program.execute(X)  # You'd replace this method with the actual score computation method\n\n    # Transform scores to probabilities (Logistic Function for bound between 0 and 1)\n    probabilities = 1 / (1 + np.exp(-scores))\n    \n    # Construct probabilities for both classes\n    class_probabilities = np.column_stack((1 - probabilities, probabilities))\n\n    return class_probabilities", "CodeBleu_score": 44.90879678031304}, {"code": "import numpy as np\n\nclass Model:\n    def __init__(self):\n        self._program = None\n        self.expected_features = None\n\n    def fit(self, X, y):\n        # This method should fit the model and set the _program and expected_features appropriately\n        # Dummy implementation:\n        self._program = lambda x: np.dot(x, np.random.rand(x.shape[1], 2))  # Example linear transformation\n        self.expected_features = X.shape[1]\n\n    def predict_proba(self, X):\n        \"\"\"\n        Computes class probabilities for the input data X. First checks if the model has been fitted \n        by verifying the presence of the '_program' attribute; raises an error if not. The input data \n        is validated and its feature count is checked against the model's expected feature count, \n        raising an error if they do not match. The function then executes the program to obtain scores, \n        transforms these scores into probabilities, and constructs a 2D array with the probabilities \n        for both classes. It returns this array of class probabilities.\n        \"\"\"\n        # Check if the model is fitted by ensuring _program exists\n        if not hasattr(self, '_program') or self._program is None:\n            raise RuntimeError(\"The model is not fitted yet.\")\n        \n        # Check if the input data has the correct number of features\n        if X.shape[1] != self.expected_features:\n            raise ValueError(\"The feature count of X does not match the expected feature count of the model.\")\n        \n        # Obtain scores using the model's _program\n        scores = self._program(X)\n        \n        # Transform scores into probabilities\n        # Assuming a logistic function for binary classification probabilities\n        probabilities = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n        \n        return probabilities", "CodeBleu_score": 44.160097426854406}]}
{"instance_id": "83f71c959df7", "generation_result": [{"code": "import copy\nimport random\n\nclass GeneticProgram:\n    def __init__(self, program, constant_range, num_features):\n        self.program = program  # Assume the program is represented as a nested list or a custom tree structure.\n        self.constant_range = constant_range  # Tuple (min, max) for possible constant values.\n        self.num_features = num_features  # Number of feature variables that can be terminals.\n    \n    def point_mutation(self, random_state):\n        \"\"\"\n        This function performs a point mutation on a program using a given random state.\n        It creates a copy of the program and identifies nodes to modify based on a uniform random distribution \n        and a probability threshold.\n        For each node to be mutated, if it is a function, it finds a valid replacement function with the same arity \n        and updates the node.\n        If the node is a terminal, it replaces it with either a constant or a variable, ensuring valid replacement \n        according to the defined constant range and number of features.\n        Finally, it returns the mutated program and the list of modified node indices.\n        \"\"\"\n        random.seed(random_state)\n        mutation_probability = 0.1  # Define mutation probability or make this an argument/attribute\n        mutated_program = copy.deepcopy(self.program)\n        modified_indices = []\n\n        def traverse(node, index=[]):\n            is_mutated = random.random() < mutation_probability\n            if is_mutated:\n                modified_indices.append(index.copy())\n\n                if isinstance(node, FunctionNode):\n                    # Pioneer a replacement function with the same arity\n                    same_arity_functions = [func for func in function_set if func.arity == node.arity]\n                    node.function = random.choice(same_arity_functions)\n                elif isinstance(node, TerminalNode):\n                    # Replace with either a constant or a variable\n                    if random.random() < 0.5:\n                        node.value = random.uniform(*self.constant_range)\n                    else:\n                        node.value = f\"x{random.randint(0, self.num_features - 1)}\"\n\n            for i, child in enumerate(node.children):\n                index.append(i)\n                traverse(child, index)\n                index.pop()\n\n        if isinstance(self.program, (FunctionNode, TerminalNode)):\n            traverse(self.program)\n        \n        return mutated_program, modified_indices\n\n# Example usage of a node class for easier management of function/terminal nodes:\nclass Node:\n    def __init__(self):\n        self.children = []\n\nclass FunctionNode(Node):\n    def __init__(self, function, arity):\n        super().__init__()\n        self.function = function\n        self.arity = arity\n\nclass TerminalNode(Node):\n    def __init__(self, value):\n        super().__init__()\n        self.value = value\n\n# Example function set for demonstration\nfunction_set = [\n    FunctionNode(\"add\", 2),\n    FunctionNode(\"sub\", 2),\n    FunctionNode(\"mul\", 2),\n    FunctionNode(\"div\", 2),\n    FunctionNode(\"neg\", 1)\n]", "CodeBleu_score": 25.859626987906882}, {"code": "import random\nfrom copy import deepcopy\n\nclass Program:\n    def __init__(self, nodes, constants_range=(-1, 1), num_features=3):\n        self.nodes = nodes  # list of nodes representing functions and terminals\n        self.constants_range = constants_range  # permissible range for constants\n        self.num_features = num_features  # number of features for variable selection\n\n    def point_mutation(self, random_state):\n        # Copy the original program nodes to avoid modifying them\n        new_nodes = deepcopy(self.nodes)\n        modified_indices = []\n\n        # Mutation probability threshold\n        mutation_probability = 0.1  # this could be an attribute or parameter, simplified here\n\n        for i, node in enumerate(new_nodes):\n            if random_state.uniform(0, 1) < mutation_probability:\n                if self.is_function(node):\n                    # Replace the function node with a valid function of the same arity\n                    new_nodes[i] = self.find_valid_function_with_same_arity(node.arity, random_state)\n                else:\n                    # Replace terminal node with either a constant or a variable\n                    new_nodes[i] = self.create_new_terminal(random_state)\n\n                # Track the index of the modified node\n                modified_indices.append(i)\n\n        return new_nodes, modified_indices\n\n    def is_function(self, node):\n        # Placeholder function to test if a node is a function\n        return hasattr(node, 'arity')\n\n    def find_valid_function_with_same_arity(self, arity, random_state):\n        # Placeholder method to find a new function of the same arity\n        # This could be a method fetching from a defined pool of function nodes\n        # Example: assuming list of functions each represented as ('func_name', arity)\n        valid_functions = [('add', 2), ('sub', 2), ('mul', 2), ('div', 2), ('sin', 1)]\n        choices = [func for func in valid_functions if func[1] == arity]\n        return random_state.choice(choices)\n\n    def create_new_terminal(self, random_state):\n        # Decide randomly to create either a new constant or a variable index\n        if random_state.random() < 0.5:\n            # Return a constant within the permissible range\n            return random_state.uniform(*self.constants_range)\n        else:\n            # Return a variable index, assuming 0-based indexing ('x0', 'x1', ...)\n            return 'x' + str(random_state.randint(0, self.num_features - 1))", "CodeBleu_score": 30.40326483073853}, {"code": "def point_mutation(self, random_state):\n    # First, we copy the program so changes are made to the new instance\n    mutated_program = self.copy()\n    \n    # Get all nodes from the program (assuming a method that lists all nodes exists)\n    nodes = mutated_program.get_nodes()\n    \n    # List to keep track of indices of mutated nodes\n    mutated_indices = []\n\n    # Probability threshold for mutating each node, assumed to be an existing class attribute\n    mutation_probability = mutated_program.mutation_probability\n    \n    # Iterate through the nodes, deciding at each step whether to mutate based on mutation probability\n    for i, node in enumerate(nodes):\n        if random_state.uniform(0, 1) < mutation_probability:\n            # Decide mutation based on the type of node: function or terminal\n            if node.is_function():\n                # Get a replacement function with the same arity (assuming a method/function exists)\n                replacements = mutated_program.get_valid_replacements(node.arity())\n                new_function = random_state.choice(replacements)\n                node.update_node(new_function)\n            elif node.is_terminal():\n                # Replace terminal node\n                if node.is_variable():\n                    # Replace variable node with another variable or constant\n                    possible_replacements = ['var_'+str(i) for i in range(mutated_program.num_features)] + \\\n                                            [random_state.uniform(mutated_program.constant_range[0], mutated_program.constant_range[1])]\n                    new_terminal = random_state.choice(possible_replacements)\n                else:\n                    # Node is a constant, replace with another constant\n                    new_terminal = random_state.uniform(mutated_program.constant_range[0], mutated_program.constant_range[1])\n                \n                node.update_node(new_terminal)\n\n            # Record the index of the mutated node\n            mutated_indices.append(i)\n\n    return mutated_program, mutated_indices", "CodeBleu_score": 31.19391085939281}]}
{"instance_id": "f1853251e671", "generation_result": [{"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\nimport pandas as pd\nimport numpy as np\n\ndef bias(close, length=10, mamode='sma', offset=0, **kwargs):\n    \"\"\"\n    Calculate the Bias (BIAS) indicator for a given series of closing prices.\n    \n    Parameters:\n        close (pd.Series): Series of closing prices.\n        length (int, optional): Length of the moving average. Defaults to 10.\n        mamode (str, optional): Type of moving average to use ('sma', 'ema', etc.). Defaults to 'sma'.\n        offset (int, optional): Offset to shift the BIAS result. Defaults to 0.\n\n    Returns:\n        pd.Series: Series containing the computed BIAS values.\n\n    Note:\n        The `**kwargs` allows passing additional parameters required by pandas' moving average functions.\n    \"\"\"\n    # Validate and process `close` Series\n    if close is None or len(close) == 0:\n        raise ValueError(\"Input 'close' is required and cannot be empty.\")\n\n    # Compute the moving average based on the specified mode\n    if mamode == 'sma':\n        ma = close.rolling(window=length, **kwargs).mean()\n    elif mamode == 'ema':\n        ma = close.ewm(span=length, adjust=False, **kwargs).mean()\n    else:\n        raise ValueError(f\"Unsupported moving average mode: {mamode}\")\n\n    # Calculate BIAS\n    bias = (close - ma) / ma * 100\n\n    # Handling the offset\n    if offset != 0:\n        bias = bias.shift(offset)\n\n    # Naming and categorizing the resulting series\n    bias.name = f'BIAS_{length}_{mamode}'\n    bias.index.name = 'Date'\n    bias.attrs['indicator_type'] = 'Momentum'\n\n    return bias", "CodeBleu_score": 37.06054880073406}, {"code": "import pandas as pd\nimport numpy as np\n\ndef bias(close, length=14, mamode='sma', offset=0, **kwargs):\n    \"\"\"\n    The function bias calculates the Bias (BIAS) indicator for a given series of closing prices. \n    It validates and sets default values for length, moving average mode, and offset. \n    It verifies the input series and calculates the moving average based on the specified mode. \n    The BIAS is then computed as the relative difference between the closing prices and \n    the moving average. The result can be shifted by the offset and filled with specified methods \n    if provided. The function names the resulting series and categorizes it as a momentum indicator before returning it.\n\n    Parameters:\n        close (pd.Series): Series of closing prices.\n        length (int, optional): The number of periods to calculate the moving average. Defaults to 14.\n        mamode (str, optional): The type of moving average to use ('sma', 'ema'). Defaults to 'sma'.\n        offset (int, optional): The number of periods to shift the BIAS indicator. Defaults to 0.\n        **kwargs: Additional keyword arguments to pass to the moving average function.\n\n    Returns:\n        pd.Series: The BIAS indicator series.\n    \"\"\"\n    # Validate input series\n    if not isinstance(close, pd.Series):\n        raise ValueError(\"The 'close' parameter should be a pandas Series object.\")\n    \n    # Selecting the moving average type\n    if mamode.lower() == 'sma':\n        ma = close.rolling(window=length, **kwargs).mean()\n    elif mamode.lower() == 'ema':\n        ma = close.ewm(span=length, **kwargs).mean()\n    else:\n        raise ValueError(\"Unsupported moving average mode. Please use either 'sma' or 'ema'.\")\n    \n    # Calculate BIAS\n    bias = (close - ma) / ma * 100\n\n    # Adding offset if needed\n    if offset != 0:\n        bias = bias.shift(offset)\n    \n    # Naming the result and treating this as a momentum indicator\n    bias.name = f'BIAS_{length}_{mamode.upper()}'\n    \n    return bias", "CodeBleu_score": 33.014585290629206}, {"code": "def bias(close, length=None, mamode=None, offset=None, **kwargs)\nimport pandas as pd\n\ndef bias(close, length=14, mamode='sma', offset=0, **kwargs):\n    \"\"\"\n    The function bias calculates the Bias (BIAS) indicator for a given series of closing prices.\n    It validates and sets default values for length, moving average mode, and offset.\n    It verifies the input series and calculates the moving average based on the specified mode.\n    The BIAS is then computed as the relative difference between the closing prices and the moving average.\n    The result can be shifted by the offset and filled with specified methods if provided.\n    The function names the resulting series and categorizes it as a momentum indicator before returning it.\n\n    Args:\n      close (pd.Series): Series of closing prices.\n      length (int, optional): The period of the moving average. Defaults to 14.\n      mamode (str, optional): The type of moving average to use ('sma', 'ema', etc.). Defaults to 'sma'.\n      offset (int, optional): The offset for the BIAS indicator to shift the resulting data. Defaults to 0.\n      **kwargs: Additional keywords for fine tuning the moving average calculation (e.g., 'min_periods' or methods like 'fillna').\n\n    Returns:\n      pd.Series: A series of the BIAS values adjusted for the provided offset, properly named.\n    \"\"\"\n    # Verify if the input series is a Pandas Series\n    if not isinstance(close, pd.Series):\n        raise ValueError('The `close` parameter must be a pandas Series')\n\n    # Choose the appropriate moving average function\n    if mamode.lower() == 'sma':\n        moving_avg = close.rolling(window=length, **kwargs).mean()\n    elif mamode.lower() == 'ema':\n        moving_avg = close.ewm(span=length, **kwargs).mean()\n    else:\n        raise ValueError(f\"Unsupported moving average mode '{mamode}'. Use 'sma' or 'ema'.\")\n\n    # Calculate the BIAS\n    bias = (close - moving_avg) / moving_avg * 100.0\n\n    # Adjust for the offset\n    bias = bias.shift(offset)\n\n    # Fill NaN values if applicable\n    if 'fillna' in kwargs:\n        bias = bias.fillna(kwargs['fillna'])\n    elif 'fillna_method' in kwargs:\n        bias = bias.fillna(method=kwargs['fillna_method'])\n\n    # Name the resulting series and indicate it's a momentum indicator\n    bias.name = f\"BIAS_{length}_{mamode.upper()}\"\n\n    return bias", "CodeBleu_score": 36.28479823492421}]}
{"instance_id": "77825be37c24", "generation_result": [{"code": "import pandas as pd\nimport requests\nimport traceback\n\ndef coins_bar(broker='hb', code='btc', ktype='D', size='2000'):\n    \"\"\"\n    This function retrieves cryptocurrency data from a specified broker.\n    It formats the URL based on the broker, cryptocurrency code, kline type, and size.\n    It fetches the data and handles any broker-specific data structures.\n    The data is then converted into a pandas DataFrame with specific columns.\n    Date formatting is adjusted based on the broker and kline type.\n    Finally, it converts the date column to datetime format and returns the DataFrame.\n    If an exception occurs, it prints the traceback.\n    \"\"\"\n    try:\n        # URL format based on broker\n        if broker == 'hb':\n            url = f'https://api.huobi.pro/market/history/kline?symbol={code}usdt&period={ktype.lower()}&size={size}'\n        else:\n            raise ValueError(\"Unsupported broker. Only 'hb' is currently supported.\")\n        \n        # Fetch the data\n        response = requests.get(url)\n        response.raise_for_status()  # will raise an HTTPError if the HTTP request returned an unsuccessful status code\n        data = response.json()\n        \n        # Check that data is valid\n        if 'data' not in data:\n            raise ValueError(\"No 'data' field in response. Please check the response format.\")\n        \n        # Convert the data to DataFrame\n        df = pd.DataFrame(data['data'])\n        \n        # Rename columns as required (broker specific transformations)\n        if broker == 'hb':\n            df.rename(columns={\n                'id': 'timestamp', \n                'open': 'open', \n                'close': 'close', \n                'low': 'low', \n                'high': 'high', \n                'amount': 'volume'\n            }, inplace=True)\n        \n        # Convert timestamps to datetime format\n        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n        \n        # Depending on the ktype, you might need additional formatting steps\n        # For daily data (D), no additional formatting is typically required\n\n        return df\n\n    except Exception as e:\n        print(\"Failed to fetch or parse data.\")\n        traceback.print_exc()\n        return None\n\n# Example call to the function\ndata_frame = coins_bar(broker='hb', code='btc', ktype='D', size='2000')\nprint(data_frame)", "CodeBleu_score": 28.918572945884247}, {"code": "bash\npip install requests pandas", "CodeBleu_score": 0.0}, {"code": "import pandas as pd\nimport requests\nimport traceback\n\ndef coins_bar(broker='hb', code='btc', ktype='D', size='2000'):\n    \"\"\"This function retrieves cryptocurrency data from a specified broker.\n    It formats the URL based on the broker, cryptocurrency code, kline type, and size.\n    It fetches the data and handles any broker-specific data structures.\n    The data is then converted into a pandas DataFrame with specific columns.\n    Date formatting is adjusted based on the broker and kline type.\n    Finally, it converts the date column to datetime format and returns the DataFrame.\n    If an exception occurs, it prints the traceback.\"\"\"\n    \n    try:\n        # Construct the URL depending on the broker\n        if broker == 'hb':\n            url = f\"https://api.huobi.pro/market/history/kline?symbol={code}usdt&period={ktype}1day&size={size}\"\n        else:\n            raise ValueError(\"Broker not supported\")\n        \n        # Send the request to the desired URL and get the JSON response\n        response = requests.get(url)\n        data = response.json()\n        \n        # Check for error in response\n        if data.get('status') != 'ok':\n            raise Exception(\"Error fetching data: \" + data.get('err-msg', 'Unknown error'))\n        \n        # Convert data to pandas DataFrame\n        # Assuming the data structure from Huobi as an example\n        df = pd.DataFrame(data['data'])\n        df.columns = ['open', 'close', 'low', 'high', 'volume', 'date']\n        \n        # Convert timestamp to appropriate datetime format\n        df['date'] = pd.to_datetime(df['date'], unit='ms')\n        \n        return df\n    \n    except Exception as e:\n        traceback.print_exc()\n        return None\n\n# Example usage:\n# df = coins_bar()\n# print(df.head())", "CodeBleu_score": 31.508873277867423}]}
{"instance_id": "c6552851da8a", "generation_result": [{"code": "import pandas as pd\nimport traceback\n\ndef fetch_data(broker, code, size):\n    # Mock function to simulate fetching data.\n    # Replace with the actual data fetching function.\n    data = {\n        'timestamp': '2023-01-01 00:00:00',\n        'asks': [[1.0, 0.5], [1.1, 0.4]],\n        'bids': [[0.9, 0.6], [0.8, 0.7]]\n    }\n    return data\n\ndef coins_snapshot(broker='hb', code='btc', size='5'):\n    try:\n        # Fetch data\n        data = fetch_data(broker, code, size)\n\n        # Process the timestamp based on the broker\n        if broker.lower() == 'hb':\n            ts = pd.to_datetime(data['timestamp'], format='%Y-%m-%d %H:%M:%S')\n        else:\n            ts = pd.Timestamp(data['timestamp'])\n\n        # Convert to DataFrames\n        asks_df = pd.DataFrame(data['asks'], columns=['price', 'quantity'])\n        bids_df = pd.DataFrame(data['bids'], columns=['price', 'quantity'])\n\n        # Adding timestamp\n        asks_df['timestamp'] = ts\n        bids_df['timestamp'] = ts\n\n        # Store in dictionary instead of Panel\n        result = {\n            'asks': asks_df,\n            'bids': bids_df\n        }\n\n        return result\n\n    except Exception as e:\n        print(\"An error occurred:\")\n        traceback.print_exc()", "CodeBleu_score": 40.26610709177344}, {"code": "import pandas as pd\nimport datetime\nimport traceback\n\ndef fetch_data(broker, code, size):\n    # This function simulates fetching data from an API.\n    # Normally you'll have actual API calling code here.\n    return {\n        'timestamp': '2023-10-01T12:00:00Z',\n        'ask': [\n            {'price': 20000 + i, 'quantity': 10 * i} for i in range(1, int(size) + 1)\n        ],\n        'bid': [\n            {'price': 20000 - i, 'quantity': 10 * i} for i in range(1, int(size) + 1)\n        ]\n    }\n\ndef coins_snapshot(broker='hb', code='btc', size='5'):\n    \"\"\"The function coins_snapshot retrieves and processes snapshot data for a specified cryptocurrency from a given broker. It attempts to fetch data using a helper function and processes the timestamp differently based on the broker. The function converts the ask and bid data into dataframes, adds the timestamp to each dataframe, and stores them in a dictionary. It then creates and returns a pandas Panel containing the ask and bid data. If an exception occurs, it prints the traceback.\"\"\"\n    try:\n        data = fetch_data(broker, code, size)\n        \n        ask_df = pd.DataFrame(data['ask'])\n        bid_df = pd.DataFrame(data['bid'])\n        \n        timestamp = pd.to_datetime(data['timestamp'])\n        \n        if broker != 'hb':\n            # Custom processing for timestamp if broker is not 'hb'\n            timestamp = timestamp + datetime.timedelta(hours=1)\n        \n        ask_df['timestamp'] = timestamp\n        bid_df['timestamp'] = timestamp\n        \n        result = {\n            'ask': ask_df,\n            'bid': bid_df\n        }\n        \n        return result\n    \n    except Exception as e:\n        print('An error occurred while fetching or processing the data.')\n        traceback.print_exc()\n        return {}\n\n# Example usage:\nresult = coins_snapshot()\nprint(result['ask'])\nprint(result['bid'])", "CodeBleu_score": 34.07486906633425}, {"code": "import pandas as pd\nimport traceback\n\ndef fetch_crypto_data(broker, code, size):\n    # Placeholder function to simulate fetching data based on broker, code and size\n    # This function should return raw data in a structured format, e.g., list of dictionaries\n    import random\n    data = [{'timestamp': pd.Timestamp('now'), 'ask': random.uniform(50000, 60000), 'bid': random.uniform(48000, 59000)} for _ in range(int(size))]\n    return data\n\ndef coins_snapshot(broker='hb', code='btc', size='5'):\n    try:\n        # Fetching crypto data with a hypothetical helper function\n        raw_data = fetch_crypto_data(broker, code, size)\n        \n        # Creating DataFrames from fetched data\n        ask_data = pd.DataFrame([{'timestamp': entry['timestamp'], 'ask': entry['ask']} for entry in raw_data])\n        bid_data = pd.DataFrame([{'timestamp': entry['timestamp'], 'bid': entry['bid']} for entry in raw_data])\n        \n        # Handling different timestamps based on broker\n        if broker == 'hb':\n            ask_data['timestamp'] = ask_data['timestamp'].dt.tz_localize('UTC')\n            bid_data['timestamp'] = bid_data['timestamp'].dt.tz_localize('UTC')\n        else:\n            ask_data['timestamp'] = ask_data['timestamp'].dt.tz_localize(None)\n            bid_data['timestamp'] = bid_data['timestamp'].dt.tz_localize(None)\n\n        # Using MultiIndex DataFrame instead of Panel\n        combined_data = pd.concat([ask_data.set_index('timestamp'), bid_data.set_index('timestamp')], axis=1)\n\n        return combined_data\n    except Exception as e:\n        print(\"An error occurred:\")\n        traceback.print_exc()\n\n# Example usage\nif __name__ == \"__main__\":\n    panel_data = coins_snapshot()\n    print(panel_data)", "CodeBleu_score": 35.488803887343245}]}
{"instance_id": "8b5fee723398", "generation_result": [{"code": "def coins_trade(broker='hb', code='btc')\nimport pandas as pd\nimport requests\nimport time\n\ndef coins_trade(broker='hb', code='btc'):\n    \"\"\"\n    This function retrieves and processes trade data for a specified cryptocurrency from a given broker.\n    It fetches data from a URL based on the broker and cryptocurrency code.\n    If the data is not None, it processes the data differently depending on the broker ('hb', 'ok', 'chbtc').\n    For 'hb', it selects and renames specific columns and converts timestamps.\n    For 'ok' and 'chbtc', it similarly selects, renames columns, and converts timestamps.\n    Finally, it standardizes the column names to ['tid', 'datetime', 'price', 'amount', 'type'] and returns the processed DataFrame.\n    \n    Parameters:\n        broker (str): The broker to fetch data from. Default is 'hb'.\n        code (str): The cryptocurrency code. Default is 'btc'.\n    \n    Returns:\n        pd.DataFrame: The processed trade data.\n    \"\"\"\n\n    # Define URLs for different brokers\n    urls = {\n        'hb': f'https://api.huobi.pro/market/trade?symbol={code}usdt',\n        'ok': f'https://www.okex.com/api/spot/v3/instruments/{code}-usdt/trades',\n        'chbtc': f'http://api.chbtc.com/data/v1/trades?currency={code}_usdt'\n    }\n\n    # Fetch the data\n    response = requests.get(urls[broker])\n    if response.status_code != 200:\n        return pd.DataFrame()  # Return empty DataFrame if the request failed\n\n    trade_data = response.json()\n\n    # Process the data based on the broker\n    if broker == 'hb':\n        # Assuming Huobi returns a dictionary with 'data' containing trades as a list of dictionaries\n        trades = pd.DataFrame(trade_data['data']['data'])\n        trades['datetime'] = pd.to_datetime(trades['ts'], unit='ms')\n        trades.rename(columns={'id': 'tid', 'price': 'price', 'amount': 'amount', 'direction': 'type'}, inplace=True)\n        trades = trades[['tid', 'datetime', 'price', 'amount', 'type']]\n    elif broker in ['ok', 'chbtc']:\n        # Assuming OKEX and CHBTC return list of dictionaries\n        trades = pd.DataFrame(trade_data)\n        trades['datetime'] = pd.to_datetime(trades['timestamp'])\n        trades.rename(columns={'trade_id': 'tid', 'price': 'price', 'size': 'amount', 'side': 'type'}, inplace=True)\n        if broker == 'ok':\n            trades['type'] = trades['type'].replace({\"buy\": \"bid\", \"sell\": \"ask\"})\n        trades = trades[['tid', 'datetime', 'price', 'amount', 'type']]\n\n    return trades\n\n# You would typically call the function like this:\n# df_trades = coins_trade('hb', 'btc')\n# print(df_trades.head())\n\nThis function implements the described functionalities and is customizable and extendable to accommodate additional requirements or changes in API data structures.", "CodeBleu_score": 36.413847985308756}, {"code": "import pandas as pd\nimport requests\n\ndef coins_trade(broker='hb', code='btc'):\n    \"\"\"\n    This function retrieves and processes trade data for a specified cryptocurrency from a given broker.\n    It fetches data from a URL based on the broker and cryptocurrency code.\n    If the data is not None, it processes the data differently depending on the broker ('hb', 'ok', 'chbtc').\n    For 'hb', it selects and renames specific columns and converts timestamps.\n    For 'ok' and 'chbtc', it similarly selects, renames columns, and converts timestamps.\n    Finally, it standardizes the column names to ['tid', 'datetime', 'price', 'amount', 'type'] and returns the processed DataFrame.\n    \"\"\"\n    \n    # Mock URLs for demonstration; In practice, these should be actual API endpoints:\n    url_dict = {\n        'hb': f\"https://api.hb.com/trades?currency={code}\",\n        'ok': f\"https://api.ok.com/trades?currency={code}\",\n        'chbtc': f\"https://api.chbtc.com/trades?currency={code}\"\n    }\n    \n    # Fetch data from API\n    response = requests.get(url_dict[broker])\n    if response.status_code != 200:\n        print(\"Failed to retrieve data\")\n        return None\n    \n    # Convert to dataframe\n    try:\n        data = pd.DataFrame(response.json())\n    except ValueError:\n        print(\"Failed to decode JSON\")\n        return None\n\n    # Process data by broker type:\n    if broker == 'hb':\n        # Example processing method\n        df = data[['trade_id', 'timestamp', 'price', 'amount', 'trade_type']]\n        df.rename(columns={'trade_id': 'tid', 'timestamp': 'datetime', 'trade_type': 'type'}, inplace=True)\n        df['datetime'] = pd.to_datetime(df['datetime'], unit='s')\n    elif broker in ['ok', 'chbtc']:\n        df = data[['id', 'date', 'price', 'quantity', 'direction']]\n        df.rename(columns={'id': 'tid', 'date': 'datetime', 'quantity': 'amount', 'direction': 'type'}, inplace=True)\n        df['datetime'] = pd.to_datetime(df['datetime'], unit='s')\n    else:\n        print(\"Broker not supported\")\n        return None\n\n    # Standardize and return data\n    df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n    return df\n\n# Example usage\nprint(coins_trade('hb', 'btc'))", "CodeBleu_score": 36.76867839828834}, {"code": "import requests\nimport pandas as pd\nfrom datetime import datetime\n\ndef coins_trade(broker='hb', code='btc'):\n    # Example URLs (these are placeholders and would need to correspond to real APIs or endpoints)\n    url_mapping = {\n        'hb': f'https://api.hbsample.com/data/trades?code={code}',\n        'ok': f'https://api.oksample.com/data/trades?code={code}',\n        'chbtc': f'https://api.chbtcsample.com/data/trades?code={code}'\n    }\n\n    # Fetch data\n    response = requests.get(url_mapping[broker])\n    if response.status_code == 200:\n        data = response.json()\n        df = pd.DataFrame(data)\n        \n        # Process data based on broker\n        if broker == 'hb':\n            df = df.rename(columns={\"time\": \"datetime\", \"vol\": \"amount\"})\n            df['datetime'] = pd.to_datetime(df['datetime'], unit='s')\n        \n        elif broker in ['ok', 'chbtc']:\n            df = df.rename(columns={\"timestamp\": \"datetime\", \"volume\": \"amount\"})\n            df['datetime'] = pd.to_datetime(df['datetime'], unit='s')\n\n        # Standardize column names if needed\n        df = df[['tid', 'datetime', 'price', 'amount', 'type']]\n        \n        return df\n    else:\n        return None\n\n# Usage example:\nresult_df = coins_trade('hb', 'btc')\nprint(result_df)", "CodeBleu_score": 39.30031263617308}]}
{"instance_id": "25e0a134c45b", "generation_result": [{"code": "import requests\nimport pandas as pd\nimport json\n\ndef _parse_fund_data(url, fund_type='open'):\n    \"\"\"\n    Given a URL and a fund type ('open' for open-end funds or another type), this function retrieves fund data.\n    The data is expected to be in JSON format. It processes the fund data, converts it into a pandas DataFrame with\n    certain columns based on the 'fund_type', treats missing values, and returns the DataFrame.\n    \n    Args:\n    - url (str): URL from which to fetch the fund data.\n    - fund_type (str): Type of fund data to parse; defaults to 'open'.\n    \n    Returns:\n    - pd.DataFrame: A DataFrame containing the fund data, formatted according to the fund type, with missing values handled.\n    \n    Raises:\n    - Prints any connection or data errors encountered.\n    \"\"\"\n    try:\n        # Sending an HTTP GET request to the URL\n        response = requests.get(url)\n        # Ensure the request was successful\n        response.raise_for_status()\n        \n        # Parse the JSON data from the response\n        data = json.loads(response.text)\n        \n        # Assuming the data structure involves a list of dicts\n        if fund_type == 'open':\n            # Select the relevant columns for open-end funds (example columns)\n            columns = ['fundName', 'nav', 'change', 'percentChange', 'date']\n        else:\n            # Different fund_type might use different columns\n            columns = ['fundName', 'totalAssets', 'riskLevel', 'returnYTD', 'date']\n        \n        # Create DataFrame using selected columns\n        df = pd.DataFrame(data, columns=columns)\n        \n        # Replace NaN values with zeros\n        df.fillna(0, inplace=True)\n        \n        return df\n    \n    except requests.RequestException as e:\n        print(f\"Request error: {e}\")\n    except json.JSONDecodeError:\n        print(\"Error decoding JSON.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")", "CodeBleu_score": 29.197508875171767}, {"code": "import pandas as pd\nimport requests\nimport json\n\ndef _parse_fund_data(url, fund_type='open'):\n    \"\"\"\n    The function _parse_fund_data retrieves and processes fund data from a given URL. It writes to the console and attempts to request the URL.\n    The function reads the response, handles text encoding, and extracts relevant data.\n    It formats the text into valid JSON and loads it into a pandas DataFrame with specific columns based on the fund type.\n    The DataFrame is then filled with zeros for any missing values and returned. If an error occurs, the function prints the error message.\n    \"\"\"\n    try:\n        # Attempt to request the URL and fetch the data\n        response = requests.get(url)\n        response.raise_for_status()  # Raise HTTPError for bad requests (4XX or 5XX)\n        \n        # Constructing DataFrame from the JSON response content\n        data = json.loads(response.text)\n        \n        # Structure the data into a DataFrame based on fund type\n        if fund_type == 'open':\n            # We are assuming a structure for `open` type funds; in a real-world case, this may differ and require customization\n            columns = ['Fund Name', 'NAV', 'Assets', 'Expense Ratio', 'Category']\n        else:\n            # Some other fund type, assuming a different set of relevant columns\n            columns = ['Fund Name', 'Performance', 'Assets', 'Manager', 'Inception Date']\n\n        # Create DataFrame ensuring only specific columns are considered; other columns are ignored if present\n        df = pd.DataFrame(data, columns=columns)\n        \n        # Fill missing values in DataFrame\n        df.fillna(0, inplace=True)\n        \n        return df\n\n    except requests.exceptions.RequestException as e:\n        # Print the error message if the URL request fails\n        print(f\"Error fetching data from URL: {e}\")\n    except json.JSONDecodeError:\n        # Error in decoding JSON\n        print(\"Error decoding JSON from the response\")\n    except Exception as e:\n        # Any other exception\n        print(f\"An unexpected error occurred: {e}\")\n\n# Example to use the function (not part of the function definition, provided for testing context)\nif __name__ == \"__main__\":\n    url = \"https://api.example.com/funds\"\n    try:\n        df = _parse_fund_data(url, 'open')\n        print(df)\n    except Exception as e:\n        print(f\"Failed to parse fund data: {e}\")", "CodeBleu_score": 31.494072571906194}, {"code": "def _parse_fund_data(url, fund_type='open')\nimport requests\nimport pandas as pd\nimport json\n\ndef _parse_fund_data(url, fund_type='open'):\n    \"\"\"\n    The function _parse_fund_data retrieves and processes fund data from a given URL.\n    It writes to the console and attempts to request the URL. The function reads the response, \n    handles text encoding, and extracts relevant data. \n    It formats the text into valid JSON and loads it into a pandas DataFrame with specific columns \n    based on the fund type. The DataFrame is then filled with zeros for any missing values and returned.\n    If an error occurs, the function prints the error message.\n    \n    Args:\n    url (str): URL from which to retrieve the fund data.\n    fund_type (str): Type of fund data needed, defaults to 'open'.\n    \n    Returns:\n    pandas.DataFrame: DataFrame containing the fund data.\n    \"\"\"\n    \n    try:\n        # Make HTTP request to the URL\n        print(\"Requesting data from URL:\", url)\n        response = requests.get(url)\n        response.raise_for_status()  # will throw an exception if the request returned an unsuccessful status code\n  \n        # Handle encoding\n        encoding = response.encoding if response.encoding is not None else 'utf-8'\n        response_text = response.text.encode(encoding).decode('utf-8')\n        \n        # Convert text to JSON\n        json_data = json.loads(response_text)\n        \n        # Load JSON into a DataFrame, with specific columns depending on fund type\n        if fund_type == 'open':\n            columns = ['fund_name', 'nav', 'aum', 'total_return']\n        elif fund_type == 'closed':\n            columns = ['fund_name', 'market_cap', 'discount_premium', 'leverage']\n        else:\n            raise ValueError(f\"Unsupported fund type '{fund_type}'\")\n        \n        # Creating DataFrame with selected columns and handling potential missing keys in json_data\n        df_data = {col: [item.get(col, 0) for item in json_data] for col in columns}\n        df = pd.DataFrame(df_data, columns=columns)\n        \n        # Fill missing values with zeros\n        df.fillna(0, inplace=True)\n        \n        return df\n    \n    except requests.RequestException as e:\n        print(f\"An error occurred while requesting the URL: {e}\")\n    except json.JSONDecodeError:\n        print(\"Failed to parse JSON from response\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")", "CodeBleu_score": 33.78010183690669}]}
{"instance_id": "401d18e122d8", "generation_result": [{"code": "def get_future_daily(start = None, end = None, market = 'CFFEX')\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef get_future_daily(start=None, end=None, market='CFFEX'):\n    \"\"\"\n    This function retrieves daily future market data for a specified market and date range.\n    It determines the appropriate function to call based on the market parameter, defaulting to 'CFFEX'.\n    The start and end dates are converted to the correct format, defaulting to today's date if not provided.\n    It initializes an empty list to hold data frames.\n    It iterates over each date in the range, retrieving daily data using the appropriate function and appending it to the list if data is found.\n    Finally, it concatenates and returns the list of data frames if any data was collected.\n    \"\"\"\n\n    # Helper functions for fetching data from different markets might look like:\n    def get_cffex_data(date):\n        # Implement fetching data for CFFEX for a given date\n        pass\n\n    def get_other_market_data(date, market):\n        # Implement fetching data for other markets for a given date\n        pass\n\n    # Default start and end dates today if not specified\n    if not start:\n        start = datetime.today().strftime('%Y-%m-%d')\n    if not end:\n        end = datetime.today().strftime('%Y-%m-%d')\n        \n    # Convert string dates to datetime\n    start_date = datetime.strptime(start, '%Y-%m-%d')\n    end_date = datetime.strptime(end, '%Y-%m-%d')\n\n    # Initialize an empty list to hold data frames\n    data_frames = []\n    \n    # Darwin over each day in the range\n    current_date = start_date\n    while current_date <= end_date:\n        # Fetch data based on market\n        if market.upper() == 'CFFEX':\n            daily_data = get_cffex_data(current_date.strftime('%Y-%m-%d'))\n        else:\n            daily_data = get_other_market_data(current_date.strftime('%Y-%m-%d'), market)\n\n        # Append to list if data found\n        if daily_data:\n            data_frames.append(daily_data)\n\n        # Increment day\n        current_date += timedelta(days=1)\n    \n    # Concatenate and return the list of data frames if any data was collected\n    if data_frames:\n        result = pd.concat(data_frames, ignore_index=True)\n        return result\n    else:\n        return pd.DataFrame()  # Return an empty DataFrame if no data was collected\n\n# Note: The actual implementations of `get_cffex_data` and `get_other_market_data` are placeholder \n# functions and need to be completed to actually fetch market data as needed.", "CodeBleu_score": 38.22178553287769}, {"code": "import pandas as pd\nfrom datetime import datetime\n\ndef fetch_cffex_daily(date):\n    \"\"\"Simulated function to fetch daily data from CFFEX for a specific date.\"\"\"\n    # This function would make a request to an API or database to retrieve the data.\n    # Here it simply returns a dummy DataFrame.\n    return pd.DataFrame({\n        'Date': [date],\n        'Open': [1000 + hash(date) % 50], \n        'High': [1010 + hash(date) % 50],\n        'Low': [995 + hash(date) % 50],\n        'Close': [1005 + hash(date) % 50],\n        'Volume': [1000 + hash(date) % 300]\n    })\n\ndef fetch_other_market_daily(date, market):\n    \"\"\"Simulated function to fetch daily data from specified market other than CFFEX.\"\"\"\n    # Similarly, this function would depend on external data sources.\n    return pd.DataFrame({\n        'Date': [date],\n        'Open': [2000 + hash(date+market) % 100], \n        'High': [2020 + hash(date+market) % 100],\n        'Low': [1980 + hash(date+market) % 100],\n        'Close': [2005 + hash(date+market) % 100],\n        'Volume': [2000 + hash(date+market) % 500]\n    })\n\ndef get_future_daily(start=None, end=None, market='CFFEX'):\n    \"\"\"\n    This function retrieves daily future market data for a specified market and date range.\n    It determines the appropriate function to call based on the market parameter, defaulting to 'CFFEX'.\n    The start and end dates are converted to the correct format, defaulting to today's date if not provided.\n    It initializes an empty list to hold data frames.\n    It iterates over each date in the range, retrieving daily data using the appropriate function and appending it to the list if data is found.\n    Finally, it concatenates and returns the list of data frames if any data was collected.\n    \"\"\"\n    if start is None:\n        start = datetime.today().strftime('%Y-%m-%d')\n    if end is None:\n        end = start\n\n    start_date = pd.to_datetime(start)\n    end_date = pd.to_datetime(end)\n\n    current_date = start_date\n    data_frames = []\n\n    while current_date <= end_date:\n        date_str = current_date.strftime('%Y-%m-%d')\n        if market == 'CFFEX':\n            daily_data = fetch_cffex_daily(date_str)\n        else:\n            daily_data = fetch_other_market_daily(date_str, market)\n        \n        if not daily_data.empty:\n            data_frames.append(daily_data)\n        \n        current_date += pd.Timedelta(days=1)\n\n    if data_frames:\n        return pd.concat(data_frames)\n    else:\n        return pd.DataFrame()  # Return an empty DataFrame if no data was collected", "CodeBleu_score": 37.491741142863575}, {"code": "import pandas as pd\nfrom datetime import datetime, timedelta\n\ndef get_future_daily(start=None, end=None, market='CFFEX'):\n    \"\"\"\n    This function retrieves daily future market data for a specified market and date range.\n    It determines the appropriate function to call based on the market parameter, defaulting to 'CFFEX'.\n    The start and end dates are converted to the correct format, defaulting to today's date if not provided.\n    It initializes an empty list to hold data frames.\n    It iterates over each date in the range, retrieving daily data using the appropriate function and appending it to the list if data is found.\n    Finally, it concatenates and returns the list of data frames if any data was collected.\n    \"\"\"\n\n    # Defaulting start and end dates to today if none provided\n    if start is None:\n        start = datetime.now().date()\n    else:\n        start = pd.to_datetime(start).date()\n\n    if end is None:\n        end = datetime.now().date()\n    else:\n        end = pd.to_datetime(end).date()\n\n    # Ensure start date is not after end date\n    if start > end:\n        raise ValueError(\"Start date must not be after the end date.\")\n    \n    # Helper function to fetch data based on market\n    def fetch_data(date, market):\n        # Mockup function to simulate different market data retrieval\n        # This would normally involve API calls or reading from a database\n        if market == \"CFFEX\":\n            return pd.DataFrame({\"Date\": [date], \"Market\": [market], \"Data\": [100]})\n        elif market == \"DCE\":\n            return pd.DataFrame({\"Date\": [date], \"Market\": [market], \"Data\": [200]})\n        else:\n            return pd.DataFrame()\n    \n    results = []\n    current_date = start\n    \n    while current_date <= end:\n        data = fetch_data(current_date, market)\n        if not data.empty:\n            results.append(data)\n        current_date += timedelta(days=1)\n    \n    if results:\n        return pd.concat(results)\n    else:\n        return pd.DataFrame()  # Return empty DataFrame if no results found", "CodeBleu_score": 38.46364568213902}]}
{"instance_id": "697779825988", "generation_result": [{"code": "import time\nimport requests\nimport pandas as pd\nfrom datetime import datetime\nimport logging\n\ndef realtime_boxoffice(retry_count=3, pause=0.001):\n    \"\"\"\n    The function realtime_boxoffice fetches real-time box office data with retries and pauses between attempts.\n    It tries up to retry_count times, pausing for a specified duration each time. During each attempt, it sends a \n    request to a specified URL and reads the response. If the response contains data, it parses the JSON, creates\n    a DataFrame from the 'data2' field, drops the 'MovieImg' and 'mId' columns, adds a timestamp column, and returns\n    the DataFrame. If an error occurs or no data is returned, it retries until the retry limit is reached.\n    \"\"\"\n    \n    url = \"https://someapi.com/boxoffice\"  # Hypothetical URL for box office API\n    \n    for attempt in range(retry_count):\n        try:\n            response = requests.get(url)\n            response.raise_for_status()  # Raise an HTTPError for bad responses\n            \n            # Parse the data into JSON\n            data = response.json()\n            \n            # Checking if 'data2' field exists in the JSON\n            if 'data2' in data:\n                # Create DataFrame from 'data2'\n                df = pd.DataFrame(data['data2'])\n                \n                # Drop 'MovieImg' and 'mId' columns if they exist\n                df.drop(columns=['MovieImg', 'mId'], errors='ignore', inplace=True)\n                \n                # Add a timestamp column\n                df['Timestamp'] = datetime.now()\n                \n                return df\n            \n        except (requests.RequestException, ValueError, KeyError) as e:\n            logging.error(f'An error occurred: {e}')\n            time.sleep(pause)\n    \n    # If all retries fail, return None\n    return None\n\n# Example on how you might utilize this method\n# Assuming you would replace 'someapi.com/boxoffice' with an actual API URL\ndf = realtime_boxoffice()\nif df is not None:\n    print(df)\nelse:\n    print(\"Failed to fetch the data after several retries.\")", "CodeBleu_score": 33.28512535844391}, {"code": "import time\nimport requests\nimport pandas as pd\n\ndef realtime_boxoffice(retry_count=3, pause=0.001):\n    \"\"\"\n    The function realtime_boxoffice fetches real-time box office data with retries and pauses between attempts. \n    It tries up to retry_count times, pausing for a specified duration each time. During each attempt, \n    it sends a request to a specified URL and reads the response. If the response contains data, \n    it parses the JSON, creates a DataFrame from the 'data2' field, drops the 'MovieImg' and 'mId' columns, \n    adds a timestamp column, and returns the DataFrame. If an error occurs or no data is returned, \n    it retries until the retry limit is reached.\n    \"\"\"\n\n    url = \"https://example.com/api/boxoffice\"  # assumed URL for box office data\n    attempts = 0\n    \n    while attempts < retry_count:\n        try:\n            # Fetching the data from the URL\n            response = requests.get(url)\n            # Check if the response status is successful\n            if response.status_code == 200:\n                # Parse the JSON data\n                data = response.json()\n                \n                # Create DataFrame from 'data2' field\n                df = pd.DataFrame(data['data2'])\n                \n                # Remove 'MovieImg' and 'mId' columns\n                df.drop(columns=['MovieImg', 'mId'], inplace=True)\n                \n                # Add a timestamp column\n                df['timestamp'] = pd.Timestamp.now()\n                \n                return df\n            else:\n                # Log some error or pass\n                print(\"Failed to fetch data, attempt\", attempts + 1)\n        \n        except Exception as e:\n            # Log the exception\n            print(\"Error during request:\", e)\n        \n        # Increment the attempt counter\n        attempts += 1\n        # Pause before the next attempt\n        time.sleep(pause)\n    \n    # Optional: return an empty DataFrame or raise an exception if no data could be fetched\n    print(\"Failed to retrieve data after\", retry_count, \"attempts.\")\n    return pd.DataFrame()", "CodeBleu_score": 31.867533695317807}, {"code": "import pandas as pd\nimport requests\nimport time\n\ndef realtime_boxoffice(retry_count=3, pause=0.001, url=\"http://example.com/api/boxoffice\"):\n    \"\"\"The function realtime_boxoffice fetches real-time box office data with retries and pauses between \n    attempts. It tries up to retry_count times, pausing for a specified duration each time. \n    During each attempt, it sends a request to a specified URL and reads the response. If the response contains data, \n    it parses the JSON, creates a DataFrame from the 'data2' field, drops the 'MovieImg' and 'mId' columns, \n    adds a timestamp column, and returns the DataFrame. If an error occurs or no data is returned, \n    it retries until the retry limit is reached.\n\n    Args:\n    retry_count (int): The number of times to retry the request.\n    pause (float): The duration to pause between retries in seconds.\n    url (str): URL to fetch the data from.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the parsed and manipulated box office data.\n    \"\"\"\n\n    # Initial attempt count\n    attempts = 0\n\n    while attempts < retry_count:\n        try:\n            # Make the GET request\n            response = requests.get(url)\n            # Check if the response was successful\n            if response.status_code == 200:\n                data = response.json()\n                \n                # Check if 'data2' is in the response and it is not empty\n                if 'data2' in data and data['data2']:\n                    # Create DataFrame from 'data2'\n                    df = pd.DataFrame(data['data2'])\n                    \n                    # Drop unwanted columns if they exist in DataFrame\n                    for column in ['MovieImg', 'mId']:\n                        if column in df.columns:\n                            df.drop(column, axis=1, inplace=True)\n\n                    # Add a timestamp column\n                    df['timestamp'] = pd.Timestamp.now()\n                    \n                    return df\n                else:\n                    raise ValueError(\"No data found in 'data2'\")\n            else:\n                raise Exception(\"Failed to receive good response, status code: {}\".format(response.status_code))\n        \n        except Exception as e:\n            # Print out the error\n            print(f\"Attempt {attempts + 1} of {retry_count} failed: {str(e)}\")\n            # Pause before retrying\n            time.sleep(pause)\n            # Increment the attempt counter\n            attempts += 1\n\n    # If no successful attempt, raise an exception as final notification\n    raise Exception(\"All attempts to fetch data have failed after {} retries.\".format(retry_count))", "CodeBleu_score": 32.34577570114064}]}
