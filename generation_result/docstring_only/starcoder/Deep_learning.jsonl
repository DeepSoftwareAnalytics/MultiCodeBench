{"instance_id": "1f9f4d85e825", "generation_result": [{"code": "def load_model_from_config(self, half_attention)\n    if self.config.get(\"cache_path\", None):\n      cache_path = self.config[\"cache_path\"]\n      if os.path.exists(cache_path):\n        with open(cache_path, \"rb\") as f:\n          model = torch.load(f)\n        if model[\"config\"] == self.config:\n          return model\n    model = load_model_from_path(self.config[\"model_path\"], self.config, half_attention)\n    if self.config.get(\"cache_path\", None):\n      with open(cache_path, \"wb\") as f:\n        torch.save(model, f)\n    return model\n\n  def load_model_from_path(self, model_path, config, half_attention):\n    \"\"\"This function loads a model from a specified path, handling different file formats (.safetensors or other PyTorch-compatible formats). The model is configured using settings from a YAML file and moved to the appropriate device. Optional settings include half-precision attention and memory format optimization. The function ensures the model is in evaluation mode and optionally caches the loaded model for future use. The function returns a dictionary containing the loaded model.\"\"\"\n    model_path = os.path.expanduser(model_path)\n    if model_path.endswith(\".safetensors\"):\n      model = torch.load(model_path)\n    else:\n      model = torch.load(model_path, map_location=\"cpu\")\n    model = model.eval()\n    if half_attention:\n      model.half()\n    if config.get(\"memory_format\", None) == \"channels_last\":\n      model = model.to(memory_format=torch.channels_last)\n    return model\n\n  def get_model_input_size(self):\n    \"\"\"This function returns the size of the model input.\"\"\"\n    return self.model_input_size\n\n  def get_model_output_size(self):\n    \"\"\"This function returns the size of the model output.\"\"\"\n    return self.model_output_size\n\n  def get_model_output_names(self):\n    \"\"\"This function returns the names of the model outputs.\"\"\"\n    return self.model_output_names\n\n  def get_model_input_names(self):\n    \"\"\"This function returns the names of the model inputs.\"\"\"\n    return self.model_input_names\n\n  def get_model_input_types(self):\n    \"\"\"This", "CodeBleu_score": 36.93019971682776}, {"code": "def load_model_from_config(self, half_attention)\n    model_path = self.config[\"model_path\"]\n    model_name = self.config[\"model_name\"]\n    cache_dir = self.config[\"cache_dir\"]\n    device = self.config[\"device\"]\n    half_precision_attention = self.config[\"half_precision_attention\"]\n    memory_format = self.config[\"memory_format\"]\n    cache_path = os.path.join(cache_dir, model_name + \".pt\")\n    if os.path.exists(cache_path):\n      try:\n        model = torch.load(cache_path, map_location=device)\n        model.eval()\n        if half_precision_attention:\n          model.half()\n        if memory_format == \"channels_last\":\n          model.to(memory_format=torch.channels_last)\n        self.logger.info(\"Loaded model from cache at %s\" % cache_path)\n        return model\n      except:\n        self.logger.info(\"Failed to load model from cache at %s\" % cache_path)\n    if model_path.endswith(\".pt\"):\n      model = torch.load(model_path, map_location=device)\n    else:\n      model = torch.load(os.path.join(model_path, \"model.pt\"), map_location=device)\n    model.eval()\n    if half_precision_attention:\n      model.half()\n    if memory_format == \"channels_last\":\n      model.to(memory_format=torch.channels_last)\n    if not os.path.exists(cache_dir):\n      os.makedirs(cache_dir)\n    torch.save(model, cache_path)\n    self.logger.info(\"Loaded model from %s\" % model_path)\n    return model\n\n  def load_dataset_from_config(self):\n    \"\"\"This function loads a dataset from a configuration file. It first checks if a cached dataset is available and valid, loading it if so. Otherwise, it loads the dataset from the specified path, handling different file formats (.safetensors or other PyTorch-compatible formats). The dataset is configured using settings from a YAML file and moved to the appropriate device. The function returns a dictionary containing the loaded dataset.\"\"\"\n    dataset_path = self.config[\"dataset_path\"]\n    dataset_name = self.config[\"dataset_name\"]\n    cache_dir = self.config[\"cache_dir", "CodeBleu_score": 32.85118528281143}, {"code": "def load_model_from_config(self, half_attention)\n    # Check if a cached model is available and valid\n    if self.cache_dir is not None:\n      cached_model_path = os.path.join(self.cache_dir, self.model_name)\n      if os.path.exists(cached_model_path):\n        # Load cached model\n        self.logger.info(\"Loading model from cache...\")\n        model = torch.load(cached_model_path, map_location=self.device)\n        # Check if cached model is valid\n        if self.check_model_valid(model):\n          # Move model to device\n          model = model.to(self.device)\n          # Set model to evaluation mode\n          model.eval()\n          # Return model\n          return model\n        else:\n          # Cached model is invalid\n          self.logger.info(\"Cached model is invalid. Reloading model from disk...\")\n    # Load model from disk\n    self.logger.info(\"Loading model from disk...\")\n    model = self.load_model_from_disk(half_attention)\n    # Cache model\n    if self.cache_dir is not None:\n      self.logger.info(\"Caching model...\")\n      torch.save(model, cached_model_path)\n    # Move model to device\n    model = model.to(self.device)\n    # Set model to evaluation mode\n    model.eval()\n    # Return model\n    return model\n\n  def load_model_from_disk(self, half_attention):\n    \"\"\"This function loads a model from a configuration file, with an option to load from a cache if available. It first checks if a cached model is available and valid, loading it if so. Otherwise, it loads the model from the specified path, handling different file formats (.safetensors or other PyTorch-compatible formats). The model is configured using settings from a YAML file and moved to the appropriate device. Optional settings include half-precision attention and memory format optimization. The function ensures the model is in evaluation mode and optionally caches the loaded model for future use. The function returns a dictionary containing the loaded model.\"\"\"\n    # Load model from disk\n    model = torch.load(self.model_path, map_location=self.device)\n    # Configure model\n    model = self.configure_model(model, half_attention)\n    # Return model\n    return model\n\n  def configure_model(self, model, half_attention):\n    \"\"\"This function configures a model to use the specified settings.", "CodeBleu_score": 35.61133054811481}]}
{"instance_id": "8dc955007aa2", "generation_result": [{"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    # Extract batch size\n    batch_size = shape[0]\n    # Print eta and steps values\n    print(f\"eta: {eta}, steps: {steps}\")\n    # Create DDIM sampler\n    sampler = DDIMSampler(model, batch_size=batch_size, eta=eta)\n    # Sample\n    samples, intermediates = sampler.sample(steps, shape, cond, callback=callback, normals_sequence=normals_sequence,\n                                            mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature,\n                                            score_corrector=score_corrector, corrector_kwargs=corrector_kwargs,\n                                            x_t=x_t)\n    # Return samples and intermediates\n    return samples, intermediates\n", "CodeBleu_score": 50.494854393604186}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n\n    # Initialize the DDIM sampler instance.\n    ddim_sampler = DDIMSampler(model)\n\n    # Extract the batch size from the shape parameter.\n    batch_size = shape[0]\n\n    # Print the eta and steps values.\n    print(f'eta: {eta}')\n    print(f'steps: {steps}')\n\n    # Call the sample method of the DDIMSampler instance.\n    samples, intermediates = ddim_sampler.sample(steps=steps, batch_size=batch_size, shape=shape, cond=cond, eta=eta, callback=callback, normals_sequence=normals_sequence, mask=mask, x0=x0, quantize_x0=quantize_x0, temperature=temperature, score_corrector=score_corrector, corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n    # Return the generated samples and intermediate values.\n    return samples, intermediates\n", "CodeBleu_score": 53.05215683688409}, {"code": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )\n    if x0 is None:\n        x0 = torch.randn(shape, device=cond.device)\n    if quantize_x0:\n        x0 = torch.quantize_per_tensor(x0, 1.0, 0, torch.quint8)\n    batch_size = shape[0]\n    ddim_sampler = DDIMSampler(model, batch_size, eta=eta, score_corrector=score_corrector,\n                               corrector_kwargs=corrector_kwargs)\n    return ddim_sampler.sample(steps, cond, callback=callback, normals_sequence=normals_sequence, mask=mask,\n                               x0=x0, temperature=temperature, x_t=x_t)\n\n", "CodeBleu_score": 40.97321295585207}]}
{"instance_id": "9bdcd9081480", "generation_result": [{"code": "def ema_scope(self, context=None)\n    return _ida_kernwin.action_handler_t_ema_scope(self, *args)\n", "CodeBleu_score": 2.2042136448360816}, {"code": "def ema_scope(self, context=None)\n    return _emat.ema_scope(self, context)\n", "CodeBleu_score": 4.677722078922578}, {"code": "def ema_scope(self, context=None)\n    return _pywrap_tensorflow_internal.MetaGraphDef_ema_scope(self, context)\n", "CodeBleu_score": 4.7895960512840405}]}
{"instance_id": "14ab009e1c5c", "generation_result": [{"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    if not os.path.exists(path):\n        print(\"Checkpoint '{}' not found\".format(path))\n        return\n    state_dict = torch.load(path, map_location=\"cpu\")\n    if \"model\" in state_dict:\n        state_dict = state_dict[\"model\"]\n    # remove ignored keys\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            state_dict.pop(k, None)\n\n    # load state_dict\n    self.load_state_dict(state_dict, strict=False)\n\n    # release memory\n    del state_dict\n\n    # print warnings\n    missing_keys, unexpected_keys = self.load_state_dict(\n        state_dict, strict=False\n    )\n    if len(missing_keys) > 0:\n        print(\"Missing Keys: {}\".format(missing_keys))\n    if len(unexpected_keys) > 0:\n        print(\"Unexpected Keys: {}\".format(unexpected_keys))\n\n", "CodeBleu_score": 37.852261236801695}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    if not os.path.exists(path):\n        raise RuntimeError(\"=> no checkpoint found at '{}'\".format(path))\n    checkpoint = torch.load(path, map_location=\"cpu\")\n    # get state_dict from checkpoint\n    if isinstance(checkpoint, dict) and \"state_dict\" in checkpoint:\n        state_dict = checkpoint[\"state_dict\"]\n    else:\n        state_dict = checkpoint\n    # strip prefix of state_dict\n    if list(state_dict.keys())[0].startswith(\"module.\"):\n        state_dict = {k[7:]: v for k, v in state_dict.items()}\n    # delete ignore_keys\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            del state_dict[k]\n    # load state_dict\n    self.load_state_dict(state_dict, strict=False)\n\n    print(\"=> loaded checkpoint '{}'\".format(path))\n    return checkpoint\n\n", "CodeBleu_score": 35.571541685204956}, {"code": "def init_from_ckpt(self, path, ignore_keys=None)\n    self.load_state_dict(torch.load(path, map_location=torch.device('cpu')), strict=False)\n    if ignore_keys is not None:\n        for k in ignore_keys:\n            del self.state_dict()[k]\n    self.eval()\n    missing_keys, unexpected_keys = self.load_state_dict(torch.load(path, map_location=torch.device('cpu')), strict=False).unexpected_keys\n    if len(missing_keys) > 0:\n        print(f\"Missing Keys: {', '.join(missing_keys)}\")\n    if len(unexpected_keys) > 0:\n        print(f\"Unexpected Keys: {', '.join(unexpected_keys)}\")\n", "CodeBleu_score": 30.290006931374364}]}
{"instance_id": "754176e27b81", "generation_result": [{"code": "def forward(self, input, return_pred_indices=False)\n    # Encode\n    encoded_input = self.encoder(input)\n    # Decode\n    decoded_output, pred_indices = self.decoder(encoded_input, return_pred_indices)\n    # Return\n    if return_pred_indices:\n      return decoded_output, pred_indices\n    else:\n      return decoded_output\n", "CodeBleu_score": 23.152599421855395}, {"code": "def forward(self, input, return_pred_indices=False)\n    if self.training:\n      return self.forward_train(input, return_pred_indices)\n    else:\n      return self.forward_test(input, return_pred_indices)\n\n  def forward_train(self, input, return_pred_indices=False):\n    \"\"\"This function processes an input through encoding and decoding steps, returning the decoded output and a difference metric.\\nOptionally, it can also return prediction indices if specified.\\n\"\"\"\n    self.reset_state()\n    self.encoder.reset_state()\n    self.decoder.reset_state()\n\n    # encode input\n    encoded_input = self.encoder(input)\n\n    # decode encoded input\n    decoded_output, pred_indices = self.decoder(encoded_input, return_pred_indices)\n\n    # compute difference metric\n    difference_metric = self.difference_metric(decoded_output, input)\n\n    if return_pred_indices:\n      return decoded_output, difference_metric, pred_indices\n    else:\n      return decoded_output, difference_metric\n\n  def forward_test(self, input, return_pred_indices=False):\n    \"\"\"This function processes an input through encoding and decoding steps, returning the decoded output and a difference metric.\\nOptionally, it can also return prediction indices if specified.\\n\"\"\"\n    self.reset_state()\n    self.encoder.reset_state()\n    self.decoder.reset_state()\n\n    # encode input\n    encoded_input = self.encoder(input)\n\n    # decode encoded input\n    decoded_output, pred_indices = self.decoder(encoded_input, return_pred_indices)\n\n    if return_pred_indices:\n      return decoded_output, pred_indices\n    else:\n      return decoded_output\n\n  def reset_state(self):\n    \"\"\"Resets the state of the model, including encoder and decoder states.\\n\"\"\"\n    self.encoder.reset_state()\n    self.decoder.reset_state()\n", "CodeBleu_score": 31.484587214928606}, {"code": "def forward(self, input, return_pred_indices=False)\n    # Encode\n    encoded = self.encoder(input)\n    # Decode\n    decoded = self.decoder(encoded)\n    # Compute difference metric\n    difference = self.difference_metric(input, decoded)\n    if return_pred_indices:\n      return decoded, difference, self.decoder.pred_indices\n    else:\n      return decoded, difference\n", "CodeBleu_score": 23.60025056282512}]}
{"instance_id": "443df6fdaf0d", "generation_result": [{"code": "def get_input(self, batch, k)\n    # Get the input data\n    data = batch[k]\n    # Make sure the input has four dimensions\n    if len(data.shape) == 3:\n        data = data.unsqueeze(0)\n    # Make sure the data is on the right device\n    data = data.to(self.device)\n    # Make sure the data is a contiguous float tensor\n    data = data.contiguous().float()\n    # If we are resizing the input, adjust the size of the input\n    if self.resize:\n        # Get the current training step\n        step = self.get_step()\n        # Interpolate between the minimum and maximum sizes\n        size = self.min_size + (self.max_size - self.min_size) * step / self.max_step\n        # Resize the input\n        data = F.interpolate(data, size=(size, size), mode='bicubic', align_corners=False)\n    # Detach the tensor from the computation graph\n    data = data.detach()\n    # Return the data\n    return data\n", "CodeBleu_score": 31.593018850602416}, {"code": "def get_input(self, batch, k)\n    # If batch resizing is enabled, adjust the size of the input.\n    if self.batch_resize:\n      # Get the current training step.\n      step = self.get_step()\n      # Interpolate the input to the new size.\n      batch = F.interpolate(batch, size=self.batch_resize_range[step], mode='bicubic')\n    # Get the input from the batch.\n    batch = batch[k]\n    # Make sure the input has four dimensions.\n    if len(batch.shape) == 2:\n      batch = batch.unsqueeze(0)\n    elif len(batch.shape) == 3:\n      batch = batch.unsqueeze(0)\n    # Permute the axes.\n    batch = batch.permute(0, 2, 3, 1)\n    # Convert the data to a contiguous float tensor.\n    batch = batch.to(dtype=torch.float32).contiguous()\n    # Detach the tensor from the computation graph.\n    batch = batch.detach()\n    # Return the input.\n    return batch\n\n  def get_step(self):\n    \"\"\"This function returns the current training step.\"\"\"\n    return self.step\n\n  def get_training_parameters(self):\n    \"\"\"This function returns the training parameters.\"\"\"\n    return self.training_parameters\n\n  def get_training_parameters_list(self):\n    \"\"\"This function returns the training parameters as a list.\"\"\"\n    return [self.training_parameters[k] for k in self.training_parameters]\n\n  def get_training_parameters_dict(self):\n    \"\"\"This function returns the training parameters as a dictionary.\"\"\"\n    return self.training_parameters\n\n  def get_training_parameters_string(self):\n    \"\"\"This function returns the training parameters as a string.\"\"\"\n    return str(self.training_parameters)\n\n  def get_training_parameters_string_list(self):\n    \"\"\"This function returns the training parameters as a list of strings.\"\"\"\n    return [str(self.training_parameters[k]) for k in self.training_parameters]\n\n  def get_training_parameters_string_dict(self):\n    \"\"\"This function returns the training parameters as a dictionary of strings.\"\"\"\n    return {k: str(self.training_parameters[k]) for k in self.training_parameters}\n\n  def get_training_parameters_string_list_dict(self):\n    \"\"\"This function returns the training parameters", "CodeBleu_score": 33.457873590457964}, {"code": "def get_input(self, batch, k)\n    input = batch[k]\n    if len(input.shape) == 3:\n        input = input.unsqueeze(0)\n    if self.resize:\n        input = F.interpolate(input, size=(self.resize_range, self.resize_range), mode='bicubic', align_corners=False)\n    input = input.permute(0, 3, 1, 2).contiguous().float()\n    return input.detach()\n\n", "CodeBleu_score": 25.898179290897573}]}
{"instance_id": "953a2632fc15", "generation_result": [{"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    # Retrieve data and predictions\n    x, y = batch\n    x_hat = self(x)\n    # Compute losses\n    if optimizer_idx == 0:\n        loss = F.mse_loss(x_hat, x)\n        self.log(\"train_loss\", loss)\n    if optimizer_idx == 1:\n        loss = self.discriminator_loss(x, x_hat)\n        self.log(\"discriminator_loss\", loss)\n    return loss\n\n", "CodeBleu_score": 10.192516259524373}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    x, y = batch\n    x = x.to(self.device)\n    y = y.to(self.device)\n\n    if optimizer_idx == 0:\n        # Autoencoder\n        y_pred = self.model(x)\n        loss = self.loss_function(y_pred, y)\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n    else:\n        # Discriminator\n        y_real = torch.ones(x.shape[0], 1).to(self.device)\n        y_fake = torch.zeros(x.shape[0], 1).to(self.device)\n        # Real loss\n        y_pred_real = self.discriminator(x)\n        loss_real = self.loss_function(y_pred_real, y_real)\n        # Fake loss\n        y_pred_fake = self.discriminator(y_pred)\n        loss_fake = self.loss_function(y_pred_fake, y_fake)\n        # Total loss\n        loss = loss_real + loss_fake\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n", "CodeBleu_score": 23.534985871353637}, {"code": "def training_step(self, batch, batch_idx, optimizer_idx)\n    # Retrieve data from batch\n    input_data, target_data = batch\n\n    # Forward pass\n    predictions = self(input_data)\n\n    # Compute loss\n    if optimizer_idx == 0:\n        loss = self.loss_function(predictions, target_data)\n    else:\n        loss = self.discriminator_loss_function(predictions, target_data)\n\n    # Log loss\n    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n\n    return loss\n", "CodeBleu_score": 17.552054983148533}]}
{"instance_id": "b449b5e8b77f", "generation_result": [{"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if not self.is_master:\n      return\n    if self.ema is not None and plot_ema:\n      weights = self.ema.get_weights()\n    else:\n      weights = None\n    if self.is_channels_first:\n      batch = batch.permute(0, 2, 3, 1)\n    if only_inputs:\n      self.log_images_to_tensorboard(batch, weights=weights, **kwargs)\n    else:\n      reconstructions = self.model(batch, weights=weights)\n      if self.is_channels_first:\n        reconstructions = reconstructions.permute(0, 2, 3, 1)\n      self.log_images_to_tensorboard(batch, reconstructions, weights=weights, **kwargs)\n\n  def log_images_to_tensorboard(self, batch, reconstructions=None, weights=None, **kwargs):\n    \"\"\"This function logs images to tensorboard during model training.\\nIt processes input data, logs either just the inputs or both inputs and reconstructions, and applies colorization for multi-channel images if needed.\\nIf EMA logging is enabled, it includes reconstructions using EMA weights in the log.\\n\"\"\"\n    if not self.is_master:\n      return\n    if self.is_channels_first:\n      batch = batch.permute(0, 2, 3, 1)\n    if reconstructions is not None:\n      if self.is_channels_first:\n        reconstructions = reconstructions.permute(0, 2, 3, 1)\n      if self.is_channels_first:\n        batch = batch.permute(0, 3, 1, 2)\n        reconstructions = reconstructions.permute(0, 3, 1, 2)\n      if self.is_channels_first:\n        batch = batch.cpu().numpy()\n        reconstructions = reconstructions.cpu().numpy()\n      else:\n        batch = batch.cpu().numpy().transpose(0, 2, 3, 1)\n        reconstructions = reconstructions.cpu().numpy().transpose(0, 2, 3, 1)\n      if batch.shape[1] == 1:\n        batch = np.concatenate([batch, batch, batch], axis=1)\n        reconstructions = np.concatenate([reconstructions, reconstructions", "CodeBleu_score": 23.78190356854595}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if not self.log_images: return\n    if not self.ema_val_enabled: plot_ema = False\n    if not plot_ema:\n        x, y = batch\n        x = x.to(self.device)\n        y = y.to(self.device)\n    else:\n        x, y = batch\n        x = x.to(self.device)\n        y = y.to(self.device)\n        self.model.eval()\n        with torch.no_grad():\n            x_ema, y_ema = self.ema_model(x, y)\n        self.model.train()\n    if not self.data_is_3d:\n        x = x.unsqueeze(1)\n        y = y.unsqueeze(1)\n    if self.num_channels > 1:\n        x = self.colorize(x)\n        y = self.colorize(y)\n        if plot_ema:\n            x_ema = self.colorize(x_ema)\n            y_ema = self.colorize(y_ema)\n    if only_inputs:\n        self.logger.experiment.add_images('images', x, self.current_epoch, dataformats='NCHW')\n    else:\n        if plot_ema:\n            self.logger.experiment.add_images('images', torch.cat([x, x_ema]), self.current_epoch, dataformats='NCHW')\n        else:\n            self.logger.experiment.add_images('images', torch.cat([x, y]), self.current_epoch, dataformats='NCHW')\n", "CodeBleu_score": 30.936722783955705}, {"code": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)\n    if self.logger is not None:\n        if self.ema_model is not None and plot_ema:\n            self.ema_model.eval()\n            ema_images = self.ema_model.module.log_images(batch, only_inputs=only_inputs, **kwargs)\n            self.ema_model.train()\n            self.logger.experiment.log({**ema_images}, global_step=self.global_step)\n        images = self.model.module.log_images(batch, only_inputs=only_inputs, **kwargs)\n        self.logger.experiment.log({**images}, global_step=self.global_step)\n", "CodeBleu_score": 21.953565705554727}]}
{"instance_id": "540d089e3481", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n      loss = torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n      loss = (pred - target)**2\n    else:\n      raise ValueError(f'Unknown loss type: {self.loss_type}')\n    if mean:\n      return torch.mean(loss)\n    else:\n      return loss\n", "CodeBleu_score": 34.44360255100334}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n      loss = torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n      if mean:\n        loss = torch.mean((pred - target)**2)\n      else:\n        loss = torch.sum((pred - target)**2)\n    else:\n      raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n    return loss\n\n  def get_grad(self, pred, target):\n    \"\"\"This function calculates the gradient of the loss with respect to the predicted value.\"\"\"\n    if self.loss_type == 'l1':\n      grad = 2 * torch.sign(pred - target)\n    elif self.loss_type == 'l2':\n      grad = 2 * (pred - target)\n    else:\n      raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n    return grad\n\n  def get_hessian(self, pred, target):\n    \"\"\"This function calculates the Hessian of the loss with respect to the predicted value.\"\"\"\n    if self.loss_type == 'l1':\n      hessian = torch.zeros(pred.shape)\n    elif self.loss_type == 'l2':\n      hessian = 2 * torch.ones(pred.shape)\n    else:\n      raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n    return hessian\n\n  def get_loss_and_grad(self, pred, target):\n    \"\"\"This function calculates the loss and gradient of the loss with respect to the predicted value.\"\"\"\n    loss = self.get_loss(pred, target)\n    grad = self.get_grad(pred, target)\n    return loss, grad\n\n  def get_loss_and_grad_and_hessian(self, pred, target):\n    \"\"\"This function calculates the loss, gradient and Hessian of the loss with respect to the predicted value.\"\"\"\n    loss = self.get_loss(pred, target)\n    grad = self.get_grad(pred, target)\n    hessian = self.get_hessian(pred, target)\n    return loss, grad, hessian\n\n  def get_loss_and_grad_and_hessian_and_pred_delta(self, pred, target):\n    \"\"\"This function calculates the loss, gradient, Hessian and delta of the loss with respect to the", "CodeBleu_score": 41.25823352062291}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n      loss = torch.mean(torch.abs(pred - target))\n    elif self.loss_type == 'l2':\n      if mean:\n        loss = torch.mean((pred - target) ** 2)\n      else:\n        loss = torch.sum((pred - target) ** 2)\n    else:\n      raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n    return loss\n", "CodeBleu_score": 43.31490034999532}]}
{"instance_id": "63a08283666f", "generation_result": [{"code": "def forward(self, x, c, *args, **kwargs)\n    # get the time steps to use\n    if self.training:\n      # during training, randomly select time steps\n      t = torch.randint(0, x.shape[1], (x.shape[0],))\n    else:\n      # during evaluation, use the last time step\n      t = torch.full((x.shape[0],), x.shape[1] - 1, dtype=torch.long)\n    # select the time steps\n    x = x[torch.arange(x.shape[0]), t]\n    # if the model has conditioning, incorporate it\n    if self.cond_channels > 0:\n      # reshape the conditioning\n      c = c.view(c.shape[0], c.shape[1], 1, 1)\n      # upsample the conditioning\n      c = c.repeat(1, 1, x.shape[2], x.shape[3])\n      # concatenate the conditioning to the inputs\n      x = torch.cat((x, c), dim=1)\n    # process the inputs according to the model's conditioning requirements\n    x = self.process_inputs(x)\n    # compute the loss\n    return self.loss(x, *args, **kwargs)\n", "CodeBleu_score": 41.424198080774495}, {"code": "def forward(self, x, c, *args, **kwargs)\n    x_shape = x.shape\n    if len(x_shape) == 2:\n      x = x.unsqueeze(1)\n    if len(x_shape) == 3:\n      x = x.unsqueeze(2)\n    x = x.permute(0, 2, 1, 3)\n    x = x.contiguous()\n    x = x.view(x_shape[0] * x_shape[2], x_shape[1], x_shape[3])\n    if self.cond_channels > 0:\n      c = c.unsqueeze(1)\n      c = c.expand(-1, x_shape[2], -1)\n      c = c.contiguous()\n      c = c.view(x_shape[0] * x_shape[2], -1)\n    else:\n      c = None\n    loss = self.model(x, c, *args, **kwargs)\n    return loss\n\n  def forward_sample(self, x, c, *args, **kwargs):\n    \"\"\"This function performs a forward pass for a model, incorporating conditioning and time step information.\\nIt randomly selects time steps, applies learned conditioning if applicable, and processes the inputs according to the model's conditioning requirements.\\nFinally, it computes and returns the loss for the given inputs and conditioning.\\n\"\"\"\n    x_shape = x.shape\n    if len(x_shape) == 2:\n      x = x.unsqueeze(1)\n    if len(x_shape) == 3:\n      x = x.unsqueeze(2)\n    x = x.permute(0, 2, 1, 3)\n    x = x.contiguous()\n    x = x.view(x_shape[0] * x_shape[2], x_shape[1], x_shape[3])\n    if self.cond_channels > 0:\n      c = c.unsqueeze(1)\n      c = c.expand(-1, x_shape[2], -1)\n      c = c.contiguous()\n      c = c.view(x_shape[0] * x_shape[2], -1)\n    else:\n      c = None\n    loss = self.model(x, c, *args, **kwargs)\n    return loss\n\n  def forward_sample_from_latents(self, latents, c, *args,", "CodeBleu_score": 37.450198355941865}, {"code": "def forward(self, x, c, *args, **kwargs)\n    if self.conditioning_method == 'conditioning_before':\n      x = self.conditioning_network(c)\n      x = self.time_step_network(x)\n      x = self.input_network(x)\n    elif self.conditioning_method == 'conditioning_after':\n      x = self.input_network(x)\n      x = self.time_step_network(x)\n      x = self.conditioning_network(x, c)\n    elif self.conditioning_method == 'conditioning_none':\n      x = self.input_network(x)\n      x = self.time_step_network(x)\n    else:\n      raise Exception('Invalid conditioning method!')\n    return self.model(x)\n\n  def loss(self, x, c, y, *args, **kwargs):\n    \"\"\"This function computes the loss for a model, incorporating conditioning and time step information.\\nIt randomly selects time steps, applies learned conditioning if applicable, and processes the inputs according to the model's conditioning requirements.\\nFinally, it computes and returns the loss for the given inputs and conditioning.\\n\"\"\"\n    if self.conditioning_method == 'conditioning_before':\n      x = self.conditioning_network(c)\n      x = self.time_step_network(x)\n      x = self.input_network(x)\n    elif self.conditioning_method == 'conditioning_after':\n      x = self.input_network(x)\n      x = self.time_step_network(x)\n      x = self.conditioning_network(x, c)\n    elif self.conditioning_method == 'conditioning_none':\n      x = self.input_network(x)\n      x = self.time_step_network(x)\n    else:\n      raise Exception('Invalid conditioning method!')\n    return self.model.loss(x, y)\n\n  def sample(self, c, t, *args, **kwargs):\n    \"\"\"This function samples from a model, incorporating conditioning and time step information.\\nIt randomly selects time steps, applies learned conditioning if applicable, and processes the inputs according to the model's conditioning requirements.\\nFinally, it samples from the model given the inputs and conditioning.\\n\"\"\"\n    if self.conditioning_method == 'conditioning_before':\n      x = self.", "CodeBleu_score": 25.55397927314771}]}
{"instance_id": "4627e1e8ed62", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n    indices = self.remap_indices(indices)\n    latent_vectors = self.codebook[indices]\n    latent_vectors = latent_vectors.reshape(shape)\n    if self.permute_latent:\n        latent_vectors = latent_vectors.permute(0, 3, 1, 2)\n    return latent_vectors\n", "CodeBleu_score": 22.685550892287747}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.codebook_indices_remapping is not None:\n      indices = self.codebook_indices_remapping[indices]\n    latent_vectors = tf.gather(self.codebook, indices)\n    latent_vectors = tf.reshape(latent_vectors, shape)\n    if self.codebook_permute_dims is not None:\n      latent_vectors = tf.transpose(latent_vectors, self.codebook_permute_dims)\n    return latent_vectors\n\n  def get_codebook_indices(self, latent_vectors):\n    \"\"\"This function retrieves the codebook indices for a given batch of latent vectors.\\nIt handles optional remapping of indices and ensures the output indices are correctly reshaped and permuted to match the original input dimensions.\\n\"\"\"\n    if self.codebook_permute_dims is not None:\n      latent_vectors = tf.transpose(latent_vectors, self.codebook_permute_dims)\n    latent_vectors = tf.reshape(latent_vectors, [-1, self.codebook_dim])\n    if self.codebook_indices_remapping is not None:\n      latent_vectors = tf.gather(self.codebook_indices_remapping, latent_vectors)\n    indices = tf.math.argmax(latent_vectors, axis=1)\n    indices = tf.reshape(indices, tf.shape(latent_vectors)[:-1])\n    if self.codebook_permute_dims is not None:\n      indices = tf.transpose(indices, self.codebook_permute_dims)\n    return indices\n\n  def get_codebook_entry_from_indices(self, indices, shape):\n    \"\"\"This function retrieves the quantized latent vectors from the codebook based on given indices and reshapes them to match a specified shape.\\nIt handles optional remapping of indices and ensures the output latent vectors are correctly reshaped and permuted to match the original input dimensions.\\n\"\"\"\n    if self.codebook_indices_remapping is not None:\n      indices = self.codebook_indices_remapping[indices]\n    latent_vectors = tf.gather(self.codebook, indices)\n    latent_vectors = tf.reshape(latent_vectors, shape)\n    if self.codebook_permute_dims is not None:\n      latent_vectors = tf.transpose(latent_vectors, self.codebook_perm", "CodeBleu_score": 37.77897449007369}, {"code": "def get_codebook_entry(self, indices, shape)\n    if self.codebook_indices is not None:\n      indices = self.codebook_indices[indices]\n    if self.codebook_indices_remapping is not None:\n      indices = self.codebook_indices_remapping[indices]\n    return self.codebook.embedding.weight[indices].reshape(shape)\n", "CodeBleu_score": 27.421754298170665}]}
{"instance_id": "c55d014827ef", "generation_result": [{"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor == -1:\n        for i in range(1, int(dimension**0.5)+1):\n            if dimension % i == 0:\n                return (i, int(dimension/i))\n    elif factor > 0:\n        if factor > dimension:\n            return (dimension, 1)\n        for i in range(1, int(dimension**0.5)+1):\n            if dimension % i == 0:\n                if i <= factor:\n                    return (i, int(dimension/i))\n                else:\n                    return (factor, int(dimension/factor))\n    else:\n        if factor < -dimension:\n            return (-dimension, 1)\n        for i in range(1, int(dimension**0.5)+1):\n            if dimension % i == 0:\n                if -i >= factor:\n                    return (-i, int(dimension/i))\n                else:\n                    return (-factor, int(dimension/-factor))\n    return (1, dimension)\n", "CodeBleu_score": 26.826963068964204}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0 and dimension % factor == 0:\n        return (factor, dimension // factor)\n    elif factor < 0:\n        for i in range(abs(factor), 0, -1):\n            if dimension % i == 0:\n                return (i, dimension // i)\n    else:\n        raise ValueError(\"Factor must be positive or negative.\")\n", "CodeBleu_score": 22.54620213317862}, {"code": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]\n    if factor > 0:\n        if factor > dimension:\n            return (factor, 1)\n        else:\n            return (dimension // factor, factor)\n    else:\n        if dimension % 2 == 0:\n            return factorization(dimension // 2)\n        else:\n            for i in range(3, dimension + 1, 2):\n                if dimension % i == 0:\n                    return factorization(dimension // i, i)\n            return (dimension, 1)\n", "CodeBleu_score": 26.23570121228796}]}
{"instance_id": "5d613ec26036", "generation_result": [{"code": "def apply_weight_decompose(self, updown, orig_weight)\n    # Ensure that the input weight and original weight have the same device and data type\n    orig_weight = orig_weight.to(updown.weight.device)\n    orig_weight = orig_weight.type(updown.weight.dtype)\n\n    # Calculate the combined weight\n    combined_weight = updown.weight + orig_weight\n\n    # Calculate the combined weight's norm\n    combined_weight_norm = torch.norm(combined_weight)\n\n    # Calculate the combined weight's norm's square root\n    combined_weight_norm_sqrt = torch.sqrt(combined_weight_norm)\n\n    # Calculate the combined weight's norm's square root's inverse\n    combined_weight_norm_sqrt_inv = 1 / combined_weight_norm_sqrt\n\n    # Calculate the scaled combined weight\n    scaled_combined_weight = self.weight_scale * combined_weight_norm_sqrt_inv * combined_weight\n\n    # Calculate the adjusted weight\n    adjusted_weight = scaled_combined_weight - orig_weight\n\n    # Return the adjusted weight\n    return adjusted_weight\n\n  def forward(self, x):\n    \"\"\"This function defines the computation performed at every call.\n    \"\"\"\n    # Calculate the output of the module\n    output = self.conv(x)\n\n    # Return the output of the module\n    return output\n", "CodeBleu_score": 25.154315526846595}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n    if updown:\n        combined_weight = orig_weight + self.weight\n    else:\n        combined_weight = orig_weight - self.weight\n    combined_weight = combined_weight.div(torch.norm(combined_weight))\n    combined_weight = combined_weight.mul(self.scale_factor)\n    return combined_weight.sub(orig_weight)\n", "CodeBleu_score": 18.629086248486402}, {"code": "def apply_weight_decompose(self, updown, orig_weight)\n    orig_weight = orig_weight.to(self.device)\n    if updown == 'up':\n        weight = self.weight_decompose_up(orig_weight)\n    elif updown == 'down':\n        weight = self.weight_decompose_down(orig_weight)\n    else:\n        raise ValueError('updown must be either \"up\" or \"down\"')\n    weight = weight.to(self.device)\n    return weight\n", "CodeBleu_score": 16.730824537455852}]}
{"instance_id": "c73d0d1cd3e5", "generation_result": [{"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    # Adjust bias\n    if ex_bias is not None:\n        updown = updown + ex_bias.reshape(updown.shape)\n    # Reshape updown to match original weight shape\n    if orig_weight.shape!= updown.shape:\n        updown = updown.reshape(orig_weight.shape)\n    # Apply weight decomposition\n    if self.decompo_list[0] is not None:\n        updown = self.decompo_list[0](updown)\n    # Apply scalar\n    if self.scalar_list[0] is not None:\n        updown = self.scalar_list[0](updown)\n    return updown\n", "CodeBleu_score": 34.50621190511324}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if ex_bias is None:\n        ex_bias = torch.zeros(output_shape)\n    if orig_weight.numel() == updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    elif orig_weight.numel() > updown.numel():\n        updown = updown.reshape(orig_weight.shape)\n    if self.bias_decomposition:\n        updown = self.decomposition_function(updown, self.bias_decomposition)\n    if self.bias_scalar!= 1:\n        updown = updown * self.bias_scalar\n    if self.bias_additive is not None:\n        ex_bias = ex_bias + self.bias_additive\n    return updown, ex_bias\n", "CodeBleu_score": 46.36388782120721}, {"code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)\n    if ex_bias is None:\n        ex_bias = torch.zeros(output_shape)\n    else:\n        ex_bias = ex_bias.reshape(output_shape)\n    if updown.shape == orig_weight.shape:\n        updown = updown.reshape(orig_weight.shape)\n    elif updown.numel() == orig_weight.numel():\n        updown = updown.reshape(orig_weight.shape)\n    else:\n        raise ValueError(\"The updown tensor must have the same number of elements as the original weight tensor, or it must have the same shape as the original weight tensor.\")\n    if self.bias is not None:\n        ex_bias = ex_bias + self.bias\n    if self.scale is not None:\n        updown = self.scale.decompose_tensor(updown)\n    return updown, ex_bias\n", "CodeBleu_score": 48.751947023819916}]}
{"instance_id": "c6a41665c62c", "generation_result": [{"code": "def calc_updown(self, orig_weight)\n    if orig_weight.is_cuda:\n        return self.calc_updown_cuda(orig_weight)\n    else:\n        return self.calc_updown_cpu(orig_weight)\n", "CodeBleu_score": 14.423930849474372}, {"code": "def calc_updown(self, orig_weight)\n    self.updated_weight = self.updated_weight.to(orig_weight.device)\n    return self.calc_updown_helper(orig_weight)\n", "CodeBleu_score": 8.253730701548786}, {"code": "def calc_updown(self, orig_weight)\n    if orig_weight.device!= self.weight.device:\n        orig_weight = orig_weight.to(self.weight.device)\n    if self.bias is not None:\n        bias = self.bias.to(self.weight.device)\n    else:\n        bias = None\n    return self.calc_updown_func(orig_weight, self.weight, bias)\n", "CodeBleu_score": 43.72106087995879}]}
{"instance_id": "1bb9e5d94af8", "generation_result": [{"code": "def purge_networks_from_memory()\n    global networks\n    if len(networks) > NETWORK_LIMIT:\n        while len(networks) > NETWORK_LIMIT:\n            networks.popitem()\n        gc.collect()\n\n", "CodeBleu_score": 27.970240681892054}, {"code": "def purge_networks_from_memory()\n    if len(networks) > MAX_NETWORKS:\n        for i in range(len(networks) - MAX_NETWORKS):\n            networks.popitem(last=False)\n        gc.collect()\n", "CodeBleu_score": 33.35930060935885}, {"code": "def purge_networks_from_memory()\n    global networks\n    while len(networks) > MAX_NETWORKS:\n        networks.popitem(last=False)\n        gc.collect()\n", "CodeBleu_score": 27.980092982402972}]}
{"instance_id": "42b48ebf4d07", "generation_result": [{"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if self.backup_weights is not None:\n        self.weight.data = self.backup_weights\n    if self.backup_bias is not None:\n        self.bias.data = self.backup_bias\n    if isinstance(self, torch.nn.MultiheadAttention):\n        self.out_proj.weight.data = self.out_proj.backup_weights\n        if self.out_proj.backup_bias is not None:\n            self.out_proj.bias.data = self.out_proj.backup_bias\n", "CodeBleu_score": 28.309441041456225}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if self.has_backup():\n        self.load_backup()\n    else:\n        raise ValueError('No backup weights found for layer'+ self.name)\n", "CodeBleu_score": 13.727445596934123}, {"code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    if hasattr(self, 'weight_backup') and hasattr(self, 'bias_backup'):\n        self.weight.data = self.weight_backup\n        self.bias.data = self.bias_backup\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.weight.data = self.out_proj.weight_backup\n            self.out_proj.bias.data = self.out_proj.bias_backup\n    else:\n        print('WARNING: No weights or biases found in backup for layer {}.'.format(self))\n", "CodeBleu_score": 30.79640757285924}]}
{"instance_id": "c8fcd357131a", "generation_result": [{"code": "def network_forward(org_module, input, original_forward)\n    if not hasattr(org_module, 'loaded_networks'):\n        return original_forward(input)\n    org_module.load_state_dict(org_module.backup_state_dict)\n    org_module.reset_parameters()\n    output = original_forward(input)\n    for layer_name, layer_network in org_module.loaded_networks.items():\n        if layer_name == org_module.__class__.__name__:\n            output = layer_network(output)\n    return output\n", "CodeBleu_score": 27.642556851567733}, {"code": "def network_forward(org_module, input, original_forward)\n    # If the module has no loaded networks, use the original forward method\n    if not hasattr(org_module, 'networks'):\n        return original_forward(org_module, input)\n\n    # Cast the input to the appropriate device\n    input = input.to(org_module.networks[0].device)\n\n    # Save the original weights and reset to the backup\n    org_weights = org_module.state_dict()\n    org_module.load_state_dict(org_module.backup)\n\n    # Call the original forward method\n    output = original_forward(org_module, input)\n\n    # Apply each loaded network in sequence\n    for network in org_module.networks:\n        if network.layer == org_module.layer:\n            output = network(output)\n\n    # Restore the original weights and return the output\n    org_module.load_state_dict(org_weights)\n    return output\n", "CodeBleu_score": 45.07514318548046}, {"code": "def network_forward(org_module, input, original_forward)\n    # Get the current device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # Cast the input to the appropriate device\n    input = input.to(device)\n    # Save the module's original weights\n    weights = org_module.state_dict()\n    # Reset the module's weights\n    org_module.reset_parameters()\n    # Call the original forward method\n    output = original_forward(input)\n    # If there are any loaded networks, check if they match the current layer\n    if networks:\n        for network in networks:\n            if network.layer == org_module:\n                # If they do, apply the network to the output\n                output = network.forward(output)\n    # Restore the module's original weights\n    org_module.load_state_dict(weights)\n    # Return the output\n    return output\n", "CodeBleu_score": 29.07535578704642}]}
{"instance_id": "1070211f5c11", "generation_result": [{"code": "def trainables(self)\n    trainables = []\n    for name, p in self.named_parameters():\n        if p.requires_grad:\n            trainables.append(p)\n    return trainables\n", "CodeBleu_score": 23.031154885518767}, {"code": "def trainables(self)\n    trainable_params = []\n    for name, param in self.named_parameters():\n        if param.requires_grad:\n            trainable_params.append(param)\n    return trainable_params\n", "CodeBleu_score": 23.684524588475384}, {"code": "def trainables(self)\n    return [p for p in self.parameters() if p.requires_grad]\n", "CodeBleu_score": 6.452447410697191}]}
{"instance_id": "0c3ba62c96a7", "generation_result": [{"code": "def load_hypernetworks(names, multipliers=None)\n    if multipliers is None:\n        multipliers = [1.0] * len(names)\n    hypernets = []\n    for name, multiplier in zip(names, multipliers):\n        if name in hypernets_loaded:\n            hypernet = hypernets_loaded[name]\n            hypernet.set_multiplier(multiplier)\n        else:\n            hypernet = Hypernetwork.load(name)\n            hypernet.set_multiplier(multiplier)\n            hypernets_loaded[name] = hypernet\n        hypernets.append(hypernet)\n    return hypernets\n", "CodeBleu_score": 28.06461045287002}, {"code": "def load_hypernetworks(names, multipliers=None)\n    global hypernetworks\n    hypernetworks = []\n    for name in names:\n        if name in hypernetworks_dict:\n            hypernetwork = hypernetworks_dict[name]\n        else:\n            hypernetwork = Hypernetwork()\n            hypernetwork.load(name)\n            hypernetworks_dict[name] = hypernetwork\n        if multipliers is not None:\n            hypernetwork.set_multipliers(multipliers[names.index(name)])\n        hypernetworks.append(hypernetwork)\n\n", "CodeBleu_score": 25.13530963087185}, {"code": "def load_hypernetworks(names, multipliers=None)\n    if multipliers is None:\n        multipliers = [1.0 for _ in names]\n    hypernetworks = []\n    for name, multiplier in zip(names, multipliers):\n        hypernetwork = load_hypernetwork(name)\n        hypernetwork.set_multiplier(multiplier)\n        hypernetworks.append(hypernetwork)\n    hypernetworks_loaded = hypernetworks_loaded + hypernetworks\n    return hypernetworks\n\n", "CodeBleu_score": 23.938637588074087}]}
{"instance_id": "bea3772ed532", "generation_result": [{"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n    context_k = context\n    context_v = context\n\n    for h in hypernetworks:\n        context_k, context_v = apply_hypernetwork(h, context_k, context_v, layer)\n\n    return context_k, context_v\n\n", "CodeBleu_score": 66.93939125806811}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n    if layer is None:\n        context_k, context_v = context, context\n    else:\n        context_k, context_v = context[0], context[1]\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hypernetwork, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 56.62051122346693}, {"code": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)\n    context_k, context_v = context\n    for hn in hypernetworks:\n        context_k, context_v = apply_hypernetwork(hn, context_k, context_v, layer)\n    return context_k, context_v\n\n", "CodeBleu_score": 58.07435799979862}]}
{"instance_id": "922efa3e092f", "generation_result": [{"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    # x: [batch, seq_len, d_model]\n    # context: [batch, seq_len, d_model]\n    # mask: [batch, seq_len]\n    # query: [batch, seq_len, d_model]\n    # key: [batch, seq_len, d_model]\n    # value: [batch, seq_len, d_model]\n    # context_hyper: [batch, d_model]\n    # query_hyper: [batch, d_model]\n    # key_hyper: [batch, d_model]\n    # value_hyper: [batch, d_model]\n    # context_hyper: [batch, d_model]\n    # query_hyper: [batch, d_model]\n    # key_hyper: [batch, d_model]\n    # value_hyper: [batch, d_model]\n    # context_hyper: [batch, d_model]\n    # query_hyper: [batch, d_model]\n    # key_hyper: [batch, d_model]\n    # value_hyper: [batch, d_model]\n    # context_hyper: [batch, d_model]\n    # query_hyper: [batch, d_model]\n    # key_hyper: [batch, d_model]\n    # value_hyper: [batch, d_model]\n    # context_hyper: [batch, d_model]\n    # query_hyper: [batch, d_model]\n    # key_hyper: [batch, d_model]\n    # value_hyper: [batch, d_model]\n    # context_hyper: [batch, d_model]\n    # query_hyper: [batch, d_model]\n    # key_hyper: [batch, d_model]\n    # value_hyper: [batch, d_model]\n    # context_hyper: [batch, d_model]\n    # query_hyper: [batch, d_model]\n    # key_hyper: [batch, d_model]\n    # value_hyper: [batch, d_model]\n    # context_hyper: [batch, d_model]\n    # query_hyper: [batch, d_model]\n    # key_hyper: [batch, d_model]\n    # value_hyper: [batch, d_model]\n    # context_hyper: [batch, d", "CodeBleu_score": 1.9834764933446987}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    # Check if context is None\n    if context is None:\n        context = x\n    # Check if mask is None\n    if mask is None:\n        mask = torch.ones(x.shape[0], x.shape[1]).to(x.device)\n    # Check if mask is a list\n    if isinstance(mask, list):\n        mask = torch.stack(mask).to(x.device)\n    # Check if mask is a tuple\n    if isinstance(mask, tuple):\n        mask = torch.stack(mask).to(x.device)\n    # Check if mask is a tensor\n    if isinstance(mask, torch.Tensor):\n        mask = mask.to(x.device)\n    # Check if mask is a numpy array\n    if isinstance(mask, np.ndarray):\n        mask = torch.from_numpy(mask).to(x.device)\n    # Check if mask is a sparse tensor\n    if isinstance(mask, torch.sparse.Tensor):\n        mask = mask.to_dense().to(x.device)\n    # Check if mask is a sparse COO tensor\n    if isinstance(mask, torch.sparse.FloatTensor):\n        mask = mask.to_dense().to(x.device)\n    # Check if mask is a sparse CSR tensor\n    if isinstance(mask, torch.sparse.FloatTensor):\n        mask = mask.to_dense().to(x.device)\n    # Check if mask is a sparse CSC tensor\n    if isinstance(mask, torch.sparse.FloatTensor):\n        mask = mask.to_dense().to(x.device)\n    # Check if mask is a sparse BSR tensor\n    if isinstance(mask, torch.sparse.FloatTensor):\n        mask = mask.to_dense().to(x.device)\n    # Check if mask is a sparse COO tensor\n    if isinstance(mask, torch.sparse.DoubleTensor):\n        mask = mask.to_dense().to(x.device)\n    # Check if mask is a sparse CSR tensor\n    if isinstance(mask, torch.sparse.DoubleTensor):\n        mask = mask.to_dense().to(x.device)\n    # Check if mask is a sparse CSC tensor\n    if isinstance(mask, torch.sparse.DoubleTensor):\n        mask = mask.to_dense().to(x.device)\n    # Check if mask is a sparse BSR tensor\n    if isinstance(mask, torch.sparse.DoubleTensor", "CodeBleu_score": 21.20783688657902}, {"code": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)\n    if context is None:\n        context = x\n    # Project the inputs to q, k, v\n    q = self.q_proj(x)\n    k = self.k_proj(context)\n    v = self.v_proj(context)\n    # Project the context to r, s, t\n    r = self.r_proj(context)\n    s = self.s_proj(context)\n    t = self.t_proj(context)\n    # Calculate the attention\n    attn_output, attn_output_weights = self.multi_head_attention(q, k, v, r, s, t, mask=mask)\n    # Reshape the output to a 2D tensor\n    attn_output = attn_output.reshape(x.shape[0], -1, self.embed_dim)\n    # Project the output back to the original input size\n    attn_output = self.out_proj(attn_output)\n    return attn_output, attn_output_weights\n\n", "CodeBleu_score": 25.091096070032048}]}
{"instance_id": "8b7455a03a0e", "generation_result": [{"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    # Save the original attributes\n    original_attributes = {}\n    for attribute_name in ['_checkpoint', '_hypernetwork_name', '_filename']:\n        original_attributes[attribute_name] = getattr(hypernetwork, attribute_name)\n    # Update the attributes of the hypernetwork\n    hypernetwork._checkpoint = checkpoint\n    hypernetwork._hypernetwork_name = hypernetwork_name\n    hypernetwork._filename = filename\n    # Save the hypernetwork\n    try:\n        hypernetwork.save()\n    except:\n        # Restore the original attributes\n        for attribute_name in original_attributes:\n            setattr(hypernetwork, attribute_name, original_attributes[attribute_name])\n        # Re-raise the exception\n        raise\n    # Restore the original attributes\n    for attribute_name in original_attributes:\n        setattr(hypernetwork, attribute_name, original_attributes[attribute_name])\n", "CodeBleu_score": 33.72703221720783}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    try:\n        hypernetwork.checkpoint = checkpoint\n        hypernetwork.save_hypernetwork(hypernetwork_name, filename)\n    except Exception as e:\n        hypernetwork.checkpoint = None\n        hypernetwork.load_hypernetwork(hypernetwork_name, filename)\n        raise e\n", "CodeBleu_score": 19.990159965211614}, {"code": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n    hypernetwork_state = hypernetwork.state_dict()\n    hypernetwork.checkpoint = checkpoint\n    hypernetwork.hypernetwork_name = hypernetwork_name\n    try:\n        hypernetwork.save(filename)\n    except Exception as e:\n        hypernetwork.load_state_dict(hypernetwork_state)\n        hypernetwork.checkpoint = None\n        hypernetwork.hypernetwork_name = None\n        raise e\n    hypernetwork.load_state_dict(hypernetwork_state)\n    hypernetwork.checkpoint = None\n    hypernetwork.hypernetwork_name = None\n", "CodeBleu_score": 27.262768572379258}]}
{"instance_id": "3175f835140f", "generation_result": [{"code": "def ema_scope(self, context=None)\n    return self._execute('ema_scope', payload=locals(), response_object=None)\n\n  def ema_scope_input_model(self, input_model, context=None):\n    \"\"\"This function manages the use of Exponential Moving Average (EMA) weights for a model, temporarily switching to EMA weights and restoring the original weights after execution.\\nIf EMA is enabled, it stores the current model parameters, copies EMA weights to the model, and optionally logs this action.\\nAfter the code block execution, it restores the original weights and optionally logs the restoration.\\n\"\"\"\n    return self._execute('ema_scope', payload=locals(), response_object=None)\n\n  def execute(self, context=None):\n    \"\"\"Executes the model.\"\"\"\n    return self._execute('execute', payload=locals(), response_object=None)\n\n  def execute_input_model(self, input_model, context=None):\n    \"\"\"Executes the model.\"\"\"\n    return self._execute('execute', payload=locals(), response_object=None)\n\n  def export(self, export_format, context=None):\n    \"\"\"Exports the model to the specified format. The exported model may be imported into other systems. Currently only TensorFlow SavedModel format is supported. Note that the exported model is not guaranteed to be compatible with the current version of the model builder. The model builder version is specified in the model metadata. The model builder version can be specified in the model metadata using the ModelMetadata.metadata_version field. The model builder version is independent of the TensorFlow version.\\n\"\"\"\n    return self._execute('export', payload=locals(), response_object=None)\n\n  def export_input_model(self, input_model, export_format, context=None):\n    \"\"\"Exports the model to the specified format. The exported model may be imported into other systems. Currently only TensorFlow SavedModel format is supported. Note that the exported model is not guaranteed to be compatible with the current version of the model builder. The model builder version is specified in the model metadata. The model builder version can be specified in the model metadata using the ModelMetadata.metadata_version field. The model builder version is independent of the TensorFlow version.\\n\"\"\"\n    return self._execute('export', payload=locals(), response_object=None)\n\n  def export_to_gcs(self, output_uri_prefix, export_format, context=None):", "CodeBleu_score": 28.971107292673132}, {"code": "def ema_scope(self, context=None)\n    if context is None:\n      context = Context.current\n    if context.executing_eagerly():\n      raise RuntimeError(\"tf.compat.v1.moving_average_variables is not supported when eager execution is enabled.\")\n    context.enter()\n    try:\n      control_flow_ops.no_op()\n    finally:\n      context.exit()\n\n", "CodeBleu_score": 21.867446511549478}, {"code": "def ema_scope(self, context=None)\n    return self._apply_op(self.ema_scope, context)\n\n  def ema_restore(self, context=None):\n    \"\"\"This function restores the original weights of a model, optionally logging this action.\\n\"\"\"\n    return self._apply_op(self.ema_restore, context)\n\n  def ema_set_decay(self, decay, context=None):\n    \"\"\"This function sets the decay rate for the EMA model, optionally logging this action.\\n\"\"\"\n    return self._apply_op(self.ema_set_decay, decay, context)\n\n  def ema_set_enabled(self, enabled, context=None):\n    \"\"\"This function enables or disables the EMA model, optionally logging this action.\\n\"\"\"\n    return self._apply_op(self.ema_set_enabled, enabled, context)\n\n  def ema_set_step(self, step, context=None):\n    \"\"\"This function sets the EMA model step, optionally logging this action.\\n\"\"\"\n    return self._apply_op(self.ema_set_step, step, context)\n\n  def ema_set_warmup_steps(self, warmup_steps, context=None):\n    \"\"\"This function sets the EMA model warmup steps, optionally logging this action.\\n\"\"\"\n    return self._apply_op(self.ema_set_warmup_steps, warmup_steps, context)\n\n  def ema_update(self, context=None):\n    \"\"\"This function updates the EMA model, optionally logging this action.\\n\"\"\"\n    return self._apply_op(self.ema_update, context)\n\n  def get_step(self, context=None):\n    \"\"\"This function returns the step of the model, optionally logging this action.\\n\"\"\"\n    return self._apply_op(self.get_step, context)\n\n  def get_warmup_steps(self, context=None):\n    \"\"\"This function returns the warmup steps of the model, optionally logging this action.\\n\"\"\"\n    return self._apply_op(self.get_warmup_steps, context)\n\n  def set_step(self, step, context=None):\n    \"\"\"This function sets the step of the model, optionally logging this action.\\n\"\"\"\n    return self._apply_op(self.set_step, step, context)\n\n  def set_warmup_steps(self, warmup", "CodeBleu_score": 27.052181444437483}]}
{"instance_id": "a7b5d5cd519a", "generation_result": [{"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    # Get the model output\n    model_output = self.model(x, t)\n\n    # Reconstruct the input\n    if clip_denoised:\n        x_recon = torch.clamp(x - model_output, min=-1, max=1)\n    else:\n        x_recon = x - model_output\n\n    # Calculate the mean, variance, and log variance\n    model_mean, posterior_variance = self.model.predict_start_from_noise(x_recon, t)\n    model_logvar = torch.log(posterior_variance)\n\n    return model_mean, posterior_variance, model_logvar\n\n", "CodeBleu_score": 39.120183679023704}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    self.model.eval()\n    with torch.no_grad():\n        x_recon = self.model.forward_sample(x, t, clip_denoised=clip_denoised)\n        model_mean, posterior_variance = self.model.forward_mean_variance(x, t, clip_denoised=clip_denoised)\n        model_log_variance = torch.log(posterior_variance)\n    return model_mean, model_log_variance, posterior_variance\n", "CodeBleu_score": 34.09242728798622}, {"code": "def p_mean_variance(self, x, t, clip_denoised: bool)\n    x_recon = self.model(x, t, clip_denoised)\n    x_recon = x_recon.clamp(-1, 1)\n    model_mean, posterior_variance = self.model.get_mean_and_variance(x_recon, t, clip_denoised)\n    model_log_variance = torch.log(posterior_variance.clamp(1e-20))\n    return model_mean, posterior_variance, model_log_variance\n", "CodeBleu_score": 23.157555586778233}]}
{"instance_id": "a8652fa78473", "generation_result": [{"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n    else:\n        raise ValueError(f'Unknown loss type {self.loss_type}')\n    return loss\n\n", "CodeBleu_score": 46.02902109631335}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        loss = torch.abs(pred - target)\n        if mean:\n            loss = torch.mean(loss)\n        return loss\n    elif self.loss_type == 'l2':\n        loss = torch.pow(pred - target, 2)\n        if mean:\n            loss = torch.mean(loss)\n        return loss\n    else:\n        raise ValueError('Unknown loss type.')\n", "CodeBleu_score": 47.47571245989022}, {"code": "def get_loss(self, pred, target, mean=True)\n    if self.loss_type == 'l1':\n        if mean:\n            return torch.mean(torch.abs(pred - target))\n        else:\n            return torch.abs(pred - target)\n    elif self.loss_type == 'l2':\n        return torch.mean((pred - target)**2)\n    else:\n        raise ValueError('Unknown loss type: {}'.format(self.loss_type))\n", "CodeBleu_score": 29.52215326084649}]}
{"instance_id": "d68e4f061349", "generation_result": [{"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.full((batch_size,), self.n_timesteps - 1, device=x_start.device, dtype=torch.long)\n    mu, logvar = self.model.encode(x_start, t)\n    kl = gaussian_kl(mu, logvar)\n    return kl.mean() / math.log(2)\n", "CodeBleu_score": 26.256516182336153}, {"code": "def _prior_bpd(self, x_start)\n    batch_size = x_start.shape[0]\n    t = torch.ones(batch_size, device=x_start.device) * (self.num_timesteps - 1)\n    x_start_pred_dist = self.model(x_start, t)\n    mean, logvar = x_start_pred_dist.mean, x_start_pred_dist.logvar\n    kl = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n    return kl / math.log(2.0)\n\n  def _get_lr(self, t):\n    \"\"\"This function returns the learning rate for the Adam optimizer given the current timestep t.\\n\"\"\"\n    return self.lr_init * self.lr_decay_rate ** (t // self.lr_decay_steps)\n\n  def _get_beta(self, t):\n    \"\"\"This function returns the beta parameter for the KL annealing schedule given the current timestep t.\\n\"\"\"\n    return self.beta_init * self.beta_decay_rate ** (t // self.beta_decay_steps)\n\n  def _get_batch_size(self, t):\n    \"\"\"This function returns the batch size for the given timestep t.\\n\"\"\"\n    return self.batch_size_init * self.batch_size_decay_rate ** (t // self.batch_size_decay_steps)\n\n  def _get_num_timesteps(self, t):\n    \"\"\"This function returns the number of timesteps for the given timestep t.\\n\"\"\"\n    return self.num_timesteps_init * self.num_timesteps_decay_rate ** (t // self.num_timesteps_decay_steps)\n\n  def _get_num_epochs(self, t):\n    \"\"\"This function returns the number of epochs for the given timestep t.\\n\"\"\"\n    return self.num_epochs_init * self.num_epochs_decay_rate ** (t // self.num_epochs_decay_steps)\n\n  def _get_num_samples(self, t):\n    \"\"\"This function returns the number of samples for the given timestep t.\\n\"\"\"\n    return self.num_samples_init * self.num_samples_decay_rate ** (t // self.num_samples_decay_steps)\n\n  def _get_num", "CodeBleu_score": 20.15404298535171}, {"code": "def _prior_bpd(self, x_start)\n\n    # Get the batch size\n    batch_size = x_start.shape[0]\n\n    # Create a tensor t representing the final timestep for each batch element\n    t = torch.tensor(self.timesteps - 1, dtype=torch.long, device=self.device)\n\n    # Calculate the mean and log variance of the predicted distribution at this timestep\n    mean, log_var = self.model(x_start, t)\n\n    # Calculate the KL divergence between this distribution and a standard normal distribution\n    kl_div = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), dim=1)\n\n    # Return the mean KL divergence normalized by the logarithm of 2\n    return torch.mean(kl_div) / math.log(2.0)\n\n  def _sample_and_log_prob(self, x_start):\n    \"\"\"This function samples from the predicted distribution and calculates the log probability of the sampled value.\\nIt determines the batch size and creates a tensor t representing the final timestep for each batch element.\\nThe function then samples from the predicted distribution at this timestep and calculates the log probability of the sampled value.\\nFinally, it returns the log probability of the sampled value.\\n\"\"\"\n\n    # Get the batch size\n    batch_size = x_start.shape[0]\n\n    # Create a tensor t representing the final timestep for each batch element\n    t = torch.tensor(self.timesteps - 1, dtype=torch.long, device=self.device)\n\n    # Sample from the predicted distribution at this timestep\n    x_t = self.model.sample(x_start, t)\n\n    # Calculate the log probability of the sampled value\n    log_prob = self.model.log_prob(x_t, t)\n\n    # Return the log probability of the sampled value\n    return log_prob\n\n  def _get_log_likelihood(self, x):\n    \"\"\"This function calculates the log likelihood of the input tensor x.\\nIt determines the batch size and creates a tensor t representing the final timestep for each batch element.\\nThe function then calculates the log likelihood of the input tensor x at this timestep.\\nFinally, it returns the log likelihood of the input tensor x.\\n\"\"\"\n\n    # Get the batch size\n    batch_size = x.shape[0]\n\n    # Create a tensor t representing the final", "CodeBleu_score": 35.7535572625032}]}
{"instance_id": "c695d93fdf2d", "generation_result": [{"code": "def make_cache(subsection: str) -> diskcache.Cache\n    cache_dir = Path.home() / \".cache\" / \"py_cui_2\" / subsection\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return diskcache.Cache(str(cache_dir), size_limit=4 * 1024 ** 3, disk_min_file_size=256)\n\n", "CodeBleu_score": 47.05103847333748}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n    cache_dir = os.path.join(CACHE_DIR, subsection)\n    cache = diskcache.Cache(cache_dir, size_limit=4 * 1024 ** 3, disk=diskcache.JSONDisk, disk_min_file_size=256)\n    return cache\n\n", "CodeBleu_score": 49.8265789325742}, {"code": "def make_cache(subsection: str) -> diskcache.Cache\n    cache_dir = os.path.join(os.path.dirname(__file__), \"cache\")\n    cache = diskcache.Cache(\n        os.path.join(cache_dir, subsection),\n        size_limit=4 * 1024 ** 3,\n        disk=diskcache.JSONDisk,\n        disk_min_file_size=256,\n    )\n    return cache\n\n", "CodeBleu_score": 56.163602118618186}]}
{"instance_id": "ed4c543f0d38", "generation_result": [{"code": "def cache(subsection)\n    global caches, cache_lock\n\n    cache_lock.acquire()\n    if subsection not in caches:\n        if not os.path.isdir(CACHE_DIR):\n            os.mkdir(CACHE_DIR)\n        if not os.path.isfile(CACHE_DIR + subsection):\n            convert_old_cached_data(subsection)\n        if not os.path.isfile(CACHE_DIR + subsection):\n            caches[subsection] = make_cache(subsection)\n    cache_lock.release()\n\n    return caches[subsection]\n\n", "CodeBleu_score": 39.907983974130076}, {"code": "def cache(subsection)\n    global caches\n    global cache_lock\n    cache_lock.acquire()\n    if subsection not in caches:\n        if not os.path.exists(os.path.join(cache_dir, subsection)):\n            convert_old_cached_data(subsection)\n        if not os.path.exists(os.path.join(cache_dir, subsection)):\n            os.makedirs(os.path.join(cache_dir, subsection))\n        if not os.path.isfile(os.path.join(cache_dir, subsection, 'cache.json')):\n            cache = make_cache(subsection)\n        else:\n            cache = get_cache(subsection)\n        caches[subsection] = cache\n    cache_lock.release()\n    return caches[subsection]\n", "CodeBleu_score": 42.25917925712911}, {"code": "def cache(subsection)\n    global caches\n    global cache_lock\n    cache_lock.acquire()\n    if subsection in caches:\n        cache = caches[subsection]\n        cache_lock.release()\n        return cache\n    else:\n        if os.path.isdir(cache_dir) and os.path.isfile(cache_file):\n            if os.path.getmtime(cache_file) + cache_timeout > time.time():\n                convert_old_cached_data()\n        if os.path.isdir(cache_dir) and os.path.isfile(cache_file):\n            if os.path.getmtime(cache_file) + cache_timeout > time.time():\n                cache = caches[subsection]\n                cache_lock.release()\n                return cache\n        else:\n            convert_old_cached_data()\n        if subsection in caches:\n            cache = caches[subsection]\n            cache_lock.release()\n            return cache\n        else:\n            cache = make_cache(subsection)\n            caches[subsection] = cache\n            cache_lock.release()\n            return cache\n", "CodeBleu_score": 40.971525178495455}]}
{"instance_id": "ccf5ea2d9fc1", "generation_result": [{"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    if title in cache:\n        entry = cache[title]\n        if os.path.getmtime(filename) < entry['mtime']:\n            del cache[title]\n    if title not in cache:\n        cache[title] = {'mtime': os.path.getmtime(filename)}\n        cache[title]['data'] = func()\n        dump_cache(subsection, cache)\n    return cache[title]['data']\n\n", "CodeBleu_score": 29.741676458863132}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        if mtime > cache[title][1]:\n            cache.pop(title)\n    if title not in cache:\n        cache[title] = (func(), mtime)\n        dump_cache(cache)\n    return cache[title][0]\n\n", "CodeBleu_score": 20.13134292707741}, {"code": "def cached_data_for_file(subsection, title, filename, func)\n    cache = cache_for_subsection(subsection)\n    mtime = os.path.getmtime(filename)\n    if title in cache:\n        cache_entry = cache[title]\n        if mtime > cache_entry['mtime']:\n            del cache[title]\n    if title not in cache:\n        cache[title] = {\n           'mtime': mtime,\n            'data': func()\n        }\n        dump_cache(subsection, cache)\n    return cache[title]['data']\n", "CodeBleu_score": 24.016005584660068}]}
{"instance_id": "e01a2d4f0ac2", "generation_result": [{"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception(exc_info=exc_info)\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n\n", "CodeBleu_score": 54.177721019029946}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception(exc_info=exc_info)\n    for line in message.splitlines():\n        print(f\"*** {line}\", file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n\n", "CodeBleu_score": 50.50127959200967}, {"code": "def report(message: str, *, exc_info: bool = False) -> None\n    record_exception(exc_info=exc_info)\n    message = message.strip()\n    for line in message.splitlines():\n        print(\"*** \" + line, file=sys.stderr)\n    if exc_info:\n        print(\"---\", file=sys.stderr)\n        traceback.print_exc(file=sys.stderr)\n\n", "CodeBleu_score": 53.91058255607164}]}
{"instance_id": "125fb9d2320b", "generation_result": [{"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n    if not style.prompt:\n        try:\n            prompt = style.negative_prompt.extract_from_prompt(prompt)\n        except:\n            return False, prompt, negative_prompt\n    if not style.negative_prompt:\n        try:\n            negative_prompt = style.prompt.extract_from_prompt(negative_prompt)\n        except:\n            return False, prompt, negative_prompt\n    return True, prompt, negative_prompt\n", "CodeBleu_score": 60.05794982636821}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        extracted_prompt = style.extract_prompt(prompt)\n        extracted_negative_prompt = style.extract_prompt(negative_prompt)\n        return True, extracted_prompt, extracted_negative_prompt\n    except:\n        return False, prompt, negative_prompt\n\n", "CodeBleu_score": 42.833318858311955}, {"code": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)\n    if style.prompt == \"\" and style.negative_prompt == \"\":\n        return False, prompt, negative_prompt\n    try:\n        positive_prompt = style.extract_positive_prompt(prompt)\n        negative_prompt = style.extract_negative_prompt(negative_prompt)\n    except:\n        return False, prompt, negative_prompt\n    return True, positive_prompt, negative_prompt\n", "CodeBleu_score": 42.92246605940092}]}
{"instance_id": "0c5f729fbce2", "generation_result": [{"code": "def random_partial(self, count, n_frames)\n    if self.utterances == []:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    partials = []\n    for u in utterances:\n        partial = u.random_partial(n_frames)\n        partials.append((u, partial))\n    return partials\n", "CodeBleu_score": 52.398408348363944}, {"code": "def random_partial(self, count, n_frames)\n    if not self.utterances_loaded:\n        self._load_utterances()\n    return [(u, u.random_partial(n_frames)) for u in self.utterance_cycler.sample(count)]\n\n", "CodeBleu_score": 43.1980086712999}, {"code": "def random_partial(self, count, n_frames)\n    if not self.utterances:\n        self._load_utterances()\n    return [u.random_partial(n_frames) for u in self.utterance_cycler.sample(count)]\n", "CodeBleu_score": 42.36241256707175}]}
{"instance_id": "31e8b27b8f94", "generation_result": [{"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 96.28790819755086}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * math.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n\n", "CodeBleu_score": 94.53554019588145}, {"code": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * math.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))\n", "CodeBleu_score": 94.53554019588145}]}
{"instance_id": "2353831e9033", "generation_result": [{"code": "def load_model(weights_fpath: Path, device=None) test.\n\n    Args:\n        weights_fpath: Path to the weights of the model to be loaded.\n        device: Device to be used for inference. Defaults to None.\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model = SpeakerEncoder(device=device)\n    model.load_state_dict(torch.load(weights_fpath))\n    model.eval()\n    print(f\"Loaded model at step {model.step} from {weights_fpath}.\")\n\n    return model\n\n", "CodeBleu_score": 16.326654049317202}, {"code": "def load_model(weights_fpath: Path, device=None) ____.\n\n    Args:\n        weights_fpath (Path): Path to the checkpoint file of the model to be loaded.\n        device (optional): Device to be used for inference. Defaults to None.\n\n    Returns:\n        model: The loaded model.\n    \"\"\"\n    # set device to cpu if cuda is not available\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n\n    # initialize the model to inference mode\n    model = SpeakerEncoder()\n    model.eval()\n\n    # load model for evaluation\n    model.load_state_dict(torch.load(weights_fpath))\n    model = model.to(device)\n\n    return model\n\n", "CodeBleu_score": 17.80018251984303}, {"code": "def load_model(weights_fpath: Path, device=None) test.\n\n    Args:\n        weights_fpath: Path to the checkpoint file to load the model weights from.\n        device: Device to use for running the model.\n\n    Returns:\n        The loaded model.\n    \"\"\"\n    device = device or \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    _model.load_state_dict(torch.load(weights_fpath, map_location=device))\n    _model.to(device).eval()\n    print(f\"Loaded model {weights_fpath}.\")\n    return _model\n\n", "CodeBleu_score": 15.56692782116922}]}
{"instance_id": "3231ad480199", "generation_result": [{"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception(\"The model is not loaded.\")\n    frames_batch_tensor = torch.from_numpy(frames_batch).to(_device)\n    embeddings_batch = _model(frames_batch_tensor).detach().cpu().numpy()\n    return embeddings_batch\n\n", "CodeBleu_score": 62.33762385458896}, {"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception(\"The model is not loaded. Please load the model first.\")\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 59.95808066567088}, {"code": "def embed_frames_batch(frames_batch)\n    if _model is None:\n        raise Exception('The model is not loaded. Please load the model using the load_model() function.')\n    frames_batch = torch.from_numpy(frames_batch).to(_device)\n    with torch.no_grad():\n        embeddings = _model(frames_batch).detach().cpu().numpy()\n    return embeddings\n\n", "CodeBleu_score": 58.572365140576956}]}
{"instance_id": "117a41716c8d", "generation_result": [{"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0 < overlap < 1, \"Overlap must be between 0 and 1.\"\n    assert 0 < min_pad_coverage < 1, \"Minimum pad coverage must be between 0 and 1.\"\n    assert 0 < partial_utterance_n_frames <= n_samples, \"Partial utterance length must be less than the number of samples.\"\n    frame_step = int(np.ceil(n_samples * (1 - overlap)))\n    assert frame_step > 0, \"Frame step must be greater than 0.\"\n    n_frames = int(np.ceil(float(n_samples - partial_utterance_n_frames) / frame_step)) + 1\n    assert n_frames > 0, \"Number of frames must be greater than 0.\"\n    # compute mel-spectrogram slices\n    mel_slices = []\n    for i in range(n_frames):\n        mel_start = int(i * frame_step)\n        mel_end = min(int((i + 1) * frame_step), n_samples)\n        mel_slices.append([mel_start, mel_end])\n    # compute audio slices\n    wav_slices = []\n    for i in range(len(mel_slices)):\n        wav_start = mel_slices[i][0]\n        wav_end = mel_slices[i][1]\n        wav_slices.append([wav_start, wav_end])\n    # if last slice is too short, drop it.\n    last_wav_slice = wav_slices[-1]\n    coverage = (last_wav_slice[1] - last_wav_slice[0]) / float(n_samples)\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        del wav_slices[-1]\n        del mel_slices[-1]\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 40.87158656995254}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n    assert 0.0 < overlap < 1.0, \"Overlap must be between 0 and 1\"\n    assert 0.0 < min_pad_coverage < 1.0, \"Minimum pad coverage must be between 0 and 1\"\n    assert partial_utterance_n_frames > 0, \"Partial utterance number of frames must be greater than 0\"\n    # Compute the number of frames and frame step, based on the sampling rate and overlap.\n    n_frames = int(np.ceil(float(n_samples) / hp.sample_rate * hp.frame_shift_ms / 1000))\n    frame_step = int(np.ceil(float(n_frames - partial_utterance_n_frames) / (hp.n_frames - 1)))\n    assert frame_step > 0, \"Frame step must be greater than 0\"\n    # Compute the wav and mel slices.\n    wav_slices = []\n    mel_slices = []\n    for mel_start in range(0, n_frames - partial_utterance_n_frames + 1, frame_step):\n        mel_end = mel_start + partial_utterance_n_frames\n        wav_start = int(round(mel_start * hp.frame_shift_ms / 1000 * hp.sample_rate))\n        wav_end = int(round(mel_end * hp.frame_shift_ms / 1000 * hp.sample_rate))\n        wav_slices.append((wav_start, wav_end))\n        mel_slices.append((mel_start, mel_end))\n    # Evaluate the coverage of the last frame, and decide whether to remove it.\n    coverage = (wav_slices[-1][1] - wav_slices[-1][0]) / float(n_samples)\n    if coverage < min_pad_coverage and len(wav_slices) > 1:\n        del wav_slices[-1]\n        del mel_slices[-1]\n    # Assert the coverage is within the specified range, and return the slices.\n    assert coverage >= min_pad_coverage, \"Coverage below minimum coverage.\"\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 35.64798040724993}, {"code": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)\n\n    # Assertions\n    assert 0.0 <= min_pad_coverage <= 1.0, \"min_pad_coverage must be between 0.0 and 1.0\"\n    assert 0.0 <= overlap < 1.0, \"overlap must be between 0.0 and 1.0\"\n    assert partial_utterance_n_frames > 0, \"partial_utterance_n_frames must be greater than 0\"\n\n    # Compute number of frames and frame step\n    n_frames = int(np.ceil(n_samples / sampling_rate * mel_step_size))\n    frame_step = int(np.round(sampling_rate / mel_step_size))\n\n    # Compute mel and wav slices\n    mel_slices, wav_slices = [], []\n    for mel_start in range(0, n_frames - partial_utterance_n_frames + 1, int(np.round(partial_utterance_n_frames * overlap))):\n        mel_end = mel_start + partial_utterance_n_frames\n        mel_slices.append([mel_start, mel_end])\n        wav_start = int(np.round(mel_start * mel_step_size / sampling_rate))\n        wav_end = int(np.round(mel_end * mel_step_size / sampling_rate))\n        wav_slices.append([wav_start, wav_end])\n\n    # Evaluate coverage\n    coverage = (wav_slices[-1][1] - wav_slices[-1][0]) / n_samples\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n\n    return wav_slices, mel_slices\n\n", "CodeBleu_score": 44.00358503034979}]}
{"instance_id": "34fe5c864f1c", "generation_result": [{"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, height)\n    ax.imshow(embed.reshape(shape), cmap=\"plasma\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(fraction=0.046, pad=0.04, ticks=np.linspace(color_range[0], color_range[1], 10))\n\n", "CodeBleu_score": 45.65852093777559}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(embed.shape[0]))\n        shape = (height, height)\n    embed = embed.reshape(shape)\n    ax.imshow(embed, cmap=plt.cm.get_cmap(\"YlGnBu\", 100), vmin=color_range[0], vmax=color_range[1])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(title)\n    plt.colorbar(fraction=0.046, pad=0.04, ax=ax)\n\n", "CodeBleu_score": 52.55584533590649}, {"code": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))\n    if ax is None:\n        ax = plt.gca()\n    if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, height)\n    ax.imshow(embed.reshape(shape), cmap=\"hot\", interpolation=\"nearest\", vmin=color_range[0], vmax=color_range[1])\n    ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.grid(False)\n    cbar = plt.colorbar(fraction=0.046, pad=0.04, ax=ax)\n    cbar.set_label(\"Embedding value\", rotation=270, labelpad=15)\n", "CodeBleu_score": 52.515372088989274}]}
{"instance_id": "92e93d50b62b", "generation_result": [{"code": "def similarity_matrix(self, embeds)\n    # get the number of speakers and utterances per speaker\n    batch_size, num_speakers, num_utterances, _ = embeds.shape\n    # initialize the similarity matrix and the mask matrix\n    sim_mat = torch.zeros((batch_size, num_speakers, num_speakers), device=self.device)\n    mask = torch.ones((batch_size, num_speakers, num_speakers), device=self.device)\n    # calculate the centroids\n    centroids_incl = torch.mean(embeds, dim=2)\n    centroids_incl = F.normalize(centroids_incl, p=2, dim=2)\n    centroids_excl = torch.mean(embeds, dim=2, keepdim=True)\n    centroids_excl = F.normalize(centroids_excl, p=2, dim=2)\n    # calculate the similarity matrix\n    for i in range(num_speakers):\n      for j in range(num_speakers):\n        if i!= j:\n          sim_mat[:, i, j] = self.sim_func(embeds[:, i, :, :], centroids_excl[:, i, :])[:, 0]\n        else:\n          mask[:, i, j] = 0.0\n    # scale and bias the similarity matrix\n    sim_mat = sim_mat * self.scale\n    sim_mat = sim_mat + self.bias\n    return sim_mat * mask\n", "CodeBleu_score": 24.71602395814131}, {"code": "def similarity_matrix(self, embeds)\n    num_speakers = embeds.shape[0]\n    num_utterances = embeds.shape[1]\n    centroids_incl = torch.zeros((num_speakers, self.embedding_size), device=self.device)\n    centroids_excl = torch.zeros((num_speakers, self.embedding_size), device=self.device)\n    mask = torch.ones((num_speakers, num_speakers), device=self.device)\n    for i in range(num_speakers):\n      speaker_embeds = embeds[i, :, :]\n      centroids_incl[i, :] = torch.sum(speaker_embeds, dim=0) / (num_utterances * self.embedding_size)\n      centroids_excl[i, :] = torch.sum(speaker_embeds, dim=0) / (num_utterances - 1 * self.embedding_size)\n      for j in range(i + 1, num_speakers):\n        centroid_incl = centroids_incl[i, :]\n        centroid_excl = centroids_excl[i, :]\n        similarity = torch.dot(centroid_incl, centroid_excl)\n        mask[i, j] = 0\n        mask[j, i] = 0\n        similarity_matrix[i, j] = similarity\n    similarity_matrix = similarity_matrix / (torch.max(similarity_matrix) + 1e-6)\n    similarity_matrix = similarity_matrix * self.similarity_matrix_scale + self.similarity_matrix_bias\n    return similarity_matrix, mask\n\n  def forward(self, embeds):\n    \"\"\"The function \\\"forward\\\" computes the speaker embeddings and similarity matrix from the input embeddings. It then computes the speaker embeddings using a softmax function. The function returns the speaker embeddings and the similarity matrix.\"\"\"\n    similarity_matrix, mask = self.similarity_matrix(embeds)\n    speaker_embeddings = torch.nn.functional.softmax(similarity_matrix, dim=1)\n    return speaker_embeddings, similarity_matrix, mask\n", "CodeBleu_score": 28.57504599132427}, {"code": "def similarity_matrix(self, embeds)\n    # get batch size, number of utterances per speaker, and number of speakers per batch\n    batch_size, num_utts_per_spk, num_spks_per_batch = embeds.shape\n    # initialize similarity matrix and mask matrix\n    similarity_matrix = torch.zeros((batch_size, num_spks_per_batch, num_spks_per_batch)).to(embeds.device)\n    mask_matrix = torch.ones_like(similarity_matrix)\n    # get the indices of the diagonal\n    diag_indices = torch.arange(num_spks_per_batch).to(embeds.device)\n    # set the diagonal to zero\n    mask_matrix[:, diag_indices, diag_indices] = 0\n    # compute the centroids\n    centroids_incl = torch.mean(embeds, dim=1)\n    centroids_excl = torch.mean(embeds * mask_matrix, dim=1)\n    # compute the similarity scores\n    similarity_scores_incl = torch.matmul(embeds, centroids_incl.transpose(0, 1))\n    similarity_scores_excl = torch.matmul(embeds, centroids_excl.transpose(0, 1))\n    # update the similarity matrix\n    similarity_matrix[:, diag_indices, diag_indices] = 1\n    similarity_matrix = similarity_matrix + similarity_scores_incl - similarity_scores_excl\n    # scale and bias the similarity matrix\n    similarity_matrix = similarity_matrix * self.scale + self.bias\n    return similarity_matrix\n", "CodeBleu_score": 16.523492505105153}]}
{"instance_id": "a6eba54108a1", "generation_result": [{"code": "def loss(self, embeds)\n\n    # Get number of speakers and utterances per speaker from the shape of the input embeddings\n    num_speakers, num_utts = embeds.shape[0], embeds.shape[1]\n\n    # Compute the similarity matrix\n    similarity_matrix = self.compute_similarity_matrix(embeds)\n\n    # Reshape the similarity matrix\n    similarity_matrix = similarity_matrix.reshape(num_speakers, num_speakers * num_utts)\n\n    # Generate ground truth labels\n    labels = np.repeat(np.arange(num_speakers), num_utts)\n\n    # Compute the loss\n    loss = self.compute_loss(similarity_matrix, labels)\n\n    # Compute the Equal Error Rate (EER)\n    eer = self.compute_eer(similarity_matrix, labels)\n\n    return loss, eer\n\n", "CodeBleu_score": 8.280895688633949}, {"code": "def loss(self, embeds)\n    # Get the number of utterances and speakers\n    num_utts, num_spks = embeds.shape[0], embeds.shape[1]\n    # Create a similarity matrix\n    sim_matrix = torch.matmul(embeds, embeds.T)\n    # Reshape the similarity matrix\n    sim_matrix = sim_matrix.reshape(num_spks, num_spks, num_utts)\n    # Create ground truth labels\n    labels = torch.arange(num_spks).repeat(num_utts)\n    # Calculate the loss\n    loss = self.criterion(sim_matrix, labels)\n    # Calculate the EER\n    eer = self.compute_eer(sim_matrix, labels)\n    # Return the loss and EER\n    return loss, eer\n\n  def compute_eer(self, sim_matrix, labels):\n    \"\"\"```The function computes the Equal Error Rate (EER) for a batch of speaker embeddings. It first computes the ROC curve and then finds the threshold where the false positive rate equals the false negative rate. It returns the EER.```\"\"\"\n    # Get the number of utterances and speakers\n    num_utts, num_spks = sim_matrix.shape[2], sim_matrix.shape[0]\n    # Create a list of all the thresholds\n    thresholds = torch.arange(num_utts) / float(num_utts)\n    # Create a list of false positive rates\n    fprs = torch.zeros(num_utts)\n    # Create a list of false negative rates\n    fprs = torch.zeros(num_utts)\n    # Loop through thresholds\n    for threshold in thresholds:\n      # Get the predicted labels\n      preds = torch.argmax(sim_matrix, dim=1)\n      # Get the true labels\n      truths = labels\n      # Get the false positive rate\n      fpr = torch.sum(torch.where(preds!= truths, torch.ones(num_utts), torch.zeros(num_utts)) * (preds!= truths).float()) / float(num_utts)\n      # Get the false negative rate\n      fnr = torch.sum(torch.where(preds!= truths, torch.ones(num_utts), torch.zeros(num_utts)) * (preds == truths).float()) / float(num_utts)\n      # Append", "CodeBleu_score": 27.826982648565256}, {"code": "def loss(self, embeds)\n\n    # Calculate the number of speakers and utterances per speaker\n    num_speakers = embeds.shape[0]\n    num_utts_per_speaker = embeds.shape[1]\n\n    # Calculate the similarity matrix\n    sim_matrix = self.cosine_similarity(embeds)\n\n    # Reshape the similarity matrix\n    sim_matrix = sim_matrix.reshape(num_speakers*num_utts_per_speaker, num_speakers)\n\n    # Generate ground truth labels\n    labels = np.repeat(np.arange(num_speakers), num_utts_per_speaker)\n\n    # Calculate the loss\n    loss = self.loss_fn(sim_matrix, torch.from_numpy(labels).long().to(self.device))\n\n    # Calculate the EER\n    eer = self.calculate_eer(sim_matrix, labels)\n\n    return loss, eer\n\n  def calculate_eer(self, sim_matrix, labels):\n    \"\"\"```The function calculates the Equal Error Rate (EER) for a batch of speaker embeddings. It first determines the number of speakers and utterances per speaker from the shape of the input embeddings. It then computes a similarity matrix and reshapes it. Ground truth labels are generated, and the loss is calculated using these labels and the similarity matrix. The EER is computed without backpropagation by comparing the predicted and true labels using the ROC curve and finding the threshold where the false positive rate equals the false negative rate. The function returns both the loss and the EER.```\"\"\"\n\n    # Calculate the number of speakers and utterances per speaker\n    num_speakers = sim_matrix.shape[1]\n\n    # Calculate the predicted labels\n    predicted_labels = np.argmax(sim_matrix, axis=1)\n\n    # Calculate the false positive rate, true positive rate, and thresholds\n    fpr, tpr, thresholds = roc_curve(labels, predicted_labels)\n\n    # Find the index of the threshold where the false positive rate equals the false negative rate\n    idx = np.argmin(np.abs(fpr - (1-tpr)))\n\n    # Calculate the EER\n    eer = (fpr[idx] + (1-tpr[idx]))/2\n\n    return eer\n\n  def cosine_similarity(self, embeds):\n    \"\"\"```The function calculates the cosine similarity for a batch of speaker embeddings. It first calculates the", "CodeBleu_score": 30.147373904492735}]}
{"instance_id": "ac7b74b9d1b1", "generation_result": [{"code": "def log_params(self)\n    if not self.disable_param_logging:\n        # Import parameter configuration\n        from. import param_config as param_config\n\n        # Create string to be logged\n        param_string = \"Model Parameters:\\n\"\n        for param in self.model_params:\n            param_string += f\"{param}: {self.model_params[param]}\\n\"\n        param_string += \"\\nData Parameters:\\n\"\n        for param in self.data_params:\n            param_string += f\"{param}: {self.data_params[param]}\\n\"\n\n        # Log string\n        self.log_visualization(\n            param_string,\n            title=\"Parameters\",\n            x_label=\"Parameters\",\n            y_label=\"Values\",\n            disable_plot=self.disable_plotting,\n        )\n\n", "CodeBleu_score": 23.49991382156512}, {"code": "def log_params(self)\n    if not self.disable_logging:\n      params = self.params\n      param_string = \"Parameters:\\n\"\n      for param_name, param_value in params.items():\n        param_string += f\"{param_name}: {param_value}\\n\"\n      self.log(param_string)\n\n  def log_data_params(self):\n    \"\"\"The function logs data parameters if not disabled.\\nIt imports parameter configurations, then iterates through the data parameters, appending each name and value to a formatted string.\\nThis string is displayed using a visualization tool, with the title \"Data Parameters\".\"\"\"\n    if not self.disable_logging:\n      params = self.data_params\n      param_string = \"Data Parameters:\\n\"\n      for param_name, param_value in params.items():\n        param_string += f\"{param_name}: {param_value}\\n\"\n      self.log(param_string)\n\n  def log_model_params(self):\n    \"\"\"The function logs model parameters if not disabled.\\nIt imports parameter configurations, then iterates through the model parameters, appending each name and value to a formatted string.\\nThis string is displayed using a visualization tool, with the title \"Model Parameters\".\"\"\"\n    if not self.disable_logging:\n      params = self.model_params\n      param_string = \"Model Parameters:\\n\"\n      for param_name, param_value in params.items():\n        param_string += f\"{param_name}: {param_value}\\n\"\n      self.log(param_string)\n\n  def log_model_summary(self):\n    \"\"\"The function logs a summary of the model architecture if not disabled.\\nIt imports parameter configurations, then displays a summary of the model architecture using the model's \"summary()\" method.\\nThis summary is displayed using a visualization tool, with the title \"Model Summary\".\"\"\"\n    if not self.disable_logging:\n      self.model.summary()\n      self.log(\"Model Summary\")\n\n  def log_data_summary(self):\n    \"\"\"The function logs a summary of the data if not disabled.\\nIt imports parameter configurations, then displays a summary of the data using the data's \"summary()\" method.\\nThis summary is displayed using a visualization tool, with the title \"Data Summary\".\"\"\"\n    if not self.disable_logging:\n      self.data.summary()\n      self.log(\"Data Summary\")\n\n  def", "CodeBleu_score": 22.855926711155046}, {"code": "def log_params(self)\n    if not self.params_disabled:\n        param_string = \"\"\n        for param in self.model_params:\n            param_string += f\"{param}: {self.model_params[param]}\\n\"\n        for param in self.data_params:\n            param_string += f\"{param}: {self.data_params[param]}\\n\"\n        self.log_visual(param_string, title=\"Parameters\")\n\n", "CodeBleu_score": 13.711963020406973}]}
{"instance_id": "4726b7864386", "generation_result": [{"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    if self.speaker_projections is None:\n      self.speaker_projections = umap.UMAP(n_components=2, n_neighbors=10, min_dist=0.1).fit_transform(embeds)\n    else:\n      self.speaker_projections = umap.UMAP(n_components=2, n_neighbors=10, min_dist=0.1).fit_transform(embeds)\n    if out_fpath is not None:\n      plt.figure(figsize=(10, 10))\n      plt.scatter(self.speaker_projections[:, 0], self.speaker_projections[:, 1], c=self.speaker_colors, s=10, alpha=0.5)\n      plt.title('Step:'+ str(step))\n      plt.savefig(out_fpath)\n    else:\n      plt.figure(figsize=(10, 10))\n      plt.scatter(self.speaker_projections[:, 0], self.speaker_projections[:, 1], c=self.speaker_colors, s=10, alpha=0.5)\n      plt.title('Step:'+ str(step))\n      plt.draw()\n      plt.pause(0.0001)\n    plt.clf()\n    plt.cla()\n    plt.close()\n    if len(self.speaker_projections) > max_speakers:\n      self.speaker_projections = self.speaker_projections[0:max_speakers]\n    self.speaker_colors = self.speaker_colors[0:len(self.speaker_projections)]\n    self.speaker_names = self.speaker_names[0:len(self.speaker_projections)]\n    self.speaker_utterances = self.speaker_utterances[0:len(self.speaker_projections)]\n    self.speaker_utterances_per_speaker = self.speaker_utterances_per_speaker[0:len(self.speaker_projections)]\n    self.speaker_projections = self.speaker_projections[0:len(self.speaker_projections)]\n    self.speaker_colors = [self.speaker_colors[i] for i in range(len(self.speaker_projections))]\n    self.speaker_names = [self.speaker_names[", "CodeBleu_score": 37.94404563788552}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    if self.verbose: print('Drawing projections...')\n    if out_fpath is None:\n      out_fpath = self.out_dir / f'projections_{step}.png'\n    if embeds is None:\n      embeds = self.embeds\n    if utterances_per_speaker is None:\n      utterances_per_speaker = self.utterances_per_speaker\n    # Get speaker names and IDs\n    speaker_names = list(self.speakers.keys())\n    speaker_ids = list(self.speakers.values())\n    # Limit number of speakers\n    if len(speaker_names) > max_speakers:\n      speaker_names = speaker_names[:max_speakers]\n      speaker_ids = speaker_ids[:max_speakers]\n    # Flatten embeddings\n    embeds = embeds.reshape(-1, embeds.shape[-1])\n    # Get colors\n    colors = [self.colors[speaker_id] for speaker_id in speaker_ids]\n    # Project embeddings\n    embeds_2d = umap.UMAP(n_components=2, random_state=self.seed).fit_transform(embeds)\n    # Draw plot\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(embeds_2d[:, 0], embeds_2d[:, 1], s=1, c=colors)\n    # Add text labels\n    for i, speaker_name in enumerate(speaker_names):\n      if utterances_per_speaker is not None:\n        text = f'{speaker_name} ({utterances_per_speaker[speaker_name]})'\n      else:\n        text = speaker_name\n      ax.annotate(text, (embeds_2d[i, 0], embeds_2d[i, 1]), fontsize=10)\n    # Set plot title\n    title = f'Projections at step {step}'\n    ax.set_title(title, fontsize=12)\n    # Save or display plot\n    if self.display_plot:\n      plt.show()\n    else:\n      plt.savefig(out_fpath, dpi=300)\n    plt.close()\n    if self.verbose: print(f'Saved projections plot to \"{out_fpath}\"')\n\n  def draw_utterances(self, utterances,", "CodeBleu_score": 37.15575921804854}, {"code": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)\n    if self.plot_projections:\n      # Get the number of speakers\n      num_speakers = min(utterances_per_speaker.shape[0], max_speakers)\n      # Flatten the embeddings\n      flattened_embeds = embeds.reshape(num_speakers, -1)\n      # Get the speaker colors\n      speaker_colors = np.array(sns.color_palette(\"hls\", num_speakers))\n      # Project the embeddings\n      projected_embeds = umap.UMAP(n_components=2, random_state=42).fit_transform(flattened_embeds)\n      # Create the plot\n      fig, ax = plt.subplots(figsize=(12, 12))\n      # Plot the projected embeddings\n      ax.scatter(projected_embeds[:, 0], projected_embeds[:, 1], c=speaker_colors, s=100)\n      # Set the plot title\n      ax.set_title(f\"Step {step} - UMAP Projection of Speaker Embeddings\")\n      # Set the axes labels\n      ax.set_xlabel(\"UMAP Dimension 1\")\n      ax.set_ylabel(\"UMAP Dimension 2\")\n      # Save the plot\n      if out_fpath is not None:\n        plt.savefig(out_fpath)\n      # Update the plot\n      if self.show_plots:\n        plt.show()\n      # Clear the plot\n      plt.clf()\n      plt.close()\n", "CodeBleu_score": 45.05831193607276}]}
{"instance_id": "274fa50985c0", "generation_result": [{"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    # Get the human-readable names of the dimensions of the input tensor\n    input_shape = x.shape\n    input_shape_names = [dim.name for dim in input_shape]\n\n    # Determine the appropriate dimension index for the speaker embedding\n    speaker_embedding_dim_index = None\n    for i, name in enumerate(input_shape_names):\n        if name.startswith(\"speaker_embedding\"):\n            speaker_embedding_dim_index = i\n\n    # Repeat the speaker embedding to match the length of the input text\n    speaker_embedding = tf.repeat(speaker_embedding, input_shape[1], axis=0)\n\n    # Reshape the speaker embedding to match the shape of the input tensor\n    speaker_embedding = tf.reshape(speaker_embedding, [input_shape[0], input_shape[1], -1])\n\n    # Transpose the speaker embedding to match the input tensor\n    speaker_embedding = tf.transpose(speaker_embedding, [0, 2, 1])\n\n    # Concatenate the input tensor and the speaker embedding along the last dimension\n    x = tf.concat([x, speaker_embedding], axis=2)\n\n    return x\n", "CodeBleu_score": 30.2962246738424}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    x_shape = x.shape\n    x_dims = x_shape.as_list()\n    if len(x_dims)!= 3:\n        raise ValueError(\"The input tensor must be 3-dimensional.\")\n    if x_dims[1]!= self.text_dim:\n        raise ValueError(\"The second dimension of the input tensor must be equal to the text dimension.\")\n    if x_dims[2]!= self.num_layers:\n        raise ValueError(\"The third dimension of the input tensor must be equal to the number of layers.\")\n    if speaker_embedding.shape[0]!= x_dims[0]:\n        raise ValueError(\"The first dimension of the speaker embedding must be equal to the first dimension of the input tensor.\")\n    speaker_embedding_dim = speaker_embedding.shape[1]\n    if speaker_embedding_dim!= self.speaker_embedding_dim:\n        raise ValueError(\"The second dimension of the speaker embedding must be equal to the speaker embedding dimension.\")\n    speaker_embedding = tf.expand_dims(speaker_embedding, 1)\n    speaker_embedding = tf.expand_dims(speaker_embedding, 1)\n    speaker_embedding = tf.tile(speaker_embedding, [1, x_dims[1], x_dims[2]])\n    speaker_embedding = tf.reshape(speaker_embedding, [x_dims[0], x_dims[1], x_dims[2], speaker_embedding_dim])\n    speaker_embedding = tf.transpose(speaker_embedding, [0, 2, 1, 3])\n    x = tf.concat([x, speaker_embedding], axis=-1)\n    return x\n\n  def add_speaker_embedding_and_normalize(self, x, speaker_embedding):\n    \"\"\"The function adds a speaker embedding to an input tensor and normalizes the result.\\nIt first calls the add_speaker_embedding function and then normalizes the result.\"\"\"\n    x = self.add_speaker_embedding(x, speaker_embedding)\n    x = self.normalize(x)\n    return x\n\n  def call(self, x, speaker_embedding):\n    \"\"\"The function applies the model to an input tensor and returns the result.\"\"\"\n    x = self.add_speaker_embedding_and_normalize(x, speaker_embedding)\n    for i in range(self.num_layers):\n      x = self.layers[i](x)\n   ", "CodeBleu_score": 27.047434043168533}, {"code": "def add_speaker_embedding(self, x, speaker_embedding)\n    # Save the dimensions of the input tensor\n    original_shape = x.shape\n    # Get the index of the last dimension\n    last_dimension_index = len(original_shape) - 1\n    # Repeat the speaker embedding to match the length of the input text\n    repeated_speaker_embedding = tf.repeat(speaker_embedding, repeats=tf.shape(x)[1], axis=1)\n    # Reshape the speaker embedding to match the shape of the input text\n    reshaped_speaker_embedding = tf.reshape(repeated_speaker_embedding, shape=tf.shape(x))\n    # Transpose the speaker embedding to match the input tensor\n    transposed_speaker_embedding = tf.transpose(reshaped_speaker_embedding, perm=[0, last_dimension_index])\n    # Concatenate the speaker embedding with the input tensor\n    speaker_embedding_added = tf.concat([x, transposed_speaker_embedding], axis=last_dimension_index)\n    return speaker_embedding_added\n", "CodeBleu_score": 21.33002246011491}]}
{"instance_id": "e6742ea0fe5e", "generation_result": [{"code": "def init_attention(self, encoder_seq_proj)\n    device = next(self.parameters()).device\n    self.attention_weights = torch.zeros(self.batch_size, self.seq_len, device=device)\n    self.attention_weights_cum = torch.zeros(self.batch_size, self.seq_len, device=device)\n    self.encoder_seq_proj = encoder_seq_proj\n", "CodeBleu_score": 21.011720938319346}, {"code": "def init_attention(self, encoder_seq_proj)\n    device = next(self.parameters()).device\n    self.attention_cum_weights = torch.zeros(\n        (self.batch_size, self.max_seq_len), device=device\n    )\n    self.attention_weights = torch.zeros(\n        (self.batch_size, self.max_seq_len), device=device\n    )\n    self.attention_seq_proj = encoder_seq_proj.to(device)\n\n", "CodeBleu_score": 24.487281426187057}, {"code": "def init_attention(self, encoder_seq_proj)\n    device = next(self.parameters()).device\n    self.cumulative_attention = torch.zeros(\n        (encoder_seq_proj.shape[0], encoder_seq_proj.shape[1]), device=device\n    )\n    self.attention = torch.zeros(\n        (encoder_seq_proj.shape[0], encoder_seq_proj.shape[1]), device=device\n    )\n\n", "CodeBleu_score": 26.092831211937582}]}
{"instance_id": "4e1fc1ed0ed7", "generation_result": [{"code": "def load(self, path, optimizer=None)\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint['epoch']\n", "CodeBleu_score": 34.58166661267153}, {"code": "def load(self, path, optimizer=None)\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint['state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer'])\n", "CodeBleu_score": 31.156237886154877}, {"code": "def load(self, path, optimizer=None)\n    checkpoint = torch.load(path, map_location=self.device)\n    self.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n", "CodeBleu_score": 32.71205665954843}]}
{"instance_id": "178e6aa16c0f", "generation_result": [{"code": "def save(self, path, optimizer=None)\n    if not path.endswith('.pt'):\n      path += '.pt'\n    if optimizer is not None:\n      state = {'model': self.state_dict(), 'optimizer': optimizer.state_dict()}\n    else:\n      state = {'model': self.state_dict()}\n    torch.save(state, path)\n\n  def load(self, path, optimizer=None):\n    \"\"\"The function loads the model state from a specified path.\\nIf an optimizer is provided, it loads both the model state and the optimizer state.\\nIf no optimizer is provided, it loads only the model state.\"\"\"\n    if not path.endswith('.pt'):\n      path += '.pt'\n    state = torch.load(path)\n    self.load_state_dict(state['model'])\n    if optimizer is not None:\n      optimizer.load_state_dict(state['optimizer'])\n", "CodeBleu_score": 44.200147015527456}, {"code": "def save(self, path, optimizer=None)\n    if optimizer is None:\n        torch.save(self.state_dict(), path)\n    else:\n        torch.save({\n           'model_state_dict': self.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n        }, path)\n\n", "CodeBleu_score": 57.964199270871944}, {"code": "def save(self, path, optimizer=None)\n    if optimizer is not None:\n      torch.save({\n       'model_state_dict': self.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n      }, path)\n    else:\n      torch.save({\n       'model_state_dict': self.state_dict()\n      }, path)\n", "CodeBleu_score": 66.29121176941072}]}
{"instance_id": "6528fd80e87b", "generation_result": [{"code": "def english_cleaners(text)\n    text = text.lower()\n    text = re.sub(r\"([,.!?])\", r\" \\1 \", text)\n    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n    text = re.sub(r\"\\s+\", r\" \", text).strip()\n    return text\n", "CodeBleu_score": 28.924178828191344}, {"code": "def english_cleaners(text)\n    text = text.strip().lower()\n    text = re.sub(r'[0-9]+', 'number', text)\n    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n    return text\n\n", "CodeBleu_score": 25.642715108143964}, {"code": "def english_cleaners(text)\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text\n\n", "CodeBleu_score": 55.88611500274403}]}
{"instance_id": "8a4fe08e0985", "generation_result": [{"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id[text_to_sequence.eos])\n    return sequence\n", "CodeBleu_score": 75.6427170191543}, {"code": "def text_to_sequence(text, cleaner_names)\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id[text_to_sequence.eos_token])\n    return sequence\n\n", "CodeBleu_score": 75.3112056036738}, {"code": "def text_to_sequence(text, cleaner_names)\n\n    sequence = []\n\n    # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n\n    # Append EOS token\n    sequence.append(_symbol_to_id['~'])\n    return sequence\n\n", "CodeBleu_score": 78.90464825185359}]}
{"instance_id": "e1c25a37af1f", "generation_result": [{"code": "def sequence_to_text(sequence)\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            symbol = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(symbol) > 1 and symbol[0] == '@':\n                symbol = '{%s}' % symbol[1:]\n            # Concatenate the current symbol to the result:\n            result += symbol\n    return result.replace('}{','')\n\n", "CodeBleu_score": 59.38407240159109}, {"code": "def sequence_to_text(sequence)\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            symbol = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(symbol) > 1 and symbol[0] == '@':\n                symbol = '{%s}' % symbol[1:]\n            # Concatenate the new symbol:\n            result += symbol\n    return result.replace('}{','')\n", "CodeBleu_score": 59.38407240159109}, {"code": "def sequence_to_text(sequence)\n    result = ''\n    for symbol_id in sequence:\n        if symbol_id in _symbol_to_id:\n            symbol = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(symbol) > 1 and symbol[0] == '@':\n                symbol = '{%s}' % symbol[1:]\n            # Concatenate current symbol to result:\n            result += symbol\n    return result\n\n", "CodeBleu_score": 57.082643694898316}]}
{"instance_id": "7807cad4b05d", "generation_result": [{"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    if not os.path.exists(encoder_model_fpath):\n        print(\"The specified encoder model file path does not exist.\")\n        return\n    if not os.path.exists(fpaths[\"utterance_fpath\"]):\n        print(\"The specified utterance file path does not exist.\")\n        return\n    if not os.path.exists(fpaths[\"utterance_embed_fpath\"]):\n        print(\"The specified utterance embedding file path does not exist.\")\n        return\n\n    # Load the encoder model if it is not already loaded.\n    if encoder_model_fpath not in encoder_model_cache:\n        encoder_model = load_encoder_model(encoder_model_fpath)\n        encoder_model_cache[encoder_model_fpath] = encoder_model\n    else:\n        encoder_model = encoder_model_cache[encoder_model_fpath]\n\n    # Load the waveform from the specified file path.\n    waveform, sample_rate = librosa.load(fpaths[\"utterance_fpath\"], sr=16000)\n\n    # Compute the speaker embedding.\n    embedding = compute_embedding(encoder_model, waveform, sample_rate)\n\n    # Save the embedding to the specified file path.\n    np.save(fpaths[\"utterance_embed_fpath\"], embedding)\n", "CodeBleu_score": 29.939978612917855}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    # Load the encoder model if it is not already loaded.\n    if encoder_model is None:\n        encoder_model = load_model(encoder_model_fpath)\n\n    # Load the waveform from the specified file path.\n    wav = load_wav(fpaths[0])\n\n    # Compute the speaker embedding.\n    emb = get_emb(wav, encoder_model)\n\n    # Save the embedding to the specified file path.\n    np.save(fpaths[1], emb)\n\n", "CodeBleu_score": 30.953228635240045}, {"code": "def embed_utterance(fpaths, encoder_model_fpath)\n    # Load the encoder model if it is not already loaded.\n    if encoder_model is None:\n        encoder_model = load_encoder(encoder_model_fpath)\n\n    # Load and preprocess the waveform from the specified file path.\n    waveform = preprocess_wav(fpaths)\n\n    # Compute the speaker embedding.\n    embedding = embed_utterance_with_model(waveform, encoder_model)\n\n    # Save the embedding to the specified file path.\n    save_embedding(embedding, fpaths)\n\n", "CodeBleu_score": 26.72812752570478}]}
{"instance_id": "9f860273a325", "generation_result": [{"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\n    # Verify that the synthesizer_root directory contains the metadata, models, and audio directories.\n    assert (synthesizer_root / \"metadata.json\").exists()\n    assert (synthesizer_root / \"models\").exists()\n    assert (synthesizer_root / \"audio\").exists()\n\n    # Verify that the encoder_model_fpath is a valid file.\n    assert encoder_model_fpath.exists() and encoder_model_fpath.is_file()\n\n    # Create a metadata file for the encoder if one doesn't already exist.\n    encoder_metadata_fpath = synthesizer_root / \"encoder\" / \"metadata.json\"\n    if not encoder_metadata_fpath.exists():\n        encoder_metadata_fpath.parent.mkdir(exist_ok=True, parents=True)\n        shutil.copyfile(synthesizer_root / \"metadata.json\", encoder_metadata_fpath)\n\n    # Load the metadata.\n    metadata = load_metadata(synthesizer_root)\n\n    # Create the directory to store the embeddings.\n    encoder_embeddings_fpath = synthesizer_root / \"encoder\" / \"embeddings\"\n    encoder_embeddings_fpath.mkdir(exist_ok=True, parents=True)\n\n    # Create the path to the temporary file used to pass arguments to the process pool.\n    temp_dir = synthesizer_root / \"temp\"\n    temp_dir.mkdir(exist_ok=True, parents=True)\n    temp_fpath = temp_dir / \"args.pkl\"\n\n    # Save paths to audio and embedding files to temporary file.\n    with open(temp_fpath, \"wb\") as f:\n        cPickle.dump(\n            [\n                encoder_model_fpath,\n                encoder_metadata_fpath,\n                encoder_embeddings_fpath,\n                metadata,\n            ],\n            f,\n        )\n\n    # Start the pool of processes that will generate the embeddings.\n    with ProcessPoolExecutor(max_workers=n_processes) as pool:\n        # Distribute the arguments for the process pool.\n        futures = []\n        for i in range(n_processes):\n            arguments = (\n                encoder_model_fpath,\n                encoder_metadata_fpath,\n                encoder_embeddings_fpath,\n                temp_fpath,\n                i,\n            )\n            futures.append(pool.submit(generate_", "CodeBleu_score": 28.940354221938136}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n    # create embeddings directory if it doesn't exist\n    embeddings_dir = synthesizer_root.joinpath(\"embeddings\")\n    if not os.path.isdir(embeddings_dir):\n        os.makedirs(embeddings_dir)\n\n    # verify that encoder model file exists\n    if not os.path.isfile(encoder_model_fpath):\n        raise Exception(\"Encoder model file does not exist {}\".format(encoder_model_fpath))\n\n    # verify that synthesizer_root/train.txt exists\n    train_txt = synthesizer_root.joinpath(\"train.txt\")\n    if not os.path.isfile(train_txt):\n        raise Exception(\"train.txt file does not exist {}\".format(train_txt))\n\n    # verify that synthesizer_root/validation.txt exists\n    val_txt = synthesizer_root.joinpath(\"validation.txt\")\n    if not os.path.isfile(val_txt):\n        raise Exception(\"validation.txt file does not exist {}\".format(val_txt))\n\n    # verify that synthesizer_root/test.txt exists\n    test_txt = synthesizer_root.joinpath(\"test.txt\")\n    if not os.path.isfile(test_txt):\n        raise Exception(\"test.txt file does not exist {}\".format(test_txt))\n\n    # read metadata\n    print(\"Reading metadata...\")\n    train_meta = pd.read_csv(train_txt, header=None, delim_whitespace=True)\n    val_meta = pd.read_csv(val_txt, header=None, delim_whitespace=True)\n    test_meta = pd.read_csv(test_txt, header=None, delim_whitespace=True)\n\n    # get audio file paths\n    print(\"Getting audio file paths...\")\n    train_files = [synthesizer_root.joinpath(x[0]) for x in train_meta.values]\n    val_files = [synthesizer_root.joinpath(x[0]) for x in val_meta.values]\n    test_files = [synthesizer_root.joinpath(x[0]) for x in test_meta.values]\n\n    # get embedding file paths\n    train_embed_files = [embeddings_dir.joinpath(x.name).with_suffix(\".npy\") for x in train_files]\n    val_embed_files = [embeddings_", "CodeBleu_score": 37.21042249747302}, {"code": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)\n\n    # Create a directory to store the embeddings.\n    embedding_dir = synthesizer_root.joinpath(\"embeddings\")\n    embedding_dir.mkdir(exist_ok=True)\n\n    # Create a directory to store the wavs.\n    wav_dir = synthesizer_root.joinpath(\"wavs\")\n    wav_dir.mkdir(exist_ok=True)\n\n    # Verify that the encoder model file exists.\n    if not encoder_model_fpath.exists():\n        raise RuntimeError(f\"Encoder model '{encoder_model_fpath}' does not exist.\")\n\n    # Load the metadata.\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    if not metadata_fpath.exists():\n        raise RuntimeError(f\"Metadata '{metadata_fpath}' does not exist.\")\n\n    metadata_df = pd.read_csv(metadata_fpath, sep=\"|\", header=None, quoting=csv.QUOTE_NONE, encoding=\"utf-8\")\n    metadata_df.columns = [\"path\", \"text\"]\n\n    # Create a mapping from the words to an index in the embeddings matrix.\n    word_to_index = create_word_index(metadata_df.text.values)\n\n    # Create a pool of processes.\n    pool = mp.Pool(processes=n_processes)\n    manager = mp.Manager()\n    return_dict = manager.dict()\n\n    # Create a progress bar.\n    bar = tqdm(total=len(metadata_df.index))\n\n    # Function for the progress bar callback.\n    def callback(arg):\n        bar.update(1)\n        return_dict[arg[\"name\"]] = arg[\"result\"]\n\n    # Generate the embeddings.\n    for i, row in metadata_df.iterrows():\n        wav_fpath = wav_dir.joinpath(f\"{i}.wav\")\n        embedding_fpath = embedding_dir.joinpath(f\"{i}.npy\")\n\n        # Check to see if the embedding already exists.\n        if embedding_fpath.exists():\n            continue\n\n        # Create the audio file.\n        if not wav_fpath.exists():\n            text = row[\"text\"]\n            audio = synthesizer.preprocess_audio(text, preprocess_config)\n            audio.export(wav_fpath, format=\"wav\")\n\n        # Create the embedding.\n        pool.apply_async(create_embedding, args=(encoder", "CodeBleu_score": 39.500335017998324}]}
{"instance_id": "9eb0abda14a9", "generation_result": [{"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    # Plot attention\n    plot_attention(attention, input_seq, step, plot_dir)\n    # Plot predicted mel spectrogram\n    plot_spectrogram(mel_prediction, step, mel_output_dir, \"Predicted Mel Spectrogram\")\n    # Plot target mel spectrogram\n    plot_spectrogram(target_spectrogram, step, mel_output_dir, \"Target Mel Spectrogram\")\n    # Plot reconstructed wav from predicted mel spectrogram\n    reconstructed_wav = inv_linear_spectrogram(mel_prediction.T, hparams)\n    plot_wav(reconstructed_wav, step, wav_dir, \"Reconstructed Wavform\")\n    # Plot comparison mel spectrogram\n    plot_spectrogram_comparison(target_spectrogram, mel_prediction, step,\n                                mel_output_dir, \"Comparison Mel Spectrogram\")\n    # Print input sequence\n    print(\"Input sequence at step {}:\".format(step))\n    print(input_seq)\n    # Print loss\n    print(\"Step {} Loss: {}\".format(step, loss))\n\n", "CodeBleu_score": 33.25402426742523}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    # Save the attention plot\n    plot_attention(attention, input_seq, step, plot_dir)\n\n    # Save the predicted mel spectrogram to disk\n    mel_filename = \"mel-prediction-%d-%d.npy\" % (step, sample_num)\n    mel_path = os.path.join(mel_output_dir, mel_filename)\n    np.save(mel_path, mel_prediction, allow_pickle=False)\n\n    # Save the target mel spectrogram to disk\n    mel_filename = \"mel-target-%d-%d.npy\" % (step, sample_num)\n    mel_path = os.path.join(mel_output_dir, mel_filename)\n    np.save(mel_path, target_spectrogram, allow_pickle=False)\n\n    # Save the predicted wav to disk\n    wav_filename = \"wav-prediction-%d-%d.wav\" % (step, sample_num)\n    wav_path = os.path.join(wav_dir, wav_filename)\n    save_wav(wav_path, mel_prediction, hparams)\n\n    # Save the target wav to disk\n    wav_filename = \"wav-target-%d-%d.wav\" % (step, sample_num)\n    wav_path = os.path.join(wav_dir, wav_filename)\n    save_wav(wav_path, target_spectrogram, hparams)\n\n    # Save the plot to disk\n    plot_filename = \"plot-%d-%d.png\" % (step, sample_num)\n    plot_path = os.path.join(plot_dir, plot_filename)\n    save_plot(plot_path, input_seq, mel_prediction, target_spectrogram, loss, hparams)\n\n    # Print the input sequence\n    print(\"\\nInput at step %d-%d:\" % (step, sample_num))\n    print(sequence_to_text(input_seq))\n\n", "CodeBleu_score": 37.38791187729061}, {"code": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)\n    # Plot attention alignment\n    plot_alignment(attention, input_seq, step, plot_dir, hparams)\n\n    # Plot predicted mel spectrogram\n    plot_spectrogram(mel_prediction, step, mel_output_dir, hparams)\n\n    # Save predicted mel spectrogram to disk\n    mel_prediction_filename ='mel-prediction-%05d.npy' % step\n    np.save(os.path.join(mel_output_dir, mel_prediction_filename), mel_prediction, allow_pickle=False)\n\n    # Save griffin lim inverted wav for debug\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav = wav[:audio.find_endpoint(wav)]\n    wav_filename ='step-%05d-waveplot.wav' % step\n    audio.save_wav(wav, os.path.join(wav_dir, wav_filename), sr=hparams.sample_rate)\n\n    # Save real and predicted mel-spectrogram plot to disk (Warning: this is quite slow)\n    plot_filename ='step-%05d-mel-prediction-vs-target.png' % step\n    plot_spectrogram_to_numpy(mel_prediction, target_spectrogram, plot_dir, plot_filename, hparams)\n\n    # Print some model information for debugging\n    print('Input at step %d: %s' % (step, sequence_to_text(input_seq)))\n    print('Loss at step %d: %f' % (step, loss))\n", "CodeBleu_score": 38.58146625363794}]}
{"instance_id": "6bad4f500286", "generation_result": [{"code": "def num_params(self)\n    model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    print('Trainable parameters: %.3fM' % (params / 1e6))\n", "CodeBleu_score": 39.44487323594}, {"code": "def num_params(self)\n    trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n    params = sum([np.prod(p.size()) for p in trainable_params])\n    print('Trainable parameters: %.3fM' % (params / 1e6))\n", "CodeBleu_score": 38.1781673417281}, {"code": "def num_params(self)\n    num_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n    print(f'The number of trainable parameters is {num_params/1000000:.3f}M')\n", "CodeBleu_score": 14.565785872443508}]}
{"instance_id": "78b876054bd4", "generation_result": [{"code": "def pad_tensor(self, x, pad, side='both')\n    if side == 'both':\n        pad = (pad, pad)\n    elif side == 'before':\n        pad = (pad, 0)\n    elif side == 'after':\n        pad = (0, pad)\n    else:\n        raise ValueError('side must be one of \"before\", \"after\", or \"both\"')\n    padded_x = torch.zeros(x.size(0) + pad[0] + pad[1], x.size(1), x.size(2), device=x.device)\n    if x.is_cuda:\n        padded_x = padded_x.cuda()\n    padded_x[pad[0]:pad[0]+x.size(0), :, :] = x\n    return padded_x\n", "CodeBleu_score": 31.628726336353296}, {"code": "def pad_tensor(self, x, pad, side='both')\n    if side == 'before':\n        new_size = (x.shape[0] + pad, x.shape[1], x.shape[2])\n        padded_tensor = torch.zeros(new_size)\n        if x.is_cuda:\n            padded_tensor = padded_tensor.cuda()\n        padded_tensor[pad:, :, :] = x\n    elif side == 'after':\n        new_size = (x.shape[0] + pad, x.shape[1], x.shape[2])\n        padded_tensor = torch.zeros(new_size)\n        if x.is_cuda:\n            padded_tensor = padded_tensor.cuda()\n        padded_tensor[:(x.shape[0] - pad), :, :] = x\n    elif side == 'both':\n        new_size = (x.shape[0] + (pad * 2), x.shape[1], x.shape[2])\n        padded_tensor = torch.zeros(new_size)\n        if x.is_cuda:\n            padded_tensor = padded_tensor.cuda()\n        padded_tensor[pad:(x.shape[0] - pad), :, :] = x\n    return padded_tensor\n\n", "CodeBleu_score": 32.40872981913021}, {"code": "def pad_tensor(self, x, pad, side='both')\n    # Determine the new size of the tensor after padding\n    if side == 'both':\n        new_size = x.size(0) + pad[0] + pad[1]\n    elif side == 'before':\n        new_size = x.size(0) + pad[0]\n    elif side == 'after':\n        new_size = x.size(0) + pad[1]\n    else:\n        raise ValueError('side must be either \"before\", \"after\", or \"both\".')\n\n    # Create a new tensor filled with zeros\n    padded_tensor = torch.zeros(new_size, x.size(1), x.size(2), x.size(3))\n    if x.is_cuda:\n        padded_tensor = padded_tensor.cuda()\n\n    # Place the original tensor into the padded tensor according to the specified side\n    if side == 'both':\n        padded_tensor[pad[0]:pad[0]+x.size(0), :, :, :] = x\n    elif side == 'before':\n        padded_tensor[pad[0]:, :, :, :] = x\n    elif side == 'after':\n        padded_tensor[:pad[1], :, :, :] = x\n    else:\n        raise ValueError('side must be either \"before\", \"after\", or \"both\".')\n\n    return padded_tensor\n", "CodeBleu_score": 30.719769446468938}]}
{"instance_id": "a7ed1715a3df", "generation_result": [{"code": "def save_attention(attn, path)    import matplotlib.pyplot as plt\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.matshow(attn, cmap='viridis')\n\n    fontdict = {'fontsize': 14}\n\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in input_sentence.split(' ')] + ['<eos>'], fontdict=fontdict, rotation=90)\n    ax.set_yticklabels([''] + output_sentence.split(' '), fontdict=fontdict)\n\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    ax.set_xlabel('Heads', fontdict=fontdict)\n    ax.set_ylabel('Timesteps', fontdict=fontdict)\n    plt.savefig(path, format='png')\n    plt.close()\n", "CodeBleu_score": 43.93020149994985}, {"code": "def save_attention(attn, path)    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(10,10))\n    ax.matshow(attn, cmap='bone')\n    ax.tick_params(labelsize=15)\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in attn.src_vocab.itos] + ['<eos>'], rotation=45)\n    ax.set_yticklabels([''] + attn.trg_vocab.itos)\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    plt.savefig(path, dpi=300)\n    plt.close()\n", "CodeBleu_score": 38.96626000922447}, {"code": "def save_attention(attn, path)    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.matshow(attn)\n    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in attn.get_text().split(' ')] + ['<eos>'], rotation=90)\n    ax.set_yticklabels([''] + attn.get_text().split(' '))\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n    plt.savefig(path, bbox_inches='tight')\n    plt.close()\n\n", "CodeBleu_score": 42.40537435949064}]}
{"instance_id": "19375c9f44a2", "generation_result": [{"code": "def plot(array)\n    fig = plt.figure(figsize=(10, 6))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.spines['left'].set_color('blue')\n    ax.spines['bottom'].set_color('blue')\n    ax.spines['right'].set_color('red')\n    ax.spines['top'].set_color('red')\n    ax.tick_params(axis='x', colors='blue', labelsize=12)\n    ax.tick_params(axis='y', colors='red', labelsize=12)\n    ax.set_xlabel('Time', color='blue', fontsize=16)\n    ax.set_ylabel('Value', color='red', fontsize=16)\n    plt.plot(array)\n    plt.show()\n\n", "CodeBleu_score": 36.34582100569194}, {"code": "def plot(array)\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.spines['bottom'].set_color('red')\n    ax.spines['top'].set_color('red')\n    ax.spines['right'].set_color('red')\n    ax.spines['left'].set_color('red')\n    ax.xaxis.label.set_color('red')\n    ax.yaxis.label.set_color('red')\n    ax.tick_params(axis='x', colors='red')\n    ax.tick_params(axis='y', colors='red')\n    plt.xlabel('x', fontsize=20, color='red')\n    plt.ylabel('y', fontsize=20, color='red')\n    plt.tick_params(axis='both', which='major', labelsize=15, colors='red')\n    plt.imshow(array, cmap='gray')\n    plt.show()\n", "CodeBleu_score": 34.38184916333289}, {"code": "def plot(array)\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111)\n    ax.set_xlabel('x', fontsize=15, color='red')\n    ax.set_ylabel('y', fontsize=15, color='red')\n    ax.tick_params(axis='both', which='major', labelsize=15, colors='red')\n    plt.imshow(array)\n    plt.show()\n", "CodeBleu_score": 22.800046236511136}]}
{"instance_id": "ff94c425920c", "generation_result": [{"code": "def write_to_csv(image_name, prediction, confidence)\n    with open(CSV_FILE, 'a') as csv_file:\n        fieldnames = ['image', 'prediction', 'confidence']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if os.stat(CSV_FILE).st_size == 0:\n            writer.writeheader()\n        writer.writerow({'image': image_name, 'prediction': prediction, 'confidence': confidence})\n", "CodeBleu_score": 45.618270561720976}, {"code": "def write_to_csv(image_name, prediction, confidence)\n    with open('predictions.csv', 'a') as csv_file:\n        fieldnames = ['image', 'prediction', 'confidence']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        if csv_file.tell() == 0:\n            writer.writeheader()\n        writer.writerow({'image': image_name, 'prediction': prediction, 'confidence': confidence})\n", "CodeBleu_score": 45.214649739439544}, {"code": "def write_to_csv(image_name, prediction, confidence)\n    data = {'image': image_name, 'prediction': prediction, 'confidence': confidence}\n    with open('predictions.csv', 'a') as f:\n        fieldnames = ['image', 'prediction', 'confidence']\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        if f.tell() == 0:\n            writer.writeheader()\n        writer.writerow(data)\n\n", "CodeBleu_score": 49.76677575765493}]}
{"instance_id": "436161602009", "generation_result": [{"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    if not isinstance(model, torch.nn.Module):\n        raise TypeError(f\"The model must be an instance of torch.nn.Module, not {type(model)}.\")\n    if not isinstance(im, torch.Tensor):\n        raise TypeError(f\"The image must be an instance of torch.Tensor, not {type(im)}.\")\n    if not isinstance(file, str):\n        raise TypeError(f\"The file must be an instance of str, not {type(file)}.\")\n    if not isinstance(optimize, bool):\n        raise TypeError(f\"The optimize must be an instance of bool, not {type(optimize)}.\")\n    if not isinstance(prefix, str):\n        raise TypeError(f\"The prefix must be an instance of str, not {type(prefix)}.\")\n    if not os.path.isfile(file):\n        raise FileNotFoundError(f\"The file {file} does not exist.\")\n    if not os.path.isdir(os.path.dirname(file)):\n        raise NotADirectoryError(f\"The directory {os.path.dirname(file)} does not exist.\")\n    if not os.path.splitext(file)[1] == \".pt\":\n        raise ValueError(f\"The file must be a.pt file, not {os.path.splitext(file)[1]}.\")\n    if not im.ndim == 4:\n        raise ValueError(f\"The image must be a 4D tensor, not {im.ndim}D.\")\n    if not im.shape[0] == 1:\n        raise ValueError(f\"The image must be a batch of size 1, not {im.shape[0]}.\")\n    if not im.shape[1] == 3:\n        raise ValueError(f\"The image must be a batch of 3 channels, not {im.shape[1]}.\")\n    if not im.shape[2] == 224:\n        raise ValueError(f\"The image must be a 224x224 image, not {im.shape[2]}x{im.shape[3]}.\")\n    if not im.shape[3] == 224:\n        raise ValueError(f\"The image must be a 224x224 image, not {im.shape[2]}x{im.shape[3]}.\")\n    if not im.dtype == torch.", "CodeBleu_score": 22.33040147788223}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    start_time = time.time()\n    print(f\"{prefix} starting export with torch {torch.__version__}...\")\n    try:\n        if optimize:\n            # mobile optimized mode\n            model.eval()\n            model.cpu()\n            traced_script_module = torch.jit.trace(model, im)\n            optimized_script_module = optimize_for_mobile(traced_script_module)\n            optimized_script_module._save_for_lite_interpreter(file)\n            print(\n                f\"{prefix} export success, saved as {colorstr('optimized')} TorchScript for mobile\"\n            )\n        else:\n            # standard TorchScript mode\n            traced_script_module = torch.jit.trace(model, im)\n            traced_script_module.save(file)\n            print(f\"{prefix} export success, saved as {colorstr('standard')} TorchScript\")\n    except Exception as e:\n        print(f\"{prefix} export failure: {e}\")\n        return None\n    print(f\"{prefix} export done, saved as {colorstr(file)}\")\n    print(f\"{prefix} export time: {time.time() - start_time:.2f}s\")\n    return file\n\n", "CodeBleu_score": 33.53582404948578}, {"code": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))\n    # Log the export start\n    print(f\"{prefix} starting export with torch {torch.__version__}...\")\n    # Trace the model with a dummy input\n    model = model.to(device)\n    model.eval()\n    example = torch.rand(im.shape)\n    traced_script_module = torch.jit.trace(model, example)\n    # Save the model\n    if optimize:\n        optimized_script_module = optimize_for_mobile(traced_script_module)\n        optimized_script_module._save_for_lite_interpreter(file)\n    else:\n        traced_script_module.save(file)\n    # Log the export end and return the file path\n    print(f\"{prefix} export complete. saved into {file}\")\n    return file\n\n", "CodeBleu_score": 39.075068445095184}]}
{"instance_id": "78335487ac1c", "generation_result": [{"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):test output.\"\"\"\n    if not coremltools_found:\n        raise Exception(\n            \"CoreML export requires coremltools. Please install it with `pip install coremltools` or `pip3 install coremltools`.\"\n        )\n\n    if nms:\n        model = iOSModel(model)\n\n    model.eval()\n\n    if half:\n        model = model.half()\n\n    if int8:\n        model = quantize_model(model)\n\n    traced_model = torch.jit.trace(model, im)\n\n    if half:\n        traced_model = traced_model.half()\n\n    if int8:\n        traced_model = quantize_model(traced_model)\n\n    coreml_model = ct.convert(traced_model, inputs=[ct.ImageType(name=\"image\", shape=im.shape)])\n\n    file = Path(file).with_suffix(\".mlmodel\")\n    coreml_model.save(file)\n\n    print(f\"{prefix} Exported model to {file}\")\n\n    return file, model(im)\n\n", "CodeBleu_score": 19.796583620319367}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):test results.\"\"\"\n    try:\n        import coremltools as ct\n    except ImportError:\n        print(colorstr(\"ERROR:\"), \"coremltools is not installed. Please install it with `pip install coremltools`\")\n        return None, None\n    print(colorstr(prefix), \"Exporting model to CoreML...\")\n    model.eval()\n    if half:\n        model.half()\n    if int8:\n        model = torch.quantization.quantize_dynamic(model, {torch.nn.Conv2d, torch.nn.Linear}, dtype=torch.qint8)\n    traced_model = torch.jit.trace(model, im)\n    mlmodel = ct.convert(traced_model, inputs=[ct.ImageType(name=\"image\", shape=im.shape, scale=1/127, bias=[-1,-1,-1])])\n    if nms:\n        mlmodel = ct.models.neural_network.NeuralNetworkBuilder(mlmodel.spec)\n        mlmodel.add_nms(name=\"nms\", input_name=\"boxes\", output_name=\"nms_out\", nms_threshold=0.5, confidence_threshold=0.0, box_encoding=\"corner-form-priors\", prohibited_classes=[0])\n        mlmodel.spec.description.input[0].shortDescription = \"Input image\"\n        mlmodel.spec.description.output[0].shortDescription = \"NMS boxes\"\n    file = Path(file)\n    if not file.suffix:\n        file = file.with_suffix(\".mlmodel\")\n    mlmodel.save(str(file))\n    print(colorstr(prefix), \"Exported model saved to %s\" % file)\n    if int8:\n        mlmodel = ct.models.MLModel(str(file))\n        results = mlmodel.predict({\"image\": im.numpy()})\n        return file, results\n    return file, None\n", "CodeBleu_score": 19.471127453010006}, {"code": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):__class__.\"\"\"\n    try:\n        import coremltools as ct\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" CoreML conversion requires the coremltools package. Please install it with `pip install coremltools`.\")\n    try:\n        import torch.onnx\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" PyTorch conversion requires the torch package. Please install it with `pip install torch`.\")\n    try:\n        import onnx\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" ONNX conversion requires the onnx package. Please install it with `pip install onnx`.\")\n    try:\n        import onnx_coreml\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" ONNX conversion requires the onnx-coreml package. Please install it with `pip install onnx-coreml`.\")\n    try:\n        import onnxruntime\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" ONNX conversion requires the onnxruntime package. Please install it with `pip install onnxruntime`.\")\n    try:\n        import torch2trt\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" TensorRT conversion requires the torch2trt package. Please install it with `pip install torch2trt`.\")\n    try:\n        import tensorrt as trt\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" TensorRT conversion requires the tensorrt package. Please install it with `pip install tensorrt`.\")\n    try:\n        import pycuda.driver as cuda\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" TensorRT conversion requires the pycuda package. Please install it with `pip install pycuda`.\")\n    try:\n        import pycuda.autoinit\n    except:\n        pass\n    try:\n        import onnx_tensorrt.backend as backend\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" TensorRT conversion requires the onnx-tensorrt package. Please install it with `pip install onnx-tensorrt`.\")\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" TensorRT conversion requires the opencv package. Please install it with `pip install opencv-python`.\")\n    try:\n        import numpy as np\n    except ImportError:\n        raise ImportError(colorstr(\"ERROR:\") + \" TensorRT conversion requires the numpy package", "CodeBleu_score": 11.614669542701114}]}
{"instance_id": "80233cf71580", "generation_result": [{"code": "def generate_individual(input_ranges, individual_length) get_random_individual\"\"\"\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n", "CodeBleu_score": 48.40487222597733}, {"code": "def generate_individual(input_ranges, individual_length) test case.\n\n    Args:\n        input_ranges (list): The list of tuples specifying the bounds of the input.\n        individual_length (int): The length of the individual.\n\n    Returns:\n        list: The generated individual.\n    \"\"\"\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[i][0], input_ranges[i][1]))\n    return individual\n", "CodeBleu_score": 24.804247318898607}, {"code": "def generate_individual(input_ranges, individual_length) test case.\"\"\"\n    individual = []\n    for i in range(individual_length):\n        individual.append(random.uniform(input_ranges[0], input_ranges[1]))\n    return individual\n", "CodeBleu_score": 43.36477304408547}]}
{"instance_id": "5ea979cb4a8e", "generation_result": [{"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == 'discrete':\n      return self.alpha_t[t]\n    elif self.schedule_type == 'linear':\n      return self.alpha_t[t] * t + self.alpha_0\n    elif self.schedule_type == 'cosine':\n      return self.alpha_0 * (np.cos(np.pi * (t - self.T_warmup) / (self.T_max - self.T_warmup)) + 1) / 2\n    else:\n      raise ValueError('Invalid schedule type!')\n\n  def marginal_log_variance_coeff(self, t):\n    \"\"\"The function computes the log of the variance coefficient beta_t for a given continuous-time label t.\\nDepending on the schedule type ('discrete', 'linear', or 'cosine'), it performs different calculations:\\n'interpolate for discrete schedule, use a quadratic expression for linear schedule, and apply a cosine-based formula for the cosine schedule.\\nIt returns the computed log(beta_t).\"\"\"\n    if self.schedule_type == 'discrete':\n      return self.beta_t[t]\n    elif self.schedule_type == 'linear':\n      return self.beta_t[t] * t**2 + self.beta_0 * t + self.beta_1\n    elif self.schedule_type == 'cosine':\n      return self.beta_0 * (np.cos(np.pi * (t - self.T_warmup) / (self.T_max - self.T_warmup)) + 1) / 2 + self.beta_1 * (t - self.T_warmup)\n    else:\n      raise ValueError('Invalid schedule type!')\n\n  def marginal_log_coeff(self, t):\n    \"\"\"The function computes the log of the coefficient alpha_t for a given continuous-time label t.\\nDepending on the schedule type ('discrete', 'linear', or 'cosine'), it performs different calculations:\\n'interpolate for discrete schedule, use a quadratic expression for linear schedule, and apply a cosine-based formula for the cosine schedule.\\nIt returns the computed log(alpha_t).\"\"\"\n    if self.schedule_type == 'discrete':\n      return self.alpha_t[t]\n    elif self.schedule_type == 'linear':\n      return self.alpha_", "CodeBleu_score": 30.14014842503413}, {"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == 'discrete':\n      return tf.math.log(tf.math.abs(self.alpha_t_minus_1) + tf.math.abs(self.alpha_t_minus_2))\n    elif self.schedule_type == 'linear':\n      return tf.math.log(tf.math.abs(self.alpha_t_minus_1) + tf.math.abs(self.alpha_t_minus_2) - 2 * tf.math.abs(self.alpha_t_minus_1) * tf.math.abs(self.alpha_t_minus_2))\n    elif self.schedule_type == 'cosine':\n      return tf.math.log(tf.math.abs(self.alpha_t_minus_1) + tf.math.abs(self.alpha_t_minus_2) - tf.math.abs(self.alpha_t_minus_1) * tf.math.abs(self.alpha_t_minus_2))\n    else:\n      raise ValueError('Unknown schedule type:'+ self.schedule_type)\n\n  def marginal_log_mean_coeff_derivative(self, t):\n    \"\"\"The function computes the derivative of the log of the mean coefficient alpha_t for a given continuous-time label t.\\nDepending on the schedule type ('discrete', 'linear', or 'cosine'), it performs different calculations:\\n'interpolate for discrete schedule, use a quadratic expression for linear schedule, and apply a cosine-based formula for the cosine schedule.\\nIt returns the computed derivative of log(alpha_t).\"\"\"\n    if self.schedule_type == 'discrete':\n      return 1.0 / (tf.math.abs(self.alpha_t_minus_1) + tf.math.abs(self.alpha_t_minus_2))\n    elif self.schedule_type == 'linear':\n      return 1.0 / (tf.math.abs(self.alpha_t_minus_1) + tf.math.abs(self.alpha_t_minus_2) - 2 * tf.math.abs(self.alpha_t_minus_1) * tf.math.abs(self.alpha_t_minus_2))\n    elif self.schedule_type == 'cosine':\n     ", "CodeBleu_score": 26.990393160104002}, {"code": "def marginal_log_mean_coeff(self, t)\n    if self.schedule_type == 'discrete':\n      return np.log(self.alpha_schedule[t])\n    elif self.schedule_type == 'linear':\n      return np.log(self.alpha_schedule[t]) + np.log(self.alpha_schedule[t+1]) - np.log(self.alpha_schedule[t-1])\n    elif self.schedule_type == 'cosine':\n      return np.log(self.alpha_schedule[t]) + np.log(self.alpha_schedule[t+1]) - np.log(self.alpha_schedule[t-1])\n    else:\n      raise ValueError('Schedule type not recognized.')\n\n  def marginal_log_var_coeff(self, t):\n    \"\"\"The function computes the log of the variance coefficient beta_t for a given continuous-time label t.\\nDepending on the schedule type ('discrete', 'linear', or 'cosine'), it performs different calculations:\\n'interpolate for discrete schedule, use a quadratic expression for linear schedule, and apply a cosine-based formula for the cosine schedule.\\nIt returns the computed log(beta_t).\"\"\"\n    if self.schedule_type == 'discrete':\n      return np.log(self.beta_schedule[t])\n    elif self.schedule_type == 'linear':\n      return np.log(self.beta_schedule[t]) + np.log(self.beta_schedule[t+1]) - np.log(self.beta_schedule[t-1])\n    elif self.schedule_type == 'cosine':\n      return np.log(self.beta_schedule[t]) + np.log(self.beta_schedule[t+1]) - np.log(self.beta_schedule[t-1])\n    else:\n      raise ValueError('Schedule type not recognized.')\n\n  def marginal_log_coeff(self, t):\n    \"\"\"The function computes the log of the mean and variance coefficients alpha_t and beta_t for a given continuous-time label t.\\nDepending on the schedule type ('discrete', 'linear', or 'cosine'), it performs different calculations:\\n'interpolate for discrete schedule, use a quadratic expression for linear schedule, and apply a cosine-based formula for the cosine schedule.\\nIt returns the computed log(alpha_t) and log(beta_t).", "CodeBleu_score": 24.771816978081034}]}
{"instance_id": "7da4fae5a278", "generation_result": [{"code": "def inverse_lambda(self, lamb)\n    if self.schedule == 'linear':\n      beta_t = self.beta_min + lamb * (self.beta_max - self.beta_min)\n      t = self.beta_to_t(beta_t)\n    elif self.schedule == 'discrete':\n      t = np.interp(lamb, self.log_alpha, self.t_list)\n    else:\n      log_alpha = self.log_alpha_min + lamb * (self.log_alpha_max - self.log_alpha_min)\n      t = np.arccos(1 - np.exp(-log_alpha)) / np.pi\n    return t\n\n  def inverse_lambda_list(self, lamb_list):\n    \"\"\"The function computes the continuous-time labels t_list in [0, T] from a given list of half-logSNR lambda_t.\\nFor a linear schedule, it calculates t_list using beta values and a temporary variable.\\nFor a discrete schedule, it interpolates log_alpha to find t_list.\\nFor other schedules, it computes log_alpha and uses an arccos function to derive t_list.\"\"\"\n    if self.schedule == 'linear':\n      beta_list = self.beta_min + lamb_list * (self.beta_max - self.beta_min)\n      t_list = self.beta_to_t(beta_list)\n    elif self.schedule == 'discrete':\n      t_list = np.interp(lamb_list, self.log_alpha, self.t_list)\n    else:\n      log_alpha_list = self.log_alpha_min + lamb_list * (self.log_alpha_max - self.log_alpha_min)\n      t_list = np.arccos(1 - np.exp(-log_alpha_list)) / np.pi\n    return t_list\n\n  def beta_to_t(self, beta):\n    \"\"\"The function converts a beta value to a continuous-time label t in [0, T].\\nFor a linear schedule, it uses a temporary variable.\\nFor a discrete schedule, it uses a temporary variable.\\nFor other schedules, it uses a temporary variable.\"\"\"\n    if self.schedule == 'linear':\n      t = (beta - self.beta_min) / (self.beta_max - self.beta_min) * self", "CodeBleu_score": 32.324297996670545}, {"code": "def inverse_lambda(self, lamb)\n    if self.schedule == 'linear':\n      return self.beta_inv(lamb)\n    elif self.schedule == 'discrete':\n      return self.log_alpha_inv(lamb)\n    else:\n      return self.log_alpha_inv(lamb) / np.pi\n", "CodeBleu_score": 10.859106275420615}, {"code": "def inverse_lambda(self, lamb)\n    if self.schedule == 'linear':\n      beta_t = self.beta_schedule(lamb)\n      t = beta_t / (1 - beta_t)\n    elif self.schedule == 'discrete':\n      log_alpha = self.log_alpha_schedule(lamb)\n      t = np.interp(log_alpha, self.log_alpha, self.t)\n    else:\n      log_alpha = self.log_alpha_schedule(lamb)\n      t = np.arccos(np.exp(-log_alpha)) / np.pi\n    return t\n\n  def inverse_lambda_discrete(self, lamb):\n    \"\"\"The function computes the discrete-time label t in [0, T] from a given half-logSNR lambda_t.\\nFor a linear schedule, it calculates t using beta values and a temporary variable.\\nFor a discrete schedule, it interpolates log_alpha to find t.\\nFor other schedules, it computes log_alpha and uses an arccos function to derive t.\"\"\"\n    if self.schedule == 'linear':\n      beta_t = self.beta_schedule(lamb)\n      t = beta_t / (1 - beta_t)\n    elif self.schedule == 'discrete':\n      log_alpha = self.log_alpha_schedule(lamb)\n      t = np.interp(log_alpha, self.log_alpha, self.t)\n    else:\n      log_alpha = self.log_alpha_schedule(lamb)\n      t = np.arccos(np.exp(-log_alpha)) / np.pi\n    return t\n\n  def inverse_lambda_linear(self, lamb):\n    \"\"\"The function computes the linear-time label t in [0, T] from a given half-logSNR lambda_t.\\nFor a linear schedule, it calculates t using beta values and a temporary variable.\\nFor a discrete schedule, it interpolates log_alpha to find t.\\nFor other schedules, it computes log_alpha and uses an arccos function to derive t.\"\"\"\n    if self.schedule == 'linear':\n      beta_t = self.beta_schedule(lamb)\n      t = beta_t / (1 - beta_t)\n    elif self.schedule == 'discrete':\n      log_alpha = self.log_alpha_schedule(lamb)\n      t = np.interp(log_alpha, self.log_alpha, self", "CodeBleu_score": 30.26885786413418}]}
{"instance_id": "a519c853a2b2", "generation_result": [{"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    if cond is None:\n        cond = torch.zeros_like(x)\n    t_continuous = t_continuous.expand(x.shape[0], -1)\n    t_continuous = t_continuous.to(x.device)\n    if model_type == \"mlp\":\n        return mlp_pred_fn(x, t_continuous, cond)\n    elif model_type == \"lstm\":\n        return lstm_pred_fn(x, t_continuous, cond)\n    elif model_type == \"transformer\":\n        return transformer_pred_fn(x, t_continuous, cond)\n    else:\n        raise ValueError(\"Model type must be one of'mlp', 'lstm', or 'transformer'.\")\n\n", "CodeBleu_score": 23.009137400919393}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    if t_continuous is None:\n        t_continuous = tf.zeros_like(x)\n    t_continuous = tf.expand_dims(t_continuous, axis=1)\n    t_continuous = tf.tile(t_continuous, [1, tf.shape(x)[1], 1])\n    t_continuous = tf.cast(t_continuous, tf.float32)\n    t_continuous = tf.math.divide_no_nan(t_continuous, 255)\n    t_continuous = tf.cast(t_continuous, tf.float32)\n    t_continuous = tf.math.subtract(t_continuous, 0.5)\n    t_continuous = tf.math.multiply(t_continuous, 2)\n    t_continuous = tf.math.add(t_continuous, 0.5)\n    t_continuous = tf.cast(t_continuous, tf.float32)\n    t_continuous = tf.math.multiply(t_continuous, 255)\n    t_continuous = tf.cast(t_continuous, tf.float32)\n    t_continuous = tf.math.divide_no_nan(t_continuous, 2)\n    t_continuous = tf.cast(t_continuous, tf.float32)\n    t_continuous = tf.math.add(t_continuous, 0.5)\n    t_continuous = tf.cast(t_continuous, tf.float32)\n    t_continuous = tf.math.multiply(t_continuous, 255)\n    t_continuous = tf.cast(t_continuous, tf.float32)\n    t_continuous = tf.squeeze(t_continuous, axis=1)\n    t_continuous = tf.cast(t_continuous, tf.float32)\n    t_continuous = tf.expand_dims(t_continuous, axis=1)\n    t_continuous = tf.tile(t_continuous, [1, tf.shape(x)[1], 1])\n    t_continuous = tf.cast(t_continuous, tf.float32)\n    x = tf.concat([x, t_continuous], axis=-1)\n    x = tf.cast(x, tf.float32)\n    x = tf.math.subtract(x, 0.5", "CodeBleu_score": 27.328863362612655}, {"code": "def noise_pred_fn(x, t_continuous, cond=None)\n    if model_type == 'continuous':\n        x_expanded = tf.expand_dims(x, axis=0)\n        t_expanded = tf.expand_dims(t_continuous, axis=0)\n        pred_noise = model(x_expanded, t_expanded, cond)\n        pred_noise = tf.squeeze(pred_noise)\n        return pred_noise\n    elif model_type == 'discrete':\n        x_expanded = tf.expand_dims(x, axis=0)\n        t_expanded = tf.expand_dims(t_continuous, axis=0)\n        pred_noise = model(x_expanded, t_expanded, cond)\n        pred_noise = tf.squeeze(pred_noise)\n        pred_noise = tf.math.round(pred_noise)\n        return pred_noise\n    elif model_type == 'hybrid':\n        x_expanded = tf.expand_dims(x, axis=0)\n        t_expanded = tf.expand_dims(t_continuous, axis=0)\n        pred_noise = model(x_expanded, t_expanded, cond)\n        pred_noise = tf.squeeze(pred_noise)\n        pred_noise = tf.math.round(pred_noise)\n        return pred_noise\n    else:\n        raise ValueError(\"Model type must be one of 'continuous', 'discrete', or 'hybrid'\")\n", "CodeBleu_score": 29.443673333777515}]}
{"instance_id": "96a9b7d54823", "generation_result": [{"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    if t is None:\n        t = self.dynamic_thresholding_quantile\n    x = x0.reshape(x0.shape[0], -1)\n    q = np.quantile(np.abs(x), t)\n    x_scale = np.maximum(np.ones(x.shape), q / np.abs(x))\n    x_scale = np.expand_dims(x_scale, axis=1)\n    x_scale = np.repeat(x_scale, x.shape[1], axis=1)\n    x_scale = torch.from_numpy(x_scale).type(torch.float32)\n    x_scale = x_scale.to(self.device)\n    x_scale = torch.clamp(x_scale, 0, 1)\n    x_scale = x_scale.reshape(x0.shape)\n    x_scale = x_scale * x0\n    return x_scale\n\n", "CodeBleu_score": 40.95767054313732}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    # Reshape x0 to compute the quantile of absolute values\n    x0 = tf.reshape(x0, [-1])\n    k = tf.cast(tf.math.ceil(tf.math.divide(tf.size(x0), 2)), tf.int32)\n    x0_abs = tf.sort(tf.math.abs(x0))\n    x0_abs_quantile = tf.gather(x0_abs, k)\n\n    # Determine the scaling factor\n    if t is None:\n      t = tf.math.divide(tf.reduce_max(x0_abs), tf.math.sqrt(tf.cast(tf.size(x0), tf.float32)))\n\n    # Clamp x0 values based on the scaling factor\n    x0_abs_quantile = tf.math.maximum(x0_abs_quantile, t)\n\n    # Rescale x0\n    x0_abs_quantile = tf.math.divide(x0_abs_quantile, tf.reduce_max(x0_abs))\n    x0_abs_quantile = tf.math.multiply(x0_abs_quantile, tf.reduce_max(x0))\n    x0 = tf.math.multiply(x0, x0_abs_quantile)\n\n    return x0\n\n  def _compute_gradients(self, x0, t=None):\n    \"\"\"Computes the gradients of the input tensor x0 with respect to the input tensor x0.\"\"\"\n    # Reshape x0 to compute the quantile of absolute values\n    x0 = tf.reshape(x0, [-1])\n    k = tf.cast(tf.math.ceil(tf.math.divide(tf.size(x0), 2)), tf.int32)\n    x0_abs = tf.sort(tf.math.abs(x0))\n    x0_abs_quantile = tf.gather(x0_abs, k)\n\n    # Determine the scaling factor\n    if t is None:\n      t = tf.math.divide(tf.reduce_max(x0_abs), tf.math.sqrt(tf.cast(tf.size(x0), tf.float32)))\n\n    # Clamp x0 values based on the scaling factor\n    x0_abs_quantile = tf.math.maximum(", "CodeBleu_score": 32.618303869517796}, {"code": "def dynamic_thresholding_fn(self, x0, t=None)\n    if t is None:\n        t = self.dynamic_threshold\n    if t is None:\n        return x0\n    x = x0.view(x0.shape[0], -1)\n    x_abs = torch.abs(x)\n    x_abs_q = torch.quantile(x_abs, t, dim=1)\n    x_abs_q = x_abs_q.view(-1, 1, 1, 1)\n    x_abs_q = x_abs_q.repeat(1, x0.shape[1], x0.shape[2], x0.shape[3])\n    x_abs_q = torch.clamp(x_abs_q, 1e-8, 1e8)\n    x_abs_q = torch.sqrt(x_abs_q)\n    x_abs_q = torch.clamp(x_abs_q, 1e-8, 1e8)\n    x_q = x / x_abs_q\n    x_q = torch.clamp(x_q, -1, 1)\n    return x_q.view(x0.shape)\n\n", "CodeBleu_score": 38.94771541392748}]}
{"instance_id": "59c6ec6d7a6a", "generation_result": [{"code": "def data_prediction_fn(self, x, t)\n    x_abs = tf.abs(x)\n    threshold = self.max_threshold * self.dynamic_ratio\n    threshold = tf.minimum(threshold, self.max_threshold)\n    x_threshold = tf.minimum(x_abs, threshold)\n    x_norm = tf.divide(x_threshold, threshold)\n    return x_norm\n\n", "CodeBleu_score": 14.148226813832027}, {"code": "def data_prediction_fn(self, x, t)\n    return self.threshold_fn(x, self.threshold_ratio, self.max_threshold)\n\n", "CodeBleu_score": 1.9772191653214501}, {"code": "def data_prediction_fn(self, x, t)\n    # Dynamic thresholding\n    # Calculate threshold value\n    threshold_value = self.dynamic_threshold(x, t)\n    # Clamp the input tensor values within the threshold\n    x_clamped = tf.math.minimum(x, threshold_value)\n    # Normalize the clamped values\n    x_norm = tf.math.divide(x_clamped, threshold_value)\n    return x_norm\n", "CodeBleu_score": 4.3214654559112}]}
{"instance_id": "65e3618468a2", "generation_result": [{"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    # Reshape sigma to match the dimensions of the input\n    sigma = sigma_in.view(sigma_in.shape[0], sigma_in.shape[1], 1, 1)\n    # Scale the input\n    input_scaled = input * sigma\n    # Compute the prediction\n    prediction = model(input_scaled, **kwargs)\n    # Adjust the prediction\n    prediction = prediction / sigma\n    return prediction\n", "CodeBleu_score": 26.414432245892634}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    sigma_in = sigma_in.reshape(input.shape)\n    input = input / sigma_in\n    output = model(input, **kwargs)\n    output = output * sigma_in\n    return output\n", "CodeBleu_score": 22.496835037048886}, {"code": "def predict_eps_sigma(model, input, sigma_in, **kwargs)\n    if sigma_in.ndim == 1:\n        sigma_in = sigma_in.reshape((1, -1))\n    if input.ndim == 1:\n        input = input.reshape((1, -1))\n    input = input / sigma_in\n    prediction = model(input, **kwargs)\n    prediction = prediction * sigma_in\n    return prediction\n\n", "CodeBleu_score": 31.009090222628554}]}
{"instance_id": "b35846dfbc66", "generation_result": [{"code": "def cal_intergrand(beta_0, beta_1, taus)\n    alpha = torch.exp(beta_0 + beta_1 * taus)\n    log_alpha = torch.log(alpha)\n    log_alpha.backward(torch.ones_like(log_alpha))\n    grad_log_alpha = taus.grad\n    taus.grad = None\n    return alpha * grad_log_alpha\n", "CodeBleu_score": 13.957716321572855}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n    alpha = torch.exp(taus) * beta_0 + beta_1\n    log_alpha = taus + torch.log(beta_0)\n    grad_log_alpha = torch.ones_like(taus, requires_grad=True)\n    log_alpha.backward(grad_log_alpha)\n    integrand = alpha * grad_log_alpha\n    return integrand\n\n", "CodeBleu_score": 22.014944094146056}, {"code": "def cal_intergrand(beta_0, beta_1, taus)\n    taus_clone = taus.clone()\n    taus_clone.requires_grad = True\n    alpha = (beta_0 + beta_1 * taus_clone).exp()\n    log_alpha = alpha.log()\n    log_alpha.backward(gradient=torch.ones_like(log_alpha))\n    grad_log_alpha = taus_clone.grad\n    integrand = (beta_1 * alpha * grad_log_alpha).sum()\n    return integrand\n", "CodeBleu_score": 34.106621391306255}]}
{"instance_id": "5f3667566a10", "generation_result": [{"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda x, s: np.random.normal(0, s, x.shape)\n    if disable is None:\n        disable = []\n    if len(disable) == 0:\n        disable = [False] * len(sigmas)\n    if callback is None:\n        callback = lambda *args: None\n    x_ = x.copy()\n    for i, sigma in enumerate(sigmas):\n        if not disable[i]:\n            x_ = denoise(model, x_, sigma, **extra_args)\n            callback(x_, sigma, i)\n            x_ += eta * ancestral_step(model, x_, sigma, **extra_args)\n            x_ += noise_sampler(x_, s_noise)\n    return x_\n", "CodeBleu_score": 29.843036719700105}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = dict()\n    if noise_sampler is None:\n        noise_sampler = GaussianNoiseSampler(s_noise)\n    if disable is None:\n        disable = []\n    if 'denoising' not in disable:\n        x = model.denoising(x, **extra_args)\n    for sigma in sigmas:\n        if'step' not in disable:\n            x = model.step(x, sigma, **extra_args)\n        if 'euler' not in disable:\n            x = model.euler(x, sigma, eta, noise_sampler, **extra_args)\n        if callback is not None:\n            callback(x)\n    if 'noise' not in disable:\n        x += noise_sampler.sample(x.shape)\n    return x\n\n", "CodeBleu_score": 29.21994078564214}, {"code": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = lambda size: np.random.normal(0, s_noise, size)\n    if disable is None:\n        disable = []\n    x = x.copy()\n    for sigma in sigmas:\n        x = denoise(model, x, sigma, **extra_args, disable=disable)\n        x_ = x.copy()\n        x_ = model.forward(x_, **extra_args)\n        x_ = x_ + sigma * noise_sampler(x_.shape)\n        x = x + eta * (x_ - x)\n        if callback is not None:\n            callback(x)\n    x = x + s_noise * noise_sampler(x.shape)\n    return x\n\n", "CodeBleu_score": 29.458312274659182}]}
{"instance_id": "6261ded88df2", "generation_result": [{"code": "def linear_multistep_coeff(order, t, i, j)\n    def prod_term(n):\n        if n == j:\n            return 0\n        else:\n            return t[i+n+1] - t[i+n]\n    if order > i:\n        raise ValueError('Order is too high for step index')\n    else:\n        return integrate.quad(prod_term, 0, order)[0]\n", "CodeBleu_score": 26.690097402594947}, {"code": "def linear_multistep_coeff(order, t, i, j)\n    if order > i:\n        raise ValueError(\"The order is too high for the step index.\")\n    else:\n        def nested_function(t):\n            return t**(order-j)\n        return integrate.quad(nested_function, t[i], t[i+1])[0]\n", "CodeBleu_score": 21.99155255544824}, {"code": "def linear_multistep_coeff(order, t, i, j)\n    if order == 1:\n        return (t[i+1] - t[i])/(t[i+1] - t[i-1])\n    elif order == 2:\n        def nested_function(x):\n            return (x - t[i-1])/(t[i+1] - t[i-1])\n        return integrate.quad(nested_function, t[i-1], t[i+1])[0]\n    elif order == 3:\n        def nested_function(x):\n            return (x - t[i-1])/(t[i+1] - t[i-1])\n        return integrate.quad(nested_function, t[i-1], t[i+1])[0] - 1/2*(t[i+1] - t[i-1])*(t[i+1] - t[i])/(t[i+1] - t[i-1])**2\n    elif order == 4:\n        def nested_function(x):\n            return (x - t[i-1])/(t[i+1] - t[i-1])\n        return integrate.quad(nested_function, t[i-1], t[i+1])[0] - 1/6*(t[i+1] - t[i-1])*(t[i+1] - t[i])/(t[i+1] - t[i-1])**2 + 1/24*(t[i+1] - t[i-1])*(t[i+1] - t[i])*(t[i+1] - t[i-1])/(t[i+1] - t[i-1])**3\n    elif order == 5:\n        def nested_function(x):\n            return (x - t[i-1])/(t[i+1] - t[i-1])\n        return integrate.quad(nested_function, t[i-1], t[i+1])[0] - 1/20*(t[i+1] - t[i-1])*(t[i+1] - t[i])/(t[i+1] - t[i-1])**2 + 1/72*(t[i+1] - t[i", "CodeBleu_score": 21.720571266367266}]}
{"instance_id": "4ee025e33ab1", "generation_result": [{"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = NoiseSampler()\n    if disable is None:\n        disable = []\n    if not isinstance(disable, list):\n        disable = [disable]\n    for d in disable:\n        if d not in ['noise','model']:\n            raise ValueError('Unknown disable option: {}'.format(d))\n    if 'noise' not in disable:\n        noise_sampler.reset()\n    if'model' not in disable:\n        model.reset()\n    x_out = x.clone()\n    for sigma in sigmas:\n        x_in = x_out.clone()\n        if sigma > 0 and 'noise' not in disable:\n            x_in = noise_sampler(x_in, sigma)\n        x_out = model(x_in, **extra_args)\n        if callback is not None:\n            callback(x_out)\n    return x_out\n", "CodeBleu_score": 30.657400368410375}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    if extra_args is None:\n        extra_args = {}\n    if noise_sampler is None:\n        noise_sampler = NoiseSampler(model)\n    if disable is None:\n        disable = []\n    for name in disable:\n        if name in extra_args:\n            del extra_args[name]\n    if 'input' not in extra_args:\n        extra_args['input'] = x\n    if 'noise_sampler' not in extra_args:\n        extra_args['noise_sampler'] = noise_sampler\n    for sigma in sigmas:\n        if sigma > 0:\n            extra_args['noise_sampler'].set_scale(sigma)\n        x = model(x, **extra_args)\n        if callback is not None:\n            callback(x)\n    return x\n", "CodeBleu_score": 31.729763428018043}, {"code": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)\n    # initialize extra arguments\n    if extra_args is None:\n        extra_args = {}\n    # initialize noise sampler\n    if noise_sampler is None:\n        noise_sampler = GaussianNoiseSampler()\n    # initialize disable\n    if disable is None:\n        disable = []\n    # initialize input\n    x_sampled = x\n    # iterate over sigmas\n    for sigma in sigmas:\n        # execute callback\n        if callback is not None:\n            callback(x_sampled)\n        # apply model\n        x_sampled = model(x_sampled, **extra_args)\n        # update input\n        x_sampled = noise_sampler.update(x_sampled, sigma, disable=disable)\n    # return final sampled value\n    return x_sampled\n", "CodeBleu_score": 27.63355272051357}]}
{"instance_id": "cb51f91da606", "generation_result": [{"code": "def append_dims(x, target_dims)    dims_to_add = target_dims - x.dim()\n    if dims_to_add < 0:\n        raise ValueError(\"The target number of dimensions is less than the current number of dimensions.\")\n    if x.device.type =='mps':\n        return x.detach().expand(*x.shape, *[1]*dims_to_add)\n    else:\n        return x.expand(*x.shape, *[1]*dims_to_add)\n", "CodeBleu_score": 26.645108666163182}, {"code": "def append_dims(x, target_dims)    if isinstance(target_dims, int):\n        target_dims = [target_dims]\n    assert isinstance(target_dims, list)\n    assert all(isinstance(d, int) for d in target_dims)\n    assert all(d > 0 for d in target_dims)\n    assert x.dim() <= max(target_dims)\n    num_dims_to_append = max(target_dims) - x.dim()\n    if num_dims_to_append > 0:\n        if x.device.type =='mps':\n            x = x.detach().clone()\n        for _ in range(num_dims_to_append):\n            x = x.unsqueeze(-1)\n    return x\n\n", "CodeBleu_score": 33.710171092191885}, {"code": "def append_dims(x, target_dims)    if len(target_dims) > len(x.shape):\n        raise ValueError(f'The target dimensions {target_dims} cannot be smaller than the current dimensions {x.shape}.')\n    if len(target_dims) == len(x.shape):\n        return x.detach().clone()\n    dims_to_append = target_dims - x.shape\n    dims_to_append = tuple(dims_to_append)\n    if x.device.type =='mps':\n        return x.expand(dims_to_append)\n    else:\n        return torch.unsqueeze(x, dim=0).expand(dims_to_append)\n\n", "CodeBleu_score": 39.664541421717125}]}
{"instance_id": "9fac874a3bc7", "generation_result": [{"code": "def download_file(path, url, digest=None)\n    # Make sure the target directory exists.\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    # Download the file if it doesn't already exist.\n    if not os.path.exists(path):\n        with requests.get(url, stream=True) as r:\n            r.raise_for_status()\n            with open(path, 'wb') as f:\n                for chunk in r.iter_content(chunk_size=8192):\n                    f.write(chunk)\n    # Check the file's hash against the provided digest.\n    if digest is not None:\n        sha256 = hashlib.sha256()\n        with open(path, 'rb') as f:\n            while True:\n                data = f.read(8192)\n                if not data:\n                    break\n                sha256.update(data)\n        if sha256.hexdigest()!= digest:\n            raise ValueError(f\"SHA-256 hash of downloaded file {path} does not match the provided digest\")\n    return path\n", "CodeBleu_score": 45.30544736449194}, {"code": "def download_file(path, url, digest=None)\n    if not os.path.exists(path):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        print(\"Downloading\", url, \"to\", path)\n        urllib.request.urlretrieve(url, path)\n        if digest is not None:\n            with open(path, \"rb\") as f:\n                actual_digest = hashlib.sha256(f.read()).hexdigest()\n            if actual_digest!= digest:\n                raise ValueError(\n                    \"SHA-256 hash of downloaded file does not match expected hash\"\n                )\n    return path\n\n", "CodeBleu_score": 45.69282942184568}, {"code": "def download_file(path, url, digest=None)\n    if not os.path.exists(path):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        print(\"Downloading {}...\".format(url))\n        urllib.request.urlretrieve(url, path)\n    else:\n        print(\"Using existing file at {}\".format(path))\n    if digest is not None:\n        print(\"Verifying SHA-256 hash...\")\n        hash = hashlib.sha256()\n        with open(path, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash.update(chunk)\n        if hash.hexdigest()!= digest:\n            raise RuntimeError(\"SHA-256 hash of {} does not match expected value {}\".format(path, digest))\n    return path\n", "CodeBleu_score": 44.240625305287374}]}
{"instance_id": "f23165e897ce", "generation_result": [{"code": "def train_mode(model, mode=True)\n    def _switch_mode(model, mode):\n        if isinstance(model, torch.nn.Module):\n            model.train(mode)\n        elif isinstance(model, (list, tuple)):\n            for m in model:\n                _switch_mode(m, mode)\n        else:\n            raise ValueError(\"model must be an instance of torch.nn.Module, list or tuple\")\n\n    def _restore_mode(model, mode):\n        if isinstance(model, torch.nn.Module):\n            model.train(mode)\n        elif isinstance(model, (list, tuple)):\n            for m in model:\n                _restore_mode(m, mode)\n        else:\n            raise ValueError(\"model must be an instance of torch.nn.Module, list or tuple\")\n\n    original_mode = []\n    if isinstance(model, torch.nn.Module):\n        original_mode.append(model.training)\n        _switch_mode(model, mode)\n    elif isinstance(model, (list, tuple)):\n        for m in model:\n            original_mode.append(m.training)\n            _switch_mode(m, mode)\n    else:\n        raise ValueError(\"model must be an instance of torch.nn.Module, list or tuple\")\n\n    try:\n        yield\n    finally:\n        if isinstance(model, torch.nn.Module):\n            _restore_mode(model, original_mode[0])\n        elif isinstance(model, (list, tuple)):\n            for m, mode in zip(model, original_mode):\n                _restore_mode(m, mode)\n        else:\n            raise ValueError(\"model must be an instance of torch.nn.Module, list or tuple\")\n\n", "CodeBleu_score": 24.37604274953415}, {"code": "def train_mode(model, mode=True)\n    if mode:\n        training_modes = []\n        for module in model.modules():\n            training_modes.append(module.training)\n            module.train(mode=True)\n        yield\n        for module, training in zip(model.modules(), training_modes):\n            module.train(training)\n    else:\n        yield\n\n", "CodeBleu_score": 32.6698012777989}, {"code": "def train_mode(model, mode=True)\n    if mode:\n        training_mode = []\n        for module in model.modules():\n            training_mode.append(module.training)\n            module.train(mode)\n        yield\n        for module, training in zip(model.modules(), training_mode):\n            module.train(training)\n    else:\n        yield\n\n", "CodeBleu_score": 32.817044066068476}]}
{"instance_id": "c6d59c6cb8e1", "generation_result": [{"code": "def ema_update(model, averaged_model, decay)\n    for param, avg_param in zip(model.parameters(), averaged_model.parameters()):\n        if param.requires_grad:\n            avg_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n\n    for buf, avg_buf in zip(model.buffers(), averaged_model.buffers()):\n        if buf is not None:\n            avg_buf.data.copy_(buf.data)\n", "CodeBleu_score": 29.859947590837432}, {"code": "def ema_update(model, averaged_model, decay)\n    for current_param, averaged_param in zip(model.parameters(), averaged_model.parameters()):\n        current_param.data.mul_(decay).add_(averaged_param.data, alpha=1 - decay)\n    for current_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        current_buffer.data.copy_(averaged_buffer.data)\n\n", "CodeBleu_score": 29.927642486832408}, {"code": "def ema_update(model, averaged_model, decay)\n    for current_params, averaged_params in zip(model.parameters(), averaged_model.parameters()):\n        current_params.data.mul_(decay).add_(averaged_params.data, alpha=1 - decay)\n\n    for current_buffer, averaged_buffer in zip(model.buffers(), averaged_model.buffers()):\n        current_buffer.data.copy_(averaged_buffer.data)\n\n", "CodeBleu_score": 30.14073651009257}]}
{"instance_id": "600c39a30798", "generation_result": [{"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    if min_value > max_value:\n        raise ValueError('The minimum value must be less than or equal to the maximum value.')\n    min_value_tensor = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value_tensor = torch.tensor(max_value, device=device, dtype=dtype)\n    min_cdf = torch.log(torch.sigmoid(min_value_tensor - loc) + 1e-10) / scale\n    max_cdf = torch.log(torch.sigmoid(max_value_tensor - loc) + 1e-10) / scale\n    uniform_samples = torch.rand(shape, device=device, dtype=dtype)\n    samples = torch.log(torch.sigmoid(torch.log(uniform_samples + 1e-10) * scale + loc) + 1e-10)\n    samples = torch.min(torch.max(samples, min_cdf), max_cdf)\n    return samples\n\n", "CodeBleu_score": 36.32075629018649}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    if isinstance(shape, int):\n        shape = (shape,)\n    if isinstance(loc, int) or isinstance(loc, float):\n        loc = torch.full(shape, loc, dtype=dtype, device=device)\n    if isinstance(scale, int) or isinstance(scale, float):\n        scale = torch.full(shape, scale, dtype=dtype, device=device)\n    if isinstance(min_value, int) or isinstance(min_value, float):\n        min_value = torch.full(shape, min_value, dtype=dtype, device=device)\n    if isinstance(max_value, int) or isinstance(max_value, float):\n        max_value = torch.full(shape, max_value, dtype=dtype, device=device)\n    min_cdf = logit(min_value, loc=loc, scale=scale)\n    max_cdf = logit(max_value, loc=loc, scale=scale)\n    cdf_range = max_cdf - min_cdf\n    rand_cdf = torch.rand(shape, dtype=dtype, device=device) * cdf_range + min_cdf\n    return torch.exp(logit(rand_cdf, loc=min_cdf, scale=cdf_range))\n\n", "CodeBleu_score": 33.017189197669815}, {"code": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)\n    min_value = torch.tensor(min_value, device=device, dtype=dtype)\n    max_value = torch.tensor(max_value, device=device, dtype=dtype)\n    if min_value.dtype!= dtype or max_value.dtype!= dtype:\n        raise TypeError(\"The min_value and max_value arguments must have the same dtype as the output tensor.\")\n    if min_value.shape!= () or max_value.shape!= ():\n        raise ValueError(\"The min_value and max_value arguments must be scalars.\")\n    if min_value > max_value:\n        raise ValueError(\"The min_value must be less than or equal to the max_value.\")\n    if loc.shape!= () or scale.shape!= ():\n        raise ValueError(\"The loc and scale arguments must be scalars.\")\n    if loc < 0. or scale <= 0.:\n        raise ValueError(\"The loc and scale arguments must be nonnegative and scale must be positive.\")\n    # Compute the CDF values of the min and max values\n    min_cdf = logit(min_value - loc)\n    max_cdf = logit(max_value - loc)\n    # Generate uniform random values in the range [0, 1]\n    uniform = torch.rand(shape, device=device, dtype=dtype)\n    # Transform the uniform random values to the range [min_cdf, max_cdf]\n    uniform = min_cdf + (max_cdf - min_cdf) * uniform\n    # Transform the uniform random values to the range [min_value, max_value]\n    samples = loc + torch.exp(scale * torch.logit(uniform))\n    return samples\n", "CodeBleu_score": 32.17118905836088}]}
{"instance_id": "75e2def2467f", "generation_result": [{"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        if antialias:\n            return nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n                nn.ELU(inplace=True),\n            )\n        else:\n            return nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        if channels is None:\n            raise ValueError(\"The number of channels must be specified for the Snake activation.\")\n        return SnakeBeta(channels, antialias=antialias)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation type {activation} not supported.\")\n\n", "CodeBleu_score": 34.2626490171152}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        activation_module = nn.ELU(inplace=True)\n    elif activation == \"snake\":\n        activation_module = SnakeBeta(channels, antialias=antialias)\n    elif activation == \"none\":\n        activation_module = nn.Identity()\n    else:\n        raise ValueError(\"The activation type must be one of 'elu','snake', or 'none'.\")\n    if antialias:\n        return nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            activation_module,\n        )\n    return activation_module\n\n", "CodeBleu_score": 47.27738139804687}, {"code": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module\n    if activation == \"elu\":\n        if antialias:\n            return nn.Sequential(\n                Activation1d(nn.ELU()),\n            )\n        else:\n            return nn.ELU()\n    elif activation == \"snake\":\n        return SnakeBeta(channels)\n    elif activation == \"none\":\n        return nn.Identity()\n    else:\n        raise ValueError(\"Activation must be one of 'elu','snake', or 'none'.\")\n\n", "CodeBleu_score": 41.105888956554786}]}
{"instance_id": "ef1714c79e51", "generation_result": [{"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    if t.dtype!= tf.float32:\n        t = tf.cast(t, tf.float32)\n    t = tf.reshape(t, [-1, t.shape[-1]])\n    t_rot = tf.einsum('btf,f->btf', t, freqs)\n    t_rot = tf.cast(t_rot, t.dtype)\n    t = tf.concat([t, t_rot], axis = -1)\n    t = tf.reshape(t, tf.shape(t)[:-1].concatenate(t.shape[-1] * 2))\n    return scale * t\n", "CodeBleu_score": 28.398094490962382}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    if t.dtype!= tf.float32:\n        t = tf.cast(t, tf.float32)\n    if len(t.shape) == 3:\n        t = tf.reshape(t, [-1, t.shape[-1]])\n    if len(freqs.shape) == 1:\n        freqs = tf.reshape(freqs, [1, -1])\n    freqs = tf.cast(freqs, tf.float32)\n    freqs = tf.expand_dims(freqs, 0)\n    freqs = tf.tile(freqs, [tf.shape(t)[0], 1])\n    t = tf.cast(t, tf.float32)\n    t = tf.expand_dims(t, 1)\n    t = tf.tile(t, [1, tf.shape(freqs)[1], 1])\n    t = tf.cast(t, tf.float32)\n    t = tf.math.sin(t * freqs)\n    t = tf.cast(t, tf.float32)\n    t = tf.reshape(t, [tf.shape(t)[0], -1])\n    t = t * scale\n    return tf.concat([t, tf.cast(t, tf.float16)], -1)\n", "CodeBleu_score": 33.64444340246756}, {"code": "def apply_rotary_pos_emb(t, freqs, scale = 1)\n    if t.dtype!= torch.float32:\n        t = t.to(torch.float32)\n    if len(t.shape) == 1:\n        t = t.unsqueeze(0)\n    if len(freqs.shape) == 1:\n        freqs = freqs.unsqueeze(0)\n    t = t.reshape(t.shape[0], t.shape[1], 1, 1)\n    freqs = freqs.reshape(1, 1, freqs.shape[0], freqs.shape[1])\n    t = torch.mul(t, freqs)\n    sin_t = torch.sin(t)\n    cos_t = torch.cos(t)\n    sin_t = sin_t.reshape(sin_t.shape[0], sin_t.shape[1], sin_t.shape[2] * sin_t.shape[3])\n    cos_t = cos_t.reshape(cos_t.shape[0], cos_t.shape[1], cos_t.shape[2] * cos_t.shape[3])\n    sin_t = sin_t.permute(0, 2, 1)\n    cos_t = cos_t.permute(0, 2, 1)\n    sin_t = sin_t.reshape(sin_t.shape[0], sin_t.shape[1], 1, 1)\n    cos_t = cos_t.reshape(cos_t.shape[0], cos_t.shape[1], 1, 1)\n    t = torch.cat((sin_t, cos_t), 3)\n    t = t.reshape(t.shape[0], t.shape[1], t.shape[2] * t.shape[3])\n    t = t.permute(0, 2, 1)\n    t = t.reshape(t.shape[0], t.shape[1] * t.shape[2])\n    if scale!= 1:\n        t = t * scale\n    return t\n", "CodeBleu_score": 30.53591953677643}]}
{"instance_id": "a7a5f178c6f6", "generation_result": [{"code": "def idx2vq(self, idx, dim=-1) __without__ any normalization.\n\n    :param idx: The indices.\n    :type idx: torch.Tensor\n    :param dim: The dimension to move the last dimension to.\n    :type dim: int, optional\n    :return: The VQ embeddings.\n    :rtype: torch.Tensor\n    \"\"\"\n    return self.codebook(idx.long()).movedim(1, dim)\n", "CodeBleu_score": 19.388071726737518}, {"code": "def idx2vq(self, idx, dim=-1) __without__ the codebook's embedding dimension.\n    \"\"\"\n    if self.vq_dim is None:\n      raise RuntimeError(\"The codebook is not a vector quantizer\")\n    if dim < 0:\n      dim = dim + self.vq_dim\n    return self.vq_embs[idx].movedim(dim, -1)\n\n  def vq_codebook(self, vq_idx):\n    \"\"\"The function returns the codebook embedding corresponding to the given VQ index.\n    \"\"\"\n    if self.vq_dim is None:\n      raise RuntimeError(\"The codebook is not a vector quantizer\")\n    return self.vq_embs[vq_idx]\n\n  def vq_codebook_size(self):\n    \"\"\"The function returns the number of embeddings in the codebook.\n    \"\"\"\n    if self.vq_dim is None:\n      raise RuntimeError(\"The codebook is not a vector quantizer\")\n    return self.vq_embs.size(0)\n\n  def vq_codebook_indices(self):\n    \"\"\"The function returns the indices of the codebook embeddings.\n    \"\"\"\n    if self.vq_dim is None:\n      raise RuntimeError(\"The codebook is not a vector quantizer\")\n    return self.vq_indices\n\n  def vq_codebook_distances(self):\n    \"\"\"The function returns the distances of the codebook embeddings to the codebook centroids.\n    \"\"\"\n    if self.vq_dim is None:\n      raise RuntimeError(\"The codebook is not a vector quantizer\")\n    return self.vq_distances\n\n  def vq_codebook_indices_and_distances(self):\n    \"\"\"The function returns the indices and distances of the codebook embeddings to the codebook centroids.\n    \"\"\"\n    if self.vq_dim is None:\n      raise RuntimeError(\"The codebook is not a vector quantizer\")\n    return self.vq_indices, self.vq_distances\n\n  def vq_codebook_indices_and_distances_and_distances_to_centroids(self):\n    \"\"\"The function returns the indices and distances of the codebook embeddings to the codebook centroids and the distances of the codebook embeddings to the centroids.\n    \"\"\"\n    if self.vq_dim is None:\n      raise RuntimeError(\"The codebook is not a vector quantizer\")\n    return self.vq_indices, self.vq_distances, self.vq_distances_to_", "CodeBleu_score": 30.60112326279726}, {"code": "def idx2vq(self, idx, dim=-1) get_vq_codebook.\n\n    :param idx: The indices of the embeddings to be retrieved.\n    :type idx: torch.Tensor\n    :param dim: The dimension to move the last dimension to.\n    :type dim: int\n    :return: The retrieved embeddings.\n    :rtype: torch.Tensor\n    \"\"\"\n    return self.vq_embeddings(idx)\n", "CodeBleu_score": 19.85005051142772}]}
{"instance_id": "4104c041fb20", "generation_result": [{"code": "def encode(self, x, quantize=False)\n    if quantize:\n      return self.quantize(x)\n    else:\n      return self.encode_without_quantization(x)\n\n  def decode(self, x):\n    \"\"\"The function decodes the input x using a series of blocks.\"\"\"\n    return self.decode_without_quantization(x)\n\n  def encode_without_quantization(self, x):\n    \"\"\"The function encodes the input x using a series of blocks.\\nIt returns the encoded output without quantization.\"\"\"\n    x = self.conv1(x)\n    x = self.conv2(x)\n    x = self.conv3(x)\n    x = self.conv4(x)\n    x = self.conv5(x)\n    x = self.conv6(x)\n    x = self.conv7(x)\n    x = self.conv8(x)\n    x = self.conv9(x)\n    x = self.conv10(x)\n    x = self.conv11(x)\n    x = self.conv12(x)\n    x = self.conv13(x)\n    x = self.conv14(x)\n    x = self.conv15(x)\n    x = self.conv16(x)\n    x = self.conv17(x)\n    x = self.conv18(x)\n    x = self.conv19(x)\n    x = self.conv20(x)\n    x = self.conv21(x)\n    x = self.conv22(x)\n    x = self.conv23(x)\n    x = self.conv24(x)\n    x = self.conv25(x)\n    x = self.conv26(x)\n    x = self.conv27(x)\n    x = self.conv28(x)\n    x = self.conv29(x)\n    x = self.conv30(x)\n    x = self.conv31(x)\n    x = self.conv32(x)\n    x = self.conv33(x)\n    x = self.conv34(x)\n    x = self.conv35(x)\n    x = self.conv36(x)\n    x = self.conv37(x)\n    x = self.conv38(x", "CodeBleu_score": 26.23821443698158}, {"code": "def encode(self, x, quantize=False)\n    x = self.conv_1(x)\n    x = self.conv_2(x)\n    x = self.conv_3(x)\n    x = self.conv_4(x)\n    x = self.conv_5(x)\n    x = self.conv_6(x)\n    x = self.conv_7(x)\n    x = self.conv_8(x)\n    x = self.conv_9(x)\n    x = self.conv_10(x)\n    x = self.conv_11(x)\n    x = self.conv_12(x)\n    x = self.conv_13(x)\n    x = self.conv_14(x)\n    x = self.conv_15(x)\n    x = self.conv_16(x)\n    x = self.conv_17(x)\n    x = self.conv_18(x)\n    x = self.conv_19(x)\n    x = self.conv_20(x)\n    x = self.conv_21(x)\n    x = self.conv_22(x)\n    x = self.conv_23(x)\n    x = self.conv_24(x)\n    x = self.conv_25(x)\n    x = self.conv_26(x)\n    x = self.conv_27(x)\n    x = self.conv_28(x)\n    x = self.conv_29(x)\n    x = self.conv_30(x)\n    x = self.conv_31(x)\n    x = self.conv_32(x)\n    x = self.conv_33(x)\n    x = self.conv_34(x)\n    x = self.conv_35(x)\n    x = self.conv_36(x)\n    x = self.conv_37(x)\n    x = self.conv_38(x)\n    x = self.conv_39(x)\n    x = self.conv_40(x)\n    x = self.conv_41(x)\n    x = self.conv_42(x)\n    x = self.conv_43(x)\n    x = self.", "CodeBleu_score": 8.731231057332076}, {"code": "def encode(self, x, quantize=False)\n    if quantize:\n      z = self.encoder(x)\n      z_q, z_orig, z_ind = self.vq.encode(z)\n      z_q = z_q.view(z_q.shape[0], -1)\n      loss, quant_loss, perplexity = self.vq.loss(z, z_q.detach())\n      return z_q, z_orig, z_ind, loss, quant_loss, perplexity\n    else:\n      z = self.encoder(x)\n      z = z.view(z.shape[0], -1)\n      return z\n\n  def decode(self, z):\n    \"\"\"The function decodes the input z using a series of blocks.\"\"\"\n    z = z.view(z.shape[0], self.z_dim, 1, 1)\n    return self.decoder(z)\n\n  def forward(self, x, quantize=False):\n    \"\"\"The function encodes the input x using a series of blocks and decodes the encoded output to generate the reconstructed input.\\nIf quantization is enabled, it applies a vector quantizer to the encoded output and returns the quantized encoding, original encoding, indices, and combined loss.\\nIf quantization is not enabled, it returns the encoded output.\"\"\"\n    z = self.encode(x, quantize)\n    if quantize:\n      z, _, _, loss, quant_loss, perplexity = z\n    x_recon = self.decode(z)\n    return x_recon, loss, quant_loss, perplexity\n\n  def loss_function(self, x, x_recon, quant_loss, perplexity, beta):\n    \"\"\"The function computes the loss for a batch of data.\"\"\"\n    recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n    loss = recon_loss + beta * quant_loss + beta * perplexity\n    return loss, recon_loss, quant_loss, perplexity\n\n  def sample(self, num_samples):\n    \"\"\"The function samples num_samples random points from the latent space and returns the decoded samples.\"\"\"\n    z = torch.randn(num_samples, self.z_dim, device=self.device)\n    samples = self.decode(z)\n    return samples\n\n  def generate(self, x):\n    \"\"\"The function generates the reconstructed input for", "CodeBleu_score": 30.345335121954275}]}
{"instance_id": "403c52ba2d3b", "generation_result": [{"code": "def ema_scope(self, context=None)\n    if context is None:\n        return self.ema_scope(context=context)\n    else:\n        return self.ema_scope(context=context)\n\n", "CodeBleu_score": 9.626247148524653}, {"code": "def ema_scope(self, context=None)\n    if self.ema is not None:\n      self.ema.apply_ema_weights(self, context)\n\n  def get_ema_weights(self):\n    \"\"\"Returns the EMA weights as a dictionary of parameter name to weight tensor.\"\"\"\n    return self.ema.get_ema_weights(self)\n\n  def restore_ema_weights(self):\n    \"\"\"Restores the EMA weights from the EMA buffer.\"\"\"\n    return self.ema.restore_ema_weights(self)\n\n  def get_ema_momentum(self):\n    \"\"\"Returns the EMA momentum.\"\"\"\n    return self.ema.get_ema_momentum()\n\n  def set_ema_momentum(self, momentum):\n    \"\"\"Sets the EMA momentum.\"\"\"\n    return self.ema.set_ema_momentum(momentum)\n\n  def get_ema_buffer(self):\n    \"\"\"Returns the EMA buffer as a dictionary of parameter name to buffer tensor.\"\"\"\n    return self.ema.get_ema_buffer(self)\n\n  def get_ema_buffer_value(self):\n    \"\"\"Returns the EMA buffer as a dictionary of parameter name to buffer value.\"\"\"\n    return self.ema.get_ema_buffer_value(self)\n\n  def get_ema_buffer_value_and_weight(self):\n    \"\"\"Returns the EMA buffer as a dictionary of parameter name to (buffer value, weight) tuple.\"\"\"\n    return self.ema.get_ema_buffer_value_and_weight(self)\n\n  def get_ema_buffer_value_and_weight_and_momentum(self):\n    \"\"\"Returns the EMA buffer as a dictionary of parameter name to (buffer value, weight, momentum) tuple.\"\"\"\n    return self.ema.get_ema_buffer_value_and_weight_and_momentum(self)\n\n  def get_ema_buffer_value_and_weight_and_momentum_and_step(self):\n    \"\"\"Returns the EMA buffer as a dictionary of parameter name to (buffer value, weight, momentum, step) tuple.\"\"\"\n    return self.ema.get_ema_buffer_value_and_weight_and_momentum_and_step(self)\n\n  def get_ema_buffer_value_and_weight_and_momentum_and_step_and_time(self):\n    \"\"\"Returns the EMA buffer as a dictionary of parameter name to (buffer value, weight, momentum, step, time) tuple", "CodeBleu_score": 31.922102041507085}, {"code": "def ema_scope(self, context=None)\n    if context is None:\n        context = self\n    with context.ema_scope():\n        yield context\n\n", "CodeBleu_score": 12.918785577112557}]}
{"instance_id": "94b7467f98e4", "generation_result": [{"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x += attn * gate_msa\n    x = x * (1 + self.norm(x))\n    x = x * (1 + self.shift_norm(shift_mlp(x)))\n    x = x * (1 + self.scale_norm(scale_mlp(x)))\n    return x * (1 + self.gate_norm(gate_mlp(x)))\n", "CodeBleu_score": 38.045826907436144}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn * gate_msa\n    x = self.layer_norm(x)\n    x = self.modulate_and_shift(x, shift_mlp, scale_mlp)\n    x = self.layer_norm(x)\n    x = self.mlp(x, gate_mlp)\n    return x\n\n", "CodeBleu_score": 40.36229146344936}, {"code": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)\n    x = x + attn * gate_msa\n    x = x * torch.sigmoid(gate_mlp)\n    x = x * torch.sigmoid(scale_mlp) + torch.tanh(shift_mlp)\n    return x\n", "CodeBleu_score": 34.35271894260149}]}
{"instance_id": "d9fdc05542a2", "generation_result": [{"code": "def _block_mixing(context, x, context_block, x_block, c)\n    # get context block configuration\n    config = context_block.config\n    # get context block hyperparameters\n    block_config = config.block_config\n    # get attention mechanism hyperparameters\n    attention_config = config.attention_config\n    # get attention mechanism\n    attention = config.attention\n    # get attention mechanism type\n    attention_type = config.attention_type\n    # get block hyperparameters\n    block_config = config.block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n    # get block hyperparameters\n    post_block_config = config.post_block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n    # get block hyperparameters\n    post_block_config = config.post_block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n    # get block hyperparameters\n    post_block_config = config.post_block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n    # get block hyperparameters\n    post_block_config = config.post_block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n    # get block hyperparameters\n    post_block_config = config.post_block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n    # get block hyperparameters\n    post_block_config = config.post_block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n    # get block hyperparameters\n    post_block_config = config.post_block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n    # get block hyperparameters\n    post_block_config = config.post_block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n    # get block hyperparameters\n    post_block_config = config.post_block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n    # get block hyperparameters\n    post_block_config = config.post_block_config\n    # get block hyperparameters\n    pre_block_config = config.pre_block_config\n   ", "CodeBleu_score": 9.451100192946713}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n    # pre-attention processing\n    context_block_pre_attn_out = context_block.pre_attention(context)\n    x_block_pre_attn_out = x_block.pre_attention(x)\n\n    # attention\n    q, k, v = _attention(context_block_pre_attn_out, x_block_pre_attn_out, c)\n\n    # post-attention processing\n    context_block_post_attn_out = context_block.post_attention(context, q, k, v)\n    x_block_post_attn_out = x_block.post_attention(x, q, k, v)\n\n    return context_block_post_attn_out, x_block_post_attn_out\n\n", "CodeBleu_score": 13.421790752421131}, {"code": "def _block_mixing(context, x, context_block, x_block, c)\n    # Pre-attention block\n    context_block_pre_attention = context_block.pre_attention\n    x_block_pre_attention = x_block.pre_attention\n\n    # Query, key, and value tensors\n    q = context_block_pre_attention(context)\n    k = x_block_pre_attention(x)\n    v = x_block_pre_attention(x)\n\n    # Intermediate tensors\n    q_context_block = context_block_pre_attention(context)\n    k_x_block = x_block_pre_attention(x)\n    v_x_block = x_block_pre_attention(x)\n\n    # Concatenate qkv tensors\n    qkv = torch.cat((q, k, v), dim=1)\n\n    # Apply optimized attention\n    qkv = _optimized_multi_head_attention(context_block, x_block, qkv, q_context_block, k_x_block, v_x_block, c)\n\n    # Split qkv back into context and x parts\n    q, k, v = torch.split(qkv, context.size(1), dim=1)\n\n    # Post-attention block\n    context_block_post_attention = context_block.post_attention\n    x_block_post_attention = x_block.post_attention\n\n    # Apply post-attention block\n    context = context_block_post_attention(context, q)\n    x = x_block_post_attention(x, v)\n\n    return context, x\n\n", "CodeBleu_score": 20.958780624087446}]}
{"instance_id": "b219bce30160", "generation_result": [{"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n    emb = tf.cast(timesteps, dtype=tf.float32)[:, None] * emb[None, :]\n    emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = tf.pad(emb, [[0, 0], [0, 1]])\n    assert emb.shape == [tf.shape(timesteps)[0], embedding_dim]\n    return emb\n\n", "CodeBleu_score": 68.67649933997359}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = torch.arange(timesteps.shape[0], dtype=torch.float32).unsqueeze(1) * emb.unsqueeze(0)\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = F.pad(emb, (0, 1, 0, 0))\n    assert emb.shape == (timesteps.shape[0], embedding_dim)\n    return emb\n\n", "CodeBleu_score": 70.59641317889546}, {"code": "def get_timestep_embedding(timesteps, embedding_dim)\n    assert len(timesteps.shape) == 1  # and timesteps.dtype == tf.int32\n    half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n    emb = tf.cast(timesteps, dtype=tf.float32)[:, None] * emb[None, :]\n    emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = tf.pad(emb, [[0, 0], [0, 1]])\n    assert emb.shape == [timesteps.shape[0], embedding_dim]\n    return emb\n\n", "CodeBleu_score": 68.83905759699184}]}
{"instance_id": "6ae8453c86be", "generation_result": [{"code": "def pytorch_attention(q, k, v)\n    try:\n        # Reshape the tensors to prepare them for attention computation.\n        q = q.view(q.shape[0], q.shape[1], 1, q.shape[2])\n        k = k.view(k.shape[0], 1, k.shape[1], k.shape[2])\n        v = v.view(v.shape[0], 1, v.shape[1], v.shape[2])\n\n        # Transpose the tensors to prepare them for attention computation.\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # Compute the attention output.\n        attention_output = torch.matmul(q, k) / math.sqrt(k.shape[-1])\n        attention_output = torch.matmul(attention_output, v).squeeze(1)\n\n        # Reshape the attention output back to the original dimensions.\n        attention_output = attention_output.view(q.shape[0], q.shape[2], v.shape[2])\n\n        return attention_output\n\n    except RuntimeError as exception:\n        if 'out of memory' in str(exception):\n            # Switch to slice attention as a fallback.\n            return slice_attention(q, k, v)\n\n        else:\n            # Re-raise the exception.\n            raise exception\n\n", "CodeBleu_score": 30.398019364804334}, {"code": "def pytorch_attention(q, k, v)\n    try:\n        # Reshape and transpose the tensors to prepare them for attention computation\n        q = q.view(q.shape[0], q.shape[1], 1, q.shape[2])\n        k = k.view(k.shape[0], k.shape[1], k.shape[2], 1)\n        v = v.view(v.shape[0], v.shape[1], v.shape[2], 1)\n        # Compute the attention\n        attention = torch.matmul(q, k) / math.sqrt(k.shape[2])\n        # Compute the softmax\n        attention = torch.nn.functional.softmax(attention, dim=1)\n        # Compute the attention output\n        attention = torch.matmul(attention, v)\n        # Reshape the attention output back to the original dimensions\n        attention = attention.view(attention.shape[0], attention.shape[1], attention.shape[2])\n        return attention\n    except RuntimeError as e:\n        # If out-of-memory (OOM) exception occurs, switch to slice attention as a fallback\n        if \"out of memory\" in str(e):\n            return pytorch_attention_slice(q, k, v)\n        else:\n            raise e\n\n", "CodeBleu_score": 27.00989833302105}, {"code": "def pytorch_attention(q, k, v)\n    # Reshape and transpose the tensors to prepare them for attention computation\n    q = q.reshape(q.shape[0], q.shape[1], 1, q.shape[2])\n    q = q.transpose(1, 2)\n    k = k.reshape(k.shape[0], 1, k.shape[1], k.shape[2])\n    k = k.transpose(1, 2)\n    v = v.reshape(v.shape[0], 1, v.shape[1], v.shape[2])\n    v = v.transpose(1, 2)\n\n    # Compute the attention\n    attn_output, attn_weights = F.softmax(torch.matmul(q, k), dim=-1), torch.matmul(q, k)\n\n    # Reshape the attention output back to the original dimensions\n    attn_output = attn_output.transpose(1, 2).reshape(attn_output.shape[0], attn_output.shape[2], attn_output.shape[3])\n    attn_weights = attn_weights.transpose(1, 2).reshape(attn_weights.shape[0], attn_weights.shape[2], attn_weights.shape[3])\n\n    return attn_output, attn_weights\n\n", "CodeBleu_score": 27.16979958078391}]}
{"instance_id": "fe4433c987ad", "generation_result": [{"code": "def apply_control(h, control, name)\n    if control and name:\n        try:\n            h += control\n            control.pop(name)\n        except:\n            logging.warning(f\"Failed to add control adjustment {name} to {h}\")\n    return h\n\n", "CodeBleu_score": 26.346691496715906}, {"code": "def apply_control(h, control, name)\n    if control is not None and name in control and control[name] is not None and control[name]!= \"\":\n        try:\n            h += control[name]\n        except:\n            logging.warning(\"Failed to add control {} to {}\".format(control[name], name))\n    return h\n", "CodeBleu_score": 43.73515539399082}, {"code": "def apply_control(h, control, name)\n    if control and name:\n        if control.get(name):\n            try:\n                h = h + control.get(name)\n            except:\n                logging.warning(f\"Unable to apply control adjustment {name} to h.\")\n            control.pop(name)\n    return h\n", "CodeBleu_score": 35.395197053888324}]}
{"instance_id": "56da4547c101", "generation_result": [{"code": "def get_codebook_entry(self, indices, shape)\n    if len(indices.shape) == 1:\n        indices = indices.reshape(-1, 1)\n    if shape is not None:\n        if len(shape) == 1:\n            shape = shape.reshape(-1, 1)\n        if len(shape)!= 2:\n            raise ValueError(\"Shape must be a vector of length 1 or a matrix of shape (1, n_features).\")\n        if shape.shape[0]!= 1:\n            raise ValueError(\"Shape must be a vector of length 1 or a matrix of shape (1, n_features).\")\n    if self.embedding_weights is None:\n        raise ValueError(\"The codebook has not been initialized yet. Please call the fit method first.\")\n    if len(self.embedding_weights.shape)!= 2:\n        raise ValueError(\"The codebook must be a matrix of shape (n_components, n_features).\")\n    if indices.shape[1]!= self.embedding_weights.shape[1]:\n        raise ValueError(\"The number of columns in the indices matrix must match the number of columns in the embedding weights matrix.\")\n    if shape is not None and shape.shape[1]!= self.embedding_weights.shape[1]:\n        raise ValueError(\"The number of columns in the shape matrix must match the number of columns in the embedding weights matrix.\")\n    if indices.dtype!= \"int32\":\n        raise ValueError(\"The indices matrix must be of type int32.\")\n    if shape is not None and shape.dtype!= \"int32\":\n        raise ValueError(\"The shape matrix must be of type int32.\")\n    if indices.min() < 0:\n        raise ValueError(\"The indices matrix must not contain negative values.\")\n    if shape is not None and (shape.min() < 1).any():\n        raise ValueError(\"The shape matrix must not contain values less than 1.\")\n    if self.embedding_weights.shape[0] < indices.max() + 1:\n        raise ValueError(\"The indices matrix must not contain values greater than the number of rows in the embedding weights matrix.\")\n    if self.embedding_weights.shape[1] < shape.max() + 1:\n        raise ValueError(\"The shape matrix must not contain values greater than the number of columns in the embedding weights matrix.\")\n    if indices.max() >= self.embedding_weights.shape[0]:\n        raise ValueError(\"The indices", "CodeBleu_score": 25.2001979167658}, {"code": "def get_codebook_entry(self, indices, shape)\n    # get quantized latent vectors\n    codebook_vectors = tf.nn.embedding_lookup(self.codebook_weights, indices)\n    # create one-hot encoding of indices\n    one_hot_indices = tf.one_hot(indices, self.codebook_size)\n    # multiply one-hot encoding with embedding weights\n    quantized_vectors = tf.matmul(one_hot_indices, self.codebook_weights)\n    # reshape and permute quantized vectors to match original input shape\n    if shape is not None:\n        quantized_vectors = tf.reshape(quantized_vectors, shape)\n        quantized_vectors = tf.transpose(quantized_vectors, [0, 3, 1, 2])\n    return quantized_vectors\n", "CodeBleu_score": 29.887482162892653}, {"code": "def get_codebook_entry(self, indices, shape)\n    indices = tf.cast(indices, tf.int32)\n    one_hot = tf.one_hot(indices, self.codebook_size)\n    quantized = tf.matmul(one_hot, self.embedding_weights)\n    if shape is not None:\n        quantized = tf.reshape(quantized, shape)\n        quantized = tf.transpose(quantized, [0, 3, 1, 2])\n    return quantized\n", "CodeBleu_score": 32.443255753617215}]}
{"instance_id": "548f8a1fb37d", "generation_result": [{"code": "def make_noise(self)\n    self.noise = [torch.zeros(1, 1, 2**i, 2**i, device=self.device)]\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            self.noise.append(torch.randn((1, 1, 2**i, 2**i), device=self.device))\n    return self.noise\n", "CodeBleu_score": 42.49399751463607}, {"code": "def make_noise(self)\n    noise = torch.zeros(4, 4).to(self.device)\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            noise = torch.cat([noise, torch.zeros(2 ** i, 2 ** i).to(self.device)])\n    return noise\n\n", "CodeBleu_score": 35.5233030659428}, {"code": "def make_noise(self)\n    noise_list = []\n    noise_list.append(torch.randn(4, 4, device=self.device))\n    for i in range(3, self.log_size, 1):\n      noise = torch.randn(2**i, 2**i, device=self.device)\n      noise_list.append(noise)\n      noise_list.append(noise)\n    return noise_list\n\n  def forward(self, x):\n    \"\"\"The function defines the forward pass of the generator.\n    It takes a batch of noise vectors as input and returns a batch of generated images.\n    \"\"\"\n    x = x.view(x.shape[0], self.z_dim, 1, 1)\n    for i in range(self.log_size - 3):\n      x = self.upsample(x)\n      x = torch.cat((x, self.noise_list[2*i]), dim=1)\n      x = self.conv_blocks[2*i](x)\n      x = self.conv_blocks[2*i + 1](x)\n    x = self.upsample(x)\n    x = torch.cat((x, self.noise_list[-2]), dim=1)\n    x = self.conv_blocks[-2](x)\n    x = self.conv_blocks[-1](x)\n    x = torch.tanh(x)\n    return x\n", "CodeBleu_score": 32.97190336552012}]}
{"instance_id": "42fd805563c6", "generation_result": [{"code": "def mean_latent(self, num_latent)\n    mean_latent_out = self.mean_style(torch.randn(num_latent, self.latent_dim, 1, 1, device=self.device))\n    mean_latent_out = mean_latent_out.mean(dim=0, keepdim=True)\n    return mean_latent_out\n", "CodeBleu_score": 25.73296074391137}, {"code": "def mean_latent(self, num_latent)\n    latent_in = torch.randn(num_latent, self.latent_dim, device=self.device)\n    latent_out = self.style_mlp(latent_in)\n    latent_mean = latent_out.mean(dim=0, keepdim=True)\n    return latent_mean\n\n", "CodeBleu_score": 32.66529550396486}, {"code": "def mean_latent(self, num_latent)\n    z = torch.randn(num_latent, self.latent_dim).to(self.device)\n    z = self.latent_mean_mlp(z)\n    return z.mean(dim=0, keepdim=True)\n\n  def get_mean_latent(self):\n    \"\"\"The function returns the mean latent vector.\"\"\"\n    return self.latent_mean\n\n  def get_std_latent(self):\n    \"\"\"The function returns the standard deviation of the latent vector.\"\"\"\n    return self.latent_std\n\n  def get_latent_dim(self):\n    \"\"\"The function returns the dimensionality of the latent vector.\"\"\"\n    return self.latent_dim\n\n  def get_latent_mean_mlp(self):\n    \"\"\"The function returns the style MLP that is used to generate the mean latent vector.\"\"\"\n    return self.latent_mean_mlp\n\n  def get_latent_std_mlp(self):\n    \"\"\"The function returns the style MLP that is used to generate the standard deviation of the latent vector.\"\"\"\n    return self.latent_std_mlp\n\n  def get_latent_mean_and_std_mlp(self):\n    \"\"\"The function returns the style MLP that is used to generate the mean and standard deviation of the latent vector.\"\"\"\n    return self.latent_mean_mlp, self.latent_std_mlp\n\n  def get_latent_mean_and_std(self):\n    \"\"\"The function returns the mean and standard deviation of the latent vector.\"\"\"\n    return self.latent_mean, self.latent_std\n\n  def get_latent_std(self):\n    \"\"\"The function returns the standard deviation of the latent vector.\"\"\"\n    return self.latent_std\n\n  def get_latent_mean(self):\n    \"\"\"The function returns the mean latent vector.\"\"\"\n    return self.latent_mean\n\n  def get_latent_mean_mlp(self):\n    \"\"\"The function returns the style MLP that is used to generate the mean latent vector.\"\"\"\n    return self.latent_mean_mlp\n\n  def get_latent_std_mlp(self):\n    \"\"\"The function returns the style MLP that is used to generate the standard deviation of the latent vector.\"\"\"\n    return self.latent_std_mlp\n\n  def get_latent_mean_and_std_mlp(self):\n    \"\"\"The function returns the style MLP that is used to generate the mean and standard deviation of the latent vector", "CodeBleu_score": 18.937111077742102}]}
{"instance_id": "2f485fd924af", "generation_result": [{"code": "def color_jitter(img, shift)\n    img = img.astype(np.float32)\n    if shift == 0:\n        return img\n    else:\n        jitter_val = np.random.uniform(-shift, shift, 3)\n        img = img + jitter_val\n        img = np.clip(img, 0., 1.)\n        return img\n", "CodeBleu_score": 55.84201512251901}, {"code": "def color_jitter(img, shift)\n    img = img.astype(np.float32)\n    img_jitter = img + np.random.uniform(-shift, shift, size=img.shape)\n    img_jitter = np.clip(img_jitter, 0, 1)\n    return img_jitter\n\n", "CodeBleu_score": 41.807834604622364}, {"code": "def color_jitter(img, shift)\n    img = np.array(img)\n    img_jitter = img + np.random.uniform(-shift, shift, 3)\n    img_jitter = np.clip(img_jitter, 0, 1)\n    return img_jitter\n\n", "CodeBleu_score": 44.44063744371466}]}
{"instance_id": "c87ccf1fbf48", "generation_result": [{"code": "def feed_data(self, data)\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n      self.gt = data['gt'].to(self.device)\n    if 'facial_component_locations' in data:\n      self.facial_component_locations = data['facial_component_locations'].to(self.device)\n", "CodeBleu_score": 52.099842281741026}, {"code": "def feed_data(self, data)\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data.keys():\n      self.gt = data['gt'].to(self.device)\n    if 'le' in data.keys():\n      self.le = data['le'].to(self.device)\n    if're' in data.keys():\n      self.re = data['re'].to(self.device)\n    if'm' in data.keys():\n      self.m = data['m'].to(self.device)\n", "CodeBleu_score": 47.79916509622883}, {"code": "def feed_data(self, data)\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n      self.gt = data['gt'].to(self.device)\n    if 'leye' in data:\n      self.leye = data['leye'].to(self.device)\n    if'reye' in data:\n      self.reye = data['reye'].to(self.device)\n    if'mouth' in data:\n      self.mouth = data['mouth'].to(self.device)\n\n", "CodeBleu_score": 56.72790266016826}]}
{"instance_id": "8f8c7cb13037", "generation_result": [{"code": "def construct_img_pyramid(self)\n    img_pyramid = [self.img_gt]\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, tf.image.resize(img_pyramid[0], (img_pyramid[0].shape[1] // 2, img_pyramid[0].shape[2] // 2), method='bilinear'))\n    return img_pyramid\n", "CodeBleu_score": 20.52133638068303}, {"code": "def construct_img_pyramid(self)\n    self.img_pyramid = [self.img]\n    for i in range(self.num_scales):\n        self.img_pyramid.insert(0, tf.image.resize(self.img_pyramid[0], (self.img_pyramid[0].shape[1]//2, self.img_pyramid[0].shape[2]//2), method=tf.image.ResizeMethod.BILINEAR))\n    return self.img_pyramid\n", "CodeBleu_score": 18.31886018607877}, {"code": "def construct_img_pyramid(self)\n    img_pyramid = [self.img_gt]\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], None, fx=1.0/2**i, fy=1.0/2**i, interpolation=cv2.INTER_LINEAR))\n    return img_pyramid\n\n  def construct_img_pyramid_bicubic(self):\n    \"\"\"The function constructs an image pyramid for intermediate restoration loss.\\nIt starts with the ground truth image and iteratively downscales it using bicubic interpolation.\\nThe downscaled images are inserted at the beginning of the pyramid list.\\nThe function returns the constructed image pyramid.\"\"\"\n    img_pyramid = [self.img_gt]\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], None, fx=1.0/2**i, fy=1.0/2**i, interpolation=cv2.INTER_CUBIC))\n    return img_pyramid\n\n  def construct_img_pyramid_bicubic_with_noise(self):\n    \"\"\"The function constructs an image pyramid for intermediate restoration loss.\\nIt starts with the ground truth image and iteratively downscales it using bicubic interpolation.\\nThe downscaled images are inserted at the beginning of the pyramid list.\\nThe function returns the constructed image pyramid.\"\"\"\n    img_pyramid = [self.img_gt]\n    for i in range(self.num_scales):\n        img_pyramid.insert(0, cv2.resize(img_pyramid[0], None, fx=1.0/2**i, fy=1.0/2**i, interpolation=cv2.INTER_CUBIC))\n    img_pyramid.insert(0, self.img_gt + np.random.normal(0, 10, size=self.img_gt.shape))\n    return img_pyramid\n\n  def construct_img_pyramid_bicubic_with_noise_and_blur(self):\n    \"\"\"The function constructs an image pyramid for intermediate restoration loss.\\nIt starts with the ground truth image and iteratively downscales it using bicubic interpolation.\\nThe downscaled", "CodeBleu_score": 23.514641835327467}]}
{"instance_id": "51247274d40c", "generation_result": [{"code": "def save(self, epoch, current_iter)\n    save_path = os.path.join(self.save_dir,'model_epoch_{}_iter_{}.pth'.format(epoch, current_iter))\n    save_dict = {'epoch': epoch, 'iter': current_iter, 'gen_state_dict': self.gen.state_dict(), 'gen_ema_state_dict': self.gen_ema.state_dict(), 'dis_state_dict': self.dis.state_dict()}\n    if self.use_facial_disc:\n      save_dict['dis_left_state_dict'] = self.dis_left.state_dict()\n      save_dict['dis_right_state_dict'] = self.dis_right.state_dict()\n      save_dict['dis_mouth_state_dict'] = self.dis_mouth.state_dict()\n    torch.save(save_dict, save_path)\n    print('The model is saved successfully.')\n\n  def load(self, checkpoint_path, gen_only=False):\n    \"\"\"The function loads the state of the model from a given checkpoint path.\\nIf gen_only is set to True, it only loads the generator network, its EMA version, and the overall training state.\"\"\"\n    checkpoint = torch.load(checkpoint_path)\n    self.gen.load_state_dict(checkpoint['gen_state_dict'])\n    self.gen_ema.load_state_dict(checkpoint['gen_ema_state_dict'])\n    if not gen_only:\n      self.dis.load_state_dict(checkpoint['dis_state_dict'])\n      if self.use_facial_disc:\n        self.dis_left.load_state_dict(checkpoint['dis_left_state_dict'])\n        self.dis_right.load_state_dict(checkpoint['dis_right_state_dict'])\n        self.dis_mouth.load_state_dict(checkpoint['dis_mouth_state_dict'])\n    print('The model is loaded successfully.')\n\n  def load_for_eval(self, checkpoint_path, gen_only=False):\n    \"\"\"The function loads the state of the model for evaluation from a given checkpoint path.\\nIf gen_only is set to True, it only loads the generator network and its EMA version.\"\"\"\n    checkpoint = torch.load(checkpoint_path)\n   ", "CodeBleu_score": 29.22367274014242}, {"code": "def save(self, epoch, current_iter)\n    print(\"Saving model...\")\n    state = {\n        \"epoch\": epoch,\n        \"iteration\": current_iter,\n        \"generator\": self.generator.state_dict(),\n        \"generator_ema\": self.generator_ema.state_dict(),\n        \"discriminator\": self.discriminator.state_dict(),\n        \"optimizer\": self.optimizer.state_dict(),\n        \"scheduler\": self.scheduler.state_dict(),\n        \"best_fid\": self.best_fid,\n        \"best_fid_iter\": self.best_fid_iter,\n        \"best_fid_epoch\": self.best_fid_epoch,\n        \"best_fid_sample_path\": self.best_fid_sample_path,\n        \"best_fid_sample_fid\": self.best_fid_sample_fid,\n        \"best_fid_sample_fid_ema\": self.best_fid_sample_fid_ema,\n        \"best_fid_sample_path_ema\": self.best_fid_sample_path_ema,\n        \"best_fid_sample_fid_real\": self.best_fid_sample_fid_real,\n        \"best_fid_sample_path_real\": self.best_fid_sample_path_real,\n        \"best_fid_sample_fid_fake\": self.best_fid_sample_fid_fake,\n        \"best_fid_sample_path_fake\": self.best_fid_sample_path_fake,\n        \"best_fid_sample_fid_real_ema\": self.best_fid_sample_fid_real_ema,\n        \"best_fid_sample_path_real_ema\": self.best_fid_sample_path_real_ema,\n        \"best_fid_sample_fid_fake_ema\": self.best_fid_sample_fid_fake_ema,\n        \"best_fid_sample_path_fake_ema\": self.best_fid_sample_path_fake_ema,\n        \"best_fid_sample_fid_real_ema_fake\": self.best_fid_sample_fid_real_ema_fake,\n        \"best_fid_sample_path_real_ema_fake\": self.best_fid_sample_path_real_ema_fake,\n        \"best_fid_sample_fid_real_ema_real\": self.", "CodeBleu_score": 24.979580153013046}, {"code": "def save(self, epoch, current_iter)\n    if not os.path.exists(self.model_dir):\n      os.makedirs(self.model_dir)\n    if not os.path.exists(self.model_dir + '/checkpoint'):\n      os.makedirs(self.model_dir + '/checkpoint')\n    if not os.path.exists(self.model_dir + '/checkpoint/' + str(epoch)):\n      os.makedirs(self.model_dir + '/checkpoint/' + str(epoch))\n    if not os.path.exists(self.model_dir + '/checkpoint/' + str(epoch) + '/' + str(current_iter)):\n      os.makedirs(self.model_dir + '/checkpoint/' + str(epoch) + '/' + str(current_iter))\n    torch.save(self.generator.state_dict(), self.model_dir + '/checkpoint/' + str(epoch) + '/' + str(current_iter) + '/generator.pth')\n    torch.save(self.generator_ema.state_dict(), self.model_dir + '/checkpoint/' + str(epoch) + '/' + str(current_iter) + '/generator_ema.pth')\n    if self.use_facial_disc:\n      torch.save(self.facial_disc_left.state_dict(), self.model_dir + '/checkpoint/' + str(epoch) + '/' + str(current_iter) + '/facial_disc_left.pth')\n      torch.save(self.facial_disc_right.state_dict(), self.model_dir + '/checkpoint/' + str(epoch) + '/' + str(current_iter) + '/facial_disc_right.pth')\n      torch.save(self.facial_disc_mouth.state_dict(), self.model_dir + '/checkpoint/' + str(epoch) + '/' + str(current_iter) + '/facial_disc_mouth.pth')\n    if self.use_discriminator:\n      torch.save(self.discriminator.state_dict(), self.model_dir + '/checkpoint/' + str(epoch) + '/' + str(current_iter) + '/discriminator.pth')\n    torch.save(self.optimizer.state_dict(), self.model_dir + '/checkpoint/' + str(epoch) + '/' + str(current_iter) + '/optimizer.pth')", "CodeBleu_score": 38.16146439550862}]}
{"instance_id": "afa0dc8128c1", "generation_result": [{"code": "def clean_folder(folder)\n    for item in os.listdir(folder):\n        path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (path, e))\n", "CodeBleu_score": 45.41035389090576}, {"code": "def clean_folder(folder)\n    for item in os.listdir(folder):\n        item_path = os.path.join(folder, item)\n        try:\n            if os.path.isfile(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except OSError as error:\n            print(f\"Error while deleting {item_path}: {error}\")\n", "CodeBleu_score": 45.449290121369195}, {"code": "def clean_folder(folder)\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n", "CodeBleu_score": 67.88650947584345}]}
{"instance_id": "53588b481bdd", "generation_result": [{"code": "def next_batch(self, sample_size=1)\n    if sample_size > len(self.memory):\n      sample_size = len(self.memory)\n    batch = random.sample(self.memory, sample_size)\n    for experience in batch:\n      self.memory.remove(experience)\n    return batch\n", "CodeBleu_score": 48.90628585935024}, {"code": "def next_batch(self, sample_size=1)\n    batch = []\n    if self.size < sample_size:\n        sample_size = self.size\n    for i in range(sample_size):\n        batch.append(self.experiences.pop(randint(0, self.size - 1)))\n    return batch\n", "CodeBleu_score": 30.24855366721382}, {"code": "def next_batch(self, sample_size=1)\n    batch = []\n    for _ in range(sample_size):\n        if len(self.buffer) > 0:\n            idx = np.random.randint(len(self.buffer))\n            batch.append(self.buffer.pop(idx))\n    return batch\n", "CodeBleu_score": 30.64872205678603}]}
{"instance_id": "6bfa6a4e621c", "generation_result": [{"code": "def find_estimated_top_configs(self)\n    # Flatten all configurations\n    all_configs_flattened = []\n    for config in self.all_configs:\n      all_configs_flattened.append(config.flatten())\n    all_configs_flattened = np.array(all_configs_flattened)\n    # Collect numerical values\n    all_configs_numerical = []\n    for config in all_configs_flattened:\n      all_configs_numerical.append(self.cost_model.collect_numerical_values(config))\n    all_configs_numerical = np.array(all_configs_numerical)\n    # Predict estimated values\n    estimated_values = self.cost_model.predict_values(all_configs_numerical)\n    # Sort configurations\n    if self.metric == \"min\":\n      sorted_indices = np.argsort(estimated_values)\n    elif self.metric == \"max\":\n      sorted_indices = np.argsort(-estimated_values)\n    return sorted_indices\n\n  def get_top_configs(self):\n    \"\"\"The function returns the top configurations based on a cost model.\\nIt flattens each configuration in all_configs, collects numerical values, and converts them into a numpy array.\\nThe cost model predicts estimates for these configurations, which are then sorted to identify the top configurations.\\nThe sorting order depends on the specified metric, and the function returns the top configurations.\"\"\"\n    # Find the estimated top configurations\n    sorted_indices = self.find_estimated_top_configs()\n    # Get the top configurations\n    top_configs = []\n    for index in sorted_indices[:self.num_configs]:\n      top_configs.append(self.all_configs[index])\n    return top_configs\n\n  def get_top_config_indices(self):\n    \"\"\"The function returns the indices of the top configurations based on a cost model.\\nIt flattens each configuration in all_configs, collects numerical values, and converts them into a numpy array.\\nThe cost model predicts estimates for these configurations, which are then sorted to identify the top configurations.\\nThe sorting order depends on the specified metric, and the function returns the indices of the top configurations.\"\"\"\n    # Find the estimated top configurations\n    sorted_indices = self.find_estimated_top_configs()\n    # Get the top configurations\n    top_config_indices = sorted_indices[:self.num_configs]\n    return top_config_", "CodeBleu_score": 33.478892160689576}, {"code": "def find_estimated_top_configs(self)\n    all_configs = self.all_configs\n    # Flatten all configurations\n    all_configs = [item for sublist in all_configs for item in sublist]\n    # Collect numerical values\n    all_configs = [list(config.values()) for config in all_configs]\n    # Convert to numpy array\n    all_configs = np.array(all_configs)\n    # Predict costs\n    costs = self.cost_model.predict(all_configs)\n    # Sort\n    if self.metric == \"cost\":\n      sorted_indices = np.argsort(costs)\n    elif self.metric == \"accuracy\":\n      accuracies = self.accuracy_model.predict(all_configs)\n      sorted_indices = np.argsort(accuracies)\n    elif self.metric == \"combined\":\n      accuracies = self.accuracy_model.predict(all_configs)\n      costs = self.cost_model.predict(all_configs)\n      combined = accuracies + costs\n      sorted_indices = np.argsort(combined)\n    # Get top indices\n    top_indices = sorted_indices[-self.num_top_configs:]\n    # Get top configurations\n    top_configs = [all_configs[i] for i in top_indices]\n    # Return top configurations\n    return top_configs, top_indices\n\n  def find_top_configs(self):\n    \"\"\"The function finds the top configurations based on a cost model.\\nIt flattens each configuration in all_configs, collects numerical values, and converts them into a numpy array.\\nThe cost model predicts costs for these configurations, which are then sorted to identify the top configurations.\\nThe sorting order depends on the specified metric, and the function returns the indices of the top configurations.\"\"\"\n    all_configs = self.all_configs\n    # Flatten all configurations\n    all_configs = [item for sublist in all_configs for item in sublist]\n    # Collect numerical values\n    all_configs = [list(config.values()) for config in all_configs]\n    # Convert to numpy array\n    all_configs = np.array(all_configs)\n    # Predict costs\n    costs = self.cost_model.predict(all_configs)\n    # Sort\n    if self.metric == \"cost\":\n      sorted_indices = np.argsort(costs)\n    elif self.metric == \"accuracy\":\n      accuracies = self.", "CodeBleu_score": 30.383050776369192}, {"code": "def find_estimated_top_configs(self)\n    if self.all_configs is None:\n      self.find_all_configs()\n    if self.all_configs is None:\n      raise ValueError(\"Could not find any configurations\")\n    if self.cost_model is None:\n      self.find_cost_model()\n    if self.cost_model is None:\n      raise ValueError(\"Could not find a cost model\")\n    if self.cost_model_type is None:\n      self.find_cost_model_type()\n    if self.cost_model_type is None:\n      raise ValueError(\"Could not find the type of cost model\")\n    if self.top_configs is None:\n      self.top_configs = []\n    if self.top_config_indices is None:\n      self.top_config_indices = []\n    if self.top_config_indices:\n      return self.top_config_indices\n    if self.top_configs:\n      return self.top_configs\n    if self.cost_model_type == \"linear\":\n      self.top_configs = []\n      self.top_config_indices = []\n      for i in range(len(self.all_configs)):\n        config = self.all_configs[i]\n        config_flattened = [item for sublist in config for item in sublist]\n        config_flattened = [item for sublist in config_flattened for item in sublist]\n        config_flattened = [item for sublist in config_flattened for item in sublist]\n        config_flattened = [item for sublist in config_flattened for item in sublist]\n        config_flattened = [item for sublist in config_flattened for item in sublist]\n        config_flattened = [item for sublist in config_flattened for item in sublist]\n        config_flattened = [item for sublist in config_flattened for item in sublist]\n        config_flattened = [item for sublist in config_flattened for item in sublist]\n        config_flattened = [item for sublist in config_flattened for item in sublist]\n        config_flattened = [item for sublist in config_flattened for item in sublist]\n        config_flattened = [item for sublist in config_flattened for item in sublist]\n        config_flattened = [item for sublist in config_flattened", "CodeBleu_score": 14.729249866701286}]}
{"instance_id": "d4cf48413a57", "generation_result": [{"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 77.57746631404694}, {"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n", "CodeBleu_score": 77.57746631404694}, {"code": "def flatten(d, parent_key='', sep='_')\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n", "CodeBleu_score": 77.57746631404694}]}
{"instance_id": "c2a514249c25", "generation_result": [{"code": "def run_after_tuning(self)\n    if self.optimal_cmd:\n      self.log_msg(self.name + \" running after_tuning\")\n      self.log_msg(self.optimal_cmd)\n      self.log_msg(\" \".join(self.optimal_cmd))\n      try:\n        subprocess.Popen(self.optimal_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT).wait()\n      except Exception as e:\n        self.log_msg(\"after_tuning: subprocess.Popen() failed: \" + str(e))\n      self.log_msg(self.name + \" after_tuning complete\")\n    else:\n      self.log_msg(self.name + \" did not find an optimal configuration\")\n", "CodeBleu_score": 38.93702764567205}, {"code": "def run_after_tuning(self)\n    if self.optimal_cmd is not None:\n      logging.info('Running optimal command: %s', self.optimal_cmd)\n      subprocess.Popen(self.optimal_cmd, shell=True).wait()\n      logging.info('Optimal command finished.')\n    else:\n      logging.info('No optimal command found.')\n\n  def run_before_tuning(self):\n    \"\"\"The function executes a command before tuning.\\nIf before_cmd is available, it runs the command using subprocess.Popen and waits for it to complete, then logs a success message.\\nIf no before command is found, it logs a message indicating that autotuning did not find an optimal configuration.\"\"\"\n    if self.before_cmd is not None:\n      logging.info('Running before command: %s', self.before_cmd)\n      subprocess.Popen(self.before_cmd, shell=True).wait()\n      logging.info('Before command finished.')\n    else:\n      logging.info('No before command found.')\n", "CodeBleu_score": 43.43129657421818}, {"code": "def run_after_tuning(self)\n    if self.optimal_cmd is not None:\n        print(f'Running {self.optimal_cmd}...')\n        process = subprocess.Popen(self.optimal_cmd, shell=True)\n        process.wait()\n        print('Done!')\n    else:\n        print('No optimal configuration found.')\n", "CodeBleu_score": 48.72713035083356}]}
{"instance_id": "61802de52df0", "generation_result": [{"code": "def run_job(self, exp: dict, reservations)\n\n    # Get the experiment's master port and result directory\n    master_port = self.get_master_port()\n    result_dir = self.get_result_dir(exp)\n\n    # Update the experiment's user arguments based on argument mappings\n    exp = self.update_user_arguments(exp)\n\n    # Create a new thread to run the experiment and start it\n    thread = threading.Thread(target=self.run_experiment, args=(exp, master_port, result_dir, reservations))\n    thread.start()\n\n    # Store the experiment details with a timestamp in running_experiments\n    self.running_experiments[exp[\"id\"]] = {\n        \"thread\": thread,\n        \"master_port\": master_port,\n        \"result_dir\": result_dir,\n        \"start_time\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n\n    # Return the experiment's ID\n    return exp[\"id\"]\n\n", "CodeBleu_score": 20.87720101330255}, {"code": "def run_job(self, exp: dict, reservations)\n    exp_id = exp[\"id\"]\n    exp_name = exp[\"name\"]\n    exp_script = exp[\"script\"]\n    exp_args = exp[\"args\"]\n    exp_master_port = exp[\"master_port\"]\n    exp_result_dir = exp[\"result_dir\"]\n    exp_user_args = exp[\"user_args\"]\n    exp_user_args_map = exp[\"user_args_map\"]\n\n    # Update the experiment's user arguments based on the argument mappings\n    if exp_user_args_map:\n        for arg_name, arg_value in exp_user_args_map.items():\n            if arg_name in exp_user_args:\n                exp_user_args[arg_name] = arg_value\n\n    # Add the experiment's master port and result directory to the experiment's arguments\n    exp_user_args[\"master_port\"] = exp_master_port\n    exp_user_args[\"result_dir\"] = exp_result_dir\n\n    # Create a new thread to run the experiment\n    thread = threading.Thread(\n        target=run_experiment,\n        args=(exp_id, exp_name, exp_script, exp_args, exp_user_args, reservations),\n    )\n    thread.start()\n\n    # Add the experiment to the running experiments list\n    self.running_experiments.append(\n        {\n            \"id\": exp_id,\n            \"name\": exp_name,\n            \"script\": exp_script,\n            \"args\": exp_args,\n            \"user_args\": exp_user_args,\n            \"start_time\": datetime.now(),\n            \"thread\": thread,\n        }\n    )\n\n", "CodeBleu_score": 28.110371144987734}, {"code": "def run_job(self, exp: dict, reservations)\n    exp_id = exp['id']\n    exp_name = exp['name']\n    exp_script = exp['script']\n    exp_args = exp['args']\n    exp_args_map = exp['args_map']\n    exp_master_port = exp['master_port']\n    exp_result_dir = exp['result_dir']\n    exp_result_dir_timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    exp_result_dir_path = os.path.join(exp_result_dir, exp_result_dir_timestamp)\n    exp_result_dir_path = os.path.join(exp_result_dir_path, exp_name)\n    exp_result_dir_path = os.path.abspath(exp_result_dir_path)\n    exp_result_dir_path = os.path.normpath(exp_result_dir_path)\n    exp_args_map = self.get_args_map(exp_args_map)\n    exp_args = self.get_args(exp_args, exp_args_map)\n    exp_script = self.get_script(exp_script)\n    exp_script_path = os.path.join(exp_script)\n    exp_script_path = os.path.abspath(exp_script_path)\n    exp_script_path = os.path.normpath(exp_script_path)\n    exp_args.append(exp_result_dir_path)\n    exp_args.append(exp_master_port)\n    exp_args.append(exp_name)\n    exp_args.append(exp_id)\n    exp_args.append(exp_script_path)\n    exp_args = self.get_args(exp_args, exp_args_map)\n    exp_args = self.get_args(exp_args, self.args_map)\n    exp_args = self.get_args(exp_args, self.args_map_global)\n    exp_args = self.get_args(exp_args, self.args_map_global_local)\n    exp_args = self.get_args(exp_args, self.args_map_local)\n    exp_args = self.get_args(exp_args, self", "CodeBleu_score": 25.71854884136914}]}
{"instance_id": "4884e2e9665c", "generation_result": [{"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.distributed:\n        if self.rank == 0:\n            self.save_exp_results_to_database(message, ranks, path)\n    else:\n        if ranks is None:\n            ranks = [0]\n        if self.rank in ranks:\n            self.save_exp_results_to_database(message, ranks, path)\n", "CodeBleu_score": 17.86845297666855}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.is_distributed:\n      if self.rank == 0:\n        self.save_exp_results_to_database(message, ranks, path)\n      else:\n        pass\n    else:\n      if path is None:\n        path = self.exp_results_path\n      if ranks is None:\n        ranks = self.ranks\n      if self.rank in ranks:\n        with open(path, 'a') as f:\n          json.dump(message, f)\n          f.write('\\n')\n", "CodeBleu_score": 29.192286741174243}, {"code": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    if self.distributed:\n      if self.rank == 0:\n        self.save_exp_results_to_database(message, ranks, path)\n      else:\n        pass\n    else:\n      self.save_exp_results_to_database(message, ranks, path)\n", "CodeBleu_score": 11.88507001511578}]}
{"instance_id": "0429344c3c4d", "generation_result": [{"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    layer_files = self.get_layer_files(layer_index)\n    if len(layer_files) == 0:\n      raise ValueError(\"No files found for layer index {}\".format(layer_index))\n    if layer_index not in self.tp_degree:\n      raise ValueError(\"No tp_degree specified for layer index {}\".format(layer_index))\n    tp_degree = self.tp_degree[layer_index]\n    layer_files = sorted(layer_files)\n    partitions = []\n    for i in range(tp_degree):\n      partitions.append([])\n    for i in range(len(layer_files)):\n      partition_index = i % tp_degree\n      partitions[partition_index].append(layer_files[i])\n    return {i: partitions[i] for i in range(tp_degree)}\n\n  def _build_tp_layer_map(self, layer_index: int):\n    \"\"\"The function builds a data map for a specific layer index.\\nIt first checks if there are any layer files and verifies the layer index.\\nIt retrieves files with a specific prefix for the given layer and partitions these files based on tp_degree.\\nThe function then creates and returns a dictionary mapping partition indices to their corresponding file lists.\"\"\"\n    layer_files = self.get_layer_files(layer_index)\n    if len(layer_files) == 0:\n      raise ValueError(\"No files found for layer index {}\".format(layer_index))\n    if layer_index not in self.tp_degree:\n      raise ValueError(\"No tp_degree specified for layer index {}\".format(layer_index))\n    tp_degree = self.tp_degree[layer_index]\n    layer_files = sorted(layer_files)\n    partitions = []\n    for i in range(tp_degree):\n      partitions.append([])\n    for i in range(len(layer_files)):\n      partition_index = i % tp_degree\n      partitions[partition_index].append(layer_files[i])\n    return {i: partitions[i] for i in range(tp_degree)}\n\n  def _build_layer_map(self, layer_index: int):\n    \"\"\"The function builds a data map for a specific layer index.\\nIt first checks if there are any layer files and verifies the layer index.\\nIt retrieves files with a specific prefix for the given layer and partitions these files based on tp_", "CodeBleu_score": 27.27566029340104}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    layer_files = self._get_layer_files(layer_index)\n    if len(layer_files) == 0:\n      raise RuntimeError(f'No layer files found for layer index {layer_index}')\n    if layer_index >= self.num_layers:\n      raise RuntimeError(f'Layer index {layer_index} is greater than the number of layers {self.num_layers}')\n    tp_other_layer_map = {}\n    for file in layer_files:\n      tp_degree = int(file.split('_')[-1])\n      if tp_degree not in tp_other_layer_map:\n        tp_other_layer_map[tp_degree] = []\n      tp_other_layer_map[tp_degree].append(file)\n    return tp_other_layer_map\n\n  def _get_layer_files(self, layer_index: int):\n    \"\"\"The function returns a list of layer files for a given layer index.\"\"\"\n    layer_files = []\n    for file in os.listdir(self.layer_dir):\n      if file.startswith(f'layer_{layer_index}_'):\n        layer_files.append(file)\n    return layer_files\n\n  def _get_layer_file_path(self, layer_index: int, partition_index: int):\n    \"\"\"The function returns the path of the layer file for a given layer index and partition index.\"\"\"\n    return os.path.join(self.layer_dir, f'layer_{layer_index}_tp_{partition_index}')\n\n  def _get_layer_file_paths(self, layer_index: int):\n    \"\"\"The function returns a list of layer file paths for a given layer index.\"\"\"\n    layer_file_paths = []\n    for partition_index in range(self.tp_degree):\n      layer_file_paths.append(self._get_layer_file_path(layer_index, partition_index))\n    return layer_file_paths\n\n  def _get_layer_file_path_for_tp_degree(self, layer_index: int, tp_degree: int):\n    \"\"\"The function returns the path of the layer file for a given layer index and tp degree.\"\"\"\n    return os.path.join(self.layer_dir, f'layer_{layer_index}_tp_{tp_degree}')\n\n  def _get_layer_file_paths_", "CodeBleu_score": 28.31239829166744}, {"code": "def _build_tp_other_layer_map(self, layer_index: int)\n    if not self._check_if_layer_files_exist():\n      raise Exception(\"Layer files do not exist.\")\n\n    if not self._check_layer_index(layer_index):\n      raise Exception(\"Layer index is invalid.\")\n\n    layer_files = self._get_layer_files(layer_index)\n    layer_files_prefix = self._get_layer_files_prefix(layer_index)\n\n    tp_degree = self._get_tp_degree()\n    tp_other_layer_map = {}\n    for i in range(tp_degree):\n      tp_other_layer_map[i] = []\n\n    for layer_file in layer_files:\n      if layer_file.startswith(layer_files_prefix):\n        partition_index = int(layer_file.split(\"_\")[1])\n        tp_other_layer_map[partition_index].append(layer_file)\n\n    return tp_other_layer_map\n\n  def _get_layer_files(self, layer_index: int):\n    \"\"\"The function returns a list of layer files for a specific layer index.\"\"\"\n    layer_files_prefix = self._get_layer_files_prefix(layer_index)\n    layer_files = []\n    for file in os.listdir(self._get_layer_dir()):\n      if file.startswith(layer_files_prefix):\n        layer_files.append(file)\n    return layer_files\n\n  def _get_layer_files_prefix(self, layer_index: int):\n    \"\"\"The function returns the prefix for layer files for a specific layer index.\"\"\"\n    return f\"layer_{layer_index}_\"\n\n  def _get_layer_dir(self):\n    \"\"\"The function returns the layer directory.\"\"\"\n    return self._layer_dir\n\n  def _get_tp_degree(self):\n    \"\"\"The function returns the tp degree.\"\"\"\n    return self._tp_degree\n\n  def _get_num_layers(self):\n    \"\"\"The function returns the number of layers.\"\"\"\n    return self._num_layers\n\n  def _get_num_nodes(self):\n    \"\"\"The function returns the number of nodes.\"\"\"\n    return self._num_nodes\n\n  def _get_num_edges(self):\n    \"\"\"The function returns the number of edges.\"\"\"\n    return self._num_edges\n\n  def _get_num_partitions(self):\n    \"\"\"The function returns the number of partitions.\"\"\"\n    return self._", "CodeBleu_score": 27.870536933335156}]}
{"instance_id": "1b65ecb9be93", "generation_result": [{"code": "def system_info()\n    system_info = {}\n    system_info[\"operating_system\"] = platform.system()\n    system_info[\"architecture\"] = platform.architecture()\n    system_info[\"version\"] = platform.version()\n    system_info[\"processor\"] = platform.processor()\n    system_info[\"python_version\"] = platform.python_version()\n    return system_info\n", "CodeBleu_score": 40.72815425213333}, {"code": "def system_info()\n    info = {'os': platform.system(), 'architecture': platform.architecture(),'version': platform.version(), 'processor': platform.processor(), 'python': platform.python_version()}\n    return info\n", "CodeBleu_score": 48.934121135399685}, {"code": "def system_info()\n    system_info = {\n        \"Operating System\": platform.system(),\n        \"Architecture\": platform.machine(),\n        \"Version\": platform.version(),\n        \"Processor\": platform.processor(),\n        \"Python Version\": platform.python_version()\n    }\n    return system_info\n", "CodeBleu_score": 50.85612039970927}]}
{"instance_id": "969bcc5cf3b5", "generation_result": [{"code": "def cuda_info()\n    try:\n        import pycuda.driver as cuda\n        import pycuda.autoinit\n        from pycuda.compiler import SourceModule\n        import numpy as np\n        import sys\n        import os\n        import platform\n        import subprocess\n        import re\n        import matplotlib\n        import matplotlib.pyplot as plt\n        import warnings\n        import time\n        import datetime\n        import pandas as pd\n        import json\n        import requests\n        import urllib.request\n        import urllib.parse\n        import urllib.error\n        import base64\n        import io\n        import glob\n        import sys\n        import os\n        import platform\n        import subprocess\n        import re\n        import matplotlib\n        import matplotlib.pyplot as plt\n        import warnings\n        import time\n        import datetime\n        import pandas as pd\n        import json\n        import requests\n        import urllib.request\n        import urllib.parse\n        import urllib.error\n        import base64\n        import io\n        import glob\n        import sys\n        import os\n        import platform\n        import subprocess\n        import re\n        import matplotlib\n        import matplotlib.pyplot as plt\n        import warnings\n        import time\n        import datetime\n        import pandas as pd\n        import json\n        import requests\n        import urllib.request\n        import urllib.parse\n        import urllib.error\n        import base64\n        import io\n        import glob\n        import sys\n        import os\n        import platform\n        import subprocess\n        import re\n        import matplotlib\n        import matplotlib.pyplot as plt\n        import warnings\n        import time\n        import datetime\n        import pandas as pd\n        import json\n        import requests\n        import urllib.request\n        import urllib.parse\n        import urllib.error\n        import base64\n        import io\n        import glob\n        import sys\n        import os\n        import platform\n        import subprocess\n        import re\n        import matplotlib\n        import matplotlib.pyplot as plt\n        import warnings\n        import time\n        import datetime\n        import pandas as pd\n        import json\n        import requests\n        import urllib.request\n        import urllib.parse\n        import urllib.error\n        import base64\n        import io\n        import glob\n        import sys\n        import os\n        import platform\n        import subprocess\n        import re\n        import matplotlib\n        import matplotlib.pyplot as plt\n        import warnings\n        import time\n        import datetime\n        import pandas as pd\n        import json\n        import requests\n        import urllib.request\n        import urllib.parse\n        import urllib.error\n        import base64\n        import io\n        import glob\n        import sys\n        import os\n        import platform\n        import subprocess\n        import re\n        import matplotlib\n        import matplotlib.pyplot as plt\n        import warnings\n        import time\n        import datetime\n        import pandas as pd\n        import json\n        import requests\n        import urllib.request\n        import urllib.", "CodeBleu_score": 23.01269000715775}, {"code": "def cuda_info()\n    if torch.cuda.is_available():\n        print(\"CUDA is available\")\n        print(\"CUDA version: \" + str(torch.version.cuda))\n        print(\"Number of CUDA devices: \" + str(torch.cuda.device_count()))\n        print(\"CUDA device name: \" + torch.cuda.get_device_name(0))\n        print(\"CUDA device capability: \" + str(torch.cuda.get_device_capability(0)))\n        print(\"CUDA device total memory: \" + str(torch.cuda.get_device_properties(0).total_memory))\n        print(\"CUDA device multiprocessors: \" + str(torch.cuda.get_device_properties(0).multi_processor_count))\n    else:\n        print(\"CUDA is not available\")\n\n", "CodeBleu_score": 32.65187507738011}, {"code": "def cuda_info()\n    import torch\n    from torch.utils.cpp_extension import CUDA_HOME\n\n    cuda_available = torch.cuda.is_available()\n    cuda_version = torch.version.cuda\n    cuda_device_count = torch.cuda.device_count()\n    cuda_device_name = torch.cuda.get_device_name(0)\n    cuda_device_capability = torch.cuda.get_device_capability(0)\n\n    print(f\"CUDA available: {cuda_available}\")\n    print(f\"CUDA version: {cuda_version}\")\n    print(f\"CUDA device count: {cuda_device_count}\")\n    print(f\"CUDA device name: {cuda_device_name}\")\n    print(f\"CUDA device capability: {cuda_device_capability}\")\n    print(f\"CUDA home: {CUDA_HOME}\")\n\n", "CodeBleu_score": 36.64954550308504}]}
{"instance_id": "a2ba908a4c70", "generation_result": [{"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    encoder_accuracy = {}\n    for class_name in encoder_manager.get_class_names():\n        encoder_accuracy[class_name] = 0\n    num_items = len(dataset_items)\n    for idx, dataset_item in enumerate(dataset_items):\n        print(f\"Processing item {idx+1}/{num_items}\")\n        audio_file = dataset_item.get_audio_file()\n        if audio_file is None:\n            print(\"No audio file found, skipping\")\n            continue\n        embedding = encoder_manager.get_embedding(audio_file)\n        if embedding is None:\n            print(\"No embedding found, skipping\")\n            continue\n        predicted_label = encoder_manager.predict(embedding)\n        true_label = dataset_item.get_class_name()\n        if true_label is None:\n            print(\"No true label found, skipping\")\n            continue\n        if predicted_label == true_label:\n            encoder_accuracy[true_label] += 1\n    num_correct = sum(encoder_accuracy.values())\n    print(f\"Encoder accuracy: {num_correct}/{num_items} = {num_correct/num_items}\")\n    for class_name in encoder_manager.get_class_names():\n        accuracy = encoder_accuracy[class_name]/num_items\n        print(f\"{class_name}: {accuracy}\")\n    print(f\"Average accuracy: {sum(encoder_accuracy.values())/len(encoder_accuracy)}\")\n", "CodeBleu_score": 30.11955231491152}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    class_names = encoder_manager.config['mappings']['classes']\n    class_ids_to_names = encoder_manager.config['mappings']['ids_to_classes']\n    class_name_to_id = {name: id for id, name in class_ids_to_names.items()}\n    class_accuracies = {name: 0.0 for name in class_names}\n    total_count = 0.0\n    for item in dataset_items:\n        audio_path = item['audio_path']\n        true_class_name = item['class_name']\n        true_class_id = item['class_id']\n        if true_class_id is not None:\n            true_class_name = class_ids_to_names[true_class_id]\n        if true_class_name not in class_names:\n            continue\n        embedding = encoder_manager.compute_embedding(audio_path)\n        predicted_class_id = encoder_manager.predict(embedding)\n        predicted_class_name = class_ids_to_names[predicted_class_id]\n        if predicted_class_name == true_class_name:\n            class_accuracies[true_class_name] += 1.0\n        total_count += 1.0\n    for class_name in class_names:\n        accuracy = class_accuracies[class_name] / total_count\n        print('Accuracy for class {}: {:.2f}%'.format(class_name, 100.0 * accuracy))\n    print('Average accuracy: {:.2f}%'.format(100.0 * (sum(class_accuracies.values()) / total_count)))\n\n", "CodeBleu_score": 28.51938396383496}, {"code": "def compute_encoder_accuracy(dataset_items, encoder_manager)\n    encoder_accuracy = {}\n    for class_name, class_id in encoder_manager.get_class_map().items():\n        encoder_accuracy[class_name] = 0\n    for dataset_item in dataset_items:\n        audio_file = dataset_item.get_audio_file()\n        if audio_file:\n            embedding = encoder_manager.compute_embedding(audio_file)\n            if embedding:\n                class_name = dataset_item.get_class_name()\n                if class_name:\n                    predicted_class_name = encoder_manager.predict(embedding)\n                    if predicted_class_name:\n                        if class_name == predicted_class_name:\n                            encoder_accuracy[class_name] += 1\n    total_accuracy = 0\n    for class_name, accuracy in encoder_accuracy.items():\n        accuracy_percentage = accuracy / len(dataset_items)\n        print(f\"{class_name}: {accuracy_percentage:.2f}\")\n        total_accuracy += accuracy_percentage\n    print(f\"Average accuracy: {total_accuracy / len(encoder_accuracy):.2f}\")\n\n", "CodeBleu_score": 24.581549765700196}]}
{"instance_id": "e01ec69ad58a", "generation_result": [{"code": "def adjust_path_and_remove_silence(audio_path)\n    # Create output path\n    output_path = audio_path.replace(input_dir, output_dir)\n    output_dir_path = os.path.dirname(output_path)\n\n    # Check if file already exists\n    if os.path.exists(output_path):\n        if force:\n            print(\"Removing silence from\", audio_path)\n        else:\n            print(output_path, \"already exists\")\n            return output_path, False\n\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n\n    # Remove silence from audio\n    print(\"Removing silence from\", audio_path)\n    audio_data, sample_rate = librosa.load(audio_path, sr=None)\n    audio_data = librosa.effects.trim(audio_data, top_db=top_db, frame_length=frame_length, hop_length=hop_length)[0]\n    librosa.output.write_wav(output_path, audio_data, sample_rate)\n\n    # Return output path and speech detection flag\n    return output_path, True\n", "CodeBleu_score": 39.300431501631095}, {"code": "def adjust_path_and_remove_silence(audio_path)\n    # Get the output path and check if the file already exists\n    output_path = audio_path.replace(input_directory, output_directory)\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n    # Create the necessary directory structure\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    # Remove silence from the audio file\n    remove_silence(audio_path, output_path, **remove_silence_kwargs)\n    # Return the output path and a flag indicating if speech was detected\n    return output_path, True\n", "CodeBleu_score": 27.619695757349977}, {"code": "def adjust_path_and_remove_silence(audio_path)\n\n    # Get the output path\n    output_path = audio_path.replace(input_dir, output_dir)\n\n    # If the file already exists and force is not specified, return the output path and a False flag\n    if os.path.exists(output_path) and not force:\n        return output_path, False\n\n    # Create the necessary directory structure\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Load the audio\n    audio, sr = librosa.load(audio_path, sr=sampling_rate)\n\n    # Remove silence from the audio\n    audio = remove_silence(audio, sr, silence_threshold, silence_buffer)\n\n    # Save the processed audio\n    sf.write(output_path, audio, sampling_rate)\n\n    # Return the output path and a True flag\n    return output_path, True\n", "CodeBleu_score": 37.01388348852783}]}
{"instance_id": "7e0bdb0feff8", "generation_result": [{"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    # Create output directory if it doesn't exist\n    if output_dir:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n    # Get list of files to process\n    files_to_process = []\n    for root, _, files in os.walk(input_dir):\n        files_to_process += [os.path.join(root, f) for f in files if f.endswith(file_ext)]\n\n    # Resample files\n    with tqdm(total=len(files_to_process), desc=\"Resampling files\") as pbar:\n        with Parallel(n_jobs=n_jobs) as parallel:\n            parallel(delayed(resample_file)(f, output_sr, output_dir) for f in files_to_process)\n            pbar.update(len(files_to_process))\n\n    print(f\"Finished resampling {len(files_to_process)} files to {output_sr} Hz.\")\n\n", "CodeBleu_score": 33.39806750762662}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    if output_dir:\n        copy_tree(input_dir, output_dir)\n    files = find_files(input_dir, file_ext)\n    resample_files_parallel(files, output_sr, n_jobs)\n    print(f\"Resampling completed. {len(files)} files resampled.\")\n\n", "CodeBleu_score": 16.45536192701296}, {"code": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)\n    # Create output directory if it doesn't exist\n    if output_dir:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n    # Get list of files\n    files = get_files(input_dir, file_ext)\n\n    # Resample files in parallel\n    resample_files_parallel(files, output_sr, output_dir, n_jobs)\n\n    # Print completion message\n    print(f\"Resampling complete. {len(files)} files resampled.\")\n\n", "CodeBleu_score": 20.08004451974466}]}
{"instance_id": "a1351834ca57", "generation_result": [{"code": "def evaluation(model, criterion, data_loader, global_step)\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize lists to store loss and batch size\n    losses = []\n    batch_sizes = []\n\n    # Iterate over data loader\n    for batch in data_loader:\n        # Get batch data and labels\n        inputs = batch[\"image\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        # Reshape labels\n        labels = labels.reshape(-1)\n\n        # Get batch size\n        batch_size = labels.shape[0]\n\n        # Zero out optimizer\n        optimizer.zero_grad()\n\n        # Forward pass\n        with torch.no_grad():\n            outputs = model(inputs)\n\n        # Compute loss\n        loss = criterion(outputs, labels)\n\n        # Append loss and batch size\n        losses.append(loss.item())\n        batch_sizes.append(batch_size)\n\n    # Calculate average loss\n    avg_loss = sum(losses)/sum(batch_sizes)\n\n    # Log evaluation statistics\n    wandb.log({\"eval_loss\": avg_loss, \"global_step\": global_step})\n\n    # Generate and log UMAP plot\n    if global_step == 0:\n        # Get embeddings\n        embeddings = model.get_embeddings()\n\n        # Get labels\n        labels = data_loader.dataset.get_labels()\n\n        # Reshape labels\n        labels = labels.reshape(-1)\n\n        # Initialize UMAP plot\n        fig = umap.UMAP().fit_transform(embeddings, y=labels)\n\n        # Log UMAP plot\n        wandb.log({\"UMAP\": wandb.Image(fig)})\n\n    # Return average loss\n    return avg_loss\n", "CodeBleu_score": 25.684020856185224}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n    model.eval()\n    loss_sum = 0\n    n_samples = 0\n    with torch.no_grad():\n        for batch in data_loader:\n            inputs, labels = batch\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            labels = labels.view(-1, 1)\n            n_samples += inputs.shape[0]\n            labels = labels.repeat(1, inputs.shape[1]).view(-1)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss_sum += loss.item() * inputs.shape[0]\n        loss_avg = loss_sum / n_samples\n        logging.info(f\"Step {global_step}: Evaluation loss: {loss_avg:.4f}\")\n        logging.info(f\"Step {global_step}: Evaluation accuracy: {accuracy(outputs, labels):.4f}\")\n        logging.info(f\"Step {global_step}: Evaluation F1 score: {f1_score(outputs, labels):.4f}\")\n        logging.info(f\"Step {global_step}: Evaluation AUC: {roc_auc_score(outputs, labels):.4f}\")\n        logging.info(f\"Step {global_step}: Evaluation PRC: {precision_recall_curve(outputs, labels):.4f}\")\n        logging.info(f\"Step {global_step}: Evaluation ROC: {roc_curve(outputs, labels):.4f}\")\n        logging.info(f\"Step {global_step}: Evaluation Confusion matrix: {confusion_matrix(outputs, labels):.4f}\")\n        logging.info(f\"Step {global_step}: Evaluation UMAP: {umap_plot(outputs, labels, global_step):.4f}\")\n    return loss_avg\n\n", "CodeBleu_score": 30.460193444221932}, {"code": "def evaluation(model, criterion, data_loader, global_step)\n    model.eval()\n    with torch.no_grad():\n        loss_sum = 0\n        for batch_idx, (data, labels) in enumerate(data_loader):\n            data = data.to(device)\n            labels = labels.to(device)\n            labels = labels.view(-1, 1)\n            data = data.view(-1, 1, 28, 28)\n            output = model(data)\n            loss = criterion(output, labels)\n            loss_sum += loss.item()\n        loss_avg = loss_sum / len(data_loader)\n        logging.info(f'Evaluation loss at step {global_step}: {loss_avg}')\n        # Generate and log UMAP plot\n        embeddings = output.cpu().detach().numpy()\n        reducer = umap.UMAP()\n        embedding_2d = reducer.fit_transform(embeddings)\n        plt.figure(figsize=(10, 10))\n        plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=labels.cpu().detach().numpy(), cmap='Spectral', s=0.5)\n        plt.gca().set_aspect('equal', 'datalim')\n        plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n        plt.title(f'UMAP projection of the last batch at step {global_step}')\n        plt.savefig(os.path.join(output_dir, f'umap_step_{global_step}.png'))\n        logging.info(f'UMAP plot saved at step {global_step}')\n        return loss_avg\n", "CodeBleu_score": 33.09622599081976}]}
{"instance_id": "565f8ed09916", "generation_result": [{"code": "def register_config(model_name: str) -> Coqpit\n    model_name = model_name.lower()\n    if model_name == \"xtts\":\n        from.model.transformer.config import XTTSConfig as model_config\n    else:\n        raise ValueError(f\"Unknown model name: {model_name}\")\n\n    config_name = model_name + \"Config\"\n\n    paths = [\n        os.path.join(os.path.dirname(__file__), \"config\", \"common\", f\"{config_name}.py\"),\n        os.path.join(os.path.dirname(__file__), \"config\", model_name, f\"{config_name}.py\"),\n        os.path.join(os.path.dirname(__file__), \"config\", \"common\", f\"{config_name}.yaml\"),\n        os.path.join(os.path.dirname(__file__), \"config\", model_name, f\"{config_name}.yaml\"),\n    ]\n\n    for path in paths:\n        if os.path.exists(path):\n            break\n    else:\n        raise ModuleNotFoundError(f\"Cannot find configuration class for model {model_name}\")\n\n    config_class = import_class(config_name, path)\n    return config_class\n\n", "CodeBleu_score": 40.465242267340976}, {"code": "def register_config(model_name: str) -> Coqpit\n    config_class_name = model_name.capitalize() + \"Config\"\n    if model_name == \"xtts\":\n        from. import xtts_config\n\n        config_class = getattr(xtts_config, config_class_name)\n    else:\n        config_class = None\n        for path in config_paths:\n            try:\n                config_class = getattr(importlib.import_module(path), config_class_name)\n                break\n            except ModuleNotFoundError:\n                continue\n        if config_class is None:\n            raise ModuleNotFoundError(\n                f\"No configuration class found for model {model_name}\"\n            )\n    return config_class\n", "CodeBleu_score": 40.18625671218704}, {"code": "def register_config(model_name: str) -> Coqpit\n    config_name = model_name.replace(\"-\", \"_\") + \"Config\"\n    if model_name == \"xtts\":\n        config_name = \"XTTSConfig\"\n    try:\n        config_class = getattr(importlib.import_module(\"coqpit.configs\"), config_name)\n    except ModuleNotFoundError:\n        for path in config_paths:\n            try:\n                config_class = getattr(importlib.import_module(path), config_name)\n                break\n            except ModuleNotFoundError:\n                continue\n        else:\n            raise ModuleNotFoundError(\n                f\"Configuration class for {model_name} not found.\"\n            )\n    return config_class\n\n", "CodeBleu_score": 35.917486855535415}]}
{"instance_id": "0ea7862506a7", "generation_result": [{"code": "def load_config(config_path: str) -> Coqpit\n    config_path = Path(config_path)\n    config_type = config_path.suffix.lower()\n    if config_type == \".yaml\":\n        with config_path.open(\"r\") as f:\n            config_dict = yaml.load(f, Loader=yaml.FullLoader)\n    elif config_type == \".json\":\n        with config_path.open(\"r\") as f:\n            config_dict = json.load(f)\n    else:\n        raise ValueError(f\"Unknown config type: {config_type}\")\n    config_dict = _process_config_dict(config_dict)\n    config_class = _get_config_class(config_dict[\"model\"])\n    config = config_class()\n    config.update(config_dict)\n    return config\n\n", "CodeBleu_score": 41.1563519751541}, {"code": "def load_config(config_path: str) -> Coqpit\n    config_path = Path(config_path)\n    config_type = config_path.suffix\n    if config_type == \".yaml\":\n        with open(config_path) as file:\n            config = yaml.load(file, Loader=yaml.FullLoader)\n    elif config_type == \".json\":\n        with open(config_path) as file:\n            config = json.load(file)\n    else:\n        raise ValueError(f\"Unknown configuration file type: {config_type}\")\n    config = process_config(config)\n    config = create_config(config)\n    return config\n\n", "CodeBleu_score": 33.74905388637586}, {"code": "def load_config(config_path: str) -> Coqpit\n    config_path = Path(config_path)\n    if config_path.suffix == \".yaml\":\n        with open(config_path, \"r\") as stream:\n            config = yaml.safe_load(stream)\n    elif config_path.suffix == \".json\":\n        with open(config_path, \"r\") as stream:\n            config = json.load(stream)\n    else:\n        raise ValueError(f\"Unknown file type: {config_path.suffix}\")\n    model_name = config[\"model_name\"]\n    config_class = get_config_class(model_name)\n    config_instance = config_class()\n    config_instance.populate_from_dict(config)\n    return config_instance\n", "CodeBleu_score": 36.46950532295558}]}
{"instance_id": "e0db68b63143", "generation_result": [{"code": "def list_files(basePath, validExts=None, contains=None)\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n", "CodeBleu_score": 78.4028354587536}, {"code": "def list_files(basePath, validExts=None, contains=None)\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 78.4028354587536}, {"code": "def list_files(basePath, validExts=None, contains=None)\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename)\n                yield imagePath\n\n", "CodeBleu_score": 78.4028354587536}]}
{"instance_id": "b836fcaf0a27", "generation_result": [{"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    clear_gpu_memory()\n    if xtts_checkpoint is None or xtts_config is None or xtts_vocab is None:\n        print(\"Error: One or more required paths are missing.\")\n        return\n    print(\"Loading configuration from \" + xtts_config + \"...\")\n    config = XTTSConfig.from_json_file(xtts_config)\n    print(\"Initializing model...\")\n    model = XTTS(config)\n    print(\"Loading checkpoint from \" + xtts_checkpoint + \"...\")\n    model.load_state_dict(torch.load(xtts_checkpoint, map_location=\"cpu\")[\"state_dict\"])\n    print(\"Loading vocabulary from \" + xtts_vocab + \"...\")\n    model.set_vocab(XTTSVocab.load_vocabulary(xtts_vocab))\n    if torch.cuda.is_available():\n        print(\"Moving model to GPU...\")\n        model.cuda()\n    print(\"Model loaded successfully.\")\n    return model\n\n", "CodeBleu_score": 44.646388764953414}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n    clear_gpu_cache()\n    if not os.path.exists(xtts_checkpoint):\n        raise Exception(f\"The provided XTTS checkpoint path {xtts_checkpoint} does not exist.\")\n    if not os.path.exists(xtts_config):\n        raise Exception(f\"The provided XTTS config path {xtts_config} does not exist.\")\n    if not os.path.exists(xtts_vocab):\n        raise Exception(f\"The provided XTTS vocab path {xtts_vocab} does not exist.\")\n    print(\"Loading XTTS model...\")\n    config = XTTSConfig.from_json_file(xtts_config)\n    model = XTTSModel(config)\n    if torch.cuda.is_available():\n        model.cuda()\n    model.load_checkpoint(checkpoint_path=xtts_checkpoint, reset_optimizer=False, reset_lr_scheduler=False)\n    model.setup_eval_embed_weights(vocab_path=xtts_vocab)\n    print(\"XTTS model loaded.\")\n    return model\n", "CodeBleu_score": 48.373358983204284}, {"code": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)\n\n    # Clear GPU cache\n    torch.cuda.empty_cache()\n\n    # Check paths\n    if not xtts_checkpoint:\n        print(\"Please specify the path to the XTTS checkpoint.\")\n        return\n    if not xtts_config:\n        print(\"Please specify the path to the XTTS config.\")\n        return\n    if not xtts_vocab:\n        print(\"Please specify the path to the XTTS vocabulary.\")\n        return\n\n    # Load configuration\n    config = XTTSConfig.from_json_file(xtts_config)\n\n    # Initialize XTTS model\n    model = XTTS(config)\n\n    # Load checkpoint and vocabulary\n    model = load_checkpoint(model, xtts_checkpoint)\n    model.src_vocab = load_vocab(xtts_vocab)\n\n    # Move to GPU\n    if torch.cuda.is_available():\n        model.cuda()\n\n    # Print message\n    print(\"Successfully loaded XTTS model.\")\n\n    # Return success message\n    return model\n", "CodeBleu_score": 49.75325747431706}]}
{"instance_id": "c5d63aa0df26", "generation_result": [{"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    if self.use_torch_spec:\n      num_frames = int(np.ceil(num_frames / self.hop_length))\n    if num_frames > x.shape[1]:\n      num_frames = x.shape[1]\n    num_eval = min(num_eval, num_frames)\n    offsets = np.linspace(0, num_frames - 1, num_eval).astype(int)\n    x = x[:, offsets, :]\n    x = x.reshape(1, -1, x.shape[2])\n    x = self.model(x)\n    if return_mean:\n      x = x.mean(dim=1)\n      if l2_norm:\n        x = F.normalize(x, p=2, dim=1)\n    return x\n", "CodeBleu_score": 36.50112829133308}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    # Adjust the number of frames based on the hop length\n    if self.torch_spec:\n      num_frames = int(np.ceil(num_frames / self.hop_length))\n    # Ensure the number of frames does not exceed the input length\n    num_frames = min(num_frames, len(x) - self.hop_length)\n    # Calculate the offset positions for evaluation\n    eval_positions = np.linspace(0, len(x) - self.hop_length, num=num_eval, endpoint=False, dtype=int)\n    # Extract frames at these offsets\n    frames = [x[pos:pos + num_frames * self.hop_length] for pos in eval_positions]\n    # Concatenate the frames into a batch\n    batch = np.concatenate(frames, axis=0)\n    # Perform inference to obtain embeddings\n    embeddings = self.inference(batch)\n    # Compute the mean of the embeddings\n    if return_mean:\n      embeddings = np.mean(embeddings, axis=0)\n      # Optionally normalize the embeddings with L2 norm\n      if l2_norm:\n        embeddings /= np.linalg.norm(embeddings)\n    # Return the result\n    return embeddings\n", "CodeBleu_score": 38.55280245233521}, {"code": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)\n    if self.use_torch_spec:\n      num_frames = int(np.ceil(num_frames / self.hop_length))\n    num_frames = min(num_frames, x.shape[1])\n    num_eval = min(num_eval, num_frames - 1)\n    eval_offset = np.linspace(0, num_frames - 1, num_eval, dtype=int)\n    eval_frames = [x[:, i : i + 1] for i in eval_offset]\n    eval_batch = torch.cat(eval_frames, dim=1)\n    eval_batch = eval_batch.to(self.device)\n    with torch.no_grad():\n      z = self.model(eval_batch).cpu()\n    if l2_norm:\n      z = F.normalize(z, p=2, dim=1)\n    if return_mean:\n      return z.mean(dim=1)\n    else:\n      return z\n", "CodeBleu_score": 39.92026808973999}]}
{"instance_id": "06f5b0335b26", "generation_result": [{"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n", "CodeBleu_score": 85.54080921158837}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n      downsample = nn.Sequential(\n        nn.Conv2d(self.inplanes, planes * block.expansion,\n                  kernel_size=1, stride=stride, bias=False),\n        nn.BatchNorm2d(planes * block.expansion),\n      )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n      layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n\n  def forward(self, x):\n    \"\"\"The function defines the forward pass of the network. It consists of the stem and the layers.\"\"\"\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.maxpool(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n\n    x = self.avgpool(x)\n    x = x.view(x.size(0), -1)\n    x = self.fc(x)\n\n    return x\n\n", "CodeBleu_score": 78.10371039293295}, {"code": "def create_layer(self, block, planes, blocks, stride=1)\n    downsample = None\n    if stride!= 1 or self.inplanes!= planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion,\n                      kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n\n    return nn.Sequential(*layers)\n", "CodeBleu_score": 85.54080921158837}]}
{"instance_id": "9ac60597b633", "generation_result": [{"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    num_classes = min(num_classes_in_batch, 10)\n    colors = plt.cm.rainbow(np.linspace(0, 1, num_classes))\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(1, 1, 1)\n    for i in range(num_classes):\n        indices = np.where(embeddings.labels == i)\n        ax.scatter(embeddings.embedding[indices, 0], embeddings.embedding[indices, 1], c=colors[i], label=i)\n    ax.set_title('UMAP projection')\n    ax.axis('equal')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    plt.savefig('umap.png')\n    return fig\n", "CodeBleu_score": 24.68178800489359}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    num_classes = min(num_classes_in_batch, 10)\n    num_utterances_per_class = []\n    for i in range(num_classes):\n        num_utterances_per_class.append(len(np.where(embeddings[:, 0] == i)[0]))\n    num_utterances_per_class = np.array(num_utterances_per_class)\n    num_utterances_per_class = num_utterances_per_class / np.sum(num_utterances_per_class)\n    num_utterances_per_class = num_utterances_per_class * 100\n    num_utterances_per_class = num_utterances_per_class.astype(int)\n    num_utterances_per_class = [str(i) + \" (\" + str(j) + \"%)\" for i, j in zip(range(num_classes), num_utterances_per_class)]\n    num_utterances_per_class = \", \".join(num_utterances_per_class)\n    embeddings = embeddings[:, 1:]\n    reducer = umap.UMAP(random_state=42)\n    embeddings_transformed = reducer.fit_transform(embeddings)\n    fig, ax = plt.subplots()\n    ax.scatter(embeddings_transformed[:, 0], embeddings_transformed[:, 1], c=embeddings[:, 0], cmap=\"Spectral\", s=0.5)\n    ax.set_aspect(\"equal\", \"datalim\")\n    ax.set_title(\"UMAP projection\", fontsize=18)\n    ax.set_xlabel(\"UMAP 1\", fontsize=14)\n    ax.set_ylabel(\"UMAP 2\", fontsize=14)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.text(0.05, 0.95, \"Classes: \" + num_utterances_per_class, transform=ax.transAxes, fontsize=12, va=\"top\")\n    fig.savefig(\"umap.png\")\n    return fig\n", "CodeBleu_score": 23.818734627606595}, {"code": "def plot_embeddings(embeddings, num_classes_in_batch)\n    num_classes = min(num_classes_in_batch, 10)\n    num_utterances_per_class = np.array([np.sum(y==i) for i in range(num_classes)])\n    num_utterances_per_class = num_utterances_per_class/np.sum(num_utterances_per_class)\n    reducer = umap.UMAP(random_state=42)\n    embeddings_2d = reducer.fit_transform(embeddings)\n    fig, ax = plt.subplots(figsize=(10,10))\n    ax.scatter(embeddings_2d[:,0], embeddings_2d[:,1], c=y, s=num_utterances_per_class*200, alpha=0.5)\n    ax.set_title(\"UMAP projection\", fontsize=16)\n    ax.set_aspect('equal')\n    plt.savefig(\"umap.png\")\n    return fig\n", "CodeBleu_score": 27.38447492516658}]}
{"instance_id": "19c2b4af256d", "generation_result": [{"code": "def calc_cosine_sim(self, dvecs, centroids)\n    cosine_sim = []\n    for i in range(len(dvecs)):\n        centroids_new = []\n        for j in range(len(centroids)):\n            if i!= j:\n                centroids_new.append(centroids[j])\n        centroids_new = np.array(centroids_new)\n        centroids_new = centroids_new.mean(axis=0)\n        centroids_new = centroids_new.reshape(1, centroids_new.shape[0])\n        sim = []\n        for j in range(len(dvecs[i])):\n            sim.append(cosine_similarity(dvecs[i][j].reshape(1, dvecs[i][j].shape[0]), centroids_new))\n        sim = np.array(sim)\n        sim = np.clip(sim, 1e-6, 1)\n        cosine_sim.append(sim)\n    cosine_sim = np.concatenate(cosine_sim, axis=0)\n    return cosine_sim\n", "CodeBleu_score": 36.09485420683922}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n    cosine_sim = []\n    for i, speaker in enumerate(dvecs):\n        cosine_sim.append([])\n        for j, utterance in enumerate(speaker):\n            # Calculate the cosine similarity between the current utterance and the centroids\n            centroids_new = []\n            for k, centroid in enumerate(centroids):\n                if k!= i:\n                    centroids_new.append(centroid)\n            centroids_new = np.array(centroids_new)\n            centroids_new = centroids_new.reshape(centroids_new.shape[0], -1)\n            cosine_sim_utterance = np.dot(utterance, centroids_new.T)\n            cosine_sim_utterance = cosine_sim_utterance / (np.linalg.norm(utterance) * np.linalg.norm(centroids_new, axis=1))\n            cosine_sim_utterance = np.clip(cosine_sim_utterance, -1, 1)\n            cosine_sim[i].append(cosine_sim_utterance)\n        cosine_sim[i] = np.concatenate(cosine_sim[i])\n    cosine_sim = np.stack(cosine_sim)\n    return cosine_sim\n\n", "CodeBleu_score": 35.67486164774065}, {"code": "def calc_cosine_sim(self, dvecs, centroids)\n    # Initialize the cosine similarity matrix\n    cos_sim = torch.zeros(dvecs.size(0), centroids.size(0))\n    # Iterate over each speaker\n    for i in range(dvecs.size(0)):\n        # Calculate the centroids excluding the current utterance\n        centroids_excl = centroids.clone()\n        centroids_excl[i] = 0\n        # Iterate over each utterance\n        for j in range(dvecs.size(1)):\n            # Calculate the cosine similarity between the current utterance and the new centroids\n            cos_sim[i, j] = F.cosine_similarity(dvecs[i, j], centroids_excl, dim=0)\n    # Clamp the similarity values\n    cos_sim = torch.clamp(cos_sim, min=0.0000001)\n    # Concatenate the similarity values and stack them to form the final cosine similarity matrix\n    cos_sim = torch.cat(cos_sim.unbind(), dim=0)\n    cos_sim = torch.stack(cos_sim.unbind(), dim=0)\n    # Return the final cosine similarity matrix\n    return cos_sim\n", "CodeBleu_score": 21.957876424238215}]}
{"instance_id": "c525b411a1e7", "generation_result": [{"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n    losses = []\n    for dvec in dvecs:\n      losses.append(-tf.nn.softmax(cos_sim_matrix(dvec)))\n    return tf.stack(losses)\n\n  def embed_loss_contrastive(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.\\nIt iterates over the embeddings, computing the contrastive loss of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    losses = []\n    for dvec in dvecs:\n      losses.append(tf.nn.relu(self.margin - cos_sim_matrix(dvec)))\n    return tf.stack(losses)\n\n  def embed_loss_triplet(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using triplet loss.\\nIt iterates over the embeddings, computing the triplet loss of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    losses = []\n    for dvec in dvecs:\n      losses.append(tf.nn.relu(cos_sim_matrix(dvec) - self.margin))\n    return tf.stack(losses)\n\n  def embed_loss_triplet_semihard(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using semihard triplet loss.\\nIt iterates over the embeddings, computing the semihard triplet loss of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    losses = []\n    for dvec in dvecs:\n      losses.append(tf.nn.relu(self.margin + cos_sim_matrix(dvec)))\n    return tf.stack(losses)\n\n  def embed_loss_triplet_hard(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using hard triplet loss.\\nIt iterates over the embeddings, computing the hard triplet loss of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    losses = []\n    for dvec in dvecs:", "CodeBleu_score": 28.62307538419157}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n    losses = []\n    for dvec in dvecs:\n      # Calculate the negative log softmax of the cosine similarity matrix for each embedding\n      neg_log_softmax = tf.nn.log_softmax(-cos_sim_matrix[dvec], axis=1)\n      # Calculate the loss for each row\n      loss = tf.reduce_sum(neg_log_softmax, axis=1)\n      # Stack the losses for each row into a tensor\n      losses.append(loss)\n    # Stack the losses for each embedding into a tensor\n    losses = tf.stack(losses)\n    # Return the final stacked tensor of losses\n    return losses\n\n  def embed_loss_contrastive(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.\\nIt iterates over the embeddings, computing the contrastive loss of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    losses = []\n    for dvec in dvecs:\n      # Calculate the contrastive loss of the cosine similarity matrix for each embedding\n      loss = tf.reduce_sum(tf.square(tf.subtract(tf.ones_like(cos_sim_matrix[dvec]), tf.cast(cos_sim_matrix[dvec] > 0, tf.float32))))\n      # Stack the losses for each row into a tensor\n      losses.append(loss)\n    # Stack the losses for each embedding into a tensor\n    losses = tf.stack(losses)\n    # Return the final stacked tensor of losses\n    return losses\n\n  def embed_loss_triplet(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using triplet loss.\\nIt iterates over the embeddings, computing the triplet loss of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    losses = []\n    for dvec in dvecs:\n      # Calculate the triplet loss of the cosine similarity matrix for each embedding\n      loss = tf.reduce_sum(tf.maximum(tf.subtract(tf.ones_like(cos_sim_matrix[dvec]), tf.cast(cos_sim_matrix[dvec] > 0, tf.float32)), 0))\n      #", "CodeBleu_score": 30.103671216354687}, {"code": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)\n    # TODO: Implement this function.\n    # Hint: you can use the torch.nn.functional.cross_entropy function to implement this.\n    # You may find torch.sum and torch.mean useful.\n    loss_stack = []\n    for i in range(len(dvecs)):\n      loss_stack.append(torch.nn.functional.cross_entropy(cos_sim_matrix[i], torch.zeros(cos_sim_matrix[i].shape[0], dtype=torch.long).cuda()))\n    return torch.stack(loss_stack)\n\n  def embed_loss_triplet(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using triplet loss.\\nIt iterates over the embeddings, computing the triplet loss for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    # TODO: Implement this function.\n    # Hint: you can use the torch.nn.functional.triplet_margin_loss function to implement this.\n    # You may find torch.sum and torch.mean useful.\n    loss_stack = []\n    for i in range(len(dvecs)):\n      loss_stack.append(torch.nn.functional.triplet_margin_loss(cos_sim_matrix[i], torch.zeros(cos_sim_matrix[i].shape[0], dtype=torch.long).cuda(), margin=self.margin))\n    return torch.stack(loss_stack)\n\n  def embed_loss_contrastive(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrastive loss.\\nIt iterates over the embeddings, computing the contrastive loss for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    # TODO: Implement this function.\n    # Hint: you can use the torch.nn.functional.triplet_margin_loss function to implement this.\n    # You may find torch.sum and torch.mean useful.\n    loss_stack = []\n    for i in range(len(dvecs)):\n      loss_stack.append(torch.nn.functional.triplet_margin_loss(cos_sim_matrix[i], torch.zeros(cos_sim_matrix[i].shape[0], dtype=torch.", "CodeBleu_score": 36.34117088940799}]}
{"instance_id": "25ac3945da2e", "generation_result": [{"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    loss_row = []\n    for i in range(len(dvecs)):\n      cos_sim_row = cos_sim_matrix[i]\n      for j in range(len(dvecs)):\n        if j!= i:\n          cos_sim_row[j] = 0\n      cos_sim_row = torch.sigmoid(cos_sim_row)\n      loss_row.append(cos_sim_row)\n    loss_row = torch.stack(loss_row)\n    return loss_row\n", "CodeBleu_score": 28.33618281531149}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    # Calculate the sigmoid of the cosine similarity matrix\n    sigmoid_matrix = torch.sigmoid(cos_sim_matrix)\n    # Initialize the loss tensor\n    loss = torch.empty(0, device=self.device)\n    # Iterate over the embeddings\n    for dvec in dvecs:\n      # Exclude the current centroid\n      sigmoid_matrix_ex = torch.cat((torch.zeros(1, self.num_centroids, device=self.device), sigmoid_matrix[1:]), dim=0)\n      # Calculate the contrast loss\n      loss_row = torch.sum(torch.pow(sigmoid_matrix_ex - sigmoid_matrix, 2), dim=0)\n      # Append the loss row to the loss tensor\n      loss = torch.cat((loss, loss_row), dim=0)\n    return loss\n\n  def embed_loss_contrast_2(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrast loss with the closest centroid.\\nIt iterates over the embeddings, computing the sigmoid of the cosine similarity matrix for each embedding.\\nFor each embedding, it excludes the current centroid, computes the contrast loss, and appends the result to the loss row.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.\"\"\"\n    # Calculate the sigmoid of the cosine similarity matrix\n    sigmoid_matrix = torch.sigmoid(cos_sim_matrix)\n    # Initialize the loss tensor\n    loss = torch.empty(0, device=self.device)\n    # Iterate over the embeddings\n    for dvec in dvecs:\n      # Exclude the current centroid\n      sigmoid_matrix_ex = torch.cat((torch.zeros(1, self.num_centroids, device=self.device), sigmoid_matrix[1:]), dim=0)\n      # Calculate the contrast loss\n      loss_row = torch.sum(torch.pow(sigmoid_matrix_ex - sigmoid_matrix, 2), dim=0)\n      # Append the loss row to the loss tensor\n      loss = torch.cat((loss, loss_row), dim=0)\n    return loss\n\n  def embed_loss_contrast_3(self, dvecs, cos_sim_matrix):\n    \"\"\"The function calculates the embedding loss using contrast loss with the closest centroid.\\nIt iterates over the embeddings", "CodeBleu_score": 28.083355652396634}, {"code": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)\n    loss = []\n    for dvec in dvecs:\n        # compute the sigmoid of the cosine similarity matrix\n        sigmoid_sim = torch.sigmoid(cos_sim_matrix)\n        # exclude the current centroid\n        sigmoid_sim[dvec] = 0\n        # compute the contrast loss\n        loss.append(self.contrast_loss(sigmoid_sim))\n    return torch.stack(loss)\n", "CodeBleu_score": 16.05197939645182}]}
{"instance_id": "878d14ffabd2", "generation_result": [{"code": "def forward(self, x, _label=None)\n    # Check if input is valid\n    if len(x.shape)!= 3:\n      raise ValueError(\"Input tensor must have dimensions (num_speakers, num_utts_per_speaker, dvec_feats).\")\n    if x.shape[1] < 2:\n      raise ValueError(\"Input tensor must have at least two utterances per speaker.\")\n\n    # Calculate the centroids\n    centroids = torch.mean(x, dim=1)\n\n    # Calculate the cosine similarity matrix\n    sim_mat = F.cosine_similarity(x, centroids.unsqueeze(1), dim=2)\n\n    # Scale and shift the similarity matrix\n    scaled_sim_mat = (self.scale * sim_mat) + self.shift\n\n    # Calculate the loss\n    if _label is not None:\n      loss = F.cross_entropy(scaled_sim_mat, _label)\n    else:\n      loss = F.cross_entropy(scaled_sim_mat)\n\n    # Return the mean loss\n    return loss\n", "CodeBleu_score": 36.27169892041737}, {"code": "def forward(self, x, _label=None)\n    # input shape: (num_speakers, num_utts_per_speaker, dvec_feats)\n    # output shape: (num_speakers, num_utts_per_speaker, num_speakers)\n    # label shape: (num_speakers, num_utts_per_speaker)\n    # label_mask shape: (num_speakers, num_utts_per_speaker)\n    # label_mask_sum shape: (num_speakers)\n    # label_mask_sum_bool shape: (num_speakers)\n    # label_mask_sum_bool_unsqueeze shape: (num_speakers, 1)\n    # label_mask_sum_bool_unsqueeze_expand shape: (num_speakers, num_speakers)\n    # label_mask_sum_bool_unsqueeze_expand_repeat shape: (num_speakers, num_speakers)\n    # label_mask_sum_unsqueeze shape: (num_speakers, 1)\n    # label_mask_sum_unsqueeze_expand shape: (num_speakers, num_speakers)\n    # label_mask_unsqueeze shape: (num_speakers, num_utts_per_speaker, 1)\n    # label_mask_unsqueeze_expand shape: (num_speakers, num_utts_per_speaker, num_speakers)\n    # label_mask_unsqueeze_expand_repeat shape: (num_speakers, num_utts_per_speaker, num_speakers)\n    # label_unsqueeze shape: (num_speakers, num_utts_per_speaker, 1)\n    # label_unsqueeze_expand shape: (num_speakers, num_utts_per_speaker, num_speakers)\n    # label_unsqueeze_expand_repeat shape: (num_speakers, num_utts_per_speaker, num_speakers)\n    # mask shape: (num_speakers, num_utts_per_speaker, num_speakers)\n    # mask_bool shape: (num_speakers, num_utts_per_speaker, num_speakers)\n    # mask_bool_sum shape: (num_speakers, num_utts_per_speaker)\n    # mask_bool_sum_bool shape: (num_speakers", "CodeBleu_score": 3.6585365853658542}, {"code": "def forward(self, x, _label=None)\n    x = x.view(-1, x.size(-1))\n    if self.normalize_embeddings:\n      x = F.normalize(x, p=2, dim=1)\n    centroids = x.mean(dim=0, keepdim=True)\n    x = x.transpose(0, 1)\n    sim_matrix = torch.matmul(x, centroids)\n    sim_matrix = self.scale * (sim_matrix - self.shift)\n    sim_matrix = torch.exp(sim_matrix)\n    if _label is not None:\n      label = _label.view(-1)\n      mask = (label.unsqueeze(0) == label.unsqueeze(1)).float()\n      mask = mask * (1 - torch.eye(mask.size(0))).to(x.device)\n      sim_matrix = sim_matrix * mask\n    sim_sum = torch.sum(sim_matrix, dim=1, keepdim=True)\n    sim_sum = torch.max(sim_sum, torch.ones_like(sim_sum))\n    sim_matrix = sim_matrix / sim_sum\n    sim_matrix = sim_matrix.transpose(0, 1)\n    loss = -torch.log(torch.diag(sim_matrix)).mean()\n    return loss\n\n", "CodeBleu_score": 34.924732641702924}]}
{"instance_id": "86a364c72caa", "generation_result": [{"code": "def get_voices(extra_voice_dirs: List[str] = [])oice_dict = {}\n    for voice_dir in extra_voice_dirs:\n        voice_dict[voice_dir] = []\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voice_dict[voice_dir].append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voice_dict[voice_dir].append(os.path.join(root, file))\n    return voice_dict\n\n", "CodeBleu_score": 28.797409965215504}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])oice_files = {}\n    for voice_dir in [VOICE_DIR] + extra_voice_dirs:\n        voice_files[voice_dir] = []\n        for root, dirs, files in os.walk(voice_dir):\n            for file in files:\n                if file.endswith(\".npz\"):\n                    voice_files[voice_dir].append(os.path.join(root, file))\n                elif file.endswith(\".wav\") or file.endswith(\".mp3\"):\n                    voice_files[voice_dir].append(os.path.join(root, file))\n    return voice_files\n", "CodeBleu_score": 28.499228285115862}, {"code": "def get_voices(extra_voice_dirs: List[str] = [])oice_files = {}\n    for directory in extra_voice_dirs:\n        for root, subdirs, files in os.walk(directory):\n            if len(files) > 0:\n                voice_files[root] = []\n                for file in files:\n                    if file.endswith('.npz'):\n                        voice_files[root].append(os.path.join(root, file))\n                    elif file.endswith('.wav') or file.endswith('.mp3'):\n                        voice_files[root].append(os.path.join(root, file))\n    return voice_files\n\n", "CodeBleu_score": 29.6178305013855}]}
{"instance_id": "02f20c8917e6", "generation_result": [{"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])   voice_paths = get_voices(model, voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(\n            f\"There are multiple voices with the same name ({voice}). Please specify the voice name with the full path.\"\n        )\n    elif len(voice_paths) == 0:\n        raise ValueError(f\"There are no voices with the name {voice}.\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        return load_npz_voice(voice_path)\n    else:\n        return load_audio_voice(voice_path)\n\n", "CodeBleu_score": 28.067524648383134}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])   voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(f\"Multiple voices with the name {voice} found in {extra_voice_dirs}.\")\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        return np.load(voice_path)\n    elif voice_path.endswith(\".wav\"):\n        voice_data = generate_voice(model, voice_path)\n        return voice_data\n    else:\n        raise ValueError(f\"Invalid voice file extension {voice_path}.\")\n\n", "CodeBleu_score": 28.501822208627104}, {"code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])   if voice == \"random\":\n        return None\n    voice_paths = get_voices(voice, extra_voice_dirs)\n    if len(voice_paths) > 1:\n        raise ValueError(\n            f\"Multiple voices found for {voice}. Please specify a directory.\"\n        )\n    voice_path = voice_paths[0]\n    if voice_path.endswith(\".npz\"):\n        voice_data = np.load(voice_path)\n        if not all(\n            key in voice_data\n            for key in [\"mel\", \"lin\", \"pitch\", \"energy\", \"duration\", \"f0\"]\n        ):\n            raise ValueError(\n                f\"Voice {voice} does not contain all necessary data. Please check {voice_path}.\"\n            )\n        return voice_data\n    else:\n        if not os.path.exists(voice_path):\n            raise ValueError(f\"Voice {voice} does not exist.\")\n        if not os.path.isdir(voice_path):\n            voice_path = os.path.dirname(voice_path)\n        return generate_voice(model, voice_path)\n\n", "CodeBleu_score": 42.66074591848903}]}
{"instance_id": "9458e981389f", "generation_result": [{"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    # Extract the tensor's dimensions\n    batch_size, seq_len, _ = pos_score.shape\n    # Create a zero tensor with an additional dimension\n    zero_tensor = torch.zeros(batch_size, seq_len, 1, device=self.device)\n    # Concatenate the zero tensor with the original position score tensor along the last dimension\n    pos_score = torch.cat([zero_tensor, pos_score], dim=2)\n    # Reshape and slice the position score tensor to achieve the relative shift\n    pos_score = pos_score.reshape(batch_size, seq_len + 1, seq_len)[:, 1:, :]\n    return pos_score\n\n  def _calculate_attention_weights(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"The function calculates the attention weights.\\nThe function first calculates the attention score, then applies the mask and softmax function to the attention score to get the attention weights.\\nThe function returns the attention weights and the attention score.\"\"\"\n    # Calculate the attention score\n    attention_score = torch.matmul(query, key.transpose(1, 2))\n    # Apply the mask\n    if mask is not None:\n      attention_score = attention_score.masked_fill(mask == 0, -1e9)\n    # Apply the softmax function to the attention score to get the attention weights\n    attention_weights = torch.softmax(attention_score / self.temperature, dim=-1)\n    return attention_weights, attention_score\n\n  def _calculate_context_vector(self, attention_weights: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n    \"\"\"The function calculates the context vector.\\nThe function first multiplies the attention weights with the value tensor to get the context vector.\\nThe function returns the context vector.\"\"\"\n    # Multiply the attention weights with the value tensor to get the context vector\n    context_vector = torch.matmul(attention_weights, value)\n    return context_vector\n\n  def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"The function calculates the output of the multi-", "CodeBleu_score": 31.110954110282584}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    pos_score_dims = pos_score.size()\n    pos_score_zero = torch.zeros(pos_score_dims[0], pos_score_dims[1], 1)\n    pos_score_concat = torch.cat((pos_score_zero, pos_score), 2)\n    pos_score_concat_dims = pos_score_concat.size()\n    pos_score_concat_reshape = pos_score_concat.reshape(\n        pos_score_concat_dims[0], pos_score_concat_dims[1], pos_score_concat_dims[2] * pos_score_concat_dims[3]\n    )\n    pos_score_concat_slice = pos_score_concat_reshape[:, :, 1:]\n    return pos_score_concat_slice\n\n", "CodeBleu_score": 22.344466176263673}, {"code": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor\n    # Extract tensor dimensions\n    batch_size = pos_score.shape[0]\n    seq_len = pos_score.shape[1]\n    # Create a zero tensor with an additional dimension\n    zero_tensor = torch.zeros(batch_size, seq_len, 1, device=pos_score.device)\n    # Concatenate the zero tensor with the original position score tensor along the last dimension\n    pos_score_shifted = torch.cat([zero_tensor, pos_score], dim=2)\n    # Reshape and slice the concatenated tensor to achieve the relative shift\n    pos_score_shifted = pos_score_shifted.reshape(batch_size, seq_len + 1, seq_len)[:, 1:, :]\n    return pos_score_shifted\n\n", "CodeBleu_score": 27.502800508847976}]}
{"instance_id": "c046d4f00e80", "generation_result": [{"code": "def run_padded_sequence(self, context, lens)\n    # Extract the context up to the specified length\n    context = context[:, :lens[0], :]\n    # Initialize the processed context\n    context_processed = torch.zeros(context.shape[0], context.shape[1], self.out_channels).to(context.device)\n    # Apply the convolutional layers\n    for i in range(self.num_layers):\n        # Apply the convolutional layer\n        context_processed += F.relu(self.conv_layers[i](context))\n        # Apply the dropout\n        context_processed = F.dropout(context_processed, p=self.dropout, training=self.training)\n    # Pad the processed context to create a consistent batch size\n    context_processed = pad_sequence(context_processed, batch_first=True, padding_value=0)\n    return context_processed\n", "CodeBleu_score": 31.849426984936674}, {"code": "def run_padded_sequence(self, context, lens)\n    # Initialize the result\n    result = []\n    # For each batch element\n    for batch_element, length in zip(context, lens):\n        # Extract the context up to the specified length\n        context_element = batch_element[:length]\n        # Apply the convolutional layers\n        for conv in self.convs:\n            context_element = F.relu(conv(context_element))\n        # Store the results\n        result.append(context_element)\n    # Pad the results\n    result = pad_sequence(result, batch_first=True)\n    # Return the padded sequence\n    return result\n\n", "CodeBleu_score": 24.06210222165698}, {"code": "def run_padded_sequence(self, context, lens)\n    # The function processes a padded sequence through a series of convolutional layers with ReLU activation and dropout.\n    # For each batch element, it extracts the context up to the specified length, applies the convolutional layers, and stores the results.\n    # The processed contexts are then padded to create a consistent batch size, and the function returns the padded sequence.\n    # Input:\n    #   context: the padded sequence of shape (batch_size, padded_length, embedding_size).\n    #   lens: the length of each sequence in the batch (batch_size,).\n    # Output:\n    #   output: the processed padded sequence of shape (batch_size, padded_length, embedding_size).\n    output = self.conv_layers(context)\n    # The function returns the processed padded sequence.\n    return output\n", "CodeBleu_score": 1.6805900644411849}]}
{"instance_id": "db2644f3c681", "generation_result": [{"code": "def run_unsorted_inputs(self, fn, context, lens)\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context.index_select(0, sorted_indices)\n    sorted_context = pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    sorted_context, _ = fn(sorted_context)\n    sorted_context, _ = pad_packed_sequence(sorted_context, batch_first=True)\n    _, unsorted_indices = torch.sort(sorted_indices)\n    return sorted_context.index_select(0, unsorted_indices)\n", "CodeBleu_score": 16.827113199540218}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n    # sort inputs by length\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    # pack and unpack sequences\n    packed_context = pack_padded_sequence(context[sorted_indices], sorted_lens, batch_first=True)\n    unpacked_context, _ = pad_packed_sequence(packed_context, batch_first=True)\n    # run the function\n    unpacked_context = fn(unpacked_context)\n    # pack and unpack sequences\n    packed_context = pack_padded_sequence(unpacked_context, sorted_lens, batch_first=True)\n    unpacked_context, _ = pad_packed_sequence(packed_context, batch_first=True)\n    # map results back to original indices\n    context = unpacked_context[sorted_indices]\n    return context\n", "CodeBleu_score": 17.56716344275833}, {"code": "def run_unsorted_inputs(self, fn, context, lens)\n    sorted_lens, sorted_indices = torch.sort(lens, descending=True)\n    sorted_context = context[sorted_indices]\n    sorted_context = pack_padded_sequence(sorted_context, sorted_lens, batch_first=True)\n    sorted_context, _ = fn(sorted_context)\n    sorted_context, _ = pad_packed_sequence(sorted_context, batch_first=True)\n    unsorted_context = sorted_context[torch.argsort(sorted_indices)]\n    return unsorted_context\n", "CodeBleu_score": 18.67863334257234}]}
{"instance_id": "ced082bffc63", "generation_result": [{"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n      block.remove_weight_norm()\n", "CodeBleu_score": 33.218347036122}, {"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n        block.remove_weight_norm()\n", "CodeBleu_score": 33.51046275391997}, {"code": "def remove_weight_norm(self)\n    self.kernel_predictor.remove_weight_norm()\n    self.convt_pre.remove_weight_norm()\n    for block in self.conv_blocks:\n      block.remove_weight_norm()\n\n  def forward(self, x, y=None, sigma=None, sigma_only=False, reverse=False):\n    \"\"\"The function computes the forward pass of the network.\\n\n    Args:\n      x (Tensor): the input tensor to the network.\n      y (Tensor): the target tensor to the network.\n      sigma (Tensor): the variance tensor to the network.\n      sigma_only (bool): whether to only return the variance.\n      reverse (bool): whether to perform the reverse pass.\n    Returns:\n      Tensor: the output tensor of the network.\n    \"\"\"\n    if not reverse:\n      if self.training:\n        if self.use_weight_norm:\n          self.apply_weight_norm()\n        if self.use_spectral_norm:\n          self.apply_spectral_norm()\n        if self.use_equalized_learning_rate:\n          self.apply_equalized_learning_rate()\n      else:\n        if self.use_weight_norm:\n          self.apply_weight_norm()\n        if self.use_spectral_norm:\n          self.apply_spectral_norm()\n        if self.use_equalized_learning_rate:\n          self.apply_equalized_learning_rate()\n    else:\n      if self.use_weight_norm:\n        self.remove_weight_norm()\n      if self.use_spectral_norm:\n        self.remove_spectral_norm()\n      if self.use_equalized_learning_rate:\n        self.remove_equalized_learning_rate()\n    if not sigma_only:\n      x = self.convt_pre(x)\n      x = self.conv_blocks[0](x)\n      x = self.conv_blocks[1](x)\n      x = self.conv_blocks[2](x)\n      x = self.conv_blocks[3](x)\n      x = self.conv_blocks[4](x)\n      x = self.conv_blocks[5](x)\n      x = self.conv_blocks[6](x)\n      x = self.conv_blocks[7](x)\n      x = self.conv_blocks[8](x", "CodeBleu_score": 33.525249144438284}]}
{"instance_id": "b5a39fd97061", "generation_result": [{"code": "def map_mx_to_torch_model(mx_name)   # convert to lowercase\n    mx_name = mx_name.lower()\n    # replace prefixes\n    mx_name = mx_name.replace('se_','senet_')\n    mx_name = mx_name.replace('senet_','senet_')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    # prefix with gluon_\n    mx_name = 'gluon_' + mx_name\n    return mx_name\n\n", "CodeBleu_score": 37.132703483663335}, {"code": "def map_mx_to_torch_model(mx_name)   mx_name = mx_name.lower()\n    mx_name = mx_name.replace('se_','senet_')\n    mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    return 'gluon_' + mx_name\n\n", "CodeBleu_score": 25.17173300277124}, {"code": "def map_mx_to_torch_model(mx_name)   # Convert the input name to lowercase\n    mx_name = mx_name.lower()\n    # Apply specific replacements for'se_','senet_', and 'inceptionv3' prefixes\n    if mx_name.startswith('se_'):\n        mx_name = mx_name.replace('se_','senet_')\n    elif mx_name.startswith('senet_'):\n        mx_name = mx_name.replace('senet_','senet154_')\n    elif mx_name.startswith('inceptionv3'):\n        mx_name = mx_name.replace('inceptionv3', 'inception_v3')\n    # Prefix the name with 'gluon_'\n    mx_name = 'gluon_' + mx_name\n    return mx_name\n\n", "CodeBleu_score": 48.62612715262544}]}
{"instance_id": "7eee5efdbf67", "generation_result": [{"code": "def _num_samples_per_worker(self)\n    # Calculate the number of samples per worker\n    if self.num_replicas > 1:\n      if self.repeats > 1:\n        num_samples_per_worker = math.ceil(self.total_samples / self.num_replicas / self.repeats)\n      else:\n        num_samples_per_worker = math.ceil(self.total_samples / self.num_replicas)\n    else:\n      num_samples_per_worker = self.total_samples\n\n    # Adjust the number of samples to be a multiple of the batch size\n    if self.batch_size is not None:\n      num_samples_per_worker = math.ceil(num_samples_per_worker / self.batch_size) * self.batch_size\n\n    # Return the number of samples as an integer\n    return int(num_samples_per_worker)\n\n  def _get_data_loader(self, dataset, shuffle=True, num_workers=None, pin_memory=False, drop_last=False):\n    \"\"\"The function returns a data loader for the specified dataset.\\nThe data loader is configured to:\\n- use the specified dataset\\n- shuffle the data\\n- use the specified number of workers\\n- pin the memory if specified\\n- drop the last batch if specified\"\"\"\n    # Set the number of workers\n    if num_workers is None:\n      num_workers = self.num_workers\n\n    # Return the data loader\n    return torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n\n  def _get_dataset(self, data, labels, transform=None, target_transform=None):\n    \"\"\"The function returns a dataset for the specified data and labels.\\nThe dataset is configured to:\\n- use the specified data and labels\\n- apply the specified transform and target transform\"\"\"\n    # Return the dataset\n    return torch.utils.data.TensorDataset(data, labels)\n\n  def _get_transform(self, transform):\n    \"\"\"The function returns a transform for the specified transform.\\nThe transform is configured to:\\n- use the specified transform\"\"\"\n    # Return the transform\n    return transform\n\n  def _get_target_transform(self, target_transform):\n    \"\"\"The function returns a target", "CodeBleu_score": 41.39806394753418}, {"code": "def _num_samples_per_worker(self)\n    if self.num_workers == 0:\n      return self.num_samples\n    if self.num_workers > self.num_samples:\n      self.num_workers = self.num_samples\n    samples_per_worker = self.num_samples // self.num_workers\n    if self.num_samples % self.num_workers!= 0:\n      samples_per_worker += 1\n    if self.batch_size > 0:\n      samples_per_worker = samples_per_worker - samples_per_worker % self.batch_size\n    return samples_per_worker\n\n  def _get_data_from_file(self, file_name):\n    \"\"\"The function reads data from a file.\"\"\"\n    data = []\n    with open(file_name, 'r') as f:\n      for line in f:\n        data.append(line.strip())\n    return data\n\n  def _get_data_from_dir(self, dir_name):\n    \"\"\"The function reads data from a directory.\"\"\"\n    data = []\n    for file_name in os.listdir(dir_name):\n      data += self._get_data_from_file(os.path.join(dir_name, file_name))\n    return data\n\n  def _get_data(self):\n    \"\"\"The function reads data from a file or a directory.\"\"\"\n    if os.path.isfile(self.data_dir):\n      return self._get_data_from_file(self.data_dir)\n    elif os.path.isdir(self.data_dir):\n      return self._get_data_from_dir(self.data_dir)\n    else:\n      raise ValueError('The data directory is not a file or a directory.')\n\n  def _get_data_size(self):\n    \"\"\"The function returns the number of samples in the data.\"\"\"\n    return len(self.data)\n\n  def _get_data_shape(self):\n    \"\"\"The function returns the shape of the data.\"\"\"\n    return self.data_shape\n\n  def _get_data_type(self):\n    \"\"\"The function returns the type of the data.\"\"\"\n    return self.data_type\n\n  def _get_data_format(self):\n    \"\"\"The function returns the format of the data.\"\"\"\n    return self.data_format\n\n  def _get_data_classes(self):\n    \"\"\"The function returns the classes of the data", "CodeBleu_score": 26.552766509124904}, {"code": "def _num_samples_per_worker(self)\n    if self.num_replicas > 1:\n      num_samples_per_worker = math.ceil(self.total_samples / self.num_replicas)\n    else:\n      num_samples_per_worker = self.total_samples\n    if self.batch_size is not None:\n      num_samples_per_worker = int(\n          math.ceil(num_samples_per_worker / self.batch_size)) * self.batch_size\n    return num_samples_per_worker\n\n  def _num_workers(self):\n    \"\"\"The function calculates the number of workers.\\nIf training or multiple replicas are used, it rounds up the number of workers.\\nIf a batch size is specified during training, it adjusts the number of workers to be a multiple of the batch size.\\nFinally, it returns the calculated number of workers as an integer.\"\"\"\n    if self.num_replicas > 1:\n      num_workers = math.ceil(self.num_replicas / self.num_workers_per_replica)\n    else:\n      num_workers = self.num_replicas\n    if self.batch_size is not None:\n      num_workers = int(math.ceil(num_workers / self.batch_size)) * self.batch_size\n    return num_workers\n\n  def _num_samples_per_epoch(self):\n    \"\"\"The function calculates the number of samples in an epoch.\\nIt computes the initial number of samples per epoch based on the repeats, total samples, and the number of workers or replicas.\\nIf training or multiple replicas are used, it rounds up the number of samples.\\nIf a batch size is specified during training, it adjusts the number of samples to be a multiple of the batch size.\\nFinally, it returns the calculated number of samples as an integer.\"\"\"\n    if self.num_replicas > 1:\n      num_samples_per_epoch = math.ceil(self.total_samples / self.num_replicas)\n    else:\n      num_samples_per_epoch = self.total_samples\n    if self.batch_size is not None:\n      num_samples_per_epoch = int(\n          math.ceil(num_samples_per_epoch / self.batch_size)) * self.batch_size\n    return num_samples_per_epoch\n\n  def _num_epochs(self):\n    \"\"\"The function calculates the number", "CodeBleu_score": 35.58580452197966}]}
{"instance_id": "a698d65ae190", "generation_result": [{"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n    if types is None:\n        types = (\".jpg\", \".jpeg\", \".png\")\n    if class_to_idx is None:\n        class_to_idx = {}\n    images = []\n    targets = []\n    for root, _, files in os.walk(folder, topdown=False):\n        for filename in files:\n            if filename.lower().endswith(types):\n                path = os.path.join(root, filename)\n                if leaf_name_only:\n                    path = os.path.basename(path)\n                label = os.path.relpath(root, folder)\n                if label not in class_to_idx:\n                    class_to_idx[label] = len(class_to_idx)\n                target = class_to_idx[label]\n                images.append(path)\n                targets.append(target)\n    if sort:\n        images = [x for _, x in sorted(zip(targets, images))]\n        targets = [x for x in sorted(targets)]\n    return images, targets, class_to_idx\n\n", "CodeBleu_score": 37.996570976244804}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n    if types is None:\n        types = ('jpg', 'jpeg', 'png', 'bmp', 'gif')\n    if class_to_idx is None:\n        class_to_idx = dict()\n    images = []\n    targets = []\n    for root, _, fnames in sorted(os.walk(folder)):\n        for fname in sorted(fnames):\n            if fname.lower().endswith(types):\n                if leaf_name_only:\n                    label = os.path.split(root)[-1]\n                else:\n                    label = os.path.relpath(root, folder)\n                if label not in class_to_idx:\n                    class_to_idx[label] = len(class_to_idx)\n                images.append(os.path.join(root, fname))\n                targets.append(class_to_idx[label])\n    if sort:\n        images = [x for _, x in sorted(zip(targets, images))]\n        targets = sorted(targets)\n    return images, targets, class_to_idx\n\n", "CodeBleu_score": 38.45577049034681}, {"code": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)\n    if types is None:\n        types = ('.jpg', '.png')\n    if class_to_idx is None:\n        class_to_idx = {}\n    images = []\n    targets = []\n    for root, _, fnames in sorted(os.walk(folder)):\n        for fname in sorted(fnames):\n            if fname.lower().endswith(types):\n                path = os.path.join(root, fname)\n                target = path.split(os.sep)[-2] if leaf_name_only else path\n                if target not in class_to_idx:\n                    class_to_idx[target] = len(class_to_idx)\n                item = (path, class_to_idx[target])\n                images.append(item)\n                targets.append(class_to_idx[target])\n    if sort:\n        images = sorted(images)\n        targets = sorted(targets)\n    return images, targets, class_to_idx\n\n", "CodeBleu_score": 35.281598988672705}]}
{"instance_id": "e7fe43bdecdc", "generation_result": [{"code": "def pytorch_worker_seed()\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        seed = 0\n    else:\n        seed = worker_info.seed\n    return seed\n\n", "CodeBleu_score": 32.15401926104778}, {"code": "def pytorch_worker_seed()\n    try:\n        seed = torch.utils.data.get_worker_info().seed\n    except Exception:\n        seed = torch.initial_seed() % 2 ** 32\n    return seed\n\n", "CodeBleu_score": 35.54233446204408}, {"code": "def pytorch_worker_seed()\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is not None:\n        seed = worker_info.seed % 2**32\n    else:\n        seed = torch.initial_seed() % 2**32\n    return seed\n\n", "CodeBleu_score": 40.59576696063154}]}
{"instance_id": "0ffc7e7ded25", "generation_result": [{"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode in [\"L\", \"RGB\"]:\n        lut = []\n        for i in range(256):\n            if i < thresh:\n                lut.append(min(255, add + i))\n            else:\n                lut.append(i)\n        if img.mode == \"L\":\n            img = img.point(lut)\n        else:\n            img = img.point(lut, 'RGB')\n    return img\n\n", "CodeBleu_score": 54.77901828717552}, {"code": "def solarize_add(img, add, thresh=128, **__)\n    return cv2.add(img, add, mask=cv2.threshold(img, thresh, 255, cv2.THRESH_BINARY)[1])\n\n", "CodeBleu_score": 11.94025066506928}, {"code": "def solarize_add(img, add, thresh=128, **__)\n    if img.mode == \"L\":\n        lut = np.array(list(range(256)))\n        lut[lut < thresh] = lut[lut < thresh] + add\n        lut[lut >= thresh] = lut[lut >= thresh] + add\n        return img.point(lut)\n    elif img.mode == \"RGB\":\n        lut = np.array(list(range(256)) * 3)\n        lut[lut < thresh] = lut[lut < thresh] + add\n        lut[lut >= thresh] = lut[lut >= thresh] + add\n        return img.point(lut.reshape(256, 3))\n    else:\n        return img\n\n", "CodeBleu_score": 33.90316720605132}]}
{"instance_id": "5a0d29577a21", "generation_result": [{"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names = list(transforms.keys())\n    probabilities = list(transforms.values())\n    probabilities = [prob / sum(probabilities) for prob in probabilities]\n    return transform_names, probabilities\n\n", "CodeBleu_score": 21.606895139862843}, {"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names = list(transforms.keys())\n    transform_probs = list(transforms.values())\n    transform_probs = [prob / sum(transform_probs) for prob in transform_probs]\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 20.865611314023923}, {"code": "def _get_weighted_transforms(transforms: Dict)\n    transform_names = []\n    transform_probs = []\n    for transform, prob in transforms.items():\n        transform_names.append(transform)\n        transform_probs.append(prob)\n    transform_probs = np.array(transform_probs)\n    transform_probs /= np.sum(transform_probs)\n    return transform_names, transform_probs\n\n", "CodeBleu_score": 34.99120925899825}]}
{"instance_id": "84bc5658c135", "generation_result": [{"code": "def _apply_basic(self, img, mixing_weights, m)\n    # initialize the mixed image as a copy of the original image\n    mixed_image = img.copy()\n    # loop over the weight values\n    for i in range(0, len(mixing_weights)):\n        # generate a random sequence of operations\n        operations = self._generate_random_sequence()\n        # apply a random sequence of operations to the image\n        for (op, weight) in operations:\n            img = op(img, weight)\n        # clip and convert the image back to the original data type\n        img = img.clip(0, 255).astype(np.uint8)\n        # blend the input image with the image generated by the operations\n        mixed_image = cv2.addWeighted(img, m, mixed_image, 1.0 - m, 0)\n    # return the mixed image\n    return mixed_image\n", "CodeBleu_score": 22.715067487965708}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n    # initialize the output image as a copy of the input image\n    mixed_image = img.copy()\n    # iterate through the given mixing weights\n    for i in range(m):\n        # initialize an augmented image as a copy of the original image\n        augmented_image = img.copy()\n        # apply a random sequence of augmentations to the image\n        for _ in range(self.n_ops):\n            op = np.random.choice(self.augmentations)\n            augmented_image = op(augmented_image)\n        # blend the augmented image with the original image\n        mixed_image = self.blend_func(mixed_image, augmented_image, mixing_weights[i])\n    # clip the mixed image to valid pixel values\n    mixed_image = np.clip(mixed_image, 0, 1)\n    # convert the mixed image from a NumPy array to a PyTorch tensor\n    mixed_image = TF.to_tensor(mixed_image)\n    # blend the original image with the mixed image\n    blended_image = self.blend_func(img, mixed_image, self.alpha)\n    # return the blended image\n    return blended_image\n", "CodeBleu_score": 29.51947405677321}, {"code": "def _apply_basic(self, img, mixing_weights, m)\n    mixed_image = np.zeros_like(img)\n    for mw in mixing_weights:\n        aug_img = self.augmentation_pipeline.augment_image(img)\n        aug_img = aug_img.astype(np.float32)\n        mixed_image += mw * aug_img\n    mixed_image = np.clip(mixed_image, 0, 255).astype(np.uint8)\n    return (1 - m) * img + m * mixed_image\n\n", "CodeBleu_score": 23.82665517685797}]}
{"instance_id": "84b5b752ff04", "generation_result": [{"code": "def adapt_to_chs(x, n)\n    if not isinstance(x, (tuple, list)):\n        x = (x,)*n\n    elif len(x)!= n:\n        x = (np.mean(x),)*n\n        warnings.warn(\"The input x has been averaged to match the required number of channels n.\")\n    else:\n        assert len(x) == n\n    return x\n", "CodeBleu_score": 41.83881045070558}, {"code": "def adapt_to_chs(x, n)\n    if isinstance(x, (tuple, list)):\n        if len(x) == n:\n            assert len(x) == n\n        elif len(x) == 1:\n            x = tuple(x * n)\n        else:\n            warnings.warn(f\"Length of x ({len(x)}) does not match the required number of channels n ({n}). The length of x is adjusted to match n by repeating the mean of x.\")\n            x = tuple(np.mean(x) * np.ones(n))\n    else:\n        x = tuple(x * n)\n    return x\n", "CodeBleu_score": 37.79759114000972}, {"code": "def adapt_to_chs(x, n)\n    if isinstance(x, tuple) or isinstance(x, list):\n        if len(x) == n:\n            assert len(x) == n\n        elif len(x) == 1:\n            x = [x[0]] * n\n            warnings.warn(f\"The input x was a list or tuple of length 1, so it was repeated {n} times to match the required number of channels n={n}.\")\n        else:\n            x = [np.mean(x)] * n\n            warnings.warn(f\"The input x was a list or tuple of length {len(x)}, so it was averaged to match the required number of channels n={n}.\")\n    else:\n        x = [x] * n\n    return tuple(x)\n", "CodeBleu_score": 34.14185010448014}]}
{"instance_id": "54754b6ba394", "generation_result": [{"code": "def _worker_init(worker_id, worker_seeding='all')\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise ValueError(\"The seeding can only be used in a worker environment\")\n    worker_id = worker_info.id\n    worker_count = worker_info.num_workers\n    worker_seed = worker_info.seed\n    worker_seed = worker_seed % 4294967295\n    worker_info.seed = worker_seed\n    worker_info.worker_id = worker_id\n    worker_info.worker_count = worker_count\n    worker_info.dataset = worker_info.dataset.dataset\n    worker_info.dataset.worker_init(worker_info)\n    if worker_seeding == 'all':\n        torch.manual_seed(worker_seed)\n        np.random.seed(worker_seed)\n    elif callable(worker_seeding):\n        worker_seeding(worker_seed)\n    elif worker_seeding == 'numpy':\n        np.random.seed(worker_seed)\n    else:\n        raise ValueError(\"The worker_seeding parameter must be either 'all', 'numpy', or a callable.\")\n\n", "CodeBleu_score": 36.6043846519388}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        raise ValueError(\"This function only works with multiprocessing.Pool\")\n    else:\n        worker_id = worker_info.id\n        worker_count = worker_info.num_workers\n        worker_seed = worker_info.seed\n        worker_seed = worker_seeding(worker_id, worker_count, worker_seed)\n        worker_info.seed = worker_seed\n        np.random.seed(worker_seed)\n        torch.manual_seed(worker_seed)\n        random.seed(worker_seed)\n\n", "CodeBleu_score": 42.37985050623426}, {"code": "def _worker_init(worker_id, worker_seeding='all')\n    worker_info = torch.utils.data.get_worker_info()\n    if worker_info is None:\n        worker_id = 0\n    else:\n        worker_id = worker_info.id\n    if worker_seeding is not None:\n        if callable(worker_seeding):\n            worker_seeding(worker_id)\n        elif isinstance(worker_seeding, str):\n            if worker_seeding == 'all':\n                np.random.seed(worker_info.seed % (2**32 - 1))\n            else:\n                raise ValueError(\"worker_seeding must be either None, callable, or 'all'\")\n    return worker_id\n\n", "CodeBleu_score": 41.49377931924704}]}
{"instance_id": "81dfdce940db", "generation_result": [{"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    # Decode the image\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    # Crop the image to the specified size\n    image = tf.image.resize(image, [image_size, image_size], method=resize_method)\n    return image\n\n", "CodeBleu_score": 3.827247781810944}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width, padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], method=resize_method)[0]\n    return image\n\n", "CodeBleu_score": 62.3275224884387}, {"code": "def _decode_and_center_crop(image_bytes, image_size, resize_method)\n    # Decode the image\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    # Get the image shape\n    image_shape = tf.shape(image)\n    # Get the image height and width\n    image_height = image_shape[0]\n    image_width = image_shape[1]\n    # Calculate the padded center crop size\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n        tf.cast(tf.minimum(image_height, image_width), tf.float32)), tf.int32)\n    # Calculate the offset for the padded center crop\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    # Crop the image\n    image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, padded_center_crop_size, padded_center_crop_size)\n    # Resize the image\n    image = tf.image.resize([image], [image_size, image_size], method=resize_method)[0]\n    return image\n", "CodeBleu_score": 50.706260081470134}]}
{"instance_id": "cf16a6d37111", "generation_result": [{"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.io.decode_image(image_bytes, channels=3)\n    image = tf.image.random_crop(image, size=(image_size, image_size, 3))\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.resize(image, size=(image_size, image_size), method=interpolation)\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n", "CodeBleu_score": 41.268566929039764}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    image = tf.io.decode_image(image_bytes, channels=3)\n    image = tf.image.random_crop(image, [image_size, image_size, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n", "CodeBleu_score": 42.42170620678286}, {"code": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    # 1. Decode the image\n    image = tf.io.decode_jpeg(image_bytes, channels=3)\n    # 2. Resize and crop the image\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n    image = tf.image.random_crop(image, [image_size, image_size, 3])\n    # 3. Randomly flip the image\n    image = tf.image.random_flip_left_right(image)\n    # 4. Rescale to [-1, 1]\n    image = image * 2. / 255. - 1.\n    # 5. Convert to the desired data type\n    if use_bfloat16:\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.cast(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 37.48308660200803}]}
{"instance_id": "78b2a3bd2142", "generation_result": [{"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    if interpolation not in ('bilinear', 'bicubic'):\n        raise ValueError('`interpolation` must be either \\'bilinear\\' or \\'bicubic\\'')\n\n    # Decode the jpeg image to a uint8 tensor.\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n\n    # Resize the image to the specified height and width.\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n\n    # Crop the central region of the image with an area containing 87.5% of the original image.\n    image = tf.image.central_crop(image, central_fraction=CROP_FRACTION)\n\n    # Rescale to [-1,1] instead of [0, 1)\n    image = image * 2.0 - 1.0\n\n    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    # Resize and crop the image to the specified height and width.\n    image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n\n    # Crop the central region of the image with an area containing 87.5% of the original image.\n    image = tf.image.central_crop(image, central_fraction=CROP_FRACTION)\n\n    # Rescale to [-1,1] instead of [0, 1)\n    image = image * 2.0 - 1.0\n\n    if use_bfloat16:\n        # Convert the image to bfloat16 dtype.\n        image = tf.image.convert_image_dtype(image, dtype=tf.bfloat16)\n    return image\n\n", "CodeBleu_score": 40.63741896144959}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    if use_bfloat16:\n        image = tf.io.decode_and_crop_jpeg(image_bytes, crop_window=_CROP_PADDING)\n        image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n        image = tf.cast(image, dtype=tf.bfloat16)\n    else:\n        image = tf.io.decode_and_crop_jpeg(image_bytes, crop_window=_CROP_PADDING)\n        image = tf.image.resize(image, [image_size, image_size], method=interpolation)\n        image = tf.cast(image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 42.46745600516978}, {"code": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')\n    decoded_image = tf.io.decode_image(image_bytes, channels=3)\n    decoded_image = tf.image.central_crop(decoded_image, central_fraction=CENTRAL_FRACTION)\n    if interpolation == 'bicubic':\n        resized_image = tf.image.resize(decoded_image, [image_size, image_size], method=tf.image.ResizeMethod.BICUBIC)\n    else:\n        resized_image = tf.image.resize(decoded_image, [image_size, image_size], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    if use_bfloat16:\n        image = tf.image.convert_image_dtype(resized_image, dtype=tf.bfloat16)\n    else:\n        image = tf.image.convert_image_dtype(resized_image, dtype=tf.float32)\n    return image\n\n", "CodeBleu_score": 41.84061008228808}]}
{"instance_id": "8bd4364a9a0e", "generation_result": [{"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    # If no dataloader is specified, use the default one\n    if eval_dataloader is None:\n      eval_dataloader = self.eval_dataloader\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize the prediction loop\n    self.prediction_loop(model, eval_dataloader, description=\"Evaluation\")\n\n    # Save the predictions\n    self.save_predictions(output_file)\n\n    # Compute the evaluation metrics\n    return self.compute_metrics()\n\n  def predict(self, model, predict_dataloader=None, output_file=\"predictions\"):\n    \"\"\"The function predicts using a specified or default prediction dataloader.\\nIt runs a prediction loop with the model and dataloader, saving the outputs to a file.\\nIt then returns the predictions.\"\"\"\n    # If no dataloader is specified, use the default one\n    if predict_dataloader is None:\n      predict_dataloader = self.predict_dataloader\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Initialize the prediction loop\n    self.prediction_loop(model, predict_dataloader, description=\"Prediction\")\n\n    # Save the predictions\n    self.save_predictions(output_file)\n\n    # Return the predictions\n    return self.predictions\n\n  def save_predictions(self, output_file=\"predictions\"):\n    \"\"\"The function saves the predictions to a file.\"\"\"\n    # Save the predictions to a file\n    if self.predictions is not None:\n      with open(output_file, \"w\") as f:\n        for prediction in self.predictions:\n          f.write(f\"{prediction}\\n\")\n\n  def compute_metrics(self):\n    \"\"\"The function computes evaluation metrics based on the predictions and returns the results.\"\"\"\n    # If no predictions are available, return None\n    if self.predictions is None:\n      return None\n\n    # Compute the evaluation metrics\n    return self.metric.compute(predictions=self.predictions)\n\n  def prediction_loop(self, model, dataloader, description=\"Prediction\"):\n    \"\"\"The function runs a prediction loop with the model and dataloader.\"\"\"\n    # Initialize the prediction loop\n    self.initialize_prediction_loop(model)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Initialize the progress bar\n    self.initialize_progress_bar(dataloader, description=description)\n\n    # Initialize the predictions\n    self.initialize_predictions()\n\n    # Loop over the batches\n    for batch in tqdm(dataloader, desc=description):\n      # Make", "CodeBleu_score": 42.734917474618406}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    if eval_dataloader is None:\n      eval_dataloader = self.eval_dataloader\n    if eval_dataloader is None:\n      raise ValueError(\"You must pass an evaluation dataloader to the model when calling `evaluate()`\")\n    self.eval()\n    self.to(self.device)\n    model.to(self.device)\n    self.compute_metrics = True\n    eval_output = self.prediction_loop(model, eval_dataloader)\n    self.compute_metrics = False\n    if output_file is not None:\n      with open(output_file, \"w\") as writer:\n        writer.write(json.dumps(eval_output) + \"\\n\")\n    return eval_output\n\n  def prediction_loop(self, model, dataloader):\n    \"\"\"The function loops over the dataloader, running the model and computing metrics on the predictions.\"\"\"\n    model.eval()\n    eval_dataloader = dataloader\n    eval_output = []\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n      model_inputs = self.prepare_inputs(batch)\n      with torch.no_grad():\n        model_outputs = model(**model_inputs)\n      if self.compute_metrics:\n        eval_output.append(self.compute_metrics_fn(model_outputs, batch))\n    return eval_output\n\n  def prepare_inputs(self, inputs):\n    \"\"\"The function prepares the inputs to be passed to the model.\"\"\"\n    for k, v in inputs.items():\n      if isinstance(v, torch.Tensor):\n        inputs[k] = v.to(self.device)\n    return inputs\n\n  def compute_metrics_fn(self, model_outputs, batch):\n    \"\"\"The function computes metrics based on the model outputs and the batch.\"\"\"\n    if self.compute_metrics:\n      raise NotImplementedError(\"You must implement `compute_metrics_fn` for your task.\")\n    else:\n      raise ValueError(\"You must set `compute_metrics` to `True` to use `compute_metrics_fn`.\")\n\n  def get_metrics(self, eval_output):\n    \"\"\"The function computes the metrics from the evaluation output.\"\"\"\n    if self.compute_metrics:\n      raise ValueError(\"You must set `compute_metrics` to `False` to use `get_metrics`.\")\n    else:\n      raise NotImplementedError(\"You must implement `get_metrics` for your task.\")\n\n  def save_model", "CodeBleu_score": 40.05358163201854}, {"code": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")\n    # set model to evaluation mode\n    model.eval()\n    # if no dataloader is specified, use the default one\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    # if no output file is specified, use the default one\n    if output_file is None:\n        output_file = self.output_file\n    # initialize the results dictionary\n    results = {}\n    # initialize the predictions list\n    predictions = []\n    # initialize the true labels list\n    true_labels = []\n    # initialize the loss list\n    loss_list = []\n    # initialize the batch index\n    batch_idx = 0\n    # iterate over the dataloader\n    for batch in eval_dataloader:\n        # send batch to device\n        batch = {k: v.to(self.device) for k, v in batch.items()}\n        # forward pass\n        with torch.no_grad():\n            outputs = model(**batch)\n        # get loss\n        loss = outputs.loss\n        # get predictions\n        logits = outputs.logits\n        # get true labels\n        labels = batch[\"labels\"]\n        # append loss to loss list\n        loss_list.append(loss.item())\n        # append predictions to predictions list\n        predictions.append(logits.detach().cpu().numpy())\n        # append true labels to true labels list\n        true_labels.append(labels.detach().cpu().numpy())\n        # increment batch index\n        batch_idx += 1\n    # convert loss list to array\n    loss_list = np.array(loss_list)\n    # convert predictions list to array\n    predictions = np.concatenate(predictions, axis=0)\n    # convert true labels list to array\n    true_labels = np.concatenate(true_labels, axis=0)\n    # compute evaluation metrics\n    results[\"loss\"] = np.mean(loss_list)\n    results[\"accuracy\"] = accuracy_score(true_labels, np.argmax(predictions, axis=1))\n    results[\"precision\"] = precision_score(true_labels, np.argmax(predictions, axis=1), average=\"macro\")\n    results[\"recall\"] = recall_score(true_labels, np.argmax(predictions, axis=1), average=\"macro\")\n    results[\"f1\"] = f1_score(true_labels, np.argmax(predictions, axis=1), average=\"macro\")\n    # save predictions to file\n    np.save(output_file,", "CodeBleu_score": 40.2259837373266}]}
{"instance_id": "22d682c958c0", "generation_result": [{"code": " def print_computed_metrics(self, metrics)\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    miou = metrics['miou']\n    print('Rank1: {0:.4f}\\nRank5: {1:.4f}\\nMIoU: {2:.4f}'.format(rank1, rank5, miou))\n\n  def print_computed_metrics_no_miou(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    rank1 = metrics['rank1']\n    rank5 = metrics['rank5']\n    print('Rank1: {0:.4f}\\nRank5: {1:.4f}'.format(rank1, rank5))\n\n  def print_computed_metrics_no_miou_no_rank5(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    rank1 = metrics['rank1']\n    print('Rank1: {0:.4f}'.format(rank1))\n\n  def print_computed_metrics_no_miou_no_rank1(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    rank5 = metrics['rank5']\n    print('Rank5: {0:.4f}'.format(rank5))\n\n  def print_computed_metrics_no_miou_no_rank1_no_rank5(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    print('No metrics to print.')\n\n  def print_computed_metrics_no_miou_no_rank1_no_rank5_no_rank10(self, metrics):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    print('No metrics to print.')\n\n  def print_computed_metrics_no_miou_no", "CodeBleu_score": 34.02542182734901}, {"code": " def print_computed_metrics(self, metrics)\n    if metrics is not None:\n      print(f\"Rank1: {metrics['rank1']:.4f}\")\n      print(f\"Rank5: {metrics['rank5']:.4f}\")\n      print(f\"mIoU: {metrics['miou']:.4f}\")\n\n  def print_metrics_table(self, metrics_list):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    if metrics_list is not None:\n      print(f\"Rank1: {metrics_list[0]:.4f} | Rank5: {metrics_list[1]:.4f} | mIoU: {metrics_list[2]:.4f}\")\n\n  def compute_metrics(self, pred_image, gt_image, n_classes):\n    \"\"\"The function computes the metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    pred_image = pred_image.cpu().numpy()\n    gt_image = gt_image.cpu().numpy()\n    pred_image = np.argmax(pred_image, axis=0)\n    gt_image = np.argmax(gt_image, axis=0)\n    pred_image = pred_image.flatten()\n    gt_image = gt_image.flatten()\n    metrics = {}\n    metrics[\"rank1\"] = np.mean(pred_image == gt_image)\n    metrics[\"rank5\"] = np.mean([pred_image[:5] == gt_image[:5]])\n    metrics[\"miou\"] = np.mean([self.compute_iou(pred_image, gt_image, n_classes)])\n    return metrics\n\n  def compute_iou(self, pred_image, gt_image, n_classes):\n    \"\"\"The function computes the intersection over union metric for the given images.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    iou_list = []\n    for i in range(n_classes):\n      pred_mask = pred_image == i\n      gt_mask = gt_image == i\n      intersection = np.logical_and(pred_mask, gt_mask).sum()\n      union = np.logical_or(pred_", "CodeBleu_score": 27.095307815635728}, {"code": " def print_computed_metrics(self, metrics)\n    print(\"Rank1: {:.4f}\\nRank5: {:.4f}\\nMIOU: {:.4f}\".format(metrics[\"rank1\"], metrics[\"rank5\"], metrics[\"miou\"]))\n\n  def print_computed_metrics_for_epoch(self, metrics, epoch):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    print(\"Epoch: {}\\nRank1: {:.4f}\\nRank5: {:.4f}\\nMIOU: {:.4f}\".format(epoch, metrics[\"rank1\"], metrics[\"rank5\"], metrics[\"miou\"]))\n\n  def print_computed_metrics_for_epoch_and_phase(self, metrics, epoch, phase):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    print(\"Epoch: {}\\nPhase: {}\\nRank1: {:.4f}\\nRank5: {:.4f}\\nMIOU: {:.4f}\".format(epoch, phase, metrics[\"rank1\"], metrics[\"rank5\"], metrics[\"miou\"]))\n\n  def print_computed_metrics_for_epoch_and_phase_and_model(self, metrics, epoch, phase, model):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    print(\"Epoch: {}\\nPhase: {}\\nModel: {}\\nRank1: {:.4f}\\nRank5: {:.4f}\\nMIOU: {:.4f}\".format(epoch, phase, model, metrics[\"rank1\"], metrics[\"rank5\"], metrics[\"miou\"]))\n\n  def print_computed_metrics_for_epoch_and_phase_and_model_and_dataset(self, metrics, epoch, phase, model, dataset):\n    \"\"\"The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.\"\"\"\n    print(\"Epoch: {}\\nPhase: {}\\nModel: {}\\nDataset: {}\\nRank1: {:.4f}\\nRank5: {:.4f}\\nMIOU: {:.", "CodeBleu_score": 24.56844179021883}]}
{"instance_id": "39c1a50b7e7d", "generation_result": [{"code": "def _eval_predictions(self, segments, data)\n    rank1 = 0\n    rank5 = 0\n    iou_sum = 0\n    for segment, data in zip(segments, data):\n        iou = segment.iou(data)\n        iou_sum += iou\n        if segment.rank(data) == 1:\n            rank1 += 1\n        if segment.rank(data) <= 5:\n            rank5 += 1\n    return rank1/len(segments), rank5/len(segments), iou_sum/len(segments)\n", "CodeBleu_score": 20.674117619670557}, {"code": "def _eval_predictions(self, segments, data)\n    ious = []\n    ranks = []\n    for segment, data_item in zip(segments, data):\n        iou = self._eval_iou(segment, data_item)\n        rank = self._eval_rank(segment, data_item)\n        ious.append(iou)\n        ranks.append(rank)\n    ious = np.array(ious)\n    ranks = np.array(ranks)\n    rank1 = np.sum(ranks == 1) / len(ranks)\n    rank5 = np.sum(ranks <= 5) / len(ranks)\n    mean_iou = np.mean(ious)\n    return rank1, rank5, mean_iou\n\n", "CodeBleu_score": 28.93997397619117}, {"code": "def _eval_predictions(self, segments, data)\n\n    # Calculate IoU and rank for each segment-data pair\n    ious = []\n    ranks = []\n    for segment, data_ in zip(segments, data):\n        iou, rank = self._eval_prediction(segment, data_)\n        ious.append(iou)\n        ranks.append(rank)\n\n    # Calculate average IoU and rank\n    ious = np.array(ious)\n    ranks = np.array(ranks)\n    rank1 = np.mean(ranks == 1)\n    rank5 = np.mean(ranks <= 5)\n    mean_iou = np.mean(ious)\n\n    return rank1, rank5, mean_iou\n\n  def _eval_prediction(self, segment, data):\n    \"\"\"The function evaluates a single prediction segment against ground truth data.\\nIt calculates IoU and rank for the top prediction.\\nThe function returns IoU and rank.\"\"\"\n\n    # Calculate IoU and rank for the top prediction\n    ious = []\n    for data_ in data:\n      iou = self._calculate_iou(segment, data_)\n      ious.append(iou)\n    ious = np.array(ious)\n    rank = np.argmax(ious) + 1\n\n    return ious[rank - 1], rank\n\n  def _calculate_iou(self, segment, data):\n    \"\"\"The function calculates intersection over union (IoU) between a prediction segment and ground truth data.\\nIt returns IoU.\"\"\"\n\n    # Calculate IoU\n    intersection = np.sum(np.logical_and(segment, data))\n    union = np.sum(np.logical_or(segment, data))\n    iou = intersection / union\n\n    return iou\n\n  def _calculate_iou_matrix(self, segments, data):\n    \"\"\"The function calculates intersection over union (IoU) between prediction segments and ground truth data.\\nIt returns a matrix of IoUs.\"\"\"\n\n    # Calculate IoU\n    iou_matrix = np.zeros((len(segments), len(data)))\n    for i, segment in enumerate(segments):\n      for j, data_ in enumerate(data):\n        intersection = np.sum(np.logical_and(segment, data_))\n        union = np.sum(np.logical_or(segment, data_))\n        iou = intersection / union\n        iou_matrix[i, j] = iou\n\n    return iou_matrix\n\n  def _calculate_rank_", "CodeBleu_score": 34.18043396988025}]}
{"instance_id": "8ccf84f8d034", "generation_result": [{"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n\n    # Initialize the empty list for scores\n    scores = []\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Transfer the model to GPU 0\n    model.to(self.device)\n\n    # Iterate over the dataloader\n    for batch in eval_dataloader:\n\n        # Transfer data to the appropriate context\n        batch = tuple(t.to(self.device) for t in batch)\n\n        # Make predictions without computing gradients\n        with torch.no_grad():\n            inputs = {'input_ids': batch[0],\n                      'attention_mask': batch[1],\n                      'token_type_ids': batch[2] if self.args.model_type in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n                      'labels': batch[3]}\n            outputs = model(**inputs)\n            _, logits = outputs[:2]\n\n        # Update the scores list\n        scores.append(logits.detach().cpu().numpy())\n\n    # Finalize and return the results\n    scores = np.concatenate(scores, axis=0)\n    if output_file is not None:\n        np.save(output_file, scores)\n    return scores\n", "CodeBleu_score": 32.22904675825365}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    # Initialize the list of scores\n    scores = []\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Transfer the model to GPU 0\n    model.cuda(0)\n\n    # Iterate over the dataloader\n    for batch in eval_dataloader:\n        # Transfer the data to the appropriate context\n        batch = tuple(t.cuda(0) for t in batch)\n\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n\n        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions\n            outputs = model(b_input_ids, token_type_ids=None,\n                            attention_mask=b_input_mask)\n\n        # Get the \"logits\" output by the model. The \"logits\" are the output values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n\n        # Accumulate the total accuracy.\n        scores.append(tmp_eval_accuracy)\n\n    # Calculate the average accuracy and set it as the value of the \"acc\" key\n    # in the output dictionary.\n    eval_accuracy = np.mean(scores)\n\n    # Set the \"acc\" key in the output dictionary\n    output_dict = {'acc': eval_accuracy}\n\n    # If an output file was specified, save the output dictionary to it\n    if output_file is not None:\n        with open(output_file, 'w') as f:\n            json.dump(output_dict, f)\n\n    # Return the output dictionary, containing the evaluation accuracy\n    return output_dict\n", "CodeBleu_score": 34.090786772065435}, {"code": "def predict_loop(self, model, eval_dataloader, output_file=None)\n    model.eval()\n    model.to(0)\n    scores = []\n    for batch in eval_dataloader:\n        batch = {k: v.to(0) for k, v in batch.items()}\n        with torch.no_grad():\n            output = model(**batch)\n        scores.append(output)\n    scores = self.finalize_scores(scores)\n    if output_file is not None:\n        with open(output_file, \"w\") as f:\n            f.write(\"\\n\".join(scores))\n    return scores\n", "CodeBleu_score": 39.129403468191455}]}
{"instance_id": "e68529ad727d", "generation_result": [{"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2, 'There should be exactly two sets of scores.'\n    video_scores, text_scores = scores\n    video_scores = torch.cat(video_scores, dim=0)\n    text_scores = torch.cat(text_scores, dim=0)\n    self.full_scores = None\n    return torch.dot(text_scores, video_scores)\n\n", "CodeBleu_score": 30.787744291221653}, {"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2, \"There should be exactly two sets of scores.\"\n    assert len(scores[0]) == len(scores[1]), \"There should be the same number of scores in each set.\"\n    assert len(scores[0][0]) == len(scores[1][0]), \"There should be the same number of scores in each set.\"\n    assert len(scores[0][0][0]) == len(scores[1][0][0]), \"There should be the same number of scores in each set.\"\n    assert len(scores[0][0][0][0]) == len(scores[1][0][0][0]), \"There should be the same number of scores in each set.\"\n    assert len(scores[0][0][0][0][0]) == len(scores[1][0][0][0][0]), \"There should be the same number of scores in each set.\"\n    assert len(scores[0][0][0][0][0][0]) == len(scores[1][0][0][0][0][0]), \"There should be the same number of scores in each set.\"\n    assert len(scores[0][0][0][0][0][0][0]) == len(scores[1][0][0][0][0][0][0]), \"There should be the same number of scores in each set.\"\n    assert len(scores[0][0][0][0][0][0][0][0]) == len(scores[1][0][0][0][0][0][0][0]), \"There should be the same number of scores in each set.\"\n    assert len(scores[0][0][0][0][0][0][0][0][0]) == len(scores[1][0][0][0][0][0][0][0][0]), \"There should be the same number of scores in each set.\"\n    assert len(scores[0][0][0][0][0][0][0][0][0][0]) == len(scores[1][0][0][0][0][0][0][0][0][0]), \"There should be the same number of scores in each set.\"\n    assert len(scores[0][0][0][0][0][0][0][0][0][0][0]) == len(scores", "CodeBleu_score": 9.546936713361859}, {"code": "def _aggregate_scores(self, scores)\n    assert len(scores) == 2, \"scores should be a list of two elements.\"\n    self.full_scores = np.concatenate(scores, axis=0)\n    self.full_scores = np.squeeze(self.full_scores)\n    self.full_scores = np.squeeze(self.full_scores)\n    return np.dot(self.full_scores[0], self.full_scores[1])\n\n  def _aggregate_scores_by_sum(self, scores):\n    \"\"\"The function aggregates scores by summing video and text hidden representations from the input scores.\\nIt asserts that there are exactly two sets of scores, sums each set along the first axis, clears the full_scores attribute, and returns the dot product of the text and video hidden representations.\"\"\"\n    assert len(scores) == 2, \"scores should be a list of two elements.\"\n    self.full_scores = np.sum(scores, axis=0)\n    self.full_scores = np.squeeze(self.full_scores)\n    self.full_scores = np.squeeze(self.full_scores)\n    return np.dot(self.full_scores[0], self.full_scores[1])\n\n  def _aggregate_scores_by_mean(self, scores):\n    \"\"\"The function aggregates scores by averaging video and text hidden representations from the input scores.\\nIt asserts that there are exactly two sets of scores, averages each set along the first axis, clears the full_scores attribute, and returns the dot product of the text and video hidden representations.\"\"\"\n    assert len(scores) == 2, \"scores should be a list of two elements.\"\n    self.full_scores = np.mean(scores, axis=0)\n    self.full_scores = np.squeeze(self.full_scores)\n    self.full_scores = np.squeeze(self.full_scores)\n    return np.dot(self.full_scores[0], self.full_scores[1])\n\n  def _aggregate_scores_by_max(self, scores):\n    \"\"\"The function aggregates scores by taking the max video and text hidden representations from the input scores.\\nIt asserts that there are exactly two sets of scores, takes the max of each set along the first axis, clears the full_scores attribute, and returns the dot product of the text and video hidden representations.\"\"\"", "CodeBleu_score": 32.28489391734087}]}
{"instance_id": "e4e1861fc357", "generation_result": [{"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n\n    # check if predictions are correct\n    correct = np.equal(Y_pred, Y_true)\n\n    # concatenate predictions and true labels\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n\n    # check if there are prediction errors\n    if not np.all(correct):\n        print('Prediction errors:')\n        for i in range(len(correct)):\n            if not correct[i]:\n                print('Sample %d: True label: %d, Predicted label: %d' % (i, Y_true[i], Y_pred[i]))\n\n    # save predictions and true labels\n    if output_file:\n        with open(output_file, 'wb') as f:\n            pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n\n    # return dictionary\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n", "CodeBleu_score": 35.29293309687843}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    Y_pred = np.array(Y_pred)\n    Y_true = np.array(Y_true)\n\n    if len(Y_pred)!= len(Y_true):\n        raise ValueError(\"The number of predicted and true labels does not match.\")\n\n    # Check for prediction errors\n    errors = []\n    for i in range(len(Y_pred)):\n        if Y_pred[i]!= Y_true[i]:\n            errors.append(i)\n\n    if len(errors) > 0:\n        print(\"Prediction errors:\")\n        for i in errors:\n            print(\"Predicted: %s, True: %s\" % (Y_pred[i], Y_true[i]))\n        print(\"\")\n\n    # Save predictions and true labels\n    if output_file:\n        with open(output_file, \"wb\") as f:\n            pickle.dump({\"Y_pred\": Y_pred, \"Y_true\": Y_true}, f)\n\n    return {\"Y_pred\": Y_pred, \"Y_true\": Y_true}\n\n", "CodeBleu_score": 39.529272001146786}, {"code": "def finalize(self, Y_pred, Y_true, output_file=None)\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y_true, axis=0)\n    self.print_errors(Y_pred, Y_true)\n    if output_file is not None:\n      with open(output_file, 'wb') as f:\n        pickle.dump({'Y_pred': Y_pred, 'Y_true': Y_true}, f)\n    return {'Y_pred': Y_pred, 'Y_true': Y_true}\n\n  def print_errors(self, Y_pred, Y_true):\n    \"\"\"The function prints samples of prediction errors.\"\"\"\n    if self.error_analysis is not None:\n      for i in range(self.error_analysis):\n        print('Prediction: ', Y_pred[i])\n        print('True label: ', Y_true[i])\n        print()\n\n  def get_predictions(self, X, batch_size=1000):\n    \"\"\"The function returns a list of predictions for the given data.\"\"\"\n    Y_pred = []\n    for i in range(0, X.shape[0], batch_size):\n      Y_pred.append(self.predict(X[i:i+batch_size]))\n    return Y_pred\n\n  def get_predictions_and_true_labels(self, X, Y, batch_size=1000):\n    \"\"\"The function returns a list of predictions for the given data and a list of true labels.\"\"\"\n    Y_pred = []\n    Y_true = []\n    for i in range(0, X.shape[0], batch_size):\n      Y_pred.append(self.predict(X[i:i+batch_size]))\n      Y_true.append(Y[i:i+batch_size])\n    return Y_pred, Y_true\n\n  def get_predictions_and_true_labels_for_all_data(self, X, Y, batch_size=1000):\n    \"\"\"The function returns a list of predictions for the given data and a list of true labels for all data.\"\"\"\n    Y_pred, Y_true = self.get_predictions_and_true_labels(X, Y, batch_size)\n    Y_pred = np.concatenate(Y_pred, axis=0)\n    Y_true = np.concatenate(Y", "CodeBleu_score": 35.2476803432437}]}
{"instance_id": "7df9ab9a4920", "generation_result": [{"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )  # round to 3 decimal places\n\n", "CodeBleu_score": 54.58085773512743}, {"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )  # round=3 to reduce the number of decimal places\n\n", "CodeBleu_score": 53.13841303087672}, {"code": "def reduce_metrics(logging_outputs) -> None\n    loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\n        \"loss\", loss_sum / sample_size, sample_size, round=3\n    )  # type: ignore\n", "CodeBleu_score": 55.59647001040926}]}
{"instance_id": "7f9c326a96f7", "generation_result": [{"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = (cmasks + vmasks).gt(0)\n    token_type_ids = torch.cat([torch.zeros_like(cmasks), torch.ones_like(vmasks)], dim=1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 27.886562558113663}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n\n    if attention_mask is None:\n        attention_mask = self._get_attention_mask(cmasks, vmasks)\n    if self.token_type_ids is None:\n        token_type_ids = self._get_token_type_ids(cmasks, vmasks)\n    else:\n        token_type_ids = self.token_type_ids\n\n    return attention_mask, token_type_ids\n", "CodeBleu_score": 26.415660586197376}, {"code": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )\n    if attention_mask is None:\n        attention_mask = torch.cat((cmasks, vmasks), dim=-1)\n    token_type_ids = torch.cat((\n        torch.zeros(cmasks.shape, dtype=torch.long),\n        torch.ones(vmasks.shape, dtype=torch.long)\n    ), dim=-1)\n    return attention_mask, token_type_ids\n\n", "CodeBleu_score": 33.38707753877641}]}
{"instance_id": "fa99d2a148a5", "generation_result": [{"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    # Adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    combined_length = input_ids.shape[-1] + input_video_embeds.shape[-1]\n    if attention_mask is None:\n        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros(input_ids.shape, dtype=torch.long, device=input_ids.device)\n    if attention_mask.shape[-1] < combined_length:\n        attention_mask = torch.cat(\n            [attention_mask,\n             torch.ones(attention_mask.shape[:-1] + (combined_length - attention_mask.shape[-1]), dtype=torch.long,\n                        device=attention_mask.device)],\n            dim=-1)\n    if token_type_ids.shape[-1] < combined_length:\n        token_type_ids = torch.cat(\n            [token_type_ids,\n             torch.zeros(token_type_ids.shape[:-1] + (combined_length - token_type_ids.shape[-1]), dtype=torch.long,\n                         device=token_type_ids.device)],\n            dim=-1)\n    # Modify the attention_mask and token_type_ids to fit the new sequence length\n    attention_mask = attention_mask[:, :combined_length]\n    token_type_ids = token_type_ids[:, :combined_length]\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        **model_kwargs,\n    }\n\n", "CodeBleu_score": 46.69449849848498}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n\n    # adjust the sequence length to match the combined length of input_ids and input_video_embeds\n    max_length = self.config.max_length\n    if input_ids is not None and input_video_embeds is not None:\n        max_length = max_length - input_video_embeds.shape[1]\n        input_ids, attention_mask, token_type_ids = self._pad_to_max_length(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n            max_length,\n            pad_on_right=False,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n    elif input_ids is not None:\n        input_ids, attention_mask, token_type_ids = self._pad_to_max_length(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n            max_length,\n            pad_on_right=False,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n    elif input_video_embeds is not None:\n        max_length = max_length - input_video_embeds.shape[1]\n        input_ids = torch.full(\n            (input_video_embeds.shape[0], max_length),\n            self.config.pad_token_id,\n            dtype=torch.long,\n            device=next(self.parameters()).device,\n        )\n        attention_mask = torch.ones(\n            (input_video_embeds.shape[0], max_length),\n            dtype=torch.long,\n            device=next(self.parameters()).device,\n        )\n        token_type_ids = torch.zeros(\n            (input_video_embeds.shape[0], max_length),\n            dtype=torch.long,\n            device=next(self.parameters()).device,\n        )\n    else:\n        raise ValueError(\n            \"At least one of input_ids and input_video_embeds must be not None\"\n        )\n\n    # update input_ids, input_video_embeds, attention_mask, and token_type_ids\n    inputs = {}\n    if input_ids is not None:\n        inputs[\"input_ids\"] = input_ids\n    if input_video_embeds is not None:\n       ", "CodeBleu_score": 31.284250327347152}, {"code": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )\n    if attention_mask is None:\n        attention_mask = torch.ones_like(input_ids)\n    if token_type_ids is None:\n        token_type_ids = torch.zeros_like(input_ids)\n\n    # set video token_type_ids to 1\n    token_type_ids[input_ids == self.config.video_token_id] = 1\n\n    # adjust sequence length to match the combined length of input_ids and input_video_embeds\n    input_ids, input_video_embeds, attention_mask, token_type_ids = self._adjust_length_to_model_inputs(\n        input_ids, input_video_embeds, attention_mask, token_type_ids, model_kwargs\n    )\n\n    # prepare inputs\n    inputs = {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }\n\n    return inputs\n\n  def _adjust_length_to_model_inputs(\n      self,\n      input_ids,\n      input_video_embeds,\n      attention_mask,\n      token_type_ids,\n      model_kwargs\n  ):\n    \"\"\"Adjust the length of the inputs to the model.\"\"\"\n    # get input_ids length\n    input_ids_length = input_ids.shape[-1]\n\n    # get video_embeds length\n    input_video_embeds_length = input_video_embeds.shape[-2]\n\n    # get model max length\n    max_length = self.config.max_length\n\n    # get difference between video_embeds_length and input_ids_length\n    difference = input_video_embeds_length - input_ids_length\n\n    # if difference is negative, we need to pad input_ids\n    if difference < 0:\n      input_ids = input_ids + [self.config.pad_token_id] * -difference\n      attention_mask = attention_mask + [0] * -difference\n      token_type_ids = token_type_ids + [0] * -difference\n\n    # if difference is positive, we need to pad input_video_embeds\n    elif difference > 0:\n      input_video_embeds = input_video_embeds + [\n         ", "CodeBleu_score": 33.75817925943898}]}
{"instance_id": "b1b0d25fe620", "generation_result": [{"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose    # remove the 'img_id' key from meta_keys if present\n    if \"meta_keys\" in cfg.data.test:\n        if \"img_id\" in cfg.data.test.meta_keys:\n            cfg.data.test.meta_keys.remove(\"img_id\")\n\n    # find the index of the 'LoadImageFromFile' transform, raising an error if not found\n    load_image_from_file_transforms = [\n        transform\n        for transform in cfg.data.test.pipeline\n        if transform[\"type\"] == \"LoadImageFromFile\"\n    ]\n    if not load_image_from_file_transforms:\n        raise ValueError(\n            \"No 'LoadImageFromFile' transform was found in the test pipeline.\"\n        )\n    if len(load_image_from_file_transforms) > 1:\n        raise ValueError(\n            \"More than one 'LoadImageFromFile' transform was found in the test pipeline.\"\n        )\n\n    # update the transform type to'mmdet.InferencerLoader'\n    load_image_from_file_transforms[0][\"type\"] = \"mmdet.InferencerLoader\"\n\n    return Compose(cfg.data.test.pipeline)\n\n", "CodeBleu_score": 28.14362950307879}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose    pipeline = cfg.data.test.pipeline\n    if 'img_id' in pipeline[0]['meta_keys']:\n        pipeline[0]['meta_keys'].remove('img_id')\n    for i, transform in enumerate(pipeline):\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_index = i\n            break\n    else:\n        raise ValueError('LoadImageFromFile is not found in the test pipeline')\n    pipeline[load_image_from_file_index]['type'] ='mmdet.InferencerLoader'\n    return Compose(pipeline)\n\n", "CodeBleu_score": 34.414957475433226}, {"code": "def _init_pipeline(self, cfg: ConfigType) -> Compose\n    # remove the 'img_id' key from meta_keys if present\n    if'meta_keys' in cfg:\n        meta_keys = cfg['meta_keys']\n        if 'img_id' in meta_keys:\n            meta_keys.remove('img_id')\n\n    # find the index of the 'LoadImageFromFile' transform\n    load_image_from_file_index = None\n    for index, transform in enumerate(cfg['pipeline']):\n        if transform['type'] == 'LoadImageFromFile':\n            load_image_from_file_index = index\n            break\n\n    # raise an error if the transform was not found\n    if load_image_from_file_index is None:\n        raise ValueError('The test pipeline must contain a transform of type \"LoadImageFromFile\"')\n\n    # update the transform type to'mmdet.InferencerLoader'\n    cfg['pipeline'][load_image_from_file_index]['type'] ='mmdet.InferencerLoader'\n\n    # wrap the pipeline configuration in a Compose object\n    return Compose(cfg['pipeline'])\n\n", "CodeBleu_score": 29.411369818118892}]}
{"instance_id": "7967962856eb", "generation_result": [{"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n      if os.path.isdir(inputs):\n        inputs = [os.path.join(inputs, i) for i in os.listdir(inputs) if i.endswith(self.image_extensions)]\n      else:\n        raise ValueError(\"The input must be a directory containing images.\")\n    if not isinstance(inputs, (list, tuple)):\n      inputs = [inputs]\n    return inputs\n\n  def _get_image_size(self, image_path: str) -> tuple:\n    \"\"\"The function gets the image size of the image at the specified path.\"\"\"\n    with Image.open(image_path) as image:\n      return image.size\n\n  def _get_image_shape(self, image_path: str) -> tuple:\n    \"\"\"The function gets the image shape of the image at the specified path.\"\"\"\n    image_size = self._get_image_size(image_path)\n    return image_size[1], image_size[0], len(image_size)\n\n  def _get_image_data(self, image_path: str) -> np.ndarray:\n    \"\"\"The function gets the image data of the image at the specified path.\"\"\"\n    image_size = self._get_image_size(image_path)\n    image_data = np.array(Image.open(image_path).convert(\"RGB\"))\n    return image_data\n\n  def _get_image_data_batch(self, image_paths: list) -> np.ndarray:\n    \"\"\"The function gets the image data of the images at the specified paths.\"\"\"\n    image_data_batch = np.array([self._get_image_data(image_path) for image_path in image_paths])\n    return image_data_batch\n\n  def _get_image_data_batch_generator(self, image_paths: list) -> Generator:\n    \"\"\"The function gets the image data generator of the images at the specified paths.\"\"\"\n    for image_path in image_paths:\n      yield self._get_image_data(image_path)\n\n  def _get_image_data_batch_generator_with_preprocessing(self, image_paths: list) -> Generator:\n    \"\"\"The function gets the image data generator of the images at the specified paths with preprocessing.\"\"\"\n    for image_path in image_paths:\n      yield self._get_image_data(image_path)", "CodeBleu_score": 41.16229431916072}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n        if os.path.isdir(inputs):\n            inputs = [os.path.join(inputs, f) for f in os.listdir(inputs) if os.path.isfile(os.path.join(inputs, f)) and os.path.splitext(f)[1].lower() in self.supported_formats]\n        else:\n            raise ValueError('The input is not a valid directory.')\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return inputs\n\n", "CodeBleu_score": 48.0242929847689}, {"code": "def _inputs_to_list(self, inputs: InputsType) -> list\n    if isinstance(inputs, str):\n      inputs = glob(inputs)\n    if not isinstance(inputs, (list, tuple)):\n      inputs = [inputs]\n    return inputs\n\n  def _get_image_size(self, image: ImageType) -> Tuple[int, int]:\n    \"\"\"The function returns the size of the image as a tuple (width, height).\"\"\"\n    if isinstance(image, str):\n      image = Image.open(image)\n    return image.size\n\n  def _get_image_mode(self, image: ImageType) -> str:\n    \"\"\"The function returns the mode of the image as a string.\"\"\"\n    if isinstance(image, str):\n      image = Image.open(image)\n    return image.mode\n\n  def _get_image_channels(self, image: ImageType) -> int:\n    \"\"\"The function returns the number of channels of the image as an integer.\"\"\"\n    if isinstance(image, str):\n      image = Image.open(image)\n    return len(image.getbands())\n\n  def _get_image_data(self, image: ImageType) -> np.ndarray:\n    \"\"\"The function returns the image data as a numpy array.\"\"\"\n    if isinstance(image, str):\n      image = Image.open(image)\n    return np.asarray(image)\n\n  def _get_image_data_size(self, image: ImageType) -> int:\n    \"\"\"The function returns the size of the image data as an integer.\"\"\"\n    if isinstance(image, str):\n      image = Image.open(image)\n    return image.size[0] * image.size[1]\n\n  def _get_image_data_shape(self, image: ImageType) -> Tuple[int, int, int]:\n    \"\"\"The function returns the shape of the image data as a tuple (width, height, channels).\"\"\"\n    if isinstance(image, str):\n      image = Image.open(image)\n    return image.size[0], image.size[1], len(image.getbands())\n\n  def _get_image_data_dtype(self, image: ImageType) -> np.dtype:\n    \"\"\"The function returns the data type of the image data as a numpy data type.\"\"\"\n    if isinstance(image, str):\n      image = Image.open(image)\n    return image.getbands()[0].dtype\n\n  def _get_image_data_range(self, image", "CodeBleu_score": 43.66906625603753}]}
{"instance_id": "74f727f06fa0", "generation_result": [{"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    for i in range(0, len(inputs), chunk_size):\n        chunk = inputs[i : i + chunk_size]\n        chunk = [self.pipeline(x) for x in chunk]\n        yield chunk\n", "CodeBleu_score": 15.398080620331367}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    chunk = []\n    for input_data in inputs:\n        chunk.append(self._pipeline(input_data))\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if chunk:\n        yield chunk\n\n", "CodeBleu_score": 16.92922115583682}, {"code": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)\n    for i in range(0, len(inputs), chunk_size):\n        chunk = inputs[i:i + chunk_size]\n        data = []\n        for sample in chunk:\n            if isinstance(sample, dict):\n                if 'img' in sample:\n                    data.append(self.pipeline(sample['img']))\n                elif 'img_path' in sample:\n                    img = self.read_image(sample['img_path'])\n                    data.append(self.pipeline(img))\n                else:\n                    raise KeyError(\"The input dictionary must contain an 'img' or 'img_path' key.\")\n            else:\n                data.append(self.pipeline(sample))\n        yield data\n\n", "CodeBleu_score": 42.188547751498064}]}
{"instance_id": "a43ab51afa59", "generation_result": [{"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if pred_out_dir:\n        if not os.path.exists(pred_out_dir):\n            os.makedirs(pred_out_dir)\n\n    if not no_save_pred and return_datasamples:\n        warnings.warn(\n            'Saving datasamples is not supported. Predictions will be saved instead.'\n        )\n\n    if return_datasamples:\n        result = {\n            'preds': preds,\n            'visualization': visualization,\n        }\n    else:\n        result = self.preds_to_dicts(preds, **kwargs)\n        if visualization is not None:\n            result['visualization'] = visualization\n\n    if print_result:\n        self.print_result(result)\n\n    if not no_save_pred:\n        self.save_result(result, pred_out_dir)\n\n    return result\n", "CodeBleu_score": 46.715887415447035}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n\n    if not no_save_pred and pred_out_dir == '':\n        raise ValueError(\n            'The postprocess function requires the path to the directory to save predictions.'\n        )\n\n    if pred_out_dir!= '' and not return_datasamples:\n        warnings.warn(\n            'Saving datasamples is not supported for the postprocess function. Returning predictions only.'\n        )\n\n    if return_datasamples and pred_out_dir!= '':\n        warnings.warn(\n            'Saving datasamples is not supported for the postprocess function. Returning predictions only.'\n        )\n\n    if pred_out_dir!= '':\n        os.makedirs(pred_out_dir, exist_ok=True)\n\n    if not return_datasamples:\n        preds = self.convert_pred(preds)\n\n    if visualization is not None:\n        preds['visualization'] = visualization\n\n    if print_result:\n        self.print_result(preds)\n\n    if not no_save_pred:\n        self.save_pred(preds, pred_out_dir)\n\n    return preds\n\n", "CodeBleu_score": 42.50519331144736}, {"code": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict\n    if pred_out_dir:\n        if not os.path.exists(pred_out_dir):\n            os.makedirs(pred_out_dir)\n        if return_datasamples:\n            warnings.warn(\n                'Saving datasamples is not supported when return_datasamples is True. Predictions are saved instead.'\n            )\n    if not return_datasamples:\n        preds = self.convert_to_dict(preds)\n    if print_result:\n        self.print_results(preds)\n    if not no_save_pred:\n        self.save_predictions(preds, pred_out_dir)\n    if visualization is not None:\n        preds['visualization'] = visualization\n    return preds\n", "CodeBleu_score": 44.15693737401353}]}
{"instance_id": "a3c2561f199c", "generation_result": [{"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    # copy first transform in dataset pipeline\n    first_transform = cfg[\"dataset\"][\"pipeline\"][0]\n    # modify first transform to include only the 'Resize' transform\n    first_transform[\"transforms\"] = [\n        transform for transform in first_transform[\"transforms\"] if transform[\"type\"] == \"Resize\"\n    ]\n    # copy last transform in dataset pipeline\n    last_transform = cfg[\"dataset\"][\"pipeline\"][-1]\n    # combine modified first and copied last transforms into a new test pipeline\n    cfg[\"test_pipeline\"] = [first_transform, last_transform]\n    # return the constructed test pipeline\n    return cfg\n\n", "CodeBleu_score": 25.417691562713863}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    # get the first transform in the dataset pipeline\n    first_transform = cfg.dataset.pipeline[0]\n    # get the last transform in the dataset pipeline\n    last_transform = cfg.dataset.pipeline[-1]\n    # copy the first transform and modify it to include only the 'Resize' transform\n    cfg.dataset.pipeline[0] = first_transform.copy()\n    cfg.dataset.pipeline[0].transforms = [\n        t for t in cfg.dataset.pipeline[0].transforms if isinstance(t, Resize)\n    ]\n    # copy the last transform\n    cfg.dataset.pipeline[-1] = last_transform.copy()\n    # combine the modified first and copied last transforms into a new test pipeline using Compose\n    cfg.dataset.pipeline = Compose(cfg.dataset.pipeline)\n    return cfg\n\n", "CodeBleu_score": 28.65504922055197}, {"code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType\n    test_pipeline = copy.deepcopy(cfg[\"dataset\"][\"pipeline\"])\n    test_pipeline[0] = dict(type=\"Resize\", img_scale=(1333, 800), keep_ratio=True)\n    test_pipeline.append(cfg[\"dataset\"][\"pipeline\"][-1])\n    cfg[\"dataset\"][\"pipeline\"] = [test_pipeline]\n    return cfg\n\n", "CodeBleu_score": 17.249908691090006}]}
{"instance_id": "48a358ab5a38", "generation_result": [{"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    # Prepare the input data\n    data = {\"image\": img, \"frame_id\": frame_id, \"original_shape\": img.shape, \"image_id\": 0, \"video_len\": video_len}\n    # Preprocess the data\n    data = model.preprocess_image(data)\n    # Perform forward pass\n    with torch.no_grad():\n        if isinstance(model, nn.parallel.DistributedDataParallel):\n            model = model.module\n        if model.device == \"cpu\":\n            model = model.to(model.device)\n            model.postprocess_mot = model.postprocess_mot.to(model.device)\n            model.roi_heads.box_predictor = model.roi_heads.box_predictor.to(model.device)\n            model.roi_heads.mask_head = model.roi_heads.mask_head.to(model.device)\n            model.roi_heads.keypoint_head = model.roi_heads.keypoint_head.to(model.device)\n        data = model.forward(data)\n    # Postprocess the data\n    return model.postprocess_mot(data)\n", "CodeBleu_score": 32.48047493634324}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    assert isinstance(model, nn.Module), \"The model must be a nn.Module.\"\n    assert isinstance(img, np.ndarray), \"The image must be a numpy array.\"\n    assert isinstance(frame_id, int), \"The frame ID must be an integer.\"\n    assert isinstance(video_len, int), \"The video length must be an integer.\"\n    assert frame_id < video_len, \"The frame ID must be less than the video length.\"\n\n    device = next(model.parameters()).device\n    assert device.type == \"cuda\" or device.type == \"cpu\", \"The device must be a cuda or cpu device.\"\n\n    if device.type == \"cuda\":\n        assert torch.cuda.is_available(), \"The cuda device is not available.\"\n        assert torch.backends.cudnn.enabled, \"The cudnn backend is not enabled.\"\n\n    img = img.astype(np.float32)\n    img = torch.from_numpy(img)\n    img = img.permute(2, 0, 1)\n    img = img.unsqueeze(0)\n    img = img.to(device)\n\n    data = {\"image\": img, \"frame_id\": frame_id, \"original_shape\": img.shape, \"image_id\": 0, \"video_len\": video_len}\n    data = SampleList(data)\n    data.dataset_name = \"MOT\"\n    data.dataset_type = \"test\"\n\n    model = model.to(device)\n    model.eval()\n\n    if device.type == \"cpu\":\n        model = model.cpu()\n        model = model.to(device)\n        assert not model.training, \"The model must be in eval mode.\"\n        for module in model.modules():\n            assert not isinstance(module, RoIPool), \"The model must not use RoIPool.\"\n\n    with torch.no_grad():\n        result = model(data)\n\n    return result\n", "CodeBleu_score": 43.11788802527154}, {"code": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList\n    # Prepare input data\n    data = {\"image\": img, \"frame_id\": frame_id, \"original_shape\": img.shape, \"image_id\": 0, \"video_len\": video_len}\n    # Perform forward pass\n    with torch.no_grad():\n        if isinstance(model, nn.parallel.DistributedDataParallel):\n            model = model.module\n        if next(model.parameters()).is_cuda:\n            torch.cuda.empty_cache()\n        model.eval()\n        data = model.preprocess_image(data)\n        data = SampleList(data)\n        data = data.to(model.device)\n        output = model.inference(data)\n        output = model.postprocess(output, data)\n    return output\n\n", "CodeBleu_score": 38.249145655996095}]}
{"instance_id": "6c4ea2ce1787", "generation_result": [{"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 79.96334663787925}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop(rng.randint(0, len(tokens_a) - 1))\n        else:\n            tokens_b.pop(rng.randint(0, len(tokens_b) - 1))\n\n", "CodeBleu_score": 49.95568189743128}, {"code": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n        if len(tokens_a) > len(tokens_b):\n            trunc_tokens = tokens_a\n            assert len(trunc_tokens) >= 1\n        else:\n            trunc_tokens = tokens_b\n            assert len(trunc_tokens) >= 1\n        assert len(trunc_tokens) >= 1\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if rng.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n", "CodeBleu_score": 63.781576515800396}]}
{"instance_id": "007181b6729d", "generation_result": [{"code": "def get_activation(activation_string)\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    act = activation_string.lower()\n    if act == \"linear\":\n        return linear\n    if act == \"sigmoid\":\n        return sigmoid\n    if act == \"tanh\":\n        return tanh\n    if act == \"relu\":\n        return relu\n    if act == \"leakyrelu\":\n        return leaky_relu\n    if act == \"elu\":\n        return elu\n    if act == \"selu\":\n        return selu\n    if act == \"softplus\":\n        return softplus\n    if act == \"softmax\":\n        return softmax\n    if act == \"swish\":\n        return swish\n    raise ValueError(\"Unsupported activation function {}\".format(activation_string))\n", "CodeBleu_score": 52.40984130468327}, {"code": "def get_activation(activation_string)\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string in ACTIVATION_FUNCTIONS:\n        return ACTIVATION_FUNCTIONS[activation_string]\n    raise ValueError('Unsupported activation function \\'{}\\'.'.format(activation_string))\n", "CodeBleu_score": 32.51913685792001}, {"code": "def get_activation(activation_string)\n    if not isinstance(activation_string, str):\n        return activation_string\n    if not activation_string:\n        return None\n    if activation_string in ACTIVATION_FUNCTIONS:\n        return ACTIVATION_FUNCTIONS[activation_string]\n    raise ValueError('Unsupported activation function: %s' % activation_string)\n", "CodeBleu_score": 37.25181191228872}]}
{"instance_id": "d5d6e0315899", "generation_result": [{"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is not None and dropout_prob > 0.0:\n        output_tensor = tf.nn.dropout(input_tensor, rate=dropout_prob)\n    else:\n        output_tensor = input_tensor\n    return output_tensor\n\n", "CodeBleu_score": 48.72195381725138}, {"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    return tf.nn.dropout(input_tensor, keep_prob=1.0 - dropout_prob)\n\n", "CodeBleu_score": 55.82574493648463}, {"code": "def dropout(input_tensor, dropout_prob)\n    if dropout_prob is None or dropout_prob == 0.0:\n        return input_tensor\n    else:\n        return tf.nn.dropout(input_tensor, keep_prob=1.0 - dropout_prob)\n\n", "CodeBleu_score": 54.480096897593114}]}
{"instance_id": "828917b9ac53", "generation_result": [{"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = tf.shape(from_tensor)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = tf.shape(to_mask)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 50.63957953145079}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = tf.shape(from_tensor)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = tf.shape(to_mask)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n", "CodeBleu_score": 54.47030159574046}, {"code": "def create_attention_mask_from_input_mask(from_tensor, to_mask)\n    from_shape = tf.shape(from_tensor)\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n\n    to_shape = tf.shape(to_mask)\n    to_seq_length = to_shape[1]\n\n    to_mask = tf.cast(\n        tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32\n    )\n\n    # We don't assume that `from_tensor` is a mask (although it could be). We\n    # don't actually care if we attend *from* padding tokens (only *to* padding)\n    # tokens so we create a tensor of all ones.\n    #\n    # `broadcast_ones` = [batch_size, from_seq_length, 1]\n    broadcast_ones = tf.ones(\n        shape=[batch_size, from_seq_length, 1], dtype=tf.float32\n    )\n\n    # Here we broadcast along two dimensions to create the mask.\n    mask = broadcast_ones * to_mask\n\n    return mask\n\n", "CodeBleu_score": 50.63957953145079}]}
{"instance_id": "d7798e41a723", "generation_result": [{"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}, {"code": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)\n    output_tensor = tf.reshape(input_tensor, [batch_size, seq_length, num_attention_heads, width])\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n", "CodeBleu_score": 54.22065594588168}]}
{"instance_id": "711d466ace09", "generation_result": [{"code": "def reshape_to_matrix(input_tensor)\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}, {"code": "def reshape_to_matrix(input_tensor)\n    if input_tensor.shape.ndims < 2:\n        raise ValueError(\"input_tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if input_tensor.shape.ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 73.65176399251389}, {"code": "def reshape_to_matrix(input_tensor)\n    ndims = input_tensor.shape.ndims\n    if ndims < 2:\n        raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                         (input_tensor.shape))\n    if ndims == 2:\n        return input_tensor\n\n    width = input_tensor.shape[-1]\n    output_tensor = tf.reshape(input_tensor, [-1, width])\n    return output_tensor\n\n", "CodeBleu_score": 83.6520561341743}]}
{"instance_id": "7adcd08f531e", "generation_result": [{"code": "def get_train_examples(self, data_dir)\n    examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        next(reader)\n        for i, line in enumerate(reader):\n            guid = \"%s-%s\" % (\"train\", i)\n            text_a = line[3]\n            text_b = line[4]\n            label = line[5]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n", "CodeBleu_score": 35.987628369770455}, {"code": "def get_train_examples(self, data_dir)\n    examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        for i, line in enumerate(reader):\n            if i == 0:\n                continue\n            guid = \"%s-%s\" % (\"train\", i)\n            text_a = line[3]\n            text_b = line[4]\n            label = line[5]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label)\n            )\n    return examples\n\n", "CodeBleu_score": 40.04012273197306}, {"code": "def get_train_examples(self, data_dir)\n    train_examples = []\n    with open(os.path.join(data_dir, \"train.tsv\"), encoding=\"utf-8\") as f:\n        reader = csv.reader(f, delimiter=\"\\t\")\n        next(reader, None)\n        for line in reader:\n            text_a = line[3]\n            text_b = line[4]\n            label = line[5]\n            if label == \"contradictory\":\n                label = \"contradiction\"\n            train_examples.append(InputExample(guid=None, text_a=text_a, text_b=text_b, label=label))\n    return train_examples\n\n", "CodeBleu_score": 29.33626219581496}]}
{"instance_id": "26a0bb96dab9", "generation_result": [{"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n\n", "CodeBleu_score": 65.47238279114057}, {"code": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)\n    logits = tf.cast(logits, tf.float32)\n    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n    accuracy = tf.metrics.accuracy(\n        labels=label_ids, predictions=predictions, weights=is_real_example)\n    loss = tf.metrics.mean(\n        values=per_example_loss, weights=is_real_example)\n    return {\n        \"eval_accuracy\": accuracy,\n        \"eval_loss\": loss,\n    }\n", "CodeBleu_score": 59.29302769575465}]}
{"instance_id": "6154e4d28509", "generation_result": [{"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 82.83594654369867}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_list,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 74.9165505721602}, {"code": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)\n\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n        feature = convert_single_example(ex_index, example, label_map,\n                                         max_seq_length, tokenizer)\n\n        features.append(feature)\n    return features\n\n", "CodeBleu_score": 73.76452750550779}]}
{"instance_id": "e09c0db71ebb", "generation_result": [{"code": "def get_next_sentence_output(bert_config, input_tensor, labels) test_predictions.\\n\"\"\"\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        logits = tf.reshape(logits, [-1, 2])\n        logits = tf.nn.log_softmax(logits, axis=-1)\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * logits, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, logits)\n\n", "CodeBleu_score": 71.5249485699211}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels) test_loss, per_example_loss, and log_probabilities.\n    Args:\n        bert_config: A BertConfig class instance with the configuration to build a new model.\n        input_tensor: A float32 tensor with shape [batch_size, seq_length, hidden_size] containing hidden states.\n        labels: A int32 tensor with shape [batch_size, seq_length] containing the binary labels indicating if the next sentence is real or fake.\n    Returns:\n        loss: A float32 scalar with the mean loss.\n        per_example_loss: A float32 tensor with shape [batch_size, seq_length] containing the loss per example.\n        log_probabilities: A float32 tensor with shape [batch_size, seq_length] containing the log probabilities for the true labels.\n    \"\"\"\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 20.281163654635357}, {"code": "def get_next_sentence_output(bert_config, input_tensor, labels) __without__ the mask.\\n\n    Args:\n        bert_config: A `BertConfig` instance with the configuration to use.\n        input_tensor: A `Tensor` of shape [batch_size, seq_length, hidden_size] corresponding to the output of a layer.\n        labels: A `Tensor` of shape [batch_size, seq_length] corresponding to the labels.\n    Returns:\n        loss: A scalar loss tensor.\n        per_example_loss: A `Tensor` of shape [batch_size, seq_length] corresponding to the per-example loss.\n        log_probs: A `Tensor` of shape [batch_size, seq_length, vocab_size] corresponding to the log probabilities.\n    \"\"\"\n    # Simple binary classification. Note that 0 is \"next sentence\" and 1 is \"random sentence\". This weight matrix is not used after pre-training.\n    with tf.variable_scope(\"cls/seq_relationship\"):\n        output_weights = tf.get_variable(\n            \"output_weights\",\n            shape=[2, bert_config.hidden_size],\n            initializer=modeling.create_initializer(bert_config.initializer_range))\n        output_bias = tf.get_variable(\n            \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n\n        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n        logits = tf.nn.bias_add(logits, output_bias)\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n        labels = tf.reshape(labels, [-1])\n        one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n        loss = tf.reduce_mean(per_example_loss)\n        return (loss, per_example_loss, log_probs)\n\n", "CodeBleu_score": 18.766295545323477}]}
{"instance_id": "1f66b020f485", "generation_result": [{"code": "def gather_indexes(sequence_tensor, positions)\n    sequence_shape = sequence_tensor.shape\n    batch_size, seq_length = sequence_shape[0], sequence_shape[1]\n    flat_offsets = torch.arange(0, batch_size, dtype=torch.long) * seq_length\n    flat_positions = positions + flat_offsets\n    flat_sequence_tensor = sequence_tensor.view(-1)\n    output_tensor = flat_sequence_tensor[flat_positions]\n    return output_tensor\n\n", "CodeBleu_score": 20.76468663770101}, {"code": "def gather_indexes(sequence_tensor, positions)\n    shape = sequence_tensor.size()\n    offsets = [i * shape[1] for i in range(shape[0])]\n    offsets = torch.tensor(offsets, dtype=torch.long, device=sequence_tensor.device)\n    flat_offsets = offsets + positions.unsqueeze(1)\n    flat_positions = flat_offsets.view(-1)\n    flat_sequence_tensor = sequence_tensor.view(-1, shape[2])\n    output_tensor = flat_sequence_tensor.index_select(0, flat_positions)\n    return output_tensor\n\n", "CodeBleu_score": 23.144105598290725}, {"code": "def gather_indexes(sequence_tensor, positions)\n    shape = sequence_tensor.size()\n    offsets = torch.arange(0, shape[0], device=sequence_tensor.device).unsqueeze(1).type_as(positions) * shape[1]\n    flat_positions = positions + offsets\n    flat_sequence_tensor = sequence_tensor.view(-1)\n    output_tensor = flat_sequence_tensor[flat_positions.view(-1)].view(*shape)\n    return output_tensor\n", "CodeBleu_score": 23.903075200734097}]}
{"instance_id": "956bffee9a33", "generation_result": [{"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n\n    return example\n\n", "CodeBleu_score": 60.19566542534298}, {"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 60.19566542534298}, {"code": "def _decode_record(record, name_to_features)\n    example = tf.io.parse_single_example(record, name_to_features)\n    for name in list(example.keys()):\n        t = example[name]\n        if t.dtype == tf.int64:\n            t = tf.cast(t, tf.int32)\n        example[name] = t\n    return example\n\n", "CodeBleu_score": 60.19566542534298}]}
{"instance_id": "6b97f354e641", "generation_result": [{"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.91916045386063}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = tokenizer.tokenize(orig_answer_text)\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == orig_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 71.3290572818458}, {"code": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n", "CodeBleu_score": 79.91916045386063}]}
{"instance_id": "aba334b92b34", "generation_result": [{"code": "def _check_is_max_context(doc_spans, cur_span_index, position) __for the token position__.\\nOtherwise, it returns False.\\n\"\"\"\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 75.24929282930427}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position) test.\\n\n    \"\"\"\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n", "CodeBleu_score": 1.7874875868917859}, {"code": "def _check_is_max_context(doc_spans, cur_span_index, position) testifies to the presence of the answer in the document.\\nOtherwise, it returns False.\\n\"\"\"\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n", "CodeBleu_score": 74.84303608456162}]}
{"instance_id": "cf92ad509f6a", "generation_result": [{"code": "def load_vocab(vocab_file)\n    vocab = OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 57.55275813468379}, {"code": "def load_vocab(vocab_file)\n    vocab = OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 57.55275813468379}, {"code": "def load_vocab(vocab_file)\n    vocab = OrderedDict()\n    index = 0\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n", "CodeBleu_score": 57.55275813468379}]}
{"instance_id": "6c865803ac12", "generation_result": [{"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab.get(item, UNK_ID))\n    return output\n\n", "CodeBleu_score": 59.42791785154598}, {"code": "def convert_by_vocab(vocab, items)\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n", "CodeBleu_score": 79.63749907165396}, {"code": "def convert_by_vocab(vocab, items)\n    return [vocab.get(w, vocab[\"<unk>\"]) for w in items]\n\n", "CodeBleu_score": 17.102204680099902}]}
{"instance_id": "e93b9200dc4a", "generation_result": [{"code": "def tokenize(self, text)\n    tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        tokens.append(sub_token)\n    return tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    \"\"\"The function converts a given list of tokens to a list of integers corresponding to the indices of the tokens in the vocabulary.\"\"\"\n    return self.wordpiece_tokenizer.convert_tokens_to_ids(tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    \"\"\"The function converts a given list of integers corresponding to the indices of the tokens in the vocabulary to a list of tokens.\"\"\"\n    return self.wordpiece_tokenizer.convert_ids_to_tokens(ids)\n\n  def convert_tokens_to_string(self, tokens):\n    \"\"\"The function converts a given list of tokens to a string by concatenating the tokens.\"\"\"\n    return \" \".join(tokens)\n\n  def save_vocabulary(self, vocab_path):\n    \"\"\"The function saves the vocabulary to a file.\"\"\"\n    if not os.path.isdir(vocab_path):\n      os.makedirs(vocab_path)\n    vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n    with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n      for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n        writer.write(token + \"\\n\")\n\n  def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n    \"\"\"The function adds special tokens to the beginning and end of a sequence for sequence classification tasks.\"\"\"\n    if token_ids_1 is None:\n      return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n    cls = [self.cls_token_id]\n    sep = [self.sep_token_id]\n    return cls + token_ids_0 + sep + token_ids_1 + sep\n\n  def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n    \"\"\"The function returns a list of integers corresponding to the indices of special", "CodeBleu_score": 51.05353873983084}, {"code": "def tokenize(self, text)\n    text = convert_to_unicode(text)\n    text = self.basic_tokenizer.tokenize(text)\n    text = self.wordpiece_tokenizer.tokenize(text)\n    return text\n\n  def convert_tokens_to_ids(self, tokens):\n    \"\"\"The function converts a sequence of tokens into ids using the vocab.\"\"\"\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    \"\"\"The function converts a sequence of ids into tokens using the vocab.\"\"\"\n    return convert_by_vocab(self.inv_vocab, ids)\n\n  def convert_tokens_to_string(self, tokens):\n    \"\"\"The function converts a sequence of tokens into a single string.\"\"\"\n    out_string =''.join(tokens).replace(' ##', '').strip()\n    return out_string\n\n  def save_vocabulary(self, vocab_path):\n    \"\"\"The function saves the vocabulary to a file.\"\"\"\n    if not os.path.isdir(vocab_path):\n      os.makedirs(vocab_path)\n    vocab_file = os.path.join(vocab_path, VOCAB_NAME)\n    with open(vocab_file, 'w', encoding='utf-8') as writer:\n      for token, index in sorted(self.vocab.items(), key=lambda tup: tup[1]):\n        if index!= self.vocab[self.unk_token]:\n          writer.write(token + u'\\n')\n    return vocab_file\n", "CodeBleu_score": 36.599378244130484}, {"code": "def tokenize(self, text)\n    text = self.basic_tokenizer.tokenize(text)\n    text = [self.wordpiece_tokenizer.tokenize(token) for token in text]\n    text = [item for sublist in text for item in sublist]\n    return text\n", "CodeBleu_score": 35.92291808497375}]}
{"instance_id": "2ff6f6db53c9", "generation_result": [{"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n\n    # Load the audio data\n    try:\n        audio_data, _ = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, mono=False, dtype=dtype)\n    except Exception as e:\n        logger.error(f\"Error loading audio data from {path} in [{offset}, {offset + duration}] seconds: {e}\")\n        return -1.0, True\n\n    return audio_data, False\n\n", "CodeBleu_score": 33.69348410128206}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        data, _ = librosa.load(path, offset=offset, duration=duration, sr=sample_rate, mono=True, dtype=dtype)\n        return data, False\n    except Exception as e:\n        logger.error(f\"Error loading audio data from {path} at {offset}s to {offset+duration}s with {sample_rate}Hz and {dtype} dtype: {e}\")\n        return -1.0, True\n\n", "CodeBleu_score": 33.950493547466245}, {"code": "def safe_load(path, offset, duration, sample_rate, dtype)\n    try:\n        audio_data = load(path, offset=offset, duration=duration, sample_rate=sample_rate, dtype=dtype)\n        return audio_data, False\n    except Exception as e:\n        logger.error(f\"Error loading audio data from {path} at {offset}s - {duration}s: {e}\")\n        return -1.0, True\n", "CodeBleu_score": 31.34435859560098}]}
{"instance_id": "12fbe1f03d8f", "generation_result": [{"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    if n_channels <= 0:\n        raise ValueError(\"n_channels must be greater than 0.\")\n    if n_channels == 1:\n        return tf.squeeze(waveform, axis=-1)\n    if n_channels == waveform.shape[-1]:\n        return waveform\n    if n_channels > waveform.shape[-1]:\n        n_channels_to_add = n_channels - waveform.shape[-1]\n        return tf.tile(waveform, [1, 1, n_channels_to_add])[:, :, :n_channels]\n    return waveform[:, :, :n_channels]\n\n", "CodeBleu_score": 43.40949049843466}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    n_waveform_channels = waveform.shape[-1]\n    if n_waveform_channels >= n_channels:\n        return waveform[..., :n_channels]\n    else:\n        return tf.tile(waveform, [1, 1, n_channels // n_waveform_channels])[..., :n_channels]\n\n", "CodeBleu_score": 40.31751619862304}, {"code": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor\n    if n_channels < 1:\n        raise ValueError(\"The number of channels must be greater than 0.\")\n    if n_channels == 1:\n        return tf.expand_dims(waveform, axis=1)\n    if waveform.shape[-1] == n_channels:\n        return waveform\n    if waveform.shape[-1] > n_channels:\n        return waveform[..., :n_channels]\n    return tf.tile(waveform, [1, n_channels])\n\n", "CodeBleu_score": 45.31462674018888}]}
{"instance_id": "5e215f029b2e", "generation_result": [{"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    return tf.cast(\n        tf.clip_by_value(\n            tf.cast(\n                tf.clip_by_value(\n                    gain_to_db(spectrogram, **kwargs),\n                    -db_range,\n                    db_range,\n                ),\n                tf.float32,\n            ),\n            0,\n            255,\n        ),\n        tf.uint8,\n    )\n\n", "CodeBleu_score": 33.534659707164884}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    return tf.cast(\n        tf.clip_by_value(\n            tf.cast(\n                tf.clip_by_value(\n                    tf.cast(\n                        tf.clip_by_value(\n                            gain_to_db(spectrogram, **kwargs),\n                            -db_range,\n                            db_range,\n                        ),\n                        tf.float32,\n                    ),\n                    tf.uint8,\n                ),\n                tf.float32,\n            ),\n            0,\n            255,\n        ),\n        tf.uint8,\n    )\n\n", "CodeBleu_score": 31.63999497237854}, {"code": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor\n    db_spectrogram = tf.cast(gain_to_db(spectrogram, **kwargs), tf.float32)\n    max_db = tf.reduce_max(db_spectrogram)\n    db_range_clipped = tf.clip_by_value(db_range, 0.0, max_db)\n    db_spectrogram = tf.clip_by_value(db_spectrogram, 0.0, db_range_clipped)\n    db_spectrogram = tf.cast(db_spectrogram, tf.uint8)\n    return db_spectrogram\n\n", "CodeBleu_score": 45.493654170968625}]}
{"instance_id": "b65c0968dae7", "generation_result": [{"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    # Compute the Short-Time Fourier Transform (STFT)\n    stft = tf.signal.stft(\n        waveform, frame_length, frame_step, window_fn=tf.signal.hann_window(frame_length) ** window_exponent\n    )\n    # Transpose the STFT tensor to get the shape (num_frames, num_bins, num_channels)\n    stft = tf.transpose(stft, perm=[1, 2, 0])\n    # Get the absolute value of the STFT tensor\n    stft = tf.abs(stft)\n    # Raise the absolute value to a specified exponent\n    stft = stft ** spec_exponent\n    # Return the computed spectrogram tensor\n    return stft\n\n", "CodeBleu_score": 34.9710415058537}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    window = tf.signal.hann_window(frame_length, periodic=True) ** window_exponent\n    window = tf.cast(window, tf.float32)\n    stft = tf.signal.stft(\n        waveform, frame_length, frame_step, window_fn=lambda _, w: w, pad_end=True\n    )\n    spectrogram = tf.abs(stft) ** spec_exponent\n    return tf.transpose(spectrogram, perm=[1, 0])\n\n", "CodeBleu_score": 39.11739519305226}, {"code": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor\n    # Apply Hann window to the waveform\n    waveform = tf.cast(waveform, dtype=tf.float32)\n    window = tf.signal.hann_window(frame_length, periodic=True)\n    windowed_waveform = tf.multiply(waveform, window)\n\n    # Compute the STFT\n    stft = tf.signal.stft(\n        windowed_waveform,\n        frame_length=frame_length,\n        frame_step=frame_step,\n        fft_length=frame_length,\n    )\n\n    # Transpose the STFT\n    stft = tf.transpose(stft, perm=[1, 2, 0])\n\n    # Compute the magnitude spectrogram\n    spectrogram = tf.math.abs(stft) ** spec_exponent\n\n    # Apply the window function\n    window = tf.signal.hann_window(frame_length, periodic=True) ** window_exponent\n    windowed_spectrogram = tf.multiply(spectrogram, window)\n\n    # Return the spectrogram\n    return windowed_spectrogram\n\n", "CodeBleu_score": 38.90159454394265}]}
{"instance_id": "a4686140a634", "generation_result": [{"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(\n        tf.cast(tf.shape(spectrogram)[1], tf.float32) * factor, tf.int32\n    )\n\n    # Resize the spectrogram\n    resized_spec = tf.image.resize(\n        spectrogram, [tf.shape(spectrogram)[0], new_time_dim], method=method\n    )\n\n    # Adjust the spectrogram to the original dimensions\n    if factor > 1.0:\n        # Crop the spectrogram\n        resized_spec = resized_spec[:, 0 : tf.shape(spectrogram)[1]]\n    else:\n        # Pad the spectrogram\n        resized_spec = tf.pad(\n            resized_spec,\n            [\n                [0, 0],\n                [0, tf.shape(spectrogram)[1] - tf.shape(resized_spec)[1]],\n            ],\n            \"CONSTANT\",\n        )\n\n    return resized_spec\n\n", "CodeBleu_score": 41.42672262362941}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\n    # Calculate the new time dimension\n    time_dim = tf.cast(tf.shape(spectrogram)[1], tf.float32) * factor\n    # Resize the spectrogram\n    spectrogram = tf.image.resize(\n        spectrogram,\n        size=tf.cast(tf.shape(spectrogram)[1:3], tf.int32),\n        method=method,\n    )\n    # Adjust the spectrogram to the original dimensions\n    spectrogram = tf.image.resize_with_crop_or_pad(\n        spectrogram,\n        target_height=tf.cast(tf.shape(spectrogram)[0], tf.int32),\n        target_width=tf.cast(time_dim, tf.int32),\n    )\n\n    return spectrogram\n\n", "CodeBleu_score": 38.67965464296897}, {"code": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the new time dimension\n    new_time_dim = tf.cast(\n        tf.round(tf.shape(spectrogram)[1] * factor), dtype=tf.int32\n    )\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=(\n            tf.shape(spectrogram)[0],\n            new_time_dim,\n        ),\n        method=method,\n    )\n    # Calculate the new time dimension after resizing\n    resized_time_dim = tf.shape(resized_spectrogram)[1]\n    # Calculate the difference in time dimension\n    diff = new_time_dim - resized_time_dim\n    # Calculate the padding on the left and right\n    pad_left = diff // 2\n    pad_right = diff - pad_left\n    # Pad the spectrogram\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[\n            [0, 0],\n            [pad_left, pad_right],\n            [0, 0],\n        ],\n        mode=\"CONSTANT\",\n        constant_values=0.0,\n    )\n    # Calculate the new time dimension after padding\n    padded_time_dim = tf.shape(padded_spectrogram)[1]\n    # Calculate the difference in time dimension\n    diff = padded_time_dim - new_time_dim\n    # Calculate the cropping on the left and right\n    crop_left = diff // 2\n    crop_right = diff - crop_left\n    # Crop the spectrogram\n    cropped_spectrogram = padded_spectrogram[:, crop_left:-crop_right, :]\n    # Return the cropped spectrogram\n    return cropped_spectrogram\n\n", "CodeBleu_score": 31.10072325532629}]}
{"instance_id": "6dd01101d4ae", "generation_result": [{"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Calculate the resizing factor\n    factor = tf.math.exp(semitone_shift * tf.constant(2.0 / 12.0, dtype=tf.float32))\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=[\n            tf.math.floor(tf.shape(spectrogram)[0] * factor),\n            tf.shape(spectrogram)[1],\n        ],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.pad(\n        resized_spectrogram,\n        paddings=[\n            [\n                tf.math.floor(tf.shape(spectrogram)[0] * factor)\n                - tf.shape(resized_spectrogram)[0],\n                0,\n            ],\n            [0, 0],\n        ],\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 28.475647954574008}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n    # Get the shape of the spectrogram\n    spectrogram_shape = tf.shape(spectrogram)\n\n    # Calculate the resizing factor\n    factor = 2 ** (semitone_shift / 12)\n\n    # Resize the spectrogram along the frequency axis\n    resized_spectrogram = tf.image.resize(\n        spectrogram,\n        size=[int(spectrogram_shape[0] * factor), spectrogram_shape[1]],\n        method=method,\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram,\n        0,\n        0,\n        spectrogram_shape[0],\n        spectrogram_shape[1],\n    )\n\n    # Return the pitch-shifted spectrogram\n    return padded_spectrogram\n\n", "CodeBleu_score": 28.837784609922608}, {"code": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor\n\n    # Get the shape of the spectrogram\n    spectrogram_shape = tf.shape(spectrogram)\n\n    # Calculate the resizing factor\n    semitone_shift_factor = tf.math.exp(semitone_shift * (2.0 / 12.0))\n    new_length = tf.cast(\n        tf.cast(spectrogram_shape[1], tf.float32) * semitone_shift_factor, tf.int32\n    )\n\n    # Resize the spectrogram\n    resized_spectrogram = tf.image.resize(\n        spectrogram, [spectrogram_shape[0], new_length], method=method\n    )\n\n    # Pad the resized spectrogram to maintain the original dimensions\n    padded_spectrogram = tf.image.pad_to_bounding_box(\n        resized_spectrogram,\n        0,\n        tf.math.maximum(0, spectrogram_shape[0] - new_length),\n        spectrogram_shape[0],\n        spectrogram_shape[1],\n    )\n\n    return padded_spectrogram\n\n", "CodeBleu_score": 26.438339882573462}]}
{"instance_id": "612f0826e56a", "generation_result": [{"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    conv_activation = params.get(\"conv_activation\", \"ReLU\")\n    if conv_activation == \"ReLU\":\n        return nn.ReLU()\n    elif conv_activation == \"ELU\":\n        return nn.ELU()\n    elif conv_activation == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\n            f\"The value of the 'conv_activation' parameter is invalid. Expected: 'ReLU', 'ELU', or 'LeakyReLU', but got {conv_activation}.\"\n        )\n\n", "CodeBleu_score": 41.24501161571496}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    if params[\"conv_activation\"] == \"ReLU\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"ELU\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(f\"The conv_activation parameter must be one of the following: ReLU, ELU, or LeakyReLU. You provided {params['conv_activation']}.\")\n\n", "CodeBleu_score": 30.630528449503203}, {"code": "def _get_conv_activation_layer(params: Dict) -> Any\n    if params[\"conv_activation\"] == \"ReLU\":\n        return nn.ReLU()\n    elif params[\"conv_activation\"] == \"ELU\":\n        return nn.ELU()\n    elif params[\"conv_activation\"] == \"LeakyReLU\":\n        return nn.LeakyReLU(negative_slope=0.2)\n    else:\n        raise ValueError(\"Incorrect value for the 'conv_activation' parameter: \" + params[\"conv_activation\"])\n\n", "CodeBleu_score": 31.70003357001341}]}
{"instance_id": "2f8b5a2fc0f2", "generation_result": [{"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n    instrument_logits = {}\n    for instrument in instruments:\n        instrument_logits[instrument] = unet(\n            input_tensor,\n            instrument=instrument,\n            params=params,\n        )[\"logits\"]\n    instrument_logits = tf.stack(list(instrument_logits.values()), axis=-1)\n    instrument_logits = tf.math.reduce_logsumexp(instrument_logits, axis=-1)\n    instrument_logits = tf.math.exp(instrument_logits)\n    instrument_logits = tf.expand_dims(instrument_logits, axis=-1)\n    instrument_logits = tf.tile(instrument_logits, [1, 1, 1, 128])\n    instrument_spectrograms = tf.math.multiply(instrument_logits, input_tensor)\n    return instrument_spectrograms\n\n", "CodeBleu_score": 35.35451473078112}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n    instrument_masks = {}\n    for instrument in instruments:\n        instrument_masks[instrument] = unet(input_tensor, instrument, params)\n    instrument_masks_combined = tf.reduce_sum(\n        tf.stack([instrument_masks[instrument] for instrument in instruments]), axis=0\n    )\n    instrument_spectrograms = {}\n    for instrument in instruments:\n        instrument_spectrograms[instrument] = tf.math.multiply(\n            instrument_masks[instrument], input_tensor\n        )\n    return instrument_spectrograms\n\n", "CodeBleu_score": 35.76392236334771}, {"code": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict\n\n    # The function accepts a list of instruments\n    assert isinstance(instruments, Iterable), \"The instruments must be specified as a list.\"\n\n    # The function accepts a dictionary of parameters\n    assert isinstance(params, dict), \"The parameters must be specified as a dictionary.\"\n\n    # The dictionary of parameters must contain the number of instruments\n    assert (\n        \"n_instruments\" in params\n    ), \"The number of instruments must be specified in the parameters dictionary.\"\n\n    # The number of instruments must be greater than 0\n    assert (\n        params[\"n_instruments\"] > 0\n    ), \"The number of instruments must be greater than 0.\"\n\n    # The dictionary of parameters must contain the number of output channels\n    assert (\n        \"n_output_channels\" in params\n    ), \"The number of output channels must be specified in the parameters dictionary.\"\n\n    # The number of output channels must be greater than 0\n    assert (\n        params[\"n_output_channels\"] > 0\n    ), \"The number of output channels must be greater than 0.\"\n\n    # The dictionary of parameters must contain the number of layers\n    assert (\n        \"n_layers\" in params\n    ), \"The number of layers must be specified in the parameters dictionary.\"\n\n    # The number of layers must be greater than 0\n    assert params[\"n_layers\"] > 0, \"The number of layers must be greater than 0.\"\n\n    # The dictionary of parameters must contain the number of filters\n    assert (\n        \"n_filters\" in params\n    ), \"The number of filters must be specified in the parameters dictionary.\"\n\n    # The number of filters must be greater than 0\n    assert params[\"n_filters\"] > 0, \"The number of filters must be greater than 0.\"\n\n    # The dictionary of parameters must contain the kernel size\n    assert (\n        \"kernel_size\" in params\n    ), \"The kernel size must be specified in the parameters dictionary.\"\n\n    # The kernel size must be greater than 0\n    assert params[\"kernel_size\"] > 0, \"The kernel size must be greater than 0.\"\n\n    # The dictionary of parameters must contain the activation function\n    assert (\n        \"activation\" in params\n    ), \"The activation function must be specified in the parameters dictionary.\"\n\n    # The activation function must be specified as a string\n    assert isinstance(\n        params[\"activation\"], str\n    ), \"The activation function must be specified as a string.\"\n\n   ", "CodeBleu_score": 14.897298656783741}]}
{"instance_id": "400b19ce8520", "generation_result": [{"code": "def compute_file_checksum(path)\n    hash_sha256 = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_sha256.update(chunk)\n    return hash_sha256.hexdigest()\n\n", "CodeBleu_score": 56.4841622322711}, {"code": "def compute_file_checksum(path)\n    sha256 = hashlib.sha256()\n    with open(path, 'rb') as f:\n        while True:\n            chunk = f.read(8192)\n            if not chunk:\n                break\n            sha256.update(chunk)\n    return sha256.hexdigest()\n", "CodeBleu_score": 41.786253748784866}, {"code": "def compute_file_checksum(path)\n    sha256_hash = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(chunk)\n    return sha256_hash.hexdigest()\n\n", "CodeBleu_score": 56.4841622322711}]}
{"instance_id": "5f2e9c6e15f1", "generation_result": [{"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code!= 200:\n      raise Exception(f\"Error retrieving checksum index: {response.status_code}\")\n    index = json.loads(response.text)\n    if name not in index:\n      raise ValueError(f\"Model {name} not found in checksum index\")\n    return index[name]\n\n  def download(self, name: str, checksum: str) -> None:\n    \"\"\"The function retrieves the model for a specified model name and checksum.\\nIt constructs a URL using host, repository, release path, model name, and checksum.\\nIt sends an HTTP GET request to this URL and checks for a successful response.\\nIt writes the response to a file using the model name.\\nIf the response is not successful, it raises an Exception.\"\"\"\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{name}/{checksum}\"\n    response = requests.get(url)\n    if response.status_code!= 200:\n      raise Exception(f\"Error retrieving model {name}: {response.status_code}\")\n    with open(name, \"wb\") as file:\n      file.write(response.content)\n\n  def download_all(self) -> None:\n    \"\"\"The function downloads all models from the checksum index.\\nIt iterates over the keys in the checksum index.\\nFor each key, it calls the download function using the key and checksum as arguments.\"\"\"\n    for name in self.checksum_index.keys():\n      self.download(name, self.checksum(name))\n", "CodeBleu_score": 42.72743872079231}, {"code": "def checksum(self, name: str) -> str\n    url = f\"{self.host}/{self.repository}/{self.release_path}/{self.checksum_index}\"\n    response = requests.get(url)\n    if response.status_code!= 200:\n        raise ValueError(\n            f\"Received status code {response.status_code} from {url}.\\n\"\n            \"Please check your host, repository, release path, and checksum index.\"\n        )\n    checksum_index = response.json()\n    if name not in checksum_index:\n        raise ValueError(\n            f\"Model {name} not found in {url}.\\n\"\n            \"Please check your host, repository, release path, and checksum index.\"\n        )\n    return checksum_index[name]\n\n", "CodeBleu_score": 36.1203532907268}, {"code": "def checksum(self, name: str) -> str\n    checksum_url = self.host + self.repository + self.release_path + self.checksum_index\n    response = requests.get(checksum_url)\n    if response.status_code!= 200:\n      raise ValueError(\n        \"The request to the URL: \" + checksum_url + \" failed with status code: \" + str(response.status_code) + \".\"\n      )\n    checksum_index = response.json()\n    if name not in checksum_index:\n      raise ValueError(\n        \"The model name: \" + name + \" is not found in the checksum index.\"\n      )\n    return checksum_index[name]\n", "CodeBleu_score": 37.02445240175453}]}
{"instance_id": "9312c8b6032e", "generation_result": [{"code": "def download(self, name: str, path: str) -> None:\n    if not self.is_model_available(name):\n        raise ValueError(f\"Model '{name}' is not available\")\n    if not self.is_model_downloaded(name):\n        self.download_model(name)\n    if not self.is_model_downloaded(name):\n        raise ValueError(f\"Model '{name}' could not be downloaded\")\n    if not self.is_model_extracted(name):\n        self.extract_model(name, path)\n    if not self.is_model_extracted(name):\n        raise ValueError(f\"Model '{name}' could not be extracted\")\n    return None\n\n", "CodeBleu_score": 15.86518512643654}, {"code": "def download(self, name: str, path: str) -> None:\n    # construct the download URL\n    url = self.url_pattern.format(name=name)\n    # log the download initiation\n    logging.info(\"Downloading model archive from %s\", url)\n    # stream the file using HTTP/2\n    with requests.get(url, stream=True) as response:\n        # check the response status code\n        if response.status_code!= 200:\n            # log the error and exit\n            logging.error(\"Failed to download model archive from %s\", url)\n            sys.exit(1)\n        # compute the expected checksum\n        expected_checksum = hashlib.sha256(response.content).hexdigest()\n        # log the expected checksum\n        logging.info(\"Expected checksum: %s\", expected_checksum)\n        # compute the actual checksum\n        actual_checksum = hashlib.sha256()\n        # stream the response content\n        for chunk in response.iter_content(chunk_size=4096):\n            actual_checksum.update(chunk)\n        # check the actual checksum\n        if actual_checksum.hexdigest()!= expected_checksum:\n            # log the error and exit\n            logging.error(\"Failed to download model archive from %s\", url)\n            sys.exit(1)\n        # log the actual checksum\n        logging.info(\"Actual checksum: %s\", actual_checksum.hexdigest())\n        # log the download completion\n        logging.info(\"Download complete\")\n        # open the temporary file\n        with tempfile.NamedTemporaryFile(delete=False) as file:\n            # log the temporary file creation\n            logging.info(\"Created temporary file %s\", file.name)\n            # stream the response content\n            for chunk in response.iter_content(chunk_size=4096):\n                file.write(chunk)\n        # log the temporary file deletion\n        logging.info(\"Deleted temporary file %s\", file.name)\n        # extract the archive\n        with tarfile.open(file.name) as tar:\n            tar.extractall(path=path)\n        # log the extraction completion\n        logging.info(\"Extraction complete\")\n", "CodeBleu_score": 42.76882210687647}, {"code": "def download(self, name: str, path: str) -> None:\n    url = self.url(name)\n    logger.info(f\"Downloading {name} from {url} to {path}\")\n    with tempfile.NamedTemporaryFile(delete=False) as temp:\n        with requests.get(url, stream=True) as response:\n            response.raise_for_status()\n            for chunk in response.iter_content(chunk_size=8192):\n                temp.write(chunk)\n    logger.info(f\"Validating checksum for {name}\")\n    checksum = self.checksum(name)\n    if not validate_checksum(temp.name, checksum):\n        logger.error(f\"Checksum validation failed for {name}\")\n        raise RuntimeError(f\"Checksum validation failed for {name}\")\n    logger.info(f\"Extracting {name} to {path}\")\n    with tarfile.open(temp.name, \"r:gz\") as tar:\n        tar.extractall(path)\n    os.remove(temp.name)\n    logger.info(f\"Successfully extracted {name} to {path}\")\n", "CodeBleu_score": 35.5734410003764}]}
{"instance_id": "ee9493f37d77", "generation_result": [{"code": "def load_configuration(descriptor: str) -> Dict\n    if descriptor.startswith('embedded:'):\n        name = descriptor[len('embedded:'):]\n        try:\n            config = json.loads(pkgutil.get_data(__name__, f'configs/{name}.json'))\n        except FileNotFoundError:\n            raise ValueError(f'Embedded configuration \"{name}\" not found')\n    else:\n        try:\n            with open(descriptor, 'r') as f:\n                config = json.load(f)\n        except FileNotFoundError:\n            raise ValueError(f'Configuration file \"{descriptor}\" not found')\n    return config\n\n", "CodeBleu_score": 43.346620384773914}, {"code": "def load_configuration(descriptor: str) -> Dict\n    if descriptor.startswith(\"{\"):\n        return json.loads(descriptor)\n    else:\n        if not os.path.exists(descriptor):\n            raise FileNotFoundError(f\"The configuration file {descriptor} was not found.\")\n        with open(descriptor) as f:\n            return json.load(f)\n\n", "CodeBleu_score": 27.814434728236325}, {"code": "def load_configuration(descriptor: str) -> Dict\n    if descriptor.startswith('!'):\n        return json.loads(pkgutil.get_data(__name__, descriptor[1:]).decode('utf-8'))\n    elif os.path.isfile(descriptor):\n        with open(descriptor, 'r') as file:\n            return json.load(file)\n    else:\n        raise ValueError('Configuration descriptor not found')\n\n", "CodeBleu_score": 24.60872990047198}]}
{"instance_id": "546134b67fbc", "generation_result": [{"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    if concat_axis not in [0, 1]:\n        raise ValueError(\n            f\"The concatenation axis must be either 0 or 1. The value of concat_axis is {concat_axis}.\"\n        )\n    concat_tensors = tf.concat(list(tensor_dict.values()), concat_axis=concat_axis)\n    processed_concat_tensors = func(concat_tensors)\n    processed_tensor_dict = {}\n    for key, value in tensor_dict.items():\n        if concat_axis == 0:\n            processed_tensor_dict[key] = tf.split(\n                processed_concat_tensors, value.shape[0], axis=0\n            )\n        else:\n            processed_tensor_dict[key] = tf.split(\n                processed_concat_tensors, value.shape[1], axis=1\n            )\n    return processed_tensor_dict\n\n", "CodeBleu_score": 44.50135432072513}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    # Check concatenation axis\n    if concat_axis not in [0, 1]:\n        raise ValueError(\"The concatenation axis must be either 0 or 1.\")\n    # Concatenate tensors\n    concat_tensor = tf.concat(list(tensor_dict.values()), concat_axis=concat_axis)\n    # Apply function\n    processed_tensor = func(concat_tensor)\n    # Split tensor back into dictionary\n    processed_tensor_dict = {}\n    start_idx = 0\n    for key in tensor_dict.keys():\n        end_idx = start_idx + tf.shape(tensor_dict[key])[concat_axis]\n        if concat_axis == 0:\n            processed_tensor_dict[key] = tf.transpose(\n                tf.slice(\n                    tf.transpose(processed_tensor), [start_idx, 0], [end_idx - start_idx, -1]\n                )\n            )\n        else:\n            processed_tensor_dict[key] = tf.slice(processed_tensor, [start_idx, 0], [end_idx - start_idx, -1])\n        start_idx = end_idx\n    return processed_tensor_dict\n\n", "CodeBleu_score": 44.29057308524843}, {"code": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]\n    if concat_axis == 0:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n    elif concat_axis == 1:\n        tensors = [tensor for tensor in tensor_dict.values()]\n        tensors = tf.concat(tensors, axis=concat_axis)\n        tensors = func(tensors)\n        tensors = tf.split(tensors, len(tensor_dict), axis=concat_axis)\n        return dict(zip(tensor_dict.keys(), tensors))\n    else:\n        raise ValueError(\"concat_axis must be either 0 or 1\")\n\n", "CodeBleu_score": 35.03130055911694}]}
{"instance_id": "df48e78d46d2", "generation_result": [{"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    with tf.name_scope(\"from_float32_to_uint8\"):\n        min_value = tf.reduce_min(tensor)\n        max_value = tf.reduce_max(tensor)\n        tensor_min = tf.cast(min_value, tf.float32)\n        tensor_max = tf.cast(max_value, tf.float32)\n        tensor_range = tf.subtract(tensor_max, tensor_min)\n        tensor_scaled = tf.divide(tf.subtract(tensor, tensor_min), tensor_range)\n        tensor_scaled = tf.multiply(tensor_scaled, 255.0)\n        tensor_scaled = tf.cast(tensor_scaled, tf.uint8)\n        return {\n            tensor_key: tensor_scaled,\n            min_key: tensor_min,\n            max_key: tensor_max,\n        }\n\n", "CodeBleu_score": 55.41180910081811}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    min_value = tf.reduce_min(tensor)\n    max_value = tf.reduce_max(tensor)\n    scaled_tensor = (tensor - min_value) / (max_value - min_value) * 255\n    scaled_tensor = tf.cast(scaled_tensor, tf.uint8)\n    return {tensor_key: scaled_tensor, min_key: min_value, max_key: max_value}\n\n", "CodeBleu_score": 46.36321100630868}, {"code": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor\n    with tf.name_scope(\"from_float32_to_uint8\"):\n        tensor = tf.cast(tensor, tf.float32)\n        min_value = tf.reduce_min(tensor)\n        max_value = tf.reduce_max(tensor)\n        tensor = (tensor - min_value) / (max_value - min_value)\n        tensor = tf.cast(tensor * 255, tf.uint8)\n        return {\n            tensor_key: tensor,\n            min_key: min_value,\n            max_key: max_value,\n        }\n\n", "CodeBleu_score": 54.79572059890459}]}
{"instance_id": "7f479e5de492", "generation_result": [{"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    # Calculate the number of segments\n    n_segments = tf.cast(tf.math.ceil(tf.shape(tensor)[0] / segment_len), tf.int32)\n\n    # Calculate the padding needed to make the tensor length a multiple of the segment length\n    padding = tf.math.maximum(0, n_segments * segment_len - tf.shape(tensor)[0])\n\n    # Pad the tensor\n    padded_tensor = tf.pad(tensor, [[0, padding], [0, 0]])\n\n    # Reshape the tensor into a new tensor with the specified segment length\n    return tf.reshape(padded_tensor, [n_segments, segment_len, -1])\n\n", "CodeBleu_score": 31.06603718759257}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    padding = segment_len - (tf.shape(tensor)[0] % segment_len)\n    padded_tensor = tf.pad(tensor, [[0, padding], [0, 0]])\n    return tf.reshape(padded_tensor, [-1, segment_len, padded_tensor.shape[-1]])\n", "CodeBleu_score": 21.9588356911722}, {"code": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor\n    # Calculate the number of padding elements needed to make the tensor length a multiple of the segment length\n    padding_len = tf.math.mod(tf.shape(tensor)[0], segment_len)\n    # Pad the tensor with zeros\n    padded_tensor = tf.pad(tensor, [[0, padding_len], [0, 0]])\n    # Reshape the padded tensor into segments of the specified length\n    return tf.reshape(padded_tensor, [-1, segment_len])\n\n", "CodeBleu_score": 20.253060701079573}]}
{"instance_id": "1789fae7253f", "generation_result": [{"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    # Calculate the number of extra rows needed for padding\n    pad_rows = frame_length - (F % frame_length)\n    # Create an extension of zeros\n    pad_extension = torch.zeros(F, pad_rows)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    padded_spec = torch.cat((instr_spec, pad_extension), 1)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    reshaped_spec = padded_spec.view(1, -1, frame_length)\n    return reshaped_spec\n\n", "CodeBleu_score": 9.10669898374723}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    # Calculate the number of rows needed for padding\n    rows_needed = frame_length - (instr_spec.shape[0] % frame_length)\n    # Create the extension of zeros\n    extension = np.zeros((rows_needed, instr_spec.shape[1]), dtype=np.float32)\n    # Concatenate the extension to the original spectrogram along the frequency axis\n    padded_spec = np.concatenate((instr_spec, extension), axis=0)\n    # Reshape the extended spectrogram by merging the first two dimensions\n    reshaped_spec = padded_spec.reshape(instr_spec.shape[0] + rows_needed, -1, F)\n    return reshaped_spec\n\n", "CodeBleu_score": 16.919330052057344}, {"code": "def pad_and_reshape(instr_spec, frame_length, F) -> Any\n    # Pad the spectrogram\n    pad_size = frame_length - (instr_spec.shape[1] % frame_length)\n    if pad_size!= frame_length:\n        instr_spec = np.pad(instr_spec, ((0, 0), (0, pad_size)), 'constant')\n\n    # Reshape the spectrogram\n    instr_spec = instr_spec.reshape(instr_spec.shape[0], -1, F)\n    instr_spec = instr_spec.transpose(0, 2, 1)\n    return instr_spec\n", "CodeBleu_score": 20.204794894907703}]}
{"instance_id": "8a4f4658d97c", "generation_result": [{"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    df = pd.read_csv(csv_path, **kwargs)\n    return tf.data.Dataset.from_tensor_slices((dict(df)))\n\n", "CodeBleu_score": 27.26343108616177}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    dataframe = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices((dict(dataframe)))\n    return dataset\n\n", "CodeBleu_score": 35.69305611078637}, {"code": "def dataset_from_csv(csv_path: str, **kwargs) -> Any\n    # Read the CSV file into a pandas DataFrame\n    df = pd.read_csv(csv_path, **kwargs)\n\n    # Convert the DataFrame columns into a TensorFlow dataset\n    dataset = tf.data.Dataset.from_tensor_slices((df[df.columns[0]], df[df.columns[1]]))\n\n    return dataset\n\n", "CodeBleu_score": 41.49421499114839}]}
{"instance_id": "9a16e81b182f", "generation_result": [{"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i in range(len(target_shape)):\n        if target_shape[i] is not None:\n            result = result and (tensor_tf.shape[i] == target_shape[i])\n    return result\n\n", "CodeBleu_score": 30.667054651916516}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i in range(len(target_shape)):\n        if target_shape[i] is not None:\n            result = result and (tensor_tf.shape[i] == target_shape[i])\n    return result\n\n", "CodeBleu_score": 30.667054651916516}, {"code": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool\n    result = True\n    for i in range(len(target_shape)):\n        if target_shape[i] is not None:\n            result = result and (tensor_tf.shape[i] == target_shape[i])\n    return result\n\n", "CodeBleu_score": 30.667054651916516}]}
{"instance_id": "49d5d3084b3a", "generation_result": [{"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    dataset_builder = DatasetBuilder(\n        audio_adapter=audio_adapter,\n        audio_params=audio_params,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n\n    dataset = dataset_builder.build(\n        train_csv_path=audio_path,\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n\n    return dataset\n\n", "CodeBleu_score": 34.050193331474574}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    dataset_builder = DatasetBuilder(\n        audio_adapter=audio_adapter,\n        audio_params=audio_params,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    dataset_builder.build(\n        train_csv=audio_path,\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        cache_dataset=audio_params[\"cache_dataset\"],\n        use_cache=audio_params[\"use_cache\"],\n        augment=audio_params[\"augment\"],\n        shuffle=audio_params[\"shuffle\"],\n    )\n    return dataset_builder.dataset\n\n", "CodeBleu_score": 32.828228555104324}, {"code": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any\n    builder = DatasetBuilder(\n        audio_adapter=audio_adapter,\n        audio_params=audio_params,\n        chunk_duration=audio_params[\"chunk_duration\"],\n        random_seed=audio_params[\"random_seed\"],\n    )\n    return builder.build(\n        train_csv=audio_path,\n        cache_dir=audio_params[\"cache_dir\"],\n        batch_size=audio_params[\"batch_size\"],\n        num_chunks_per_song=audio_params[\"num_chunks_per_song\"],\n        augment=audio_params[\"augment\"],\n        cache=audio_params[\"cache\"],\n    )\n\n", "CodeBleu_score": 33.75757900982572}]}
{"instance_id": "fe8bbbba06e3", "generation_result": [{"code": "def instruments(self) -> Any\n    if self._instruments is None:\n      self._instruments = [\n          InstrumentDatasetBuilder(name, self._market_data_config)\n          for name in self._config.instruments\n      ]\n    yield from self._instruments\n\n  def _generate_example(self) -> tf.data.Dataset:\n    \"\"\"Creates a dataset of tf.Examples for each instrument.\"\"\"\n    for instrument_builder in self.instruments:\n      yield from instrument_builder.build()\n\n  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n    \"\"\"Returns SplitGenerators.\"\"\"\n    # Download the data and define splits\n    data_path = dl_manager.download_and_extract(self._config.data_path)\n    data_path = os.path.join(data_path, self._config.data_dir)\n\n    # Split the data into train, validation and test\n    train_files = []\n    val_files = []\n    test_files = []\n    for file in tf.io.gfile.listdir(data_path):\n      if file.endswith('.tfrecord'):\n        if self._config.train_file_pattern in file:\n          train_files.append(os.path.join(data_path, file))\n        elif self._config.val_file_pattern in file:\n          val_files.append(os.path.join(data_path, file))\n        elif self._config.test_file_pattern in file:\n          test_files.append(os.path.join(data_path, file))\n\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            gen_kwargs={'file_paths': train_files},\n        ),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.VALIDATION,\n            gen_kwargs={'file_paths': val_files},\n        ),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            gen_kwargs={'file_paths': test_files},\n        ),\n    ]\n\n  def _generate_examples(self, file_paths):\n    \"\"\"Yields examples.\"\"\"\n    for file_path in file_paths:\n      for record in tf.data.TFRecordDataset(file_path):\n        example = tf.train.Example()\n        example.ParseFromString(record.numpy())\n        yield example.features.feature", "CodeBleu_score": 20.911568258796372}, {"code": "def instruments(self) -> Any\n    if not self._instruments:\n      for instrument in self._instruments_list:\n        self._instruments[instrument] = InstrumentDatasetBuilder(\n            name=instrument,\n            time_series_dataset=self._time_series_dataset,\n            time_series_identifiers=self._time_series_identifiers,\n            time_series_features=self._time_series_features,\n            time_series_labels=self._time_series_labels,\n            time_series_start_time=self._time_series_start_time,\n            time_series_end_time=self._time_series_end_time,\n            time_series_label_start_time=self._time_series_label_start_time,\n            time_series_label_end_time=self._time_series_label_end_time,\n            time_series_label_names=self._time_series_label_names,\n            time_series_label_columns=self._time_series_label_columns,\n            time_series_label_types=self._time_series_label_types,\n            time_series_label_shapes=self._time_series_label_shapes,\n            time_series_label_classes=self._time_series_label_classes,\n            time_series_label_names_to_ids=self._time_series_label_names_to_ids,\n            time_series_label_ids_to_names=self._time_series_label_ids_to_names,\n            time_series_label_id_column=self._time_series_label_id_column,\n            time_series_label_id_column_dtype=self._time_series_label_id_column_dtype,\n            time_series_label_id_column_shape=self._time_series_label_id_column_shape,\n            time_series_label_id_column_values=self._time_series_label_id_column_values,\n            time_series_label_id_column_values_shape=self._time_series_label_id_column_values_shape,\n            time_series_label_id_column_values_dtype=self._time_series_label_id_column_values_dtype,\n            time_series_label_id_column_values_map=self", "CodeBleu_score": 12.790236024351422}, {"code": "def instruments(self) -> Any\n    if self.instrument_dataset_builders is None:\n      self.instrument_dataset_builders = []\n      for instrument in self.instruments_list:\n        self.instrument_dataset_builders.append(\n            InstrumentDatasetBuilder(instrument, self.dataset_name))\n    for builder in self.instrument_dataset_builders:\n      yield builder\n\n  def build_dataset(self) -> None:\n    \"\"\"The function builds a dataset using the instrument dataset builders.\"\"\"\n    for builder in self.instruments():\n      builder.build_dataset()\n\n  def get_dataset(self) -> tf.data.Dataset:\n    \"\"\"The function returns a dataset built using the instrument dataset builders.\"\"\"\n    if self.dataset is None:\n      self.dataset = tf.data.experimental.sample_from_datasets(\n          [builder.get_dataset() for builder in self.instrument_dataset_builders])\n    return self.dataset\n\n  def get_dataset_size(self) -> int:\n    \"\"\"The function returns the size of the dataset built using the instrument dataset builders.\"\"\"\n    if self.dataset_size is None:\n      self.dataset_size = sum(\n          builder.get_dataset_size() for builder in self.instrument_dataset_builders)\n    return self.dataset_size\n\n  def get_dataset_batch_size(self) -> int:\n    \"\"\"The function returns the batch size of the dataset built using the instrument dataset builders.\"\"\"\n    if self.dataset_batch_size is None:\n      self.dataset_batch_size = max(\n          builder.get_dataset_batch_size() for builder in self.instrument_dataset_builders)\n    return self.dataset_batch_size\n\n  def get_dataset_instrument_names(self) -> List[str]:\n    \"\"\"The function returns the list of instrument names used to build the dataset.\"\"\"\n    if self.dataset_instrument_names is None:\n      self.dataset_instrument_names = [\n          builder.instrument_name for builder in self.instrument_dataset_builders\n      ]\n    return self.dataset_instrument_names\n\n  def get_dataset_instrument_names_string(self) -> str:\n    \"\"\"The function returns a string containing the list of instrument names used to build the dataset.\"\"\"\n    if self.dataset_instrument_names_string is None:\n      self.dataset_instrument_names_string = ', '.join(\n          self.get_dataset_instrument", "CodeBleu_score": 48.90192787757898}]}
{"instance_id": "438b57fd3413", "generation_result": [{"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is None:\n        return dataset\n    if wait:\n        while not os.path.exists(cache):\n            logging.info(f\"Waiting for {cache} to be created...\")\n            time.sleep(10)\n    os.makedirs(os.path.dirname(cache), exist_ok=True)\n    with open(cache, \"wb\") as f:\n        pickle.dump(dataset, f)\n    logging.info(f\"Cached dataset to {cache}\")\n    return dataset\n\n", "CodeBleu_score": 49.97571026475372}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is None:\n        return dataset\n    else:\n        if wait:\n            while not os.path.exists(cache + \".index\"):\n                time.sleep(1)\n                logging.info(f\"Waiting for cache at {cache}...\")\n        os.makedirs(os.path.dirname(cache), exist_ok=True)\n        with open(cache + \".index\", \"w\") as f:\n            f.write(str(time.time()))\n        with open(cache, \"wb\") as f:\n            pickle.dump(dataset, f)\n        return dataset\n\n", "CodeBleu_score": 41.41999714065329}, {"code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any\n    if cache is None:\n        return dataset\n    if wait:\n        while not os.path.exists(cache):\n            logger.info(f\"Waiting for cache index file {cache}...\")\n            time.sleep(1)\n    logger.info(f\"Caching dataset to {cache}...\")\n    os.makedirs(os.path.dirname(cache), exist_ok=True)\n    with open(cache, \"wb\") as file:\n        pickle.dump(dataset, file)\n    return dataset\n\n", "CodeBleu_score": 50.17534159643947}]}
{"instance_id": "8838a3a09a66", "generation_result": [{"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_provider = ModelProvider(params)\n    params = model_provider.get_updated_model_parameters(params)\n    if MWF:\n        model_provider.load_model_weights(params)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_function(),\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 50.74450054043442}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_provider = ModelProvider()\n    model_provider.load_model_dir(params)\n    params = model_provider.update_params(params)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(session_config=session_config)\n    estimator = tf.estimator.Estimator(\n        model_fn=model_provider.get_model_fn(MWF),\n        model_dir=params[\"model_dir\"],\n        params=params,\n        config=run_config,\n    )\n    return estimator\n\n", "CodeBleu_score": 34.611702373824926}, {"code": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor\n    model_provider = ModelProvider(params)\n    model_provider.load_model_dir()\n    model_provider.update_params(MWF)\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig().replace(session_config=session_config)\n    estimator = tf.estimator.Estimator(model_fn=model_provider.model_fn, model_dir=model_provider.model_dir, params=model_provider.params, config=run_config)\n    return estimator\n", "CodeBleu_score": 29.21458024834817}]}
{"instance_id": "779c812bf2cb", "generation_result": [{"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    if audio_descriptor.channels!= 2:\n        raise ValueError(\n            \"TensorFlow source separation model only supports stereo input.\"\n        )\n    self._ensure_model_loaded()\n    self._ensure_model_is_tensorflow()\n    self._ensure_model_is_loaded()\n    self._ensure_model_is_ready()\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_stereo(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_audio_is_loaded(waveform)\n    self._ensure_", "CodeBleu_score": 24.9467376434584}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    if waveform.ndim == 1:\n        waveform = np.expand_dims(waveform, axis=0)\n    if waveform.shape[1] == 1:\n        waveform = np.concatenate([waveform, waveform], axis=1)\n    self._update_data_generator(waveform, audio_descriptor)\n    prediction = next(self._prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n  def _separate_pytorch(\n      self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n  ) -> Dict:\n    \"\"\"The function performs audio source separation using PyTorch.\\nIt ensures the input waveform is stereo, then obtains a prediction generator.\\nThe data generator is updated with the waveform and audio descriptor.\\nA prediction is generated, and the \"audio_id\" key is removed from the prediction before returning it.\"\"\"\n    if waveform.ndim == 1:\n        waveform = np.expand_dims(waveform, axis=0)\n    if waveform.shape[1] == 1:\n        waveform = np.concatenate([waveform, waveform], axis=1)\n    self._update_data_generator(waveform, audio_descriptor)\n    prediction = next(self._prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n  def _separate_librosa(\n      self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n  ) -> Dict:\n    \"\"\"The function performs audio source separation using librosa.\\nIt ensures the input waveform is stereo, then obtains a prediction generator.\\nThe data generator is updated with the waveform and audio descriptor.\\nA prediction is generated, and the \"audio_id\" key is removed from the prediction before returning it.\"\"\"\n    if waveform.ndim == 1:\n        waveform = np.expand_dims(waveform, axis=0)\n    if waveform.shape[1] == 1:\n        waveform = np.concatenate([waveform, waveform], axis=1)\n    self._update_data_generator(waveform, audio_descriptor)\n    prediction = next(self._prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction\n\n  def _separate_tensorflow_lite(\n      self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n  ) -> Dict:\n   ", "CodeBleu_score": 36.45081150405015}, {"code": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict\n    # Ensure the waveform is stereo\n    waveform = self._ensure_stereo(waveform)\n\n    # Obtain a prediction generator\n    prediction_generator = self._model.predict(\n        self._data_generator.update_data(waveform, audio_descriptor)\n    )\n\n    # Generate a prediction\n    prediction = next(prediction_generator)\n\n    # Remove the \"audio_id\" key from the prediction\n    del prediction[\"audio_id\"]\n\n    return prediction\n\n  def _separate_pytorch(\n      self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n  ) -> Dict:\n    \"\"\"The function performs audio source separation using PyTorch.\\nIt ensures the input waveform is stereo, then obtains a prediction generator.\\nThe data generator is updated with the waveform and audio descriptor.\\nA prediction is generated, and the \"audio_id\" key is removed from the prediction before returning it.\"\"\"\n    # Ensure the waveform is stereo\n    waveform = self._ensure_stereo(waveform)\n\n    # Obtain a prediction generator\n    prediction_generator = self._model.predict(\n        self._data_generator.update_data(waveform, audio_descriptor)\n    )\n\n    # Generate a prediction\n    prediction = next(prediction_generator)\n\n    # Remove the \"audio_id\" key from the prediction\n    del prediction[\"audio_id\"]\n\n    return prediction\n\n  def _separate_librosa(\n      self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n  ) -> Dict:\n    \"\"\"The function performs audio source separation using librosa.\\nIt ensures the input waveform is stereo, then obtains a prediction generator.\\nThe data generator is updated with the waveform and audio descriptor.\\nA prediction is generated, and the \"audio_id\" key is removed from the prediction before returning it.\"\"\"\n    # Ensure the waveform is stereo\n    waveform = self._ensure_stereo(waveform)\n\n    # Obtain a prediction generator\n    prediction_generator = self._model.predict(\n        self._data_generator.update_data(waveform, audio_descriptor)\n    )\n\n    # Generate a prediction\n    prediction = next(prediction_generator)\n\n    # Remove the \"audio_id\" key from the prediction\n    del prediction[\"audio_id\"]\n\n    return prediction\n\n  def _separate_timbral(\n      self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n  ) ->", "CodeBleu_score": 44.85574196344413}]}
{"instance_id": "cf580cd2ed93", "generation_result": [{"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES])\n", "CodeBleu_score": 95.198357853076}, {"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not a callable function or a valid backbone name\"\n\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES]\n    )\n\n", "CodeBleu_score": 85.92133818329147}, {"code": "def compute_backbone_shapes(config, image_shape)\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n    # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"], \"backbone is not resnet50 or resnet101\"\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES])\n\n", "CodeBleu_score": 88.35928973797768}]}
{"instance_id": "ebd61a2e59ca", "generation_result": [{"code": "def clip_boxes_graph(boxes, window)\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\n", "CodeBleu_score": 56.01484333396114}, {"code": "def clip_boxes_graph(boxes, window)\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n", "CodeBleu_score": 56.01484333396114}, {"code": "def clip_boxes_graph(boxes, window)\n    # Split\n    x_min, y_min, x_max, y_max = tf.split(boxes, 4, axis=1)\n    # Clip\n    x_min_clipped = tf.maximum(tf.minimum(x_min, window[1] - 1), 0)\n    y_min_clipped = tf.maximum(tf.minimum(y_min, window[0] - 1), 0)\n    x_max_clipped = tf.maximum(tf.minimum(x_max, window[1] - 1), 0)\n    y_max_clipped = tf.maximum(tf.minimum(y_max, window[0] - 1), 0)\n    # Concatenate\n    clipped_boxes = tf.concat([x_min_clipped, y_min_clipped, x_max_clipped, y_max_clipped], axis=1)\n    clipped_boxes = tf.reshape(clipped_boxes, [-1, 4])\n    return clipped_boxes\n", "CodeBleu_score": 18.242324794809896}]}
{"instance_id": "8e0411587a70", "generation_result": [{"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    # Shared convolutional base of the RPN\n    shared = Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map)\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared)\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = Reshape((-1, 2))(x)\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\", activation='linear', name='rpn_bbox_pred')(shared)\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = Reshape((-1, 4))(x)\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 48.70219862435608}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    shared = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv_shared')(feature_map)\n    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_class_raw')(shared)\n    rpn_class_logits = Reshape((-1, 2))(x)\n    rpn_probs = Activation(\"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_bbox_pred')(shared)\n    rpn_bbox_pred = Reshape((-1, 4))(x)\n    return [rpn_class_logits, rpn_probs, rpn_bbox_pred]\n", "CodeBleu_score": 36.331801139845425}, {"code": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)\n    # Shared convolutional base of the RPN\n    shared = Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride,\n                    name='rpn_conv_shared')(feature_map)\n\n    # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared)\n\n    # Reshape to [batch, anchors, 2]\n    rpn_class_logits = Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n\n    # Softmax on last dimension of BG/FG.\n    rpn_probs = Activation('softmax', name='rpn_class_xxx')(rpn_class_logits)\n\n    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = Conv2D(anchors_per_location * 4, (1, 1), padding='valid', activation='linear', name='rpn_bbox_pred')(shared)\n\n    # Reshape to [batch, anchors, 4]\n    rpn_bbox = Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n\n    return [rpn_class_logits, rpn_probs, rpn_bbox]\n\n", "CodeBleu_score": 65.94671439333204}]}
{"instance_id": "70777c9261d8", "generation_result": [{"code": "def smooth_l1_loss(y_true, y_pred)\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n", "CodeBleu_score": 60.928513203413225}, {"code": "def smooth_l1_loss(y_true, y_pred)\n    loss = K.abs(y_true - y_pred)\n    mask = K.cast(K.less(loss, 1.0), \"float32\")\n    return (loss * mask) + ((loss - (0.5 * mask)) * (1 - mask))\n\n", "CodeBleu_score": 26.215324871742723}, {"code": "def smooth_l1_loss(y_true, y_pred)\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n", "CodeBleu_score": 60.928513203413225}]}
{"instance_id": "0e747b604087", "generation_result": [{"code": "def build_model(x_train, num_classes)\n    tf.reset_default_graph()\n    x = tf.placeholder(tf.float32, [None, x_train.shape[1], x_train.shape[2], x_train.shape[3]])\n    y = tf.placeholder(tf.float32, [None, num_classes])\n    keep_prob = tf.placeholder(tf.float32)\n\n    conv1 = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[5, 5], padding='same', activation=tf.nn.relu)\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n    conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[5, 5], padding='same', activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n    flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense1 = tf.layers.dense(inputs=flat, units=1024, activation=tf.nn.relu)\n    dense2 = tf.layers.dense(inputs=dense1, units=num_classes)\n    y_pred = tf.nn.softmax(dense2)\n    y_pred_cls = tf.argmax(y_pred, axis=1)\n\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=dense2, labels=y)\n    cost = tf.reduce_mean(cross_entropy)\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n    correct_prediction = tf.equal(y_pred_cls, tf.argmax(y, axis=1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    return x, y, keep_prob, y_pred_cls, optimizer, cost, accuracy\n\n", "CodeBleu_score": 23.6163714132301}, {"code": "def build_model(x_train, num_classes)\n    tf.reset_default_graph()\n    # Define input layer\n    X = tf.placeholder(tf.float32, [None, x_train.shape[1]], name='X')\n    # Define convolutional layer 1\n    conv1 = tf.layers.conv1d(inputs=X, filters=16, kernel_size=3, activation=tf.nn.relu, name='conv1')\n    # Define max-pooling layer 1\n    pool1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, name='pool1')\n    # Define convolutional layer 2\n    conv2 = tf.layers.conv1d(inputs=pool1, filters=32, kernel_size=3, activation=tf.nn.relu, name='conv2')\n    # Define max-pooling layer 2\n    pool2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, name='pool2')\n    # Flatten output of max-pooling layer 2\n    pool2_flat = tf.reshape(pool2, [-1, 32 * 10], name='pool2_flat')\n    # Define dense layer 1\n    dense1 = tf.layers.dense(inputs=pool2_flat, units=128, activation=tf.nn.relu, name='dense1')\n    # Define output layer\n    Y = tf.layers.dense(inputs=dense1, units=num_classes, activation=tf.nn.softmax, name='Y')\n    # Define loss function\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Y, labels=tf.one_hot(indices=tf.cast(X, tf.int32), depth=num_classes), name='cross_entropy')\n    loss = tf.reduce_mean(cross_entropy, name='loss')\n    # Define optimizer\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, name='optimizer')\n    train_op = optimizer.minimize(loss, name='train_op')\n    # Define accuracy\n    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(tf.one_hot(indices=tf", "CodeBleu_score": 31.753251691039736}, {"code": "def build_model(x_train, num_classes)\n    tf.reset_default_graph()\n    inputs = tf.keras.Input(shape=x_train.shape[1:])\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(inputs)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(units=128, activation='relu')(x)\n    outputs = tf.keras.layers.Dense(units=num_classes, activation='softmax')(x)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=tf.train.AdamOptimizer(),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n", "CodeBleu_score": 31.25386811763938}]}
{"instance_id": "813e3e2844f4", "generation_result": [{"code": "def extract_bboxes(mask)\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this layer, therefore bounding box is zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)\n\n", "CodeBleu_score": 79.8090520516883}, {"code": "def extract_bboxes(mask)\n    bboxes = np.zeros((mask.shape[2], 4))\n    for layer in range(mask.shape[2]):\n        horizontal_indicies = np.where(np.any(mask[:, :, layer], axis=0))[0]\n        vertical_indicies = np.where(np.any(mask[:, :, layer], axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            bboxes[layer] = np.array([y1, x1, y2, x2])\n    return bboxes.astype(np.int32)\n\n", "CodeBleu_score": 36.27356152370519}, {"code": "def extract_bboxes(mask)\n    bboxes = []\n    for mask_layer in mask:\n        mask_layer = mask_layer.squeeze()\n        if mask_layer.sum() == 0:\n            bboxes.append([0, 0, 0, 0])\n        else:\n            horizontal_indicies = np.where(np.any(mask_layer, axis=0))[0]\n            vertical_indicies = np.where(np.any(mask_layer, axis=1))[0]\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            x2 += 1\n            y2 += 1\n            bboxes.append([x1, y1, x2, y2])\n    return np.array(bboxes, dtype=np.int32)\n\n", "CodeBleu_score": 47.79054525729168}]}
{"instance_id": "7ad5d430d6ed", "generation_result": [{"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    intersection = np.minimum(box[0], boxes[:, 0]) + np.minimum(box[1], boxes[:, 1])\n    intersection = np.maximum(intersection, 0)\n    intersection_area = intersection[:, 0] * intersection[:, 1]\n    union_area = box_area + boxes_area[:] - intersection_area[:]\n    iou = intersection_area / union_area\n    return iou\n\n", "CodeBleu_score": 44.25273086639582}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # Calculate intersection areas\n    x1 = np.maximum(box[0], boxes[:, 0])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n\n", "CodeBleu_score": 55.49021365037661}, {"code": "def compute_iou(box, boxes, box_area, boxes_area)\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou\n", "CodeBleu_score": 58.663259083089194}]}
{"instance_id": "15941b852859", "generation_result": [{"code": "def compute_overlaps(boxes1, boxes2)\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps\n", "CodeBleu_score": 62.2853282032944}, {"code": "def compute_overlaps(boxes1, boxes2)\n    # 1. Calculate the area of each bounding box in both sets.\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n\n    # 2. Initialize the overlap matrix.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n\n    # 3. Calculate the IoU of each bounding box with every other box.\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1)\n\n    return overlaps\n\n", "CodeBleu_score": 58.30928071872607}]}
{"instance_id": "597533722ce3", "generation_result": [{"code": "def load_image(self, image_id):\n    image_path = self.image_info[image_id]['path']\n    image = skimage.io.imread(image_path)\n    image = image.astype(np.uint8)\n    if image.ndim!= 3:\n        image = skimage.color.gray2rgb(image)\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image\n", "CodeBleu_score": 76.73493177591311}, {"code": "def load_image(self, image_id):\n    image_path = self.image_info[image_id][\"path\"]\n    # load image\n    image = skimage.io.imread(image_path)\n    # if image has an alpha color channel, remove it\n    if(image.shape[-1] == 4):\n        image = image[..., :3]\n    # convert image to RGB\n    image = skimage.color.gray2rgb(image)\n    return image\n", "CodeBleu_score": 63.28052247080098}, {"code": "def load_image(self, image_id):\n    image = Image.open(self.image_dict[image_id]['path'])\n    if image.mode!= 'RGB':\n        image = image.convert('RGB')\n    if image.mode == 'RGBA':\n        image = image.convert('RGB')\n    return image\n", "CodeBleu_score": 38.963560667013475}]}
{"instance_id": "9b852997ca05", "generation_result": [{"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    # Set default values if none are provided\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    if cols is None:\n        cols = len(images)\n\n    # Create figure and set display parameters\n    fig = plt.figure()\n    plt.title('Displaying Images')\n    plt.suptitle(titles[0])\n    plt.axis('off')\n    plt.grid(None)\n    plt.ioff()\n\n    # Create subplots\n    for i in range(len(images)):\n        ax = fig.add_subplot(len(images) / cols + 1, cols, i + 1)\n        plt.imshow(images[i], cmap=cmap, norm=norm, interpolation=interpolation)\n        plt.title(titles[i])\n        plt.axis('off')\n\n    # Show figure\n    plt.show()\n\n", "CodeBleu_score": 38.46863288432172}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    # Set default titles if none are provided\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    # Calculate number of rows and columns\n    rows = len(images) // cols + 1\n    # Create figure and set title\n    plt.figure()\n    # Adjust subplot parameters to give specified number of rows and columns\n    # and turn off axes\n    plt.subplot(rows, cols, 1)\n    plt.axis('off')\n    # Iterrate through images and titles\n    for i, (image, title) in enumerate(zip(images, titles)):\n        a = plt.subplot(rows, cols, i + 1)\n        # Turn off axes\n        plt.axis('off')\n        # Display image\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n        # Set title\n        a.set_title(title)\n    # Show the figure\n    plt.show()\n\n", "CodeBleu_score": 50.237977152558045}, {"code": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)\n    # Set default values if not provided\n    if titles is None:\n        titles = ['Image (%d)' % i for i in range(1, len(images) + 1)]\n    # Set default values if not provided\n    if cols is None:\n        cols = len(images)\n    # Set default values if not provided\n    if cmap is None:\n        cmap = 'gray'\n    # Set default values if not provided\n    if norm is None:\n        norm = plt.Normalize(0, 1)\n    # Set default values if not provided\n    if interpolation is None:\n        interpolation = 'none'\n    # Create figure and set title\n    figure = plt.figure()\n    # Set figure title\n    figure.suptitle('Images', fontsize=14, fontweight='bold')\n    # Calculate number of rows needed\n    rows = len(images) // cols + 1\n    # Iterate over the images and titles\n    for i, (image, title) in enumerate(zip(images, titles)):\n        # Create subplot and set title\n        a = figure.add_subplot(rows, cols, i + 1)\n        a.set_title(title)\n        # Show the image\n        plt.imshow(image, cmap=cmap, norm=norm, interpolation=interpolation)\n    # Show the figure\n    plt.show()\n\n", "CodeBleu_score": 42.76178575108523}]}
