[
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _get_err_type(self, context: RequestContext, response: Response) -> Optional[str]",
        "start_line": "55",
        "end_line": "69",
        "file_path": "localstack-core/localstack/aws/handlers/analytics.py",
        "docstring": "The `_get_err_type` function determines the error type from a given context and response in a cloud service request.\\nIt first checks if there is a service exception in the context and returns its error code if present.\\nIf not, it attempts to parse the response to extract the error code from the response's error details.\\nIf an exception occurs during parsing and debug analytics are enabled, it logs the exception.\\nIn case of any error, it returns `None`.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e0ef90a6fa5f",
        "ground_truth": "def _get_err_type(self, context: RequestContext, response: Response) -> Optional[str]:\n    \"\"\"\n    Attempts to re-use the existing service_response, or parse and return the error type from the response body,\n    e.g. ``ResourceInUseException``.\n    \"\"\"\n    try:\n        if context.service_exception:\n            return context.service_exception.code\n        response = parse_response(context.operation, response)\n        return response[\"Error\"][\"Code\"]\n    except Exception:\n        if config.DEBUG_ANALYTICS:\n            LOG.exception(\"error parsing error response\")\n        return None",
        "import_statements": [
            "import logging",
            "import threading",
            "from typing import Optional",
            "from localstack import config",
            "from localstack.aws.api import RequestContext",
            "from localstack.aws.chain import HandlerChain",
            "from localstack.aws.client import parse_response",
            "from localstack.http import Response",
            "from localstack.utils.analytics.service_request_aggregator import (\n    ServiceRequestAggregator,\n    ServiceRequestInfo,\n)"
        ],
        "reference_api": [
            "parse_response",
            "LOG.exception"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "parse_response",
            "LOG.exception"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def is_cors_origin_allowed(headers: Headers) -> bool",
        "start_line": "190",
        "end_line": "200",
        "file_path": "localstack-core/localstack/aws/handlers/cors.py",
        "docstring": "The is_cors_origin_allowed function checks if a request's origin or referer header is allowed for Cross-Origin Resource Sharing (CORS).\\nIt retrieves the origin and referer from the headers.\\nIf the origin is present, it checks if the origin is in the list of allowed CORS origins.\\nIf the referer is present but the origin is not, it parses the referer to extract the URI scheme and netloc, then checks if this URI is in the allowed origins list.\\nIf neither header is present, it allows the request by default, accommodating clients like awscli that do not send these headers.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cd573a4dd47e",
        "ground_truth": "def is_cors_origin_allowed(headers: Headers) -> bool:\n    \"\"\"Returns true if origin is allowed to perform cors requests, false otherwise.\"\"\"\n    origin = headers.get(\"origin\")\n    referer = headers.get(\"referer\")\n    if origin:\n        return CorsEnforcer._is_in_allowed_origins(ALLOWED_CORS_ORIGINS, origin)\n    elif referer:\n        referer_uri = \"{uri.scheme}://{uri.netloc}\".format(uri=urlparse(referer))\n        return CorsEnforcer._is_in_allowed_origins(ALLOWED_CORS_ORIGINS, referer_uri)\n    # If both headers are not set, let it through (awscli etc. do not send these headers)\n    return True",
        "import_statements": [
            "import logging",
            "import re",
            "from typing import List, Set",
            "from urllib.parse import urlparse",
            "from werkzeug.datastructures import Headers",
            "from localstack import config",
            "from localstack.aws.api import RequestContext",
            "from localstack.aws.chain import Handler, HandlerChain",
            "from localstack.config import EXTRA_CORS_ALLOWED_HEADERS, EXTRA_CORS_EXPOSE_HEADERS",
            "from localstack.constants import LOCALHOST, LOCALHOST_HOSTNAME, PATH_USER_REQUEST",
            "from localstack.http import Response",
            "from localstack.utils.urls import localstack_host"
        ],
        "reference_api": [
            "urlparse",
            "CorsEnforcer._is_in_allowed_origins",
            "format",
            "headers.get"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "headers.get",
            "headers.get",
            "CorsEnforcer._is_in_allowed_origins",
            "urlparse",
            "CorsEnforcer._is_in_allowed_origins"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _prepare_logger(self, logger: logging.Logger, formatter: Type)",
        "start_line": "74",
        "end_line": "80",
        "file_path": "localstack-core/localstack/aws/handlers/logging.py",
        "docstring": "The _prepare_logger function configures a provided logger with a specified formatter if the logger's level is set to DEBUG.\\nIf the logger is enabled for DEBUG, it disables message propagation and creates a default handler with the logger's current level.\\nThe handler is then assigned the specified formatter, and added to the logger.\\nFinally, the function returns the configured logger.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cf4c04a65ab7",
        "ground_truth": "def _prepare_logger(self, logger: logging.Logger, formatter: Type):\n    if logger.isEnabledFor(logging.DEBUG):\n        logger.propagate = False\n        handler = create_default_handler(logger.level)\n        handler.setFormatter(formatter())\n        logger.addHandler(handler)\n    return logger",
        "import_statements": [
            "import logging",
            "from functools import cached_property",
            "from typing import Type",
            "from localstack.aws.api import RequestContext, ServiceException",
            "from localstack.aws.chain import ExceptionHandler, HandlerChain",
            "from localstack.http import Response",
            "from localstack.http.request import restore_payload",
            "from localstack.logging.format import AwsTraceLoggingFormatter, TraceLoggingFormatter",
            "from localstack.logging.setup import create_default_handler"
        ],
        "reference_api": [
            "handler.setFormatter",
            "create_default_handler",
            "logger.addHandler",
            "logger.isEnabledFor",
            "formatter"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "create_default_handler",
            "handler.setFormatter"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def record_exception(\n        self, chain: HandlerChain, exception: Exception, context: RequestContext, response: Response\n    )",
        "start_line": "163",
        "end_line": "169",
        "file_path": "localstack-core/localstack/aws/handlers/metric_handler.py",
        "docstring": "The record_exception function logs exceptions for metric collection if the metrics collection mode is enabled.\\nIt takes a HandlerChain, an Exception, a RequestContext, and a Response as parameters.\\nIf metrics collection is enabled, it retrieves a metric handler item for the given context and sets the caught_exception_name attribute to the name of the exception class.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "eadd6152c067",
        "ground_truth": "def record_exception(\n    self, chain: HandlerChain, exception: Exception, context: RequestContext, response: Response\n):\n    if not config.is_collect_metrics_mode():\n        return\n    item = self._get_metric_handler_item_for_context(context)\n    item.caught_exception_name = exception.__class__.__name__",
        "import_statements": [
            "import logging",
            "from typing import List, Optional",
            "from localstack import config",
            "from localstack.aws.api import RequestContext",
            "from localstack.aws.chain import HandlerChain",
            "from localstack.http import Response"
        ],
        "reference_api": [
            "self._get_metric_handler_item_for_context",
            "config.is_collect_metrics_mode"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_metric_handler_item_for_context",
                "code": "def _get_metric_handler_item_for_context(self, context: RequestContext) -> MetricHandlerItem:\n        return self.metrics_handler_items[context]"
            }
        ],
        "third_party": [
            "config.is_collect_metrics_mode"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def modify_response_guard(self, response: Response)",
        "start_line": "176",
        "end_line": "190",
        "file_path": "localstack-core/localstack/aws/handlers/partition_rewriter.py",
        "docstring": "The modify_response_guard function modifies an HTTP response by adjusting its headers and data using a specified partition. It rewrites the response headers and data through the _adjust_partition method with the static partition DEFAULT_INBOUND_PARTITION. After modifying these components, it calls the _post_process_response_headers method to perform additional processing on the response headers.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4f80c976e8f8",
        "ground_truth": "def modify_response_guard(self, response: Response):\n    \"\"\"\n    Modifies the supplied response by rewriting the ARNs to default partition\n    :param response: Response to be modified\n    :param request_region: Region the original request was meant for\n    \"\"\"\n    # rewrite response\n    response.headers = self._adjust_partition(\n        dict(response.headers), static_partition=self.DEFAULT_INBOUND_PARTITION\n    )\n    response.data = self._adjust_partition(\n        response.data, static_partition=self.DEFAULT_INBOUND_PARTITION\n    )\n    self._post_process_response_headers(response)",
        "import_statements": [
            "import base64",
            "import hashlib",
            "import logging",
            "import re",
            "from re import Match",
            "from typing import Optional",
            "from urllib.parse import urlparse",
            "from localstack import config",
            "from localstack.aws.api import RequestContext",
            "from localstack.aws.chain import Handler, HandlerChain",
            "from localstack.http import Response",
            "from localstack.http.proxy import forward",
            "from localstack.http.request import Request, get_full_raw_path, get_raw_path, restore_payload",
            "from localstack.utils.aws.aws_responses import calculate_crc32",
            "from localstack.utils.aws.request_context import extract_region_from_headers",
            "from localstack.utils.run import to_str",
            "from localstack.utils.strings import to_bytes"
        ],
        "reference_api": [
            "self._adjust_partition",
            "self._post_process_response_headers",
            "dict"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._adjust_partition",
                "code": "def _adjust_partition(\n        self,\n        source,\n        static_partition: str = None,\n        request_region: str = None,\n        encoded: bool = False,\n    ):\n        # Call this function recursively if we get a dictionary or a list\n        if isinstance(source, dict):\n            result = {}\n            for k, v in source.items():\n                result[k] = self._adjust_partition(\n                    v, static_partition, request_region, encoded=encoded\n                )\n            return result\n        if isinstance(source, list):\n            result = []\n            for v in source:\n                result.append(\n                    self._adjust_partition(v, static_partition, request_region, encoded=encoded)\n                )\n            return result\n        elif isinstance(source, bytes):\n            try:\n                decoded = to_str(source)\n                adjusted = self._adjust_partition(\n                    decoded, static_partition, request_region, encoded=encoded\n                )\n                return to_bytes(adjusted)\n            except UnicodeDecodeError:\n                # If the body can't be decoded to a string, we return the initial source\n                return source\n        elif not isinstance(source, str):\n            # Ignore any other types\n            return source\n        regex = self.arn_regex if not encoded else self.arn_regex_encoded\n        return regex.sub(\n            lambda m: self._adjust_match(m, static_partition, request_region, encoded=encoded),\n            source,\n        )"
            },
            {
                "name": "self._adjust_partition",
                "code": "def _adjust_partition(\n        self,\n        source,\n        static_partition: str = None,\n        request_region: str = None,\n        encoded: bool = False,\n    ):\n        # Call this function recursively if we get a dictionary or a list\n        if isinstance(source, dict):\n            result = {}\n            for k, v in source.items():\n                result[k] = self._adjust_partition(\n                    v, static_partition, request_region, encoded=encoded\n                )\n            return result\n        if isinstance(source, list):\n            result = []\n            for v in source:\n                result.append(\n                    self._adjust_partition(v, static_partition, request_region, encoded=encoded)\n                )\n            return result\n        elif isinstance(source, bytes):\n            try:\n                decoded = to_str(source)\n                adjusted = self._adjust_partition(\n                    decoded, static_partition, request_region, encoded=encoded\n                )\n                return to_bytes(adjusted)\n            except UnicodeDecodeError:\n                # If the body can't be decoded to a string, we return the initial source\n                return source\n        elif not isinstance(source, str):\n            # Ignore any other types\n            return source\n        regex = self.arn_regex if not encoded else self.arn_regex_encoded\n        return regex.sub(\n            lambda m: self._adjust_match(m, static_partition, request_region, encoded=encoded),\n            source,\n        )"
            }
        ],
        "third_party": [
            "self._post_process_response_headers"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def parse_and_enrich(self, context: RequestContext)",
        "start_line": "65",
        "end_line": "71",
        "file_path": "localstack-core/localstack/aws/handlers/service.py",
        "docstring": "The parse_and_enrich function processes a RequestContext object by creating a parser based on the service specified in the context.\\nIt uses the parser to extract the operation and instance from the context's request.\\nThen, it enriches the context by assigning the parsed operation and service request instance to the context's operation and service_request attributes, respectively.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "450bef605fb4",
        "ground_truth": "def parse_and_enrich(self, context: RequestContext):\n    parser = create_parser(context.service)\n    operation, instance = parser.parse(context.request)\n    # enrich context\n    context.operation = operation\n    context.service_request = instance",
        "import_statements": [
            "import logging",
            "import traceback",
            "from collections import defaultdict",
            "from typing import Any, Dict, Union",
            "from botocore.model import OperationModel, ServiceModel",
            "from localstack import config",
            "from localstack.http import Response",
            "from localstack.utils.coverage_docs import get_coverage_link_for_service"
        ],
        "reference_api": [
            "create_parser",
            "parser.parse"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "create_parser"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def add_skeleton(self, skeleton: Skeleton)",
        "start_line": "124",
        "end_line": "132",
        "file_path": "localstack-core/localstack/aws/handlers/service.py",
        "docstring": "The add_skeleton function registers a Skeleton object with the service handler.\\nIt first retrieves the service name from the skeleton's service attribute.\\nA SkeletonHandler instance is then created using the provided skeleton.\\nFor each operation in the skeleton's dispatch table, the function adds a handler by calling add_handler with a ServiceOperation object (constructed using the service name and operation) and the SkeletonHandler instance.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "510326692eb2",
        "ground_truth": "def add_skeleton(self, skeleton: Skeleton):\n    \"\"\"\n    Creates for each entry in the dispatch table of the skeleton a new route.\n    \"\"\"\n    service = skeleton.service.service_name\n    handler = SkeletonHandler(skeleton)\n    for operation in skeleton.dispatch_table.keys():\n        self.add_handler(ServiceOperation(service, operation), handler)",
        "import_statements": [
            "import logging",
            "import traceback",
            "from collections import defaultdict",
            "from typing import Any, Dict, Union",
            "from botocore.model import OperationModel, ServiceModel",
            "from localstack import config",
            "from localstack.http import Response",
            "from localstack.utils.coverage_docs import get_coverage_link_for_service"
        ],
        "reference_api": [
            "keys",
            "ServiceOperation",
            "SkeletonHandler",
            "self.add_handler"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.add_handler",
                "code": "def add_handler(self, key: ServiceOperation, handler: Handler):\n        if key in self.handlers:\n            LOG.warning(\"overwriting existing route for %s\", key)\n\n        self.handlers[key] = handler"
            }
        ],
        "third_party": [
            "SkeletonHandler",
            "keys",
            "ServiceOperation"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _parse_shape(\n        self, request: Request, shape: Shape, node: Any, uri_params: Mapping[str, Any] = None\n    ) -> Any",
        "start_line": "219",
        "end_line": "272",
        "file_path": "localstack-core/localstack/aws/protocol/parser.py",
        "docstring": "The _parse_shape function processes various types of data from a request based on the specified shape and location.\\nIt handles data extraction from headers, query strings, and URI parameters depending on the serialization location provided in the shape.\\nFor headers, it processes single or comma-separated list values.\\nFor headers maps, it calls a dedicated method to parse them.\\nFor query strings, it extracts single or list values based on the type.\\nFor URI parameters, it retrieves the corresponding value if present.\\nIf no specific location is provided, it uses the given node.\\nThe function then dynamically calls the appropriate handler method to parse the payload based on the shape's type name, and raises a ProtocolParserError for any type mismatch or parsing issues.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "061ea6601cc8",
        "ground_truth": "def _parse_shape(\n    self, request: Request, shape: Shape, node: Any, uri_params: Mapping[str, Any] = None\n) -> Any:\n    \"\"\"\n    Main parsing method which dynamically calls the parsing function for the specific shape.\n    :param request: the complete Request\n    :param shape: of the node\n    :param node: the single part of the HTTP request to parse\n    :param uri_params: the extracted URI path params\n    :return: result of the parsing operation, the type depends on the shape\n    \"\"\"\n    if shape is None:\n        return None\n    location = shape.serialization.get(\"location\")\n    if location is not None:\n        if location == \"header\":\n            header_name = shape.serialization.get(\"name\")\n            payload = request.headers.get(header_name)\n            if payload and shape.type_name == \"list\":\n                # headers may contain a comma separated list of values (e.g., the ObjectAttributes member in\n                # s3.GetObjectAttributes), so we prepare it here for the handler, which will be `_parse_list`.\n                # Header lists can contain optional whitespace, so we strip it\n                # https://www.rfc-editor.org/rfc/rfc9110.html#name-lists-rule-abnf-extension\n                payload = [value.strip() for value in payload.split(\",\")]\n        elif location == \"headers\":\n            payload = self._parse_header_map(shape, request.headers)\n            # shapes with the location trait \"headers\" only contain strings and are not further processed\n            return payload\n        elif location == \"querystring\":\n            query_name = shape.serialization.get(\"name\")\n            parsed_query = request.args\n            if shape.type_name == \"list\":\n                payload = parsed_query.getlist(query_name)\n            else:\n                payload = parsed_query.get(query_name)\n        elif location == \"uri\":\n            uri_param_name = shape.serialization.get(\"name\")\n            if uri_param_name in uri_params:\n                payload = uri_params[uri_param_name]\n        else:\n            raise UnknownParserError(\"Unknown shape location '%s'.\" % location)\n    else:\n        # If we don't have to use a specific location, we use the node\n        payload = node\n    fn_name = \"_parse_%s\" % shape.type_name\n    handler = getattr(self, fn_name, self._noop_parser)\n    try:\n        return handler(request, shape, payload, uri_params) if payload is not None else None\n    except (TypeError, ValueError, AttributeError) as e:\n        raise ProtocolParserError(\n            f\"Invalid type when parsing {shape.name}: '{payload}' cannot be parsed to {shape.type_name}.\"\n        ) from e",
        "import_statements": [
            "import abc",
            "import base64",
            "import datetime",
            "import functools",
            "import re",
            "from abc import ABC",
            "from email.utils import parsedate_to_datetime",
            "from typing import IO, Any, Dict, List, Mapping, Optional, Tuple, Union",
            "from xml.etree import ElementTree as ETree",
            "import dateutil.parser",
            "from botocore.model import (\n    ListShape,\n    MapShape,\n    OperationModel,\n    OperationNotFoundError,\n    ServiceModel,\n    Shape,\n    StructureShape,\n)",
            "from cbor2._decoder import loads as cbor2_loads",
            "from werkzeug.exceptions import BadRequest, NotFound",
            "from localstack.aws.protocol.op_router import RestServiceOperationRouter",
            "from localstack.config import LEGACY_V2_S3_PROVIDER",
            "from localstack.http import Request"
        ],
        "reference_api": [
            "getattr",
            "payload.split",
            "value.strip",
            "self._parse_header_map",
            "parsed_query.getlist",
            "UnknownParserError",
            "get",
            "parsed_query.get",
            "handler",
            "ProtocolParserError"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get",
            "get",
            "get",
            "value.strip",
            "payload.split",
            "self._parse_header_map",
            "get",
            "parsed_query.getlist",
            "parsed_query.get",
            "get",
            "UnknownParserError",
            "handler",
            "ProtocolParserError"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _convert_str_to_timestamp(self, value: str, timestamp_format=None)",
        "start_line": "327",
        "end_line": "333",
        "file_path": "localstack-core/localstack/aws/protocol/parser.py",
        "docstring": "The _convert_str_to_timestamp function converts a string value to a timestamp using a specified format.\\nIf no format is provided, it defaults to the class's TIMESTAMP_FORMAT attribute.\\nThe format is converted to lowercase and used to dynamically select a corresponding conversion method from the class using getattr.\\nThe selected conversion method is then applied to the input string to obtain the final timestamp value, which is returned by the function.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "162b441c7d27",
        "ground_truth": "def _convert_str_to_timestamp(self, value: str, timestamp_format=None):\n    if timestamp_format is None:\n        timestamp_format = self.TIMESTAMP_FORMAT\n    timestamp_format = timestamp_format.lower()\n    converter = getattr(self, \"_timestamp_%s\" % timestamp_format)\n    final_value = converter(value)\n    return final_value",
        "import_statements": [
            "import abc",
            "import base64",
            "import datetime",
            "import functools",
            "import re",
            "from abc import ABC",
            "from email.utils import parsedate_to_datetime",
            "from typing import IO, Any, Dict, List, Mapping, Optional, Tuple, Union",
            "from xml.etree import ElementTree as ETree",
            "import dateutil.parser",
            "from botocore.model import (\n    ListShape,\n    MapShape,\n    OperationModel,\n    OperationNotFoundError,\n    ServiceModel,\n    Shape,\n    StructureShape,\n)",
            "from cbor2._decoder import loads as cbor2_loads",
            "from werkzeug.exceptions import BadRequest, NotFound",
            "from localstack.aws.protocol.op_router import RestServiceOperationRouter",
            "from localstack.config import LEGACY_V2_S3_PROVIDER",
            "from localstack.http import Request"
        ],
        "reference_api": [
            "getattr",
            "converter",
            "timestamp_format.lower"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "timestamp_format.lower",
            "converter"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _parse_structure(\n        self,\n        request: Request,\n        shape: StructureShape,\n        node: dict,\n        uri_params: Mapping[str, Any] = None,\n    ) -> dict",
        "start_line": "416",
        "end_line": "437",
        "file_path": "localstack-core/localstack/aws/protocol/parser.py",
        "docstring": "The _parse_structure function processes a given request, extracting data based on a defined structure shape.\\nIt initializes an empty dictionary called result.\\nFor each member in the structure shape, it determines the serialized name for the member, considering if it is flattened and part of a list.\\nIt processes the member using the _process_member method, obtaining its value from the node.\\nIf the value is not None or the member is required, it adds the member and its value to the result dictionary.\\nFinally, it returns the result dictionary if it contains any members, otherwise, it returns None.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3e11e38c68ac",
        "ground_truth": "def _parse_structure(\n    self,\n    request: Request,\n    shape: StructureShape,\n    node: dict,\n    uri_params: Mapping[str, Any] = None,\n) -> dict:\n    result = {}\n    for member, member_shape in shape.members.items():\n        # The key in the node is either the serialization config \"name\" of the shape, or the name of the member\n        member_name = self._get_serialized_name(member_shape, member, node)\n        # BUT, if it's flattened and a list, the name is defined by the list's member's name\n        if member_shape.serialization.get(\"flattened\"):\n            if isinstance(member_shape, ListShape):\n                member_name = self._get_serialized_name(member_shape.member, member, node)\n        value = self._process_member(request, member_name, member_shape, node, uri_params)\n        if value is not None or member in shape.required_members:\n            # If the member is required, but not existing, we explicitly set None\n            result[member] = value\n    return result if len(result) > 0 else None",
        "import_statements": [
            "import abc",
            "import base64",
            "import datetime",
            "import functools",
            "import re",
            "from abc import ABC",
            "from email.utils import parsedate_to_datetime",
            "from typing import IO, Any, Dict, List, Mapping, Optional, Tuple, Union",
            "from xml.etree import ElementTree as ETree",
            "import dateutil.parser",
            "from botocore.model import (\n    ListShape,\n    MapShape,\n    OperationModel,\n    OperationNotFoundError,\n    ServiceModel,\n    Shape,\n    StructureShape,\n)",
            "from cbor2._decoder import loads as cbor2_loads",
            "from werkzeug.exceptions import BadRequest, NotFound",
            "from localstack.aws.protocol.op_router import RestServiceOperationRouter",
            "from localstack.config import LEGACY_V2_S3_PROVIDER",
            "from localstack.http import Request"
        ],
        "reference_api": [
            "len",
            "isinstance",
            "self._process_member",
            "get",
            "items",
            "self._get_serialized_name"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_serialized_name",
                "code": "_prefix\n\n\nclass BaseRestRequestParser(RequestParser):\n    \"\"\"\n    The ``BaseRestRequestParser`` is the base class for all \"resty\" AWS service protocols.\n    The operation which should be invoked is determined based on the HTTP method and the path suffix.\n    The body encoding is done in th"
            },
            {
                "name": "self._get_serialized_name",
                "code": "_prefix\n\n\nclass BaseRestRequestParser(RequestParser):\n    \"\"\"\n    The ``BaseRestRequestParser`` is the base class for all \"resty\" AWS service protocols.\n    The operation which should be invoked is determined based on the HTTP method and the path suffix.\n    The body encoding is done in th"
            },
            {
                "name": "self._process_member",
                "code": " sub_node is not None\n            else None\n        )\n\n    def _parse_structure(\n        self,\n        request: Request,\n        shape: StructureShape,\n        node: dict,\n        uri_params: Mapping[str, Any] = None,\n    ) -> dict:\n        result = {}\n\n        for member, member_shape in shape.members.items():\n            # The key in the node is either the serialization config \"name\" of the shape, or the name of the member\n            member_name = self._get_serialized_name(member_shape, member, node)\n            # BUT, if it's flattened and a list, the name is defined by the list's member's name\n            if member_shape.serialization.get(\"flattened\"):\n                if isinstance(member_shape, ListShape):\n                    member_name = self._get_serialized_name(member_shape.member, member, node)\n            value = self._process_memb"
            }
        ],
        "third_party": [
            "items",
            "get"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def parse(self, request: Request) -> Tuple[OperationModel, Any]",
        "start_line": "563",
        "end_line": "576",
        "file_path": "localstack-core/localstack/aws/protocol/parser.py",
        "docstring": "The parse function processes an incoming request and returns a tuple containing the operation model and parsed data.\\nIt first attempts to match the request to an operation and extract URI parameters using the _operation_router.\\nIf no matching operation is found, it raises an OperationNotFoundParserError with details about the service, method, and path.\\nIf an operation is found, it retrieves the input shape and initializes a final_parsed dictionary.\\nIf the shape is not None, it calls _parse_payload to populate final_parsed with the parsed data from the request, including the URI parameters.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c309bfe3623c",
        "ground_truth": "def parse(self, request: Request) -> Tuple[OperationModel, Any]:\n    try:\n        operation, uri_params = self._operation_router.match(request)\n    except NotFound as e:\n        raise OperationNotFoundParserError(\n            f\"Unable to find operation for request to service \"\n            f\"{self.service.service_name}: {request.method} {request.path}\"\n        ) from e\n    shape: StructureShape = operation.input_shape\n    final_parsed = {}\n    if shape is not None:\n        self._parse_payload(request, shape, shape.members, uri_params, final_parsed)\n    return operation, final_parsed",
        "import_statements": [
            "import abc",
            "import base64",
            "import datetime",
            "import functools",
            "import re",
            "from abc import ABC",
            "from email.utils import parsedate_to_datetime",
            "from typing import IO, Any, Dict, List, Mapping, Optional, Tuple, Union",
            "from xml.etree import ElementTree as ETree",
            "import dateutil.parser",
            "from botocore.model import (\n    ListShape,\n    MapShape,\n    OperationModel,\n    OperationNotFoundError,\n    ServiceModel,\n    Shape,\n    StructureShape,\n)",
            "from cbor2._decoder import loads as cbor2_loads",
            "from werkzeug.exceptions import BadRequest, NotFound",
            "from localstack.aws.protocol.op_router import RestServiceOperationRouter",
            "from localstack.config import LEGACY_V2_S3_PROVIDER",
            "from localstack.http import Request"
        ],
        "reference_api": [
            "match",
            "self._parse_payload",
            "OperationNotFoundParserError"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._parse_payload",
                "code": "parsed[payload_member_name] = body\n            elif body_shape.type_name == \"string\":\n                # Only set the value if it's not empty (the request's data is an empty binary by default)\n                if request.data:\n                    body = request.data\n                    if isinstance(body, bytes):\n                        body = body.decode(self.DEFAULT_ENCODING)\n                    payload_parsed[payload_member_name] = body\n            elif body_shape.type_name == \"blob\":\n                # This control path is equivalent to operation.has_streaming_input (shape has a payload which is a blob)\n                # in which case we assume essentially an IO[bytes] to be passed. Since the payload can be optional, we\n                # only set the parameter if content_length=0, which indicates an empty request. If the content length is\n                # not set, it could be a streaming response.\n                if request.content_length != 0:\n                    payload_parsed[payload_member_name] = self.create_input_stream(request)\n            else:\n                original_parsed = self._initial_body_parse(request)\n                payload_parsed[payload_member_name] = self._parse_shape(\n                    request, body_shape, original_parsed, uri_params\n                )\n        else:\n            # The payload covers the whole body. We only parse the body if it hasn't been handled by the payload logic.\n            try:\n                non_payload_parsed = self._initial_body_parse(request)\n            except ProtocolParserError:\n                # GET requests should ignore the body, so we just let them pass\n                if not (request.method in [\"GET\", \"HEAD\"] and self.ignore_get_body_errors):\n                    raise\n\n        # even if the payload has been parsed, the rest of the shape needs to be processed as well\n        # (for members which are located outside of the body, like uri or header)\n        non_payload_parsed = self._parse_shape(request, shape, non_payload_parsed, uri_params)\n        # update the final result with the parsed body and the parsed payload (where the payload has precedence)\n        final_parsed.update(non_payload_parsed)\n        final_parsed.update(payload_parsed)\n\n    def _initial_body_parse(self, request: Request) -> Any:\n        \"\"\"\n        This method executes the initial parsing of the body (XML, JSON, or CBOR).\n        The parsed body will afterwards still be walked through and the nodes will be converted to the appropriate\n        types, but this method does the first round of parsing.\n\n        :param request: of which the body should be parsed\n        :return: depending on the actual implementation\n        \"\"\"\n        raise NotImplementedError(\"_initial_body_parse\")\n\n    def _create_event_stream(self, request: Request, shape: Shape) -> Any:\n        # TODO handle event streams\n        raise NotImplementedError(\"_create_event_stream\")\n\n    def create_input_stream(self, request: Request) -> IO[bytes]:\n        \"\"\"\n        Returns an IO object tha"
            }
        ],
        "third_party": [
            "match",
            "OperationNotFoundParserError"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _member_key_name(shape: Shape, member_name: str) -> str:",
        "start_line": "759",
        "end_line": "771",
        "file_path": "localstack-core/localstack/aws/protocol/parser.py",
        "docstring": "The _member_key_name function determines the serialized key name for a given member in a shape.\\nIf the shape is a ListShape and is marked as \"flattened,\" it checks for a specific serialized name for the list member and returns it if available.\\nIf not, it checks for a general serialized name for the shape and returns it if found.\\nIf neither is available, it returns the original member name.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ff2386ce52f6",
        "ground_truth": "def _member_key_name(shape: Shape, member_name: str) -> str:\n    # This method is needed because we have to special case flattened list\n    # with a serialization name.  If this is the case we use the\n    # locationName from the list's member shape as the key name for the\n    # surrounding structure.\n    if isinstance(shape, ListShape) and shape.serialization.get(\"flattened\"):\n        list_member_serialized_name = shape.member.serialization.get(\"name\")\n        if list_member_serialized_name is not None:\n            return list_member_serialized_name\n    serialized_name = shape.serialization.get(\"name\")\n    if serialized_name is not None:\n        return serialized_name\n    return member_name",
        "import_statements": [
            "import abc",
            "import base64",
            "import datetime",
            "import functools",
            "import re",
            "from abc import ABC",
            "from email.utils import parsedate_to_datetime",
            "from typing import IO, Any, Dict, List, Mapping, Optional, Tuple, Union",
            "from xml.etree import ElementTree as ETree",
            "import dateutil.parser",
            "from botocore.model import (\n    ListShape,\n    MapShape,\n    OperationModel,\n    OperationNotFoundError,\n    ServiceModel,\n    Shape,\n    StructureShape,\n)",
            "from cbor2._decoder import loads as cbor2_loads",
            "from werkzeug.exceptions import BadRequest, NotFound",
            "from localstack.aws.protocol.op_router import RestServiceOperationRouter",
            "from localstack.config import LEGACY_V2_S3_PROVIDER",
            "from localstack.http import Request"
        ],
        "reference_api": [
            "get",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get",
            "get",
            "get"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _build_name_to_xml_node(self, parent_node: Union[list, ETree.Element]) -> dict",
        "start_line": "785",
        "end_line": "806",
        "file_path": "localstack-core/localstack/aws/protocol/parser.py",
        "docstring": "The _build_name_to_xml_node function constructs a dictionary mapping XML node names to their corresponding XML elements from a given parent node.\\nIf the parent node is a list, it recursively processes the first element.\\nIt iterates over the child nodes of the parent node and uses the node's tag as the key.\\nIf a key appears multiple times, the corresponding value in the dictionary is converted to a list containing all nodes with that tag.\\nOtherwise, the node is directly stored as the value for that key in the dictionary.\\nThe resulting dictionary is returned.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "27b88dd05f45",
        "ground_truth": "def _build_name_to_xml_node(self, parent_node: Union[list, ETree.Element]) -> dict:\n    # If the parent node is actually a list. We should not be trying\n    # to serialize it to a dictionary. Instead, return the first element\n    # in the list.\n    if isinstance(parent_node, list):\n        return self._build_name_to_xml_node(parent_node[0])\n    xml_dict = {}\n    for item in parent_node:\n        key = self._node_tag(item)\n        if key in xml_dict:\n            # If the key already exists, the most natural\n            # way to handle this is to aggregate repeated\n            # keys into a single list.\n            # <foo>1</foo><foo>2</foo> -> {'foo': [Node(1), Node(2)]}\n            if isinstance(xml_dict[key], list):\n                xml_dict[key].append(item)\n            else:\n                # Convert from a scalar to a list.\n                xml_dict[key] = [xml_dict[key], item]\n        else:\n            xml_dict[key] = item\n    return xml_dict",
        "import_statements": [
            "import abc",
            "import base64",
            "import datetime",
            "import functools",
            "import re",
            "from abc import ABC",
            "from email.utils import parsedate_to_datetime",
            "from typing import IO, Any, Dict, List, Mapping, Optional, Tuple, Union",
            "from xml.etree import ElementTree as ETree",
            "import dateutil.parser",
            "from botocore.model import (\n    ListShape,\n    MapShape,\n    OperationModel,\n    OperationNotFoundError,\n    ServiceModel,\n    Shape,\n    StructureShape,\n)",
            "from cbor2._decoder import loads as cbor2_loads",
            "from werkzeug.exceptions import BadRequest, NotFound",
            "from localstack.aws.protocol.op_router import RestServiceOperationRouter",
            "from localstack.config import LEGACY_V2_S3_PROVIDER",
            "from localstack.http import Request"
        ],
        "reference_api": [
            "append",
            "self._node_tag",
            "self._build_name_to_xml_node",
            "isinstance"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._build_name_to_xml_node",
                "code": "xml_dict[key].append(item)\n                else:\n                    # Convert from a scalar to a list.\n                    xml_dict[key] = [xml_dict[key], item]\n            else:\n                xml_dict[key] = item\n        return xml_dict\n\n    def _create_event_stream(self, request: Request, shape: Shape) -> Any:\n        # TODO handle event streams\n        raise NotImplementedError(\"_create_event_stream\")\n\n\nclass BaseJSONRequestParser(RequestParser, ABC):\n    \"\"\"\n    The ``BaseJSONRequestParser`` is the base class for all JSON-based AWS service protocols.\n    This base-class handles parsing the payload / body as JSON.\n    \"\"\"\n\n    # default timestamp format for JSON requests\n    TIMESTAMP_FORMAT = \"unixtimestamp\"\n    # timestamp format for requests with CBOR content type\n    CBOR_TIMESTAMP_FORMAT = \"unixtimestampmillis\"\n\n    def _parse_structure(\n        self,\n        request: Request,\n        shape: StructureShape,\n        value: Optional[dict],\n        uri_params: Mapping[str, Any] = None,\n    ) -> Optional[dict]:\n        "
            },
            {
                "name": "self._node_tag",
                "code": "t None:\n            return serialized_name\n        return member_name\n\n    @staticmethod\n    def _pa"
            }
        ],
        "third_party": [
            "append"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def serialize_to_response(\n        self,\n        response: dict,\n        operation_model: OperationModel,\n        headers: Optional[Dict | Headers],\n        request_id: str,\n    ) -> Response",
        "start_line": "187",
        "end_line": "230",
        "file_path": "localstack-core/localstack/aws/protocol/serializer.py",
        "docstring": "The serialize_to_response function converts a response dictionary into a Response object for a given operation model.\\nIt first determines the preferred MIME type based on supported MIME types and the Accept header.\\nIf the operation has a streaming output, it calls a specific serialization method for event streams.\\nOtherwise, it creates a default serialized response and retrieves the output shape and its members.\\nThe function then serializes the response data, adds any additional traits to the response, and returns the final serialized response object.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "03c381c1c3c7",
        "ground_truth": "def serialize_to_response(\n    self,\n    response: dict,\n    operation_model: OperationModel,\n    headers: Optional[Dict | Headers],\n    request_id: str,\n) -> Response:\n    \"\"\"\n    Takes a response dict and serializes it to an actual HttpResponse.\n    :param response: to serialize\n    :param operation_model: specification of the service & operation containing information about the shape of the\n                            service's output / response\n    :param headers: the headers of the incoming request this response should be serialized for. This is necessary\n                    for features like Content-Negotiation (define response content type based on request headers).\n    :param request_id: autogenerated AWS request ID identifying the original request\n    :return: Response which can be sent to the calling client\n    :raises: ResponseSerializerError (either a ProtocolSerializerError or an UnknownSerializerError)\n    \"\"\"\n    # determine the preferred mime type (based on the serializer's supported mime types and the Accept header)\n    mime_type = self._get_mime_type(headers)\n    # if the operation has a streaming output, handle the serialization differently\n    if operation_model.has_event_stream_output:\n        return self._serialize_event_stream(response, operation_model, mime_type, request_id)\n    serialized_response = self._create_default_response(operation_model, mime_type)\n    shape = operation_model.output_shape\n    # The shape can also be none (for empty responses), but it still needs to be serialized (to add some metadata)\n    shape_members = shape.members if shape is not None else None\n    self._serialize_response(\n        response,\n        serialized_response,\n        shape,\n        shape_members,\n        operation_model,\n        mime_type,\n        request_id,\n    )\n    serialized_response = self._prepare_additional_traits_in_response(\n        serialized_response, operation_model, request_id\n    )\n    return serialized_response",
        "import_statements": [
            "import abc",
            "import base64",
            "import functools",
            "import json",
            "import logging",
            "import string",
            "from abc import ABC",
            "from binascii import crc32",
            "from datetime import datetime",
            "from email.utils import formatdate",
            "from struct import pack",
            "from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Union",
            "from xml.etree import ElementTree as ETree",
            "import xmltodict",
            "from botocore.model import ListShape, MapShape, OperationModel, ServiceModel, Shape, StructureShape",
            "from botocore.serialize import ISO8601, ISO8601_MICRO",
            "from botocore.utils import calculate_md5, is_json_value_header, parse_to_aware_datetime",
            "from cbor2._encoder import dumps as cbor2_dumps",
            "from werkzeug import Request as WerkzeugRequest",
            "from werkzeug import Response as WerkzeugResponse",
            "from werkzeug.datastructures import Headers, MIMEAccept",
            "from werkzeug.http import parse_accept_header",
            "from localstack.aws.api import CommonServiceException, ServiceException",
            "from localstack.aws.spec import ProtocolName, load_service",
            "from localstack.constants import (\n    APPLICATION_AMZ_CBOR_1_1,\n    APPLICATION_AMZ_JSON_1_0,\n    APPLICATION_AMZ_JSON_1_1,\n    APPLICATION_CBOR,\n    APPLICATION_JSON,\n    APPLICATION_XML,\n    TEXT_XML,\n)",
            "from localstack.http import Response",
            "from localstack.utils.common import to_bytes, to_str",
            "from localstack.utils.strings import long_uid",
            "from localstack.utils.xml import strip_xmlns"
        ],
        "reference_api": [
            "self._get_mime_type",
            "self._create_default_response",
            "self._serialize_event_stream",
            "self._serialize_response",
            "self._prepare_additional_traits_in_response"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_mime_type",
                "code": "should prefer JSON encoding.\n            content_type = headers.get(\"Content-Type\")\n            LOG.debug(\n                \"No accept header given. Using request's Content-Type (%s) as preferred response Content-Type.\",\n                content_type,\n            )\n            accept_header = content_type + \", */*\"\n        mime_accept: MIMEAccept = parse_accept_header(accept_header, MIMEAccept)\n        mime_type = mime_accept.best_match(self.SUPPORTED_MIME_TYPES)\n        if not mime_type:\n            # There is no match between the supported mime types and the requested one(s)\n            mime_type = self.SUPPORTED_MIME_TYPES[0]\n            LOG.debug(\n                \"Determined accept type (%s) is not supported by this serializer. Using default of this serializer: %s\",\n                accept_header,\n                mime_type,\n            )\n        return mime_type\n\n    # Some extra utility methods subclasses can use.\n\n    @staticmethod\n    def _timestamp_iso8601(value: datetime) -> str:\n        if value.microsecond > 0:\n            timestamp_format = ISO8601_MICRO\n        else:\n            timestamp_format = ISO8601\n        return value.strftime(timestamp_format)\n\n    @staticmethod\n    def _timestamp_unixtimestamp(value: datetime) -> float:\n        return value.timestamp()\n\n    def _timestamp_rfc822(self, value: datetime) -> str:\n        if isinstance(value, datetime):\n            value = self._timestamp_unixtimestamp(value)\n        return formatdate(value, usegmt=True)\n\n    def _convert_timestamp_to_str(\n        self, value: Union[int, str, datetime], timestamp_format=None\n    ) -> str:\n        if timestamp_format is None:\n            timestamp_format = self.TIMESTAMP_FORMAT\n        timestamp_format = timestamp_format.lower()\n        datetime_obj = parse_to_aware_datetime(value)\n        converter = g"
            },
            {
                "name": "self._serialize_event_stream",
                "code": "n operation specific serialization\n        def event_stream_serializer() -> Iterable[bytes]:\n            yield self._encode_event_payload(\"initial-response\")\n\n            # create a default response\n            serialized_event_response = self._create_default_response(operation_model, mime_type)\n            # get the members of the event stream shape\n            event_stream_shape_members = (\n                event_stream_shape.members if event_stream_shape is not None else None\n            )\n            # extract the generator from the given response data\n            event_generator = response.get(event_stream_member_name)\n            if not isinstance(event_generator, Iterator):\n                raise ProtocolSerializerError(\n                    \"Expected iterator for streaming event serialization.\"\n                )\n\n            # yield one event per generated event\n            for event in event_generator:\n                # find the actual event payload (the member with event=true)\n                event_member_shape = None\n                event_member_name = None\n                for member_name, member_shape in event_stream_shape_members.items():\n                    if member_shape.serialization.get(\"event\") and member_name in event:\n                        event_member_shape = member_shape\n                        event_member_name = member_name\n                        break\n                if event_member_shape is None:\n                    raise UnknownSerializerError(\"Couldn't find event shape for serialization.\")\n\n                # serialize the part of the response for the event\n                self._serialize_response(\n                    event.get(event_member_name),\n                    serialized_event_response,\n                    event_member_shape,\n                    event_member_shape.members if event_member_shape is not None else None,\n                    operation_model,\n                    mime_type,\n                    request_id,\n                )\n                # execute additional response traits (might be modifying the response)\n                serialized_event_response = self._prepare_additional_traits_in_response(\n                    serialized_event_response, operation_model, request_id\n                )\n                # encode the event and yield it\n                yield self._encode_event_payload(\n                    event_type=event_member_name, content=serialized_event_response.data\n                )\n\n        return Response(\n            response=event_stream_serializer(),\n            status=operation_model.http.get(\"responseCode\", 200),\n        )\n\n    def _encode_event_payload(\n        self,\n        event_type: str,\n        content: Union[str, bytes] = \"\",\n        error_code: Optional[str] = None,\n        error_message: Optional[str] = None,\n    ) -> bytes:\n        \"\"\"\n        Encodes the given event payload according to AWS specific binary event encoding.\n        A specification of the format can be found in the AWS docs:\n        https://docs.aws.amazon.com/AmazonS3/latest/API/RESTSelectObjectAppendix.html\n\n        :param content: string or bytes of the event payload\n        :param event_type: type of the event. Usually the name of the event shape or specific event types like\n                            \"initial-response\".\n        :param error_code: Optional. Error code if the payload represents an error.\n        :param error_message: Optional. Error message if the payload represents an error.\n        :return: bytes with the AWS-specific encoded event payload\n        \"\"\"\n\n        # "
            },
            {
                "name": "self._create_default_response",
                "code": " preferred mime type to be used by the serializer (if it is not accepted by the client,\n                 an error is logged)\n        \"\"\"\n        accept_header = None\n        if headers and \"Accept\" in headers and not headers.get(\"Accept\") == \"*/*\":\n            accept_header = headers.get(\"Accept\")\n        elif headers and headers.get(\"Content-Type\"):\n            # If there is no specific Accept header given, we use the given Content-Type as a fallback.\n            # i.e. if the request content was JSON encoded and the client doesn't send a specific an Accept header, the\n            # seria"
            },
            {
                "name": "self._serialize_response",
                "code": "equest\n        :return: string containing the serialized body\n        \"\"\"\n        raise NotImplementedError\n\n    def _serialize_error(\n        self,\n        error: ServiceException,\n        response: Response,\n        shape: StructureShape,\n        operation_model: OperationModel,\n        mi"
            },
            {
                "name": "self._prepare_additional_traits_in_response",
                "code": "d5_digest\n\n    def _get_error_message(self, error: Exception) -> Optional[str]:\n        return str(error) if error is not None and str(error) != \"None\" else None\n\n\nclass BaseXMLResponseSerializer(ResponseSerializer):\n    \"\"\"\n    The BaseXMLResponseSerializer performs the basic logic for the XML response serialization.\n    It is slightly ada"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def serialize_error_to_response(\n        self,\n        error: ServiceException,\n        operation_model: OperationModel,\n        headers: Optional[Dict | Headers],\n        request_id: str,\n    ) -> Response",
        "start_line": "233",
        "end_line": "271",
        "file_path": "localstack-core/localstack/aws/protocol/serializer.py",
        "docstring": "The serialize_error_to_response function creates a serialized HTTP response for a given ServiceException.\\nIt first determines the preferred mime type based on the headers.\\nIt then initializes a default response based on the operation model and mime type.\\nIf the error is not a ServiceException, it raises a ProtocolSerializerError.\\nThe function retrieves the shape for the error code from the service model and sets the status code for the response.\\nIt serializes the error details into the response using the determined shape and mime type.\\nFinally, it prepares any additional traits in the response and returns the serialized response.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "785759eb9862",
        "ground_truth": "def serialize_error_to_response(\n    self,\n    error: ServiceException,\n    operation_model: OperationModel,\n    headers: Optional[Dict | Headers],\n    request_id: str,\n) -> Response:\n    \"\"\"\n    Takes an error instance and serializes it to an actual HttpResponse.\n    Therefore, this method is used for errors which should be serialized and transmitted to the calling client.\n    :param error: to serialize\n    :param operation_model: specification of the service & operation containing information about the shape of the\n                            service's output / response\n    :param headers: the headers of the incoming request this response should be serialized for. This is necessary\n                    for features like Content-Negotiation (define response content type based on request headers).\n    :param request_id: autogenerated AWS request ID identifying the original request\n    :return: HttpResponse which can be sent to the calling client\n    :raises: ResponseSerializerError (either a ProtocolSerializerError or an UnknownSerializerError)\n    \"\"\"\n    # determine the preferred mime type (based on the serializer's supported mime types and the Accept header)\n    mime_type = self._get_mime_type(headers)\n    # TODO implement streaming error serialization\n    serialized_response = self._create_default_response(operation_model, mime_type)\n    if not error or not isinstance(error, ServiceException):\n        raise ProtocolSerializerError(\n            f\"Error to serialize ({error.__class__.__name__ if error else None}) is not a ServiceException.\"\n        )\n    shape = operation_model.service_model.shape_for_error_code(error.code)\n    serialized_response.status_code = error.status_code\n    self._serialize_error(\n        error, serialized_response, shape, operation_model, mime_type, request_id\n    )\n    serialized_response = self._prepare_additional_traits_in_response(\n        serialized_response, operation_model, request_id\n    )\n    return serialized_response",
        "import_statements": [
            "import abc",
            "import base64",
            "import functools",
            "import json",
            "import logging",
            "import string",
            "from abc import ABC",
            "from binascii import crc32",
            "from datetime import datetime",
            "from email.utils import formatdate",
            "from struct import pack",
            "from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Union",
            "from xml.etree import ElementTree as ETree",
            "import xmltodict",
            "from botocore.model import ListShape, MapShape, OperationModel, ServiceModel, Shape, StructureShape",
            "from botocore.serialize import ISO8601, ISO8601_MICRO",
            "from botocore.utils import calculate_md5, is_json_value_header, parse_to_aware_datetime",
            "from cbor2._encoder import dumps as cbor2_dumps",
            "from werkzeug import Request as WerkzeugRequest",
            "from werkzeug import Response as WerkzeugResponse",
            "from werkzeug.datastructures import Headers, MIMEAccept",
            "from werkzeug.http import parse_accept_header",
            "from localstack.aws.api import CommonServiceException, ServiceException",
            "from localstack.aws.spec import ProtocolName, load_service",
            "from localstack.constants import (\n    APPLICATION_AMZ_CBOR_1_1,\n    APPLICATION_AMZ_JSON_1_0,\n    APPLICATION_AMZ_JSON_1_1,\n    APPLICATION_CBOR,\n    APPLICATION_JSON,\n    APPLICATION_XML,\n    TEXT_XML,\n)",
            "from localstack.http import Response",
            "from localstack.utils.common import to_bytes, to_str",
            "from localstack.utils.strings import long_uid",
            "from localstack.utils.xml import strip_xmlns"
        ],
        "reference_api": [
            "ProtocolSerializerError",
            "self._serialize_error",
            "self._get_mime_type",
            "self._create_default_response",
            "isinstance",
            "shape_for_error_code",
            "self._prepare_additional_traits_in_response"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_mime_type",
                "code": "should prefer JSON encoding.\n            content_type = headers.get(\"Content-Type\")\n            LOG.debug(\n                \"No accept header given. Using request's Content-Type (%s) as preferred response Content-Type.\",\n                content_type,\n            )\n            accept_header = content_type + \", */*\"\n        mime_accept: MIMEAccept = parse_accept_header(accept_header, MIMEAccept)\n        mime_type = mime_accept.best_match(self.SUPPORTED_MIME_TYPES)\n        if not mime_type:\n            # There is no match between the supported mime types and the requested one(s)\n            mime_type = self.SUPPORTED_MIME_TYPES[0]\n            LOG.debug(\n                \"Determined accept type (%s) is not supported by this serializer. Using default of this serializer: %s\",\n                accept_header,\n                mime_type,\n            )\n        return mime_type\n\n    # Some extra utility methods subclasses can use.\n\n    @staticmethod\n    def _timestamp_iso8601(value: datetime) -> str:\n        if value.microsecond > 0:\n            timestamp_format = ISO8601_MICRO\n        else:\n            timestamp_format = ISO8601\n        return value.strftime(timestamp_format)\n\n    @staticmethod\n    def _timestamp_unixtimestamp(value: datetime) -> float:\n        return value.timestamp()\n\n    def _timestamp_rfc822(self, value: datetime) -> str:\n        if isinstance(value, datetime):\n            value = self._timestamp_unixtimestamp(value)\n        return formatdate(value, usegmt=True)\n\n    def _convert_timestamp_to_str(\n        self, value: Union[int, str, datetime], timestamp_format=None\n    ) -> str:\n        if timestamp_format is None:\n            timestamp_format = self.TIMESTAMP_FORMAT\n        timestamp_format = timestamp_format.lower()\n        datetime_obj = parse_to_aware_datetime(value)\n        converter = g"
            },
            {
                "name": "self._create_default_response",
                "code": " preferred mime type to be used by the serializer (if it is not accepted by the client,\n                 an error is logged)\n        \"\"\"\n        accept_header = None\n        if headers and \"Accept\" in headers and not headers.get(\"Accept\") == \"*/*\":\n            accept_header = headers.get(\"Accept\")\n        elif headers and headers.get(\"Content-Type\"):\n            # If there is no specific Accept header given, we use the given Content-Type as a fallback.\n            # i.e. if the request content was JSON encoded and the client doesn't send a specific an Accept header, the\n            # seria"
            },
            {
                "name": "self._serialize_error",
                "code": "     :return: Response which can directly be sent to the client (in chunks)\n        \"\"\"\n        event_stream_shape = operation_model.get_event_stream_output()\n        event_stream_member_name = operation_model.output_shape.event_stream_name\n\n        # wrap the gener"
            },
            {
                "name": "self._prepare_additional_traits_in_response",
                "code": "d5_digest\n\n    def _get_error_message(self, error: Exception) -> Optional[str]:\n        return str(error) if error is not None and str(error) != \"None\" else None\n\n\nclass BaseXMLResponseSerializer(ResponseSerializer):\n    \"\"\"\n    The BaseXMLResponseSerializer performs the basic logic for the XML response serialization.\n    It is slightly ada"
            }
        ],
        "third_party": [
            "ProtocolSerializerError",
            "shape_for_error_code"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _node_to_string(self, root: Optional[ETree.Element], mime_type: str) -> Optional[str]",
        "start_line": "854",
        "end_line": "865",
        "file_path": "localstack-core/localstack/aws/protocol/serializer.py",
        "docstring": "The _node_to_string function converts an XML node to a string representation based on the specified MIME type.\\nIf the root node is not None, it first converts the XML element to a string with the default encoding and XML declaration.\\nIf the MIME type is APPLICATION_JSON, it converts the XML content to a dictionary using xmltodict, strips the namespaces, and then serializes it to a JSON string.\\nThe function returns the resulting string content.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "33055e116784",
        "ground_truth": "def _node_to_string(self, root: Optional[ETree.Element], mime_type: str) -> Optional[str]:\n    \"\"\"Generates the string representation of the given XML element.\"\"\"\n    if root is not None:\n        content = ETree.tostring(\n            element=root, encoding=self.DEFAULT_ENCODING, xml_declaration=True\n        )\n        if mime_type == APPLICATION_JSON:\n            # FIXME try to directly convert the ElementTree node to JSON\n            xml_dict = xmltodict.parse(content)\n            xml_dict = strip_xmlns(xml_dict)\n            content = json.dumps(xml_dict)\n        return content",
        "import_statements": [
            "import abc",
            "import base64",
            "import functools",
            "import json",
            "import logging",
            "import string",
            "from abc import ABC",
            "from binascii import crc32",
            "from datetime import datetime",
            "from email.utils import formatdate",
            "from struct import pack",
            "from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Union",
            "from xml.etree import ElementTree as ETree",
            "import xmltodict",
            "from botocore.model import ListShape, MapShape, OperationModel, ServiceModel, Shape, StructureShape",
            "from botocore.serialize import ISO8601, ISO8601_MICRO",
            "from botocore.utils import calculate_md5, is_json_value_header, parse_to_aware_datetime",
            "from cbor2._encoder import dumps as cbor2_dumps",
            "from werkzeug import Request as WerkzeugRequest",
            "from werkzeug import Response as WerkzeugResponse",
            "from werkzeug.datastructures import Headers, MIMEAccept",
            "from werkzeug.http import parse_accept_header",
            "from localstack.aws.api import CommonServiceException, ServiceException",
            "from localstack.aws.spec import ProtocolName, load_service",
            "from localstack.constants import (\n    APPLICATION_AMZ_CBOR_1_1,\n    APPLICATION_AMZ_JSON_1_0,\n    APPLICATION_AMZ_JSON_1_1,\n    APPLICATION_CBOR,\n    APPLICATION_JSON,\n    APPLICATION_XML,\n    TEXT_XML,\n)",
            "from localstack.http import Response",
            "from localstack.utils.common import to_bytes, to_str",
            "from localstack.utils.strings import long_uid",
            "from localstack.utils.xml import strip_xmlns"
        ],
        "reference_api": [
            "json.dumps",
            "ETree.tostring",
            "strip_xmlns",
            "xmltodict.parse"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ETree.tostring",
            "xmltodict.parse",
            "strip_xmlns"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _prepare_additional_traits_in_xml(self, root: Optional[ETree.Element], request_id: str)",
        "start_line": "1182",
        "end_line": "1194",
        "file_path": "localstack-core/localstack/aws/protocol/serializer.py",
        "docstring": "The _prepare_additional_traits_in_xml function modifies an XML tree structure. If the root element has children, it moves all child elements from the first child of the root to the root itself and then removes the first child. Additionally, it creates a new sub-element named \"requestId\" in the root and sets its text to the provided request_id value.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4d70aff91753",
        "ground_truth": "def _prepare_additional_traits_in_xml(self, root: Optional[ETree.Element], request_id: str):\n    # The EC2 protocol does not use the root output shape, therefore we need to remove the hierarchy level\n    # below the root level\n    if len(root) > 0:\n        output_node = root[0]\n        for child in output_node:\n            root.append(child)\n        root.remove(output_node)\n    # Add the requestId here (it's not defined in the specs)\n    # For the ec2 and the query protocol, the root cannot be None at this time.\n    request_id_element = ETree.SubElement(root, \"requestId\")\n    request_id_element.text = request_id",
        "import_statements": [
            "import abc",
            "import base64",
            "import functools",
            "import json",
            "import logging",
            "import string",
            "from abc import ABC",
            "from binascii import crc32",
            "from datetime import datetime",
            "from email.utils import formatdate",
            "from struct import pack",
            "from typing import Any, Dict, Iterable, Iterator, List, Optional, Tuple, Union",
            "from xml.etree import ElementTree as ETree",
            "import xmltodict",
            "from botocore.model import ListShape, MapShape, OperationModel, ServiceModel, Shape, StructureShape",
            "from botocore.serialize import ISO8601, ISO8601_MICRO",
            "from botocore.utils import calculate_md5, is_json_value_header, parse_to_aware_datetime",
            "from cbor2._encoder import dumps as cbor2_dumps",
            "from werkzeug import Request as WerkzeugRequest",
            "from werkzeug import Response as WerkzeugResponse",
            "from werkzeug.datastructures import Headers, MIMEAccept",
            "from werkzeug.http import parse_accept_header",
            "from localstack.aws.api import CommonServiceException, ServiceException",
            "from localstack.aws.spec import ProtocolName, load_service",
            "from localstack.constants import (\n    APPLICATION_AMZ_CBOR_1_1,\n    APPLICATION_AMZ_JSON_1_0,\n    APPLICATION_AMZ_JSON_1_1,\n    APPLICATION_CBOR,\n    APPLICATION_JSON,\n    APPLICATION_XML,\n    TEXT_XML,\n)",
            "from localstack.http import Response",
            "from localstack.utils.common import to_bytes, to_str",
            "from localstack.utils.strings import long_uid",
            "from localstack.utils.xml import strip_xmlns"
        ],
        "reference_api": [
            "ETree.SubElement",
            "root.remove",
            "root.append",
            "len"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "root.append",
            "root.remove",
            "ETree.SubElement"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _extract_service_indicators(request: Request) -> _ServiceIndicators",
        "start_line": "49",
        "end_line": "74",
        "file_path": "localstack-core/localstack/aws/protocol/service_router.py",
        "docstring": "The _extract_service_indicators function extracts service-related indicators from an incoming request to determine the target service.\\nIt retrieves the 'x-amz-target' and 'authorization' headers from the request.\\nIf the 'authorization' header is present and uses AWS's signature version 4, it parses it to extract the signing name.\\nFor the 'x-amz-target' header, it splits the value into target prefix and operation if applicable.\\nThe function returns a _ServiceIndicators object containing the signing name, target prefix, operation, request host, and request path.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5d789e377edb",
        "ground_truth": "def _extract_service_indicators(request: Request) -> _ServiceIndicators:\n    \"\"\"Extracts all different fields that might indicate which service a request is targeting.\"\"\"\n    x_amz_target = request.headers.get(\"x-amz-target\")\n    authorization = request.headers.get(\"authorization\")\n     signing_name = None\n    if authorization:\n        try:\n            auth_type, auth_info = authorization.split(None, 1)\n            auth_type = auth_type.lower().strip()\n            if auth_type == \"aws4-hmac-sha256\":\n                values = parse_dict_header(auth_info)\n                _, _, _, signing_name, _ = values[\"Credential\"].split(\"/\")\n        except (ValueError, KeyError):\n            LOG.debug(\"auth header could not be parsed for service routing: %s\", authorization)\n            pass\n    if x_amz_target:\n        if \".\" in x_amz_target:\n            target_prefix, operation = x_amz_target.split(\".\", 1)\n        else:\n            target_prefix = None\n            operation = x_amz_target\n    else:\n        target_prefix, operation = None, None\n     return _ServiceIndicators(signing_name, target_prefix, operation, request.host, request.path)",
        "import_statements": [
            "import logging",
            "import os",
            "from typing import NamedTuple, Optional, Set",
            "import botocore",
            "from botocore.model import ServiceModel",
            "from werkzeug.exceptions import RequestEntityTooLarge",
            "from werkzeug.http import parse_dict_header",
            "from localstack import config",
            "from localstack.aws.spec import (\n    ServiceCatalog,\n    ServiceModelIdentifier,\n    build_service_index_cache,\n    load_service_index_cache,\n)",
            "from localstack.constants import LOCALHOST_HOSTNAME, PATH_USER_REQUEST, VERSION",
            "from localstack.http import Request",
            "from localstack.services.s3.utils import uses_host_addressing",
            "from localstack.services.sqs.utils import is_sqs_queue_url",
            "from localstack.utils.objects import singleton_factory",
            "from localstack.utils.strings import to_bytes",
            "from localstack.utils.urls import hostname_from_url"
        ],
        "reference_api": [
            "_ServiceIndicators",
            "parse_dict_header",
            "authorization.split",
            "split",
            "get",
            "LOG.debug",
            "strip",
            "x_amz_target.split",
            "auth_type.lower"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get",
            "get",
            "authorization.split",
            "strip",
            "auth_type.lower",
            "parse_dict_header",
            "split",
            "LOG.debug",
            "x_amz_target.split",
            "_ServiceIndicators"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def custom_signing_name_rules(signing_name: str, path: str) -> Optional[ServiceModelIdentifier]",
        "start_line": "116",
        "end_line": "136",
        "file_path": "localstack-core/localstack/aws/protocol/service_router.py",
        "docstring": "The custom_signing_name_rules function determines the appropriate ServiceModelIdentifier based on the signing name and request path.\\nIt first checks if there are predefined rules for the given signing name.\\nIf no rules exist and the signing name is \"servicecatalog\", it distinguishes between \"servicecatalog\" and \"servicecatalog-appregistry\" based on whether the path is root (\"/\") or not.\\nFor other signing names with rules, it matches the path against the prefixes in the rules and returns the corresponding service model identifier.\\nIf no prefix matches, it defaults to the identifier for the signing name.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f4ae1963301f",
        "ground_truth": "def custom_signing_name_rules(signing_name: str, path: str) -> Optional[ServiceModelIdentifier]:\n    \"\"\"\n    Rules which are based on the signing name (in the auth header) and the request path.\n    \"\"\"\n    rules = signing_name_path_prefix_rules.get(signing_name)\n     if not rules:\n        if signing_name == \"servicecatalog\":\n            if path == \"/\":\n                # servicecatalog uses the protocol json (only uses root-path URIs, i.e. only /)\n                return ServiceModelIdentifier(\"servicecatalog\")\n            else:\n                # servicecatalog-appregistry uses rest-json (only uses non-root-path request URIs)\n                return ServiceModelIdentifier(\"servicecatalog-appregistry\")\n        return\n     for prefix, service_model_identifier in rules.items():\n        if path.startswith(prefix):\n            return service_model_identifier\n     return rules.get(\"*\", ServiceModelIdentifier(signing_name))",
        "import_statements": [
            "import logging",
            "import os",
            "from typing import NamedTuple, Optional, Set",
            "import botocore",
            "from botocore.model import ServiceModel",
            "from werkzeug.exceptions import RequestEntityTooLarge",
            "from werkzeug.http import parse_dict_header",
            "from localstack import config",
            "from localstack.aws.spec import (\n    ServiceCatalog,\n    ServiceModelIdentifier,\n    build_service_index_cache,\n    load_service_index_cache,\n)",
            "from localstack.constants import LOCALHOST_HOSTNAME, PATH_USER_REQUEST, VERSION",
            "from localstack.http import Request",
            "from localstack.services.s3.utils import uses_host_addressing",
            "from localstack.services.sqs.utils import is_sqs_queue_url",
            "from localstack.utils.objects import singleton_factory",
            "from localstack.utils.strings import to_bytes",
            "from localstack.utils.urls import hostname_from_url"
        ],
        "reference_api": [
            "signing_name_path_prefix_rules.get",
            "path.startswith",
            "ServiceModelIdentifier",
            "rules.items",
            "rules.get"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "signing_name_path_prefix_rules.get",
            "ServiceModelIdentifier",
            "ServiceModelIdentifier",
            "rules.items",
            "path.startswith",
            "rules.get",
            "ServiceModelIdentifier"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def custom_path_addressing_rules(path: str) -> Optional[ServiceModelIdentifier]",
        "start_line": "158",
        "end_line": "167",
        "file_path": "localstack-core/localstack/aws/protocol/service_router.py",
        "docstring": "The custom_path_addressing_rules function determines the service model identifier based on the request path.\\nIt returns a ServiceModelIdentifier for SQS if the path matches an SQS queue URL using the query protocol.\\nIf the path starts with \"/2015-03-31/functions/\", it returns a ServiceModelIdentifier for Lambda.\\nIf neither condition is met, it returns None.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "23c0d3396905",
        "ground_truth": "def custom_path_addressing_rules(path: str) -> Optional[ServiceModelIdentifier]:\n    \"\"\"\n    Rules which are only based on the request path.\n    \"\"\"\n     if is_sqs_queue_url(path):\n        return ServiceModelIdentifier(\"sqs\", protocol=\"query\")\n     if path.startswith(\"/2015-03-31/functions/\"):\n        return ServiceModelIdentifier(\"lambda\")",
        "import_statements": [
            "import logging",
            "import os",
            "from typing import NamedTuple, Optional, Set",
            "import botocore",
            "from botocore.model import ServiceModel",
            "from werkzeug.exceptions import RequestEntityTooLarge",
            "from werkzeug.http import parse_dict_header",
            "from localstack import config",
            "from localstack.aws.spec import (\n    ServiceCatalog,\n    ServiceModelIdentifier,\n    build_service_index_cache,\n    load_service_index_cache,\n)",
            "from localstack.constants import LOCALHOST_HOSTNAME, PATH_USER_REQUEST, VERSION",
            "from localstack.http import Request",
            "from localstack.services.s3.utils import uses_host_addressing",
            "from localstack.services.sqs.utils import is_sqs_queue_url",
            "from localstack.utils.objects import singleton_factory",
            "from localstack.utils.strings import to_bytes",
            "from localstack.utils.urls import hostname_from_url"
        ],
        "reference_api": [
            "path.startswith",
            "is_sqs_queue_url",
            "ServiceModelIdentifier"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "is_sqs_queue_url",
            "ServiceModelIdentifier",
            "path.startswith",
            "ServiceModelIdentifier"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def determine_aws_service_model_for_data_plane(\n    request: Request, services: ServiceCatalog = None\n) -> Optional[ServiceModel]",
        "start_line": "316",
        "end_line": "326",
        "file_path": "localstack-core/localstack/aws/protocol/service_router.py",
        "docstring": "The function determine_aws_service_model_for_data_plane attempts to determine the AWS service model for a data plane request.\\nIt takes a request object and an optional ServiceCatalog object as input parameters.\\nIt checks if the request host matches custom host addressing rules.\\nIf a match is found, it retrieves the service catalog (if not provided) and returns the corresponding service model using the custom host match details.\\nIf no custom host match is found, the function returns None.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ddc3bc153bef",
        "ground_truth": "def determine_aws_service_model_for_data_plane(\n    request: Request, services: ServiceCatalog = None\n) -> Optional[ServiceModel]:\n    \"\"\"\n    A stripped down version of ``determine_aws_service_model`` which only checks hostname indicators for\n    the AWS data plane, such as s3 websites, lambda function URLs, or API gateway routes.\n    \"\"\"\n    custom_host_match = custom_host_addressing_rules(request.host)\n    if custom_host_match:\n        services = services or get_service_catalog()\n        return services.get(*custom_host_match)",
        "import_statements": [
            "import logging",
            "import os",
            "from typing import NamedTuple, Optional, Set",
            "import botocore",
            "from botocore.model import ServiceModel",
            "from werkzeug.exceptions import RequestEntityTooLarge",
            "from werkzeug.http import parse_dict_header",
            "from localstack import config",
            "from localstack.aws.spec import (\n    ServiceCatalog,\n    ServiceModelIdentifier,\n    build_service_index_cache,\n    load_service_index_cache,\n)",
            "from localstack.constants import LOCALHOST_HOSTNAME, PATH_USER_REQUEST, VERSION",
            "from localstack.http import Request",
            "from localstack.services.s3.utils import uses_host_addressing",
            "from localstack.services.sqs.utils import is_sqs_queue_url",
            "from localstack.utils.objects import singleton_factory",
            "from localstack.utils.strings import to_bytes",
            "from localstack.utils.urls import hostname_from_url"
        ],
        "reference_api": [
            "services.get",
            "get_service_catalog",
            "custom_host_addressing_rules"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "custom_host_addressing_rules",
                "code": "def custom_host_addressing_rules(host: str) -> Optional[ServiceModelIdentifier]:\n    \"\"\"\n    Rules based on the host header of the request, which is typically the data plane of a service.\n\n    Some services are added through a patch in ext.\n    \"\"\"\n\n    # a note on ``.execute-api.`` and why it shouldn't be added as a check here: ``.execute-api.`` was previously\n    # mapped distinctly to ``apigateway``, but this assumption is too strong, since the URL can be apigw v1, v2,\n    # or apigw management api. so in short, simply based on the host header, it's not possible to unambiguously\n    # associate a specific apigw service to the request.\n\n    if \".lambda-url.\" in host:\n        return ServiceModelIdentifier(\"lambda\")\n\n    if \".s3-website.\" in host:\n        return ServiceModelIdentifier(\"s3\")"
            }
        ],
        "third_party": [
            "get_service_catalog",
            "services.get"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def get_account_id_from_access_key_id(access_key_id: str) -> str",
        "start_line": "60",
        "end_line": "81",
        "file_path": "localstack-core/localstack/aws/accounts.py",
        "docstring": "The get_account_id_from_access_key_id function returns the Account ID associated with a given AWS Access Key ID.\\nIf the Access Key ID is a 12-digit number, it directly returns this as the Account ID.\\nIf the Access Key ID has a length of 20 or more characters and matches certain prefixes (\"ASIA\" or \"AKIA\"), it falls back to a default account ID or extracts the account ID using a specific method based on a configuration setting.\\nFor other prefixes (\"LSIA\" or \"LKIA\"), it extracts the account ID using the same method if specific conditions are met.\\nIf no conditions match, it returns a default AWS Account ID.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "760a9eb2bc34",
        "ground_truth": "def get_account_id_from_access_key_id(access_key_id: str) -> str:\n    \"\"\"Return the Account ID associated the Access Key ID.\"\"\"\n     # If AWS_ACCESS_KEY_ID has a 12-digit integer value, use it as the account ID\n    if re.match(r\"\\d{12}\", access_key_id):\n        return access_key_id\n     elif len(access_key_id) >= 20:\n        if not config.PARITY_AWS_ACCESS_KEY_ID:\n            # If AWS_ACCESS_KEY_ID has production AWS credentials, ignore them\n            if access_key_id.startswith(\"ASIA\") or access_key_id.startswith(\"AKIA\"):\n                LOG.debug(\n                    \"Ignoring production AWS credentials provided to LocalStack. Falling back to default account ID.\"\n                )\n             elif access_key_id.startswith(\"LSIA\") or access_key_id.startswith(\"LKIA\"):\n                return extract_account_id_from_access_key_id(access_key_id)\n        else:\n            if access_key_id.startswith(\"ASIA\") or access_key_id.startswith(\"AKIA\"):\n                return extract_account_id_from_access_key_id(access_key_id)\n     return DEFAULT_AWS_ACCOUNT_ID",
        "import_statements": [
            "import base64",
            "import binascii",
            "import logging",
            "import re",
            "from localstack import config",
            "from localstack.constants import DEFAULT_AWS_ACCOUNT_ID"
        ],
        "reference_api": [
            "extract_account_id_from_access_key_id",
            "len",
            "access_key_id.startswith",
            "LOG.debug",
            "re.match"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "extract_account_id_from_access_key_id",
                "code": "def extract_account_id_from_access_key_id(access_key_id: str) -> str:\n    \"\"\"\n    Extract account id from access key id\n\n    Example:\n        \"ASIAQAAAAAAAGMKEM7X5\" => \"000000000000\"\n        \"AKIARZPUZDIKGB2VALC4\" => \"123456789012\"\n    :param access_key_id: Access key id. Must start with either ASIA or AKIA and has at least 20 characters\n    :return: Account ID (as string), 12 digits\n    \"\"\"\n    account_id_part = access_key_id[4:12]\n    # decode account id part\n    try:\n        account_id_part_int = int.from_bytes(base64.b32decode(account_id_part), byteorder=\"big\")\n    except binascii.Error:\n        LOG.warning(\n            \"Invalid Access Key Id format. Falling back to default id: %s\", DEFAULT_AWS_ACCOUNT_ID\n        )\n        return DEFAULT_AWS_ACCOUNT_ID\n\n    account_id = 2 * (account_id_part_int - ACCOUNT_OFFSET)\n    try:\n        if AWS_ACCESS_KEY_ALPHABET.index(access_key_id[12]) >= 16:\n            account_id += 1\n    except ValueError:\n        LOG.warning(\n            \"Char at index 12 not from base32 alphabet. Falling back to default id: %s\",\n            DEFAULT_AWS_ACCOUNT_ID,\n        )\n        return DEFAULT_AWS_ACCOUNT_ID\n    if account_id < 0 or account_id > 999999999999:\n        LOG.warning(\n            \"Extracted account id not between 000000000000 and 999999999999. Falling back to default id: %s\",\n            DEFAULT_AWS_ACCOUNT_ID,\n        )\n        return DEFAULT_AWS_ACCOUNT_ID\n    return f\"{account_id:012}\""
            },
            {
                "name": "extract_account_id_from_access_key_id",
                "code": "def extract_account_id_from_access_key_id(access_key_id: str) -> str:\n    \"\"\"\n    Extract account id from access key id\n\n    Example:\n        \"ASIAQAAAAAAAGMKEM7X5\" => \"000000000000\"\n        \"AKIARZPUZDIKGB2VALC4\" => \"123456789012\"\n    :param access_key_id: Access key id. Must start with either ASIA or AKIA and has at least 20 characters\n    :return: Account ID (as string), 12 digits\n    \"\"\"\n    account_id_part = access_key_id[4:12]\n    # decode account id part\n    try:\n        account_id_part_int = int.from_bytes(base64.b32decode(account_id_part), byteorder=\"big\")\n    except binascii.Error:\n        LOG.warning(\n            \"Invalid Access Key Id format. Falling back to default id: %s\", DEFAULT_AWS_ACCOUNT_ID\n        )\n        return DEFAULT_AWS_ACCOUNT_ID\n\n    account_id = 2 * (account_id_part_int - ACCOUNT_OFFSET)\n    try:\n        if AWS_ACCESS_KEY_ALPHABET.index(access_key_id[12]) >= 16:\n            account_id += 1\n    except ValueError:\n        LOG.warning(\n            \"Char at index 12 not from base32 alphabet. Falling back to default id: %s\",\n            DEFAULT_AWS_ACCOUNT_ID,\n        )\n        return DEFAULT_AWS_ACCOUNT_ID\n    if account_id < 0 or account_id > 999999999999:\n        LOG.warning(\n            \"Extracted account id not between 000000000000 and 999999999999. Falling back to default id: %s\",\n            DEFAULT_AWS_ACCOUNT_ID,\n        )\n        return DEFAULT_AWS_ACCOUNT_ID\n    return f\"{account_id:012}\""
            }
        ],
        "third_party": [
            "access_key_id.startswith",
            "access_key_id.startswith",
            "LOG.debug",
            "access_key_id.startswith",
            "access_key_id.startswith",
            "access_key_id.startswith",
            "access_key_id.startswith"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def create_http_request(aws_request: awsrequest.AWSPreparedRequest) -> Request",
        "start_line": "28",
        "end_line": "56",
        "file_path": "localstack-core/localstack/aws/client.py",
        "docstring": "The create_http_request function converts an AWSPreparedRequest object into a Request object suitable for HTTP communication.\\nIt parses the URL of the AWS request to extract the host and port information, handling cases where the port may be absent.\\nThe function then prepares the headers by converting them to string format and constructs the Request object with the method, path, query string, headers, body, and server information derived from the AWSPreparedRequest.\\nThis function is used to bridge AWS request formats to a more generic HTTP request format.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c35f464498df",
        "ground_truth": "def create_http_request(aws_request: awsrequest.AWSPreparedRequest) -> Request:\n    \"\"\"\n    Create an ASF HTTP Request from a botocore AWSPreparedRequest.\n     :param aws_request: the botocore prepared request\n    :return: a new Request\n    \"\"\"\n    split_url = urlsplit(aws_request.url)\n    host = split_url.netloc.split(\":\")\n    if len(host) == 1:\n        server = (to_str(host[0]), None)\n    elif len(host) == 2:\n        server = (to_str(host[0]), int(host[1]))\n    else:\n        raise ValueError\n     # prepare the RequestContext\n    headers = Headers()\n    for k, v in aws_request.headers.items():\n        headers[k] = to_str(v)\n     return Request(\n        method=aws_request.method,\n        path=split_url.path,\n        query_string=split_url.query,\n        headers=headers,\n        body=aws_request.body,\n        server=server,\n    )",
        "import_statements": [
            "import io",
            "import logging",
            "from datetime import datetime, timezone",
            "from typing import Dict, Iterable, Optional",
            "from urllib.parse import urlsplit",
            "from botocore import awsrequest",
            "from botocore.endpoint import Endpoint",
            "from botocore.model import OperationModel",
            "from botocore.parsers import ResponseParser, ResponseParserFactory",
            "from werkzeug.datastructures import Headers",
            "from localstack import config",
            "from localstack.http import Request, Response",
            "from localstack.runtime import hooks",
            "from localstack.utils.patch import Patch, patch",
            "from localstack.utils.strings import to_str"
        ],
        "reference_api": [
            "urlsplit",
            "to_str",
            "int",
            "len",
            "split",
            "items",
            "Request",
            "Headers"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "urlsplit",
            "split",
            "to_str",
            "to_str",
            "Headers",
            "items",
            "to_str",
            "Request"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def attribute_name_to_service_name(attribute_name)",
        "start_line": "92",
        "end_line": "104",
        "file_path": "localstack-core/localstack/aws/connect.py",
        "docstring": "The attribute_name_to_service_name function converts an attribute name to a service name by removing any trailing underscores and replacing all underscores with hyphens.\\nFor example, \"lambda_\" becomes \"lambda\" and \"cognito_idp\" becomes \"cognito-idp\".",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bb2539405acd",
        "ground_truth": "def attribute_name_to_service_name(attribute_name):\n    \"\"\"\n    Converts a python-compatible attribute name to the boto service name\n    :param attribute_name: Python compatible attribute name using the following replacements:\n                            a) Add an underscore suffix `_` to any reserved Python keyword (PEP-8).\n                            b) Replace any dash `-` with an underscore `_`\n    :return:\n    \"\"\"\n    if attribute_name.endswith(\"_\"):\n        # lambda_ -> lambda\n        attribute_name = attribute_name[:-1]\n    # replace all _ with -: cognito_idp -> cognito-idp\n    return attribute_name.replace(\"_\", \"-\")",
        "import_statements": [
            "import json",
            "import logging",
            "import re",
            "import threading",
            "from abc import ABC, abstractmethod",
            "from functools import lru_cache, partial",
            "from typing import Any, Callable, Generic, Optional, TypedDict, TypeVar",
            "from boto3.session import Session",
            "from botocore.client import BaseClient",
            "from botocore.config import Config",
            "from botocore.waiter import Waiter",
            "from localstack import config as localstack_config",
            "from localstack.aws.spec import LOCALSTACK_BUILTIN_DATA_PATH",
            "from localstack.constants import (\n    AWS_REGION_US_EAST_1,\n    INTERNAL_AWS_ACCESS_KEY_ID,\n    INTERNAL_AWS_SECRET_ACCESS_KEY,\n    MAX_POOL_CONNECTIONS,\n)",
            "from localstack.utils.aws.aws_stack import get_s3_hostname",
            "from localstack.utils.aws.client_types import ServicePrincipal, TypedServiceClientFactory",
            "from localstack.utils.patch import patch",
            "from localstack.utils.strings import short_uid"
        ],
        "reference_api": [
            "attribute_name.replace",
            "attribute_name.endswith"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "attribute_name.endswith",
            "attribute_name.replace"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _get_client(\n        self,\n        service_name: str,\n        region_name: str,\n        use_ssl: bool,\n        verify: Optional[bool],\n        endpoint_url: Optional[str],\n        aws_access_key_id: Optional[str],\n        aws_secret_access_key: Optional[str],\n        aws_session_token: Optional[str],\n        config: Config,\n    ) -> BaseClient",
        "start_line": "367",
        "end_line": "418",
        "file_path": "localstack-core/localstack/aws/connect.py",
        "docstring": "The _get_client function creates and returns a client for an AWS service using the provided parameters such as service name, region, SSL usage, verification, endpoint URL, and AWS credentials.\\nIt ensures thread-safety using a lock while creating the client.\\nIf retries are disabled in the local configuration, it uses a custom config with zero retry attempts; otherwise, it uses the default configuration.\\nThe function merges the provided configuration with the default one before creating the client.\\nFinally, it applies a post-creation hook to the client and returns it.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5cc29c220dd4",
        "ground_truth": "def _get_client(\n    self,\n    service_name: str,\n    region_name: str,\n    use_ssl: bool,\n    verify: Optional[bool],\n    endpoint_url: Optional[str],\n    aws_access_key_id: Optional[str],\n    aws_secret_access_key: Optional[str],\n    aws_session_token: Optional[str],\n    config: Config,\n) -> BaseClient:\n    \"\"\"\n    Returns a boto3 client with the given configuration, and the hooks added by `_get_client_post_hook`.\n    This is a cached call, so modifications to the used client will affect others.\n    Please use another instance of the factory, should you want to modify clients.\n    Client creation is behind a lock as it is not generally thread safe.\n    :param service_name: Service to build the client for, eg. `s3`\n    :param region_name: Name of the AWS region to be associated with the client\n        If set to None, loads from botocore session.\n    :param aws_access_key_id: Access key to use for the client.\n        If set to None, loads from botocore session.\n    :param aws_secret_access_key: Secret key to use for the client.\n        If set to None, loads from botocore session.\n    :param aws_session_token: Session token to use for the client.\n        Not being used if not set.\n    :param endpoint_url: Full endpoint URL to be used by the client.\n        Defaults to appropriate LocalStack endpoint.\n    :param config: Boto config for advanced use.\n    :return: Boto3 client.\n    \"\"\"\n    with self._create_client_lock:\n        default_config = (\n            Config(retries={\"max_attempts\": 0})\n            if localstack_config.DISABLE_BOTO_RETRIES\n            else Config()\n        )\n        client = self._session.client(\n            service_name=service_name,\n            region_name=region_name,\n            use_ssl=use_ssl,\n            verify=verify,\n            endpoint_url=endpoint_url,\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n            aws_session_token=aws_session_token,\n            config=config.merge(default_config),\n        )\n    return self._get_client_post_hook(client)",
        "import_statements": [
            "import json",
            "import logging",
            "import re",
            "import threading",
            "from abc import ABC, abstractmethod",
            "from functools import lru_cache, partial",
            "from typing import Any, Callable, Generic, Optional, TypedDict, TypeVar",
            "from boto3.session import Session",
            "from botocore.client import BaseClient",
            "from botocore.config import Config",
            "from botocore.waiter import Waiter",
            "from localstack import config as localstack_config",
            "from localstack.aws.spec import LOCALSTACK_BUILTIN_DATA_PATH",
            "from localstack.constants import (\n    AWS_REGION_US_EAST_1,\n    INTERNAL_AWS_ACCESS_KEY_ID,\n    INTERNAL_AWS_SECRET_ACCESS_KEY,\n    MAX_POOL_CONNECTIONS,\n)",
            "from localstack.utils.aws.aws_stack import get_s3_hostname",
            "from localstack.utils.aws.client_types import ServicePrincipal, TypedServiceClientFactory",
            "from localstack.utils.patch import patch",
            "from localstack.utils.strings import short_uid"
        ],
        "reference_api": [
            "Config",
            "client",
            "config.merge",
            "self._get_client_post_hook"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_client_post_hook",
                "code": "def _get_client_post_hook(self, client: BaseClient) -> BaseClient:\n        \"\"\"\n        This is called after the client is created by Boto.\n\n        Any modifications to the client can be implemented here in subclasses\n        without affecting the caching mechanism.\n        \"\"\"\n        return client"
            }
        ],
        "third_party": [
            "Config",
            "Config",
            "client",
            "config.merge"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def _get_client_post_hook(self, client: BaseClient) -> BaseClient",
        "start_line": "440",
        "end_line": "458",
        "file_path": "localstack-core/localstack/aws/connect.py",
        "docstring": "The _get_client_post_hook function registers event handlers to enable internal data object transfer for internal clients.\\nIt registers handlers for \"provide-client-params.*.*\" and \"before-call.*.*\" events to manage request parameters and inject DTO headers, respectively.\\nIf the IN_MEMORY_CLIENT configuration is enabled, it modifies the client to call the gateway directly using the GatewayShortCircuit class and the current runtime's gateway component.\\nFinally, it returns the modified client.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8f0faec089a8",
        "ground_truth": "def _get_client_post_hook(self, client: BaseClient) -> BaseClient:\n    \"\"\"\n    Register handlers that enable internal data object transfer mechanism\n    for internal clients.\n    \"\"\"\n    client.meta.events.register(\n        \"provide-client-params.*.*\", handler=_handler_create_request_parameters\n    )\n    client.meta.events.register(\"before-call.*.*\", handler=_handler_inject_dto_header)\n    if localstack_config.IN_MEMORY_CLIENT:\n        # this make the client call the gateway directly\n        from localstack.aws.client import GatewayShortCircuit\n        from localstack.runtime import get_current_runtime\n        GatewayShortCircuit.modify_client(client, get_current_runtime().components.gateway)\n    return client",
        "import_statements": [
            "import json",
            "import logging",
            "import re",
            "import threading",
            "from abc import ABC, abstractmethod",
            "from functools import lru_cache, partial",
            "from typing import Any, Callable, Generic, Optional, TypedDict, TypeVar",
            "from boto3.session import Session",
            "from botocore.client import BaseClient",
            "from botocore.config import Config",
            "from botocore.waiter import Waiter",
            "from localstack import config as localstack_config",
            "from localstack.aws.spec import LOCALSTACK_BUILTIN_DATA_PATH",
            "from localstack.constants import (\n    AWS_REGION_US_EAST_1,\n    INTERNAL_AWS_ACCESS_KEY_ID,\n    INTERNAL_AWS_SECRET_ACCESS_KEY,\n    MAX_POOL_CONNECTIONS,\n)",
            "from localstack.utils.aws.aws_stack import get_s3_hostname",
            "from localstack.utils.aws.client_types import ServicePrincipal, TypedServiceClientFactory",
            "from localstack.utils.patch import patch",
            "from localstack.utils.strings import short_uid"
        ],
        "reference_api": [
            "register",
            "get_current_runtime",
            "GatewayShortCircuit.modify_client"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "register",
            "register",
            "GatewayShortCircuit.modify_client",
            "get_current_runtime"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def new_request_context(self, original: RequestContext, service_request: ServiceRequest)",
        "start_line": "86",
        "end_line": "99",
        "file_path": "localstack-core/localstack/aws/forwarder.py",
        "docstring": "The new_request_context function creates a new AWS request context based on an existing RequestContext and a ServiceRequest.\\nIt uses the original request's service name, operation name, parameters, and region to create the new context.\\nIt then updates the new context with non-payload specific headers from the original request, excluding \"Content-Type\" and \"Content-Length\" headers.\\nThe updated context is returned at the end of the function.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "63e00bda942b",
        "ground_truth": "def new_request_context(self, original: RequestContext, service_request: ServiceRequest):\n    context = create_aws_request_context(\n        service_name=original.service.service_name,\n        action=original.operation.name,\n        parameters=service_request,\n        region=original.region,\n    )\n    # update the newly created context with non-payload specific request headers (the payload can differ from\n    # the original request, f.e. it could be JSON encoded now while the initial request was CBOR encoded)\n    headers = Headers(original.request.headers)\n    headers.pop(\"Content-Type\", None)\n    headers.pop(\"Content-Length\", None)\n    context.request.headers.update(headers)\n    return context",
        "import_statements": [
            "from typing import Any, Callable, Mapping, Optional, Union",
            "from botocore.awsrequest import AWSPreparedRequest, prepare_request_dict",
            "from botocore.config import Config as BotoConfig",
            "from werkzeug.datastructures import Headers",
            "from localstack.aws.api.core import (\n    RequestContext,\n    ServiceRequest,\n    ServiceRequestHandler,\n    ServiceResponse,\n)",
            "from localstack.aws.client import create_http_request, parse_response, raise_service_exception",
            "from localstack.aws.connect import connect_to",
            "from localstack.aws.skeleton import DispatchTable, create_dispatch_table",
            "from localstack.aws.spec import load_service",
            "from localstack.constants import AWS_REGION_US_EAST_1",
            "from localstack.http import Response",
            "from localstack.http.proxy import Proxy"
        ],
        "reference_api": [
            "create_aws_request_context",
            "update",
            "Headers",
            "headers.pop"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "create_aws_request_context",
                "code": "def create_aws_request_context(\n    service_name: str,\n    action: str,\n    parameters: Mapping[str, Any] = None,\n    region: str = None,\n    endpoint_url: Optional[str] = None,\n) -> RequestContext:\n    \"\"\"\n    This is a stripped-down version of what the botocore client does to perform an HTTP request from a client call. A\n    client call looks something like this: boto3.client(\"sqs\").create_queue(QueueName=\"myqueue\"), which will be\n    serialized into an HTTP request. This method does the same, without performing the actual request, and with a\n    more low-level interface. An equivalent call would be\n\n         create_aws_request_context(\"sqs\", \"CreateQueue\", {\"QueueName\": \"myqueue\"})\n\n    :param service_name: the AWS service\n    :param action: the action to invoke\n    :param parameters: the invocation parameters\n    :param region: the region name (default is us-east-1)\n    :param endpoint_url: the endpoint to call (defaults to localstack)\n    :return: a RequestContext object that describes this request\n    \"\"\"\n    if parameters is None:\n        parameters = {}\n    if region is None:\n        region = AWS_REGION_US_EAST_1\n\n    service = load_service(service_name)\n    operation = service.operation_model(action)\n\n    # we re-use botocore internals here to serialize the HTTP request,\n    # but deactivate validation (validation errors should be handled by the backend)\n    # and don't send it yet\n    client = connect_to.get_client(\n        service_name,\n        endpoint_url=endpoint_url,\n        region_name=region,\n        config=_non_validating_boto_config,\n    )\n    request_context = {\n        \"client_region\": region,\n        \"has_streaming_input\": operation.has_streaming_input,\n        \"auth_type\": operation.auth_type,\n    }\n\n    # The endpoint URL is mandatory here, set a dummy if not given (doesn't _need_ to be localstack specific)\n    if not endpoint_url:\n        endpoint_url = \"http://localhost.localstack.cloud\"\n    # pre-process the request args (some params are modified using botocore event handlers)\n    parameters = client._emit_api_params(parameters, operation, request_context)\n    request_dict = client._convert_to_request_dict(\n        parameters, operation, endpoint_url, context=request_context\n    )\n\n    if auth_path := request_dict.get(\"auth_path\"):\n        # botocore >= 1.28 might modify the url path of the request dict (specifically for S3).\n        # It will then set the original url path as \"auth_path\". If the auth_path is set, we reset the url_path.\n        # Since botocore 1.31.2, botocore will strip the query from the `authPart`\n        # We need to add it back from `requestUri` field\n        # Afterwards the request needs to be prepared again.\n        path, sep, query = request_dict[\"url_path\"].partition(\"?\")\n        request_dict[\"url_path\"] = f\"{auth_path}{sep}{query}\"\n        prepare_request_dict(\n            request_dict,\n            endpoint_url=endpoint_url,\n            user_agent=client._client_config.user_agent,\n            context=request_context,\n        )\n\n    aws_request: AWSPreparedRequest = client._endpoint.create_request(request_dict, operation)\n    context = RequestContext()\n    context.service = service\n    context.operation = operation\n    context.region = region\n    context.request = create_http_request(aws_request)\n    context.service_request = parameters\n\n    return context"
            }
        ],
        "third_party": [
            "Headers",
            "headers.pop",
            "headers.pop",
            "update"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def dispatch_to_backend(\n    context: RequestContext,\n    http_request_dispatcher: Callable[[RequestContext], Response],\n    include_response_metadata=False,\n) -> ServiceResponse",
        "start_line": "171",
        "end_line": "188",
        "file_path": "localstack-core/localstack/aws/forwarder.py",
        "docstring": "The dispatch_to_backend function handles the dispatching of an HTTP request to the backend service.\\nIt takes in a RequestContext object, a dispatcher callable, and an optional flag for including response metadata.\\nThe function calls the dispatcher with the provided context to obtain an HTTP response.\\nThis response is then parsed based on the context's operation and the include_response_metadata flag.\\nIf the HTTP response indicates an error, a service exception is raised using the parsed response.\\nFinally, the parsed response is returned as the ServiceResponse.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2942fc816fbf",
        "ground_truth": "def dispatch_to_backend(\n    context: RequestContext,\n    http_request_dispatcher: Callable[[RequestContext], Response],\n    include_response_metadata=False,\n) -> ServiceResponse:\n    \"\"\"\n    Dispatch the given request to a backend by using the `request_forwarder` function to\n    fetch an HTTP response, converting it to a ServiceResponse.\n    :param context: the request context\n    :param http_request_dispatcher: dispatcher that performs the request and returns an HTTP response\n    :param include_response_metadata: whether to include boto3 response metadata in the response\n    :return: parsed service response\n    :raises ServiceException: if the dispatcher returned an error response\n    \"\"\"\n    http_response = http_request_dispatcher(context)\n    parsed_response = parse_response(context.operation, http_response, include_response_metadata)\n    raise_service_exception(http_response, parsed_response)\n    return parsed_response",
        "import_statements": [
            "from typing import Any, Callable, Mapping, Optional, Union",
            "from botocore.awsrequest import AWSPreparedRequest, prepare_request_dict",
            "from botocore.config import Config as BotoConfig",
            "from werkzeug.datastructures import Headers",
            "from localstack.aws.api.core import (\n    RequestContext,\n    ServiceRequest,\n    ServiceRequestHandler,\n    ServiceResponse,\n)",
            "from localstack.aws.client import create_http_request, parse_response, raise_service_exception",
            "from localstack.aws.connect import connect_to",
            "from localstack.aws.skeleton import DispatchTable, create_dispatch_table",
            "from localstack.aws.spec import load_service",
            "from localstack.constants import AWS_REGION_US_EAST_1",
            "from localstack.http import Response",
            "from localstack.http.proxy import Proxy"
        ],
        "reference_api": [
            "parse_response",
            "raise_service_exception",
            "http_request_dispatcher"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "http_request_dispatcher",
            "parse_response",
            "raise_service_exception"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def shape_graph(root: Shape) -> ShapeGraph",
        "start_line": "106",
        "end_line": "128",
        "file_path": "localstack-core/localstack/aws/mocking.py",
        "docstring": "The shape_graph function constructs a directed graph (DiGraph) from a given root Shape using the NetworkX library.\\nIt first initializes the graph and sets its root to the provided Shape.\\nThe populate_graph function is called to populate the graph with nodes and edges starting from the root.\\nThe function then searches for cycles within the graph, adding any found cycles to a list and recording the shapes involved in these cycles.\\nThese cycles and cycle shapes are stored as attributes of the graph.\\nFinally, the function returns the graph cast as a ShapeGraph.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "78f94a505579",
        "ground_truth": "def shape_graph(root: Shape) -> ShapeGraph:\n    graph = networkx.DiGraph()\n    graph.root = root\n    populate_graph(graph, root)\n     cycles = list()\n    shapes = set()\n    for node in graph.nodes:\n        try:\n            cycle = networkx.find_cycle(graph, source=node)\n            for k, v in cycle:\n                shapes.add(k)\n                shapes.add(v)\n             if cycle not in cycles:\n                cycles.append(cycle)\n        except networkx.NetworkXNoCycle:\n            pass\n     graph.cycles = cycles\n    graph.cycle_shapes = list(shapes)\n     return cast(ShapeGraph, graph)",
        "import_statements": [
            "import logging",
            "import math",
            "import random",
            "import re",
            "from datetime import date, datetime",
            "from functools import lru_cache, singledispatch",
            "from typing import Dict, List, Optional, Set, Tuple, Union, cast",
            "import botocore",
            "import networkx",
            "import rstr",
            "from botocore.model import ListShape, MapShape, OperationModel, Shape, StringShape, StructureShape",
            "from localstack.aws.api import RequestContext, ServiceRequest, ServiceResponse",
            "from localstack.aws.skeleton import DispatchTable, ServiceRequestDispatcher, Skeleton",
            "from localstack.aws.spec import load_service",
            "from localstack.utils.sync import retry"
        ],
        "reference_api": [
            "populate_graph",
            "list",
            "shapes.add",
            "cycles.append",
            "networkx.DiGraph",
            "set",
            "cast",
            "networkx.find_cycle"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "populate_graph",
                "code": "def populate_graph(graph: networkx.DiGraph, root: Shape):\n    stack: List[Shape] = [root]\n    visited: Set[str] = set()\n\n    while stack:\n        cur = stack.pop()\n        if cur is None:\n            continue\n\n        if cur.name in visited:\n            continue\n\n        visited.add(cur.name)\n        graph.add_node(cur.name, shape=cur)\n\n        if isinstance(cur, ListShape):\n            graph.add_edge(cur.name, cur.member.name)\n            stack.append(cur.member)\n        elif isinstance(cur, StructureShape):\n            for member in cur.members.values():\n                stack.append(member)\n                graph.add_edge(cur.name, member.name)\n        elif isinstance(cur, MapShape):\n            stack.append(cur.key)\n            stack.append(cur.value)\n            graph.add_edge(cur.name, cur.key.name)\n            graph.add_edge(cur.name, cur.value.name)\n\n        else:  # leaf types (int, string, bool, ...)\n            pass"
            }
        ],
        "third_party": [
            "networkx.DiGraph",
            "networkx.find_cycle",
            "shapes.add",
            "shapes.add",
            "cycles.append",
            "cast"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def request_operation(self) -> Optional[OperationModel]",
        "start_line": "78",
        "end_line": "89",
        "file_path": "localstack-core/localstack/aws/scaffold.py",
        "docstring": "The request_operation function iterates through all the operation names of a service to find an operation whose input shape name matches the name of the current shape, after converting both to valid Python names.\\nIf a matching operation is found, it returns the corresponding OperationModel object.\\nIf no match is found, the function returns None.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fc9951f7cd76",
        "ground_truth": "def request_operation(self) -> Optional[OperationModel]:\n    for operation_name in self.service.operation_names:\n        operation = self.service.operation_model(operation_name)\n        if operation.input_shape is None:\n            continue\n        if to_valid_python_name(self.shape.name) == to_valid_python_name(\n            operation.input_shape.name\n        ):\n            return operation\n    return None",
        "import_statements": [
            "import io",
            "import keyword",
            "import re",
            "from functools import cached_property",
            "from multiprocessing import Pool",
            "from pathlib import Path",
            "from typing import Dict, List, Optional, Set",
            "import click",
            "from botocore import xform_name",
            "from botocore.exceptions import UnknownServiceError",
            "from botocore.model import (\n    ListShape,\n    MapShape,\n    OperationModel,\n    ServiceModel,\n    Shape,\n    StringShape,\n    StructureShape,\n)",
            "from typing_extensions import OrderedDict",
            "from localstack.aws.spec import load_service",
            "from localstack.utils.common import camel_to_snake_case, snake_to_camel_case"
        ],
        "reference_api": [
            "operation_model",
            "to_valid_python_name"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "to_valid_python_name",
                "code": "def to_valid_python_name(spec_name: str) -> str:\n    sanitized = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", spec_name)\n\n    if sanitized[0].isnumeric():\n        sanitized = \"i_\" + sanitized\n\n    if is_keyword(sanitized):\n        sanitized += \"_\"\n\n    if sanitized.startswith(\"__\"):\n        sanitized = sanitized[1:]\n\n    return sanitized"
            },
            {
                "name": "to_valid_python_name",
                "code": "def to_valid_python_name(spec_name: str) -> str:\n    sanitized = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", spec_name)\n\n    if sanitized[0].isnumeric():\n        sanitized = \"i_\" + sanitized\n\n    if is_keyword(sanitized):\n        sanitized += \"_\"\n\n    if sanitized.startswith(\"__\"):\n        sanitized = sanitized[1:]\n\n    return sanitized"
            }
        ],
        "third_party": [
            "operation_model"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def on_service_exception(\n        self, serializer: ResponseSerializer, context: RequestContext, exception: ServiceException\n    ) -> Response",
        "start_line": "181",
        "end_line": "196",
        "file_path": "localstack-core/localstack/aws/skeleton.py",
        "docstring": "The on_service_exception function handles exceptions that occur during a service request.\\nIt takes a ResponseSerializer, RequestContext, and ServiceException as parameters.\\nThe function assigns the exception to the context's service_exception attribute.\\nIt then uses the serializer to convert the exception into a response, including details about the operation, request headers, and request ID.\\nFinally, it returns the serialized response.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3486509035be",
        "ground_truth": "def on_service_exception(\n    self, serializer: ResponseSerializer, context: RequestContext, exception: ServiceException\n) -> Response:\n    \"\"\"\n    Called by invoke if the handler of the operation raised a ServiceException.\n    :param serializer: serializer which should be used to serialize the exception\n    :param context: the request context\n    :param exception: the exception that was raised\n    :return: a Response object\n    \"\"\"\n    context.service_exception = exception\n    return serializer.serialize_error_to_response(\n        exception, context.operation, context.request.headers, context.request_id\n    )",
        "import_statements": [
            "import inspect",
            "import logging",
            "from typing import Any, Callable, Dict, NamedTuple, Optional, Union",
            "from botocore import xform_name",
            "from botocore.model import ServiceModel",
            "from localstack.aws.api import (\n    CommonServiceException,\n    RequestContext,\n    ServiceException,\n)",
            "from localstack.aws.api.core import ServiceRequest, ServiceRequestHandler, ServiceResponse",
            "from localstack.aws.protocol.parser import create_parser",
            "from localstack.aws.protocol.serializer import ResponseSerializer, create_serializer",
            "from localstack.aws.spec import load_service",
            "from localstack.http import Response",
            "from localstack.utils import analytics",
            "from localstack.utils.coverage_docs import get_coverage_link_for_service"
        ],
        "reference_api": [
            "serializer.serialize_error_to_response"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "serializer.serialize_error_to_response"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def load_service(\n    service: ServiceName, version: Optional[str] = None, protocol: Optional[ProtocolName] = None\n) -> ServiceModel",
        "start_line": "95",
        "end_line": "130",
        "file_path": "localstack-core/localstack/aws/spec.py",
        "docstring": "The load_service function loads a service model for a specified cloud service.\\nIt takes the service name, optional version, and optional protocol as arguments.\\nThe service description is loaded using the loader's load_service_model method.\\nIf a protocol is specified and it does not match the protocol defined in the service description metadata, the function checks if the service name ends with the protocol name.\\nIf it does, it raises an UnknownServiceProtocolError.\\nIf not, it recursively attempts to load the service with the protocol name appended.\\nIf this also fails, it raises an UnknownServiceProtocolError.\\nFor specific services like \"sqs-query\", it maps them to their base service name (\"sqs\").\\nFinally, it returns a ServiceModel instance initialized with the service description and the service name.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7528024a87b6",
        "ground_truth": "def load_service(\n    service: ServiceName, version: Optional[str] = None, protocol: Optional[ProtocolName] = None\n) -> ServiceModel:\n    \"\"\"\n    Loads a service\n    :param service: to load, f.e. \"sqs\". For custom, internalized, service protocol specs (f.e. sqs-query) it's also\n                    possible to directly define the protocol in the service name (f.e. use sqs-query)\n    :param version: of the service to load, f.e. \"2012-11-05\", by default the latest version will be used\n    :param protocol: specific protocol to load for the specific service, f.e. \"json\" for the \"sqs\" service\n                     if the service cannot be found\n    :return: Loaded service model of the service\n    :raises: UnknownServiceError if the service cannot be found\n    :raises: UnknownServiceProtocolError if the specific protocol of the service cannot be found\n    \"\"\"\n    service_description = loader.load_service_model(service, \"service-2\", version)\n     # check if the protocol is defined, and if so, if the loaded service defines this protocol\n    if protocol is not None and protocol != service_description.get(\"metadata\", {}).get(\"protocol\"):\n        # if the protocol is defined, but not the one of the currently loaded service,\n        # check if we already loaded the custom spec based on the naming convention (<service>-<protocol>),\n        # f.e. \"sqs-query\"\n        if service.endswith(f\"-{protocol}\"):\n            # if so, we raise an exception\n            raise UnknownServiceProtocolError(service_name=service, protocol=protocol)\n        # otherwise we try to load it (recursively)\n        try:\n            return load_service(f\"{service}-{protocol}\", version, protocol=protocol)\n        except UnknownServiceError:\n            # raise an unknown protocol error in case the service also can't be loaded with the naming convention\n            raise UnknownServiceProtocolError(service_name=service, protocol=protocol)\n     # remove potential protocol names from the service name\n    # FIXME add more protocols here if we have to internalize more than just sqs-query\n    # TODO this should not contain specific internalized serivce names\n    service = {\"sqs-query\": \"sqs\"}.get(service, service)\n    return ServiceModel(service_description, service)",
        "import_statements": [
            "import dataclasses",
            "import json",
            "import logging",
            "import os",
            "from collections import defaultdict",
            "from functools import cached_property, lru_cache",
            "from typing import Dict, Generator, List, Literal, NamedTuple, Optional, Tuple",
            "import jsonpatch",
            "from botocore.exceptions import UnknownServiceError",
            "from botocore.loaders import Loader, instance_cache",
            "from botocore.model import OperationModel, ServiceModel"
        ],
        "reference_api": [
            "service_description.get",
            "service.endswith",
            "get",
            "UnknownServiceProtocolError",
            "load_service",
            "ServiceModel",
            "loader.load_service_model"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "load_service",
                "code": "def load_service(\n    service: ServiceName, version: Optional[str] = None, protocol: Optional[ProtocolName] = None\n) -> ServiceModel:\n    \"\"\"\n    Loads a service\n    :param service: to load, f.e. \"sqs\". For custom, internalized, service protocol specs (f.e. sqs-query) it's also\n                    possible to directly define the protocol in the service name (f.e. use sqs-query)\n    :param version: of the service to load, f.e. \"2012-11-05\", by default the latest version will be used\n    :param protocol: specific protocol to load for the specific service, f.e. \"json\" for the \"sqs\" service\n                     if the service cannot be found\n    :return: Loaded service model of the service\n    :raises: UnknownServiceError if the service cannot be found\n    :raises: UnknownServiceProtocolError if the specific protocol of the service cannot be found\n    \"\"\"\n    service_description = loader.load_service_model(service, \"service-2\", version)\n\n    # check if the protocol is defined, and if so, if the loaded service defines this protocol\n    if protocol is not None and protocol != service_description.get(\"metadata\", {}).get(\"protocol\"):\n        # if the protocol is defined, but not the one of the currently loaded service,\n        # check if we already loaded the custom spec based on the naming convention (<service>-<protocol>),\n        # f.e. \"sqs-query\"\n        if service.endswith(f\"-{protocol}\"):\n            # if so, we raise an exception\n            raise UnknownServiceProtocolError(service_name=service, protocol=protocol)\n        # otherwise we try to load it (recursively)\n        try:\n            return load_service(f\"{service}-{protocol}\", version, protocol=protocol)\n        except UnknownServiceError:\n            # raise an unknown protocol error in case the service also can't be loaded with the naming convention\n            raise UnknownServiceProtocolError(service_name=service, protocol=protocol)\n\n    # remove potential protocol names from the service name\n    # FIXME add more protocols here if we have to internalize more than just sqs-query\n    # TODO this should not contain specific internalized serivce names\n    service = {\"sqs-query\": \"sqs\"}.get(service, service)\n    return ServiceModel(service_description, service)"
            }
        ],
        "third_party": [
            "loader.load_service_model",
            "get",
            "service_description.get",
            "service.endswith",
            "UnknownServiceProtocolError",
            "UnknownServiceProtocolError",
            "get",
            "ServiceModel"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def target_prefix_index(self) -> Dict[str, List[ServiceModelIdentifier]]",
        "start_line": "168",
        "end_line": "177",
        "file_path": "localstack-core/localstack/aws/spec.py",
        "docstring": "The target_prefix_index function constructs and returns a dictionary where each key is a targetPrefix extracted from service model metadata.\\nEach corresponding value is a list of ServiceModelIdentifier objects, which contain the service name and protocol for each service model.\\nThe function iterates over the service models in the _services attribute, checking the metadata for a targetPrefix, and populates the result dictionary accordingly.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "97626f19c673",
        "ground_truth": "def target_prefix_index(self) -> Dict[str, List[ServiceModelIdentifier]]:\n    result = defaultdict(list)\n    for service_models in self._services.values():\n        for service_model in service_models:\n            target_prefix = service_model.metadata.get(\"targetPrefix\")\n            if target_prefix:\n                result[target_prefix].append(\n                    ServiceModelIdentifier(service_model.service_name, service_model.protocol)\n                )\n    return dict(result)",
        "import_statements": [
            "import dataclasses",
            "import json",
            "import logging",
            "import os",
            "from collections import defaultdict",
            "from functools import cached_property, lru_cache",
            "from typing import Dict, Generator, List, Literal, NamedTuple, Optional, Tuple",
            "import jsonpatch",
            "from botocore.exceptions import UnknownServiceError",
            "from botocore.loaders import Loader, instance_cache",
            "from botocore.model import OperationModel, ServiceModel"
        ],
        "reference_api": [
            "defaultdict",
            "ServiceModelIdentifier",
            "values",
            "dict",
            "append",
            "get"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "defaultdict",
            "values",
            "get",
            "append",
            "ServiceModelIdentifier"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def signing_name_index(self) -> Dict[str, List[ServiceModelIdentifier]]",
        "start_line": "180",
        "end_line": "187",
        "file_path": "localstack-core/localstack/aws/spec.py",
        "docstring": "The signing_name_index function creates a dictionary where the keys are signing names of service models, and the values are lists of ServiceModelIdentifier objects.\\nIt iterates through the service models stored in the _services attribute, grouping them by their signing names.\\nFor each service model, it adds a ServiceModelIdentifier containing the service name and protocol to the corresponding list in the dictionary.\\nThe function returns this dictionary, converting it from a defaultdict to a standard dictionary.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c03161f8d649",
        "ground_truth": "def signing_name_index(self) -> Dict[str, List[ServiceModelIdentifier]]:\n    result = defaultdict(list)\n    for service_models in self._services.values():\n        for service_model in service_models:\n            result[service_model.signing_name].append(\n                ServiceModelIdentifier(service_model.service_name, service_model.protocol)\n            )\n    return dict(result)",
        "import_statements": [
            "import dataclasses",
            "import json",
            "import logging",
            "import os",
            "from collections import defaultdict",
            "from functools import cached_property, lru_cache",
            "from typing import Dict, Generator, List, Literal, NamedTuple, Optional, Tuple",
            "import jsonpatch",
            "from botocore.exceptions import UnknownServiceError",
            "from botocore.loaders import Loader, instance_cache",
            "from botocore.model import OperationModel, ServiceModel"
        ],
        "reference_api": [
            "defaultdict",
            "ServiceModelIdentifier",
            "values",
            "dict",
            "append"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "defaultdict",
            "values",
            "append",
            "ServiceModelIdentifier"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def load_service_index_cache(file: str) -> ServiceCatalogIndex",
        "start_line": "274",
        "end_line": "284",
        "file_path": "localstack-core/localstack/aws/spec.py",
        "docstring": "The load_service_index_cache function loads a ServiceCatalogIndex object from a specified file.\\nIt uses the pickle module to deserialize the object from a binary file.\\nThe function opens the file in read-binary mode, reads its contents using pickle.load, and returns the deserialized ServiceCatalogIndex object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d10c635a5880",
        "ground_truth": "def load_service_index_cache(file: str) -> ServiceCatalogIndex:\n    \"\"\"\n    Loads from the given file the pickled ServiceCatalogIndex.\n     :param file: the file to load from\n    :return: the loaded ServiceCatalogIndex\n    \"\"\"\n    import pickle\n     with open(file, \"rb\") as fd:\n        return pickle.load(fd)",
        "import_statements": [
            "import dataclasses",
            "import json",
            "import logging",
            "import os",
            "from collections import defaultdict",
            "from functools import cached_property, lru_cache",
            "from typing import Dict, Generator, List, Literal, NamedTuple, Optional, Tuple",
            "import jsonpatch",
            "from botocore.exceptions import UnknownServiceError",
            "from botocore.loaders import Loader, instance_cache",
            "from botocore.model import OperationModel, ServiceModel"
        ],
        "reference_api": [
            "pickle.load",
            "open"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "localstack/localstack",
        "function_declaration": "def save_service_index_cache(index: LazyServiceCatalogIndex, file_path: str) -> ServiceCatalogIndex",
        "start_line": "287",
        "end_line": "307",
        "file_path": "localstack-core/localstack/aws/spec.py",
        "docstring": "The save_service_index_cache function serializes a LazyServiceCatalogIndex object to a specified file path using the pickle module.\\nIt creates a ServiceCatalogIndex object by extracting various indices from the LazyServiceCatalogIndex.\\nIt then opens the specified file in write-binary mode and dumps the serialized ServiceCatalogIndex into the file.\\nFinally, it returns the created ServiceCatalogIndex object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "139246be8fb1",
        "ground_truth": "def save_service_index_cache(index: LazyServiceCatalogIndex, file_path: str) -> ServiceCatalogIndex:\n    \"\"\"\n    Creates from the given LazyServiceCatalogIndex a ``ServiceCatalogIndex`, pickles its contents into the given file,\n    and then returns the newly created index.\n     :param index: the LazyServiceCatalogIndex to store the index from.\n    :param file_path: the path to pickle to\n    :return: the created ServiceCatalogIndex\n    \"\"\"\n    import pickle\n     cache = ServiceCatalogIndex(\n        service_names=index.service_names,\n        endpoint_prefix_index=index.endpoint_prefix_index,\n        operations_index=index.operations_index,\n        signing_name_index=index.signing_name_index,\n        target_prefix_index=index.target_prefix_index,\n    )\n    with open(file_path, \"wb\") as fd:\n        pickle.dump(cache, fd)\n    return cache",
        "import_statements": [
            "import dataclasses",
            "import json",
            "import logging",
            "import os",
            "from collections import defaultdict",
            "from functools import cached_property, lru_cache",
            "from typing import Dict, Generator, List, Literal, NamedTuple, Optional, Tuple",
            "import jsonpatch",
            "from botocore.exceptions import UnknownServiceError",
            "from botocore.loaders import Loader, instance_cache",
            "from botocore.model import OperationModel, ServiceModel"
        ],
        "reference_api": [
            "pickle.dump",
            "open",
            "ServiceCatalogIndex"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ServiceCatalogIndex"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "function normalizeApigV1Headers({\n  multiValueHeaders,\n  headers,\n}: APIGatewayProxyEvent)",
        "start_line": "430",
        "end_line": "448",
        "file_path": "packages/astro-sst/src/lib/event-mapper.ts",
        "docstring": "The normalizeApigV1Headers function takes an APIGatewayProxyEvent object and combines its multiValueHeaders and headers properties into a single record with normalized header names.\\nIt first iterates over the multiValueHeaders, joining any multiple values into a single comma-separated string, and adds them to the combinedHeaders record with lowercase keys.\\nThen, it iterates over the headers, adding them to the combinedHeaders record with lowercase keys.\\nThe function returns the combinedHeaders record.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3a05772e3880",
        "ground_truth": "function normalizeApigV1Headers({\n  multiValueHeaders,\n  headers,\n}: APIGatewayProxyEvent) {\n  const combinedHeaders: Record<string, string> = {};\n   for (const [key, values] of Object.entries(multiValueHeaders ?? {})) {\n    if (values) {\n      combinedHeaders[key.toLowerCase()] = values.join(\",\");\n    }\n  }\n  for (const [key, value] of Object.entries(headers ?? {})) {\n    if (value) {\n      combinedHeaders[key.toLowerCase()] = value;\n    }\n  }\n   return combinedHeaders;\n}",
        "import_statements": [
            "import type {\n  APIGatewayProxyEventV2,\n  APIGatewayProxyResultV2,\n  APIGatewayProxyEvent,\n  APIGatewayProxyResult,\n  CloudFrontRequestEvent,\n  CloudFrontRequestResult,\n  CloudFrontHeaders,\n} from \"aws-lambda\";",
            "{\n  APIGatewayProxyEventV2,\n  APIGatewayProxyResultV2,\n  APIGatewayProxyEvent,\n  APIGatewayProxyResult,\n  CloudFrontRequestEvent,\n  CloudFrontRequestResult,\n  CloudFrontHeaders,\n}",
            "{\n  APIGatewayProxyEventV2,\n  APIGatewayProxyResultV2,\n  APIGatewayProxyEvent,\n  APIGatewayProxyResult,\n  CloudFrontRequestEvent,\n  CloudFrontRequestResult,\n  CloudFrontHeaders,\n}",
            "APIGatewayProxyEventV2",
            "APIGatewayProxyResultV2",
            "APIGatewayProxyEvent",
            "APIGatewayProxyResult",
            "CloudFrontRequestEvent",
            "CloudFrontRequestResult",
            "CloudFrontHeaders",
            "import type { ResponseStream } from \"./types\";",
            "{ ResponseStream }",
            "{ ResponseStream }",
            "ResponseStream",
            "import { splitCookiesString, parse, Cookie } from \"set-cookie-parser\";",
            "{ splitCookiesString, parse, Cookie }",
            "{ splitCookiesString, parse, Cookie }",
            "splitCookiesString",
            "parse",
            "Cookie",
            "import { isBinaryContentType } from \"./binary.js\";",
            "{ isBinaryContentType }",
            "{ isBinaryContentType }",
            "isBinaryContentType",
            "import zlib from \"zlib\";",
            "zlib"
        ],
        "reference_api": [
            "Object.entries",
            "key.toLowerCase",
            "values.join"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "key.toLowerCase",
                "code": "["
            },
            {
                "name": "values.join",
                "code": "["
            }
        ],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "function normalizeCfHeaders(event: CloudFrontRequestEvent)",
        "start_line": "450",
        "end_line": "464",
        "file_path": "packages/astro-sst/src/lib/event-mapper.ts",
        "docstring": "The normalizeCfHeaders function processes a CloudFrontRequestEvent to normalize its headers.\\nIt initializes an empty object, combinedHeaders, to store the results.\\nIt iterates over the headers of the first record's request in the event, converting each header key to lowercase and adding it to combinedHeaders with its associated value if the value is not null.\\nFinally, it returns the combinedHeaders object.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c17e051a8600",
        "ground_truth": "function normalizeCfHeaders(event: CloudFrontRequestEvent) {\n  const combinedHeaders: Record<string, string> = {};\n   for (const [key, values] of Object.entries(\n    event.Records[0].cf.request.headers\n  )) {\n    for (const { value } of values) {\n      if (value) {\n        combinedHeaders[key.toLowerCase()] = value;\n      }\n    }\n  }\n   return combinedHeaders;\n}",
        "import_statements": [
            "import type {\n  APIGatewayProxyEventV2,\n  APIGatewayProxyResultV2,\n  APIGatewayProxyEvent,\n  APIGatewayProxyResult,\n  CloudFrontRequestEvent,\n  CloudFrontRequestResult,\n  CloudFrontHeaders,\n} from \"aws-lambda\";",
            "{\n  APIGatewayProxyEventV2,\n  APIGatewayProxyResultV2,\n  APIGatewayProxyEvent,\n  APIGatewayProxyResult,\n  CloudFrontRequestEvent,\n  CloudFrontRequestResult,\n  CloudFrontHeaders,\n}",
            "{\n  APIGatewayProxyEventV2,\n  APIGatewayProxyResultV2,\n  APIGatewayProxyEvent,\n  APIGatewayProxyResult,\n  CloudFrontRequestEvent,\n  CloudFrontRequestResult,\n  CloudFrontHeaders,\n}",
            "APIGatewayProxyEventV2",
            "APIGatewayProxyResultV2",
            "APIGatewayProxyEvent",
            "APIGatewayProxyResult",
            "CloudFrontRequestEvent",
            "CloudFrontRequestResult",
            "CloudFrontHeaders",
            "import type { ResponseStream } from \"./types\";",
            "{ ResponseStream }",
            "{ ResponseStream }",
            "ResponseStream",
            "import { splitCookiesString, parse, Cookie } from \"set-cookie-parser\";",
            "{ splitCookiesString, parse, Cookie }",
            "{ splitCookiesString, parse, Cookie }",
            "splitCookiesString",
            "parse",
            "Cookie",
            "import { isBinaryContentType } from \"./binary.js\";",
            "{ isBinaryContentType }",
            "{ isBinaryContentType }",
            "isBinaryContentType",
            "import zlib from \"zlib\";",
            "zlib"
        ],
        "reference_api": [
            "Object.entries",
            "key.toLowerCase"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "key.toLowerCase",
                "code": "["
            }
        ],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "function createRequest(internalEvent: InternalEvent)",
        "start_line": "35",
        "end_line": "45",
        "file_path": "packages/astro-sst/src/entrypoint.ts",
        "docstring": "The createRequest function constructs a new Request object using the details provided in an InternalEvent object.\\nIt extracts the URL, HTTP method, headers, and body from the InternalEvent.\\nThe function sets the request body to undefined for GET and HEAD methods; otherwise, it uses the provided body.\\nIt then returns the new Request object with the specified URL and properties.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2bf8a4793a2b",
        "ground_truth": "function createRequest(internalEvent: InternalEvent) {\n  const requestUrl = internalEvent.url;\n  const requestProps = {\n    method: internalEvent.method,\n    headers: internalEvent.headers,\n    body: [\"GET\", \"HEAD\"].includes(internalEvent.method)\n      ? undefined\n      : internalEvent.body,\n  };\n  return new Request(requestUrl, requestProps);\n}",
        "import_statements": [
            "import type { SSRManifest } from \"astro\";",
            "{ SSRManifest }",
            "{ SSRManifest }",
            "SSRManifest",
            "import { version as ASTRO_VERSION } from \"astro/package.json\";",
            "{ version as ASTRO_VERSION }",
            "{ version as ASTRO_VERSION }",
            "version as ASTRO_VERSION",
            "import type {\n  APIGatewayProxyEventV2,\n  CloudFrontRequestEvent,\n} from \"aws-lambda\";",
            "{\n  APIGatewayProxyEventV2,\n  CloudFrontRequestEvent,\n}",
            "{\n  APIGatewayProxyEventV2,\n  CloudFrontRequestEvent,\n}",
            "APIGatewayProxyEventV2",
            "CloudFrontRequestEvent",
            "import type { RequestHandler, ResponseMode, ResponseStream } from \"./lib/types\";",
            "{ RequestHandler, ResponseMode, ResponseStream }",
            "{ RequestHandler, ResponseMode, ResponseStream }",
            "RequestHandler",
            "ResponseMode",
            "ResponseStream",
            "import { NodeApp } from \"astro/app/node\";",
            "{ NodeApp }",
            "{ NodeApp }",
            "NodeApp",
            "import { polyfill } from \"@astrojs/webapi\";",
            "{ polyfill }",
            "{ polyfill }",
            "polyfill",
            "import { InternalEvent, convertFrom, convertTo } from \"./lib/event-mapper.js\";",
            "{ InternalEvent, convertFrom, convertTo }",
            "{ InternalEvent, convertFrom, convertTo }",
            "InternalEvent",
            "convertFrom",
            "convertTo",
            "import { debug } from \"./lib/logger.js\";",
            "{ debug }",
            "{ debug }",
            "debug",
            "import { RenderOptions } from \"astro/app\";",
            "{ RenderOptions }",
            "{ RenderOptions }",
            "RenderOptions"
        ],
        "reference_api": [
            "[\"GET\", \"HEAD\"].includes"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "[\"GET\", \"HEAD\"].includes"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "export function HeaderSwitcher(\n  props: React.PropsWithChildren<HeaderSwitcherProps>\n)",
        "start_line": "113",
        "end_line": "131",
        "file_path": "packages/console/src/App/Stage/components.tsx",
        "docstring": "The HeaderSwitcher function is a React component that creates a dropdown menu using the DropdownMenu library.\\nIt accepts props of type React.PropsWithChildren<HeaderSwitcherProps>.\\nThe dropdown trigger displays the current value passed via props.value within a HeaderSwitcherValue component.\\nWhen triggered, the dropdown content is displayed below the trigger and aligns to the start.\\nThe dropdown content includes the children elements passed via props.children.\\nAn optional input filter for filtering options is commented out within the dropdown content.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7d0a7922f328",
        "ground_truth": "export function HeaderSwitcher(\n  props: React.PropsWithChildren<HeaderSwitcherProps>\n) {\n  return (\n    <DropdownMenu.Root>\n      <DropdownMenu.Trigger asChild>\n        <HeaderSwitcherValue>{props.value}</HeaderSwitcherValue>\n      </DropdownMenu.Trigger>\n      <DropdownMenu.Content side=\"bottom\" align=\"start\">\n        {/*\n        <HeaderSwitcherFilter>\n          <input autoFocus placeholder=\"Filter...\" />\n        </HeaderSwitcherFilter>\n        */}\n        {props.children}\n      </DropdownMenu.Content>\n    </DropdownMenu.Root>\n  );\n}",
        "import_statements": [
            "import ReactDOM from \"react-dom\";",
            "ReactDOM",
            "import { Link } from \"react-router-dom\";",
            "{ Link }",
            "{ Link }",
            "Link",
            "import { Anchor, DropdownMenu } from \"~/components\";",
            "{ Anchor, DropdownMenu }",
            "{ Anchor, DropdownMenu }",
            "Anchor",
            "DropdownMenu",
            "import { styled } from \"~/stitches.config\";",
            "{ styled }",
            "{ styled }",
            "styled"
        ],
        "reference_api": [],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "export async function deployStack(\n  options: DeployStackOptions\n): Promise<DeployStackResult | undefined>",
        "start_line": "247",
        "end_line": "276",
        "file_path": "packages/sst/src/cdk/deploy-stack.ts",
        "docstring": "The deployStack function is an asynchronous operation that deploys a CloudFormation stack using the provided DeployStackOptions.\\nIt initializes necessary variables such as stackArtifact and stackEnv from the options.\\nIt appends a custom user agent to the SDK and retrieves the CloudFormation client.\\nIt determines the deploy name and looks up the CloudFormation stack using a retry mechanism.\\nIf the stack exists and had previously failed during creation, it deletes the stack and waits for the deletion to complete before proceeding.\\nIf the stack deletion fails, it throws an error.\\nFinally, it sets the cloudFormationStack variable to a non-existent state if the stack was successfully deleted.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d9eef47c2a55",
        "ground_truth": "export async function deployStack(\n  options: DeployStackOptions\n): Promise<DeployStackResult | undefined> {\n  const stackArtifact = options.stack;\n   const stackEnv = options.resolvedEnvironment;\n   options.sdk.appendCustomUserAgent(options.extraUserAgent);\n  const cfn = options.sdk.cloudFormation();\n  const deployName = options.deployName || stackArtifact.stackName;\n  let cloudFormationStack = await callWithRetry(() =>\n    CloudFormationStack.lookup(cfn, deployName)\n  );\n   if (cloudFormationStack.stackStatus.isCreationFailure) {\n    debug(\n      `Found existing stack ${deployName} that had previously failed creation. Deleting it before attempting to re-create it.`\n    );\n    await cfn.deleteStack({ StackName: deployName }).promise();\n    const deletedStack = await waitForStackDelete(cfn, deployName);\n    if (deletedStack && deletedStack.stackStatus.name !== \"DELETE_COMPLETE\") {\n      throw new Error(\n        `Failed deleting stack ${deployName} that had previously failed creation (current state: ${deletedStack.stackStatus})`\n      );\n    }\n    // Update variable to mark that the stack does not exist anymore, but avoid\n    // doing an actual lookup in CloudFormation (which would be silly to do if\n    // we just deleted it).\n    cloudFormationStack = CloudFormationStack.doesNotExist(cfn, deployName);\n  }",
        "import_statements": [
            "import * as cxapi from \"@aws-cdk/cx-api\";",
            "* as cxapi",
            "* as cxapi",
            "import type { CloudFormation } from \"aws-sdk\";",
            "{ CloudFormation }",
            "{ CloudFormation }",
            "CloudFormation",
            "import * as uuid from \"uuid\";",
            "* as uuid",
            "* as uuid",
            "import {\n  TemplateBodyParameter,\n  makeBodyParameter,\n} from \"sst-aws-cdk/lib/api/util/template-body-parameter.js\";",
            "{\n  TemplateBodyParameter,\n  makeBodyParameter,\n}",
            "{\n  TemplateBodyParameter,\n  makeBodyParameter,\n}",
            "TemplateBodyParameter",
            "makeBodyParameter",
            "import { addMetadataAssetsToManifest } from \"sst-aws-cdk/lib/assets.js\";",
            "{ addMetadataAssetsToManifest }",
            "{ addMetadataAssetsToManifest }",
            "addMetadataAssetsToManifest",
            "import { Tag } from \"sst-aws-cdk/lib/cdk-toolkit.js\";",
            "{ Tag }",
            "{ Tag }",
            "Tag",
            "import { debug, print, warning } from \"sst-aws-cdk/lib/logging.js\";",
            "{ debug, print, warning }",
            "{ debug, print, warning }",
            "debug",
            "print",
            "warning",
            "import { AssetManifestBuilder } from \"sst-aws-cdk/lib/util/asset-manifest-builder.js\";",
            "{ AssetManifestBuilder }",
            "{ AssetManifestBuilder }",
            "AssetManifestBuilder",
            "import { publishAssets } from \"sst-aws-cdk/lib/util/asset-publishing.js\";",
            "{ publishAssets }",
            "{ publishAssets }",
            "publishAssets",
            "import { ISDK, SdkProvider } from \"sst-aws-cdk/lib/api/aws-auth/index.js\";",
            "{ ISDK, SdkProvider }",
            "{ ISDK, SdkProvider }",
            "ISDK",
            "SdkProvider",
            "import { EnvironmentResources } from \"sst-aws-cdk/lib/api/environment-resources.js\";",
            "{ EnvironmentResources }",
            "{ EnvironmentResources }",
            "EnvironmentResources",
            "import { CfnEvaluationException } from \"sst-aws-cdk/lib/api/evaluate-cloudformation-template.js\";",
            "{ CfnEvaluationException }",
            "{ CfnEvaluationException }",
            "CfnEvaluationException",
            "import { HotswapMode, ICON } from \"sst-aws-cdk/lib/api/hotswap/common.js\";",
            "{ HotswapMode, ICON }",
            "{ HotswapMode, ICON }",
            "HotswapMode",
            "ICON",
            "import { tryHotswapDeployment } from \"sst-aws-cdk/lib/api/hotswap-deployments.js\";",
            "{ tryHotswapDeployment }",
            "{ tryHotswapDeployment }",
            "tryHotswapDeployment",
            "import {\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n} from \"sst-aws-cdk/lib/api/util/cloudformation.js\";",
            "{\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n}",
            "{\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n}",
            "changeSetHasNoChanges",
            "CloudFormationStack",
            "TemplateParameters",
            "waitForChangeSet",
            "waitForStackDeploy",
            "waitForStackDelete",
            "ParameterValues",
            "ParameterChanges",
            "ResourcesToImport",
            "import {\n  // StackActivityMonitor,\n  StackActivityProgress,\n} from \"sst-aws-cdk/lib/api/util/cloudformation/stack-activity-monitor.js\";",
            "{\n  // StackActivityMonitor,\n  StackActivityProgress,\n}",
            "{\n  // StackActivityMonitor,\n  StackActivityProgress,\n}",
            "StackActivityProgress",
            "import { blue } from \"colorette\";",
            "{ blue }",
            "{ blue }",
            "blue",
            "import { callWithRetry } from \"./util.js\";",
            "{ callWithRetry }",
            "{ callWithRetry }",
            "callWithRetry"
        ],
        "reference_api": [
            "CloudFormationStack.lookup"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "CloudFormationStack.lookup",
                "code": ")"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "public async performDeployment(): Promise<DeployStackResult | undefined>",
        "start_line": "423",
        "end_line": "441",
        "file_path": "packages/sst/src/cdk/deploy-stack.ts",
        "docstring": "The performDeployment function asynchronously initiates a deployment process and returns a promise that resolves to a DeployStackResult or undefined.\\nIt first determines the deployment method, defaulting to \"change-set\" if not specified.\\nIf the method is \"direct\" and there are resources to import, it throws an error because importing resources requires a changeset deployment.\\nDepending on the deployment method, it either calls changeSetDeployment or directDeployment to perform the deployment.\n",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c8ad45817af9",
        "ground_truth": "public async performDeployment(): Promise<DeployStackResult | undefined> {\n  const deploymentMethod = this.options.deploymentMethod ?? {\n    method: \"change-set\",\n  };\n  if (\n    deploymentMethod.method === \"direct\" &&\n    this.options.resourcesToImport\n  ) {\n    throw new Error(\"Importing resources requires a changeset deployment\");\n  }\n  switch (deploymentMethod.method) {\n    case \"change-set\":\n      return this.changeSetDeployment(deploymentMethod);\n    case \"direct\":\n      return this.directDeployment();\n  }\n}",
        "import_statements": [
            "import * as cxapi from \"@aws-cdk/cx-api\";",
            "* as cxapi",
            "* as cxapi",
            "import type { CloudFormation } from \"aws-sdk\";",
            "{ CloudFormation }",
            "{ CloudFormation }",
            "CloudFormation",
            "import * as uuid from \"uuid\";",
            "* as uuid",
            "* as uuid",
            "import {\n  TemplateBodyParameter,\n  makeBodyParameter,\n} from \"sst-aws-cdk/lib/api/util/template-body-parameter.js\";",
            "{\n  TemplateBodyParameter,\n  makeBodyParameter,\n}",
            "{\n  TemplateBodyParameter,\n  makeBodyParameter,\n}",
            "TemplateBodyParameter",
            "makeBodyParameter",
            "import { addMetadataAssetsToManifest } from \"sst-aws-cdk/lib/assets.js\";",
            "{ addMetadataAssetsToManifest }",
            "{ addMetadataAssetsToManifest }",
            "addMetadataAssetsToManifest",
            "import { Tag } from \"sst-aws-cdk/lib/cdk-toolkit.js\";",
            "{ Tag }",
            "{ Tag }",
            "Tag",
            "import { debug, print, warning } from \"sst-aws-cdk/lib/logging.js\";",
            "{ debug, print, warning }",
            "{ debug, print, warning }",
            "debug",
            "print",
            "warning",
            "import { AssetManifestBuilder } from \"sst-aws-cdk/lib/util/asset-manifest-builder.js\";",
            "{ AssetManifestBuilder }",
            "{ AssetManifestBuilder }",
            "AssetManifestBuilder",
            "import { publishAssets } from \"sst-aws-cdk/lib/util/asset-publishing.js\";",
            "{ publishAssets }",
            "{ publishAssets }",
            "publishAssets",
            "import { ISDK, SdkProvider } from \"sst-aws-cdk/lib/api/aws-auth/index.js\";",
            "{ ISDK, SdkProvider }",
            "{ ISDK, SdkProvider }",
            "ISDK",
            "SdkProvider",
            "import { EnvironmentResources } from \"sst-aws-cdk/lib/api/environment-resources.js\";",
            "{ EnvironmentResources }",
            "{ EnvironmentResources }",
            "EnvironmentResources",
            "import { CfnEvaluationException } from \"sst-aws-cdk/lib/api/evaluate-cloudformation-template.js\";",
            "{ CfnEvaluationException }",
            "{ CfnEvaluationException }",
            "CfnEvaluationException",
            "import { HotswapMode, ICON } from \"sst-aws-cdk/lib/api/hotswap/common.js\";",
            "{ HotswapMode, ICON }",
            "{ HotswapMode, ICON }",
            "HotswapMode",
            "ICON",
            "import { tryHotswapDeployment } from \"sst-aws-cdk/lib/api/hotswap-deployments.js\";",
            "{ tryHotswapDeployment }",
            "{ tryHotswapDeployment }",
            "tryHotswapDeployment",
            "import {\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n} from \"sst-aws-cdk/lib/api/util/cloudformation.js\";",
            "{\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n}",
            "{\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n}",
            "changeSetHasNoChanges",
            "CloudFormationStack",
            "TemplateParameters",
            "waitForChangeSet",
            "waitForStackDeploy",
            "waitForStackDelete",
            "ParameterValues",
            "ParameterChanges",
            "ResourcesToImport",
            "import {\n  // StackActivityMonitor,\n  StackActivityProgress,\n} from \"sst-aws-cdk/lib/api/util/cloudformation/stack-activity-monitor.js\";",
            "{\n  // StackActivityMonitor,\n  StackActivityProgress,\n}",
            "{\n  // StackActivityMonitor,\n  StackActivityProgress,\n}",
            "StackActivityProgress",
            "import { blue } from \"colorette\";",
            "{ blue }",
            "{ blue }",
            "blue",
            "import { callWithRetry } from \"./util.js\";",
            "{ callWithRetry }",
            "{ callWithRetry }",
            "callWithRetry"
        ],
        "reference_api": [
            "this.changeSetDeployment",
            "performDeployment",
            "{\n      method: \"change-set\",\n    };\n\n    if (\n      deploymentMethod.method === \"direct\" &&\n      this.options.resourcesToImport\n    )",
            "{\n      method: \"change-set\",\n    }",
            "this.directDeployment"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "this.changeSetDeployment",
                "code": ")"
            },
            {
                "name": "performDeployment",
                "code": ")"
            },
            {
                "name": "{\n      method: \"change-set\",\n    };\n\n    if (\n      deploymentMethod.method === \"direct\" &&\n      this.options.resourcesToImport\n    )",
                "code": ")"
            },
            {
                "name": "{\n      method: \"change-set\",\n    }",
                "code": ")"
            },
            {
                "name": "this.directDeployment",
                "code": ")"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "private async cleanupOldChangeset(changeSetName: string)",
        "start_line": "564",
        "end_line": "578",
        "file_path": "packages/sst/src/cdk/deploy-stack.ts",
        "docstring": "The cleanupOldChangeset function asynchronously deletes an existing CloudFormation change set if it exists.\\nIt first checks if the CloudFormation stack exists.\\nIf it does, it logs a debug message indicating the removal of the change set with the specified name.\\nThen, it calls the deleteChangeSet method on the CloudFormation service object, passing the stack name and change set name, and awaits its promise to ensure completion.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "872c0a1fd7db",
        "ground_truth": "private async cleanupOldChangeset(changeSetName: string) {\n  if (this.cloudFormationStack.exists) {\n    // Delete any existing change sets generated by CDK since change set names must be unique.\n    // The delete request is successful as long as the stack exists (even if the change set does not exist).\n    debug(\n      `Removing existing change set with name ${changeSetName} if it exists`\n    );\n    await this.cfn\n      .deleteChangeSet({\n        StackName: this.stackName,\n        ChangeSetName: changeSetName,\n      })\n      .promise();\n  }\n}",
        "import_statements": [
            "import * as cxapi from \"@aws-cdk/cx-api\";",
            "* as cxapi",
            "* as cxapi",
            "import type { CloudFormation } from \"aws-sdk\";",
            "{ CloudFormation }",
            "{ CloudFormation }",
            "CloudFormation",
            "import * as uuid from \"uuid\";",
            "* as uuid",
            "* as uuid",
            "import {\n  TemplateBodyParameter,\n  makeBodyParameter,\n} from \"sst-aws-cdk/lib/api/util/template-body-parameter.js\";",
            "{\n  TemplateBodyParameter,\n  makeBodyParameter,\n}",
            "{\n  TemplateBodyParameter,\n  makeBodyParameter,\n}",
            "TemplateBodyParameter",
            "makeBodyParameter",
            "import { addMetadataAssetsToManifest } from \"sst-aws-cdk/lib/assets.js\";",
            "{ addMetadataAssetsToManifest }",
            "{ addMetadataAssetsToManifest }",
            "addMetadataAssetsToManifest",
            "import { Tag } from \"sst-aws-cdk/lib/cdk-toolkit.js\";",
            "{ Tag }",
            "{ Tag }",
            "Tag",
            "import { debug, print, warning } from \"sst-aws-cdk/lib/logging.js\";",
            "{ debug, print, warning }",
            "{ debug, print, warning }",
            "debug",
            "print",
            "warning",
            "import { AssetManifestBuilder } from \"sst-aws-cdk/lib/util/asset-manifest-builder.js\";",
            "{ AssetManifestBuilder }",
            "{ AssetManifestBuilder }",
            "AssetManifestBuilder",
            "import { publishAssets } from \"sst-aws-cdk/lib/util/asset-publishing.js\";",
            "{ publishAssets }",
            "{ publishAssets }",
            "publishAssets",
            "import { ISDK, SdkProvider } from \"sst-aws-cdk/lib/api/aws-auth/index.js\";",
            "{ ISDK, SdkProvider }",
            "{ ISDK, SdkProvider }",
            "ISDK",
            "SdkProvider",
            "import { EnvironmentResources } from \"sst-aws-cdk/lib/api/environment-resources.js\";",
            "{ EnvironmentResources }",
            "{ EnvironmentResources }",
            "EnvironmentResources",
            "import { CfnEvaluationException } from \"sst-aws-cdk/lib/api/evaluate-cloudformation-template.js\";",
            "{ CfnEvaluationException }",
            "{ CfnEvaluationException }",
            "CfnEvaluationException",
            "import { HotswapMode, ICON } from \"sst-aws-cdk/lib/api/hotswap/common.js\";",
            "{ HotswapMode, ICON }",
            "{ HotswapMode, ICON }",
            "HotswapMode",
            "ICON",
            "import { tryHotswapDeployment } from \"sst-aws-cdk/lib/api/hotswap-deployments.js\";",
            "{ tryHotswapDeployment }",
            "{ tryHotswapDeployment }",
            "tryHotswapDeployment",
            "import {\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n} from \"sst-aws-cdk/lib/api/util/cloudformation.js\";",
            "{\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n}",
            "{\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n}",
            "changeSetHasNoChanges",
            "CloudFormationStack",
            "TemplateParameters",
            "waitForChangeSet",
            "waitForStackDeploy",
            "waitForStackDelete",
            "ParameterValues",
            "ParameterChanges",
            "ResourcesToImport",
            "import {\n  // StackActivityMonitor,\n  StackActivityProgress,\n} from \"sst-aws-cdk/lib/api/util/cloudformation/stack-activity-monitor.js\";",
            "{\n  // StackActivityMonitor,\n  StackActivityProgress,\n}",
            "{\n  // StackActivityMonitor,\n  StackActivityProgress,\n}",
            "StackActivityProgress",
            "import { blue } from \"colorette\";",
            "{ blue }",
            "{ blue }",
            "blue",
            "import { callWithRetry } from \"./util.js\";",
            "{ callWithRetry }",
            "{ callWithRetry }",
            "callWithRetry"
        ],
        "reference_api": [
            "cleanupOldChangeset",
            "this.cfn\n        .deleteChangeSet({\n          StackName: this.stackName,\n          ChangeSetName: changeSetName,\n        })\n        .promise",
            "debug",
            "this.cfn\n        .deleteChangeSet"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "cleanupOldChangeset",
                "code": ")"
            },
            {
                "name": "this.cfn\n        .deleteChangeSet({\n          StackName: this.stackName,\n          ChangeSetName: changeSetName,\n        })\n        .promise",
                "code": ")"
            },
            {
                "name": "debug",
                "code": ")"
            },
            {
                "name": "this.cfn\n        .deleteChangeSet",
                "code": ")"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "export async function destroyStack(options: DestroyStackOptions)",
        "start_line": "752",
        "end_line": "790",
        "file_path": "packages/sst/src/cdk/deploy-stack.ts",
        "docstring": "The destroyStack function deletes a specified AWS CloudFormation stack.\\nIt takes a DestroyStackOptions object as an argument, which includes the stack information and optional settings.\\nThe function first determines the stack name to delete and initializes the CloudFormation client.\\nIt then checks if the stack exists; if not, it returns immediately.\\nIf the stack exists, the function attempts to delete it using the deleteStack method and waits for the deletion to complete.\\nIf the deletion fails or the final stack status is not \"DELETE_COMPLETE\", it throws an error.\\nThe function includes commented-out code for monitoring stack activity, which can be enabled if needed.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7c984f38da7f",
        "ground_truth": "export async function destroyStack(options: DestroyStackOptions) {\n  const deployName = options.deployName || options.stack.stackName;\n  const cfn = options.sdk.cloudFormation();\n   const currentStack = await CloudFormationStack.lookup(cfn, deployName);\n  if (!currentStack.exists) {\n    return;\n  }\n  /*\n  const monitor = options.quiet\n    ? undefined\n    : StackActivityMonitor.withDefaultPrinter(cfn, deployName, options.stack, {\n        ci: options.ci,\n      }).start();\n  */\n   try {\n    await cfn\n      .deleteStack({ StackName: deployName, RoleARN: options.roleArn })\n      .promise();\n    const destroyedStack = await waitForStackDelete(cfn, deployName);\n    if (\n      destroyedStack &&\n      destroyedStack.stackStatus.name !== \"DELETE_COMPLETE\"\n    ) {\n      throw new Error(\n        `Failed to destroy ${deployName}: ${destroyedStack.stackStatus}`\n      );\n    }\n  } catch (e: any) {\n    throw new Error(suffixWithErrors(e.message /* , monitor?.errors */));\n  } finally {\n    /*\n    if (monitor) {\n      await monitor.stop();\n    }\n    */\n  }\n}",
        "import_statements": [
            "import * as cxapi from \"@aws-cdk/cx-api\";",
            "* as cxapi",
            "* as cxapi",
            "import type { CloudFormation } from \"aws-sdk\";",
            "{ CloudFormation }",
            "{ CloudFormation }",
            "CloudFormation",
            "import * as uuid from \"uuid\";",
            "* as uuid",
            "* as uuid",
            "import {\n  TemplateBodyParameter,\n  makeBodyParameter,\n} from \"sst-aws-cdk/lib/api/util/template-body-parameter.js\";",
            "{\n  TemplateBodyParameter,\n  makeBodyParameter,\n}",
            "{\n  TemplateBodyParameter,\n  makeBodyParameter,\n}",
            "TemplateBodyParameter",
            "makeBodyParameter",
            "import { addMetadataAssetsToManifest } from \"sst-aws-cdk/lib/assets.js\";",
            "{ addMetadataAssetsToManifest }",
            "{ addMetadataAssetsToManifest }",
            "addMetadataAssetsToManifest",
            "import { Tag } from \"sst-aws-cdk/lib/cdk-toolkit.js\";",
            "{ Tag }",
            "{ Tag }",
            "Tag",
            "import { debug, print, warning } from \"sst-aws-cdk/lib/logging.js\";",
            "{ debug, print, warning }",
            "{ debug, print, warning }",
            "debug",
            "print",
            "warning",
            "import { AssetManifestBuilder } from \"sst-aws-cdk/lib/util/asset-manifest-builder.js\";",
            "{ AssetManifestBuilder }",
            "{ AssetManifestBuilder }",
            "AssetManifestBuilder",
            "import { publishAssets } from \"sst-aws-cdk/lib/util/asset-publishing.js\";",
            "{ publishAssets }",
            "{ publishAssets }",
            "publishAssets",
            "import { ISDK, SdkProvider } from \"sst-aws-cdk/lib/api/aws-auth/index.js\";",
            "{ ISDK, SdkProvider }",
            "{ ISDK, SdkProvider }",
            "ISDK",
            "SdkProvider",
            "import { EnvironmentResources } from \"sst-aws-cdk/lib/api/environment-resources.js\";",
            "{ EnvironmentResources }",
            "{ EnvironmentResources }",
            "EnvironmentResources",
            "import { CfnEvaluationException } from \"sst-aws-cdk/lib/api/evaluate-cloudformation-template.js\";",
            "{ CfnEvaluationException }",
            "{ CfnEvaluationException }",
            "CfnEvaluationException",
            "import { HotswapMode, ICON } from \"sst-aws-cdk/lib/api/hotswap/common.js\";",
            "{ HotswapMode, ICON }",
            "{ HotswapMode, ICON }",
            "HotswapMode",
            "ICON",
            "import { tryHotswapDeployment } from \"sst-aws-cdk/lib/api/hotswap-deployments.js\";",
            "{ tryHotswapDeployment }",
            "{ tryHotswapDeployment }",
            "tryHotswapDeployment",
            "import {\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n} from \"sst-aws-cdk/lib/api/util/cloudformation.js\";",
            "{\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n}",
            "{\n  changeSetHasNoChanges,\n  CloudFormationStack,\n  TemplateParameters,\n  waitForChangeSet,\n  waitForStackDeploy,\n  waitForStackDelete,\n  ParameterValues,\n  ParameterChanges,\n  ResourcesToImport,\n}",
            "changeSetHasNoChanges",
            "CloudFormationStack",
            "TemplateParameters",
            "waitForChangeSet",
            "waitForStackDeploy",
            "waitForStackDelete",
            "ParameterValues",
            "ParameterChanges",
            "ResourcesToImport",
            "import {\n  // StackActivityMonitor,\n  StackActivityProgress,\n} from \"sst-aws-cdk/lib/api/util/cloudformation/stack-activity-monitor.js\";",
            "{\n  // StackActivityMonitor,\n  StackActivityProgress,\n}",
            "{\n  // StackActivityMonitor,\n  StackActivityProgress,\n}",
            "StackActivityProgress",
            "import { blue } from \"colorette\";",
            "{ blue }",
            "{ blue }",
            "blue",
            "import { callWithRetry } from \"./util.js\";",
            "{ callWithRetry }",
            "{ callWithRetry }",
            "callWithRetry"
        ],
        "reference_api": [
            "options.sdk.cloudFormation",
            "cfn\n      .deleteStack",
            "suffixWithErrors",
            "cfn\n      .deleteStack({ StackName: deployName, RoleARN: options.roleArn })\n      .promise",
            "waitForStackDelete",
            "CloudFormationStack.lookup"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "options.sdk.cloudFormation",
                "code": ")"
            },
            {
                "name": "cfn\n      .deleteStack",
                "code": ")"
            },
            {
                "name": "suffixWithErrors",
                "code": ")"
            },
            {
                "name": "cfn\n      .deleteStack({ StackName: deployName, RoleARN: options.roleArn })\n      .promise",
                "code": ")"
            },
            {
                "name": "waitForStackDelete",
                "code": ")"
            },
            {
                "name": "CloudFormationStack.lookup",
                "code": ")"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "public async resolveEnvironment(\n    stack: cxapi.CloudFormationStackArtifact\n  ): Promise<cxapi.Environment>",
        "start_line": "360",
        "end_line": "364",
        "file_path": "packages/sst/src/cdk/deployments.ts",
        "docstring": "The resolveEnvironment function is an asynchronous method that resolves the environment for a given CloudFormation stack artifact.\\nIt takes a CloudFormationStackArtifact object as an argument and returns a Promise that resolves to an Environment object.\\nThe method uses the sdkProvider's resolveEnvironment method to achieve this.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ca956e9b4b18",
        "ground_truth": "public async resolveEnvironment(\n  stack: cxapi.CloudFormationStackArtifact\n): Promise<cxapi.Environment> {\n  return this.sdkProvider.resolveEnvironment(stack.environment);\n}",
        "import_statements": [
            "import * as cxapi from \"@aws-cdk/cx-api\";",
            "* as cxapi",
            "* as cxapi",
            "import * as cdk_assets from \"cdk-assets\";",
            "* as cdk_assets",
            "* as cdk_assets",
            "import { AssetManifest, IManifestEntry } from \"cdk-assets\";",
            "{ AssetManifest, IManifestEntry }",
            "{ AssetManifest, IManifestEntry }",
            "AssetManifest",
            "IManifestEntry",
            "import { Tag } from \"sst-aws-cdk/lib/cdk-toolkit.js\";",
            "{ Tag }",
            "{ Tag }",
            "Tag",
            "import { debug, warning, error } from \"sst-aws-cdk/lib/logging.js\";",
            "{ debug, warning, error }",
            "{ debug, warning, error }",
            "debug",
            "warning",
            "error",
            "import {\n  buildAssets,\n  publishAssets,\n  BuildAssetsOptions,\n  PublishAssetsOptions,\n  PublishingAws,\n  EVENT_TO_LOGGER,\n} from \"sst-aws-cdk/lib/util/asset-publishing.js\";",
            "{\n  buildAssets,\n  publishAssets,\n  BuildAssetsOptions,\n  PublishAssetsOptions,\n  PublishingAws,\n  EVENT_TO_LOGGER,\n}",
            "{\n  buildAssets,\n  publishAssets,\n  BuildAssetsOptions,\n  PublishAssetsOptions,\n  PublishingAws,\n  EVENT_TO_LOGGER,\n}",
            "buildAssets",
            "publishAssets",
            "BuildAssetsOptions",
            "PublishAssetsOptions",
            "PublishingAws",
            "EVENT_TO_LOGGER",
            "import { Mode } from \"sst-aws-cdk/lib/api/aws-auth/credentials.js\";",
            "{ Mode }",
            "{ Mode }",
            "Mode",
            "import { ISDK } from \"sst-aws-cdk/lib/api/aws-auth/sdk.js\";",
            "{ ISDK }",
            "{ ISDK }",
            "ISDK",
            "import {\n  CredentialsOptions,\n  SdkForEnvironment,\n  SdkProvider,\n} from \"sst-aws-cdk/lib/api/aws-auth/sdk-provider.js\";",
            "{\n  CredentialsOptions,\n  SdkForEnvironment,\n  SdkProvider,\n}",
            "{\n  CredentialsOptions,\n  SdkForEnvironment,\n  SdkProvider,\n}",
            "CredentialsOptions",
            "SdkForEnvironment",
            "SdkProvider",
            "import {\n  deployStack,\n  DeployStackResult,\n  destroyStack,\n  DeploymentMethod,\n} from \"./deploy-stack.js\";",
            "{\n  deployStack,\n  DeployStackResult,\n  destroyStack,\n  DeploymentMethod,\n}",
            "{\n  deployStack,\n  DeployStackResult,\n  destroyStack,\n  DeploymentMethod,\n}",
            "deployStack",
            "DeployStackResult",
            "destroyStack",
            "DeploymentMethod",
            "import {\n  EnvironmentResources,\n  EnvironmentResourcesRegistry,\n} from \"sst-aws-cdk/lib/api/environment-resources.js\";",
            "{\n  EnvironmentResources,\n  EnvironmentResourcesRegistry,\n}",
            "{\n  EnvironmentResources,\n  EnvironmentResourcesRegistry,\n}",
            "EnvironmentResources",
            "EnvironmentResourcesRegistry",
            "import {\n  loadCurrentTemplateWithNestedStacks,\n  loadCurrentTemplate,\n  RootTemplateWithNestedStacks,\n} from \"sst-aws-cdk/lib/api/nested-stack-helpers.js\";",
            "{\n  loadCurrentTemplateWithNestedStacks,\n  loadCurrentTemplate,\n  RootTemplateWithNestedStacks,\n}",
            "{\n  loadCurrentTemplateWithNestedStacks,\n  loadCurrentTemplate,\n  RootTemplateWithNestedStacks,\n}",
            "loadCurrentTemplateWithNestedStacks",
            "loadCurrentTemplate",
            "RootTemplateWithNestedStacks",
            "import {\n  CloudFormationStack,\n  Template,\n  ResourcesToImport,\n  ResourceIdentifierSummaries,\n} from \"sst-aws-cdk/lib/api/util/cloudformation.js\";",
            "{\n  CloudFormationStack,\n  Template,\n  ResourcesToImport,\n  ResourceIdentifierSummaries,\n}",
            "{\n  CloudFormationStack,\n  Template,\n  ResourcesToImport,\n  ResourceIdentifierSummaries,\n}",
            "CloudFormationStack",
            "Template",
            "ResourcesToImport",
            "ResourceIdentifierSummaries",
            "import { StackActivityProgress } from \"sst-aws-cdk/lib/api/util/cloudformation/stack-activity-monitor.js\";",
            "{ StackActivityProgress }",
            "{ StackActivityProgress }",
            "StackActivityProgress",
            "import { replaceEnvPlaceholders } from \"sst-aws-cdk/lib/api/util/placeholders.js\";",
            "{ replaceEnvPlaceholders }",
            "{ replaceEnvPlaceholders }",
            "replaceEnvPlaceholders",
            "import { makeBodyParameterAndUpload } from \"sst-aws-cdk/lib/api/util/template-body-parameter.js\";",
            "{ makeBodyParameterAndUpload }",
            "{ makeBodyParameterAndUpload }",
            "makeBodyParameterAndUpload",
            "import { callWithRetry } from \"./util.js\";",
            "{ callWithRetry }",
            "{ callWithRetry }",
            "callWithRetry",
            "import { HotswapMode } from \"sst-aws-cdk/lib/api/hotswap/common.js\";",
            "{ HotswapMode }",
            "{ HotswapMode }",
            "HotswapMode"
        ],
        "reference_api": [
            "resolveEnvironment",
            "Promise<cxapi.Environment> {\n    return this.sdkProvider.resolveEnvironment"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "resolveEnvironment",
            "Promise<cxapi.Environment> {\n    return this.sdkProvider.resolveEnvironment"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "async function build(route: any)",
        "start_line": "19",
        "end_line": "37",
        "file_path": "packages/sst/src/cli/commands/plugins/pothos.ts",
        "docstring": "The async function build generates a schema using Pothos based on the provided route's schema and internalPackages.\\nIt writes the generated schema to the specified output file and optionally executes additional commands if provided.\\nUpon successful execution, it logs a success message using the Colors module.\\nIf an error occurs, it logs a failure message and prints the error details line by line.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e91a4b37ea2c",
        "ground_truth": "async function build(route: any) {\n  try {\n    const schema = await Pothos.generate({\n      schema: route.schema,\n      internalPackages: route.internalPackages,\n    });\n    await fs.writeFile(route.output, schema);\n    // bus.publish(\"pothos.extracted\", { file: route.output });\n    if (Array.isArray(route.commands) && route.commands.length > 0) {\n      await Promise.all(route.commands.map((cmd: string) => execAsync(cmd)));\n    }\n    Colors.line(Colors.success(`\u2714`), \" Pothos: Extracted pothos schema\");\n  } catch (ex: any) {\n    Colors.line(Colors.danger(`\u2716`), \" Pothos: Failed to extract schema:\");\n    for (let line of ex.message.split(\"\\n\")) {\n      console.log(`  `, line);\n    }\n  }\n}",
        "import_statements": [
            "import { useBus } from \"../../../bus.js\";",
            "{ useBus }",
            "{ useBus }",
            "useBus",
            "import { ApiMetadata } from \"../../../constructs/Metadata.js\";",
            "{ ApiMetadata }",
            "{ ApiMetadata }",
            "ApiMetadata",
            "import { Pothos } from \"../../../pothos.js\";",
            "{ Pothos }",
            "{ Pothos }",
            "Pothos",
            "import fs from \"fs/promises\";",
            "fs",
            "import { exec } from \"child_process\";",
            "{ exec }",
            "{ exec }",
            "exec",
            "import { promisify } from \"util\";",
            "{ promisify }",
            "{ promisify }",
            "promisify",
            "import path from \"path\";",
            "path",
            "import { Colors } from \"../../colors.js\";",
            "{ Colors }",
            "{ Colors }",
            "Colors",
            "import { lazy } from \"../../../util/lazy.js\";",
            "{ lazy }",
            "{ lazy }",
            "lazy"
        ],
        "reference_api": [
            "execAsync"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "execAsync",
                "code": ")"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "sst/sst",
        "function_declaration": "async function getLocalIamCredentials()",
        "start_line": "404",
        "end_line": "412",
        "file_path": "packages/sst/src/cli/commands/bind.ts",
        "docstring": "The getLocalIamCredentials function asynchronously retrieves AWS credentials.\\nIt imports the useAWSCredentials function from the specified path and then calls it to obtain the credentials.\\nIt returns an object containing AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_SESSION_TOKEN extracted from the retrieved credentials.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9933fe9d9dba",
        "ground_truth": "async function getLocalIamCredentials() {\n  const { useAWSCredentials } = await import(\"../../credentials.js\");\n  const credentials = await useAWSCredentials();\n  return {\n    AWS_ACCESS_KEY_ID: credentials.accessKeyId,\n    AWS_SECRET_ACCESS_KEY: credentials.secretAccessKey,\n    AWS_SESSION_TOKEN: credentials.sessionToken,\n  };\n}",
        "import_statements": [
            "import path from \"path\";",
            "path",
            "import { VisibleError } from \"../../error.js\";",
            "{ VisibleError }",
            "{ VisibleError }",
            "VisibleError",
            "import type { Program } from \"../program.js\";",
            "{ Program }",
            "{ Program }",
            "Program",
            "import type {\n  ServiceMetadata,\n  SlsNextjsMetadata,\n  SSRSiteMetadata,\n  StaticSiteMetadata,\n} from \"../../constructs/Metadata.js\";",
            "{\n  ServiceMetadata,\n  SlsNextjsMetadata,\n  SSRSiteMetadata,\n  StaticSiteMetadata,\n}",
            "{\n  ServiceMetadata,\n  SlsNextjsMetadata,\n  SSRSiteMetadata,\n  StaticSiteMetadata,\n}",
            "ServiceMetadata",
            "SlsNextjsMetadata",
            "SSRSiteMetadata",
            "StaticSiteMetadata"
        ],
        "reference_api": [
            "import",
            "useAWSCredentials"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "import",
                "code": "p"
            },
            {
                "name": "useAWSCredentials",
                "code": "p"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "kubernetes-sigs/kubespray",
        "function_declaration": "def parse_args(self)",
        "start_line": "19",
        "end_line": "31",
        "file_path": "contrib/aws_inventory/kubespray-aws-inventory.py",
        "docstring": "The parse_args function checks if the environment variable VPC_VISIBILITY is set and assigns its value to self.vpc_visibility, defaulting to \"private\" if not set.\\nIt then creates an argument parser to support --list and --host flags, where --list lists instances and --host retrieves variables for a specific instance.\\nThe parsed arguments are stored in self.args.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "18eb42e5cab0",
        "ground_truth": "def parse_args(self):\n  ##Check if VPC_VISIBILITY is set, if not default to private\n  if \"VPC_VISIBILITY\" in os.environ:\n    self.vpc_visibility = os.environ['VPC_VISIBILITY']\n  else:\n    self.vpc_visibility = \"private\"\n  ##Support --list and --host flags. We largely ignore the host one.\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--list', action='store_true', default=False, help='List instances')\n  parser.add_argument('--host', action='store_true', help='Get all the variables about a specific instance')\n  self.args = parser.parse_args()",
        "import_statements": [
            "import boto3",
            "import os",
            "import argparse",
            "import json"
        ],
        "reference_api": [
            "parser.add_argument",
            "argparse.ArgumentParser",
            "parser.parse_args"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "kubernetes-sigs/kubespray",
        "function_declaration": "def purge_invalid_hosts(self, hostnames, protected_names=[])",
        "start_line": "311",
        "end_line": "326",
        "file_path": "contrib/inventory_builder/inventory.py",
        "docstring": "The purge_invalid_hosts function removes invalid host entries from a configuration.\\nIt iterates through roles in the yaml_config, excluding the 'k8s_cluster' role, and deletes hosts not present in the provided hostnames or protected_names lists.\\nIt also removes invalid hosts from the top-level 'all' category in the yaml_config.\\nThe function uses the debug method to log the removal of hosts.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "70450af1157e",
        "ground_truth": "def purge_invalid_hosts(self, hostnames, protected_names=[]):\n    for role in self.yaml_config['all']['children']:\n        if role != 'k8s_cluster' and self.yaml_config['all']['children'][role]['hosts']:  # noqa\n            all_hosts = self.yaml_config['all']['children'][role]['hosts'].copy()  # noqa\n            for host in all_hosts.keys():\n                if host not in hostnames and host not in protected_names:\n                    self.debug(\n                        \"Host {0} removed from role {1}\".format(host, role))  # noqa\n                    del self.yaml_config['all']['children'][role]['hosts'][host]  # noqa\n    # purge from all\n    if self.yaml_config['all']['hosts']:\n        all_hosts = self.yaml_config['all']['hosts'].copy()\n        for host in all_hosts.keys():\n            if host not in hostnames and host not in protected_names:\n                self.debug(\"Host {0} removed from role all\".format(host))\n                del self.yaml_config['all']['hosts'][host]",
        "import_statements": [
            "from collections import OrderedDict",
            "from ipaddress import ip_address",
            "from ruamel.yaml import YAML",
            "import os",
            "import re",
            "import subprocess",
            "import sys"
        ],
        "reference_api": [
            "all_hosts.keys",
            "self.debug",
            "format",
            "copy"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.debug",
                "code": "def debug(self, msg):\n        if DEBUG:\n            print(\"DEBUG: {0}\".format(msg))"
            },
            {
                "name": "self.debug",
                "code": "def debug(self, msg):\n        if DEBUG:\n            print(\"DEBUG: {0}\".format(msg))"
            }
        ],
        "third_party": [
            "all_hosts.keys",
            "all_hosts.keys"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "kubernetes-sigs/kubespray",
        "function_declaration": "def add_host_to_group(self, group, host, opts=\"\")",
        "start_line": "328",
        "end_line": "339",
        "file_path": "contrib/inventory_builder/inventory.py",
        "docstring": "The add_host_to_group function adds a host to a specified group within a YAML configuration.\\nIt logs the action, checks if the group is 'all' or not, and updates the YAML configuration accordingly.\\nFor the 'all' group, it ensures the hosts field is not None and then sets the host with optional settings.\\nFor other groups, except 'k8s_cluster:children', it similarly ensures the group's hosts field is not None and then adds the host.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b8eb5d61eca5",
        "ground_truth": "def add_host_to_group(self, group, host, opts=\"\"):\n    self.debug(\"adding host {0} to group {1}\".format(host, group))\n    if group == 'all':\n        if self.yaml_config['all']['hosts'] is None:\n            self.yaml_config['all']['hosts'] = {host: None}\n        self.yaml_config['all']['hosts'][host] = opts\n    elif group != 'k8s_cluster:children':\n        if self.yaml_config['all']['children'][group]['hosts'] is None:\n            self.yaml_config['all']['children'][group]['hosts'] = {\n                host: None}\n        else:\n            self.yaml_config['all']['children'][group]['hosts'][host] = None  # noqa",
        "import_statements": [
            "from collections import OrderedDict",
            "from ipaddress import ip_address",
            "from ruamel.yaml import YAML",
            "import os",
            "import re",
            "import subprocess",
            "import sys"
        ],
        "reference_api": [
            "self.debug",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.debug",
                "code": "def debug(self, msg):\n        if DEBUG:\n            print(\"DEBUG: {0}\".format(msg))"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "AWS",
        "owner/repo": "kubernetes-sigs/kubespray",
        "function_declaration": "def load_file(self, files=None)",
        "start_line": "387",
        "end_line": "413",
        "file_path": "contrib/inventory_builder/inventory.py",
        "docstring": "The load_file function loads JSON data into an inventory.\\nIt requires a list of files as input and raises an exception if no files are specified.\\nIt attempts to read each file as JSON and raises an exception if the file cannot be read.\\nIt ensures the presence of required groups, sets the Kubernetes cluster, and processes each group and host in the JSON data.\\nIt adds each host to the 'all' group and its specific group with necessary options and writes the configuration to a file.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cdb2e6478f7d",
        "ground_truth": "def load_file(self, files=None):\n    '''Directly loads JSON to inventory.'''\n    if not files:\n        raise Exception(\"No input file specified.\")\n    import json\n    for filename in list(files):\n        # Try JSON\n        try:\n            with open(filename, 'r') as f:\n                data = json.load(f)\n        except ValueError:\n            raise Exception(\"Cannot read %s as JSON, or CSV\", filename)\n        self.ensure_required_groups(ROLES)\n        self.set_k8s_cluster()\n        for group, hosts in data.items():\n            self.ensure_required_groups([group])\n            for host, opts in hosts.items():\n                optstring = {'ansible_host': opts['ip'],\n                             'ip': opts['ip'],\n                             'access_ip': opts['ip']}\n                self.add_host_to_group('all', host, optstring)\n                self.add_host_to_group(group, host)\n        self.write_config(self.config_file)",
        "import_statements": [
            "from collections import OrderedDict",
            "from ipaddress import ip_address",
            "from ruamel.yaml import YAML",
            "import os",
            "import re",
            "import subprocess",
            "import sys"
        ],
        "reference_api": [
            "list",
            "self.write_config",
            "self.add_host_to_group",
            "self.ensure_required_groups",
            "hosts.items",
            "data.items",
            "Exception",
            "self.set_k8s_cluster",
            "open",
            "json.load"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.ensure_required_groups",
                "code": "def ensure_required_groups(self, groups):\n        for group in groups:\n            if group == 'all':\n                self.debug(\"Adding group {0}\".format(group))\n                if group not in self.yaml_config:\n                    all_dict = OrderedDict([('hosts', OrderedDict({})),\n                                            ('children', OrderedDict({}))])\n                    self.yaml_config = {'all': all_dict}\n            else:\n                self.debug(\"Adding group {0}\".format(group))\n                if group not in self.yaml_config['all']['children']:\n                    self.yaml_config['all']['children'][group] = {'hosts': {}}"
            },
            {
                "name": "self.set_k8s_cluster",
                "code": "def set_k8s_cluster(self):\n        k8s_cluster = {'children': {'kube_control_plane': None,\n                                    'kube_node': None}}\n        self.yaml_config['all']['children']['k8s_cluster'] = k8s_cluster"
            },
            {
                "name": "self.ensure_required_groups",
                "code": "def ensure_required_groups(self, groups):\n        for group in groups:\n            if group == 'all':\n                self.debug(\"Adding group {0}\".format(group))\n                if group not in self.yaml_config:\n                    all_dict = OrderedDict([('hosts', OrderedDict({})),\n                                            ('children', OrderedDict({}))])\n                    self.yaml_config = {'all': all_dict}\n            else:\n                self.debug(\"Adding group {0}\".format(group))\n                if group not in self.yaml_config['all']['children']:\n                    self.yaml_config['all']['children'][group] = {'hosts': {}}"
            },
            {
                "name": "self.add_host_to_group",
                "code": "def add_host_to_group(self, group, host, opts=\"\"):\n        self.debug(\"adding host {0} to group {1}\".format(host, group))\n        if group == 'all':\n            if self.yaml_config['all']['hosts'] is None:\n                self.yaml_config['all']['hosts'] = {host: None}\n            self.yaml_config['all']['hosts'][host] = opts\n        elif group != 'k8s_cluster:children':\n            if self.yaml_config['all']['children'][group]['hosts'] is None:\n                self.yaml_config['all']['children'][group]['hosts'] = {\n                    host: None}\n            else:\n                self.yaml_config['all']['children'][group]['hosts'][host] = None  # noqa"
            },
            {
                "name": "self.add_host_to_group",
                "code": "def add_host_to_group(self, group, host, opts=\"\"):\n        self.debug(\"adding host {0} to group {1}\".format(host, group))\n        if group == 'all':\n            if self.yaml_config['all']['hosts'] is None:\n                self.yaml_config['all']['hosts'] = {host: None}\n            self.yaml_config['all']['hosts'][host] = opts\n        elif group != 'k8s_cluster:children':\n            if self.yaml_config['all']['children'][group]['hosts'] is None:\n                self.yaml_config['all']['children'][group]['hosts'] = {\n                    host: None}\n            else:\n                self.yaml_config['all']['children'][group]['hosts'][host] = None  # noqa"
            },
            {
                "name": "self.write_config",
                "code": "def write_config(self, config_file):\n        if config_file:\n            with open(self.config_file, 'w') as f:\n                yaml.dump(self.yaml_config, f)\n\n        else:\n            print(\"WARNING: Unable to save config. Make sure you set \"\n                  \"CONFIG_FILE env var.\")"
            }
        ],
        "third_party": [
            "data.items",
            "hosts.items"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "kubernetes-sigs/kubespray",
        "function_declaration": "def convert_to_v3_structure(attributes, prefix='')",
        "start_line": "41",
        "end_line": "60",
        "file_path": "contrib/terraform/terraform.py",
        "docstring": "The function convert_to_v3_structure converts attributes from a v4 structure to a v3 structure.\\nIt receives a dictionary and returns a transformed dictionary.\\nIf the input is a string, it returns a dictionary with a randomly generated key prefixed by the given prefix.\\nFor lists, it adds an entry with the list length and recursively converts each element.\\nFor dictionaries, it adds an entry with the dictionary length and converts each key-value pair.\\nOther values are directly added to the result with the given prefix.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6fc8e1e9b377",
        "ground_truth": "def convert_to_v3_structure(attributes, prefix=''):\n    \"\"\" Convert the attributes from v4 to v3\n    Receives a dict and return a dictionary \"\"\"\n    result = {}\n    if isinstance(attributes, str):\n        # In the case when we receive a string (e.g. values for security_groups)\n        return {'{}{}'.format(prefix, random.randint(1,10**10)): attributes}\n    for key, value in attributes.items():\n        if isinstance(value, list):\n            if len(value):\n                result['{}{}.#'.format(prefix, key, hash)] = len(value)\n            for i, v in enumerate(value):\n                result.update(convert_to_v3_structure(v, '{}{}.{}.'.format(prefix, key, i)))\n        elif isinstance(value, dict):\n            result['{}{}.%'.format(prefix, key)] = len(value)\n            for k, v in value.items():\n                result['{}{}.{}'.format(prefix, key, k)] = v\n        else:\n            result['{}{}'.format(prefix, key)] = value\n    return result",
        "import_statements": [
            "import argparse",
            "from collections import defaultdict",
            "import random",
            "from functools import wraps",
            "import json",
            "import os",
            "import re"
        ],
        "reference_api": [
            "result.update",
            "len",
            "isinstance",
            "attributes.items",
            "convert_to_v3_structure",
            "random.randint",
            "enumerate",
            "format",
            "value.items"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "convert_to_v3_structure",
                "code": "def convert_to_v3_structure(attributes, prefix=''):\n    \"\"\" Convert the attributes from v4 to v3\n    Receives a dict and return a dictionary \"\"\"\n    result = {}\n    if isinstance(attributes, str):\n        # In the case when we receive a string (e.g. values for security_groups)\n        return {'{}{}'.format(prefix, random.randint(1,10**10)): attributes}\n    for key, value in attributes.items():\n        if isinstance(value, list):\n            if len(value):\n                result['{}{}.#'.format(prefix, key, hash)] = len(value)\n            for i, v in enumerate(value):\n                result.update(convert_to_v3_structure(v, '{}{}.{}.'.format(prefix, key, i)))\n        elif isinstance(value, dict):\n            result['{}{}.%'.format(prefix, key)] = len(value)\n            for k, v in value.items():\n                result['{}{}.{}'.format(prefix, key, k)] = v\n        else:\n            result['{}{}'.format(prefix, key)] = value\n    return result"
            }
        ],
        "third_party": [
            "attributes.items",
            "result.update",
            "value.items"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "kubernetes-sigs/kubespray",
        "function_declaration": "def iterhosts(resources)",
        "start_line": "104",
        "end_line": "113",
        "file_path": "contrib/terraform/terraform.py",
        "docstring": "The iterhosts function generates tuples containing host information.\\nIt takes a list of resources and iterates through each resource.\\nFor each resource, it splits the key to obtain the resource type and name.\\nIt then attempts to retrieve a parser for the resource type from the PARSERS dictionary.\\nIf a parser is found, it yields the result of parsing the resource along with the module name.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4c2889e90aa8",
        "ground_truth": "def iterhosts(resources):\n    '''yield host tuples of (name, attributes, groups)'''\n    for module_name, key, resource in resources:\n        resource_type, name = key.split('.', 1)\n        try:\n            parser = PARSERS[resource_type]\n        except KeyError:\n            continue\n         yield parser(resource, module_name)",
        "import_statements": [
            "import argparse",
            "from collections import defaultdict",
            "import random",
            "from functools import wraps",
            "import json",
            "import os",
            "import re"
        ],
        "reference_api": [
            "parser",
            "key.split"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "key.split"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "kubernetes-sigs/kubespray",
        "function_declaration": "def query_list(hosts)",
        "start_line": "386",
        "end_line": "402",
        "file_path": "contrib/terraform/terraform.py",
        "docstring": "The query_list function processes a list of hosts to organize them into groups for Ansible inventory.\\nIt initializes dictionaries for groups and metadata.\\nFor each host, it iterates through its hostgroups, setting default group names to \"all\" if empty.\\nIt appends host names to the appropriate groups and adds their attributes to metadata.\\nFinally, it adds metadata to the '_meta' group and returns the grouped hosts.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2bb904cdf641",
        "ground_truth": "def query_list(hosts):\n    groups = defaultdict(dict)\n    meta = {}\n     for name, attrs, hostgroups in hosts:\n        for group in set(hostgroups):\n            # Ansible 2.6.2 stopped supporting empty group names: https://github.com/ansible/ansible/pull/42584/commits/d4cd474b42ed23d8f8aabb2a7f84699673852eaf\n            # Empty group name defaults to \"all\" in Ansible < 2.6.2 so we alter empty group names to \"all\"\n            if not group: group = \"all\"\n             groups[group].setdefault('hosts', [])\n            groups[group]['hosts'].append(name)\n         meta[name] = attrs\n     groups['_meta'] = {'hostvars': meta}\n    return groups",
        "import_statements": [
            "import argparse",
            "from collections import defaultdict",
            "import random",
            "from functools import wraps",
            "import json",
            "import os",
            "import re"
        ],
        "reference_api": [
            "append",
            "defaultdict",
            "setdefault",
            "set"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "defaultdict",
            "setdefault",
            "append"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def add_tag(self, tag, attrs=None, is_start=True)",
        "start_line": "66",
        "end_line": "79",
        "file_path": "awscli/bcdoc/docstringparser.py",
        "docstring": "The add_tag function handles the addition of tags to a document tree.\\nIf the document does not have a handler for the tag, it appends the tag to unhandled_tags and returns.\\nIf it is a start tag, it creates a new LineItemNode for 'li' tags or a TagNode for other tags, then adds it as a child to the current node and updates the current node to this new node.\\nFor end tags, it sets the current node to its parent node.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b3cb072523d2",
        "ground_truth": "def add_tag(self, tag, attrs=None, is_start=True):\n    if not self._doc_has_handler(tag, is_start):\n        self.unhandled_tags.append(tag)\n        return\n    if is_start:\n        if tag == 'li':\n            node = LineItemNode(attrs)\n        else:\n            node = TagNode(tag, attrs)\n        self.current_node.add_child(node)\n        self.current_node = node\n    else:\n        self.current_node = self.current_node.parent",
        "import_statements": [
            "from html.parser import HTMLParser"
        ],
        "reference_api": [
            "LineItemNode",
            "TagNode",
            "append",
            "add_child",
            "self._doc_has_handler"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._doc_has_handler",
                "code": "def _doc_has_handler(self, tag, is_start):\n        if is_start:\n            handler_name = 'start_%s' % tag\n        else:\n            handler_name = 'end_%s' % tag\n\n        return hasattr(self.doc.style, handler_name)"
            },
            {
                "name": "add_child",
                "code": "def add_child(self, child):\n        child.parent = self\n        self.children.append(child)"
            }
        ],
        "third_party": [
            "append",
            "LineItemNode",
            "TagNode"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def add_new_section(self, name, context=None)",
        "start_line": "164",
        "end_line": "188",
        "file_path": "awscli/bcdoc/restdoc.py",
        "docstring": "The add_new_section function creates and adds a new section to the current structure.\\nIt initializes the new section with the given name, target, and context, and sets its path based on the parent section.\\nThe new section inherits indentation style, translation map, and hrefs from the parent section.\\nFinally, it adds the new section to the parent's structure and returns it.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "23e9adb76863",
        "ground_truth": "def add_new_section(self, name, context=None):\n    \"\"\"Adds a new section to the current document structure\n    This document structure will be considered a section to the\n    current document structure but will in itself be an entirely\n    new document structure that can be written to and have sections\n    as well\n    :param name: The name of the section.\n    :param context: A dictionary of data to store with the strucuture. These\n        are only stored per section not the entire structure.\n    :rtype: DocumentStructure\n    :returns: A new document structure to add to but lives as a section\n        to the document structure it was instantiated from.\n    \"\"\"\n    # Add a new section\n    section = self.__class__(name=name, target=self.target,\n                             context=context)\n    section.path = self.path + [name]\n    # Indent the section apporpriately as well\n    section.style.indentation = self.style.indentation\n    section.translation_map = self.translation_map\n    section.hrefs = self.hrefs\n    self._structure[name] = section\n    return section",
        "import_statements": [
            "import logging",
            "from botocore.compat import OrderedDict",
            "from awscli.bcdoc.docstringparser import DocStringParser",
            "from awscli.bcdoc.style import ReSTStyle"
        ],
        "reference_api": [
            "self.__class__"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self.__class__"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def flush_structure(self)",
        "start_line": "198",
        "end_line": "214",
        "file_path": "awscli/bcdoc/restdoc.py",
        "docstring": "The flush_structure function processes and returns the structure of a document.\\nIf at the root (path length of 1), it begins by flushing links at the beginning of the document, creating new paragraphs and defining link targets for each href.\\nIt then retrieves the current value of the document and iterates through the sections, appending their flushed structures to the value.\\nFinally, it returns the accumulated value.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f67ae046ac3c",
        "ground_truth": "def flush_structure(self):\n    \"\"\"Flushes a doc structure to a ReSTructed string\n    The document is flushed out in a DFS style where sections and their\n    subsections' values are added to the string as they are visited.\n    \"\"\"\n    # We are at the root flush the links at the beginning of the\n    # document\n    if len(self.path) == 1:\n        if self.hrefs:\n            self.style.new_paragraph()\n            for refname, link in self.hrefs.items():\n                self.style.link_target_definition(refname, link)\n    value = self.getvalue()\n    for name, section in self._structure.items():\n        value += section.flush_structure()\n    return value",
        "import_statements": [
            "import logging",
            "from botocore.compat import OrderedDict",
            "from awscli.bcdoc.docstringparser import DocStringParser",
            "from awscli.bcdoc.style import ReSTStyle"
        ],
        "reference_api": [
            "self.getvalue",
            "len",
            "new_paragraph",
            "link_target_definition",
            "items",
            "section.flush_structure"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "new_paragraph",
                "code": "def new_paragraph(self):\n        return '\\n%s' % self.spaces()"
            },
            {
                "name": "link_target_definition",
                "code": "def link_target_definition(self, refname, link):\n        self.doc.writeln('.. _%s: %s' % (refname, link))"
            },
            {
                "name": "self.getvalue",
                "code": "def getvalue(self):\n        \"\"\"\n        Returns the current content of the document as a string.\n        \"\"\"\n        if self.hrefs:\n            self.style.new_paragraph()\n            for refname, link in self.hrefs.items():\n                self.style.link_target_definition(refname, link)\n        return ''.join(self._writes).encode('utf-8')"
            },
            {
                "name": "section.flush_structure",
                "code": "def flush_structure(self):\n        \"\"\"Flushes a doc structure to a ReSTructed string\n\n        The document is flushed out in a DFS style where sections and their\n        subsections' values are added to the string as they are visited.\n        \"\"\"\n        # We are at the root flush the links at the beginning of the\n        # document\n        if len(self.path) == 1:\n            if self.hrefs:\n                self.style.new_paragraph()\n                for refname, link in self.hrefs.items():\n                    self.style.link_target_definition(refname, link)\n        value = self.getvalue()\n        for name, section in self._structure.items():\n            value += section.flush_structure()\n        return value"
            }
        ],
        "third_party": [
            "items",
            "items"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def zip_folder(folder_path)",
        "start_line": "162",
        "end_line": "179",
        "file_path": "awscli/customizations/cloudformation/artifact_exporter.py",
        "docstring": "The zip_folder function creates a zip file from a specified folder path.\\nIt generates a unique filename using a temporary directory and a UUID.\\nThe make_zip function is called to create the zip file with the generated filename and folder path.\\nThe function yields the zipfile name for use and ensures the zip file is removed after processing if it still exists.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "62364f6fdf67",
        "ground_truth": "def zip_folder(folder_path):\n    \"\"\"\n    Zip the entire folder and return a file to the zip. Use this inside\n    a \"with\" statement to cleanup the zipfile after it is used.\n     :param folder_path:\n    :return: Name of the zipfile\n    \"\"\"\n     filename = os.path.join(\n        tempfile.gettempdir(), \"data-\" + uuid.uuid4().hex)\n     zipfile_name = make_zip(filename, folder_path)\n    try:\n        yield zipfile_name\n    finally:\n        if os.path.exists(zipfile_name):\n            os.remove(zipfile_name)",
        "import_statements": [
            "import logging",
            "import os",
            "import tempfile",
            "import zipfile",
            "import contextlib",
            "import uuid",
            "import shutil",
            "from botocore.utils import set_value_from_jmespath",
            "from awscli.compat import urlparse",
            "from contextlib import contextmanager",
            "from awscli.customizations.cloudformation import exceptions",
            "from awscli.customizations.cloudformation.yamlhelper import yaml_dump, \\\n    yaml_parse",
            "import jmespath"
        ],
        "reference_api": [
            "join",
            "exists",
            "make_zip",
            "tempfile.gettempdir",
            "os.remove",
            "uuid.uuid4"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "make_zip",
                "code": "def make_zip(filename, source_root):\n    zipfile_name = \"{0}.zip\".format(filename)\n    source_root = os.path.abspath(source_root)\n    with open(zipfile_name, 'wb') as f:\n        zip_file = zipfile.ZipFile(f, 'w', zipfile.ZIP_DEFLATED)\n        with contextlib.closing(zip_file) as zf:\n            for root, dirs, files in os.walk(source_root, followlinks=True):\n                for filename in files:\n                    full_path = os.path.join(root, filename)\n                    relative_path = os.path.relpath(\n                        full_path, source_root)\n                    zf.write(full_path, relative_path)\n\n    return zipfile_name"
            }
        ],
        "third_party": [
            "join",
            "exists"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def deploy(self, deployer, stack_name, template_str,\n               parameters, capabilities, execute_changeset, role_arn,\n               notification_arns, s3_uploader, tags,\n               fail_on_empty_changeset=True, disable_rollback=False)",
        "start_line": "321",
        "end_line": "353",
        "file_path": "awscli/customizations/cloudformation/deploy.py",
        "docstring": "The deploy function orchestrates the deployment of an AWS CloudFormation stack using the provided deployer object.\\nIt starts by creating and waiting for a changeset using the stack name, CloudFormation template, parameters, capabilities, role ARN, notification ARNs, S3 uploader, and tags.\\nIf the changeset is empty and fail_on_empty_changeset is True, it raises an exception; otherwise, it logs the exception and returns 0.\\nIf execute_changeset is True, it executes the changeset, waits for execution, and logs success; otherwise, it logs that no changeset was executed.\\nFinally, it flushes the stdout buffer and returns 0.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fe104a977a22",
        "ground_truth": "def deploy(self, deployer, stack_name, template_str,\n           parameters, capabilities, execute_changeset, role_arn,\n           notification_arns, s3_uploader, tags,\n           fail_on_empty_changeset=True, disable_rollback=False):\n    try:\n        result = deployer.create_and_wait_for_changeset(\n            stack_name=stack_name,\n            cfn_template=template_str,\n            parameter_values=parameters,\n            capabilities=capabilities,\n            role_arn=role_arn,\n            notification_arns=notification_arns,\n            s3_uploader=s3_uploader,\n            tags=tags\n        )\n    except exceptions.ChangeEmptyError as ex:\n        if fail_on_empty_changeset:\n            raise\n        write_exception(ex, outfile=get_stdout_text_writer())\n        return 0\n    if execute_changeset:\n        deployer.execute_changeset(result.changeset_id, stack_name,\n                                   disable_rollback)\n        deployer.wait_for_execute(stack_name, result.changeset_type)\n        sys.stdout.write(self.MSG_EXECUTE_SUCCESS.format(\n                stack_name=stack_name))\n    else:\n        sys.stdout.write(self.MSG_NO_EXECUTE_CHANGESET.format(\n                changeset_id=result.changeset_id))\n    sys.stdout.flush()\n    return 0",
        "import_statements": [
            "import os",
            "import sys",
            "import logging",
            "from botocore.client import Config",
            "from awscli.customizations.cloudformation import exceptions",
            "from awscli.customizations.cloudformation.deployer import Deployer",
            "from awscli.customizations.s3uploader import S3Uploader",
            "from awscli.customizations.cloudformation.yamlhelper import yaml_parse",
            "from awscli.customizations.commands import BasicCommand",
            "from awscli.compat import get_stdout_text_writer",
            "from awscli.utils import write_exception"
        ],
        "reference_api": [
            "write_exception",
            "write",
            "deployer.create_and_wait_for_changeset",
            "get_stdout_text_writer",
            "deployer.wait_for_execute",
            "flush",
            "deployer.execute_changeset",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "deployer.create_and_wait_for_changeset",
                "code": "def create_and_wait_for_changeset(self, stack_name, cfn_template,\n                                      parameter_values, capabilities, role_arn,\n                                      notification_arns, s3_uploader, tags):\n\n        result = self.create_changeset(\n                stack_name, cfn_template, parameter_values, capabilities,\n                role_arn, notification_arns, s3_uploader, tags)\n        self.wait_for_changeset(result.changeset_id, stack_name)\n\n        return result"
            },
            {
                "name": "write_exception",
                "code": "def write_exception(ex, outfile):\n    outfile.write(\"\\n\")\n    outfile.write(str(ex))\n    outfile.write(\"\\n\")"
            },
            {
                "name": "get_stdout_text_writer",
                "code": "def get_stdout_text_writer():\n    return _get_text_writer(sys.stdout, errors=\"strict\")"
            },
            {
                "name": "deployer.execute_changeset",
                "code": "def execute_changeset(self, changeset_id, stack_name,\n                          disable_rollback=False):\n        \"\"\"\n        Calls CloudFormation to execute changeset\n\n        :param changeset_id: ID of the changeset\n        :param stack_name: Name or ID of the stack\n        :param disable_rollback: Disable rollback of all resource changes\n        :return: Response from execute-change-set call\n        \"\"\"\n        return self._client.execute_change_set(\n                ChangeSetName=changeset_id,\n                StackName=stack_name,\n                DisableRollback=disable_rollback)"
            },
            {
                "name": "deployer.wait_for_execute",
                "code": "def wait_for_execute(self, stack_name, changeset_type):\n\n        sys.stdout.write(\"Waiting for stack create/update to complete\\n\")\n        sys.stdout.flush()\n\n        # Pick the right waiter\n        if changeset_type == \"CREATE\":\n            waiter = self._client.get_waiter(\"stack_create_complete\")\n        elif changeset_type == \"UPDATE\":\n            waiter = self._client.get_waiter(\"stack_update_complete\")\n        else:\n            raise RuntimeError(\"Invalid changeset type {0}\"\n                               .format(changeset_type))\n\n        # Poll every 30 seconds. Polling too frequently risks hitting rate limits\n        # on CloudFormation's DescribeStacks API\n        waiter_config = {\n            'Delay': 30,\n            'MaxAttempts': 120,\n        }\n\n        try:\n            waiter.wait(StackName=stack_name, WaiterConfig=waiter_config)\n        except botocore.exceptions.WaiterError as ex:\n            LOG.debug(\"Execute changeset waiter exception\", exc_info=ex)\n\n            raise exceptions.DeployFailedError(stack_name=stack_name)"
            }
        ],
        "third_party": [
            "write",
            "write",
            "flush"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def wait_for_changeset(self, changeset_id, stack_name)",
        "start_line": "146",
        "end_line": "178",
        "file_path": "awscli/customizations/cloudformation/deployer.py",
        "docstring": "The wait_for_changeset function monitors the creation of a CloudFormation changeset.\\nIt writes a message to stdout and configures a waiter to poll the changeset status every 5 seconds.\\nIf the changeset creation fails, it logs the error details and raises a ChangeEmptyError if there are no changes to be performed.\\nOtherwise, it raises a RuntimeError with the failure details.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e8c7a14f7502",
        "ground_truth": "def wait_for_changeset(self, changeset_id, stack_name):\n    \"\"\"\n    Waits until the changeset creation completes\n    :param changeset_id: ID or name of the changeset\n    :param stack_name:   Stack name\n    :return: Latest status of the create-change-set operation\n    \"\"\"\n    sys.stdout.write(\"\\nWaiting for changeset to be created..\\n\")\n    sys.stdout.flush()\n    # Wait for changeset to be created\n    waiter = self._client.get_waiter(\"change_set_create_complete\")\n    # Poll every 5 seconds. Changeset creation should be fast\n    waiter_config = {'Delay': 5}\n    try:\n        waiter.wait(ChangeSetName=changeset_id, StackName=stack_name,\n                    WaiterConfig=waiter_config)\n    except botocore.exceptions.WaiterError as ex:\n        LOG.debug(\"Create changeset waiter exception\", exc_info=ex)\n        resp = ex.last_response\n        status = resp[\"Status\"]\n        reason = resp[\"StatusReason\"]\n        if status == \"FAILED\" and \\\n           \"The submitted information didn't contain changes.\" in reason or \\\n                        \"No updates are to be performed\" in reason:\n                raise exceptions.ChangeEmptyError(stack_name=stack_name)\n        raise RuntimeError(\"Failed to create the changeset: {0} \"\n                           \"Status: {1}. Reason: {2}\"\n                           .format(ex, status, reason))",
        "import_statements": [
            "import sys",
            "import time",
            "import logging",
            "import botocore",
            "import collections",
            "from awscli.customizations.cloudformation import exceptions",
            "from awscli.customizations.cloudformation.artifact_exporter import mktempfile, parse_s3_url",
            "from datetime import datetime"
        ],
        "reference_api": [
            "RuntimeError",
            "write",
            "flush",
            "LOG.debug",
            "waiter.wait",
            "get_waiter",
            "exceptions.ChangeEmptyError",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "write",
            "flush",
            "get_waiter",
            "waiter.wait",
            "LOG.debug",
            "exceptions.ChangeEmptyError"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def wait_for_execute(self, stack_name, changeset_type)",
        "start_line": "195",
        "end_line": "221",
        "file_path": "awscli/customizations/cloudformation/deployer.py",
        "docstring": "The wait_for_execute function waits for a CloudFormation stack operation to complete.\\nIt outputs a waiting message to the console.\\nDepending on the changeset type (CREATE or UPDATE), it selects the appropriate waiter from the CloudFormation client.\\nIt configures the waiter to poll every 30 seconds, with a maximum of 120 attempts to avoid hitting rate limits.\\nThe function waits for the stack operation to complete and handles any exceptions by logging the error and raising a DeployFailedError.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6b26f10c65a1",
        "ground_truth": "def wait_for_execute(self, stack_name, changeset_type):\n    sys.stdout.write(\"Waiting for stack create/update to complete\\n\")\n    sys.stdout.flush()\n    # Pick the right waiter\n    if changeset_type == \"CREATE\":\n        waiter = self._client.get_waiter(\"stack_create_complete\")\n    elif changeset_type == \"UPDATE\":\n        waiter = self._client.get_waiter(\"stack_update_complete\")\n    else:\n        raise RuntimeError(\"Invalid changeset type {0}\"\n                           .format(changeset_type))\n    # Poll every 30 seconds. Polling too frequently risks hitting rate limits\n    # on CloudFormation's DescribeStacks API\n    waiter_config = {\n        'Delay': 30,\n        'MaxAttempts': 120,\n    }\n    try:\n        waiter.wait(StackName=stack_name, WaiterConfig=waiter_config)\n    except botocore.exceptions.WaiterError as ex:\n        LOG.debug(\"Execute changeset waiter exception\", exc_info=ex)\n        raise exceptions.DeployFailedError(stack_name=stack_name)",
        "import_statements": [
            "import sys",
            "import time",
            "import logging",
            "import botocore",
            "import collections",
            "from awscli.customizations.cloudformation import exceptions",
            "from awscli.customizations.cloudformation.artifact_exporter import mktempfile, parse_s3_url",
            "from datetime import datetime"
        ],
        "reference_api": [
            "RuntimeError",
            "exceptions.DeployFailedError",
            "write",
            "flush",
            "LOG.debug",
            "waiter.wait",
            "get_waiter",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "write",
            "flush",
            "get_waiter",
            "get_waiter",
            "waiter.wait",
            "LOG.debug",
            "exceptions.DeployFailedError"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def yaml_dump(dict_to_dump)",
        "start_line": "61",
        "end_line": "72",
        "file_path": "awscli/customizations/cloudformation/yamlhelper.py",
        "docstring": "The yaml_dump function serializes a dictionary to a YAML-formatted string.\\nIt first adds a representer for OrderedDict to the custom FlattenAliasDumper.\\nThen, it uses yaml.dump to convert the dictionary to a YAML string with a block style format and returns the result.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1ffa9f89ef3c",
        "ground_truth": "def yaml_dump(dict_to_dump):\n    \"\"\"\n    Dumps the dictionary as a YAML document\n    :param dict_to_dump:\n    :return:\n    \"\"\"\n    FlattenAliasDumper.add_representer(OrderedDict, _dict_representer)\n    return yaml.dump(\n        dict_to_dump,\n        default_flow_style=False,\n        Dumper=FlattenAliasDumper,\n    )",
        "import_statements": [
            "from botocore.compat import json",
            "from botocore.compat import OrderedDict",
            "import yaml",
            "from yaml.resolver import ScalarNode, SequenceNode"
        ],
        "reference_api": [
            "yaml.dump",
            "FlattenAliasDumper.add_representer"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "FlattenAliasDumper.add_representer",
            "yaml.dump"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def _get_bucket_region(self, bucket_name)",
        "start_line": "176",
        "end_line": "183",
        "file_path": "awscli/customizations/cloudtrail/validation.py",
        "docstring": "The _get_bucket_region function retrieves the region of a specified S3 bucket.\\nIf the bucket's region is not already cached, it creates a client to fetch the bucket's location.\\nThe region is then extracted from the response, defaulting to 'us-east-1' if not specified.\\nThe region is cached for future requests and returned.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bd1b7e3d54cb",
        "ground_truth": "def _get_bucket_region(self, bucket_name):\n    \"\"\"Returns the region of a bucket\"\"\"\n    if bucket_name not in self._region_cache:\n        client = self._create_client(self._get_bucket_location_region)\n        result = client.get_bucket_location(Bucket=bucket_name)\n        region = result['LocationConstraint'] or 'us-east-1'\n        self._region_cache[bucket_name] = region\n    return self._region_cache[bucket_name]",
        "import_statements": [
            "import base64",
            "import binascii",
            "import json",
            "import hashlib",
            "import logging",
            "import re",
            "import sys",
            "import zlib",
            "from zlib import error as ZLibError",
            "from datetime import datetime, timedelta",
            "from dateutil import tz, parser",
            "from pyasn1.error import PyAsn1Error",
            "import rsa",
            "from awscli.customizations.cloudtrail.utils import get_trail_by_arn, \\\n    get_account_id_from_arn",
            "from awscli.customizations.commands import BasicCommand",
            "from botocore.exceptions import ClientError",
            "from awscli.schema import ParameterRequiredError"
        ],
        "reference_api": [
            "client.get_bucket_location",
            "self._create_client"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._create_client",
                "code": "def _create_client(self, region_name):\n        \"\"\"Creates an Amazon S3 client for the given region name\"\"\"\n        if region_name not in self._client_cache:\n            client = self._session.create_client('s3', region_name)\n            # Remove the CLI error event that prevents exceptions.\n            self._client_cache[region_name] = client\n        return self._client_cache[region_name]"
            }
        ],
        "third_party": [
            "client.get_bucket_location"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def _create_client(self, region_name)",
        "start_line": "185",
        "end_line": "191",
        "file_path": "awscli/customizations/cloudtrail/validation.py",
        "docstring": "The _create_client function creates an Amazon S3 client for a specified region.\\nIf the client for the given region does not exist in the cache, it creates a new S3 client using the session's create_client method and stores it in the cache.\\nIt then returns the cached client for the specified region.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "339e86755d20",
        "ground_truth": "def _create_client(self, region_name):\n    \"\"\"Creates an Amazon S3 client for the given region name\"\"\"\n    if region_name not in self._client_cache:\n        client = self._session.create_client('s3', region_name)\n        # Remove the CLI error event that prevents exceptions.\n        self._client_cache[region_name] = client\n    return self._client_cache[region_name]",
        "import_statements": [
            "import base64",
            "import binascii",
            "import json",
            "import hashlib",
            "import logging",
            "import re",
            "import sys",
            "import zlib",
            "from zlib import error as ZLibError",
            "from datetime import datetime, timedelta",
            "from dateutil import tz, parser",
            "from pyasn1.error import PyAsn1Error",
            "import rsa",
            "from awscli.customizations.cloudtrail.utils import get_trail_by_arn, \\\n    get_account_id_from_arn",
            "from awscli.customizations.commands import BasicCommand",
            "from botocore.exceptions import ClientError",
            "from awscli.schema import ParameterRequiredError"
        ],
        "reference_api": [
            "create_client"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "create_client"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def setup_services(self, parsed_globals)",
        "start_line": "734",
        "end_line": "747",
        "file_path": "awscli/customizations/cloudtrail/validation.py",
        "docstring": "The setup_services function configures AWS service clients using the provided global parameters.\\nIt sets the source region and initializes the S3ClientProvider with the session and region.\\nIt creates an 'organizations' client with the specified region and SSL verification settings.\\nIf an endpoint URL is provided, it includes it in the client arguments.\\nFinally, it creates a 'cloudtrail' client using the configured client arguments.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "585d253b8102",
        "ground_truth": "def setup_services(self, parsed_globals):\n    self._source_region = parsed_globals.region\n    # Use the the same region as the region of the CLI to get locations.\n    self.s3_client_provider = S3ClientProvider(\n        self._session, self._source_region)\n    client_args = {'region_name': parsed_globals.region,\n                   'verify': parsed_globals.verify_ssl}\n    self.organization_client = self._session.create_client(\n        'organizations', **client_args)\n    if parsed_globals.endpoint_url is not None:\n        client_args['endpoint_url'] = parsed_globals.endpoint_url\n    self.cloudtrail_client = self._session.create_client(\n        'cloudtrail', **client_args)",
        "import_statements": [
            "import base64",
            "import binascii",
            "import json",
            "import hashlib",
            "import logging",
            "import re",
            "import sys",
            "import zlib",
            "from zlib import error as ZLibError",
            "from datetime import datetime, timedelta",
            "from dateutil import tz, parser",
            "from pyasn1.error import PyAsn1Error",
            "import rsa",
            "from awscli.customizations.cloudtrail.utils import get_trail_by_arn, \\\n    get_account_id_from_arn",
            "from awscli.customizations.commands import BasicCommand",
            "from botocore.exceptions import ClientError",
            "from awscli.schema import ParameterRequiredError"
        ],
        "reference_api": [
            "create_client",
            "S3ClientProvider"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "S3ClientProvider",
            "create_client",
            "create_client"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def login(self, dry_run=False)",
        "start_line": "130",
        "end_line": "149",
        "file_path": "awscli/customizations/codeartifact/login.py",
        "docstring": "The login function handles user authentication for a repository.\\nIt retrieves the authentication scope and necessary commands using the repository endpoint and authentication token.\\nFor non-macOS systems, it constructs a .netrc entry with the repository hostname and authentication token.\\nIf in dry run mode, it displays the new .netrc entry; otherwise, it updates the .netrc file with the new entry.\\nFinally, it runs the specified commands using the Swift package manager, optionally in dry run mode.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "df6d47ae71dd",
        "ground_truth": "def login(self, dry_run=False):\n    scope = self.get_scope(\n        self.namespace\n    )\n    commands = self.get_commands(\n        self.repository_endpoint, self.auth_token, scope=scope\n    )\n    if not is_macos:\n        hostname = urlparse.urlparse(self.repository_endpoint).hostname\n        new_entry = self.DEFAULT_NETRC_FMT.format(\n            hostname=hostname,\n            auth_token=self.auth_token\n        )\n        if dry_run:\n            self._display_new_netrc_entry(new_entry, self.get_netrc_path())\n        else:\n            self._update_netrc_entry(hostname, new_entry, self.get_netrc_path())\n    self._run_commands('swift', commands, dry_run)",
        "import_statements": [
            "import errno",
            "import os",
            "import platform",
            "import sys",
            "import subprocess",
            "import re",
            "from datetime import datetime",
            "from dateutil.tz import tzutc",
            "from dateutil.relativedelta import relativedelta",
            "from botocore.utils import parse_timestamp",
            "from awscli.compat import (\n    is_windows, urlparse, RawConfigParser, StringIO,\n    get_stderr_encoding, is_macos\n)",
            "from awscli.customizations import utils as cli_utils",
            "from awscli.customizations.commands import BasicCommand",
            "from awscli.customizations.utils import uni_print"
        ],
        "reference_api": [
            "self.get_netrc_path",
            "self._update_netrc_entry",
            "self._run_commands",
            "urlparse.urlparse",
            "self._display_new_netrc_entry",
            "self.get_commands",
            "self.get_scope",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._display_new_netrc_entry",
                "code": "def _display_new_netrc_entry(self, new_entry, netrc_path):\n        sys.stdout.write('Dryrun mode is enabled, not writing to netrc.')\n        sys.stdout.write(os.linesep)\n        sys.stdout.write(\n            f'The following line would have been written to {netrc_path}:'\n        )\n        sys.stdout.write(os.linesep)\n        sys.stdout.write(os.linesep)\n        sys.stdout.write(new_entry)\n        sys.stdout.write(os.linesep)\n        sys.stdout.write(os.linesep)\n        sys.stdout.write('And would have run the following commands:')\n        sys.stdout.write(os.linesep)\n        sys.stdout.write(os.linesep)"
            },
            {
                "name": "self._update_netrc_entry",
                "code": "def _update_netrc_entry(self, hostname, new_entry, netrc_path):\n        pattern = re.compile(\n            self.NETRC_REGEX_FMT.format(escaped_hostname=re.escape(hostname)),\n            re.M\n        )\n        if not os.path.isfile(netrc_path):\n            self._create_netrc_file(netrc_path, new_entry)\n        else:\n            with open(netrc_path, 'r') as f:\n                contents = f.read()\n            escaped_auth_token = self.auth_token.replace('\\\\', r'\\\\')\n            new_contents = re.sub(\n                pattern,\n                rf\"\\g<entry_start>{escaped_auth_token}\",\n                contents\n            )\n\n            if new_contents == contents:\n                new_contents = self._append_netrc_entry(new_contents, new_entry)\n\n            with open(netrc_path, 'w') as f:\n                f.write(new_contents)"
            },
            {
                "name": "self._run_commands",
                "code": "def _run_commands(self, tool, commands, dry_run=False):\n        if dry_run:\n            self._dry_run_commands(tool, commands)\n            return\n\n        for command in commands:\n            self._run_command(tool, command)\n\n        self._write_success_message(tool)"
            }
        ],
        "third_party": [
            "self.get_scope",
            "self.get_commands",
            "urlparse.urlparse",
            "self.get_netrc_path",
            "self.get_netrc_path"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def _delete_user_policy(self, params)",
        "start_line": "125",
        "end_line": "139",
        "file_path": "awscli/customizations/codedeploy/deregister.py",
        "docstring": "The _delete_user_policy function deletes all IAM user policies for a specified user.\\nIt writes a message indicating the deletion process has started.\\nIt uses a paginator to list all user policies for the given username.\\nFor each policy, it deletes the policy using the IAM delete_user_policy method.\\nIf a ClientError occurs and the error is not 'NoSuchEntity', the exception is raised.\\nFinally, it writes a 'DONE' message indicating the completion of the process.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3c01f36e2748",
        "ground_truth": "def _delete_user_policy(self, params):\n    sys.stdout.write('Deleting the IAM user policies... ')\n    list_user_policies = self.iam.get_paginator('list_user_policies')\n    try:\n        for response in list_user_policies.paginate(\n                UserName=params.user_name):\n            for policy_name in response['PolicyNames']:\n                self.iam.delete_user_policy(\n                    UserName=params.user_name,\n                    PolicyName=policy_name\n                )\n    except ClientError as e:\n        if e.response.get('Error', {}).get('Code') != 'NoSuchEntity':\n            raise e\n    sys.stdout.write('DONE\\n')",
        "import_statements": [
            "import sys",
            "from botocore.exceptions import ClientError",
            "from awscli.customizations.commands import BasicCommand",
            "from awscli.customizations.codedeploy.utils import \\\n    validate_region, validate_instance_name, INSTANCE_NAME_ARG"
        ],
        "reference_api": [
            "write",
            "list_user_policies.paginate",
            "get",
            "delete_user_policy",
            "get_paginator"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "write",
            "get_paginator",
            "list_user_policies.paginate",
            "delete_user_policy",
            "get",
            "get",
            "write"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def _delete_access_key(self, params)",
        "start_line": "141",
        "end_line": "155",
        "file_path": "awscli/customizations/codedeploy/deregister.py",
        "docstring": "The _delete_access_key function deletes IAM user access keys for a specified user.\\nIt uses a paginator to list all access keys of the user and attempts to delete each one.\\nIf a ClientError occurs, it raises the exception unless the error code is 'NoSuchEntity'.\\nIt prints a message to indicate the progress and completion of the deletion process.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9c6e2f55ee26",
        "ground_truth": "def _delete_access_key(self, params):\n    sys.stdout.write('Deleting the IAM user access keys... ')\n    list_access_keys = self.iam.get_paginator('list_access_keys')\n    try:\n        for response in list_access_keys.paginate(\n                UserName=params.user_name):\n            for access_key in response['AccessKeyMetadata']:\n                self.iam.delete_access_key(\n                    UserName=params.user_name,\n                    AccessKeyId=access_key['AccessKeyId']\n                )\n    except ClientError as e:\n        if e.response.get('Error', {}).get('Code') != 'NoSuchEntity':\n            raise e\n    sys.stdout.write('DONE\\n')",
        "import_statements": [
            "import sys",
            "from botocore.exceptions import ClientError",
            "from awscli.customizations.commands import BasicCommand",
            "from awscli.customizations.codedeploy.utils import \\\n    validate_region, validate_instance_name, INSTANCE_NAME_ARG"
        ],
        "reference_api": [
            "write",
            "list_access_keys.paginate",
            "get",
            "delete_access_key",
            "get_paginator"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "write",
            "get_paginator",
            "list_access_keys.paginate",
            "delete_access_key",
            "get",
            "get",
            "write"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def _create_access_key(self, params)",
        "start_line": "131",
        "end_line": "145",
        "file_path": "awscli/customizations/codedeploy/register.py",
        "docstring": "The _create_access_key function generates an IAM user access key by calling the create_access_key method with the specified username from params.\\nIt writes the status to the console and updates params with the AccessKeyId and SecretAccessKey from the response.\\nFinally, it prints the AccessKeyId and SecretAccessKey.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d020a93977f8",
        "ground_truth": "def _create_access_key(self, params):\n    sys.stdout.write('Creating the IAM user access key... ')\n    response = self.iam.create_access_key(\n        UserName=params.user_name\n    )\n    params.access_key_id = response['AccessKey']['AccessKeyId']\n    params.secret_access_key = response['AccessKey']['SecretAccessKey']\n    sys.stdout.write(\n        'DONE\\n'\n        'AccessKeyId: {0}\\n'\n        'SecretAccessKey: {1}\\n'.format(\n            params.access_key_id,\n            params.secret_access_key\n        )\n    )",
        "import_statements": [
            "import sys",
            "from awscli.customizations.commands import BasicCommand",
            "from awscli.customizations.codedeploy.systems import DEFAULT_CONFIG_FILE",
            "from awscli.customizations.codedeploy.utils import \\\n    validate_region, validate_instance_name, validate_tags, \\\n    validate_iam_user_arn, INSTANCE_NAME_ARG, IAM_USER_ARN_ARG"
        ],
        "reference_api": [
            "create_access_key",
            "format",
            "write"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "write",
            "create_access_key",
            "write"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def _register_instance(self, params)",
        "start_line": "194",
        "end_line": "200",
        "file_path": "awscli/customizations/codedeploy/register.py",
        "docstring": "The _register_instance function registers an on-premises instance with AWS CodeDeploy.\\nIt takes parameters including instance name and IAM user ARN.\\nThe function writes a message to stdout indicating the start of registration.\\nIt then calls register_on_premises_instance on the codedeploy client using the provided parameters.\\nAfter successful registration, it writes 'DONE' to stdout.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bd9ae907e3e2",
        "ground_truth": "def _register_instance(self, params):\n    sys.stdout.write('Registering the on-premises instance... ')\n    self.codedeploy.register_on_premises_instance(\n        instanceName=params.instance_name,\n        iamUserArn=params.iam_user_arn\n    )\n    sys.stdout.write('DONE\\n')",
        "import_statements": [
            "import sys",
            "from awscli.customizations.commands import BasicCommand",
            "from awscli.customizations.codedeploy.systems import DEFAULT_CONFIG_FILE",
            "from awscli.customizations.codedeploy.utils import \\\n    validate_region, validate_instance_name, validate_tags, \\\n    validate_iam_user_arn, INSTANCE_NAME_ARG, IAM_USER_ARN_ARG"
        ],
        "reference_api": [
            "register_on_premises_instance",
            "write"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "write",
            "register_on_premises_instance",
            "write"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def validate_instance(params)",
        "start_line": "102",
        "end_line": "119",
        "file_path": "awscli/customizations/codedeploy/utils.py",
        "docstring": "The validate_instance function determines the system type of the current platform and validates if it is supported.\\nIt first checks if the platform is Linux and identifies the distribution (Ubuntu or RHEL), setting the system accordingly.\\nIf the platform is Windows, it sets the system to Windows.\\nIf the system type is not set, it raises a RuntimeError for unsupported systems.\\nIt then tries to access the EC2 metadata URL and raises a RuntimeError if it is an Amazon EC2 instance, otherwise it ignores URL-related errors.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "278356faa556",
        "ground_truth": "def validate_instance(params):\n    if platform.system() == 'Linux':\n        distribution = awscli.compat.linux_distribution()[0]\n        if 'Ubuntu' in distribution:\n            params.system = Ubuntu(params)\n        if 'Red Hat Enterprise Linux Server' in distribution:\n            params.system = RHEL(params)\n    elif platform.system() == 'Windows':\n        params.system = Windows(params)\n    if 'system' not in params:\n        raise RuntimeError(\n            System.UNSUPPORTED_SYSTEM_MSG\n        )\n    try:\n        urlopen('http://169.254.169.254/latest/meta-data/', timeout=1)\n        raise RuntimeError('Amazon EC2 instances are not supported.')\n    except (URLError, timeout):\n        pass",
        "import_statements": [
            "import platform",
            "import re",
            "import awscli.compat",
            "from awscli.compat import urlopen, URLError",
            "from awscli.customizations.codedeploy.systems import System, Ubuntu, Windows, RHEL",
            "from socket import timeout"
        ],
        "reference_api": [
            "RuntimeError",
            "urlopen",
            "RHEL",
            "Ubuntu",
            "platform.system",
            "linux_distribution",
            "Windows"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "linux_distribution",
            "Ubuntu",
            "RHEL",
            "Windows",
            "urlopen"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def _check_configure_recorder_status(self, configuration_recorder)",
        "start_line": "57",
        "end_line": "69",
        "file_path": "awscli/customizations/configservice/getstatus.py",
        "docstring": "The _check_configure_recorder_status function checks the status of a configuration recorder.\\nIt retrieves and prints the name and recording status of the recorder.\\nThe recording status is mapped to 'ON' or 'OFF'.\\nIf the recorder is on, it calls another function, _check_last_status, to get and print the last status.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8197d0c8f4be",
        "ground_truth": "def _check_configure_recorder_status(self, configuration_recorder):\n    # Get the name of the recorder and print it out.\n    name = configuration_recorder['name']\n    sys.stdout.write('name: %s\\n' % name)\n    # Get the recording status and print it out.\n    recording = configuration_recorder['recording']\n    recording_map = {False: 'OFF', True: 'ON'}\n    sys.stdout.write('recorder: %s\\n' % recording_map[recording])\n    # If the recorder is on, get the last status and print it out.\n    if recording:\n        self._check_last_status(configuration_recorder)",
        "import_statements": [
            "import sys",
            "from awscli.customizations.commands import BasicCommand"
        ],
        "reference_api": [
            "self._check_last_status",
            "write"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._check_last_status",
                "code": "def _check_last_status(self, status, status_name=''):\n        last_status = status['lastStatus']\n        sys.stdout.write('last %sstatus: %s\\n' % (status_name, last_status))\n        if last_status == \"FAILURE\":\n            sys.stdout.write('error code: %s\\n' % status['lastErrorCode'])\n            sys.stdout.write('message: %s\\n' % status['lastErrorMessage'])"
            }
        ],
        "third_party": [
            "write",
            "write"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def _check_delivery_channel_status(self, delivery_channel)",
        "start_line": "78",
        "end_line": "94",
        "file_path": "awscli/customizations/configservice/getstatus.py",
        "docstring": "The _check_delivery_channel_status function checks the status of a given delivery channel.\\nIt retrieves and prints the delivery channel's name.\\nIt obtains delivery statuses for the configuration stream, configuration history, and configuration snapshot.\\nIf these statuses exist, it calls the _check_last_status method to print out their respective statuses.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9a723027e34e",
        "ground_truth": "def _check_delivery_channel_status(self, delivery_channel):\n    # Get the name of the delivery channel and print it out.\n    name = delivery_channel['name']\n    sys.stdout.write('name: %s\\n' % name)\n    # Obtain the various delivery statuses.\n    stream_delivery = delivery_channel['configStreamDeliveryInfo']\n    history_delivery = delivery_channel['configHistoryDeliveryInfo']\n    snapshot_delivery = delivery_channel['configSnapshotDeliveryInfo']\n    # Print the statuses out if they exist.\n    if stream_delivery:\n        self._check_last_status(stream_delivery, 'stream delivery ')\n    if history_delivery:\n        self._check_last_status(history_delivery, 'history delivery ')\n    if snapshot_delivery:\n        self._check_last_status(snapshot_delivery, 'snapshot delivery ')",
        "import_statements": [
            "import sys",
            "from awscli.customizations.commands import BasicCommand"
        ],
        "reference_api": [
            "self._check_last_status",
            "write"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._check_last_status",
                "code": "def _check_last_status(self, status, status_name=''):\n        last_status = status['lastStatus']\n        sys.stdout.write('last %sstatus: %s\\n' % (status_name, last_status))\n        if last_status == \"FAILURE\":\n            sys.stdout.write('error code: %s\\n' % status['lastErrorCode'])\n            sys.stdout.write('message: %s\\n' % status['lastErrorMessage'])"
            },
            {
                "name": "self._check_last_status",
                "code": "def _check_last_status(self, status, status_name=''):\n        last_status = status['lastStatus']\n        sys.stdout.write('last %sstatus: %s\\n' % (status_name, last_status))\n        if last_status == \"FAILURE\":\n            sys.stdout.write('error code: %s\\n' % status['lastErrorCode'])\n            sys.stdout.write('message: %s\\n' % status['lastErrorMessage'])"
            },
            {
                "name": "self._check_last_status",
                "code": "def _check_last_status(self, status, status_name=''):\n        last_status = status['lastStatus']\n        sys.stdout.write('last %sstatus: %s\\n' % (status_name, last_status))\n        if last_status == \"FAILURE\":\n            sys.stdout.write('error code: %s\\n' % status['lastErrorCode'])\n            sys.stdout.write('message: %s\\n' % status['lastErrorMessage'])"
            }
        ],
        "third_party": [
            "write"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def get_model_location(session, service_definition, service_name=None)",
        "start_line": "48",
        "end_line": "83",
        "file_path": "awscli/customizations/configure/addmodel.py",
        "docstring": "The get_model_location function determines the file path for a service model JSON file.\\nIt creates a ServiceModel object from the service definition and derives the service name if not provided by using the endpoint prefix.\\nIt constructs the file name based on the service model version and returns the full path, combining data_path, service name, API version, and the constructed file name.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "164018f35078",
        "ground_truth": "def get_model_location(session, service_definition, service_name=None):\n    \"\"\"Gets the path of where a service-2.json file should go in ~/.aws/models\n     :type session: botocore.session.Session\n    :param session: A session object\n     :type service_definition: dict\n    :param service_definition: The json loaded service definition\n     :type service_name: str\n    :param service_name: The service name to use. If this not provided,\n        this will be determined from a combination of available services\n        and the service definition.\n     :returns: The path to where are model should be placed based on\n        the service definition and the current services in botocore.\n    \"\"\"\n    # Add the ServiceModel abstraction over the service json definition to\n    # make it easier to work with.\n    service_model = ServiceModel(service_definition)\n     # Determine the service_name if not provided\n    if service_name is None:\n        endpoint_prefix = service_model.endpoint_prefix\n        service_name = _get_service_name(session, endpoint_prefix)\n    api_version = service_model.api_version\n     # For the model location we only want the custom data path (~/.aws/models\n    # not the one set by AWS_DATA_PATH)\n    data_path = session.get_component('data_loader').CUSTOMER_DATA_PATH\n    # Use the version of the model to determine the file's naming convention.\n    service_model_name = (\n        'service-%d.json' % int(\n            float(service_definition.get('version', '2.0'))))\n    return os.path.join(data_path, service_name, api_version,\n        service_model_name)",
        "import_statements": [
            "import json",
            "import os",
            "from botocore.model import ServiceModel",
            "from awscli.customizations.commands import BasicCommand"
        ],
        "reference_api": [
            "join",
            "_get_service_name",
            "float",
            "int",
            "service_definition.get",
            "session.get_component",
            "ServiceModel"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_get_service_name",
                "code": "def _get_service_name(session, endpoint_prefix):\n    if endpoint_prefix in session.get_available_services():\n        # Check if the endpoint prefix is a pre-existing service.\n        # If it is, use that endpoint prefix as the service name.\n        return endpoint_prefix\n    else:\n        # The service may have a different endpoint prefix than its name\n        # So we need to determine what the correct mapping may be.\n\n        # Figure out the mappings of endpoint prefix to service names.\n        name_mappings = _get_endpoint_prefix_to_name_mappings(session)\n        # Determine the service name from the mapping.\n        # If it does not exist in the mapping, return the original endpoint\n        # prefix.\n        return name_mappings.get(endpoint_prefix, endpoint_prefix)"
            }
        ],
        "third_party": [
            "ServiceModel",
            "session.get_component",
            "service_definition.get",
            "join"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def api_to_definition(definition)",
        "start_line": "49",
        "end_line": "63",
        "file_path": "awscli/customizations/datapipeline/translator.py",
        "docstring": "The api_to_definition function transforms a given API definition dictionary by renaming and processing certain keys.\\nIf 'pipelineObjects' exists in the definition, it is converted using _api_to_objects_definition and renamed to 'objects'.\\nSimilarly, 'parameterObjects' is converted to 'parameters' using _api_to_parameters_definition, and 'parameterValues' is converted to 'values' using _api_to_values_definition.\\nThe function then returns the modified definition.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8941a3dc2218",
        "ground_truth": "def api_to_definition(definition):\n    # When we're translating from api_response -> definition\n    # we have to be careful *not* to mutate the existing\n    # response as other code might need to the original\n    # api_response.\n    if 'pipelineObjects' in definition:\n        definition['objects'] = _api_to_objects_definition(\n            definition.pop('pipelineObjects'))\n    if 'parameterObjects' in definition:\n        definition['parameters'] = _api_to_parameters_definition(\n            definition.pop('parameterObjects'))\n    if 'parameterValues' in definition:\n        definition['values'] = _api_to_values_definition(\n            definition.pop('parameterValues'))\n    return definition",
        "import_statements": [
            "import json",
            "from awscli.clidriver import CLIOperationCaller"
        ],
        "reference_api": [
            "_api_to_parameters_definition",
            "definition.pop",
            "_api_to_objects_definition",
            "_api_to_values_definition"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_api_to_objects_definition",
                "code": "def _api_to_objects_definition(api_response):\n    pipeline_objects = []\n    for element in api_response:\n        current = {\n            'id': element['id'],\n            'name': element['name']\n        }\n        for field in element['fields']:\n            key = field['key']\n            if 'stringValue' in field:\n                value = field['stringValue']\n            else:\n                value = {'ref': field['refValue']}\n            _add_value(key, value, current)\n        pipeline_objects.append(current)\n    return pipeline_objects"
            },
            {
                "name": "_api_to_parameters_definition",
                "code": "def _api_to_parameters_definition(api_response):\n    parameter_objects = []\n    for element in api_response:\n        current = {\n            'id': element['id']\n        }\n        for attribute in element['attributes']:\n            _add_value(attribute['key'], attribute['stringValue'], current)\n        parameter_objects.append(current)\n    return parameter_objects"
            },
            {
                "name": "_api_to_values_definition",
                "code": "def _api_to_values_definition(api_response):\n    pipeline_values = {}\n    for element in api_response:\n        _add_value(element['id'], element['stringValue'], pipeline_values)\n    return pipeline_values"
            }
        ],
        "third_party": [
            "definition.pop",
            "definition.pop",
            "definition.pop"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "aws/aws-cli",
        "function_declaration": "def check_if_statement_exists(expected_statement, actual_assume_role_document)",
        "start_line": "40",
        "end_line": "50",
        "file_path": "awscli/customizations/emrcontainers/update_role_trust_policy.py",
        "docstring": "The check_if_statement_exists function determines whether a given expected_statement exists within an actual_assume_role_document.\\nIf the document is None, it returns False.\\nIt retrieves the \"Statement\" list from the document and iterates through each statement, checking if it matches the expected statement using check_if_dict_matches.\\nIf a match is found, it returns True; otherwise, it returns False.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "83cfc6bb11f0",
        "ground_truth": "def check_if_statement_exists(expected_statement, actual_assume_role_document):\n    if actual_assume_role_document is None:\n        return False\n     existing_statements = actual_assume_role_document.get(\"Statement\", [])\n    for existing_statement in existing_statements:\n        matches = check_if_dict_matches(expected_statement, existing_statement)\n        if matches:\n            return True\n     return False",
        "import_statements": [
            "import json",
            "import logging",
            "from awscli.customizations.commands import BasicCommand",
            "from awscli.customizations.emrcontainers.constants \\\n    import TRUST_POLICY_STATEMENT_FORMAT, \\\n    TRUST_POLICY_STATEMENT_ALREADY_EXISTS, \\\n    TRUST_POLICY_UPDATE_SUCCESSFUL",
            "from awscli.customizations.emrcontainers.base36 import Base36",
            "from awscli.customizations.emrcontainers.eks import EKS",
            "from awscli.customizations.emrcontainers.iam import IAM",
            "from awscli.customizations.utils import uni_print, get_policy_arn_suffix"
        ],
        "reference_api": [
            "check_if_dict_matches",
            "actual_assume_role_document.get"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "check_if_dict_matches",
                "code": "def check_if_dict_matches(expected_dict, actual_dict):\n    if len(expected_dict) != len(actual_dict):\n        return False\n\n    for key in expected_dict:\n        key_str = str(key)\n        val = expected_dict[key_str]\n        if isinstance(val, dict):\n            if not check_if_dict_matches(val, actual_dict.get(key_str, {})):\n                return False\n        else:\n            if key_str not in actual_dict or actual_dict[key_str] != str(val):\n                return False\n\n    return True"
            }
        ],
        "third_party": [
            "actual_assume_role_document.get"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "GoogleCloudPlatform/terraformer",
        "function_declaration": "func (g *AccessAnalyzerGenerator) InitResources() error",
        "start_line": "30",
        "end_line": "55",
        "file_path": "providers/aws/accessanalyzer.go",
        "docstring": "The InitResources function initializes AWS Access Analyzer resources for Terraform.\\nIt first generates the AWS config using generateConfig.\\nThen, it creates an Access Analyzer service client and a paginator to list analyzers.\\nFor each page of analyzers, it iterates through the analyzers, creating a Terraform resource for each analyzer using its name as the resource name.\\nThese resources are added to the g.Resources slice.\\nIf any error occurs during the process, it returns the error.\n",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "104031d6696b",
        "ground_truth": "func (g *AccessAnalyzerGenerator) InitResources() error {\n config, e := g.generateConfig()\n if e != nil {\n  return e\n }\n svc := accessanalyzer.NewFromConfig(config)\n p := accessanalyzer.NewListAnalyzersPaginator(svc, &accessanalyzer.ListAnalyzersInput{})\n var resources []terraformutils.Resource\n for p.HasMorePages() {\n  page, e := p.NextPage(context.TODO())\n  if e != nil {\n   return e\n  }\n  for _, analyzer := range page.Analyzers {\n   resourceName := *analyzer.Name\n   resources = append(resources, terraformutils.NewSimpleResource(\n    resourceName,\n    resourceName,\n    \"aws_accessanalyzer_analyzer\",\n    \"aws\",\n    accessanalyzerAllowEmptyValues))\n  }\n }\n g.Resources = resources\n return nil\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\n\t\"github.com/GoogleCloudPlatform/terraformer/terraformutils\"\n\t\"github.com/aws/aws-sdk-go-v2/service/accessanalyzer\"\n)"
        ],
        "reference_api": [
            "g.generateConfig",
            "terraformutils.NewSimpleResource",
            "accessanalyzer.NewListAnalyzersPaginator",
            "p.HasMorePages",
            "p.NextPage",
            "append",
            "context.TODO",
            "accessanalyzer.NewFromConfig"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "g.generateConfig",
            "terraformutils.NewSimpleResource",
            "accessanalyzer.NewListAnalyzersPaginator",
            "p.HasMorePages",
            "p.NextPage",
            "append",
            "context.TODO",
            "accessanalyzer.NewFromConfig"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "GoogleCloudPlatform/terraformer",
        "function_declaration": "func (g *AlbGenerator) loadLB(svc *elasticloadbalancingv2.Client) error",
        "start_line": "34",
        "end_line": "57",
        "file_path": "providers/aws/alb.go",
        "docstring": "The function loadLB in the AlbGenerator struct loads information about Load Balancers using the AWS Elastic Load Balancing v2 service client.\\nIt uses a paginator to iterate through pages of load balancers.\\nFor each load balancer, it retrieves its name and ARN, creates a new Terraform resource, and appends it to the generator's resources.\\nAdditionally, it calls loadLBListener to load listeners for each load balancer, logging any errors encountered.\\nIf any page retrieval fails, it returns the error; otherwise, it completes successfully.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2912d6cee389",
        "ground_truth": "func (g *AlbGenerator) loadLB(svc *elasticloadbalancingv2.Client) error {\n p := elasticloadbalancingv2.NewDescribeLoadBalancersPaginator(svc, &elasticloadbalancingv2.DescribeLoadBalancersInput{})\n for p.HasMorePages() {\n  page, err := p.NextPage(context.TODO())\n  if err != nil {\n   return err\n  }\n  for _, lb := range page.LoadBalancers {\n   resourceName := StringValue(lb.LoadBalancerName)\n   g.Resources = append(g.Resources, terraformutils.NewSimpleResource(\n    *lb.LoadBalancerArn,\n    resourceName,\n    \"aws_lb\",\n    \"aws\",\n    AlbAllowEmptyValues,\n   ))\n   err := g.loadLBListener(svc, lb.LoadBalancerArn)\n   if err != nil {\n    log.Println(err)\n   }\n  }\n }\n return nil\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/GoogleCloudPlatform/terraformer/terraformutils\"\n\t\"github.com/aws/aws-sdk-go-v2/aws\"\n\t\"github.com/aws/aws-sdk-go-v2/service/elasticloadbalancingv2\"\n\t\"github.com/aws/aws-sdk-go-v2/service/elasticloadbalancingv2/types\"\n)"
        ],
        "reference_api": [
            "terraformutils.NewSimpleResource",
            "p.HasMorePages",
            "p.NextPage",
            "append",
            "context.TODO",
            "log.Println",
            "g.loadLBListener",
            "elasticloadbalancingv2.NewDescribeLoadBalancersPaginator",
            "StringValue"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "terraformutils.NewSimpleResource",
            "p.HasMorePages",
            "p.NextPage",
            "append",
            "context.TODO",
            "g.loadLBListener",
            "elasticloadbalancingv2.NewDescribeLoadBalancersPaginator",
            "StringValue"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "GoogleCloudPlatform/terraformer",
        "function_declaration": "func (g *APIGatewayGenerator) shouldFilterRestAPI(tags map[string]string) bool",
        "start_line": "97",
        "end_line": "108",
        "file_path": "providers/aws/api_gateway.go",
        "docstring": "The shouldFilterRestAPI function in the APIGatewayGenerator struct determines if a REST API should be filtered out based on its tags.\\nIt iterates through a list of filters, checking if each filter's FieldPath starts with \"tags.\" and is applicable to \"api_gateway_rest_api\".\\nIf so, it extracts the tag name and checks if the tag's value is in the list of acceptable values for that filter.\\nIf the tag value is not in the acceptable values, or if the tag does not exist, the function returns true to indicate the API should be filtered out.\\nOtherwise, it returns false.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f79a45391a9b",
        "ground_truth": "func (g *APIGatewayGenerator) shouldFilterRestAPI(tags map[string]string) bool {\n for _, filter := range g.Filter {\n  if strings.HasPrefix(filter.FieldPath, \"tags.\") && filter.IsApplicable(\"api_gateway_rest_api\") {\n   tagName := strings.Replace(filter.FieldPath, \"tags.\", \"\", 1)\n   if val, ok := tags[tagName]; ok {\n    return !terraformerstring.ContainsString(filter.AcceptableValues, val)\n   }\n   return true\n  }\n }\n return false\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t\"log\"\n\t\"strings\"\n\n\t\"github.com/GoogleCloudPlatform/terraformer/terraformutils\"\n\t\"github.com/GoogleCloudPlatform/terraformer/terraformutils/terraformerstring\"\n\t\"github.com/aws/aws-sdk-go-v2/service/apigateway\"\n\t\"github.com/aws/aws-sdk-go-v2/service/apigateway/types\"\n)"
        ],
        "reference_api": [
            "terraformerstring.ContainsString",
            "strings.Replace",
            "filter.IsApplicable",
            "strings.HasPrefix"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "terraformerstring.ContainsString",
            "filter.IsApplicable"
        ]
    },
    {
        "subclass": "AWS",
        "owner/repo": "GoogleCloudPlatform/terraformer",
        "function_declaration": "func (s *AWSService) buildBaseConfig() (aws.Config, error)",
        "start_line": "73",
        "end_line": "85",
        "file_path": "providers/aws/aws_service.go",
        "docstring": "The buildBaseConfig function constructs the base AWS configuration for a service.\\nIt initializes an empty list of load options.\\nIf a profile is specified in the service arguments, it adds the profile to the load options.\\nIf a region is specified, it sets the AWS_REGION environment variable to that region.\\nIt also configures the AssumeRole credential options to use a token provider that reads from stdin.\\nFinally, it loads and returns the default AWS configuration using the specified load options.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5fd67e9afe98",
        "ground_truth": "func (s *AWSService) buildBaseConfig() (aws.Config, error) {\n var loadOptions []func(*config.LoadOptions) error\n if s.GetArgs()[\"profile\"].(string) != \"\" {\n  loadOptions = append(loadOptions, config.WithSharedConfigProfile(s.GetArgs()[\"profile\"].(string)))\n }\n if s.GetArgs()[\"region\"].(string) != \"\" {\n  os.Setenv(\"AWS_REGION\", s.GetArgs()[\"region\"].(string))\n }\n loadOptions = append(loadOptions, config.WithAssumeRoleCredentialOptions(func(options *stscreds.AssumeRoleOptions) {\n  options.TokenProvider = stscreds.StdinTokenProvider\n }))\n return config.LoadDefaultConfig(context.TODO(), loadOptions...)\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t\"os\"\n\t\"regexp\"\n\n\t\"github.com/aws/aws-sdk-go-v2/service/sts\"\n\n\t\"github.com/aws/aws-sdk-go-v2/aws\"\n\t\"github.com/aws/aws-sdk-go-v2/config\"\n\t\"github.com/aws/aws-sdk-go-v2/credentials/stscreds\"\n\n\t\"github.com/GoogleCloudPlatform/terraformer/terraformutils\"\n)"
        ],
        "reference_api": [
            "config.WithSharedConfigProfile",
            "config.LoadDefaultConfig",
            "append",
            "context.TODO",
            "s.GetArgs",
            "os.Setenv",
            "config.WithAssumeRoleCredentialOptions"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "config.WithSharedConfigProfile",
            "config.LoadDefaultConfig",
            "append",
            "context.TODO",
            "s.GetArgs",
            "config.WithAssumeRoleCredentialOptions"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "public async Task AddItemAsync(string userId, string productId, int quantity)",
        "start_line": "62",
        "end_line": "94",
        "file_path": "src/cartservice/src/cartstore/AlloyDBCartStore.cs",
        "docstring": "The AddItemAsync function adds a specified quantity of a product to a user's cart in a PostgreSQL database.\\nIt prints a message indicating the operation has started.\\nIt creates a connection to the database using NpgsqlDataSource.\\nIt constructs a SQL command to fetch the current quantity of the specified product for the given user.\\nIt executes the command and reads the current quantity from the database.\\nIt calculates the total quantity by adding the new quantity to the current quantity.\\nIt constructs an SQL command to insert the total quantity into the database.\\nIt executes the insert command asynchronously.\\nIf an error occurs, it throws an RpcException with details about the failure.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "08ade5b493ee",
        "ground_truth": "public async Task AddItemAsync(string userId, string productId, int quantity)\n{\n    Console.WriteLine($\"AddItemAsync for {userId} called\");\n    try\n    {\n        await using var dataSource = NpgsqlDataSource.Create(connectionString);\n        // Fetch the current quantity for our userId/productId tuple\n        var fetchCmd = $\"SELECT quantity FROM {tableName} WHERE userID='{userId}' AND productID='{productId}'\";\n        var currentQuantity = 0;\n        var cmdRead = dataSource.CreateCommand(fetchCmd);\n        await using (var reader = await cmdRead.ExecuteReaderAsync())\n        {\n            while (await reader.ReadAsync())\n                currentQuantity += reader.GetInt32(0);\n        }\n        var totalQuantity = quantity + currentQuantity;\n        var insertCmd = $\"INSERT INTO {tableName} (userId, productId, quantity) VALUES ('{userId}', '{productId}', {totalQuantity})\";\n        await using (var cmdInsert = dataSource.CreateCommand(insertCmd))\n        {\n            await Task.Run(() =>\n            {\n                return cmdInsert.ExecuteNonQueryAsync();\n            });\n        }\n    }\n    catch (Exception ex)\n    {\n        throw new RpcException(\n            new Status(StatusCode.FailedPrecondition, $\"Can't access cart storage at {connectionString}. {ex}\"));\n    }\n}",
        "import_statements": [
            "using System",
            "using Grpc.Core",
            "using Npgsql",
            "using Microsoft.Extensions.Configuration",
            "using System.Threading.Tasks",
            "using Google.Api.Gax.ResourceNames",
            "using Google.Cloud.SecretManager.V1"
        ],
        "reference_api": [
            "Console.WriteLine",
            "Task.Run",
            "NpgsqlDataSource.Create",
            "reader.GetInt32",
            "cmdRead.ExecuteReaderAsync",
            "reader.ReadAsync",
            "cmdInsert.ExecuteNonQueryAsync",
            "dataSource.CreateCommand"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "NpgsqlDataSource.Create",
            "reader.GetInt32",
            "cmdRead.ExecuteReaderAsync",
            "reader.ReadAsync",
            "cmdInsert.ExecuteNonQueryAsync",
            "dataSource.CreateCommand"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "public async Task EmptyCartAsync(string userId)",
        "start_line": "134",
        "end_line": "155",
        "file_path": "src/cartservice/src/cartstore/AlloyDBCartStore.cs",
        "docstring": "The EmptyCartAsync function asynchronously empties the shopping cart for a specified user.\\nIt logs the user ID being processed.\\nIt tries to create a PostgreSQL data source using a connection string and then constructs a DELETE SQL command to remove entries for the given user ID from a table.\\nIt executes the command asynchronously using ExecuteNonQueryAsync.\\nIf an exception occurs, it throws an RpcException with a failed precondition status, indicating that the cart storage is inaccessible.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e5d6689bf8b5",
        "ground_truth": "public async Task EmptyCartAsync(string userId)\n{\n    Console.WriteLine($\"EmptyCartAsync called for userId={userId}\");\n    try\n    {\n        await using var dataSource = NpgsqlDataSource.Create(connectionString);\n        var deleteCmd = $\"DELETE FROM {tableName} WHERE userID = '{userId}'\";\n        await using (var cmd = dataSource.CreateCommand(deleteCmd))\n        {\n            await Task.Run(() =>\n            {\n                return cmd.ExecuteNonQueryAsync();\n            });\n        }\n    }\n    catch (Exception ex)\n    {\n        throw new RpcException(\n            new Status(StatusCode.FailedPrecondition, $\"Can't access cart storage at {connectionString}. {ex}\"));\n    }\n}",
        "import_statements": [
            "using System",
            "using Grpc.Core",
            "using Npgsql",
            "using Microsoft.Extensions.Configuration",
            "using System.Threading.Tasks",
            "using Google.Api.Gax.ResourceNames",
            "using Google.Cloud.SecretManager.V1"
        ],
        "reference_api": [
            "cmd.ExecuteNonQueryAsync",
            "Console.WriteLine",
            "Task.Run",
            "NpgsqlDataSource.Create",
            "dataSource.CreateCommand"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "cmd.ExecuteNonQueryAsync",
            "NpgsqlDataSource.Create",
            "dataSource.CreateCommand"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "function _carry (amount)",
        "start_line": "114",
        "end_line": "120",
        "file_path": "src/currencyservice/server.js",
        "docstring": "The function _carry normalizes an amount object containing units and nanos.\\nIt defines fractionSize as 10^9.\\nIt then adds the fractional part of units (converted to nanos) to amount.nanos.\\nNext, it updates amount.units to the sum of its integer part and the integer division of amount.nanos by fractionSize.\\nFinally, it updates amount.nanos to the remainder of its division by fractionSize.\\nThe function returns the normalized amount.",
        "language": "JavaScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "73254300f5f4",
        "ground_truth": "function _carry (amount) {\n  const fractionSize = Math.pow(10, 9);\n  amount.nanos += (amount.units % 1) * fractionSize;\n  amount.units = Math.floor(amount.units) + Math.floor(amount.nanos / fractionSize);\n  amount.nanos = amount.nanos % fractionSize;\n  return amount;\n}",
        "import_statements": [],
        "reference_api": [
            "Math.floor",
            "Math.pow"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "def EmptyCart(self, request, context)",
        "start_line": "69",
        "end_line": "73",
        "file_path": "src/emailservice/demo_pb2_grpc.py",
        "docstring": "The EmptyCart function is a method designed to handle a request to empty a cart.\\nIt lacks an associated documentation comment in the .proto file.\\nWhen called, it sets the gRPC context status code to UNIMPLEMENTED and provides a message indicating the method is not implemented.\\nIt then raises a NotImplementedError exception with the same message.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "14aeb805c905",
        "ground_truth": "def EmptyCart(self, request, context):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details('Method not implemented!')\n    raise NotImplementedError('Method not implemented!')",
        "import_statements": [
            "import grpc"
        ],
        "reference_api": [
            "NotImplementedError",
            "context.set_details",
            "context.set_code"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "context.set_code",
            "context.set_details"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "def add_CurrencyServiceServicer_to_server(servicer, server)",
        "start_line": "498",
        "end_line": "513",
        "file_path": "src/emailservice/demo_pb2_grpc.py",
        "docstring": "The add_CurrencyServiceServicer_to_server function registers a servicer to a gRPC server.\\nIt creates RPC method handlers for the servicer's GetSupportedCurrencies and Convert methods.\\nThe request deserializer and response serializer for each method are specified using protocol buffer message types.\\nThese method handlers are then added to a generic gRPC handler for the CurrencyService.\\nFinally, this generic handler is added to the provided gRPC server.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f8c17bc73f55",
        "ground_truth": "def add_CurrencyServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n            'GetSupportedCurrencies': grpc.unary_unary_rpc_method_handler(\n                    servicer.GetSupportedCurrencies,\n                    request_deserializer=demo__pb2.Empty.FromString,\n                    response_serializer=demo__pb2.GetSupportedCurrenciesResponse.SerializeToString,\n            ),\n            'Convert': grpc.unary_unary_rpc_method_handler(\n                    servicer.Convert,\n                    request_deserializer=demo__pb2.CurrencyConversionRequest.FromString,\n                    response_serializer=demo__pb2.Money.SerializeToString,\n            ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n            'hipstershop.CurrencyService', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))",
        "import_statements": [
            "import grpc"
        ],
        "reference_api": [
            "grpc.unary_unary_rpc_method_handler",
            "server.add_generic_rpc_handlers",
            "grpc.method_handlers_generic_handler"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "grpc.unary_unary_rpc_method_handler",
            "grpc.unary_unary_rpc_method_handler",
            "grpc.method_handlers_generic_handler",
            "server.add_generic_rpc_handlers"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "def add_PaymentServiceServicer_to_server(servicer, server)",
        "start_line": "587",
        "end_line": "597",
        "file_path": "src/emailservice/demo_pb2_grpc.py",
        "docstring": "The add_PaymentServiceServicer_to_server function registers a PaymentService servicer with a gRPC server.\\nIt creates a dictionary of RPC method handlers for the 'Charge' method, specifying the request deserializer and response serializer using the ChargeRequest and ChargeResponse classes from demo_pb2.\\nIt then creates a generic handler for the 'hipstershop.PaymentService' using the rpc_method_handlers dictionary.\\nFinally, it adds this generic handler to the provided gRPC server by calling server.add_generic_rpc_handlers with the generic handler.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5158cfaafc3a",
        "ground_truth": "def add_PaymentServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n            'Charge': grpc.unary_unary_rpc_method_handler(\n                    servicer.Charge,\n                    request_deserializer=demo__pb2.ChargeRequest.FromString,\n                    response_serializer=demo__pb2.ChargeResponse.SerializeToString,\n            ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n            'hipstershop.PaymentService', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))",
        "import_statements": [
            "import grpc"
        ],
        "reference_api": [
            "grpc.unary_unary_rpc_method_handler",
            "server.add_generic_rpc_handlers",
            "grpc.method_handlers_generic_handler"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "grpc.unary_unary_rpc_method_handler",
            "grpc.method_handlers_generic_handler",
            "server.add_generic_rpc_handlers"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "def send_confirmation_email(email, order)",
        "start_line": "25",
        "end_line": "36",
        "file_path": "src/emailservice/email_client.py",
        "docstring": "The send_confirmation_email function sends an order confirmation email using gRPC.\\nIt creates an insecure gRPC channel to the address '[::]:8080' and initializes the EmailServiceStub.\\nThe function then attempts to send an order confirmation request with the provided email and order details.\\nIf the request is successful, it logs that the request was sent.\\nIf an RPC error occurs, it logs the error details, including the error code name and value.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "942d6791edd2",
        "ground_truth": "def send_confirmation_email(email, order):\n  channel = grpc.insecure_channel('[::]:8080')\n  stub = demo_pb2_grpc.EmailServiceStub(channel)\n  try:\n    response = stub.SendOrderConfirmation(demo_pb2.SendOrderConfirmationRequest(\n      email = email,\n      order = order\n    ))\n    logger.info('Request sent.')\n  except grpc.RpcError as err:\n    logger.error(err.details())\n    logger.error('{}, {}'.format(err.code().name, err.code().value))",
        "import_statements": [
            "import grpc",
            "import demo_pb2",
            "import demo_pb2_grpc",
            "from logger import getJSONLogger"
        ],
        "reference_api": [
            "grpc.insecure_channel",
            "demo_pb2.SendOrderConfirmationRequest",
            "demo_pb2_grpc.EmailServiceStub",
            "logger.info",
            "stub.SendOrderConfirmation",
            "err.details",
            "err.code",
            "logger.error",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "stub.SendOrderConfirmation",
                "code": "def SendOrderConfirmation(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')"
            }
        ],
        "third_party": [
            "grpc.insecure_channel",
            "demo_pb2_grpc.EmailServiceStub",
            "demo_pb2.SendOrderConfirmationRequest",
            "err.details",
            "err.code",
            "err.code"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "def send_email(client, email_address, content)",
        "start_line": "66",
        "end_line": "83",
        "file_path": "src/emailservice/email_server.py",
        "docstring": "The send_email function sends an email using the provided client.\\nIt calls the send_message method on the client with several parameters:\\nthe sender information, recipient email address, and email content.\\nThe email includes the sender's address, recipient's address, subject line, and HTML body content.\\nAfter sending the email, it logs the message ID of the sent email.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "68fd884c7732",
        "ground_truth": "def send_email(client, email_address, content):\n  response = client.send_message(\n    sender = client.sender_path(project_id, region, sender_id),\n    envelope_from_authority = '',\n    header_from_authority = '',\n    envelope_from_address = from_address,\n    simple_message = {\n      \"from\": {\n        \"address_spec\": from_address,\n      },\n      \"to\": [{\n        \"address_spec\": email_address\n      }],\n      \"subject\": \"Your Confirmation Email\",\n      \"html_body\": content\n    }\n  )\n  logger.info(\"Message sent: {}\".format(response.rfc822_message_id))",
        "import_statements": [
            "from concurrent import futures",
            "import argparse",
            "import os",
            "import sys",
            "import time",
            "import grpc",
            "import traceback",
            "from jinja2 import Environment, FileSystemLoader, select_autoescape, TemplateError",
            "from google.api_core.exceptions import GoogleAPICallError",
            "from google.auth.exceptions import DefaultCredentialsError",
            "import demo_pb2",
            "import demo_pb2_grpc",
            "from grpc_health.v1 import health_pb2",
            "from grpc_health.v1 import health_pb2_grpc",
            "from opentelemetry import trace",
            "from opentelemetry.instrumentation.grpc import GrpcInstrumentorServer",
            "from opentelemetry.sdk.trace import TracerProvider",
            "from opentelemetry.sdk.trace.export import BatchSpanProcessor",
            "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter",
            "import googlecloudprofiler",
            "from logger import getJSONLogger"
        ],
        "reference_api": [
            "client.sender_path",
            "logger.info",
            "client.send_message",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "client.send_message",
            "client.sender_path"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "def SendOrderConfirmation(self, request, context)",
        "start_line": "85",
        "end_line": "105",
        "file_path": "src/emailservice/email_server.py",
        "docstring": "The SendOrderConfirmation function processes an order confirmation request.\\nIt extracts the email and order details from the request.\\nIt attempts to render a confirmation email using the order details.\\nIf rendering fails, it logs an error, sets an internal status code, and returns an empty response.\\nIf rendering is successful, it tries to send the email via the EmailService.\\nIf sending fails, it logs an error, sets an internal status code, and returns an empty response.\\nIf sending is successful, it returns an empty response.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "75c508f4e74b",
        "ground_truth": "def SendOrderConfirmation(self, request, context):\n  email = request.email\n  order = request.order\n  try:\n    confirmation = template.render(order = order)\n  except TemplateError as err:\n    context.set_details(\"An error occurred when preparing the confirmation mail.\")\n    logger.error(err.message)\n    context.set_code(grpc.StatusCode.INTERNAL)\n    return demo_pb2.Empty()\n  try:\n    EmailService.send_email(self.client, email, confirmation)\n  except GoogleAPICallError as err:\n    context.set_details(\"An error occurred when sending the email.\")\n    print(err.message)\n    context.set_code(grpc.StatusCode.INTERNAL)\n    return demo_pb2.Empty()\n  return demo_pb2.Empty()",
        "import_statements": [
            "from concurrent import futures",
            "import argparse",
            "import os",
            "import sys",
            "import time",
            "import grpc",
            "import traceback",
            "from jinja2 import Environment, FileSystemLoader, select_autoescape, TemplateError",
            "from google.api_core.exceptions import GoogleAPICallError",
            "from google.auth.exceptions import DefaultCredentialsError",
            "import demo_pb2",
            "import demo_pb2_grpc",
            "from grpc_health.v1 import health_pb2",
            "from grpc_health.v1 import health_pb2_grpc",
            "from opentelemetry import trace",
            "from opentelemetry.instrumentation.grpc import GrpcInstrumentorServer",
            "from opentelemetry.sdk.trace import TracerProvider",
            "from opentelemetry.sdk.trace.export import BatchSpanProcessor",
            "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter",
            "import googlecloudprofiler",
            "from logger import getJSONLogger"
        ],
        "reference_api": [
            "print",
            "context.set_code",
            "EmailService.send_email",
            "demo_pb2.Empty",
            "logger.error",
            "context.set_details",
            "template.render"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "template.render",
            "context.set_details",
            "context.set_code",
            "demo_pb2.Empty",
            "EmailService.send_email",
            "context.set_details",
            "context.set_code",
            "demo_pb2.Empty",
            "demo_pb2.Empty"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "def start(dummy_mode)",
        "start_line": "117",
        "end_line": "136",
        "file_path": "src/emailservice/email_server.py",
        "docstring": "The start function initializes and starts a gRPC server.\\nIt creates a server with a thread pool of 10 workers.\\nIf dummy_mode is enabled, it instantiates a DummyEmailService.\\nCurrently, non-dummy mode is not implemented and raises an exception.\\nThe email and health services are added to the server.\\nThe server listens on a port specified by the PORT environment variable, defaulting to 8080.\\nIt starts the server and keeps it running, handling keyboard interrupts to stop the server gracefully.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2b8f0ca367c3",
        "ground_truth": "def start(dummy_mode):\n  server = grpc.server(futures.ThreadPoolExecutor(max_workers=10),)\n  service = None\n  if dummy_mode:\n    service = DummyEmailService()\n  else:\n    raise Exception('non-dummy mode not implemented yet')\n   demo_pb2_grpc.add_EmailServiceServicer_to_server(service, server)\n  health_pb2_grpc.add_HealthServicer_to_server(service, server)\n   port = os.environ.get('PORT', \"8080\")\n  logger.info(\"listening on port: \"+port)\n  server.add_insecure_port('[::]:'+port)\n  server.start()\n  try:\n    while True:\n      time.sleep(3600)\n  except KeyboardInterrupt:\n    server.stop(0)",
        "import_statements": [
            "from concurrent import futures",
            "import argparse",
            "import os",
            "import sys",
            "import time",
            "import grpc",
            "import traceback",
            "from jinja2 import Environment, FileSystemLoader, select_autoescape, TemplateError",
            "from google.api_core.exceptions import GoogleAPICallError",
            "from google.auth.exceptions import DefaultCredentialsError",
            "import demo_pb2",
            "import demo_pb2_grpc",
            "from grpc_health.v1 import health_pb2",
            "from grpc_health.v1 import health_pb2_grpc",
            "from opentelemetry import trace",
            "from opentelemetry.instrumentation.grpc import GrpcInstrumentorServer",
            "from opentelemetry.sdk.trace import TracerProvider",
            "from opentelemetry.sdk.trace.export import BatchSpanProcessor",
            "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter",
            "import googlecloudprofiler",
            "from logger import getJSONLogger"
        ],
        "reference_api": [
            "server.add_insecure_port",
            "grpc.server",
            "DummyEmailService",
            "futures.ThreadPoolExecutor",
            "server.start",
            "server.stop",
            "demo_pb2_grpc.add_EmailServiceServicer_to_server",
            "health_pb2_grpc.add_HealthServicer_to_server",
            "logger.info",
            "get",
            "time.sleep",
            "Exception"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "demo_pb2_grpc.add_EmailServiceServicer_to_server",
                "code": "def add_EmailServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n            'SendOrderConfirmation': grpc.unary_unary_rpc_method_handler(\n                    servicer.SendOrderConfirmation,\n                    request_deserializer=demo__pb2.SendOrderConfirmationRequest.FromString,\n                    response_serializer=demo__pb2.Empty.SerializeToString,\n            ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n            'hipstershop.EmailService', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))"
            },
            {
                "name": "server.start",
                "code": "def start(dummy_mode):\n  server = grpc.server(futures.ThreadPoolExecutor(max_workers=10),)\n  service = None\n  if dummy_mode:\n    service = DummyEmailService()\n  else:\n    raise Exception('non-dummy mode not implemented yet')\n\n  demo_pb2_grpc.add_EmailServiceServicer_to_server(service, server)\n  health_pb2_grpc.add_HealthServicer_to_server(service, server)\n\n  port = os.environ.get('PORT', \"8080\")\n  logger.info(\"listening on port: \"+port)\n  server.add_insecure_port('[::]:'+port)\n  server.start()\n  try:\n    while True:\n      time.sleep(3600)\n  except KeyboardInterrupt:\n    server.stop(0)"
            }
        ],
        "third_party": [
            "grpc.server",
            "futures.ThreadPoolExecutor",
            "DummyEmailService",
            "health_pb2_grpc.add_HealthServicer_to_server",
            "get",
            "server.add_insecure_port",
            "server.stop"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "func ValidationErrorResponse(err error) error",
        "start_line": "73",
        "end_line": "83",
        "file_path": "src/frontend/validator/validator.go",
        "docstring": "The ValidationErrorResponse function processes a validation error and returns a formatted error message.\\nIt first checks if the error is of type validator.ValidationErrors.\\nIf not, it returns a generic invalid validation error message.\\nIf it is, it iterates over each validation error, appending a formatted string for each invalid field and its corresponding validation tag to a message string.\\nFinally, it returns the concatenated message as a formatted error.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "442ae43f9b54",
        "ground_truth": "func ValidationErrorResponse(err error) error {\n validationErrs, ok := err.(validator.ValidationErrors)\n if !ok {\n  return errors.New(\"invalid validation error format\")\n }\n var msg string\n for _, err := range validationErrs {\n  msg += fmt.Sprintf(\"Field '%s' is invalid: %s\\n\", err.Field(), err.Tag())\n }\n return fmt.Errorf(msg)\n}",
        "import_statements": [
            "import (\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/go-playground/validator/v10\"\n)"
        ],
        "reference_api": [
            "fmt.Errorf",
            "err.Tag",
            "fmt.Sprintf",
            "errors.New",
            "err.Field"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "err.Tag",
            "err.Field"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "func (lh *logHandler) ServeHTTP(w http.ResponseWriter, r *http.Request)",
        "start_line": "57",
        "end_line": "83",
        "file_path": "src/frontend/middleware.go",
        "docstring": "The ServeHTTP function in the logHandler struct handles HTTP requests with enhanced logging capabilities.\\nIt starts by generating a unique request ID and adding it to the request context.\\nIt initializes a responseRecorder to capture response details.\\nThe logger is enhanced with request-specific details such as the request path, method, and ID.\\nIf a session ID is present in the context, it adds it to the log.\\nThe function logs the start of the request and ensures that upon completion, it logs the response time, status, and bytes written.\\nThe context is updated with the enhanced logger, and the request is passed to the next handler in the chain.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0a0967bbda83",
        "ground_truth": "func (lh *logHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n ctx := r.Context()\n requestID, _ := uuid.NewRandom()\n ctx = context.WithValue(ctx, ctxKeyRequestID{}, requestID.String())\n  start := time.Now()\n rr := &responseRecorder{w: w}\n log := lh.log.WithFields(logrus.Fields{\n  \"http.req.path\":   r.URL.Path,\n  \"http.req.method\": r.Method,\n  \"http.req.id\":     requestID.String(),\n })\n if v, ok := r.Context().Value(ctxKeySessionID{}).(string); ok {\n  log = log.WithField(\"session\", v)\n }\n log.Debug(\"request started\")\n defer func() {\n  log.WithFields(logrus.Fields{\n   \"http.resp.took_ms\": int64(time.Since(start) / time.Millisecond),\n   \"http.resp.status\":  rr.status,\n   \"http.resp.bytes\":   rr.b}).Debugf(\"request complete\")\n }()\n  ctx = context.WithValue(ctx, ctxKeyLog{}, log)\n r = r.WithContext(ctx)\n lh.next.ServeHTTP(rr, r)\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t\"net/http\"\n\t\"time\"\n\t\"os\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/sirupsen/logrus\"\n)"
        ],
        "reference_api": [
            "r.WithContext",
            "context.WithValue",
            "log.Debug",
            "r.Context().Value",
            "requestID.String",
            "r.Context",
            "log.WithField",
            "log.WithFields",
            "int64",
            "uuid.NewRandom",
            "lh.log.WithFields",
            "lh.next.ServeHTTP",
            "func() {\n\t\tlog.WithFields(logrus.Fields{\n\t\t\t\"http.resp.took_ms\": int64(time.Since(start) / time.Millisecond),\n\t\t\t\"http.resp.status\":  rr.status,\n\t\t\t\"http.resp.bytes\":   rr.b}).Debugf(\"request complete\")\n\t}",
            "time.Since",
            "log.WithFields(logrus.Fields{\n\t\t\t\"http.resp.took_ms\": int64(time.Since(start) / time.Millisecond),\n\t\t\t\"http.resp.status\":  rr.status,\n\t\t\t\"http.resp.bytes\":   rr.b}).Debugf",
            "time.Now"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "r.WithContext",
            "context.WithValue",
            "log.Debug",
            "r.Context().Value",
            "requestID.String",
            "r.Context",
            "log.WithField",
            "log.WithFields",
            "int64",
            "uuid.NewRandom",
            "lh.log.WithFields",
            "lh.next.ServeHTTP",
            "func() {\n\t\tlog.WithFields(logrus.Fields{\n\t\t\t\"http.resp.took_ms\": int64(time.Since(start) / time.Millisecond),\n\t\t\t\"http.resp.status\":  rr.status,\n\t\t\t\"http.resp.bytes\":   rr.b}).Debugf(\"request complete\")\n\t}",
            "time.Since",
            "log.WithFields(logrus.Fields{\n\t\t\t\"http.resp.took_ms\": int64(time.Since(start) / time.Millisecond),\n\t\t\t\"http.resp.status\":  rr.status,\n\t\t\t\"http.resp.bytes\":   rr.b}).Debugf"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "func ensureSessionID(next http.Handler) http.HandlerFunc",
        "start_line": "85",
        "end_line": "111",
        "file_path": "src/frontend/middleware.go",
        "docstring": "The ensureSessionID function is a middleware for HTTP handlers that ensures each request has a session ID.\\nIt checks if a session ID cookie exists in the request.\\nIf the cookie is not found and the ENABLE_SINGLE_SHARED_SESSION environment variable is true,\\nit assigns a hard-coded session ID.\\nOtherwise, it generates a new UUID as the session ID and sets it as a cookie in the response.\\nIf there is an error other than a missing cookie, it returns without modifying the request.\\nThe session ID is added to the request context, and the next handler is called with the modified request.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2838015b7adc",
        "ground_truth": "func ensureSessionID(next http.Handler) http.HandlerFunc {\n return func(w http.ResponseWriter, r *http.Request) {\n  var sessionID string\n  c, err := r.Cookie(cookieSessionID)\n  if err == http.ErrNoCookie {\n   if os.Getenv(\"ENABLE_SINGLE_SHARED_SESSION\") == \"true\" {\n    // Hard coded user id, shared across sessions\n    sessionID = \"12345678-1234-1234-1234-123456789123\"\n   } else {\n    u, _ := uuid.NewRandom()\n    sessionID = u.String()\n   }\n   http.SetCookie(w, &http.Cookie{\n    Name:   cookieSessionID,\n    Value:  sessionID,\n    MaxAge: cookieMaxAge,\n   })\n  } else if err != nil {\n   return\n  } else {\n   sessionID = c.Value\n  }\n  ctx := context.WithValue(r.Context(), ctxKeySessionID{}, sessionID)\n  r = r.WithContext(ctx)\n  next.ServeHTTP(w, r)\n }\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t\"net/http\"\n\t\"time\"\n\t\"os\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/sirupsen/logrus\"\n)"
        ],
        "reference_api": [
            "r.Cookie",
            "context.WithValue",
            "r.WithContext",
            "r.Context",
            "next.ServeHTTP",
            "u.String",
            "uuid.NewRandom",
            "os.Getenv",
            "http.SetCookie"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "r.Cookie",
            "context.WithValue",
            "r.WithContext",
            "r.Context",
            "next.ServeHTTP",
            "u.String",
            "uuid.NewRandom",
            "http.SetCookie"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "GoogleCloudPlatform/microservices-demo",
        "function_declaration": "loadAllProtos(protoRoot)",
        "start_line": "84",
        "end_line": "101",
        "file_path": "src/paymentservice/server.js",
        "docstring": "The loadAllProtos function loads and registers all necessary protocol buffer services to the gRPC server.\\nIt retrieves the HipsterShop and health packages from the predefined package structure.\\nThe function adds the PaymentService and its charge handler to the server.\\nIt also adds the Health service and its check handler to the server.\\nHandlers are bound to the current HipsterShopServer instance to ensure proper context.",
        "language": "JavaScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "efd297aa9fc2",
        "ground_truth": "loadAllProtos(protoRoot) {\n  const hipsterShopPackage = this.packages.hipsterShop.hipstershop;\n  const healthPackage = this.packages.health.grpc.health.v1;\n  this.server.addService(\n    hipsterShopPackage.PaymentService.service,\n    {\n      charge: HipsterShopServer.ChargeServiceHandler.bind(this)\n    }\n  );\n  this.server.addService(\n    healthPackage.Health.service,\n    {\n      check: HipsterShopServer.CheckHandler.bind(this)\n    }\n  );\n}",
        "import_statements": [],
        "reference_api": [
            "HipsterShopServer.ChargeServiceHandler.bind",
            "HipsterShopServer.CheckHandler.bind",
            "loadAllProtos",
            "this.server.addService"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "HipsterShopServer.ChargeServiceHandler.bind",
                "code": "static ChargeServiceHandler(call, callback) {\n    try {\n      logger.info(`PaymentService#Charge invoked with request ${JSON.stringify(call.request)}`);\n      const response = charge(call.request);\n      callback(null, response);\n    } catch (err) {\n      console.warn(err);\n      callback(err);\n    }\n  }"
            },
            {
                "name": "HipsterShopServer.CheckHandler.bind",
                "code": "static CheckHandler(call, callback) {\n    callback(null, { status: 'SERVING' });\n  }"
            },
            {
                "name": "loadAllProtos",
                "code": "loadAllProtos(protoRoot) {\n    const hipsterShopPackage = this.packages.hipsterShop.hipstershop;\n    const healthPackage = this.packages.health.grpc.health.v1;\n\n    this.server.addService(\n      hipsterShopPackage.PaymentService.service,\n      {\n        charge: HipsterShopServer.ChargeServiceHandler.bind(this)\n      }\n    );\n\n    this.server.addService(\n      healthPackage.Health.service,\n      {\n        check: HipsterShopServer.CheckHandler.bind(this)\n      }\n    );\n  }"
            }
        ],
        "third_party": [
            "this.server.addService"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def list_resources(provider: str, resource_type: str)",
        "start_line": "23",
        "end_line": "27",
        "file_path": "cli/cli.py",
        "docstring": "The list_resources function lists resources based on the provided resource_type for a given provider.\\nIf the resource_type is \"services,\" it calls list_services(provider) and prints the services using print_services().\\nIf the resource_type is \"fixers,\" it calls list_fixers(provider) and prints the fixers using print_fixers().\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e6d887877049",
        "ground_truth": "def list_resources(provider: str, resource_type: str):\n    if resource_type == \"services\":\n        print_services(list_services(provider))\n    elif resource_type == \"fixers\":\n        print_fixers(list_fixers(provider))",
        "import_statements": [
            "import typer",
            "from prowler.lib.banner import print_banner",
            "from prowler.lib.check.check import (\n    list_fixers,\n    list_services,\n    print_fixers,\n    print_services,\n)"
        ],
        "reference_api": [
            "list_services",
            "print_fixers",
            "list_fixers",
            "print_services"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "print_services",
                "code": "def print_services(service_list: set):\n    services_num = len(service_list)\n    plural_string = f\"\\nThere are {Fore.YELLOW}{services_num}{Style.RESET_ALL} available services.\\n\"\n    singular_string = (\n        f\"\\nThere is {Fore.YELLOW}{services_num}{Style.RESET_ALL} available service.\\n\"\n    )\n\n    message = plural_string if services_num > 1 else singular_string\n\n    for service in service_list:\n        print(f\"- {service}\")\n\n    print(message)"
            },
            {
                "name": "list_services",
                "code": "def list_services(provider: str) -> set:\n    available_services = set()\n    checks_tuple = recover_checks_from_provider(provider)\n    for _, check_path in checks_tuple:\n        # Format: /absolute_path/prowler/providers/{provider}/services/{service_name}/{check_name}\n        if os.name == \"nt\":\n            service_name = check_path.split(\"\\\\\")[-2]\n        else:\n            service_name = check_path.split(\"/\")[-2]\n        available_services.add(service_name)\n    return sorted(available_services)"
            },
            {
                "name": "print_fixers",
                "code": "def print_fixers(fixers_list: set):\n    fixers_num = len(fixers_list)\n    plural_string = (\n        f\"\\nThere are {Fore.YELLOW}{fixers_num}{Style.RESET_ALL} available fixers.\\n\"\n    )\n    singular_string = (\n        f\"\\nThere is {Fore.YELLOW}{fixers_num}{Style.RESET_ALL} available fixer.\\n\"\n    )\n\n    message = plural_string if fixers_num > 1 else singular_string\n\n    for service in fixers_list:\n        print(f\"- {service}\")\n\n    print(message)"
            },
            {
                "name": "list_fixers",
                "code": "def list_fixers(provider: str) -> set:\n    available_fixers = set()\n    checks = recover_checks_from_provider(provider, include_fixers=True)\n    # Build list of check's metadata files\n    for check_info in checks:\n        # Build check path name\n        check_name = check_info[0]\n        # Ignore non fixer files\n        if not check_name.endswith(\"_fixer\"):\n            continue\n        # Remove _fixer suffix\n        check_name = check_name.replace(\"_fixer\", \"\")\n        available_fixers.add(check_name)\n    return sorted(available_fixers)"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def create_date_dropdown_compliance(assesment_times: list) -> html.Div",
        "start_line": "43",
        "end_line": "67",
        "file_path": "dashboard/lib/dropdowns.py",
        "docstring": "The create_date_dropdown_compliance function creates an HTML division containing a dropdown menu for selecting assessment dates.\\nIt takes a list of assessment times as an argument and returns a styled html.Div component.\\nThe component includes a label \"Assessment Date:\" with specific CSS classes for styling.\\nIt also includes a dropdown menu (dcc.Dropdown) with options derived from the assessment times list.\\nThe dropdown menu is non-clearable, single-select, and styled with a black text color and full-width.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ef69811a7354",
        "ground_truth": "def create_date_dropdown_compliance(assesment_times: list) -> html.Div:\n    \"\"\"\n    Dropdown to select the date of the last available scan for each account.\n    Args:\n        assesment_times (list): List of dates of the last available scan for each account.\n    Returns:\n        html.Div: Dropdown to select the date of the last available scan for each account.\n    \"\"\"\n    return html.Div(\n        [\n            html.Label(\n                \"Assesment Date:\", className=\"text-prowler-stone-900 font-bold text-sm\"\n            ),\n            dcc.Dropdown(\n                id=\"date-filter-analytics\",\n                options=[\n                    {\"label\": account, \"value\": account} for account in assesment_times\n                ],\n                value=assesment_times[0],\n                clearable=False,\n                multi=False,\n                style={\"color\": \"#000000\", \"width\": \"100%\"},\n            ),\n        ],\n    )",
        "import_statements": [
            "from dash import dcc, html"
        ],
        "reference_api": [
            "html.Div",
            "html.Label",
            "dcc.Dropdown"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "dcc.Dropdown"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def load_csv_files(files)",
        "start_line": "244",
        "end_line": "252",
        "file_path": "dashboard/pages/compliance.py",
        "docstring": "The load_csv_files function loads multiple CSV files into a single pandas DataFrame.\\nIt initializes an empty list to store DataFrames.\\nFor each file in the provided list, it reads the CSV file using pandas read_csv with a semicolon separator, skipping bad lines, and using a specified encoding format.\\nThe data is then converted to strings and appended to the list.\\nFinally, the function concatenates all DataFrames in the list into a single DataFrame, ignoring the index, and returns the result.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "37c03310774a",
        "ground_truth": "def load_csv_files(files):\n    \"\"\"Load CSV files into a single pandas DataFrame.\"\"\"\n    dfs = []\n    for file in files:\n        df = pd.read_csv(\n            file, sep=\";\", on_bad_lines=\"skip\", encoding=encoding_format\n        )\n        dfs.append(df.astype(str))\n    return pd.concat(dfs, ignore_index=True)",
        "import_statements": [
            "import csv",
            "import glob",
            "import importlib",
            "import os",
            "import re",
            "import warnings",
            "import dash",
            "from dash import callback, dcc, html",
            "from dash.dependencies import Input, Output",
            "from dashboard.config import (\n    encoding_format,\n    error_action,\n    fail_color,\n    folder_path_compliance,\n    info_color,\n    manual_color,\n    pass_color,\n)",
            "from dashboard.lib.dropdowns import (\n    create_account_dropdown_compliance,\n    create_compliance_dropdown,\n    create_date_dropdown_compliance,\n    create_region_dropdown_compliance,\n)",
            "from dashboard.lib.layouts import create_layout_compliance",
            "from prowler.lib.logger import logger"
        ],
        "reference_api": [
            "pd.concat",
            "dfs.append",
            "pd.read_csv",
            "df.astype"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "pd.read_csv",
            "dfs.append",
            "df.astype",
            "pd.concat"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def toggle_collapse(n_clicks, is_open)",
        "start_line": "1394",
        "end_line": "1400",
        "file_path": "dashboard/pages/overview.py",
        "docstring": "The toggle_collapse function toggles the state of a collapsible element based on click events.\\nIt initializes the click count (n_clicks) to 0 if not provided.\\nIt identifies the element that triggered the callback by examining the callback context.\\nIt extracts the index of the triggered element and toggles its state in the is_open list.\\nFinally, it returns the updated is_open list reflecting the toggled state of the collapsible elements.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "765f09656877",
        "ground_truth": "def toggle_collapse(n_clicks, is_open):\n    n_clicks = n_clicks or 0\n    triggered = callback_context.triggered[0][\"prop_id\"].split(\".\")[0]\n    if triggered:\n        idx = json.loads(triggered)[\"index\"]\n        is_open[idx] = not is_open[idx]\n    return is_open",
        "import_statements": [
            "import csv",
            "import glob",
            "import json",
            "import os",
            "import warnings",
            "from datetime import datetime, timedelta",
            "from itertools import product",
            "import dash",
            "from dash import callback, callback_context, ctx, dcc, html",
            "from dash.dependencies import Input, Output",
            "from dashboard.config import (\n    critical_color,\n    encoding_format,\n    fail_color,\n    folder_path_overview,\n    high_color,\n    info_color,\n    informational_color,\n    low_color,\n    manual_color,\n    medium_color,\n    muted_fail_color,\n    muted_manual_color,\n    muted_pass_color,\n    pass_color,\n)",
            "from dashboard.lib.cards import create_provider_card",
            "from dashboard.lib.dropdowns import (\n    create_account_dropdown,\n    create_date_dropdown,\n    create_region_dropdown,\n    create_service_dropdown,\n    create_severity_dropdown,\n    create_status_dropdown,\n    create_table_row_dropdown,\n)",
            "from dashboard.lib.layouts import create_layout_overview"
        ],
        "reference_api": [
            "split",
            "json.loads"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "split"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def parse_checks_from_file(input_file: str, provider: str) -> set",
        "start_line": "111",
        "end_line": "125",
        "file_path": "prowler/lib/check/check.py",
        "docstring": "The function parse_checks_from_file reads and returns a set of checks from a specified JSON file for a given provider.\\nIt opens the input file, parses it as JSON, and retrieves the checks associated with the provider, adding them to a set.\\nIf any exception occurs during this process, it logs the error with its class name and the line number where it occurred.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "749062ce1b8b",
        "ground_truth": "def parse_checks_from_file(input_file: str, provider: str) -> set:\n    \"\"\"parse_checks_from_file returns a set of checks read from the given file\"\"\"\n    try:\n        checks_to_execute = set()\n        with open_file(input_file) as f:\n            json_file = parse_json_file(f)\n         for check_name in json_file[provider]:\n            checks_to_execute.add(check_name)\n         return checks_to_execute\n    except Exception as error:\n        logger.error(\n            f\"{error.__class__.__name__}[{error.__traceback__.tb_lineno}] -- {error}\"\n        )",
        "import_statements": [
            "import functools",
            "import importlib",
            "import json",
            "import os",
            "import re",
            "import shutil",
            "import sys",
            "import traceback",
            "from pkgutil import walk_packages",
            "from types import ModuleType",
            "from typing import Any",
            "from alive_progress import alive_bar",
            "from colorama import Fore, Style",
            "import prowler",
            "from prowler.config.config import orange_color",
            "from prowler.lib.check.compliance_models import load_compliance_framework",
            "from prowler.lib.check.custom_checks_metadata import update_check_metadata",
            "from prowler.lib.check.models import Check, load_check_metadata",
            "from prowler.lib.logger import logger",
            "from prowler.lib.mutelist.mutelist import mutelist_findings",
            "from prowler.lib.outputs.outputs import report",
            "from prowler.lib.utils.utils import open_file, parse_json_file, print_boxes",
            "from prowler.providers.common.models import Audit_Metadata"
        ],
        "reference_api": [
            "open_file",
            "set",
            "checks_to_execute.add",
            "logger.error",
            "parse_json_file"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "open_file",
                "code": "def open_file(input_file: str, mode: str = \"r\") -> TextIOWrapper:\n    \"\"\"open_file returns a handler to the file using the specified mode.\"\"\"\n    try:\n        f = open(input_file, mode, encoding=enconding_format_utf_8)\n    except OSError as os_error:\n        if os_error.strerror == \"Too many open files\":\n            logger.critical(\n                \"Ooops! You reached your user session maximum open files. To solve this issue, increase the shell session limit by running this command `ulimit -n 4096`. For more info visit https://docs.prowler.cloud/en/latest/troubleshooting/\"\n            )\n        else:\n            logger.critical(\n                f\"{input_file}: OSError[{os_error.errno}] {os_error.strerror}\"\n            )\n        sys.exit(1)\n    except Exception as e:\n        logger.critical(\n            f\"{input_file}: {e.__class__.__name__}[{e.__traceback__.tb_lineno}]\"\n        )\n        sys.exit(1)\n    else:\n        return f"
            },
            {
                "name": "parse_json_file",
                "code": "def parse_json_file(input_file: TextIOWrapper) -> dict:\n    \"\"\"parse_json_file loads a JSON file and returns a dictionary with the JSON content.\"\"\"\n    try:\n        json_file = json.load(input_file)\n    except Exception as e:\n        logger.critical(\n            f\"{input_file.name}: {e.__class__.__name__}[{e.__traceback__.tb_lineno}]\"\n        )\n        sys.exit(1)\n    else:\n        return json_file"
            }
        ],
        "third_party": [
            "checks_to_execute.add"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def list_services(provider: str) -> set",
        "start_line": "195",
        "end_line": "205",
        "file_path": "prowler/lib/check/check.py",
        "docstring": "The list_services function identifies and lists available services for a given provider.\\nIt initializes an empty set to store the service names.\\nThe function retrieves checks associated with the provider by calling recover_checks_from_provider.\\nIt then iterates over the retrieved check paths.\\nDepending on the operating system, it extracts the service name from the check path by splitting the string.\\nThe service name is added to the set of available services.\\nFinally, the function returns the sorted list of unique service names.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d524bccbd262",
        "ground_truth": "def list_services(provider: str) -> set:\n    available_services = set()\n    checks_tuple = recover_checks_from_provider(provider)\n    for _, check_path in checks_tuple:\n        # Format: /absolute_path/prowler/providers/{provider}/services/{service_name}/{check_name}\n        if os.name == \"nt\":\n            service_name = check_path.split(\"\\\\\")[-2]\n        else:\n            service_name = check_path.split(\"/\")[-2]\n        available_services.add(service_name)\n    return sorted(available_services)",
        "import_statements": [
            "import functools",
            "import importlib",
            "import json",
            "import os",
            "import re",
            "import shutil",
            "import sys",
            "import traceback",
            "from pkgutil import walk_packages",
            "from types import ModuleType",
            "from typing import Any",
            "from alive_progress import alive_bar",
            "from colorama import Fore, Style",
            "import prowler",
            "from prowler.config.config import orange_color",
            "from prowler.lib.check.compliance_models import load_compliance_framework",
            "from prowler.lib.check.custom_checks_metadata import update_check_metadata",
            "from prowler.lib.check.models import Check, load_check_metadata",
            "from prowler.lib.logger import logger",
            "from prowler.lib.mutelist.mutelist import mutelist_findings",
            "from prowler.lib.outputs.outputs import report",
            "from prowler.lib.utils.utils import open_file, parse_json_file, print_boxes",
            "from prowler.providers.common.models import Audit_Metadata"
        ],
        "reference_api": [
            "recover_checks_from_provider",
            "sorted",
            "check_path.split",
            "available_services.add",
            "set"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "recover_checks_from_provider",
                "code": "def recover_checks_from_provider(\n    provider: str, service: str = None, include_fixers: bool = False\n) -> list[tuple]:\n    \"\"\"\n    Recover all checks from the selected provider and service\n\n    Returns a list of tuples with the following format (check_name, check_path)\n    \"\"\"\n    try:\n        checks = []\n        modules = list_modules(provider, service)\n        for module_name in modules:\n            # Format: \"prowler.providers.{provider}.services.{service}.{check_name}.{check_name}\"\n            check_module_name = module_name.name\n            # We need to exclude common shared libraries in services\n            if (\n                check_module_name.count(\".\") == 6\n                and \"lib\" not in check_module_name\n                and (not check_module_name.endswith(\"_fixer\") or include_fixers)\n            ):\n                check_path = module_name.module_finder.path\n                # Check name is the last part of the check_module_name\n                check_name = check_module_name.split(\".\")[-1]\n                check_info = (check_name, check_path)\n                checks.append(check_info)\n    except ModuleNotFoundError:\n        logger.critical(f\"Service {service} was not found for the {provider} provider.\")\n        sys.exit(1)\n    except Exception as e:\n        logger.critical(f\"{e.__class__.__name__}[{e.__traceback__.tb_lineno}]: {e}\")\n        sys.exit(1)\n    else:\n        return checks"
            }
        ],
        "third_party": [
            "check_path.split",
            "check_path.split",
            "available_services.add"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def parse_checks_from_compliance_framework(\n    compliance_frameworks: list, bulk_compliance_frameworks: dict\n) -> list",
        "start_line": "352",
        "end_line": "374",
        "file_path": "prowler/lib/check/check.py",
        "docstring": "The parse_checks_from_compliance_framework function extracts checks from specified compliance frameworks.\\nIt takes a list of compliance frameworks and a dictionary of bulk compliance frameworks as inputs.\\nIt initializes an empty set checks_to_execute to store unique checks.\\nFor each framework in compliance_frameworks, it gathers the list of checks from the Requirements section in bulk_compliance_frameworks.\\nIt reduces the nested lists of checks into a single list and merges this list with checks_to_execute.\\nIf an exception occurs, it logs the error with its class name and line number.\\nFinally, it returns the set of checks to be executed.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "70a3e2ea3468",
        "ground_truth": "def parse_checks_from_compliance_framework(\n    compliance_frameworks: list, bulk_compliance_frameworks: dict\n) -> list:\n    \"\"\"parse_checks_from_compliance_framework returns a set of checks from the given compliance_frameworks\"\"\"\n    checks_to_execute = set()\n    try:\n        for framework in compliance_frameworks:\n            # compliance_framework_json[\"Requirements\"][*][\"Checks\"]\n            compliance_framework_checks_list = [\n                requirement.Checks\n                for requirement in bulk_compliance_frameworks[framework].Requirements\n            ]\n            # Reduce nested list into a list\n            # Pythonic functional magic\n            compliance_framework_checks = functools.reduce(\n                lambda x, y: x + y, compliance_framework_checks_list\n            )\n            # Then union this list of checks with the initial one\n            checks_to_execute = checks_to_execute.union(compliance_framework_checks)\n    except Exception as e:\n        logger.error(f\"{e.__class__.__name__}[{e.__traceback__.tb_lineno}] -- {e}\")\n     return checks_to_execute",
        "import_statements": [
            "import functools",
            "import importlib",
            "import json",
            "import os",
            "import re",
            "import shutil",
            "import sys",
            "import traceback",
            "from pkgutil import walk_packages",
            "from types import ModuleType",
            "from typing import Any",
            "from alive_progress import alive_bar",
            "from colorama import Fore, Style",
            "import prowler",
            "from prowler.config.config import orange_color",
            "from prowler.lib.check.compliance_models import load_compliance_framework",
            "from prowler.lib.check.custom_checks_metadata import update_check_metadata",
            "from prowler.lib.check.models import Check, load_check_metadata",
            "from prowler.lib.logger import logger",
            "from prowler.lib.mutelist.mutelist import mutelist_findings",
            "from prowler.lib.outputs.outputs import report",
            "from prowler.lib.utils.utils import open_file, parse_json_file, print_boxes",
            "from prowler.providers.common.models import Audit_Metadata"
        ],
        "reference_api": [
            "checks_to_execute.union",
            "logger.error",
            "functools.reduce",
            "set"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "checks_to_execute.union"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def recover_checks_from_service(service_list: list, provider: str) -> set",
        "start_line": "776",
        "end_line": "804",
        "file_path": "prowler/lib/check/check.py",
        "docstring": "The recover_checks_from_service function retrieves a set of checks for a given list of services and a provider.\\nIt first normalizes the service names by replacing \"lambda\" with \"awslambda\".\\nFor each service in the list, it calls recover_checks_from_provider to get the checks for that service.\\nIf no checks are found for a service, it logs an error message.\\nAll retrieved checks are added to a set, which is returned at the end.\\nIf an exception occurs during the process, it logs the error details.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "322576278730",
        "ground_truth": "def recover_checks_from_service(service_list: list, provider: str) -> set:\n    \"\"\"\n    Recover all checks from the selected provider and service\n     Returns a set of checks from the given services\n    \"\"\"\n    try:\n        checks = set()\n        service_list = [\n            \"awslambda\" if service == \"lambda\" else service for service in service_list\n        ]\n        for service in service_list:\n            service_checks = recover_checks_from_provider(provider, service)\n            if not service_checks:\n                logger.error(f\"Service '{service}' does not have checks.\")\n             else:\n                for check in service_checks:\n                    # Recover check name and module name from import path\n                    # Format: \"providers.{provider}.services.{service}.{check_name}.{check_name}\"\n                    check_name = check[0].split(\".\")[-1]\n                    # If the service is present in the group list passed as parameters\n                    # if service_name in group_list: checks_from_arn.add(check_name)\n                    checks.add(check_name)\n        return checks\n    except Exception as error:\n        logger.error(\n            f\"{error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}\"\n        )",
        "import_statements": [
            "import functools",
            "import importlib",
            "import json",
            "import os",
            "import re",
            "import shutil",
            "import sys",
            "import traceback",
            "from pkgutil import walk_packages",
            "from types import ModuleType",
            "from typing import Any",
            "from alive_progress import alive_bar",
            "from colorama import Fore, Style",
            "import prowler",
            "from prowler.config.config import orange_color",
            "from prowler.lib.check.compliance_models import load_compliance_framework",
            "from prowler.lib.check.custom_checks_metadata import update_check_metadata",
            "from prowler.lib.check.models import Check, load_check_metadata",
            "from prowler.lib.logger import logger",
            "from prowler.lib.mutelist.mutelist import mutelist_findings",
            "from prowler.lib.outputs.outputs import report",
            "from prowler.lib.utils.utils import open_file, parse_json_file, print_boxes",
            "from prowler.providers.common.models import Audit_Metadata"
        ],
        "reference_api": [
            "recover_checks_from_provider",
            "checks.add",
            "set",
            "split",
            "logger.error"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "recover_checks_from_provider",
                "code": "def recover_checks_from_provider(\n    provider: str, service: str = None, include_fixers: bool = False\n) -> list[tuple]:\n    \"\"\"\n    Recover all checks from the selected provider and service\n\n    Returns a list of tuples with the following format (check_name, check_path)\n    \"\"\"\n    try:\n        checks = []\n        modules = list_modules(provider, service)\n        for module_name in modules:\n            # Format: \"prowler.providers.{provider}.services.{service}.{check_name}.{check_name}\"\n            check_module_name = module_name.name\n            # We need to exclude common shared libraries in services\n            if (\n                check_module_name.count(\".\") == 6\n                and \"lib\" not in check_module_name\n                and (not check_module_name.endswith(\"_fixer\") or include_fixers)\n            ):\n                check_path = module_name.module_finder.path\n                # Check name is the last part of the check_module_name\n                check_name = check_module_name.split(\".\")[-1]\n                check_info = (check_name, check_path)\n                checks.append(check_info)\n    except ModuleNotFoundError:\n        logger.critical(f\"Service {service} was not found for the {provider} provider.\")\n        sys.exit(1)\n    except Exception as e:\n        logger.critical(f\"{e.__class__.__name__}[{e.__traceback__.tb_lineno}]: {e}\")\n        sys.exit(1)\n    else:\n        return checks"
            }
        ],
        "third_party": [
            "split",
            "checks.add"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def load_compliance_framework(\n    compliance_specification_file: str,\n) -> Compliance_Base_Model",
        "start_line": "219",
        "end_line": "233",
        "file_path": "prowler/lib/check/compliance_models.py",
        "docstring": "The load_compliance_framework function loads and parses a compliance framework specification from a given file.\\nIt takes the file path as an argument and attempts to parse it using the ComplianceBaseModel.\\nIf a ValidationError occurs, it logs a critical error message and exits the program with status 1.\\nIf the parsing is successful, it returns the parsed compliance framework.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8ec19d148eee",
        "ground_truth": "def load_compliance_framework(\n    compliance_specification_file: str,\n) -> ComplianceBaseModel:\n    \"\"\"load_compliance_framework loads and parse a Compliance Framework Specification\"\"\"\n    try:\n        compliance_framework = ComplianceBaseModel.parse_file(\n            compliance_specification_file\n        )\n    except ValidationError as error:\n        logger.critical(\n            f\"Compliance Framework Specification from {compliance_specification_file} is not valid: {error}\"\n        )\n        sys.exit(1)\n    else:\n        return compliance_framework",
        "import_statements": [
            "import sys",
            "from enum import Enum",
            "from typing import Optional, Union",
            "from pydantic import BaseModel, ValidationError, root_validator",
            "from prowler.lib.logger import logger"
        ],
        "reference_api": [
            "logger.critical",
            "ComplianceBaseModel.parse_file",
            "sys.exit"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ComplianceBaseModel.parse_file"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def update_checks_metadata(bulk_checks_metadata, custom_checks_metadata)",
        "start_line": "95",
        "end_line": "110",
        "file_path": "prowler/lib/check/custom_checks_metadata.py",
        "docstring": "The update_checks_metadata function updates the bulk_checks_metadata dictionary with custom metadata provided in custom_checks_metadata.\\nIt iterates through each check in the custom_checks_metadata and updates the corresponding entry in the bulk_checks_metadata using the update_check_metadata function.\\nIf an error occurs during the update process, it logs the error and exits the program.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c16ab0cbb95a",
        "ground_truth": "def update_checks_metadata(bulk_checks_metadata, custom_checks_metadata):\n    \"\"\"update_checks_metadata returns the bulk_checks_metadata with the check's metadata updated based on the custom_checks_metadata provided.\"\"\"\n    try:\n        # Update checks metadata from CustomChecksMetadata file\n        for check, custom_metadata in custom_checks_metadata[\"Checks\"].items():\n            check_metadata = bulk_checks_metadata.get(check)\n            if check_metadata:\n                bulk_checks_metadata[check] = update_check_metadata(\n                    check_metadata, custom_metadata\n                )\n        return bulk_checks_metadata\n    except Exception as error:\n        logger.critical(\n            f\"{error.__class__.__name__} -- {error}[{error.__traceback__.tb_lineno}]\"\n        )\n        sys.exit(1)",
        "import_statements": [
            "import sys",
            "import yaml",
            "from jsonschema import validate",
            "from prowler.config.config import valid_severities",
            "from prowler.lib.logger import logger"
        ],
        "reference_api": [
            "bulk_checks_metadata.get",
            "logger.critical",
            "sys.exit",
            "items",
            "update_check_metadata"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "update_check_metadata",
                "code": "def update_check_metadata(check_metadata, custom_metadata):\n    \"\"\"update_check_metadata updates the check_metadata fields present in the custom_metadata and returns the updated version of the check_metadata. If some field is not present or valid the check_metadata is returned with the original fields.\"\"\"\n    try:\n        if custom_metadata:\n            for attribute in custom_metadata:\n                if attribute == \"Remediation\":\n                    for remediation_attribute in custom_metadata[attribute]:\n                        update_check_metadata_remediation(\n                            check_metadata,\n                            custom_metadata,\n                            attribute,\n                            remediation_attribute,\n                        )\n                else:\n                    try:\n                        setattr(check_metadata, attribute, custom_metadata[attribute])\n                    except ValueError:\n                        pass\n    finally:\n        return check_metadata"
            }
        ],
        "third_party": [
            "items",
            "bulk_checks_metadata.get"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def validate_mutelist(mutelist: dict) -> dict",
        "start_line": "23",
        "end_line": "31",
        "file_path": "prowler/lib/mutelist/mutelist.py",
        "docstring": "The validate_mutelist function validates a given mutelist dictionary against a predefined schema using mutelist_schema.\\nIf validation is successful, it returns the validated mutelist.\\nIf an error occurs during validation, it logs the error with details including the error type and line number where the error occurred, and returns an empty dictionary.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5d07ff9da55b",
        "ground_truth": "def validate_mutelist(mutelist: dict) -> dict:\n    try:\n        mutelist = mutelist_schema.validate(mutelist)\n        return mutelist\n    except Exception as error:\n        logger.error(\n            f\"{error.__class__.__name__} -- Mutelist YAML is malformed - {error}[{error.__traceback__.tb_lineno}]\"\n        )\n        return {}",
        "import_statements": [
            "import re",
            "from typing import Any",
            "import yaml",
            "from prowler.lib.logger import logger",
            "from prowler.lib.mutelist.models import mutelist_schema",
            "from prowler.lib.outputs.utils import unroll_tags"
        ],
        "reference_api": [
            "mutelist_schema.validate",
            "logger.error"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "mutelist_schema.validate"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def is_muted_in_tags(muted_tags, finding_tags) -> bool",
        "start_line": "242",
        "end_line": "259",
        "file_path": "prowler/lib/mutelist/mutelist.py",
        "docstring": "The function is_muted_in_tags checks if any tags in finding_tags match those in muted_tags by calling the helper function __is_item_matched__. If an exception occurs during the matching process, it logs the error's class name, message, and line number before returning False.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "45507d572f10",
        "ground_truth": "def is_muted_in_tags(muted_tags, finding_tags) -> bool:\n    \"\"\"\n    Check if any of the muted tags are present in the finding tags.\n     Args:\n        muted_tags (list): List of muted tags to be checked.\n        finding_tags (str): String containing tags to search for muted tags.\n     Returns:\n        bool: True if any of the muted tags are present in the finding tags, otherwise False.\n    \"\"\"\n    try:\n        return __is_item_matched__(muted_tags, finding_tags)\n    except Exception as error:\n        logger.error(\n            f\"{error.__class__.__name__} -- {error}[{error.__traceback__.tb_lineno}]\"\n        )\n        return False",
        "import_statements": [
            "import re",
            "from typing import Any",
            "import yaml",
            "from prowler.lib.logger import logger",
            "from prowler.lib.mutelist.models import mutelist_schema",
            "from prowler.lib.outputs.utils import unroll_tags"
        ],
        "reference_api": [
            "__is_item_matched__",
            "logger.error"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "__is_item_matched__",
                "code": "def __is_item_matched__(matched_items, finding_items):\n    \"\"\"\n    Check if any of the items in matched_items are present in finding_items.\n\n    Args:\n        matched_items (list): List of items to be matched.\n        finding_items (str): String to search for matched items.\n\n    Returns:\n        bool: True if any of the matched_items are present in finding_items, otherwise False.\n    \"\"\"\n    try:\n        is_item_matched = False\n        if matched_items and (finding_items or finding_items == \"\"):\n            for item in matched_items:\n                if item.startswith(\"*\"):\n                    item = \".*\" + item[1:]\n                if re.search(item, finding_items):\n                    is_item_matched = True\n                    break\n        return is_item_matched\n    except Exception as error:\n        logger.error(\n            f\"{error.__class__.__name__} -- {error}[{error.__traceback__.tb_lineno}]\"\n        )\n        return False"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def batch_write_data_to_file(self) -> None",
        "start_line": "30",
        "end_line": "50",
        "file_path": "prowler/lib/outputs/csv/models.py",
        "docstring": "The batch_write_data_to_file function writes collected data to a file in CSV format.\\nIt checks if a valid file descriptor is available and not closed, and if data exists.\\nUsing a CSV DictWriter, it writes the header and each row of data to the file.\\nAfter writing, it closes the file descriptor.\\nIf any exceptions occur during this process, they are logged with an error message including the exception type and line number.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fbab2e8bc8ab",
        "ground_truth": "def batch_write_data_to_file(self) -> None:\n    \"\"\"Writes the findings to a file using the CSV format using the `Output._file_descriptor`.\"\"\"\n    try:\n        if (\n            getattr(self, \"_file_descriptor\", None)\n            and not self._file_descriptor.closed\n            and self._data\n        ):\n            csv_writer = DictWriter(\n                self._file_descriptor,\n                fieldnames=self._data[0].keys(),\n                delimiter=\";\",\n            )\n            csv_writer.writeheader()\n            for finding in self._data:\n                csv_writer.writerow(finding)\n            self._file_descriptor.close()\n    except Exception as error:\n        logger.error(\n            f\"{error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}\"\n        )",
        "import_statements": [
            "from csv import DictWriter",
            "from prowler.lib.logger import logger",
            "from prowler.lib.outputs.finding import Finding",
            "from prowler.lib.outputs.output import Output",
            "from prowler.lib.outputs.utils import unroll_dict, unroll_list"
        ],
        "reference_api": [
            "getattr",
            "keys",
            "csv_writer.writeheader",
            "DictWriter",
            "csv_writer.writerow",
            "close",
            "logger.error"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "DictWriter",
            "keys",
            "csv_writer.writeheader",
            "csv_writer.writerow",
            "close"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def send(self, stats: dict, args: str) -> SlackResponse",
        "start_line": "28",
        "end_line": "53",
        "file_path": "prowler/lib/outputs/slack/slack.py",
        "docstring": "The send function sends a message to a Slack channel using the Slack API.\\nIt initializes a WebClient with an authentication token.\\nIt creates the message identity and logo by calling __create_message_identity__ with the provider.\\nIt attempts to post the message using chat_postMessage, setting the username to \"Prowler\" and icon_url to square_logo_img.\\nThe channel is specified, and message blocks are created using __create_message_blocks__ with identity, logo, stats, and args.\\nIf successful, it returns the response from Slack.\\nIf an exception occurs, it logs the error with the class name, line number, and error message, returning the error object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b4f09cba6b88",
        "ground_truth": "def send(self, stats: dict, args: str) -> SlackResponse:\n    \"\"\"\n    Sends the findings to Slack.\n    Args:\n        stats (dict): A dictionary containing audit statistics.\n        args (str): Command line arguments used for the audit.\n    Returns:\n        SlackResponse: Slack response if successful, error object if an exception occurs.\n    \"\"\"\n    try:\n        client = WebClient(token=self.token)\n        identity, logo = self.__create_message_identity__(self._provider)\n        response = client.chat_postMessage(\n            username=\"Prowler\",\n            icon_url=square_logo_img,\n            channel=f\"#{self.channel}\",\n            blocks=self.__create_message_blocks__(identity, logo, stats, args),\n        )\n        return response\n    except Exception as error:\n        logger.error(\n            f\"{error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}\"\n        )\n        return error",
        "import_statements": [
            "from typing import Any",
            "from slack_sdk import WebClient",
            "from slack_sdk.web.base_client import SlackResponse",
            "from prowler.config.config import aws_logo, azure_logo, gcp_logo, square_logo_img",
            "from prowler.lib.logger import logger"
        ],
        "reference_api": [
            "self.__create_message_identity__",
            "self.__create_message_blocks__",
            "client.chat_postMessage",
            "logger.error",
            "WebClient"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.__create_message_identity__",
                "code": "def __create_message_identity__(self, provider: Any):\n        \"\"\"\n        Create a Slack message identity based on the provider type.\n\n        Parameters:\n        - provider (Provider): The Provider (e.g. \"AwsProvider\", \"GcpProvider\", \"AzureProvide\").\n\n        Returns:\n        - identity (str): The message identity based on the provider type.\n        - logo (str): The logo URL associated with the provider type.\n        \"\"\"\n\n        # TODO: support kubernetes\n        try:\n            identity = \"\"\n            logo = aws_logo\n            if provider.type == \"aws\":\n                identity = f\"AWS Account *{provider.identity.account}*\"\n            elif provider.type == \"gcp\":\n                identity = f\"GCP Projects *{', '.join(provider.project_ids)}*\"\n                logo = gcp_logo\n            elif provider.type == \"azure\":\n                printed_subscriptions = []\n                for key, value in provider.identity.subscriptions.items():\n                    intermediate = f\"- *{key}: {value}*\\n\"\n                    printed_subscriptions.append(intermediate)\n                identity = f\"Azure Subscriptions:\\n{''.join(printed_subscriptions)}\"\n                logo = azure_logo\n            return identity, logo\n        except Exception as error:\n            logger.error(\n                f\"{error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}\"\n            )"
            },
            {
                "name": "self.__create_message_blocks__",
                "code": "def __create_message_blocks__(self, identity, logo, stats, args) -> list:\n        \"\"\"\n        Create the Slack message blocks.\n\n        Args:\n            identity: message identity.\n            logo: logo URL.\n            stats: audit statistics.\n            args: command line arguments used.\n\n        Returns:\n            list: list of Slack message blocks.\n        \"\"\"\n        try:\n            blocks = [\n                {\n                    \"type\": \"section\",\n                    \"text\": {\n                        \"type\": \"mrkdwn\",\n                        \"text\": self.__create_title__(identity, stats),\n                    },\n                    \"accessory\": {\n                        \"type\": \"image\",\n                        \"image_url\": logo,\n                        \"alt_text\": \"Provider Logo\",\n                    },\n                },\n                {\"type\": \"divider\"},\n                {\n                    \"type\": \"section\",\n                    \"text\": {\n                        \"type\": \"mrkdwn\",\n                        \"text\": f\"\\n:white_check_mark: *{stats['total_pass']} Passed findings* ({round(stats['total_pass'] / stats['findings_count'] * 100 , 2)}%)\\n\",\n                    },\n                },\n                {\n                    \"type\": \"section\",\n                    \"text\": {\n                        \"type\": \"mrkdwn\",\n                        \"text\": f\"\\n:x: *{stats['total_fail']} Failed findings* ({round(stats['total_fail'] / stats['findings_count'] * 100 , 2)}%)\\n \",\n                    },\n                },\n                {\n                    \"type\": \"section\",\n                    \"text\": {\n                        \"type\": \"mrkdwn\",\n                        \"text\": f\"\\n:bar_chart: *{stats['resources_count']} Scanned Resources*\\n\",\n                    },\n                },\n                {\"type\": \"divider\"},\n                {\n                    \"type\": \"context\",\n                    \"elements\": [\n                        {\n                            \"type\": \"mrkdwn\",\n                            \"text\": f\"Used parameters: `prowler {args}`\",\n                        }\n                    ],\n                },\n                {\"type\": \"divider\"},\n                {\n                    \"type\": \"section\",\n                    \"text\": {\"type\": \"mrkdwn\", \"text\": \"Join our Slack Community!\"},\n                    \"accessory\": {\n                        \"type\": \"button\",\n                        \"text\": {\"type\": \"plain_text\", \"text\": \"Prowler :slack:\"},\n                        \"url\": \"https://join.slack.com/t/prowler-workspace/shared_invite/zt-1hix76xsl-2uq222JIXrC7Q8It~9ZNog\",\n                    },\n                },\n                {\n                    \"type\": \"section\",\n                    \"text\": {\n                        \"type\": \"mrkdwn\",\n                        \"text\": \"Feel free to contact us in our repo\",\n                    },\n                    \"accessory\": {\n                        \"type\": \"button\",\n                        \"text\": {\"type\": \"plain_text\", \"text\": \"Prowler :github:\"},\n                        \"url\": \"https://github.com/prowler-cloud/prowler\",\n                    },\n                },\n                {\n                    \"type\": \"section\",\n                    \"text\": {\n                        \"type\": \"mrkdwn\",\n                        \"text\": \"See all the things you can do with ProwlerPro\",\n                    },\n                    \"accessory\": {\n                        \"type\": \"button\",\n                        \"text\": {\"type\": \"plain_text\", \"text\": \"Prowler Pro\"},\n                        \"url\": \"https://prowler.pro\",\n                    },\n                },\n            ]\n            return blocks\n        except Exception as error:\n            logger.error(\n                f\"{error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}\"\n            )"
            }
        ],
        "third_party": [
            "WebClient",
            "client.chat_postMessage"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def __create_message_identity__(self, provider: Any)",
        "start_line": "55",
        "end_line": "87",
        "file_path": "prowler/lib/outputs/slack/slack.py",
        "docstring": "The function __create_message_identity__ generates an identity message and logo based on the provided cloud service provider.\\nFor AWS, it constructs a message with the AWS account number and sets the logo to aws_logo.\\nFor GCP, it lists project IDs and sets the logo to gcp_logo.\\nFor Azure, it formats a list of subscription IDs and names, setting the logo to azure_logo.\\nIn case of an exception, it logs the error type, line number, and message.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "455762d0e663",
        "ground_truth": "def __create_message_identity__(self, provider: Any):\n    \"\"\"\n    Create a Slack message identity based on the provider type.\n    Parameters:\n    - provider (Provider): The Provider (e.g. \"AwsProvider\", \"GcpProvider\", \"AzureProvide\").\n    Returns:\n    - identity (str): The message identity based on the provider type.\n    - logo (str): The logo URL associated with the provider type.\n    \"\"\"\n    # TODO: support kubernetes\n    try:\n        identity = \"\"\n        logo = aws_logo\n        if provider.type == \"aws\":\n            identity = f\"AWS Account *{provider.identity.account}*\"\n        elif provider.type == \"gcp\":\n            identity = f\"GCP Projects *{', '.join(provider.project_ids)}*\"\n            logo = gcp_logo\n        elif provider.type == \"azure\":\n            printed_subscriptions = []\n            for key, value in provider.identity.subscriptions.items():\n                intermediate = f\"- *{key}: {value}*\\n\"\n                printed_subscriptions.append(intermediate)\n            identity = f\"Azure Subscriptions:\\n{''.join(printed_subscriptions)}\"\n            logo = azure_logo\n        return identity, logo\n    except Exception as error:\n        logger.error(\n            f\"{error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}\"\n        )",
        "import_statements": [
            "from typing import Any",
            "from slack_sdk import WebClient",
            "from slack_sdk.web.base_client import SlackResponse",
            "from prowler.config.config import aws_logo, azure_logo, gcp_logo, square_logo_img",
            "from prowler.lib.logger import logger"
        ],
        "reference_api": [
            "join",
            "printed_subscriptions.append",
            "logger.error",
            "items"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "join",
            "items",
            "printed_subscriptions.append",
            "join"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def initialize_file_descriptor(\n    filename: str,\n    output_mode: str,\n    provider: Any = None,\n    format: Any = FindingOutput,\n) -> TextIOWrapper",
        "start_line": "23",
        "end_line": "53",
        "file_path": "prowler/lib/outputs/file_descriptors.py",
        "docstring": "The initialize_file_descriptor function opens or creates an output file, setting it up for writing.\\nIt takes parameters such as the filename, output mode, provider, format, and whether to write headers.\\nIf the file already exists, it opens the file in append mode.\\nIf the file does not exist, it creates the file and writes headers based on the provided format.\\nThe function uses the DictWriter to handle CSV format, writing headers if specified.\\nIn case of an exception, it logs the error with details.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d99b1930eb46",
        "ground_truth": "def initialize_file_descriptor(\n    filename: str,\n    output_mode: str,\n    provider: Any = None,\n    format: Any = Finding,\n    write_header: bool = True,\n) -> TextIOWrapper:\n    \"\"\"Open/Create the output file. If needed include headers or the required format, by default will use the Finding\"\"\"\n    try:\n        if file_exists(filename):\n            file_descriptor = open_file(\n                filename,\n                \"a\",\n            )\n        else:\n            file_descriptor = open_file(\n                filename,\n                \"a\",\n            )\n            # Format is the class model of the CSV format to print the headers\n            csv_header = [x.upper() for x in generate_csv_fields(format)]\n            csv_writer = DictWriter(\n                file_descriptor, fieldnames=csv_header, delimiter=\";\"\n            )\n            if write_header:\n                csv_writer.writeheader()\n        return file_descriptor\n    except Exception as error:\n        logger.error(\n            f\"{error.__class__.__name__}[{error.__traceback__.tb_lineno}]: {error}\"\n        )",
        "import_statements": [
            "from csv import DictWriter",
            "from io import TextIOWrapper",
            "from typing import Any",
            "from prowler.config.config import csv_file_suffix",
            "from prowler.lib.logger import logger",
            "from prowler.lib.outputs.compliance.mitre_attack.models import (\n    MitreAttackAWS,\n    MitreAttackAzure,\n    MitreAttackGCP,\n)",
            "from prowler.lib.outputs.compliance.models import (\n    Check_Output_CSV_AWS_ISO27001_2013,\n    Check_Output_CSV_AWS_Well_Architected,\n    Check_Output_CSV_ENS_RD2022,\n    Check_Output_CSV_Generic_Compliance,\n)",
            "from prowler.lib.outputs.csv.csv import generate_csv_fields",
            "from prowler.lib.outputs.output import Finding",
            "from prowler.lib.utils.utils import file_exists, open_file"
        ],
        "reference_api": [
            "csv_writer.writeheader",
            "DictWriter",
            "file_exists",
            "generate_csv_fields",
            "x.upper",
            "open_file",
            "logger.error"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "file_exists",
                "code": "def file_exists(filename: str):\n    \"\"\"file_exists returns True if the given file exists, otherwise returns False.\"\"\"\n    try:\n        exists_filename = exists(filename)\n    except Exception as e:\n        logger.critical(\n            f\"{filename}: {e.__class__.__name__}[{e.__traceback__.tb_lineno}]\"\n        )\n        sys.exit(1)\n    else:\n        return exists_filename"
            },
            {
                "name": "open_file",
                "code": "def open_file(input_file: str, mode: str = \"r\") -> TextIOWrapper:\n    \"\"\"open_file returns a handler to the file using the specified mode.\"\"\"\n    try:\n        f = open(input_file, mode, encoding=enconding_format_utf_8)\n    except OSError as os_error:\n        if os_error.strerror == \"Too many open files\":\n            logger.critical(\n                \"Ooops! You reached your user session maximum open files. To solve this issue, increase the shell session limit by running this command `ulimit -n 4096`. For more info visit https://docs.prowler.cloud/en/latest/troubleshooting/\"\n            )\n        else:\n            logger.critical(\n                f\"{input_file}: OSError[{os_error.errno}] {os_error.strerror}\"\n            )\n        sys.exit(1)\n    except Exception as e:\n        logger.critical(\n            f\"{input_file}: {e.__class__.__name__}[{e.__traceback__.tb_lineno}]\"\n        )\n        sys.exit(1)\n    else:\n        return f"
            },
            {
                "name": "open_file",
                "code": "def open_file(input_file: str, mode: str = \"r\") -> TextIOWrapper:\n    \"\"\"open_file returns a handler to the file using the specified mode.\"\"\"\n    try:\n        f = open(input_file, mode, encoding=enconding_format_utf_8)\n    except OSError as os_error:\n        if os_error.strerror == \"Too many open files\":\n            logger.critical(\n                \"Ooops! You reached your user session maximum open files. To solve this issue, increase the shell session limit by running this command `ulimit -n 4096`. For more info visit https://docs.prowler.cloud/en/latest/troubleshooting/\"\n            )\n        else:\n            logger.critical(\n                f\"{input_file}: OSError[{os_error.errno}] {os_error.strerror}\"\n            )\n        sys.exit(1)\n    except Exception as e:\n        logger.critical(\n            f\"{input_file}: {e.__class__.__name__}[{e.__traceback__.tb_lineno}]\"\n        )\n        sys.exit(1)\n    else:\n        return f"
            },
            {
                "name": "generate_csv_fields",
                "code": "def generate_csv_fields(format: Any) -> list[str]:\n    \"\"\"Generates the CSV headers for the given class\"\"\"\n    csv_fields = []\n    # __fields__ is always available in the Pydantic's BaseModel class\n    for field in format.__dict__.get(\"__fields__\").keys():\n        csv_fields.append(field)\n    return csv_fields"
            }
        ],
        "third_party": [
            "x.upper",
            "DictWriter",
            "csv_writer.writeheader"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def open_file(input_file: str, mode: str = \"r\") -> TextIOWrapper",
        "start_line": "29",
        "end_line": "49",
        "file_path": "prowler/lib/utils/utils.py",
        "docstring": "The open_file function attempts to open a specified file in a given mode, defaulting to read mode.\\nIf an OSError occurs due to too many open files, it logs a critical message with instructions to increase the session limit and exits.\\nFor other OSErrors, it logs the error number and message and exits.\\nFor any other exceptions, it logs the exception type and line number and exits.\\nIf successful, it returns the file handler.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "860bf57a8ef9",
        "ground_truth": "def open_file(input_file: str, mode: str = \"r\") -> TextIOWrapper:\n    \"\"\"open_file returns a handler to the file using the specified mode.\"\"\"\n    try:\n        f = open(input_file, mode, encoding=enconding_format_utf_8)\n    except OSError as os_error:\n        if os_error.strerror == \"Too many open files\":\n            logger.critical(\n                \"Ooops! You reached your user session maximum open files. To solve this issue, increase the shell session limit by running this command `ulimit -n 4096`. For more info visit https://docs.prowler.cloud/en/latest/troubleshooting/\"\n            )\n        else:\n            logger.critical(\n                f\"{input_file}: OSError[{os_error.errno}] {os_error.strerror}\"\n            )\n        sys.exit(1)\n    except Exception as e:\n        logger.critical(\n            f\"{input_file}: {e.__class__.__name__}[{e.__traceback__.tb_lineno}]\"\n        )\n        sys.exit(1)\n    else:\n        return f",
        "import_statements": [
            "import json",
            "import os",
            "import re",
            "import sys",
            "import tempfile",
            "from datetime import datetime",
            "from hashlib import sha512",
            "from io import TextIOWrapper",
            "from ipaddress import ip_address",
            "from os.path import exists",
            "from time import mktime",
            "from typing import Optional",
            "from colorama import Style",
            "from detect_secrets import SecretsCollection",
            "from detect_secrets.settings import default_settings",
            "from prowler.config.config import enconding_format_utf_8",
            "from prowler.lib.logger import logger"
        ],
        "reference_api": [
            "logger.critical",
            "sys.exit",
            "open"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def parse_json_file(input_file: TextIOWrapper) -> dict",
        "start_line": "52",
        "end_line": "62",
        "file_path": "prowler/lib/utils/utils.py",
        "docstring": "The parse_json_file function reads a JSON file and returns its content as a dictionary.\\nIt tries to load the JSON content from the provided input file.\\nIf an error occurs during the loading process, it logs a critical error message with the filename, error type, and line number where the error occurred.\\nIn case of an error, the function exits the program with status code 1.\\nIf successful, it returns the parsed JSON content as a dictionary.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c5175a3d8016",
        "ground_truth": "def parse_json_file(input_file: TextIOWrapper) -> dict:\n    \"\"\"parse_json_file loads a JSON file and returns a dictionary with the JSON content.\"\"\"\n    try:\n        json_file = json.load(input_file)\n    except Exception as e:\n        logger.critical(\n            f\"{input_file.name}: {e.__class__.__name__}[{e.__traceback__.tb_lineno}]\"\n        )\n        sys.exit(1)\n    else:\n        return json_file",
        "import_statements": [
            "import json",
            "import os",
            "import re",
            "import sys",
            "import tempfile",
            "from datetime import datetime",
            "from hashlib import sha512",
            "from io import TextIOWrapper",
            "from ipaddress import ip_address",
            "from os.path import exists",
            "from time import mktime",
            "from typing import Optional",
            "from colorama import Style",
            "from detect_secrets import SecretsCollection",
            "from detect_secrets.settings import default_settings",
            "from prowler.config.config import enconding_format_utf_8",
            "from prowler.lib.logger import logger"
        ],
        "reference_api": [
            "logger.critical",
            "sys.exit",
            "json.load"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "GCP",
        "owner/repo": "prowler-cloud/prowler",
        "function_declaration": "def detect_secrets_scan(data)",
        "start_line": "83",
        "end_line": "97",
        "file_path": "prowler/lib/utils/utils.py",
        "docstring": "The detect_secrets_scan function scans a given data string for secrets using the detect-secrets library.\\nIt creates a temporary file, writes the data to it, and then scans the file for secrets.\\nAfter scanning, it removes the temporary file and returns the detected secrets in JSON format if any are found.\\nIf no secrets are found, it returns None.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "217ce6dbcbb7",
        "ground_truth": "def detect_secrets_scan(data):\n    temp_data_file = tempfile.NamedTemporaryFile(delete=False)\n    temp_data_file.write(bytes(data, encoding=\"raw_unicode_escape\"))\n    temp_data_file.close()\n     secrets = SecretsCollection()\n    with default_settings():\n        secrets.scan_file(temp_data_file.name)\n    os.remove(temp_data_file.name)\n     detect_secrets_output = secrets.json()\n    if detect_secrets_output:\n        return detect_secrets_output[temp_data_file.name]\n    else:\n        return None",
        "import_statements": [
            "import json",
            "import os",
            "import re",
            "import sys",
            "import tempfile",
            "from datetime import datetime",
            "from hashlib import sha512",
            "from io import TextIOWrapper",
            "from ipaddress import ip_address",
            "from os.path import exists",
            "from time import mktime",
            "from typing import Optional",
            "from colorama import Style",
            "from detect_secrets import SecretsCollection",
            "from detect_secrets.settings import default_settings",
            "from prowler.config.config import enconding_format_utf_8",
            "from prowler.lib.logger import logger"
        ],
        "reference_api": [
            "temp_data_file.close",
            "temp_data_file.write",
            "default_settings",
            "tempfile.NamedTemporaryFile",
            "os.remove",
            "bytes",
            "secrets.json",
            "SecretsCollection",
            "secrets.scan_file"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "temp_data_file.write",
            "temp_data_file.close",
            "SecretsCollection",
            "default_settings"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "def _build_arbitrary_client(self, client_name, client_version, force_new=False)",
        "start_line": "24",
        "end_line": "41",
        "file_path": "ScoutSuite/providers/gcp/facade/basefacade.py",
        "docstring": "The _build_arbitrary_client function constructs a client for interacting with a specific service using the Google API discovery service.\\nIf force_new is True, it creates a new client instance and sets a custom user agent.\\nIf force_new is False and a client instance does not already exist, it creates a new client instance, sets the custom user agent, and stores it in the _client attribute.\\nIf a client instance already exists and force_new is False, it returns the existing client instance.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "79309fa2fb70",
        "ground_truth": "def _build_arbitrary_client(self, client_name, client_version, force_new=False):\n    \"\"\"\n    :param client_name: name of the service\n    :param client_version:  version of the client to create\n    :param force_new: whether to create a new client - useful to create arbitrary clients from facades\n    :return:\n    \"\"\"\n    if force_new:\n        client = discovery.build(client_name, client_version, cache_discovery=False, cache=MemoryCache())\n        http.set_user_agent(client._http, get_user_agent())  # force set custom user agent\n        return client\n    else:\n        if not self._client:\n            client = discovery.build(client_name, client_version, cache_discovery=False, cache=MemoryCache())\n            http.set_user_agent(client._http, get_user_agent())  # force set custom user agent\n            self._client = client\n        return self._client",
        "import_statements": [
            "import httplib2shim",
            "from googleapiclient import http",
            "from googleapiclient import discovery",
            "from ScoutSuite.utils import get_user_agent"
        ],
        "reference_api": [
            "MemoryCache",
            "get_user_agent",
            "discovery.build",
            "http.set_user_agent"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "get_user_agent",
                "code": "def get_user_agent():\n    return 'Scout Suite/{} (https://github.com/nccgroup/ScoutSuite)'.format(__version__)"
            },
            {
                "name": "get_user_agent",
                "code": "def get_user_agent():\n    return 'Scout Suite/{} (https://github.com/nccgroup/ScoutSuite)'.format(__version__)"
            }
        ],
        "third_party": [
            "discovery.build",
            "MemoryCache",
            "discovery.build",
            "MemoryCache"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "async def _get_dataset(self, dataset_id: str, project_id: str)",
        "start_line": "28",
        "end_line": "38",
        "file_path": "ScoutSuite/providers/gcp/facade/bigquery.py",
        "docstring": "The async function _get_dataset retrieves a specific dataset from Google BigQuery using the provided dataset_id and project_id.\\nIt initializes a BigQuery client, creates a request to get the dataset, and executes the request concurrently.\\nIf the dataset retrieval fails, it catches the exception, logs an error message, and returns an empty dictionary.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ffacec8591b8",
        "ground_truth": "async def _get_dataset(self, dataset_id: str, project_id: str):\n    try:\n        bigquery_client = self._get_client()\n        datasets = bigquery_client.datasets()\n        request = datasets.get(projectId=project_id, datasetId=dataset_id)\n        return await run_concurrently(\n            lambda: request.execute()\n        )\n    except Exception as e:\n        print_exception(f'Failed to retrieve BigQuery datasets {dataset_id}: {e}')\n        return {}",
        "import_statements": [
            "from ScoutSuite.core.console import print_exception",
            "from ScoutSuite.providers.gcp.facade.basefacade import GCPBaseFacade",
            "from ScoutSuite.providers.gcp.facade.utils import GCPFacadeUtils",
            "from ScoutSuite.providers.utils import map_concurrently, run_concurrently"
        ],
        "reference_api": [
            "print_exception",
            "request.execute",
            "run_concurrently",
            "self._get_client",
            "datasets.get",
            "bigquery_client.datasets"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_client",
                "code": "def _get_client(self) -> discovery.Resource:\n        return self._build_client()"
            },
            {
                "name": "datasets.get",
                "code": "def get(self, url):\n        return MemoryCache._cache.get(url)"
            },
            {
                "name": "run_concurrently",
                "code": "async def run_concurrently(function, backoff_seconds=15):\n    try:\n        async with asyncio.get_event_loop().throttler:\n            return await run_function_concurrently(function)\n    except Exception as e:\n        raise\n        \"\"\"\n        Commented out so this does not trigger errors from is_throttled, which is not fully implemented\n        # Determine whether the exception is due to API throttling\n        if is_throttled(e):\n            source_file = inspect.getsourcefile(function)\n            source_file_line = inspect.getsourcelines(function)[1]\n            print_warning(f'Hitting API rate limiting ({\"/\".join(source_file.split(\"/\")[-2:])} L{source_file_line}), will retry in {backoff_seconds}s')\n            await asyncio.sleep(backoff_seconds)\n            return await run_concurrently(function, backoff_seconds + 15)\n        else:\n            raise\n        \"\"\""
            },
            {
                "name": "print_exception",
                "code": "def print_exception(exception, additional_details=None):\n    try:\n        exc = True\n        exc_type, exc_obj, exc_tb = sys.exc_info()\n        if exc_tb and traceback:\n            file_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            line_number = exc_tb.tb_lineno\n            traceback_exc = traceback.format_exc()\n            str = f'{file_name} L{line_number}: {exception}'\n        else:\n            file_name = None\n            line_number = None\n            traceback_exc = None\n            str = f'{exception}'\n            exc = False  # if there isn't an actual exception then it's pointless\n    except Exception as e:\n        file_name = None\n        line_number = None\n        traceback_exc = None\n        str = f'{exception}'\n\n    if verbose_exceptions and exc:\n        logger.exception(str)\n    else:\n        logger.error(str)\n\n    ERRORS_LIST.append({'file': file_name,\n                        'line': line_number,\n                        'exception': f'{exception}',\n                        'traceback': f'{traceback_exc}',\n                        'additional_details': additional_details})"
            }
        ],
        "third_party": [
            "bigquery_client.datasets",
            "request.execute"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "async def get_member_bindings(self, project_id: str)",
        "start_line": "9",
        "end_line": "18",
        "file_path": "ScoutSuite/providers/gcp/facade/cloudresourcemanager.py",
        "docstring": "The async function get_member_bindings retrieves IAM policy bindings for a given project ID.\\nIt initializes a Cloud Resource Manager client and executes an asynchronous request to get the IAM policy for the specified project.\\nIf successful, it returns the list of bindings from the response.\\nIf an exception occurs, it prints an error message and returns an empty list.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3d818d5eb773",
        "ground_truth": "async def get_member_bindings(self, project_id: str):\n    try:\n        cloudresourcemanager_client = self._get_client()\n        response = await run_concurrently(\n                lambda: cloudresourcemanager_client.projects().getIamPolicy(resource=project_id).execute()\n        )\n        return response.get('bindings', [])\n    except Exception as e:\n        print_exception(f'Failed to retrieve project IAM policy bindings: {e}')\n        return []",
        "import_statements": [
            "from ScoutSuite.core.console import print_exception",
            "from ScoutSuite.providers.gcp.facade.basefacade import GCPBaseFacade",
            "from ScoutSuite.providers.utils import run_concurrently"
        ],
        "reference_api": [
            "print_exception",
            "run_concurrently",
            "self._get_client",
            "execute",
            "getIamPolicy",
            "response.get",
            "cloudresourcemanager_client.projects"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_client",
                "code": "def _get_client(self) -> discovery.Resource:\n        return self._build_client()"
            },
            {
                "name": "run_concurrently",
                "code": "async def run_concurrently(function, backoff_seconds=15):\n    try:\n        async with asyncio.get_event_loop().throttler:\n            return await run_function_concurrently(function)\n    except Exception as e:\n        raise\n        \"\"\"\n        Commented out so this does not trigger errors from is_throttled, which is not fully implemented\n        # Determine whether the exception is due to API throttling\n        if is_throttled(e):\n            source_file = inspect.getsourcefile(function)\n            source_file_line = inspect.getsourcelines(function)[1]\n            print_warning(f'Hitting API rate limiting ({\"/\".join(source_file.split(\"/\")[-2:])} L{source_file_line}), will retry in {backoff_seconds}s')\n            await asyncio.sleep(backoff_seconds)\n            return await run_concurrently(function, backoff_seconds + 15)\n        else:\n            raise\n        \"\"\""
            },
            {
                "name": "response.get",
                "code": "def get(self, url):\n        return MemoryCache._cache.get(url)"
            },
            {
                "name": "print_exception",
                "code": "def print_exception(exception, additional_details=None):\n    try:\n        exc = True\n        exc_type, exc_obj, exc_tb = sys.exc_info()\n        if exc_tb and traceback:\n            file_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            line_number = exc_tb.tb_lineno\n            traceback_exc = traceback.format_exc()\n            str = f'{file_name} L{line_number}: {exception}'\n        else:\n            file_name = None\n            line_number = None\n            traceback_exc = None\n            str = f'{exception}'\n            exc = False  # if there isn't an actual exception then it's pointless\n    except Exception as e:\n        file_name = None\n        line_number = None\n        traceback_exc = None\n        str = f'{exception}'\n\n    if verbose_exceptions and exc:\n        logger.exception(str)\n    else:\n        logger.error(str)\n\n    ERRORS_LIST.append({'file': file_name,\n                        'line': line_number,\n                        'exception': f'{exception}',\n                        'traceback': f'{traceback_exc}',\n                        'additional_details': additional_details})"
            }
        ],
        "third_party": [
            "execute",
            "getIamPolicy",
            "cloudresourcemanager_client.projects"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "async def get_backups(self, project_id: str, instance_name: str)",
        "start_line": "10",
        "end_line": "18",
        "file_path": "ScoutSuite/providers/gcp/facade/cloudsql.py",
        "docstring": "The async function get_backups retrieves the list of backups for a specific Google Cloud SQL instance.\\nIt initializes a Cloud SQL client, creates a request to list backups for the given project and instance, and uses the GCPFacadeUtils.get_all method to fetch all backup items.\\nIf an exception occurs, it prints an error message and returns an empty list.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b02577363aa2",
        "ground_truth": "async def get_backups(self, project_id: str, instance_name: str):\n    try:\n        cloudsql_client = self._get_client()\n        backups_group = cloudsql_client.backupRuns()\n        request = backups_group.list(project=project_id, instance=instance_name)\n        return await GCPFacadeUtils.get_all('items', request, backups_group)\n    except Exception as e:\n        print_exception(f'Failed to retrieve database instance backups: {e}')\n        return []",
        "import_statements": [
            "from ScoutSuite.core.console import print_exception",
            "from ScoutSuite.providers.gcp.facade.basefacade import GCPBaseFacade",
            "from ScoutSuite.providers.gcp.facade.utils import GCPFacadeUtils",
            "from ScoutSuite.providers.utils import run_concurrently"
        ],
        "reference_api": [
            "print_exception",
            "backups_group.list",
            "self._get_client",
            "cloudsql_client.backupRuns",
            "GCPFacadeUtils.get_all"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_client",
                "code": "def _get_client(self) -> discovery.Resource:\n        return self._build_client()"
            },
            {
                "name": "print_exception",
                "code": "def print_exception(exception, additional_details=None):\n    try:\n        exc = True\n        exc_type, exc_obj, exc_tb = sys.exc_info()\n        if exc_tb and traceback:\n            file_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            line_number = exc_tb.tb_lineno\n            traceback_exc = traceback.format_exc()\n            str = f'{file_name} L{line_number}: {exception}'\n        else:\n            file_name = None\n            line_number = None\n            traceback_exc = None\n            str = f'{exception}'\n            exc = False  # if there isn't an actual exception then it's pointless\n    except Exception as e:\n        file_name = None\n        line_number = None\n        traceback_exc = None\n        str = f'{exception}'\n\n    if verbose_exceptions and exc:\n        logger.exception(str)\n    else:\n        logger.error(str)\n\n    ERRORS_LIST.append({'file': file_name,\n                        'line': line_number,\n                        'exception': f'{exception}',\n                        'traceback': f'{traceback_exc}',\n                        'additional_details': additional_details})"
            }
        ],
        "third_party": [
            "cloudsql_client.backupRuns",
            "backups_group.list",
            "GCPFacadeUtils.get_all"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "async def get_users(self, project_id: str, instance_name: str)",
        "start_line": "30",
        "end_line": "42",
        "file_path": "ScoutSuite/providers/gcp/facade/cloudsql.py",
        "docstring": "The async function get_users retrieves the list of users for a given project and instance in Google Cloud SQL.\\nIt initializes the Cloud SQL client and executes a concurrent request to fetch the users.\\nIf successful, it returns the list of users from the response.\\nIf an exception occurs, it handles specific error messages and prints an exception for other errors, returning an empty list in these cases.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d85823c77428",
        "ground_truth": "async def get_users(self, project_id: str, instance_name: str):\n    try:\n        cloudsql_client = self._get_client()\n        response = await run_concurrently(\n                lambda: cloudsql_client.users().list(project=project_id, instance=instance_name).execute()\n        )\n        return response.get('items', [])\n    except Exception as e:\n        if 'The requested operation is not valid for an on-premises instance.' in str(e):\n            return []\n        if 'Invalid request since instance is not running' not in str(e):\n            print_exception(f'Failed to retrieve database instance users: {e}')\n        return []",
        "import_statements": [
            "from ScoutSuite.core.console import print_exception",
            "from ScoutSuite.providers.gcp.facade.basefacade import GCPBaseFacade",
            "from ScoutSuite.providers.gcp.facade.utils import GCPFacadeUtils",
            "from ScoutSuite.providers.utils import run_concurrently"
        ],
        "reference_api": [
            "print_exception",
            "list",
            "run_concurrently",
            "self._get_client",
            "execute",
            "cloudsql_client.users",
            "response.get",
            "str"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_client",
                "code": "def _get_client(self) -> discovery.Resource:\n        return self._build_client()"
            },
            {
                "name": "run_concurrently",
                "code": "async def run_concurrently(function, backoff_seconds=15):\n    try:\n        async with asyncio.get_event_loop().throttler:\n            return await run_function_concurrently(function)\n    except Exception as e:\n        raise\n        \"\"\"\n        Commented out so this does not trigger errors from is_throttled, which is not fully implemented\n        # Determine whether the exception is due to API throttling\n        if is_throttled(e):\n            source_file = inspect.getsourcefile(function)\n            source_file_line = inspect.getsourcelines(function)[1]\n            print_warning(f'Hitting API rate limiting ({\"/\".join(source_file.split(\"/\")[-2:])} L{source_file_line}), will retry in {backoff_seconds}s')\n            await asyncio.sleep(backoff_seconds)\n            return await run_concurrently(function, backoff_seconds + 15)\n        else:\n            raise\n        \"\"\""
            },
            {
                "name": "response.get",
                "code": "def get(self, url):\n        return MemoryCache._cache.get(url)"
            },
            {
                "name": "print_exception",
                "code": "def print_exception(exception, additional_details=None):\n    try:\n        exc = True\n        exc_type, exc_obj, exc_tb = sys.exc_info()\n        if exc_tb and traceback:\n            file_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            line_number = exc_tb.tb_lineno\n            traceback_exc = traceback.format_exc()\n            str = f'{file_name} L{line_number}: {exception}'\n        else:\n            file_name = None\n            line_number = None\n            traceback_exc = None\n            str = f'{exception}'\n            exc = False  # if there isn't an actual exception then it's pointless\n    except Exception as e:\n        file_name = None\n        line_number = None\n        traceback_exc = None\n        str = f'{exception}'\n\n    if verbose_exceptions and exc:\n        logger.exception(str)\n    else:\n        logger.error(str)\n\n    ERRORS_LIST.append({'file': file_name,\n                        'line': line_number,\n                        'exception': f'{exception}',\n                        'traceback': f'{traceback_exc}',\n                        'additional_details': additional_details})"
            }
        ],
        "third_party": [
            "execute",
            "cloudsql_client.users"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "async def get_buckets(self, project_id: str)",
        "start_line": "17",
        "end_line": "26",
        "file_path": "ScoutSuite/providers/gcp/facade/cloudstorage.py",
        "docstring": "The async function get_buckets retrieves storage buckets for a given project_id.\\nIt initializes a client for the project, lists all buckets concurrently, and then concurrently sets bucket logging and IAM policy information for each bucket.\\nIf an error occurs during the process, it logs the exception and returns an empty list.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2507eb48369f",
        "ground_truth": "async def get_buckets(self, project_id: str):\n    try:\n        client = self.get_client(project_id)\n        buckets = await run_concurrently(lambda: list(client.list_buckets()))\n        await get_and_set_concurrently([self._get_and_set_bucket_logging, \n            self._get_and_set_bucket_iam_policy], buckets)\n        return buckets\n    except Exception as e:\n        print_exception(f'Failed to retrieve storage buckets: {e}')\n        return []",
        "import_statements": [
            "from google.cloud import storage",
            "from google.api_core.gapic_v1.client_info import ClientInfo",
            "from ScoutSuite.core.console import print_exception",
            "from ScoutSuite.providers.utils import run_concurrently, get_and_set_concurrently",
            "from ScoutSuite.utils import get_user_agent"
        ],
        "reference_api": [
            "print_exception",
            "list",
            "run_concurrently",
            "self.get_client",
            "get_and_set_concurrently",
            "client.list_buckets"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.get_client",
                "code": "def get_client(self, project_id: str):\n        client_info = ClientInfo(user_agent=get_user_agent())\n        client = storage.Client(project=project_id,\n                                client_info=client_info)\n        return client"
            },
            {
                "name": "run_concurrently",
                "code": "async def run_concurrently(function, backoff_seconds=15):\n    try:\n        async with asyncio.get_event_loop().throttler:\n            return await run_function_concurrently(function)\n    except Exception as e:\n        raise\n        \"\"\"\n        Commented out so this does not trigger errors from is_throttled, which is not fully implemented\n        # Determine whether the exception is due to API throttling\n        if is_throttled(e):\n            source_file = inspect.getsourcefile(function)\n            source_file_line = inspect.getsourcelines(function)[1]\n            print_warning(f'Hitting API rate limiting ({\"/\".join(source_file.split(\"/\")[-2:])} L{source_file_line}), will retry in {backoff_seconds}s')\n            await asyncio.sleep(backoff_seconds)\n            return await run_concurrently(function, backoff_seconds + 15)\n        else:\n            raise\n        \"\"\""
            },
            {
                "name": "get_and_set_concurrently",
                "code": "async def get_and_set_concurrently(get_and_set_funcs: [], entities: [], **kwargs):\n    \"\"\"\n    Given a list of get_and_set_* functions (ex: get_and_set_description, get_and_set_attributes,\n    get_and_set_policy, etc.) and a list of entities (ex: stacks, keys, load balancers, vpcs, etc.),\n    get_and_set_concurrently will call each of these functions concurrently on each entity.\n\n    :param get_and_set_funcs: list of functions that takes a region and an entity (they must have the following\n    signature: region: str, entity: {}) and then fetch and set some kind of attributes to this entity.\n    :param entities: list of a same kind of entities\n    :param kwargs: used to pass cloud provider specific parameters (ex: region or vpc for AWS, etc.) to the given\n    functions.\n\n    :return:\n    \"\"\"\n\n    if len(entities) == 0:\n        return\n\n    tasks = {\n        asyncio.ensure_future(\n            get_and_set_func(entity, **kwargs)\n        ) for entity in entities for get_and_set_func in get_and_set_funcs\n    }\n    await asyncio.wait(tasks)"
            },
            {
                "name": "print_exception",
                "code": "def print_exception(exception, additional_details=None):\n    try:\n        exc = True\n        exc_type, exc_obj, exc_tb = sys.exc_info()\n        if exc_tb and traceback:\n            file_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            line_number = exc_tb.tb_lineno\n            traceback_exc = traceback.format_exc()\n            str = f'{file_name} L{line_number}: {exception}'\n        else:\n            file_name = None\n            line_number = None\n            traceback_exc = None\n            str = f'{exception}'\n            exc = False  # if there isn't an actual exception then it's pointless\n    except Exception as e:\n        file_name = None\n        line_number = None\n        traceback_exc = None\n        str = f'{exception}'\n\n    if verbose_exceptions and exc:\n        logger.exception(str)\n    else:\n        logger.error(str)\n\n    ERRORS_LIST.append({'file': file_name,\n                        'line': line_number,\n                        'exception': f'{exception}',\n                        'traceback': f'{traceback_exc}',\n                        'additional_details': additional_details})"
            }
        ],
        "third_party": [
            "client.list_buckets"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "async def get_disks(self, project_id, zone)",
        "start_line": "11",
        "end_line": "19",
        "file_path": "ScoutSuite/providers/gcp/facade/gce.py",
        "docstring": "The async function get_disks retrieves a list of disks for a specified project and zone in Google Cloud Platform.\\nIt initializes a Google Compute Engine (GCE) client and sends a request to list the disks.\\nThe function uses GCPFacadeUtils.get_all to fetch all disk items from the response.\\nIf an exception occurs, it prints an error message and returns an empty list.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e5a6885447cd",
        "ground_truth": "async def get_disks(self, project_id, zone):\n    try:\n        gce_client = self._get_client()\n        request = gce_client.disks().list(project=project_id, zone=zone)\n        disks_group = gce_client.disks()\n        return await GCPFacadeUtils.get_all('items', request, disks_group)\n    except Exception as e:\n        print_exception(f'Failed to retrieve disks: {e}')\n        return []",
        "import_statements": [
            "from ScoutSuite.core.console import print_exception, print_warning",
            "from ScoutSuite.providers.gcp.facade.basefacade import GCPBaseFacade",
            "from ScoutSuite.providers.gcp.facade.utils import GCPFacadeUtils",
            "from ScoutSuite.providers.utils import run_concurrently"
        ],
        "reference_api": [
            "print_exception",
            "list",
            "self._get_client",
            "GCPFacadeUtils.get_all",
            "gce_client.disks"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_client",
                "code": "def _get_client(self) -> discovery.Resource:\n        return self._build_client()"
            },
            {
                "name": "print_exception",
                "code": "def print_exception(exception, additional_details=None):\n    try:\n        exc = True\n        exc_type, exc_obj, exc_tb = sys.exc_info()\n        if exc_tb and traceback:\n            file_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            line_number = exc_tb.tb_lineno\n            traceback_exc = traceback.format_exc()\n            str = f'{file_name} L{line_number}: {exception}'\n        else:\n            file_name = None\n            line_number = None\n            traceback_exc = None\n            str = f'{exception}'\n            exc = False  # if there isn't an actual exception then it's pointless\n    except Exception as e:\n        file_name = None\n        line_number = None\n        traceback_exc = None\n        str = f'{exception}'\n\n    if verbose_exceptions and exc:\n        logger.exception(str)\n    else:\n        logger.error(str)\n\n    ERRORS_LIST.append({'file': file_name,\n                        'line': line_number,\n                        'exception': f'{exception}',\n                        'traceback': f'{traceback_exc}',\n                        'additional_details': additional_details})"
            }
        ],
        "third_party": [
            "gce_client.disks",
            "gce_client.disks",
            "GCPFacadeUtils.get_all"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "async def _add_metadata(self, project_id, instances)",
        "start_line": "45",
        "end_line": "50",
        "file_path": "ScoutSuite/providers/gcp/facade/gce.py",
        "docstring": "The async function _add_metadata retrieves project metadata and adds it to a list of instances.\\nIt first fetches the project metadata using the project_id, converts this metadata to a dictionary, and assigns it to the variable common_instance_metadata.\\nThen, for each instance in the instances list, it converts the instance's metadata to a dictionary and adds the common instance metadata to the instance.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3fd25b42fc14",
        "ground_truth": "async def _add_metadata(self, project_id, instances):\n    project = await self.get_project(project_id)\n    common_instance_metadata = self.metadata_to_dict(project['commonInstanceMetadata'])\n    for instance in instances:\n        instance['metadata'] = self.metadata_to_dict(instance['metadata'])\n        instance['commonInstanceMetadata'] = common_instance_metadata",
        "import_statements": [
            "from ScoutSuite.core.console import print_exception, print_warning",
            "from ScoutSuite.providers.gcp.facade.basefacade import GCPBaseFacade",
            "from ScoutSuite.providers.gcp.facade.utils import GCPFacadeUtils",
            "from ScoutSuite.providers.utils import run_concurrently"
        ],
        "reference_api": [
            "self.get_project",
            "self.metadata_to_dict"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.get_project",
                "code": "async def get_project(self, project_id):\n        try:\n            gce_client = self._get_client()\n            return await run_concurrently(\n                lambda: gce_client.projects().get(project=project_id).execute()\n            )\n        except Exception as e:\n            print_exception(f'Failed to retrieve GCE project: {e}')\n            return None"
            },
            {
                "name": "self.metadata_to_dict",
                "code": "def metadata_to_dict(self, metadata):\n        return {item['key']: item['value'] for item in metadata['items']} if 'items' in metadata else {}"
            },
            {
                "name": "self.metadata_to_dict",
                "code": "def metadata_to_dict(self, metadata):\n        return {item['key']: item['value'] for item in metadata['items']} if 'items' in metadata else {}"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "async def get_clusters(self, project_id)",
        "start_line": "13",
        "end_line": "25",
        "file_path": "ScoutSuite/providers/gcp/facade/gke.py",
        "docstring": "The async function get_clusters retrieves the list of clusters for a given Google Cloud project.\\nIt first obtains a GKE client and then concurrently executes a request to list all clusters in the specified project.\\nIf clusters are found, it concurrently retrieves and sets the private Google access settings for each cluster.\\nIn case of an exception, it prints the error message and returns an empty list.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "62d660e3ce92",
        "ground_truth": "async def get_clusters(self, project_id):\n    try:\n        gke_client = self._get_client()\n        response = await run_concurrently(\n            lambda: gke_client.projects().locations().clusters().list(parent=f\"projects/{project_id}/locations/-\").execute()\n        )\n        clusters = response.get('clusters', [])\n        await get_and_set_concurrently([self._get_and_set_private_google_access_enabled],\n                                       clusters, project_id=project_id)\n        return clusters\n    except Exception as e:\n        print_exception('Failed to retrieve clusters: {}'.format(e))\n        return []",
        "import_statements": [
            "import re",
            "from ScoutSuite.core.console import print_exception",
            "from ScoutSuite.providers.gcp.facade.base import GCPBaseFacade",
            "from ScoutSuite.providers.utils import run_concurrently, get_and_set_concurrently"
        ],
        "reference_api": [
            "get_and_set_concurrently",
            "list",
            "print_exception",
            "run_concurrently",
            "format",
            "self._get_client",
            "execute",
            "response.get",
            "clusters",
            "locations",
            "gke_client.projects"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "run_concurrently",
                "code": "async def run_concurrently(function, backoff_seconds=15):\n    try:\n        async with asyncio.get_event_loop().throttler:\n            return await run_function_concurrently(function)\n    except Exception as e:\n        raise\n        \"\"\"\n        Commented out so this does not trigger errors from is_throttled, which is not fully implemented\n        # Determine whether the exception is due to API throttling\n        if is_throttled(e):\n            source_file = inspect.getsourcefile(function)\n            source_file_line = inspect.getsourcelines(function)[1]\n            print_warning(f'Hitting API rate limiting ({\"/\".join(source_file.split(\"/\")[-2:])} L{source_file_line}), will retry in {backoff_seconds}s')\n            await asyncio.sleep(backoff_seconds)\n            return await run_concurrently(function, backoff_seconds + 15)\n        else:\n            raise\n        \"\"\""
            },
            {
                "name": "get_and_set_concurrently",
                "code": "async def get_and_set_concurrently(get_and_set_funcs: [], entities: [], **kwargs):\n    \"\"\"\n    Given a list of get_and_set_* functions (ex: get_and_set_description, get_and_set_attributes,\n    get_and_set_policy, etc.) and a list of entities (ex: stacks, keys, load balancers, vpcs, etc.),\n    get_and_set_concurrently will call each of these functions concurrently on each entity.\n\n    :param get_and_set_funcs: list of functions that takes a region and an entity (they must have the following\n    signature: region: str, entity: {}) and then fetch and set some kind of attributes to this entity.\n    :param entities: list of a same kind of entities\n    :param kwargs: used to pass cloud provider specific parameters (ex: region or vpc for AWS, etc.) to the given\n    functions.\n\n    :return:\n    \"\"\"\n\n    if len(entities) == 0:\n        return\n\n    tasks = {\n        asyncio.ensure_future(\n            get_and_set_func(entity, **kwargs)\n        ) for entity in entities for get_and_set_func in get_and_set_funcs\n    }\n    await asyncio.wait(tasks)"
            },
            {
                "name": "print_exception",
                "code": "def print_exception(exception, additional_details=None):\n    try:\n        exc = True\n        exc_type, exc_obj, exc_tb = sys.exc_info()\n        if exc_tb and traceback:\n            file_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            line_number = exc_tb.tb_lineno\n            traceback_exc = traceback.format_exc()\n            str = f'{file_name} L{line_number}: {exception}'\n        else:\n            file_name = None\n            line_number = None\n            traceback_exc = None\n            str = f'{exception}'\n            exc = False  # if there isn't an actual exception then it's pointless\n    except Exception as e:\n        file_name = None\n        line_number = None\n        traceback_exc = None\n        str = f'{exception}'\n\n    if verbose_exceptions and exc:\n        logger.exception(str)\n    else:\n        logger.error(str)\n\n    ERRORS_LIST.append({'file': file_name,\n                        'line': line_number,\n                        'exception': f'{exception}',\n                        'traceback': f'{traceback_exc}',\n                        'additional_details': additional_details})"
            }
        ],
        "third_party": [
            "self._get_client",
            "execute",
            "clusters",
            "locations",
            "gke_client.projects",
            "response.get"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "async def list_keys(self, project_id: str, location: str, keyring_name: str)",
        "start_line": "46",
        "end_line": "56",
        "file_path": "ScoutSuite/providers/gcp/facade/kms.py",
        "docstring": "The async function list_keys retrieves a list of KMS keys for a specified key ring in Google Cloud.\\nIt constructs the parent path using the project ID, location, and key ring name.\\nThen it initializes the KMS client and prepares a request to list the keys.\\nThe function uses GCPFacadeUtils.get_all to fetch all keys asynchronously and returns them.\\nIf an error occurs, it prints an error message and returns an empty list.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7d81677f9819",
        "ground_truth": "async def list_keys(self, project_id: str, location: str, keyring_name: str):\n    try:\n        parent = self.cloud_client.key_ring_path(project_id, location, keyring_name)\n        kms_client = self._get_client()\n        cryptokeys = kms_client.projects().locations().keyRings().cryptoKeys()\n        request = cryptokeys.list(parent=parent)\n        return await GCPFacadeUtils.get_all('cryptoKeys', request, cryptokeys)\n    except Exception as e:\n        print_exception(f'Failed to retrieve KMS keys for key ring {keyring_name}: {e}')\n        return []",
        "import_statements": [
            "from google.cloud import kms",
            "from google.api_core.gapic_v1.client_info import ClientInfo",
            "from ScoutSuite.core.console import print_exception",
            "from ScoutSuite.providers.gcp.facade.basefacade import GCPBaseFacade",
            "from ScoutSuite.providers.gcp.facade.utils import GCPFacadeUtils",
            "from ScoutSuite.providers.utils import run_concurrently",
            "from ScoutSuite.utils import get_user_agent"
        ],
        "reference_api": [
            "cryptoKeys",
            "key_ring_path",
            "cryptokeys.list",
            "print_exception",
            "self._get_client",
            "keyRings",
            "kms_client.projects",
            "GCPFacadeUtils.get_all",
            "locations"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_client",
                "code": "def _get_client(self) -> discovery.Resource:\n        return self._build_client()"
            },
            {
                "name": "print_exception",
                "code": "def print_exception(exception, additional_details=None):\n    try:\n        exc = True\n        exc_type, exc_obj, exc_tb = sys.exc_info()\n        if exc_tb and traceback:\n            file_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            line_number = exc_tb.tb_lineno\n            traceback_exc = traceback.format_exc()\n            str = f'{file_name} L{line_number}: {exception}'\n        else:\n            file_name = None\n            line_number = None\n            traceback_exc = None\n            str = f'{exception}'\n            exc = False  # if there isn't an actual exception then it's pointless\n    except Exception as e:\n        file_name = None\n        line_number = None\n        traceback_exc = None\n        str = f'{exception}'\n\n    if verbose_exceptions and exc:\n        logger.exception(str)\n    else:\n        logger.error(str)\n\n    ERRORS_LIST.append({'file': file_name,\n                        'line': line_number,\n                        'exception': f'{exception}',\n                        'traceback': f'{traceback_exc}',\n                        'additional_details': additional_details})"
            }
        ],
        "third_party": [
            "key_ring_path",
            "cryptoKeys",
            "keyRings",
            "locations",
            "kms_client.projects",
            "cryptokeys.list",
            "GCPFacadeUtils.get_all"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "async def get_redis_instances(self, project_id: str)",
        "start_line": "10",
        "end_line": "20",
        "file_path": "ScoutSuite/providers/gcp/facade/memorystoreredis.py",
        "docstring": "The async function get_redis_instances retrieves Redis instances for a specified project using the Cloud Memorystore API.\\nIt constructs the API request for listing instances within the given project and attempts to fetch all instances.\\nIf an exception occurs during the process, it prints an error message and returns an empty list.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "85e98b11508d",
        "ground_truth": "async def get_redis_instances(self, project_id: str):\n    # Retrieves Redis Instances using the Cloud Memorystore API\n    try:\n        formatted_parent = f'projects/{project_id}/locations/-'\n        cloudmem_client = self._get_client()\n        instances_group = cloudmem_client.projects().locations().instances()\n        request = instances_group.list(parent=formatted_parent)\n        return await GCPFacadeUtils.get_all('instances', request, instances_group)\n    except Exception as e:\n        print_exception(f'Failed to retrieve redis instances: {e}')\n        return []",
        "import_statements": [
            "from ScoutSuite.core.console import print_exception",
            "from ScoutSuite.providers.gcp.facade.basefacade import GCPBaseFacade",
            "from ScoutSuite.providers.gcp.facade.utils import GCPFacadeUtils"
        ],
        "reference_api": [
            "print_exception",
            "self._get_client",
            "instances_group.list",
            "GCPFacadeUtils.get_all",
            "locations",
            "instances",
            "cloudmem_client.projects"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_client",
                "code": "def _get_client(self) -> discovery.Resource:\n        return self._build_client()"
            },
            {
                "name": "print_exception",
                "code": "def print_exception(exception, additional_details=None):\n    try:\n        exc = True\n        exc_type, exc_obj, exc_tb = sys.exc_info()\n        if exc_tb and traceback:\n            file_name = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n            line_number = exc_tb.tb_lineno\n            traceback_exc = traceback.format_exc()\n            str = f'{file_name} L{line_number}: {exception}'\n        else:\n            file_name = None\n            line_number = None\n            traceback_exc = None\n            str = f'{exception}'\n            exc = False  # if there isn't an actual exception then it's pointless\n    except Exception as e:\n        file_name = None\n        line_number = None\n        traceback_exc = None\n        str = f'{exception}'\n\n    if verbose_exceptions and exc:\n        logger.exception(str)\n    else:\n        logger.error(str)\n\n    ERRORS_LIST.append({'file': file_name,\n                        'line': line_number,\n                        'exception': f'{exception}',\n                        'traceback': f'{traceback_exc}',\n                        'additional_details': additional_details})"
            }
        ],
        "third_party": [
            "instances",
            "locations",
            "cloudmem_client.projects",
            "instances_group.list",
            "GCPFacadeUtils.get_all"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "def _sqlservers_cross_db_ownership_chaining_flag_off(self, raw_instance, flag_name: str)",
        "start_line": "159",
        "end_line": "166",
        "file_path": "ScoutSuite/providers/gcp/resources/cloudsql/database_instances.py",
        "docstring": "The _sqlservers_cross_db_ownership_chaining_flag_off function checks if the cross-database ownership chaining flag is turned off for a given SQL Server instance.\\nIt takes a raw_instance dictionary and a flag_name string as inputs.\\nIf the database version is SQLSERVER, it iterates through the database flags to find the flag with the specified name and checks if its value is 'off'.\\nIf found, it returns True; otherwise, it returns False.\\nIf the database version is not SQLSERVER, it returns None.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9f0ddb065cbc",
        "ground_truth": "def _sqlservers_cross_db_ownership_chaining_flag_off(self, raw_instance, flag_name: str):\n    if 'SQLSERVER' in raw_instance['databaseVersion']:\n        for flag in raw_instance['settings'].get('databaseFlags', []):\n            if flag['name'] == flag_name and flag['value'] == 'off':\n                return True\n        return False\n    else:\n        return None",
        "import_statements": [
            "from ScoutSuite.core.console import print_exception",
            "from ScoutSuite.providers.gcp.facade.base import GCPFacade",
            "from ScoutSuite.providers.gcp.resources.base import GCPCompositeResources",
            "from ScoutSuite.providers.gcp.resources.cloudsql.backups import Backups",
            "from ScoutSuite.providers.gcp.resources.cloudsql.users import Users",
            "from ScoutSuite.providers.utils import get_non_provider_id"
        ],
        "reference_api": [
            "get"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "def _postgres_log_min_error_statement_flags(self, raw_instance)",
        "start_line": "132",
        "end_line": "139",
        "file_path": "ScoutSuite/providers/gcp/resources/cloudsql/database_instances.py",
        "docstring": "The _postgres_log_min_error_statement_flags function checks if the 'log_min_error_statement' flag is set for a PostgreSQL database instance.\\nIt first verifies if the database version contains 'POSTGRES'.\\nIf true, it iterates through the database flags in the instance settings and returns True if the 'log_min_error_statement' flag is found and has a non-null value.\\nIf the flag is not found or has a null value, it returns False.\\nIf the database is not PostgreSQL, it returns None.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6dc1200687d6",
        "ground_truth": "def _postgres_log_min_error_statement_flags(self, raw_instance):\n    if 'POSTGRES' in raw_instance['databaseVersion']:\n        for flag in raw_instance['settings'].get('databaseFlags', []):\n            if flag['name'] == 'log_min_error_statement' and flag['value'] is not None:\n                return True\n        return False\n    else:\n        return None",
        "import_statements": [
            "from ScoutSuite.core.console import print_exception",
            "from ScoutSuite.providers.gcp.facade.base import GCPFacade",
            "from ScoutSuite.providers.gcp.resources.base import GCPCompositeResources",
            "from ScoutSuite.providers.gcp.resources.cloudsql.backups import Backups",
            "from ScoutSuite.providers.gcp.resources.cloudsql.users import Users",
            "from ScoutSuite.providers.utils import get_non_provider_id"
        ],
        "reference_api": [
            "get"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "nccgroup/ScoutSuite",
        "function_declaration": "def _public_ip_adresses(self, raw_instance)",
        "start_line": "97",
        "end_line": "102",
        "file_path": "ScoutSuite/providers/gcp/resources/gce/instances.py",
        "docstring": "The _public_ip_adresses function checks if any network interfaces in a given raw_instance have public IP addresses.\\nIt iterates through the network interfaces, checking for 'accessConfigs'.\\nIf 'accessConfigs' is found, it returns True, indicating a public IP address exists.\\nIf no 'accessConfigs' are found, it returns False.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8c75f38b32ed",
        "ground_truth": "def _public_ip_adresses(self, raw_instance):\n    for network in raw_instance['networkInterfaces']:\n        access_configs = network.get('accessConfigs', None)\n        if access_configs:\n            return True\n    return False",
        "import_statements": [
            "from ScoutSuite.providers.gcp.facade.base import GCPFacade",
            "from ScoutSuite.providers.gcp.resources.base import GCPCompositeResources",
            "from ScoutSuite.providers.gcp.resources.gce.instance_disks import InstanceDisks",
            "from ScoutSuite.providers.utils import get_non_provider_id"
        ],
        "reference_api": [
            "network.get"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "network.get"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "cloud-custodian/cloud-custodian",
        "function_declaration": "def process_resource_set(self, client, model, resources)",
        "start_line": "67",
        "end_line": "83",
        "file_path": "tools/c7n_gcp/c7n_gcp/actions/core.py",
        "docstring": "The process_resource_set function processes a set of resources using a specified client and model.\\nIt retrieves result_key and annotation_key from method_spec.\\nFor each resource, it gets the operation name and parameters, then tries to invoke the API.\\nIf an HttpError occurs, it handles the error and attempts to recover.\\nIf a result is obtained and both result_key and annotation_key are specified, the result is added to the resource under the annotation_key.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "852b4c87f6f7",
        "ground_truth": "def process_resource_set(self, client, model, resources):\n    result_key = self.method_spec.get('result_key')\n    annotation_key = self.method_spec.get('annotation_key')\n    for resource in resources:\n        op_name = self.get_operation_name(model, resource)\n        params = self.get_resource_params(model, resource)\n        try:\n            result = self.invoke_api(client, op_name, params)\n        except HttpError as e:\n            result = self.handle_resource_error(\n                client, model, resource, op_name, params, e\n            )\n            # if the error handler recovered it returns a result\n            if not result:\n                raise\n        if result and result_key and annotation_key:\n            resource[annotation_key] = result.get(result_key)",
        "import_statements": [
            "from googleapiclient.errors import HttpError",
            "from c7n.actions import Action as BaseAction",
            "from c7n.utils import local_session, chunks"
        ],
        "reference_api": [
            "self.invoke_api",
            "self.get_resource_params",
            "self.get_operation_name",
            "self.handle_resource_error",
            "result.get",
            "get"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.get_operation_name",
                "code": "def get_operation_name(self, model, resource):\n        return self.method_spec['op']"
            },
            {
                "name": "self.get_resource_params",
                "code": "def get_resource_params(self, model, resource):\n        raise NotImplementedError(\"subclass responsibility\")"
            },
            {
                "name": "self.invoke_api",
                "code": "def invoke_api(self, client, op_name, params):\n        try:\n            return client.execute_command(op_name, params)\n        except HttpError as e:\n            if e.resp.status in self.ignore_error_codes:\n                return e\n            raise"
            },
            {
                "name": "self.handle_resource_error",
                "code": "def handle_resource_error(self, client, model, resource, op_name, params, error):\n        \"\"\" subclasses implement specific error handling\n        \"\"\""
            }
        ],
        "third_party": [
            "get",
            "get",
            "result.get"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "cloud-custodian/cloud-custodian",
        "function_declaration": "def _add_bindings(self, existing_bindings, bindings_to_add)",
        "start_line": "129",
        "end_line": "166",
        "file_path": "tools/c7n_gcp/c7n_gcp/actions/iampolicy.py",
        "docstring": "The _add_bindings function updates existing IAM policy bindings with new bindings.\\nIt first organizes the existing and new bindings into dictionaries mapping roles to their members.\\nFor each role in the new bindings, it updates the members by adding those not already in the existing members.\\nIt then appends these updated bindings to the list.\\nFinally, it adds any roles from the existing bindings that were not in the new bindings, ensuring no existing roles are omitted.\\nThe function returns the updated list of bindings.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2715c91ce95a",
        "ground_truth": "def _add_bindings(self, existing_bindings, bindings_to_add):\n    \"\"\"\n    Converts the provided lists using `_get_roles_to_bindings_dict`, then iterates through\n    them so that the returned list combines:\n    - among the roles mentioned in a policy, the existing members merged with the ones to add\n      so that there are no duplicates,\n    - as for the other roles, all their members.\n    The roles or members that are mentioned in the policy and already present\n    in the existing bindings are simply ignored with no errors produced.\n    An empty list could be returned only if both `existing_bindings` and `bindings_to_remove`\n    are empty, the possibility of which is defined by the caller of the method.\n    For additional information on how the method works, please refer to the tests\n    (e.g. test_spanner).\n    :param existing_bindings: a list of dictionaries containing the 'role' and 'members' keys\n                              taken from the resource the action is applied to\n    :param bindings_to_add: a list of dictionaries containing the 'role' and 'members' keys\n                            taken from the policy\n    \"\"\"\n    bindings = []\n    roles_to_existing_bindings = self._get_roles_to_bindings_dict(existing_bindings)\n    roles_to_bindings_to_add = self._get_roles_to_bindings_dict(bindings_to_add)\n    for role in roles_to_bindings_to_add:\n        updated_members = dict(roles_to_bindings_to_add[role])\n        if role in roles_to_existing_bindings:\n            existing_members = roles_to_existing_bindings[role]['members']\n            members_to_add = list(filter(lambda member: member not in existing_members,\n                                         updated_members['members']))\n            updated_members['members'] = existing_members + members_to_add\n        bindings.append(updated_members)\n    for role in roles_to_existing_bindings:\n        if role not in roles_to_bindings_to_add:\n            bindings.append(roles_to_existing_bindings[role])\n    return bindings",
        "import_statements": [
            "from c7n.utils import local_session, type_schema",
            "from c7n_gcp.actions import MethodAction"
        ],
        "reference_api": [
            "filter",
            "list",
            "bindings.append",
            "self._get_roles_to_bindings_dict",
            "dict"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_roles_to_bindings_dict",
                "code": "def _get_roles_to_bindings_dict(self, bindings_list):\n        \"\"\"\n        Converts a given list to a dictionary, values under the 'role' key in elements of whose\n        become keys in the resulting dictionary while the elements themselves become values\n        associated with these keys.\n\n        :param bindings_list: a list whose elements are expected to have the 'role' key\n        \"\"\"\n        return {binding['role']: binding for binding in bindings_list}"
            },
            {
                "name": "self._get_roles_to_bindings_dict",
                "code": "def _get_roles_to_bindings_dict(self, bindings_list):\n        \"\"\"\n        Converts a given list to a dictionary, values under the 'role' key in elements of whose\n        become keys in the resulting dictionary while the elements themselves become values\n        associated with these keys.\n\n        :param bindings_list: a list whose elements are expected to have the 'role' key\n        \"\"\"\n        return {binding['role']: binding for binding in bindings_list}"
            }
        ],
        "third_party": [
            "bindings.append",
            "bindings.append"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "cloud-custodian/cloud-custodian",
        "function_declaration": "def _remove_bindings(self, existing_bindings, bindings_to_remove)",
        "start_line": "168",
        "end_line": "209",
        "file_path": "tools/c7n_gcp/c7n_gcp/actions/iampolicy.py",
        "docstring": "The _remove_bindings function updates existing bindings by removing specific members.\\nIt first converts the existing bindings and bindings to remove into dictionaries keyed by roles.\\nFor each role in the bindings to remove, it updates the members list in the existing bindings by excluding the members specified for removal.\\nIf the updated members list is not empty, it adds the updated binding to the result.\\nRoles not in the bindings to remove are directly added to the result.\\nThe function returns the updated list of bindings.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b9786edbb8bd",
        "ground_truth": "def _remove_bindings(self, existing_bindings, bindings_to_remove):\n    \"\"\"\n    Converts the provided lists using `_get_roles_to_bindings_dict`, then iterates through\n    them so that the returned list combines:\n    - among the roles mentioned in a policy, only the members that are not marked for removal,\n    - as for the other roles, all their members.\n    The roles or members that are mentioned in the policy but are absent\n    in the existing bindings are simply ignored with no errors produced.\n    As can be observed, it is possible to have an empty list returned either if\n    `existing_bindings` is already empty or `bindings_to_remove` filters everything out.\n    In addition, a star wildcard could be used as the `members` key value (members: '*')\n    in order to remove all members from a role.\n    For additional information on how the method works, please refer to the tests\n    (e.g. test_spanner).\n    :param existing_bindings: a list of dictionaries containing the 'role' and 'members' keys\n                              taken from the resource the action is applied to\n    :param bindings_to_remove: a list of dictionaries containing the 'role' and 'members' keys\n                               taken from the policy\n    \"\"\"\n    bindings = []\n    roles_to_existing_bindings = self._get_roles_to_bindings_dict(existing_bindings)\n    roles_to_bindings_to_remove = self._get_roles_to_bindings_dict(bindings_to_remove)\n    for role in roles_to_bindings_to_remove:\n        if (role in roles_to_existing_bindings and\n                roles_to_bindings_to_remove[role]['members'] != '*'):\n            updated_members = dict(roles_to_existing_bindings[role])\n            members_to_remove = roles_to_bindings_to_remove[role]\n            updated_members['members'] = list(filter(\n                lambda member: member not in members_to_remove['members'],\n                updated_members['members']))\n            if len(updated_members['members']) > 0:\n                bindings.append(updated_members)\n    for role in roles_to_existing_bindings:\n        if role not in roles_to_bindings_to_remove:\n            bindings.append(roles_to_existing_bindings[role])\n    return bindings",
        "import_statements": [
            "from c7n.utils import local_session, type_schema",
            "from c7n_gcp.actions import MethodAction"
        ],
        "reference_api": [
            "filter",
            "list",
            "bindings.append",
            "self._get_roles_to_bindings_dict",
            "len",
            "dict"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_roles_to_bindings_dict",
                "code": "def _get_roles_to_bindings_dict(self, bindings_list):\n        \"\"\"\n        Converts a given list to a dictionary, values under the 'role' key in elements of whose\n        become keys in the resulting dictionary while the elements themselves become values\n        associated with these keys.\n\n        :param bindings_list: a list whose elements are expected to have the 'role' key\n        \"\"\"\n        return {binding['role']: binding for binding in bindings_list}"
            },
            {
                "name": "self._get_roles_to_bindings_dict",
                "code": "def _get_roles_to_bindings_dict(self, bindings_list):\n        \"\"\"\n        Converts a given list to a dictionary, values under the 'role' key in elements of whose\n        become keys in the resulting dictionary while the elements themselves become values\n        associated with these keys.\n\n        :param bindings_list: a list whose elements are expected to have the 'role' key\n        \"\"\"\n        return {binding['role']: binding for binding in bindings_list}"
            }
        ],
        "third_party": [
            "bindings.append",
            "bindings.append"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "cloud-custodian/cloud-custodian",
        "function_declaration": "def handle_resource_error(self, client, model, resource, op_name, params, error)",
        "start_line": "52",
        "end_line": "62",
        "file_path": "tools/c7n_gcp/c7n_gcp/actions/labels.py",
        "docstring": "The handle_resource_error function handles errors related to resource operations.\\nIt checks if the error reason contains 'fingerprint' and if the model allows refresh.\\nIf both conditions are met, it attempts to refresh the resource and update the 'labelFingerprint' in the parameters.\\nIt then retries the API operation using invoke_api.\\nIf an HttpError occurs and its status is in the ignore_error_codes list, it returns the error.\\nOtherwise, it raises the error.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5538f6649257",
        "ground_truth": "def handle_resource_error(self, client, model, resource, op_name, params, error):\n    if 'fingerprint' not in error.reason or not model.refresh:\n        return\n    try:\n        resource = model.refresh(client, resource)\n        params['body']['labelFingerprint'] = resource['labelFingerprint']\n        return self.invoke_api(client, op_name, params)\n    except HttpError as e:\n        if e.resp.status in self.ignore_error_codes:\n            return e\n        raise",
        "import_statements": [
            "from datetime import datetime, timedelta",
            "from dateutil import tz as tzutil",
            "from googleapiclient.errors import HttpError",
            "from c7n.utils import type_schema",
            "from c7n.filters import FilterValidationError",
            "from c7n.filters.offhours import Time",
            "from c7n.lookup import Lookup",
            "from c7n_gcp.actions import MethodAction",
            "from c7n_gcp.filters.labels import LabelActionFilter",
            "from c7n_gcp.provider import resources as gcp_resources"
        ],
        "reference_api": [
            "model.refresh",
            "self.invoke_api"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "model.refresh",
            "self.invoke_api"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "cloud-custodian/cloud-custodian",
        "function_declaration": "def generate_timestamp(self, days, hours)",
        "start_line": "205",
        "end_line": "216",
        "file_path": "tools/c7n_gcp/c7n_gcp/actions/labels.py",
        "docstring": "The generate_timestamp function creates a formatted timestamp string based on the current datetime and given days and hours.\\nIf days or hours are not provided, it defaults to 4 days.\\nIt adds the specified days and hours to the current datetime and formats the result as 'YYYY_MM_DD__HH_MM' if hours are greater than 0.\\nOtherwise, it formats the result as 'YYYY_MM_DD__0_0'.\\nThe function returns this formatted timestamp string.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "92efd757df8e",
        "ground_truth": "def generate_timestamp(self, days, hours):\n    n = datetime.now(tz=self.tz)\n    if days is None or hours is None:\n        # maintains default value of days being 4 if nothing is provided\n        days = 4\n    action_date = (n + timedelta(days=days, hours=hours))\n    if hours > 0:\n        action_date_string = action_date.strftime('%Y_%m_%d__%H_%M')\n    else:\n        action_date_string = action_date.strftime('%Y_%m_%d__0_0')\n    return action_date_string",
        "import_statements": [
            "from datetime import datetime, timedelta",
            "from dateutil import tz as tzutil",
            "from googleapiclient.errors import HttpError",
            "from c7n.utils import type_schema",
            "from c7n.filters import FilterValidationError",
            "from c7n.filters.offhours import Time",
            "from c7n.lookup import Lookup",
            "from c7n_gcp.actions import MethodAction",
            "from c7n_gcp.filters.labels import LabelActionFilter",
            "from c7n_gcp.provider import resources as gcp_resources"
        ],
        "reference_api": [
            "action_date.strftime",
            "datetime.now",
            "timedelta"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "timedelta",
            "action_date.strftime",
            "action_date.strftime"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "cloud-custodian/cloud-custodian",
        "function_declaration": "def publish_message(self, message, client)",
        "start_line": "89",
        "end_line": "99",
        "file_path": "tools/c7n_gcp/c7n_gcp/actions/notify.py",
        "docstring": "The publish_message function sends a message to a Google Cloud Platform (GCP) Pub/Sub topic using a client.\\nIt executes the 'publish' command on the client with the topic specified in self.data['transport']['topic'] and the message data, which is packed using the self.pack method.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "10b9f8ef096d",
        "ground_truth": "def publish_message(self, message, client):\n    \"\"\"Publish message to a GCP pub/sub topic\n     \"\"\"\n    return client.execute_command('publish', {\n        'topic': self.data['transport']['topic'],\n        'body': {\n            'messages': {\n                'data': self.pack(message)\n            }\n        }\n    })",
        "import_statements": [
            "from c7n.actions import BaseNotify",
            "from c7n import utils",
            "from c7n.resolver import ValuesFrom",
            "from c7n_gcp.provider import resources as gcp_resources",
            "from c7n.version import version"
        ],
        "reference_api": [
            "self.pack",
            "client.execute_command"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "client.execute_command",
            "self.pack"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "cloud-custodian/cloud-custodian",
        "function_declaration": "def process(self, resources, event=None)",
        "start_line": "144",
        "end_line": "165",
        "file_path": "tools/c7n_gcp/c7n_gcp/filters/iampolicy.py",
        "docstring": "The process function retrieves IAM policy information for each resource in the resources list.\\nIt initializes a session and client, then iterates through the resources.\\nFor each resource, it gets the IAM policy using either the 'projectId' or 'name' key.\\nIt constructs a user-to-roles mapping from the policy bindings and adds this mapping to the resource under 'c7n:iamPolicyUserRolePair'.\\nFinally, it calls the superclass's process method with the updated resources.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8d288f98a7db",
        "ground_truth": "def process(self, resources, event=None):\n    model = self.manager.get_model()\n    session = local_session(self.manager.session_factory)\n    client = self.get_client(session, model)\n    for r in resources:\n        resource_key = 'projectId' if 'projectId' in r else 'name'\n        iam_policy = client.execute_command('getIamPolicy', {\"resource\": r[resource_key]})\n        r[\"c7n:iamPolicyUserRolePair\"] = {}\n        userToRolesMap = {}\n        for b in iam_policy[\"bindings\"]:\n            role, members = b[\"role\"], b[\"members\"]\n            for user in members:\n                if user in userToRolesMap:\n                    userToRolesMap[user].append(role)\n                else:\n                    userToRolesMap[user] = [role]\n        for user, roles in userToRolesMap.items():\n            r[\"c7n:iamPolicyUserRolePair\"][user] = roles\n    return super(IamPolicyUserRolePairFilter, self).process(resources)",
        "import_statements": [
            "import copy",
            "from c7n.filters.core import Filter, ValueFilter",
            "from c7n.utils import local_session, type_schema"
        ],
        "reference_api": [
            "process",
            "super",
            "get_model",
            "self.get_client",
            "local_session",
            "client.execute_command",
            "append",
            "userToRolesMap.items"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "get_model",
                "code": "def get_model(self):\n        return ListItemModel"
            },
            {
                "name": "local_session",
                "code": "def local_session(factory, region=None):\n    \"\"\"Cache a session thread local for up to 45m\"\"\"\n    factory_region = getattr(factory, 'region', 'global')\n    if region:\n        factory_region = region\n    s = getattr(CONN_CACHE, factory_region, {}).get('session')\n    t = getattr(CONN_CACHE, factory_region, {}).get('time')\n\n    n = time.time()\n    if s is not None and t + (60 * 45) > n:\n        return s\n    s = factory()\n\n    setattr(CONN_CACHE, factory_region, {'session': s, 'time': n})\n    return s"
            },
            {
                "name": "self.get_client",
                "code": "def get_client(self, session, model):\n        return session.client(\n            model.service, model.version, model.component)"
            },
            {
                "name": "process",
                "code": "def process(self, resources, event=None):\n        if 'doc' in self.data:\n            try:\n                resources = self.process_resources(resources)\n            except TypeError:\n                valueFilter = IamPolicyValueFilter(self.data['doc'], self.manager, \"bucket\")\n                resources = valueFilter.process(resources)\n        if 'user-role' in self.data:\n            user_role = self.data['user-role']\n            key = user_role['user']\n            val = user_role['role']\n            op = 'in' if user_role.get('has', True) else 'not-in'\n            value_type = 'swap'\n            userRolePairFilter = IamPolicyUserRolePairFilter({'key': key, 'value': val,\n            'op': op, 'value_type': value_type}, self.manager)\n            resources = userRolePairFilter.process(resources)\n\n        return resources"
            }
        ],
        "third_party": [
            "client.execute_command",
            "append",
            "userToRolesMap.items"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "cloud-custodian/cloud-custodian",
        "function_declaration": "def process_resource(self, resource)",
        "start_line": "195",
        "end_line": "209",
        "file_path": "tools/c7n_gcp/c7n_gcp/filters/metrics.py",
        "docstring": "The process_resource function processes a given resource to determine if it meets specified metric criteria.\\nIt sets up a 'c7n.metrics' dictionary in the resource and retrieves the resource's metric name.\\nThe function checks if the metric exists in resource_metric_dict; if not, it uses a missing value if defined.\\nThe metric value is converted to a float, and the 'c7n.metrics' dictionary is updated with the metric.\\nFinally, it compares the metric value with a specified value using a comparison operation (op) and returns the result.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5168cc45c9eb",
        "ground_truth": "def process_resource(self, resource):\n    resource_metric = resource.setdefault('c7n.metrics', {})\n    resource_name = self.manager.resource_type.get_metric_resource_name(resource)\n    metric = self.resource_metric_dict.get(resource_name)\n    if not metric and not self.missing_value:\n        return False\n    if not metric:\n        metric_value = self.missing_value\n    else:\n        metric_value = float(list(metric[\"points\"][0][\"value\"].values())[0])\n    resource_metric[self.c7n_metric_key] = metric\n    matched = self.op(metric_value, self.value)\n    return matched",
        "import_statements": [
            "from datetime import datetime, timedelta",
            "from c7n.filters.core import Filter, OPERATORS, FilterValidationError",
            "from c7n.utils import local_session, type_schema, jmespath_search",
            "from c7n_gcp.provider import resources as gcp_resources"
        ],
        "reference_api": [
            "list",
            "resource.setdefault",
            "float",
            "values",
            "get_metric_resource_name",
            "get",
            "self.op"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get_metric_resource_name",
            "get",
            "values",
            "self.op"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "cloud-custodian/cloud-custodian",
        "function_declaration": "def process_resources(self, resources)",
        "start_line": "45",
        "end_line": "48",
        "file_path": "tools/c7n_gcp/c7n_gcp/resources/cloudrun.py",
        "docstring": "The process_resources function processes a list of resources using an IamPolicyValueFilter. It initializes the filter with a document and a manager from the instance's data. It also sets the filter's _verb_arguments attribute to the instance's _verb_arguments. Finally, it calls the filter's process method with the provided resources and returns the result.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f24c5f261442",
        "ground_truth": "def process_resources(self, resources):\n    value_filter = IamPolicyValueFilter(self.data[\"doc\"], self.manager)\n    value_filter._verb_arguments = self._verb_arguments\n    return value_filter.process(resources)",
        "import_statements": [
            "from c7n_gcp.provider import resources",
            "from c7n_gcp.query import QueryResourceManager, TypeInfo",
            "from c7n_gcp.filters import IamPolicyFilter",
            "from c7n_gcp.filters.iampolicy import IamPolicyValueFilter",
            "from c7n.utils import local_session"
        ],
        "reference_api": [
            "IamPolicyValueFilter",
            "value_filter.process"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "IamPolicyValueFilter",
            "value_filter.process"
        ]
    },
    {
        "subclass": "GCP",
        "owner/repo": "encoredev/encore",
        "function_declaration": "func gcpProjectIDFromMetadata() string",
        "start_line": "106",
        "end_line": "131",
        "file_path": "runtimes/go/appruntime/shared/cloudtrace/gcp.go",
        "docstring": "The gcpProjectIDFromMetadata function retrieves the Google Cloud Platform project ID from the metadata server. It sends a GET request to the metadata URL with the required header. If the request is successful and returns a 200 OK status, it reads the project ID from the response body. If any error occurs during these steps, it returns an empty string.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2c8c89f62d7d",
        "ground_truth": "func gcpProjectIDFromMetadata() string {\n const url = \"http://metadata.google.internal/computeMetadata/v1/project/project-id\"\n // nosemgrep\n req, err := http.NewRequest(\"GET\", url, nil)\n if err != nil {\n  return \"\"\n }\n req.Header.Add(\"Metadata-Flavor\", \"Google\")\n  resp, err := http.DefaultClient.Do(req)\n if err != nil {\n  return \"\"\n }\n defer func() { _ = resp.Body.Close() }()\n  if resp.StatusCode != http.StatusOK {\n  return \"\"\n }\n  projectIDBytes, err := io.ReadAll(resp.Body)\n if err != nil {\n  return \"\"\n }\n  return string(projectIDBytes)\n}",
        "import_statements": [
            "import (\n\t\"encoding/json\"\n\t\"io\"\n\t\"net/http\"\n\t\"os\"\n\t\"sync\"\n)"
        ],
        "reference_api": [
            "string",
            "func() { _ = resp.Body.Close() }",
            "io.ReadAll",
            "http.DefaultClient.Do",
            "req.Header.Add",
            "resp.Body.Close",
            "http.NewRequest"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "string",
            "func() { _ = resp.Body.Close() }",
            "http.DefaultClient.Do",
            "req.Header.Add",
            "resp.Body.Close"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "pulumi/pulumi",
        "function_declaration": "func (p *AssetArchiveProvider) CheckConfig(\n\t_ context.Context, req plugin.CheckConfigRequest,\n) (plugin.CheckConfigResponse, error)",
        "start_line": "107",
        "end_line": "129",
        "file_path": "cmd/pulumi-test-language/providers/asset_archive_provider.go",
        "docstring": "The CheckConfig function of the AssetArchiveProvider validates the configuration by checking if the version is present, is a string, and equals \"5.0.0\". If any of these conditions are not met, it returns a CheckConfigResponse with an appropriate failure message. If there are any additional properties in the configuration, it also returns a failure message. If all conditions are met, it returns a CheckConfigResponse with the provided properties.\n",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9b68879c2952",
        "ground_truth": "func (p *AssetArchiveProvider) CheckConfig(\n _ context.Context, req plugin.CheckConfigRequest,\n) (plugin.CheckConfigResponse, error) {\n // Expect just the version\n version, ok := req.News[\"version\"]\n if !ok {\n  return plugin.CheckConfigResponse{Failures: makeCheckFailure(\"version\", \"missing version\")}, nil\n }\n if !version.IsString() {\n  return plugin.CheckConfigResponse{Failures: makeCheckFailure(\"version\", \"version is not a string\")}, nil\n }\n if version.StringValue() != \"5.0.0\" {\n  return plugin.CheckConfigResponse{Failures: makeCheckFailure(\"version\", \"version is not 5.0.0\")}, nil\n }\n  if len(req.News) != 1 {\n  return plugin.CheckConfigResponse{\n   Failures: makeCheckFailure(\"\", fmt.Sprintf(\"too many properties: %v\", req.News)),\n  }, nil\n }\n  return plugin.CheckConfigResponse{Properties: req.News}, nil\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\n\t\"github.com/blang/semver\"\n\n\t\"github.com/pulumi/pulumi/pkg/v3/codegen/schema\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource/plugin\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/tokens\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/workspace\"\n)"
        ],
        "reference_api": [
            "makeCheckFailure",
            "version.IsString",
            "len",
            "version.StringValue",
            "fmt.Sprintf"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "makeCheckFailure",
            "version.IsString",
            "len",
            "version.StringValue"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "pulumi/pulumi",
        "function_declaration": "func (p *AssetArchiveProvider) Create(\n\t_ context.Context, req plugin.CreateRequest,\n) (plugin.CreateResponse, error)",
        "start_line": "160",
        "end_line": "178",
        "file_path": "cmd/pulumi-test-language/providers/asset_archive_provider.go",
        "docstring": "The Create function in the AssetArchiveProvider handles resource creation requests. It first checks the type of the requested resource using the checkType method. If the type check fails, it returns an unknown status with an error. If the request is a preview, it sets the resource ID to an empty string; otherwise, it sets it to \"id\". Finally, it returns a response with the resource ID, the original properties, and a status of OK.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "065896d88fa9",
        "ground_truth": "func (p *AssetArchiveProvider) Create(\n _ context.Context, req plugin.CreateRequest,\n) (plugin.CreateResponse, error) {\n _, err := p.checkType(req.URN)\n if err != nil {\n  return plugin.CreateResponse{Status: resource.StatusUnknown}, err\n }\n  id := \"id\"\n if req.Preview {\n  id = \"\"\n }\n  return plugin.CreateResponse{\n  ID:         resource.ID(id),\n  Properties: req.Properties,\n  Status:     resource.StatusOK,\n }, nil\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\n\t\"github.com/blang/semver\"\n\n\t\"github.com/pulumi/pulumi/pkg/v3/codegen/schema\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource/plugin\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/tokens\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/workspace\"\n)"
        ],
        "reference_api": [
            "resource.ID",
            "p.checkType"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "resource.ID",
            "p.checkType"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "pulumi/pulumi",
        "function_declaration": "func Start(ctx context.Context) (LanguageTestServer, error)",
        "start_line": "71",
        "end_line": "93",
        "file_path": "cmd/pulumi-test-language/interface.go",
        "docstring": "The Start function initializes and starts a gRPC server for a language test service. It creates a new language test server instance with a context and a cancel channel. The function then sets up the gRPC server to listen on an available port and registers the language test server with the gRPC server. If the server starts successfully, it returns the server instance and its address. If there is an error during setup, it returns the error.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0050b261c5b3",
        "ground_truth": "func Start(ctx context.Context) (LanguageTestServer, error) {\n // New up an engine RPC server.\n server := &languageTestServer{\n  ctx:    ctx,\n  cancel: make(chan bool),\n }\n  // Fire up a gRPC server and start listening for incomings.\n port, done, err := rpcutil.Serve(0, server.cancel, []func(*grpc.Server) error{\n  func(srv *grpc.Server) error {\n   testingrpc.RegisterLanguageTestServer(srv, server)\n   return nil\n  },\n }, nil)\n if err != nil {\n  return nil, err\n }\n  server.addr = fmt.Sprintf(\"127.0.0.1:%d\", port)\n server.done = done\n  return server, nil\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"context\"\n\tb64 \"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\n\tmapset \"github.com/deckarep/golang-set/v2\"\n\n\t\"github.com/blang/semver\"\n\t\"github.com/pulumi/pulumi/pkg/v3/backend\"\n\tbackendDisplay \"github.com/pulumi/pulumi/pkg/v3/backend/display\"\n\t\"github.com/pulumi/pulumi/pkg/v3/backend/diy\"\n\t\"github.com/pulumi/pulumi/pkg/v3/codegen/schema\"\n\t\"github.com/pulumi/pulumi/pkg/v3/engine\"\n\t\"github.com/pulumi/pulumi/pkg/v3/resource/deploy\"\n\t\"github.com/pulumi/pulumi/pkg/v3/resource/stack\"\n\tb64secrets \"github.com/pulumi/pulumi/pkg/v3/secrets/b64\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/apitype\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/diag\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/diag/colors\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource/plugin\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/tokens\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/contract\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/rpcutil\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/workspace\"\n\tpulumirpc \"github.com/pulumi/pulumi/sdk/v3/proto/go\"\n\ttestingrpc \"github.com/pulumi/pulumi/sdk/v3/proto/go/testing\"\n\t\"github.com/segmentio/encoding/json\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n)"
        ],
        "reference_api": [
            "fmt.Sprintf",
            "make",
            "rpcutil.Serve",
            "testingrpc.RegisterLanguageTestServer"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "make",
            "rpcutil.Serve",
            "testingrpc.RegisterLanguageTestServer"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "pulumi/pulumi",
        "function_declaration": "func (l *providerLoader) LoadPackageReference(pkg string, version *semver.Version) (schema.PackageReference, error)",
        "start_line": "127",
        "end_line": "168",
        "file_path": "cmd/pulumi-test-language/interface.go",
        "docstring": "The LoadPackageReference function attempts to load a package reference for a specified package name and version. If the package is \"pulumi\", it returns the default Pulumi package reference. Otherwise, it searches through available providers to find one matching the package name and version. If a matching provider is found, it retrieves and parses the provider's schema into a package reference. If no matching provider is found or if any errors occur, appropriate error messages are returned.\n",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f9a44f80f260",
        "ground_truth": "func (l *providerLoader) LoadPackageReference(pkg string, version *semver.Version) (schema.PackageReference, error) {\n if pkg == \"pulumi\" {\n  return schema.DefaultPulumiPackage.Reference(), nil\n }\n  // Find the provider with the given package name\n var provider plugin.Provider\n for _, p := range l.providers {\n  if string(p.Pkg()) == pkg {\n   info, err := p.GetPluginInfo(context.TODO())\n   if err != nil {\n    return nil, fmt.Errorf(\"get plugin info for %s: %w\", pkg, err)\n   }\n    if version == nil || (info.Version != nil && version.EQ(*info.Version)) {\n    provider = p\n    break\n   }\n  }\n }\n  if provider == nil {\n  return nil, fmt.Errorf(\"could not load schema for %s, provider not known\", pkg)\n }\n  jsonSchema, err := provider.GetSchema(context.TODO(), plugin.GetSchemaRequest{})\n if err != nil {\n  return nil, fmt.Errorf(\"get schema for %s: %w\", pkg, err)\n }\n  var spec schema.PartialPackageSpec\n if _, err := json.Parse(jsonSchema.Schema, &spec, json.ZeroCopy); err != nil {\n  return nil, err\n }\n  p, err := schema.ImportPartialSpec(spec, nil, l)\n if err != nil {\n  return nil, err\n }\n  return p, nil\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"context\"\n\tb64 \"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\n\tmapset \"github.com/deckarep/golang-set/v2\"\n\n\t\"github.com/blang/semver\"\n\t\"github.com/pulumi/pulumi/pkg/v3/backend\"\n\tbackendDisplay \"github.com/pulumi/pulumi/pkg/v3/backend/display\"\n\t\"github.com/pulumi/pulumi/pkg/v3/backend/diy\"\n\t\"github.com/pulumi/pulumi/pkg/v3/codegen/schema\"\n\t\"github.com/pulumi/pulumi/pkg/v3/engine\"\n\t\"github.com/pulumi/pulumi/pkg/v3/resource/deploy\"\n\t\"github.com/pulumi/pulumi/pkg/v3/resource/stack\"\n\tb64secrets \"github.com/pulumi/pulumi/pkg/v3/secrets/b64\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/apitype\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/diag\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/diag/colors\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource/plugin\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/tokens\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/contract\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/rpcutil\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/workspace\"\n\tpulumirpc \"github.com/pulumi/pulumi/sdk/v3/proto/go\"\n\ttestingrpc \"github.com/pulumi/pulumi/sdk/v3/proto/go/testing\"\n\t\"github.com/segmentio/encoding/json\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n)"
        ],
        "reference_api": [
            "schema.DefaultPulumiPackage.Reference",
            "provider.GetSchema",
            "fmt.Errorf",
            "string",
            "schema.ImportPartialSpec",
            "p.GetPluginInfo",
            "json.Parse",
            "context.TODO",
            "p.Pkg",
            "version.EQ"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "schema.DefaultPulumiPackage.Reference",
            "provider.GetSchema",
            "string",
            "schema.ImportPartialSpec",
            "p.GetPluginInfo",
            "json.Parse",
            "context.TODO",
            "p.Pkg",
            "version.EQ"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "pulumi/pulumi",
        "function_declaration": "func (l *providerLoader) LoadPackage(pkg string, version *semver.Version) (*schema.Package, error) ",
        "start_line": "170",
        "end_line": "176",
        "file_path": "cmd/pulumi-test-language/interface.go",
        "docstring": "The LoadPackage function in the providerLoader struct attempts to load a package by first calling the LoadPackageReference method with the specified package name and version. If successful, it then retrieves and returns the package definition using the Definition method. If an error occurs during the reference loading, it returns the error.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "18a8d6382c8d",
        "ground_truth": "func (l *providerLoader) LoadPackage(pkg string, version *semver.Version) (*schema.Package, error) {\n ref, err := l.LoadPackageReference(pkg, version)\n if err != nil {\n  return nil, err\n }\n return ref.Definition()\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"context\"\n\tb64 \"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\n\tmapset \"github.com/deckarep/golang-set/v2\"\n\n\t\"github.com/blang/semver\"\n\t\"github.com/pulumi/pulumi/pkg/v3/backend\"\n\tbackendDisplay \"github.com/pulumi/pulumi/pkg/v3/backend/display\"\n\t\"github.com/pulumi/pulumi/pkg/v3/backend/diy\"\n\t\"github.com/pulumi/pulumi/pkg/v3/codegen/schema\"\n\t\"github.com/pulumi/pulumi/pkg/v3/engine\"\n\t\"github.com/pulumi/pulumi/pkg/v3/resource/deploy\"\n\t\"github.com/pulumi/pulumi/pkg/v3/resource/stack\"\n\tb64secrets \"github.com/pulumi/pulumi/pkg/v3/secrets/b64\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/apitype\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/diag\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/diag/colors\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource/plugin\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/tokens\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/contract\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/rpcutil\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/workspace\"\n\tpulumirpc \"github.com/pulumi/pulumi/sdk/v3/proto/go\"\n\ttestingrpc \"github.com/pulumi/pulumi/sdk/v3/proto/go/testing\"\n\t\"github.com/segmentio/encoding/json\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n)"
        ],
        "reference_api": [
            "ref.Definition",
            "l.LoadPackageReference"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ref.Definition",
            "l.LoadPackageReference"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "pulumi/pulumi",
        "function_declaration": "func (h *testHost) EnsurePlugins(plugins []workspace.PluginSpec, kinds plugin.Flags) error",
        "start_line": "267",
        "end_line": "298",
        "file_path": "cmd/pulumi-test-language/interface.go",
        "docstring": "The EnsurePlugins function checks if the specified plugins match the expected plugins for a test host. It creates a set of expected plugins based on the runtime name and providers, then compares this set to the actual plugins provided. If there are any differences, such as missing or unexpected plugins, it returns an error with details of the discrepancies. If the sets match, it returns nil.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7c1f0b0de22c",
        "ground_truth": "func (h *testHost) EnsurePlugins(plugins []workspace.PluginSpec, kinds plugin.Flags) error {\n // EnsurePlugins will be called with the result of GetRequiredPlugins, so we can use this to check\n // that that returned the expected plugins (with expected versions).\n expected := mapset.NewSet(\n  fmt.Sprintf(\"language-%s@<nil>\", h.runtimeName),\n )\n for _, provider := range h.providers {\n  pkg := provider.Pkg()\n  version, err := getProviderVersion(provider)\n  if err != nil {\n   return fmt.Errorf(\"get provider version %s: %w\", pkg, err)\n  }\n  expected.Add(fmt.Sprintf(\"resource-%s@%s\", pkg, version))\n }\n  actual := mapset.NewSetWithSize[string](len(plugins))\n for _, plugin := range plugins {\n  actual.Add(fmt.Sprintf(\"%s-%s@%s\", plugin.Kind, plugin.Name, plugin.Version))\n }\n  // Symmetric difference, we want to know if there are any unexpected plugins, or any missing plugins.\n diff := expected.SymmetricDifference(actual)\n if !diff.IsEmpty() {\n  expectedSlice := expected.ToSlice()\n  slices.Sort(expectedSlice)\n  actualSlice := actual.ToSlice()\n  slices.Sort(actualSlice)\n  return fmt.Errorf(\"unexpected required plugins: actual %v, expected %v\", actualSlice, expectedSlice)\n }\n  return nil\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"context\"\n\tb64 \"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\n\tmapset \"github.com/deckarep/golang-set/v2\"\n\n\t\"github.com/blang/semver\"\n\t\"github.com/pulumi/pulumi/pkg/v3/backend\"\n\tbackendDisplay \"github.com/pulumi/pulumi/pkg/v3/backend/display\"\n\t\"github.com/pulumi/pulumi/pkg/v3/backend/diy\"\n\t\"github.com/pulumi/pulumi/pkg/v3/codegen/schema\"\n\t\"github.com/pulumi/pulumi/pkg/v3/engine\"\n\t\"github.com/pulumi/pulumi/pkg/v3/resource/deploy\"\n\t\"github.com/pulumi/pulumi/pkg/v3/resource/stack\"\n\tb64secrets \"github.com/pulumi/pulumi/pkg/v3/secrets/b64\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/apitype\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/diag\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/diag/colors\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/resource/plugin\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/tokens\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/contract\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/rpcutil\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/workspace\"\n\tpulumirpc \"github.com/pulumi/pulumi/sdk/v3/proto/go\"\n\ttestingrpc \"github.com/pulumi/pulumi/sdk/v3/proto/go/testing\"\n\t\"github.com/segmentio/encoding/json\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/credentials/insecure\"\n)"
        ],
        "reference_api": [
            "getProviderVersion",
            "mapset.NewSet",
            "provider.Pkg",
            "fmt.Errorf",
            "actual.ToSlice",
            "expected.Add",
            "diff.IsEmpty",
            "expected.ToSlice",
            "len",
            "slices.Sort",
            "fmt.Sprintf",
            "actual.Add",
            "expected.SymmetricDifference"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getProviderVersion",
                "code": "func getProviderVersion(provider plugin.Provider) (semver.Version, error) {\n\tpkg := provider.Pkg()\n\tinfo, err := provider.GetPluginInfo(context.TODO())\n\tif err != nil {\n\t\treturn semver.Version{}, fmt.Errorf(\"get plugin info for %s: %w\", pkg, err)\n\t}\n\tif info.Version == nil {\n\t\treturn semver.Version{}, fmt.Errorf(\"provider %s has no version\", pkg)\n\t}\n\treturn *info.Version, nil\n}"
            }
        ],
        "third_party": [
            "mapset.NewSet",
            "provider.Pkg",
            "actual.ToSlice",
            "expected.Add",
            "diff.IsEmpty",
            "expected.ToSlice",
            "len",
            "slices.Sort",
            "actual.Add",
            "expected.SymmetricDifference"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "pulumi/pulumi",
        "function_declaration": "func editSnapshot(snapshotDirectory string, edits []compiledReplacement) (string, error)",
        "start_line": "223",
        "end_line": "239",
        "file_path": "cmd/pulumi-test-language/snapshots.go",
        "docstring": "The editSnapshot function copies a snapshot directory to a temporary directory and applies a list of edits if any are provided. If edits are to be applied, it creates a temporary directory, copies the contents of the snapshot directory to this temporary directory while applying the specified edits, and then returns the path to the temporary directory. If no edits are provided, it returns the original snapshot directory path.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e31339121a24",
        "ground_truth": "func editSnapshot(snapshotDirectory string, edits []compiledReplacement) (string, error) {\n // If we have any edits to apply then we need to copy to a temporary directory and apply the edits there.\n result := snapshotDirectory\n if len(edits) > 0 {\n  var err error\n  result, err = os.MkdirTemp(\"\", \"pulumi-test-language\")\n  if err != nil {\n   return \"\", fmt.Errorf(\"create temp dir: %w\", err)\n  }\n   err = copyDirectory(os.DirFS(snapshotDirectory), \".\", result, edits, nil)\n  if err != nil {\n   return \"\", fmt.Errorf(\"copy source dir: %w\", err)\n  }\n }\n return result, nil\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\tiofs \"io/fs\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\t\"github.com/hexops/gotextdiff\"\n\t\"github.com/hexops/gotextdiff/myers\"\n\t\"github.com/hexops/gotextdiff/span\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/cmdutil\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/contract\"\n)"
        ],
        "reference_api": [
            "fmt.Errorf",
            "os.MkdirTemp",
            "copyDirectory",
            "os.DirFS",
            "len"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "copyDirectory",
                "code": "func copyDirectory(fs iofs.FS, src string, dst string, edits []compiledReplacement, filter []string) error {\n\treturn iofs.WalkDir(fs, src, func(path string, d iofs.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tinclude := true\n\t\tfor _, f := range filter {\n\t\t\tif strings.Contains(path, f) {\n\t\t\t\tinclude = false\n\t\t\t}\n\t\t}\n\t\tif !include {\n\t\t\treturn nil\n\t\t}\n\n\t\trelativePath, err := filepath.Rel(src, path)\n\t\tcontract.AssertNoErrorf(err, \"path %s should be relative to %s\", path, src)\n\n\t\tsrcPath := filepath.Join(src, relativePath)\n\t\tdstPath := filepath.Join(dst, relativePath)\n\t\tcontract.Assertf(srcPath == path, \"srcPath %s should be equal to path %s\", srcPath, path)\n\n\t\tif d.IsDir() {\n\t\t\treturn os.MkdirAll(dstPath, 0o700)\n\t\t}\n\n\t\tsrcFile, err := fs.Open(srcPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"open file %s: %w\", srcPath, err)\n\t\t}\n\t\tdefer srcFile.Close()\n\n\t\tdstFile, err := os.Create(dstPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"create file %s: %w\", dstPath, err)\n\t\t}\n\t\tdefer dstFile.Close()\n\n\t\teditsToApply := []compiledReplacement{}\n\t\tfor _, replace := range edits {\n\t\t\tif replace.Path.MatchString(relativePath) {\n\t\t\t\teditsToApply = append(editsToApply, replace)\n\t\t\t}\n\t\t}\n\n\t\tif len(editsToApply) > 0 {\n\t\t\t// Apply edits to the file\n\t\t\tdata, err := io.ReadAll(srcFile)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"read file %s: %w\", srcPath, err)\n\t\t\t}\n\n\t\t\tsrc := string(data)\n\t\t\tfor len(src) > 0 {\n\t\t\t\t// Find a line of text\n\t\t\t\ti := strings.Index(src, \"\\n\")\n\t\t\t\ttext := src\n\t\t\t\tif i == -1 {\n\t\t\t\t\t// Last line, process text (set to src above) then exit the loop\n\t\t\t\t\tsrc = \"\"\n\t\t\t\t} else {\n\t\t\t\t\t// Extract the line of text _including_ the newline and remove it from src\n\t\t\t\t\ttext = src[:i+1]\n\t\t\t\t\tsrc = src[i+1:]\n\t\t\t\t}\n\n\t\t\t\tfor _, edit := range editsToApply {\n\t\t\t\t\ttext = edit.Pattern.ReplaceAllString(text, edit.Replacement)\n\t\t\t\t}\n\t\t\t\t_, err = dstFile.WriteString(text)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"write file %s: %w\", dstPath, err)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t// Can just do a straight copy\n\t\t\t_, err = io.Copy(dstFile, srcFile)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"copy file %s->%s: %w\", srcPath, dstPath, err)\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n}"
            }
        ],
        "third_party": [
            "os.MkdirTemp",
            "os.DirFS",
            "len"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "pulumi/pulumi",
        "function_declaration": "func doSnapshot(\n\tdisableSnapshotWriting bool,\n\tsourceDirectory, snapshotDirectory string,\n) ([]string, error)",
        "start_line": "243",
        "end_line": "269",
        "file_path": "cmd/pulumi-test-language/snapshots.go",
        "docstring": "The doSnapshot function manages the creation and validation of snapshots for a specified source directory. If snapshot writing is enabled and the PULUMI_ACCEPT environment variable is set, it first removes any existing snapshot directory, then creates a new snapshot directory, and copies the contents from the source directory to the snapshot directory. If snapshot writing is disabled, it compares the source directory with the snapshot directory and returns any validation results. Any errors encountered during these operations are returned.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8cd99604eebf",
        "ground_truth": "func doSnapshot(\n disableSnapshotWriting bool,\n sourceDirectory, snapshotDirectory string,\n) ([]string, error) {\n if !disableSnapshotWriting && cmdutil.IsTruthy(os.Getenv(\"PULUMI_ACCEPT\")) {\n  // Write files\n  err := os.RemoveAll(snapshotDirectory)\n  if err != nil {\n   return nil, fmt.Errorf(\"remove snapshot dir: %w\", err)\n  }\n  err = os.MkdirAll(snapshotDirectory, 0o755)\n  if err != nil {\n   return nil, fmt.Errorf(\"create snapshot dir: %w\", err)\n  }\n  err = copyDirectory(os.DirFS(sourceDirectory), \".\", snapshotDirectory, nil, nil)\n  if err != nil {\n   return nil, fmt.Errorf(\"copy snapshot dir: %w\", err)\n  }\n  return nil, nil\n }\n validations, err := compareDirectories(sourceDirectory, snapshotDirectory, false /* allowNewFiles */)\n if err != nil {\n  return nil, err\n }\n  return validations, nil\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\tiofs \"io/fs\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\t\"github.com/hexops/gotextdiff\"\n\t\"github.com/hexops/gotextdiff/myers\"\n\t\"github.com/hexops/gotextdiff/span\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/cmdutil\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/common/util/contract\"\n)"
        ],
        "reference_api": [
            "fmt.Errorf",
            "copyDirectory",
            "os.DirFS",
            "compareDirectories",
            "os.RemoveAll",
            "cmdutil.IsTruthy",
            "os.MkdirAll",
            "os.Getenv"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "copyDirectory",
                "code": "func copyDirectory(fs iofs.FS, src string, dst string, edits []compiledReplacement, filter []string) error {\n\treturn iofs.WalkDir(fs, src, func(path string, d iofs.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tinclude := true\n\t\tfor _, f := range filter {\n\t\t\tif strings.Contains(path, f) {\n\t\t\t\tinclude = false\n\t\t\t}\n\t\t}\n\t\tif !include {\n\t\t\treturn nil\n\t\t}\n\n\t\trelativePath, err := filepath.Rel(src, path)\n\t\tcontract.AssertNoErrorf(err, \"path %s should be relative to %s\", path, src)\n\n\t\tsrcPath := filepath.Join(src, relativePath)\n\t\tdstPath := filepath.Join(dst, relativePath)\n\t\tcontract.Assertf(srcPath == path, \"srcPath %s should be equal to path %s\", srcPath, path)\n\n\t\tif d.IsDir() {\n\t\t\treturn os.MkdirAll(dstPath, 0o700)\n\t\t}\n\n\t\tsrcFile, err := fs.Open(srcPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"open file %s: %w\", srcPath, err)\n\t\t}\n\t\tdefer srcFile.Close()\n\n\t\tdstFile, err := os.Create(dstPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"create file %s: %w\", dstPath, err)\n\t\t}\n\t\tdefer dstFile.Close()\n\n\t\teditsToApply := []compiledReplacement{}\n\t\tfor _, replace := range edits {\n\t\t\tif replace.Path.MatchString(relativePath) {\n\t\t\t\teditsToApply = append(editsToApply, replace)\n\t\t\t}\n\t\t}\n\n\t\tif len(editsToApply) > 0 {\n\t\t\t// Apply edits to the file\n\t\t\tdata, err := io.ReadAll(srcFile)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"read file %s: %w\", srcPath, err)\n\t\t\t}\n\n\t\t\tsrc := string(data)\n\t\t\tfor len(src) > 0 {\n\t\t\t\t// Find a line of text\n\t\t\t\ti := strings.Index(src, \"\\n\")\n\t\t\t\ttext := src\n\t\t\t\tif i == -1 {\n\t\t\t\t\t// Last line, process text (set to src above) then exit the loop\n\t\t\t\t\tsrc = \"\"\n\t\t\t\t} else {\n\t\t\t\t\t// Extract the line of text _including_ the newline and remove it from src\n\t\t\t\t\ttext = src[:i+1]\n\t\t\t\t\tsrc = src[i+1:]\n\t\t\t\t}\n\n\t\t\t\tfor _, edit := range editsToApply {\n\t\t\t\t\ttext = edit.Pattern.ReplaceAllString(text, edit.Replacement)\n\t\t\t\t}\n\t\t\t\t_, err = dstFile.WriteString(text)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"write file %s: %w\", dstPath, err)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t// Can just do a straight copy\n\t\t\t_, err = io.Copy(dstFile, srcFile)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"copy file %s->%s: %w\", srcPath, dstPath, err)\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n}"
            },
            {
                "name": "compareDirectories",
                "code": "func compareDirectories(actualDir, expectedDir string, allowNewFiles bool) ([]string, error) {\n\t// Validate files, we need to walk twice to get this correct because we need to check all expected\n\t// files are present, but also that no unexpected files are present.\n\n\tvar validations []string\n\t// Check that every file in expected is also in actual with the same content\n\terr := filepath.WalkDir(expectedDir, func(path string, d iofs.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// No need to check directories, just recurse into them\n\t\tif d.IsDir() {\n\t\t\treturn nil\n\t\t}\n\n\t\trelativePath, err := filepath.Rel(expectedDir, path)\n\t\tcontract.AssertNoErrorf(err, \"path %s should be relative to %s\", path, expectedDir)\n\n\t\t// Check that the file is present in the expected directory and has the same contents\n\t\texpectedContents, err := os.ReadFile(filepath.Join(expectedDir, relativePath))\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"read expected file: %w\", err)\n\t\t}\n\n\t\tactualPath := filepath.Join(actualDir, relativePath)\n\t\tactualContents, err := os.ReadFile(actualPath)\n\t\t// An error here is a test failure rather than an error, add this to the validation list\n\t\tif err != nil {\n\t\t\tvalidations = append(validations, fmt.Sprintf(\"expected file %s could not be read\", relativePath))\n\t\t\t// Move on to the next file\n\t\t\treturn nil\n\t\t}\n\n\t\tif !bytes.Equal(actualContents, expectedContents) {\n\t\t\tedits := myers.ComputeEdits(\n\t\t\t\tspan.URIFromPath(\"expected\"), string(expectedContents), string(actualContents),\n\t\t\t)\n\t\t\tdiff := gotextdiff.ToUnified(\"expected\", \"actual\", string(expectedContents), edits)\n\n\t\t\tvalidations = append(validations, fmt.Sprintf(\n\t\t\t\t\"expected file %s does not match actual file:\\n\\n%s\", relativePath, diff),\n\t\t\t)\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"walk expected dir: %w\", err)\n\t}\n\n\t// Now walk the actual directory and check every file found is present in the expected directory, i.e.\n\t// there aren't any new files that aren't expected. We've already done contents checking so we just need\n\t// existence checks.\n\tif !allowNewFiles {\n\t\terr = filepath.WalkDir(actualDir, func(path string, d iofs.DirEntry, err error) error {\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// No need to check directories\n\t\t\tif d.IsDir() {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\trelativePath, err := filepath.Rel(actualDir, path)\n\t\t\tcontract.AssertNoErrorf(err, \"path %s should be relative to %s\", path, actualDir)\n\n\t\t\t// Just need to see if this file exists in expected, if it doesn't return add a validation failure.\n\t\t\t_, err = os.Stat(filepath.Join(expectedDir, relativePath))\n\t\t\tif err == nil {\n\t\t\t\t// File exists in expected, we've already done a contents check so just move on to\n\t\t\t\t// the next file.\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\t// Check if this was a NotFound error in which case add a validation failure, else return the error\n\t\t\tif os.IsNotExist(err) {\n\t\t\t\tvalidations = append(validations, fmt.Sprintf(\"file %s is not expected\", relativePath))\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\treturn err\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"walk actual dir: %w\", err)\n\t\t}\n\t}\n\n\treturn validations, nil\n}"
            }
        ],
        "third_party": [
            "os.DirFS",
            "cmdutil.IsTruthy"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def joblib_loader(load_from_dir, model_spec)",
        "start_line": "32",
        "end_line": "35",
        "file_path": "contrib/azureml_designer_modules/entries/score_sar_entry.py",
        "docstring": "The joblib_loader function loads a model from a specified directory. It takes in two parameters: load_from_dir, which is the directory path, and model_spec, which contains the file name of the model to be loaded. The function constructs the full path to the model file, opens it in binary read mode, and then uses joblib to load and return the model.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "277220a056f1",
        "ground_truth": "def joblib_loader(load_from_dir, model_spec):\n    file_name = model_spec[\"file_name\"]\n    with open(Path(load_from_dir) / file_name, \"rb\") as fin:\n        return joblib.load(fin)",
        "import_statements": [
            "import argparse",
            "from distutils.util import strtobool",
            "from enum import Enum",
            "from pathlib import Path",
            "import joblib",
            "from azureml.studio.core.data_frame_schema import DataFrameSchema",
            "from azureml.studio.core.logger import module_logger as logger",
            "from azureml.studio.core.io.data_frame_directory import (\n    load_data_frame_from_directory,\n    save_data_frame_to_directory,\n)",
            "from azureml.studio.core.io.model_directory import load_model_from_directory"
        ],
        "reference_api": [
            "joblib.load",
            "open",
            "Path"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "Path",
            "joblib.load"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def recommend_items(\n        self, ranking_metric, top_k, sort_top_k, remove_seen, normalize\n    )",
        "start_line": "51",
        "end_line": "70",
        "file_path": "contrib/azureml_designer_modules/entries/score_sar_entry.py",
        "docstring": "The recommend_items function generates item recommendations based on a specified ranking metric. It supports three metrics: RATING, SIMILARITY, and POPULARITY. Depending on the chosen metric, it calls the corresponding method from the model to recommend items. For RATING, it uses recommend_k_items with additional parameters like top_k, sort_top_k, remove_seen, and normalize. For SIMILARITY, it uses get_item_based_topk with top_k and sort_top_k. For POPULARITY, it uses get_popularity_based_topk with top_k and sort_top_k. If an unexpected metric is provided, it raises a ValueError.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7161c2bc4787",
        "ground_truth": "def recommend_items(\n    self, ranking_metric, top_k, sort_top_k, remove_seen, normalize\n):\n    if ranking_metric == RankingMetric.RATING:\n        return self.model.recommend_k_items(\n            test=self.input_data,\n            top_k=top_k,\n            sort_top_k=sort_top_k,\n            remove_seen=remove_seen,\n            normalize=normalize,\n        )\n    if ranking_metric == RankingMetric.SIMILARITY:\n        return self.model.get_item_based_topk(\n            items=self.input_data, top_k=top_k, sort_top_k=sort_top_k\n        )\n    if ranking_metric == RankingMetric.POPULARITY:\n        return self.model.get_popularity_based_topk(\n            top_k=top_k, sort_top_k=sort_top_k\n        )\n    raise ValueError(f\"Got unexpected ranking metric: {ranking_metric}.\")",
        "import_statements": [
            "import argparse",
            "from distutils.util import strtobool",
            "from enum import Enum",
            "from pathlib import Path",
            "import joblib",
            "from azureml.studio.core.data_frame_schema import DataFrameSchema",
            "from azureml.studio.core.logger import module_logger as logger",
            "from azureml.studio.core.io.data_frame_directory import (\n    load_data_frame_from_directory,\n    save_data_frame_to_directory,\n)",
            "from azureml.studio.core.io.model_directory import load_model_from_directory"
        ],
        "reference_api": [
            "recommend_k_items",
            "ValueError",
            "get_popularity_based_topk",
            "get_item_based_topk"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "recommend_k_items",
            "get_item_based_topk",
            "get_popularity_based_topk"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def model_dumper(save_to)",
        "start_line": "20",
        "end_line": "27",
        "file_path": "contrib/azureml_designer_modules/entries/train_sar_entry.py",
        "docstring": "The model_dumper function saves a model to a specified path using joblib. It constructs the full file path from the save_to directory and file_name, ensures the save_to directory exists, and writes the model data to a file with protocol 4. After saving the model, it returns a dictionary containing the model type and file name.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "090385c15635",
        "ground_truth": "def model_dumper(save_to):\n    full_path = Path(save_to) / file_name\n    ensure_folder(Path(save_to))\n    with open(full_path, \"wb\") as fout:\n        joblib.dump(data, fout, protocol=4)\n    model_spec = {\"model_type\": \"joblib\", \"file_name\": file_name}\n    return model_spec",
        "import_statements": [
            "import argparse",
            "from distutils.util import strtobool",
            "import time",
            "import joblib",
            "from pathlib import Path",
            "from recommenders.models.sar import SAR",
            "from azureml.studio.core.logger import module_logger as logger",
            "from azureml.studio.core.utils.fileutils import ensure_folder",
            "from azureml.studio.core.io.data_frame_directory import load_data_frame_from_directory",
            "from azureml.studio.core.io.model_directory import save_model_to_directory"
        ],
        "reference_api": [
            "ensure_folder",
            "joblib.dump",
            "open",
            "Path"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "Path",
            "ensure_folder",
            "Path",
            "joblib.dump"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def get_review_data(reviews_file)",
        "start_line": "20",
        "end_line": "30",
        "file_path": "recommenders/datasets/amazon_reviews.py",
        "docstring": "The get_review_data function processes review data from a specified file. It extracts the file name from the provided path, downloads and extracts the review data, then preprocesses it using the _reviews_preprocessing function, and returns the processed review data.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2500585c3b99",
        "ground_truth": "def get_review_data(reviews_file):\n    \"\"\"Downloads amazon review data (only), prepares in the required format\n    and stores in the same location\n     Args:\n        reviews_file (str): Filename for downloaded reviews dataset.\n    \"\"\"\n    reviews_name = reviews_file.split(\"/\")[-1]  # *.json (for url)\n    download_and_extract(reviews_name, reviews_file)\n    reviews_output = _reviews_preprocessing(reviews_file)\n    return reviews_output",
        "import_statements": [
            "import os",
            "import shutil",
            "import gzip",
            "import random",
            "import logging",
            "from recommenders.utils.constants import SEED",
            "from recommenders.datasets.download_utils import maybe_download"
        ],
        "reference_api": [
            "reviews_file.split",
            "download_and_extract",
            "_reviews_preprocessing"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "download_and_extract",
                "code": "def download_and_extract(name, dest_path):\n    \"\"\"Downloads and extracts Amazon reviews and meta datafiles if they don\u2019t already exist\n\n    Args:\n        name (str): Category of reviews.\n        dest_path (str): File path for the downloaded file.\n\n    Returns:\n        str: File path for the extracted file.\n    \"\"\"\n    dirs, _ = os.path.split(dest_path)\n    if not os.path.exists(dirs):\n        os.makedirs(dirs)\n\n    file_path = os.path.join(dirs, name)\n    if not os.path.exists(file_path):\n        _download_reviews(name, dest_path)\n        _extract_reviews(file_path, dest_path)\n\n    return file_path\n\n"
            },
            {
                "name": "_reviews_preprocessing",
                "code": "def _reviews_preprocessing(reviews_readfile):\n    logger.info(\"start reviews preprocessing...\")\n    reviews_writefile = reviews_readfile + \"_output\"\n    reviews_r = open(reviews_readfile, \"r\")\n    reviews_w = open(reviews_writefile, \"w\")\n    for line in reviews_r:\n        line_new = eval(line.strip())\n        reviews_w.write(\n            str(line_new[\"reviewerID\"])\n            + \"\\t\"\n            + str(line_new[\"asin\"])\n            + \"\\t\"\n            + str(line_new[\"unixReviewTime\"])\n            + \"\\n\"\n        )\n    reviews_r.close()\n    reviews_w.close()\n    return reviews_writefile"
            }
        ],
        "third_party": [
            "reviews_file.split"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def _create_item2cate(instance_file)",
        "start_line": "355",
        "end_line": "363",
        "file_path": "recommenders/datasets/amazon_reviews.py",
        "docstring": "The _create_item2cate function reads a tab-separated values (TSV) file into a pandas DataFrame, logs the creation of the item2cate dictionary, and then constructs a global dictionary mapping item IDs to category IDs. The input file is expected to have columns labeled \"label\", \"user_id\", \"item_id\", \"timestamp\", and \"cate_id\". The dictionary is created by setting the DataFrame's index to \"item_id\" and converting the \"cate_id\" column to a dictionary.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ebd279ea64eb",
        "ground_truth": "def _create_item2cate(instance_file):\n    logger.info(\"creating item2cate dict\")\n    global item2cate\n    instance_df = pd.read_csv(\n        instance_file,\n        sep=\"\\t\",\n        names=[\"label\", \"user_id\", \"item_id\", \"timestamp\", \"cate_id\"],\n    )\n    item2cate = instance_df.set_index(\"item_id\")[\"cate_id\"].to_dict()",
        "import_statements": [
            "import os",
            "import shutil",
            "import gzip",
            "import random",
            "import logging",
            "from recommenders.utils.constants import SEED",
            "from recommenders.datasets.download_utils import maybe_download"
        ],
        "reference_api": [
            "instance_df.set_index",
            "logger.info",
            "pd.read_csv",
            "to_dict"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "pd.read_csv",
            "to_dict",
            "instance_df.set_index"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def _meta_preprocessing(meta_readfile)",
        "start_line": "385",
        "end_line": "395",
        "file_path": "recommenders/datasets/amazon_reviews.py",
        "docstring": "The _meta_preprocessing function processes a given meta file and creates an output file with specific information. It opens the input meta file for reading and creates a new output file for writing. For each line in the input file, it evaluates the line as a Python expression, extracts the \"asin\" and the last category from the \"categories\" list, and writes these values to the output file separated by a tab. The function logs the start of the preprocessing and returns the name of the output file.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c0f4caacb2e8",
        "ground_truth": "def _meta_preprocessing(meta_readfile):\n    logger.info(\"start meta preprocessing...\")\n    meta_writefile = meta_readfile + \"_output\"\n    meta_r = open(meta_readfile, \"r\")\n    meta_w = open(meta_writefile, \"w\")\n    for line in meta_r:\n        line_new = eval(line)\n        meta_w.write(line_new[\"asin\"] + \"\\t\" + line_new[\"categories\"][0][-1] + \"\\n\")\n    meta_r.close()\n    meta_w.close()\n    return meta_writefile",
        "import_statements": [
            "import os",
            "import shutil",
            "import gzip",
            "import random",
            "import logging",
            "from recommenders.utils.constants import SEED",
            "from recommenders.datasets.download_utils import maybe_download"
        ],
        "reference_api": [
            "eval",
            "meta_w.close",
            "logger.info",
            "meta_w.write",
            "open",
            "meta_r.close"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "meta_w.write",
            "meta_r.close",
            "meta_w.close"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def download_and_extract(name, dest_path)",
        "start_line": "499",
        "end_line": "518",
        "file_path": "recommenders/datasets/amazon_reviews.py",
        "docstring": "The download_and_extract function ensures that the destination directory exists, then downloads and extracts a file if it is not already present. It first checks if the directory for the destination path exists, creating it if necessary. Next, it checks if the file already exists at the specified path. If the file is not present, it calls _download_reviews to download the file and _extract_reviews to extract its contents to the destination path. The function returns the file path of the downloaded and extracted file.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b275787662c2",
        "ground_truth": "def download_and_extract(name, dest_path):\n    \"\"\"Downloads and extracts Amazon reviews and meta datafiles if they don\u00e2\u20ac\u2122t already exist\n     Args:\n        name (str): Category of reviews.\n        dest_path (str): File path for the downloaded file.\n     Returns:\n        str: File path for the extracted file.\n    \"\"\"\n    dirs, _ = os.path.split(dest_path)\n    if not os.path.exists(dirs):\n        os.makedirs(dirs)\n     file_path = os.path.join(dirs, name)\n    if not os.path.exists(file_path):\n        _download_reviews(name, dest_path)\n        _extract_reviews(file_path, dest_path)\n     return file_path",
        "import_statements": [
            "import os",
            "import shutil",
            "import gzip",
            "import random",
            "import logging",
            "from recommenders.utils.constants import SEED",
            "from recommenders.datasets.download_utils import maybe_download"
        ],
        "reference_api": [
            "join",
            "exists",
            "_download_reviews",
            "split",
            "_extract_reviews",
            "os.makedirs"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_download_reviews",
                "code": "f _download_reviews(name, dest_path):\n    \"\"\"Downloads Amazon reviews datafile.\n\n    Args:\n        name (str): Category of reviews\n        dest_path (str): File path for the downloaded file\n    \"\"\"\n\n    url = (\n        \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/\"\n        + name\n        + \".gz\"\n    )\n\n    dirs, file = os.path.split(dest_path)\n    maybe_download(url, file + \".gz\", work_directory=dirs)\n\n"
            },
            {
                "name": "_extract_reviews",
                "code": "f _extract_reviews(file_path, zip_path):\n    \"\"\"Extract Amazon reviews and meta datafiles from the raw zip files.\n\n    To extract all files,\n    use ZipFile's extractall(path) instead.\n\n    Args:\n        file_path (str): Destination path for datafile\n        zip_path (str): zipfile path\n    \"\"\"\n    with gzip.open(zip_path + \".gz\", \"rb\") as zf, open(file_path, \"wb\") as f:\n        shutil.copyfileobj(zf, f)\n"
            }
        ],
        "third_party": [
            "split",
            "exists",
            "join",
            "exists"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def find_collection(client, dbid, id)",
        "start_line": "6",
        "end_line": "30",
        "file_path": "recommenders/datasets/cosmos_cli.py",
        "docstring": "The find_collection function checks if a collection with a specified ID exists within a database in a Cosmos DB instance. It constructs a database link using the provided database ID and queries the collections in that database for the collection with the given ID using a SQL-like query. The function returns True if the collection is found and False otherwise.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4e75931ea2c3",
        "ground_truth": "def find_collection(client, dbid, id):\n    \"\"\"Find whether or not a CosmosDB collection exists.\n     Args:\n        client (object): A pydocumentdb client object.\n        dbid (str): Database ID.\n        id (str): Collection ID.\n     Returns:\n        bool: True if the collection exists, False otherwise.\n    \"\"\"\n    database_link = \"dbs/\" + dbid\n    collections = list(\n        client.QueryCollections(\n            database_link,\n            {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [{\"name\": \"@id\", \"value\": id}],\n            },\n        )\n    )\n    if len(collections) > 0:\n        return True\n    else:\n        return False",
        "import_statements": [],
        "reference_api": [
            "list",
            "client.QueryCollections",
            "len"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "client.QueryCollections"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def read_database(client, id)",
        "start_line": "56",
        "end_line": "74",
        "file_path": "recommenders/datasets/cosmos_cli.py",
        "docstring": "The read_database function retrieves a database using a given client and database ID. It constructs the database link from the provided ID and attempts to read the database. If the database does not exist (404 error), it prints an error message. For other errors, it raises an HTTPFailure exception with the status code.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b766f116c702",
        "ground_truth": "def read_database(client, id):\n    \"\"\"Read a CosmosDB database.\n     Args:\n        client (object): A pydocumentdb client object.\n        id (str): Database ID.\n     Returns:\n        object: A database.\n    \"\"\"\n    try:\n        database_link = \"dbs/\" + id\n        database = client.ReadDatabase(database_link)\n        return database\n    except errors.DocumentDBError as e:\n        if e.status_code == 404:\n            print(\"A database with id '{0}' does not exist\".format(id))\n        else:\n            raise errors.HTTPFailure(e.status_code)",
        "import_statements": [],
        "reference_api": [
            "errors.HTTPFailure",
            "print",
            "format",
            "client.ReadDatabase"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "client.ReadDatabase",
            "errors.HTTPFailure"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def remove_nan(df, cols)",
        "start_line": "63",
        "end_line": "81",
        "file_path": "recommenders/datasets/covid_utils.py",
        "docstring": "The remove_nan function processes a DataFrame by iterating over specified columns to clean and filter the data. For each column, it converts empty string cells to NaN values and then removes any rows that contain NaN values in those columns. The cleaned DataFrame is then returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "be7b0e0b581c",
        "ground_truth": "def remove_nan(df, cols):\n    \"\"\"Remove rows with NaN values in specified column.\n     Args:\n        df (pandas.DataFrame): Pandas dataframe.\n        cols (list of str): Name of columns in which to look for NaN.\n     Returns:\n        df (pandas.DataFrame): Pandas dataframe with invalid rows dropped.\n     \"\"\"\n    for col in cols:\n        # Convert any empty string cells to nan\n        df[col].replace(\"\", np.nan, inplace=True)\n         # Remove NaN rows\n        df = df[df[col].notna()]\n     return df",
        "import_statements": [
            "import requests"
        ],
        "reference_api": [
            "notna",
            "replace"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "replace",
            "notna"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def load_spark_df(\n    spark,\n    size=\"sample\",\n    header=DEFAULT_HEADER,\n    local_cache_path=None,\n    dbfs_datapath=\"dbfs:/FileStore/dac\",\n    dbutils=None,\n)",
        "start_line": "64",
        "end_line": "123",
        "file_path": "recommenders/datasets/criteo.py",
        "docstring": "The load_spark_df function loads a Spark DataFrame from a specified dataset. It accepts parameters for Spark session, data size, header configuration, local cache path, DBFS data path, and dbutils. The function downloads and extracts the dataset to a local or DBFS path based on the environment (Databricks or not). If running on Databricks, it copies the file to DBFS using dbutils. It then reads the CSV file into a Spark DataFrame with a specified schema and triggers execution to cache the DataFrame. The function returns the loaded DataFrame.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0a019d4e59cd",
        "ground_truth": "def load_spark_df(\n    spark,\n    size=\"sample\",\n    header=DEFAULT_HEADER,\n    local_cache_path=None,\n    dbfs_datapath=\"dbfs:/FileStore/dac\",\n    dbutils=None,\n):\n    \"\"\"Loads the Criteo DAC dataset as `pySpark.DataFrame`.\n     The dataset consists of a portion of Criteo\u2019s traffic over a period\n    of 24 days. Each row corresponds to a display ad served by Criteo and the first\n    column is indicates whether this ad has been clicked or not.\n     There are 13 features taking integer values (mostly count features) and 26\n    categorical features. The values of the categorical features have been hashed\n    onto 32 bits for anonymization purposes.\n     The schema is:\n     .. code-block:: python\n         <label> <integer feature 1> ... <integer feature 13> <categorical feature 1> ... <categorical feature 26>\n     More details (need to accept user terms to see the information):\n    http://labs.criteo.com/2013/12/download-terabyte-click-logs/\n     Args:\n        spark (pySpark.SparkSession): Spark session.\n        size (str): Dataset size. It can be \"sample\" or \"full\".\n        local_cache_path (str): Path where to cache the tar.gz file locally.\n        header (list): Dataset header names.\n        dbfs_datapath (str): Where to store the extracted files on Databricks.\n        dbutils (Databricks.dbutils): Databricks utility object.\n     Returns:\n        pyspark.sql.DataFrame: Criteo DAC training dataset.\n    \"\"\"\n    with download_path(local_cache_path) as path:\n        filepath = download_criteo(size, path)\n        filepath = extract_criteo(size, filepath)\n         if is_databricks():\n            try:\n                # Driver node's file path\n                node_path = \"file:\" + filepath\n                # needs to be on dbfs to load\n                dbutils.fs.cp(node_path, dbfs_datapath, recurse=True)\n                path = dbfs_datapath\n            except Exception:\n                raise ValueError(\n                    \"To use on a Databricks notebook, dbutils object should be passed as an argument\"\n                )\n        else:\n            path = filepath\n         schema = get_spark_schema(header)\n        df = spark.read.csv(path, schema=schema, sep=\"\\t\", header=False)\n        df.cache().count()  # trigger execution to overcome spark's lazy evaluation\n    return df",
        "import_statements": [
            "import os",
            "import tarfile",
            "from recommenders.datasets.download_utils import maybe_download, download_path",
            "from recommenders.utils.notebook_utils import is_databricks"
        ],
        "reference_api": [
            "download_criteo",
            "is_databricks",
            "count",
            "get_spark_schema",
            "ValueError",
            "cp",
            "csv",
            "extract_criteo",
            "download_path",
            "df.cache"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "download_criteo",
                "code": "download_criteo(size=\"sample\", work_directory=\".\"):\n    \"\"\"Download criteo dataset as a compressed file.\n\n    Args:\n        size (str): Size of criteo dataset. It can be \"full\" or \"sample\".\n        work_directory (str): Working directory.\n\n    Returns:\n        str: Path of the downloaded file.\n\n    \"\"\"\n    url = CRITEO_URL[size]\n    return maybe_download(url, work_directory=work_directory)\n\n\nd"
            },
            {
                "name": "extract_criteo",
                "code": "extract_criteo(size, compressed_file, path=None):\n    \"\"\"Extract Criteo dataset tar.\n\n    Args:\n        size (str): Size of Criteo dataset. It can be \"full\" or \"sample\".\n        compressed_file (str): Path to compressed file.\n        path (str): Path to extract the file.\n\n    Returns:\n        str: Path to the extracted file.\n\n    \"\"\"\n    if path is None:\n        folder = os.path.dirname(compressed_file)\n        extracted_dir = os.path.join(folder, \"dac\")\n    else:\n        extracted_dir = path\n\n    with tarfile.open(compressed_file) as tar:\n\n        def is_within_directory(directory, target):\n\n            abs_directory = os.path.abspath(directory)\n            abs_target = os.path.abspath(target)\n\n            prefix = os.path.commonprefix([abs_directory, abs_target])\n\n            return prefix == abs_directory\n\n        def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n\n            for member in tar.getmembers():\n                member_path = os.path.join(path, member.name)\n                if not is_within_directory(path, member_path):\n                    raise Exception(\"Attempted Path Traversal in Tar File\")\n\n            tar.extractall(path, members, numeric_owner=numeric_owner)\n\n        safe_extract(tar, extracted_dir)\n\n    filename_selector = {\"sample\": \"dac_sample.txt\", \"full\": \"train.txt\"}\n    return os.path.join(extracted_dir, filename_selector[size])\n\n\nd"
            },
            {
                "name": "is_databricks",
                "code": "def is_databricks():\n    \"\"\"Check if the module is running on Databricks.\n\n    Returns:\n        bool: True if the module is running on Databricks notebook,\n        False otherwise.\n    \"\"\"\n    try:\n        if os.path.realpath(\".\") == \"/databricks/driver\":\n            return True\n        else:\n            return False\n    except NameError:\n        return False"
            },
            {
                "name": "get_spark_schema",
                "code": "get_spark_schema(header=DEFAULT_HEADER):\n    \"\"\"Get Spark schema from header.\n\n    Args:\n        header (list): Dataset header names.\n\n    Returns:\n        pyspark.sql.types.StructType: Spark schema.\n    \"\"\"\n    # create schema\n    schema = StructType()\n    # do label + ints\n    n_ints = 14\n    for i in range(n_ints):\n        schema.add(StructField(header[i], IntegerType()))\n    # do categoricals\n    for i in range(26):\n        schema.add(StructField(header[i + n_ints], StringType()))\n    return schema\n"
            }
        ],
        "third_party": [
            "download_path",
            "cp",
            "count",
            "df.cache"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "\ndef download_criteo(size=\"sample\", work_directory=\".\")",
        "start_line": "126",
        "end_line": "138",
        "file_path": "recommenders/datasets/criteo.py",
        "docstring": "The download_criteo function downloads the Criteo dataset of the specified size (\"sample\" by default) to the given work directory. It retrieves the appropriate URL from the CRITEO_URL dictionary using the provided size and then calls the maybe_download function with the URL and work_directory as arguments to perform the download.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5218656302bd",
        "ground_truth": "def download_criteo(size=\"sample\", work_directory=\".\"):\n    \"\"\"Download criteo dataset as a compressed file.\n     Args:\n        size (str): Size of criteo dataset. It can be \"full\" or \"sample\".\n        work_directory (str): Working directory.\n     Returns:\n        str: Path of the downloaded file.\n     \"\"\"\n    url = CRITEO_URL[size]\n    return maybe_download(url, work_directory=work_directory)",
        "import_statements": [
            "import os",
            "import tarfile",
            "from recommenders.datasets.download_utils import maybe_download, download_path",
            "from recommenders.utils.notebook_utils import is_databricks"
        ],
        "reference_api": [
            "maybe_download"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "maybe_download"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def python_random_split(data, ratio=0.75, seed=42)",
        "start_line": "19",
        "end_line": "44",
        "file_path": "recommenders/datasets/python_splitters.py",
        "docstring": "The python_random_split function splits a dataset into training and testing subsets based on a specified ratio and seed for randomness. It first processes the ratio to determine if it is a multi-split operation. If it is, the function uses split_pandas_data_with_ratios to shuffle and split the data accordingly, removing the \"split_index\" column from the resulting splits before returning them. If it is not a multi-split operation, the function uses scikit-learn's train_test_split to perform the split and returns the training and testing subsets based on the given ratio and seed.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ca7209ac4d73",
        "ground_truth": "def python_random_split(data, ratio=0.75, seed=42):\n    \"\"\"Pandas random splitter.\n     The splitter randomly splits the input data.\n     Args:\n        data (pandas.DataFrame): Pandas DataFrame to be split.\n        ratio (float or list): Ratio for splitting data. If it is a single float number\n            it splits data into two halves and the ratio argument indicates the ratio\n            of training data set; if it is a list of float numbers, the splitter splits\n            data into several portions corresponding to the split ratios. If a list is\n            provided and the ratios are not summed to 1, they will be normalized.\n        seed (int): Seed.\n     Returns:\n        list: Splits of the input data as pandas.DataFrame.\n    \"\"\"\n    multi_split, ratio = process_split_ratio(ratio)\n     if multi_split:\n        splits = split_pandas_data_with_ratios(data, ratio, shuffle=True, seed=seed)\n        splits_new = [x.drop(\"split_index\", axis=1) for x in splits]\n         return splits_new\n    else:\n        return sk_split(data, test_size=None, train_size=ratio, random_state=seed)",
        "import_statements": [
            "from sklearn.model_selection import train_test_split as sk_split",
            "from recommenders.utils.constants import (\n    DEFAULT_ITEM_COL,\n    DEFAULT_USER_COL,\n    DEFAULT_TIMESTAMP_COL,\n)",
            "from recommenders.datasets.split_utils import (\n    process_split_ratio,\n    min_rating_filter_pandas,\n    split_pandas_data_with_ratios,\n)"
        ],
        "reference_api": [
            "process_split_ratio",
            "x.drop",
            "sk_split",
            "split_pandas_data_with_ratios"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "process_split_ratio",
                "code": "def process_split_ratio(ratio):\n    \"\"\"Generate split ratio lists.\n\n    Args:\n        ratio (float or list): a float number that indicates split ratio or a list of float\n        numbers that indicate split ratios (if it is a multi-split).\n\n    Returns:\n        tuple:\n        - bool: A boolean variable multi that indicates if the splitting is multi or single.\n        - list: A list of normalized split ratios.\n    \"\"\"\n    if isinstance(ratio, float):\n        if ratio <= 0 or ratio >= 1:\n            raise ValueError(\"Split ratio has to be between 0 and 1\")\n\n        multi = False\n    elif isinstance(ratio, list):\n        if any([x <= 0 for x in ratio]):\n            raise ValueError(\n                \"All split ratios in the ratio list should be larger than 0.\"\n            )\n\n        # normalize split ratios if they are not summed to 1\n        if math.fsum(ratio) != 1.0:\n            ratio = [x / math.fsum(ratio) for x in ratio]\n\n        multi = True\n    else:\n        raise TypeError(\"Split ratio should be either float or a list of floats.\")\n\n    return multi, ratio"
            },
            {
                "name": "split_pandas_data_with_ratios",
                "code": "def split_pandas_data_with_ratios(data, ratios, seed=42, shuffle=False):\n    \"\"\"Helper function to split pandas DataFrame with given ratios\n\n    Note:\n        Implementation referenced from `this source <https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test>`_.\n\n    Args:\n        data (pandas.DataFrame): Pandas data frame to be split.\n        ratios (list of floats): list of ratios for split. The ratios have to sum to 1.\n        seed (int): random seed.\n        shuffle (bool): whether data will be shuffled when being split.\n\n    Returns:\n        list: List of pd.DataFrame split by the given specifications.\n    \"\"\"\n    if math.fsum(ratios) != 1.0:\n        raise ValueError(\"The ratios have to sum to 1\")\n\n    split_index = np.cumsum(ratios).tolist()[:-1]\n\n    if shuffle:\n        data = data.sample(frac=1, random_state=seed)\n\n    splits = np.split(data, [round(x * len(data)) for x in split_index])\n\n    # Add split index (this makes splitting by group more efficient).\n    for i in range(len(ratios)):\n        splits[i][\"split_index\"] = i\n\n    return splits"
            }
        ],
        "third_party": [
            "x.drop",
            "sk_split"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "recommenders-team/recommenders",
        "function_declaration": "def gen_affinity_matrix(self)",
        "start_line": "109",
        "end_line": "143",
        "file_path": "recommenders/datasets/sparse.py",
        "docstring": "The gen_affinity_matrix function generates a user/item affinity matrix from a dataframe containing user ratings. It logs the start of the process, generates an index, and extracts the ratings, hashed item IDs, and hashed user IDs from the dataframe. It uses these to create a sparse matrix representation with scipy's coo_matrix, which is then converted to an array format. The function calculates the sparsity of the matrix by finding the percentage of zero values and logs this information. Finally, it returns the affinity matrix along with user and item mappings.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4e6b312b2e42",
        "ground_truth": "def gen_affinity_matrix(self):\n    \"\"\"Generate the user/item affinity matrix.\n    As a first step, two new columns are added to the input DF, containing the index maps\n    generated by the gen_index() method. The new indices, together with the ratings, are\n    then used to generate the user/item affinity matrix using scipy's sparse matrix method\n    coo_matrix; for reference see:\n    https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html.\n    The input format is: `coo_matrix((data, (rows, columns)), shape=(rows, columns))`\n    Returns:\n        scipy.sparse.coo_matrix: User-affinity matrix of dimensions (Nusers, Nitems) in numpy format.\n        Unrated movies are assigned a value of 0.\n    \"\"\"\n    log.info(\"Generating the user/item affinity matrix...\")\n    self._gen_index()\n    ratings = self.df_[self.col_rating]  # ratings\n    itm_id = self.df_[\"hashedItems\"]  # itm_id serving as columns\n    usr_id = self.df_[\"hashedUsers\"]  # usr_id serving as rows\n    # generate a sparse matrix representation using scipy's coo_matrix and convert to array format\n    self.AM = coo_matrix(\n        (ratings, (usr_id, itm_id)), shape=(self.Nusers, self.Nitems)\n    ).toarray()\n    zero = (self.AM == 0).sum()  # number of unrated items\n    total = self.AM.shape[0] * self.AM.shape[1]  # number of elements in the matrix\n    sparsness = zero / total * 100  # Percentage of zeros in the matrix\n    log.info(\"Matrix generated, sparseness percentage: %d\" % sparsness)\n    return self.AM, self.map_users, self.map_items",
        "import_statements": [
            "import itertools",
            "from scipy.sparse import coo_matrix",
            "import logging",
            "from recommenders.utils.constants import (\n    DEFAULT_USER_COL,\n    DEFAULT_ITEM_COL,\n    DEFAULT_RATING_COL,\n    DEFAULT_PREDICTION_COL,\n)"
        ],
        "reference_api": [
            "self._gen_index",
            "coo_matrix",
            "toarray",
            "sum",
            "log.info"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._gen_index",
                "code": "def _gen_index(self):\n        \"\"\"\n        Generate the user/item index:\n        map_users, map_items: dictionaries mapping the original user/item index to matrix indices\n        map_back_users, map_back_items: dictionaries to map back the matrix elements to the original\n        dataframe indices\n\n        Basic mechanics:\n        As a first step we retieve the unique elements in the dataset. In this way we can take care\n        of either completely missing rows (a user with no ratings) or completely missing columns\n        (an item that has not being reviewed by anyone). The original indices in the dataframe are\n        then mapped to an ordered, contiguous integer series to generate a compact matrix representation.\n        Functions to map back to the original indices are also provided and can be saved in order to use\n        a pretrained model.\n        \"\"\"\n        # sort entries by user index\n        self.df_ = self.df.sort_values(by=[self.col_user])\n\n        # find unique user and item index\n        unique_users = self.df_[self.col_user].unique()\n\n        if self.items_list is not None:\n            unique_items = self.items_list  # use this list if provided\n        else:\n            unique_items = self.df_[\n                self.col_item\n            ].unique()  # otherwise use unique items from DF\n\n        self.Nusers = len(unique_users)\n        self.Nitems = len(unique_items)\n\n        # create a dictionary to map unique users/items to hashed values to generate the matrix\n        self.map_users = {x: i for i, x in enumerate(unique_users)}\n        self.map_items = {x: i for i, x in enumerate(unique_items)}\n\n        # map back functions used to get back the original dataframe\n        self.map_back_users = {i: x for i, x in enumerate(unique_users)}\n        self.map_back_items = {i: x for i, x in enumerate(unique_items)}\n\n        self.df_.loc[:, \"hashedItems\"] = self.df_[self.col_item].map(self.map_items)\n        self.df_.loc[:, \"hashedUsers\"] = self.df_[self.col_user].map(self.map_users)\n\n        # optionally save the inverse dictionary to work with trained models\n        if self.save_path is not None:\n\n            np.save(self.save_path + \"/user_dict\", self.map_users)\n            np.save(self.save_path + \"/item_dict\", self.map_items)\n\n            np.save(self.save_path + \"/user_back_dict\", self.map_back_users)\n            np.save(self.save_path + \"/item_back_dict\", self.map_back_items)"
            }
        ],
        "third_party": [
            "log.info",
            "toarray",
            "coo_matrix",
            "log.info"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "getsops/sops",
        "function_declaration": "func (ks *Server) encryptWithGcpKms(key *GcpKmsKey, plaintext []byte) ([]byte, error)",
        "start_line": "41",
        "end_line": "50",
        "file_path": "keyservice/server.go",
        "docstring": "The encryptWithGcpKms function in the Server struct encrypts plaintext using Google Cloud KMS. It initializes a gcpkms.MasterKey with the given key's resource ID, then encrypts the plaintext. If the encryption is successful, it returns the encrypted key as a byte slice; otherwise, it returns an error.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "91767e6e851f",
        "ground_truth": "func (ks *Server) encryptWithGcpKms(key *GcpKmsKey, plaintext []byte) ([]byte, error) {\n gcpKmsKey := gcpkms.MasterKey{\n  ResourceID: key.ResourceId,\n }\n err := gcpKmsKey.Encrypt(plaintext)\n if err != nil {\n  return nil, err\n }\n return []byte(gcpKmsKey.EncryptedKey), nil\n}",
        "import_statements": [
            "import (\n\t\"fmt\"\n\n\t\"github.com/getsops/sops/v3/age\"\n\t\"github.com/getsops/sops/v3/azkv\"\n\t\"github.com/getsops/sops/v3/gcpkms\"\n\t\"github.com/getsops/sops/v3/hcvault\"\n\t\"github.com/getsops/sops/v3/kms\"\n\t\"github.com/getsops/sops/v3/pgp\"\n\t\"golang.org/x/net/context\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/status\"\n)"
        ],
        "reference_api": [
            "gcpKmsKey.Encrypt"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "gcpKmsKey.Encrypt"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "getsops/sops",
        "function_declaration": "func (ks *Server) encryptWithAzureKeyVault(key *AzureKeyVaultKey, plaintext []byte) ([]byte, error)",
        "start_line": "52",
        "end_line": "63",
        "file_path": "keyservice/server.go",
        "docstring": "The function encryptWithAzureKeyVault encrypts plaintext using an Azure Key Vault key. It initializes an azkv.MasterKey with the provided Vault URL, key name, and version. The function then calls the Encrypt method on the azkvKey object to perform the encryption. If encryption fails, it returns an error; otherwise, it returns the encrypted key as a byte slice.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e7082c4edcb0",
        "ground_truth": "func (ks *Server) encryptWithAzureKeyVault(key *AzureKeyVaultKey, plaintext []byte) ([]byte, error) {\n azkvKey := azkv.MasterKey{\n  VaultURL: key.VaultUrl,\n  Name:     key.Name,\n  Version:  key.Version,\n }\n err := azkvKey.Encrypt(plaintext)\n if err != nil {\n  return nil, err\n }\n return []byte(azkvKey.EncryptedKey), nil\n}",
        "import_statements": [
            "import (\n\t\"fmt\"\n\n\t\"github.com/getsops/sops/v3/age\"\n\t\"github.com/getsops/sops/v3/azkv\"\n\t\"github.com/getsops/sops/v3/gcpkms\"\n\t\"github.com/getsops/sops/v3/hcvault\"\n\t\"github.com/getsops/sops/v3/kms\"\n\t\"github.com/getsops/sops/v3/pgp\"\n\t\"golang.org/x/net/context\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/status\"\n)"
        ],
        "reference_api": [
            "azkvKey.Encrypt"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "azkvKey.Encrypt"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "getsops/sops",
        "function_declaration": "func (ks *Server) decryptWithGcpKms(key *GcpKmsKey, ciphertext []byte) ([]byte, error)",
        "start_line": "104",
        "end_line": "111",
        "file_path": "keyservice/server.go",
        "docstring": "The decryptWithGcpKms function in the Server struct decrypts a given ciphertext using Google Cloud KMS. It creates a gcpkms.MasterKey object with the provided GcpKmsKey's ResourceID and sets its EncryptedKey attribute to the ciphertext. It then calls the Decrypt method on the gcpKmsKey object to obtain the plaintext. The function returns the plaintext as a byte slice and any error encountered during the decryption process.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4264f3b331cb",
        "ground_truth": "func (ks *Server) decryptWithGcpKms(key *GcpKmsKey, ciphertext []byte) ([]byte, error) {\n gcpKmsKey := gcpkms.MasterKey{\n  ResourceID: key.ResourceId,\n }\n gcpKmsKey.EncryptedKey = string(ciphertext)\n plaintext, err := gcpKmsKey.Decrypt()\n return []byte(plaintext), err\n}",
        "import_statements": [
            "import (\n\t\"fmt\"\n\n\t\"github.com/getsops/sops/v3/age\"\n\t\"github.com/getsops/sops/v3/azkv\"\n\t\"github.com/getsops/sops/v3/gcpkms\"\n\t\"github.com/getsops/sops/v3/hcvault\"\n\t\"github.com/getsops/sops/v3/kms\"\n\t\"github.com/getsops/sops/v3/pgp\"\n\t\"golang.org/x/net/context\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/status\"\n)"
        ],
        "reference_api": [
            "string",
            "gcpKmsKey.Decrypt"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "string",
            "gcpKmsKey.Decrypt"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "getsops/sops",
        "function_declaration": "func NewMasterKeyFromArn(arn string, context map[string]*string, awsProfile string) *MasterKey",
        "start_line": "104",
        "end_line": "118",
        "file_path": "kms/keysource.go",
        "docstring": "The NewMasterKeyFromArn function creates a new MasterKey object using an AWS ARN (Amazon Resource Name). It removes any spaces from the ARN and checks for a role ARN within the ARN string. If a role ARN is found, it splits the ARN, setting the MasterKey's Arn and Role fields accordingly. The function also assigns the provided encryption context, sets the current UTC time as the creation date, and stores the given AWS profile. Finally, it returns the configured MasterKey object.\n",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a04155014ce0",
        "ground_truth": "func NewMasterKeyFromArn(arn string, context map[string]*string, awsProfile string) *MasterKey {\n key := &MasterKey{}\n arn = strings.Replace(arn, \" \", \"\", -1)\n key.Arn = arn\n roleIndex := strings.Index(arn, \"+arn:aws:iam::\")\n if roleIndex > 0 {\n  // Overwrite ARN\n  key.Arn = arn[:roleIndex]\n  key.Role = arn[roleIndex+1:]\n }\n key.EncryptionContext = context\n key.CreationDate = time.Now().UTC()\n key.AwsProfile = awsProfile\n return key\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/aws/aws-sdk-go-v2/aws\"\n\t\"github.com/aws/aws-sdk-go-v2/config\"\n\t\"github.com/aws/aws-sdk-go-v2/credentials\"\n\t\"github.com/aws/aws-sdk-go-v2/service/kms\"\n\t\"github.com/aws/aws-sdk-go-v2/service/sts\"\n\t\"github.com/sirupsen/logrus\"\n\n\t\"github.com/getsops/sops/v3/logging\"\n)"
        ],
        "reference_api": [
            "time.Now().UTC",
            "strings.Replace",
            "strings.Index",
            "time.Now"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "time.Now().UTC"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "getsops/sops",
        "function_declaration": "func stsSessionName() (string, error)",
        "start_line": "377",
        "end_line": "391",
        "file_path": "kms/keysource.go",
        "docstring": "The stsSessionName function generates a sanitized STS session name. It first retrieves the hostname using osHostname. If an error occurs, it returns an error message. The function then uses a regular expression (stsSessionRegex) to remove unwanted characters from the hostname. It constructs the session name by prefixing \"sops@\" to the sanitized hostname. If the resulting name exceeds the roleSessionNameLengthLimit, it truncates the name to fit within the limit. Finally, it returns the sanitized session name.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "dc2ee5384e96",
        "ground_truth": "func stsSessionName() (string, error) {\n hostname, err := osHostname()\n if err != nil {\n  return \"\", fmt.Errorf(\"failed to construct STS session name: %w\", err)\n }\n  re := regexp.MustCompile(stsSessionRegex)\n sanitizedHostname := re.ReplaceAllString(hostname, \"\")\n  name := \"sops@\" + sanitizedHostname\n if len(name) >= roleSessionNameLengthLimit {\n  name = name[:roleSessionNameLengthLimit]\n }\n return name, nil\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/aws/aws-sdk-go-v2/aws\"\n\t\"github.com/aws/aws-sdk-go-v2/config\"\n\t\"github.com/aws/aws-sdk-go-v2/credentials\"\n\t\"github.com/aws/aws-sdk-go-v2/service/kms\"\n\t\"github.com/aws/aws-sdk-go-v2/service/sts\"\n\t\"github.com/sirupsen/logrus\"\n\n\t\"github.com/getsops/sops/v3/logging\"\n)"
        ],
        "reference_api": [
            "fmt.Errorf",
            "osHostname",
            "len",
            "re.ReplaceAllString",
            "regexp.MustCompile"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "osHostname",
            "len",
            "re.ReplaceAllString",
            "regexp.MustCompile"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "danny-avila/LibreChat",
        "function_declaration": "const genAzureChatCompletion = (\n  { azureOpenAIApiInstanceName, azureOpenAIApiDeploymentName, azureOpenAIApiVersion },\n  modelName,\n  client,\n) =>",
        "start_line": "36",
        "end_line": "58",
        "file_path": "api/utils/azureUtils.js",
        "docstring": "The genAzureChatCompletion function generates a URL for Azure OpenAI chat completion. It takes configuration parameters (azureOpenAIApiInstanceName, azureOpenAIApiDeploymentName, azureOpenAIApiVersion), a modelName, and a client object. The function determines the deployment segment of the URL based on the modelName or azureOpenAIApiDeploymentName. If the environment variable AZURE_USE_MODEL_AS_DEPLOYMENT_NAME is enabled and a modelName is provided, it uses a sanitized version of the modelName as the deployment segment and updates the client object. If not, it uses azureOpenAIApiDeploymentName. If neither is available and AZURE_OPENAI_BASEURL is omitted, it throws an error. Finally, the function returns the constructed URL.",
        "language": "JavaScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "968bf06c1515",
        "ground_truth": "const genAzureChatCompletion = (\n  { azureOpenAIApiInstanceName, azureOpenAIApiDeploymentName, azureOpenAIApiVersion },\n  modelName,\n  client,\n) => {\n  // Determine the deployment segment of the URL based on provided modelName or azureOpenAIApiDeploymentName\n  let deploymentSegment;\n  if (isEnabled(process.env.AZURE_USE_MODEL_AS_DEPLOYMENT_NAME) && modelName) {\n    const sanitizedModelName = sanitizeModelName(modelName);\n    deploymentSegment = `${sanitizedModelName}`;\n    client &&\n      typeof client === 'object' &&\n      (client.azure.azureOpenAIApiDeploymentName = sanitizedModelName);\n  } else if (azureOpenAIApiDeploymentName) {\n    deploymentSegment = azureOpenAIApiDeploymentName;\n  } else if (!process.env.AZURE_OPENAI_BASEURL) {\n    throw new Error(\n      'Either a model name with the `AZURE_USE_MODEL_AS_DEPLOYMENT_NAME` setting or a deployment name must be provided if `AZURE_OPENAI_BASEURL` is omitted.',\n    );\n  }\n   return `https://${azureOpenAIApiInstanceName}.openai.azure.com/openai/deployments/${deploymentSegment}/chat/completions?api-version=${azureOpenAIApiVersion}`;\n};",
        "import_statements": [],
        "reference_api": [
            "sanitizeModelName",
            "isEnabled"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "sanitizeModelName",
            "isEnabled"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "danny-avila/LibreChat",
        "function_declaration": "function constructAzureURL({ baseURL, azureOptions })",
        "start_line": "84",
        "end_line": "97",
        "file_path": "api/utils/azureUtils.js",
        "docstring": "The constructAzureURL function generates a final URL by replacing placeholders in a baseURL with actual values from azureOptions. It starts with the baseURL and, if azureOptions are provided, replaces the ${INSTANCE_NAME} and ${DEPLOYMENT_NAME} placeholders with the corresponding values from azureOptions (azureOpenAIApiInstanceName and azureOpenAIApiDeploymentName). If these values are not available, it replaces the placeholders with empty strings. Finally, it returns the constructed URL.",
        "language": "JavaScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1f23b1408ff0",
        "ground_truth": "function constructAzureURL({ baseURL, azureOptions }) {\n  let finalURL = baseURL;\n   // Replace INSTANCE_NAME and DEPLOYMENT_NAME placeholders with actual values if available\n  if (azureOptions) {\n    finalURL = finalURL.replace('${INSTANCE_NAME}', azureOptions.azureOpenAIApiInstanceName ?? '');\n    finalURL = finalURL.replace(\n      '${DEPLOYMENT_NAME}',\n      azureOptions.azureOpenAIApiDeploymentName ?? '',\n    );\n  }\n   return finalURL;\n}",
        "import_statements": [],
        "reference_api": [
            "finalURL.replace"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "finalURL.replace"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "danny-avila/LibreChat",
        "function_declaration": "async _call(data)",
        "start_line": "81",
        "end_line": "101",
        "file_path": "api/app/clients/tools/structured/AzureAISearch.js",
        "docstring": "The async function _call performs a search query using Azure AI Search. It accepts a data object containing the query. The function sets up search options including queryType, top, and select fields if specified. It then executes the search query using this.client.search with the configured options. The results are processed, and the documents are collected in the resultDocuments array. The function returns the results as a JSON string. If an error occurs, it logs the error and returns a generic error message.",
        "language": "JavaScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c013e8c610c0",
        "ground_truth": "async _call(data) {\n  const { query } = data;\n  try {\n    const searchOption = {\n      queryType: this.queryType,\n      top: this.top,\n    };\n    if (this.select) {\n      searchOption.select = this.select.split(',');\n    }\n    const searchResults = await this.client.search(query, searchOption);\n    const resultDocuments = [];\n    for await (const result of searchResults.results) {\n      resultDocuments.push(result.document);\n    }\n    return JSON.stringify(resultDocuments);\n  } catch (error) {\n    logger.error('Azure AI Search request failed', error);\n    return 'There was an error with Azure AI Search.';\n  }\n}",
        "import_statements": [],
        "reference_api": [
            "JSON.stringify",
            "this.client.search",
            "async",
            "resultDocuments.push",
            "logger.error",
            "this.select.split"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "this.client.search",
            "async",
            "resultDocuments.push",
            "logger.error",
            "this.select.split"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "danny-avila/LibreChat",
        "function_declaration": "const getCurrentVersion = async (req, endpoint) =>",
        "start_line": "19",
        "end_line": "36",
        "file_path": "api/server/controllers/assistants/helpers.js",
        "docstring": "The getCurrentVersion function retrieves the API version from the request object and endpoint. It first attempts to extract the version from the base URL of the request. If not found, it checks the request body for a version. If still not found and an endpoint is provided, it retrieves cached endpoint configuration to determine the version. If the version does not start with 'v' or is not two characters long, it throws an error. Finally, it returns the determined version.",
        "language": "JavaScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8b9e2f777564",
        "ground_truth": "const getCurrentVersion = async (req, endpoint) => {\n  const index = req.baseUrl.lastIndexOf('/v');\n  let version = index !== -1 ? req.baseUrl.substring(index + 1, index + 3) : null;\n  if (!version && req.body.version) {\n    version = `v${req.body.version}`;\n  }\n  if (!version && endpoint) {\n    const cache = getLogStores(CacheKeys.CONFIG_STORE);\n    const cachedEndpointsConfig = await cache.get(CacheKeys.ENDPOINT_CONFIG);\n    version = `v${\n      cachedEndpointsConfig?.[endpoint]?.version ?? defaultAssistantsVersion[endpoint]\n    }`;\n  }\n  if (!version?.startsWith('v') && version.length !== 2) {\n    throw new Error(`[${req.baseUrl}] Invalid version: ${version}`);\n  }\n  return version;\n};",
        "import_statements": [],
        "reference_api": [
            "req.baseUrl.substring",
            "getLogStores",
            "req.baseUrl.lastIndexOf",
            "cache.get",
            "version?.startsWith"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "req.baseUrl.substring",
            "getLogStores",
            "req.baseUrl.lastIndexOf",
            "cache.get",
            "version?.startsWith"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "danny-avila/LibreChat",
        "function_declaration": "function filterAssistants({ assistants, userId, assistantsConfig })",
        "start_line": "254",
        "end_line": "264",
        "file_path": "api/server/controllers/assistants/helpers.js",
        "docstring": "The filterAssistants function filters a list of assistants based on a given configuration and userId. It takes an object with assistants, userId, and assistantsConfig properties as its argument. The assistantsConfig contains supportedIds, excludedIds, and privateAssistants. If privateAssistants is true, it returns assistants authored by the userId. If supportedIds is provided, it returns assistants whose IDs are in supportedIds. If excludedIds is provided, it returns assistants whose IDs are not in excludedIds. If none of these conditions are met, it returns the original list of assistants.",
        "language": "JavaScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "531d744595be",
        "ground_truth": "function filterAssistants({ assistants, userId, assistantsConfig }) {\n  const { supportedIds, excludedIds, privateAssistants } = assistantsConfig;\n  if (privateAssistants) {\n    return assistants.filter((assistant) => userId === assistant.metadata?.author);\n  } else if (supportedIds?.length) {\n    return assistants.filter((assistant) => supportedIds.includes(assistant.id));\n  } else if (excludedIds?.length) {\n    return assistants.filter((assistant) => !excludedIds.includes(assistant.id));\n  }\n  return assistants;\n}",
        "import_statements": [],
        "reference_api": [
            "supportedIds.includes",
            "excludedIds.includes",
            "assistants.filter"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "supportedIds.includes",
            "excludedIds.includes",
            "assistants.filter"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "caprover/caprover",
        "function_declaration": "verifyCustomDomainBelongsToApp(appName: string, customDomain: string)",
        "start_line": "479",
        "end_line": "498",
        "file_path": "src/datastore/AppsDataStore.ts",
        "docstring": "The verifyCustomDomainBelongsToApp function checks if a given custom domain is associated with a specified application. It takes two parameters: appName and customDomain. It fetches the application definition using the appName and iterates through the custom domains associated with the app. If the customDomain is found in the app's custom domains, it returns true. If the customDomain is not found, it throws an error indicating that the custom domain is not attached to the specified app.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e5e78d8f0cb9",
        "ground_truth": "verifyCustomDomainBelongsToApp(appName: string, customDomain: string) {\n    const self = this\n    return self.getAppDefinition(appName).then(function (app) {\n        app.customDomain = app.customDomain || []\n        if (app.customDomain.length > 0) {\n            for (let idx = 0; idx < app.customDomain.length; idx++) {\n                if (app.customDomain[idx].publicDomain === customDomain) {\n                    return true\n                }\n            }\n        }\n        throw ApiStatusCodes.createError(\n            ApiStatusCodes.ILLEGAL_PARAMETER,\n            `customDomain ${customDomain} is not attached to app ${appName}`\n        )\n    })\n}",
        "import_statements": [
            "import { v4 as uuid } from 'uuid'",
            "{ v4 as uuid }",
            "{ v4 as uuid }",
            "v4 as uuid",
            "import ApiStatusCodes from '../api/ApiStatusCodes'",
            "ApiStatusCodes",
            "import { IBuiltImage } from '../models/IBuiltImage'",
            "{ IBuiltImage }",
            "{ IBuiltImage }",
            "IBuiltImage",
            "import Authenticator from '../user/Authenticator'",
            "Authenticator",
            "import ApacheMd5 from '../utils/ApacheMd5'",
            "ApacheMd5",
            "import CaptainConstants from '../utils/CaptainConstants'",
            "CaptainConstants",
            "import CaptainEncryptor from '../utils/Encryptor'",
            "CaptainEncryptor",
            "import Logger from '../utils/Logger'",
            "Logger",
            "import Utils from '../utils/Utils'",
            "Utils",
            "import configstore = require('configstore')",
            "configstore = require('configstore')",
            "import isValidPath = require('is-valid-path')",
            "isValidPath = require('is-valid-path')"
        ],
        "reference_api": [
            "self.getAppDefinition(appName).then",
            "verifyCustomDomainBelongsToApp",
            "self.getAppDefinition",
            "ApiStatusCodes.createError"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.getAppDefinition(appName).then",
                "code": "v"
            },
            {
                "name": "verifyCustomDomainBelongsToApp",
                "code": "v"
            },
            {
                "name": "self.getAppDefinition",
                "code": "v"
            },
            {
                "name": "ApiStatusCodes.createError",
                "code": "v"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "azure",
        "owner/repo": "caprover/caprover",
        "function_declaration": "addCustomDomainForAppForMigration(\n        appName: string,\n        hasDefaultSubDomainSsl: boolean,\n        customDomains: any[]\n    )",
        "start_line": "455",
        "end_line": "477",
        "file_path": "src/datastore/AppsDataStore.ts",
        "docstring": "The addCustomDomainForAppForMigration function adds custom domains to a specified app and updates its SSL settings. It takes appName, hasDefaultSubDomainSsl, and customDomains as parameters. It first retrieves the app definition using getAppDefinition. Then, it initializes the customDomain array if it doesn't exist and adds each custom domain from the customDomains array, ensuring each entry includes a publicDomain and hasSsl flag. It sets the hasDefaultSubDomainSsl property to the provided value and saves the updated app definition using saveApp.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3af3cd1183d3",
        "ground_truth": "addCustomDomainForAppForMigration(\n    appName: string,\n    hasDefaultSubDomainSsl: boolean,\n    customDomains: any[]\n) {\n    const self = this\n    return this.getAppDefinition(appName) //\n        .then(function (app) {\n            app.customDomain = app.customDomain || []\n            for (let idx = 0; idx < customDomains.length; idx++) {\n                app.customDomain.push({\n                    publicDomain: customDomains[idx].publicDomain + '',\n                    hasSsl: !!customDomains[idx].hasSsl,\n                })\n            }\n            app.hasDefaultSubDomainSsl = !!hasDefaultSubDomainSsl\n            return self.saveApp(appName, app)\n        })\n}",
        "import_statements": [
            "import { v4 as uuid } from 'uuid'",
            "{ v4 as uuid }",
            "{ v4 as uuid }",
            "v4 as uuid",
            "import ApiStatusCodes from '../api/ApiStatusCodes'",
            "ApiStatusCodes",
            "import { IBuiltImage } from '../models/IBuiltImage'",
            "{ IBuiltImage }",
            "{ IBuiltImage }",
            "IBuiltImage",
            "import Authenticator from '../user/Authenticator'",
            "Authenticator",
            "import ApacheMd5 from '../utils/ApacheMd5'",
            "ApacheMd5",
            "import CaptainConstants from '../utils/CaptainConstants'",
            "CaptainConstants",
            "import CaptainEncryptor from '../utils/Encryptor'",
            "CaptainEncryptor",
            "import Logger from '../utils/Logger'",
            "Logger",
            "import Utils from '../utils/Utils'",
            "Utils",
            "import configstore = require('configstore')",
            "configstore = require('configstore')",
            "import isValidPath = require('is-valid-path')",
            "isValidPath = require('is-valid-path')"
        ],
        "reference_api": [
            "this.getAppDefinition",
            "app.customDomain.push",
            "self.saveApp",
            "addCustomDomainForAppForMigration",
            "this.getAppDefinition(appName) //\n            .then"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "this.getAppDefinition",
                "code": "v"
            },
            {
                "name": "app.customDomain.push",
                "code": "v"
            },
            {
                "name": "self.saveApp",
                "code": "v"
            },
            {
                "name": "addCustomDomainForAppForMigration",
                "code": "v"
            },
            {
                "name": "this.getAppDefinition(appName) //\n            .then",
                "code": "v"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "azure",
        "owner/repo": "caprover/caprover",
        "function_declaration": "insertOneClickBaseUrl(url: string)",
        "start_line": "327",
        "end_line": "342",
        "file_path": "src/datastore/DataStore.ts",
        "docstring": "The insertOneClickBaseUrl function adds a given URL to a list of custom one-click application URLs stored in the self.data object. It returns a Promise that resolves once the URL has been added. The function first retrieves the current list of URLs from self.data, parses it, and adds the new URL to the list. It then updates self.data with the new list in JSON format.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d65d494463c1",
        "ground_truth": "insertOneClickBaseUrl(url: string) {\n    const self = this\n    return new Promise<void>(function (resolve, reject) {\n        const parsedArray = JSON.parse(\n            self.data.get(CUSTOM_ONE_CLICK_APP_URLS) || '[]'\n        ) as string[]\n        parsedArray.push(url)\n        self.data.set(\n            CUSTOM_ONE_CLICK_APP_URLS,\n            JSON.stringify(parsedArray)\n        )\n        resolve()\n    })\n}",
        "import_statements": [
            "import Configstore = require('configstore')",
            "Configstore = require('configstore')",
            "import fs = require('fs-extra')",
            "fs = require('fs-extra')",
            "import {\n    AutomatedCleanupConfigsCleaner,\n    IAutomatedCleanupConfigs,\n} from '../models/AutomatedCleanupConfigs'",
            "{\n    AutomatedCleanupConfigsCleaner,\n    IAutomatedCleanupConfigs,\n}",
            "{\n    AutomatedCleanupConfigsCleaner,\n    IAutomatedCleanupConfigs,\n}",
            "AutomatedCleanupConfigsCleaner",
            "IAutomatedCleanupConfigs",
            "import CaptainConstants from '../utils/CaptainConstants'",
            "CaptainConstants",
            "import CaptainEncryptor from '../utils/Encryptor'",
            "CaptainEncryptor",
            "import AppsDataStore from './AppsDataStore'",
            "AppsDataStore",
            "import ProDataStore from './ProDataStore'",
            "ProDataStore",
            "import RegistriesDataStore from './RegistriesDataStore'",
            "RegistriesDataStore"
        ],
        "reference_api": [
            "JSON.stringify",
            "self.data.set",
            "resolve",
            "JSON.parse",
            "self.data.get",
            "parsedArray.push",
            "insertOneClickBaseUrl"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.data.set",
                "code": "i"
            },
            {
                "name": "resolve",
                "code": "i"
            },
            {
                "name": "self.data.get",
                "code": "i"
            },
            {
                "name": "parsedArray.push",
                "code": "i"
            },
            {
                "name": "insertOneClickBaseUrl",
                "code": "i"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "azure",
        "owner/repo": "caprover/caprover",
        "function_declaration": "private saveAllRegistries(registries: IRegistryInfo[])",
        "start_line": "258",
        "end_line": "278",
        "file_path": "src/datastore/RegistriesDataStore.ts",
        "docstring": "The saveAllRegistries function saves a list of registry information by first encrypting the registry passwords. It takes an array of registry information objects (registries) and processes each object to create a new array of encrypted registry information objects. For each registry object, it extracts the relevant details, encrypts the registry password using the self.encryptor.encrypt method, and constructs an encrypted registry information object. This encrypted object is then added to the encryptedList array. Finally, the function sets the encryptedList into self.data under the key DOCKER_REGISTRIES.",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4d35440575f4",
        "ground_truth": "private saveAllRegistries(registries: IRegistryInfo[]) {\n    const self = this\n    return Promise.resolve() //\n        .then(function () {\n            const encryptedList: IRegistryInfoEncrypted[] = []\n            for (let i = 0; i < registries.length; i++) {\n                const element = registries[i]\n                encryptedList.push({\n                    id: element.id,\n                    registryDomain: element.registryDomain,\n                    registryImagePrefix: element.registryImagePrefix,\n                    registryUser: element.registryUser,\n                    registryPasswordEncrypted: self.encryptor.encrypt(\n                        element.registryPassword\n                    ),\n                    registryType: element.registryType,\n                })\n            }\n            self.data.set(DOCKER_REGISTRIES, encryptedList)\n        })\n}",
        "import_statements": [
            "import { v4 as uuid } from 'uuid'",
            "{ v4 as uuid }",
            "{ v4 as uuid }",
            "v4 as uuid",
            "import ApiStatusCodes from '../api/ApiStatusCodes'",
            "ApiStatusCodes",
            "import {\n    IRegistryInfo,\n    IRegistryInfoEncrypted,\n    IRegistryType,\n    IRegistryTypes,\n} from '../models/IRegistryInfo'",
            "{\n    IRegistryInfo,\n    IRegistryInfoEncrypted,\n    IRegistryType,\n    IRegistryTypes,\n}",
            "{\n    IRegistryInfo,\n    IRegistryInfoEncrypted,\n    IRegistryType,\n    IRegistryTypes,\n}",
            "IRegistryInfo",
            "IRegistryInfoEncrypted",
            "IRegistryType",
            "IRegistryTypes",
            "import CaptainEncryptor from '../utils/Encryptor'",
            "CaptainEncryptor",
            "import configstore = require('configstore')",
            "configstore = require('configstore')"
        ],
        "reference_api": [
            "encryptedList.push",
            "Promise.resolve",
            "self.encryptor.encrypt",
            "self.data.set",
            "saveAllRegistries",
            "Promise.resolve() //\n            .then"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "encryptedList.push",
            "self.encryptor.encrypt",
            "self.data.set",
            "saveAllRegistries",
            "Promise.resolve() //\n            .then"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "caprover/caprover",
        "function_declaration": "export function injectGlobal()",
        "start_line": "22",
        "end_line": "45",
        "file_path": "src/injection/Injector.ts",
        "docstring": "The injectGlobal function is a middleware for Express.js. It sets several properties on the res.locals object based on request headers and configuration values. The middleware first sets the namespace from the request header or defaults it to the root namespace. If the namespace is not the root namespace, it throws an error. It then sets the initialized, forceSsl, and userManagerForLoginOnly properties on res.locals using values from the CaptainManager and UserManagerProvider. Finally, it calls the next middleware in the stack.\n",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "087457f1e975",
        "ground_truth": "export function injectGlobal() {\n    return function (req: Request, res: Response, next: NextFunction) {\n        const locals = res.locals\n         locals.namespace =\n            req.header(CaptainConstants.headerNamespace) ||\n            CaptainConstants.rootNameSpace\n         if (locals.namespace !== CaptainConstants.rootNameSpace) {\n            throw ApiStatusCodes.createError(\n                ApiStatusCodes.STATUS_ERROR_GENERIC,\n                'Namespace unknown'\n            )\n        }\n         locals.initialized = CaptainManager.get().isInitialized()\n        locals.forceSsl = CaptainManager.get().getForceSslValue()\n        locals.userManagerForLoginOnly = UserManagerProvider.get(\n            locals.namespace\n        )\n         next()\n    }\n}",
        "import_statements": [
            "import { NextFunction, Request, Response } from 'express'",
            "{ NextFunction, Request, Response }",
            "{ NextFunction, Request, Response }",
            "NextFunction",
            "Request",
            "Response",
            "import ApiStatusCodes from '../api/ApiStatusCodes'",
            "ApiStatusCodes",
            "import BaseApi from '../api/BaseApi'",
            "BaseApi",
            "import DataStoreProvider from '../datastore/DataStoreProvider'",
            "DataStoreProvider",
            "import DockerApiProvider from '../docker/DockerApi'",
            "DockerApiProvider",
            "import * as UserModel from '../models/InjectionInterfaces'",
            "* as UserModel",
            "* as UserModel",
            "import { CaptainError } from '../models/OtherTypes'",
            "{ CaptainError }",
            "{ CaptainError }",
            "CaptainError",
            "import Authenticator from '../user/Authenticator'",
            "Authenticator",
            "import OtpAuthenticator from '../user/pro/OtpAuthenticator'",
            "OtpAuthenticator",
            "import ServiceManager from '../user/ServiceManager'",
            "ServiceManager",
            "import CaptainManager from '../user/system/CaptainManager'",
            "CaptainManager",
            "import { UserManagerProvider } from '../user/UserManagerProvider'",
            "{ UserManagerProvider }",
            "{ UserManagerProvider }",
            "UserManagerProvider",
            "import CaptainConstants from '../utils/CaptainConstants'",
            "CaptainConstants",
            "import Logger from '../utils/Logger'",
            "Logger",
            "import InjectionExtractor from './InjectionExtractor'",
            "InjectionExtractor"
        ],
        "reference_api": [
            "next",
            "CaptainManager.get",
            "CaptainManager.get().isInitialized",
            "req.header",
            "ApiStatusCodes.createError",
            "CaptainManager.get().getForceSslValue",
            "UserManagerProvider.get"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "next",
            "CaptainManager.get",
            "CaptainManager.get().isInitialized",
            "req.header",
            "ApiStatusCodes.createError",
            "CaptainManager.get().getForceSslValue",
            "UserManagerProvider.get"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "caprover/caprover",
        "function_declaration": "export function injectUserUsingCookieDataOnly()",
        "start_line": "271",
        "end_line": "294",
        "file_path": "src/injection/Injector.ts",
        "docstring": "The injectUserUsingCookieDataOnly function is a middleware for Express.js that injects user information into the response object using authentication data stored in cookies. It retrieves the authenticator for the root namespace and attempts to decode the authentication token from the cookies in the request. If successful, it assigns the decoded user information to res.locals.user and calls the next middleware. If an error occurs, it sends an error response if the error has a specific type or logs the error, sets res.locals.user to undefined, and calls the next middleware.\n",
        "language": "TypeScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5e1b863b61e3",
        "ground_truth": "export function injectUserUsingCookieDataOnly() {\n    return function (req: Request, res: Response, next: NextFunction) {\n        Authenticator.getAuthenticator(CaptainConstants.rootNameSpace)\n            .decodeAuthTokenFromCookies(\n                req.cookies[CaptainConstants.headerCookieAuth]\n            )\n            .then(function (user) {\n                res.locals.user = user\n                 next()\n            })\n            .catch(function (error) {\n                if (error && error.captainErrorType) {\n                    res.send(\n                        new BaseApi(error.captainErrorType, error.apiMessage)\n                    )\n                    return\n                }\n                Logger.e(error)\n                res.locals.user = undefined\n                next()\n            })\n    }\n}",
        "import_statements": [
            "import { NextFunction, Request, Response } from 'express'",
            "{ NextFunction, Request, Response }",
            "{ NextFunction, Request, Response }",
            "NextFunction",
            "Request",
            "Response",
            "import ApiStatusCodes from '../api/ApiStatusCodes'",
            "ApiStatusCodes",
            "import BaseApi from '../api/BaseApi'",
            "BaseApi",
            "import DataStoreProvider from '../datastore/DataStoreProvider'",
            "DataStoreProvider",
            "import DockerApiProvider from '../docker/DockerApi'",
            "DockerApiProvider",
            "import * as UserModel from '../models/InjectionInterfaces'",
            "* as UserModel",
            "* as UserModel",
            "import { CaptainError } from '../models/OtherTypes'",
            "{ CaptainError }",
            "{ CaptainError }",
            "CaptainError",
            "import Authenticator from '../user/Authenticator'",
            "Authenticator",
            "import OtpAuthenticator from '../user/pro/OtpAuthenticator'",
            "OtpAuthenticator",
            "import ServiceManager from '../user/ServiceManager'",
            "ServiceManager",
            "import CaptainManager from '../user/system/CaptainManager'",
            "CaptainManager",
            "import { UserManagerProvider } from '../user/UserManagerProvider'",
            "{ UserManagerProvider }",
            "{ UserManagerProvider }",
            "UserManagerProvider",
            "import CaptainConstants from '../utils/CaptainConstants'",
            "CaptainConstants",
            "import Logger from '../utils/Logger'",
            "Logger",
            "import InjectionExtractor from './InjectionExtractor'",
            "InjectionExtractor"
        ],
        "reference_api": [
            "Authenticator.getAuthenticator(CaptainConstants.rootNameSpace)\n            .decodeAuthTokenFromCookies(\n                req.cookies[CaptainConstants.headerCookieAuth]\n            )\n            .then(function (user) {\n                res.locals.user = user\n\n                next()\n            })\n            .catch",
            "next",
            "Logger.e",
            "Authenticator.getAuthenticator(CaptainConstants.rootNameSpace)\n            .decodeAuthTokenFromCookies",
            "Authenticator.getAuthenticator(CaptainConstants.rootNameSpace)\n            .decodeAuthTokenFromCookies(\n                req.cookies[CaptainConstants.headerCookieAuth]\n            )\n            .then",
            "Authenticator.getAuthenticator",
            "res.send"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "Authenticator.getAuthenticator(CaptainConstants.rootNameSpace)\n            .decodeAuthTokenFromCookies(\n                req.cookies[CaptainConstants.headerCookieAuth]\n            )\n            .then(function (user) {\n                res.locals.user = user\n\n                next()\n            })\n            .catch",
            "next",
            "Logger.e",
            "Authenticator.getAuthenticator(CaptainConstants.rootNameSpace)\n            .decodeAuthTokenFromCookies",
            "Authenticator.getAuthenticator(CaptainConstants.rootNameSpace)\n            .decodeAuthTokenFromCookies(\n                req.cookies[CaptainConstants.headerCookieAuth]\n            )\n            .then",
            "Authenticator.getAuthenticator",
            "res.send"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func (c *APIClient) DoQueries(queries []GraphQLQuery) ([]gjson.Result, error)",
        "start_line": "59",
        "end_line": "67",
        "file_path": "internal/apiclient/client.go",
        "docstring": "The DoQueries function in the APIClient struct executes a list of GraphQL queries. If the queries list is empty, it logs a debug message and returns an empty result set. Otherwise, it sends a POST request to the \"/graphql\" endpoint with the queries. The function returns the parsed JSON response as a slice of gjson.Result and any error encountered during the request.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ff28059f5f81",
        "ground_truth": "func (c *APIClient) DoQueries(queries []GraphQLQuery) ([]gjson.Result, error) {\n if len(queries) == 0 {\n  logging.Logger.Debug().Msg(\"Skipping GraphQL request as no queries have been specified\")\n  return []gjson.Result{}, nil\n }\n  respBody, err := c.doRequest(\"POST\", \"/graphql\", queries)\n return gjson.ParseBytes(respBody).Array(), err\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"strings\"\n\n\tjson \"github.com/json-iterator/go\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/tidwall/gjson\"\n\n\t\"github.com/infracost/infracost/internal/logging\"\n\t\"github.com/infracost/infracost/internal/version\"\n)"
        ],
        "reference_api": [
            "gjson.ParseBytes(respBody).Array",
            "c.doRequest",
            "gjson.ParseBytes",
            "len",
            "logging.Logger.Debug",
            "logging.Logger.Debug().Msg"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "gjson.ParseBytes(respBody).Array",
            "c.doRequest",
            "gjson.ParseBytes",
            "len",
            "logging.Logger.Debug",
            "logging.Logger.Debug().Msg"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func (c *APIClient) AddAuthHeaders(req *http.Request)",
        "start_line": "128",
        "end_line": "139",
        "file_path": "internal/apiclient/client.go",
        "docstring": "The AddAuthHeaders function in the APIClient struct adds authentication headers to an HTTP request. It first calls AddDefaultHeaders to add any default headers. If the apiKey starts with \"ics\", it sets the Authorization header with a Bearer token. Otherwise, it sets the X-Api-Key header with the apiKey. Additionally, if the uuid is not nil, it sets the X-Infracost-Trace-Id header with the uuid value formatted as \"cli={uuid}\".",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a4e038bde66b",
        "ground_truth": "func (c *APIClient) AddAuthHeaders(req *http.Request) {\n c.AddDefaultHeaders(req)\n if strings.HasPrefix(c.apiKey, \"ics\") {\n  req.Header.Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", c.apiKey))\n } else {\n  req.Header.Set(\"X-Api-Key\", c.apiKey)\n }\n  if c.uuid != uuid.Nil {\n  req.Header.Set(\"X-Infracost-Trace-Id\", fmt.Sprintf(\"cli=%s\", c.uuid.String()))\n }\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"strings\"\n\n\tjson \"github.com/json-iterator/go\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/tidwall/gjson\"\n\n\t\"github.com/infracost/infracost/internal/logging\"\n\t\"github.com/infracost/infracost/internal/version\"\n)"
        ],
        "reference_api": [
            "c.AddDefaultHeaders",
            "c.uuid.String",
            "req.Header.Set",
            "strings.HasPrefix",
            "fmt.Sprintf"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "c.AddDefaultHeaders",
            "c.uuid.String",
            "req.Header.Set"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func NewDashboardAPIClient(ctx *config.RunContext) *DashboardAPIClient",
        "start_line": "70",
        "end_line": "82",
        "file_path": "internal/apiclient/dashboard.go",
        "docstring": "The NewDashboardAPIClient function initializes a new DashboardAPIClient instance. It creates a retryable HTTP client, sets up a custom logger for the client, and then constructs a DashboardAPIClient with the HTTP client, API endpoint, API key, and UUID from the provided configuration context (ctx). The function ensures the client is configured to handle retries and logging appropriately.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c4e8e5c2218a",
        "ground_truth": "func NewDashboardAPIClient(ctx *config.RunContext) *DashboardAPIClient {\n client := retryablehttp.NewClient()\n client.Logger = &LeveledLogger{Logger: logging.Logger.With().Str(\"library\", \"retryablehttp\").Logger()}\n  return &DashboardAPIClient{\n  APIClient: APIClient{\n   httpClient: client.StandardClient(),\n   endpoint:   ctx.Config.DashboardAPIEndpoint,\n   apiKey:     ctx.Config.APIKey,\n   uuid:       ctx.UUID(),\n  },\n }\n}",
        "import_statements": [
            "import (\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-retryablehttp\"\n\tjson \"github.com/json-iterator/go\"\n\n\t\"github.com/pkg/errors\"\n\n\t\"github.com/infracost/infracost/internal/config\"\n\t\"github.com/infracost/infracost/internal/logging\"\n\t\"github.com/infracost/infracost/internal/output\"\n\t\"github.com/infracost/infracost/internal/schema\"\n)"
        ],
        "reference_api": [
            "retryablehttp.NewClient",
            "client.StandardClient",
            "logging.Logger.With().Str(\"library\", \"retryablehttp\").Logger",
            "ctx.UUID",
            "logging.Logger.With().Str",
            "logging.Logger.With"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "retryablehttp.NewClient",
            "client.StandardClient",
            "logging.Logger.With().Str(\"library\", \"retryablehttp\").Logger",
            "ctx.UUID",
            "logging.Logger.With().Str",
            "logging.Logger.With"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func (c *PolicyAPIClient) filterResources(rds []*schema.ResourceData) []policy2Resource ",
        "start_line": "173",
        "end_line": "186",
        "file_path": "internal/apiclient/policy.go",
        "docstring": "The filterResources function filters and processes a list of resource data objects. It iterates over each resource data object (rd) and checks if the resource type is present in the allowLists of the PolicyAPIClient instance. If it is, it filters the resource using the filterResource function and appends the result to a list of policy2Resource objects (p2rs). Finally, it sorts the list of policy2Resource objects by their Address field in ascending order and returns the sorted list.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c7d8dd22a2e6",
        "ground_truth": "func (c *PolicyAPIClient) filterResources(rds []*schema.ResourceData) []policy2Resource {\n var p2rs []policy2Resource\n for _, rd := range rds {\n  if f, ok := c.allowLists[rd.Type]; ok {\n   p2rs = append(p2rs, filterResource(rd, f))\n  }\n }\n  sort.Slice(p2rs, func(i, j int) bool {\n  return p2rs[i].Address < p2rs[j].Address\n })\n  return p2rs\n}",
        "import_statements": [
            "import (\n\t\"crypto/sha256\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"sort\"\n\t\"sync\"\n\n\t\"github.com/hashicorp/go-retryablehttp\"\n\tjson \"github.com/json-iterator/go\"\n\t\"github.com/tidwall/gjson\"\n\n\t\"github.com/infracost/infracost/internal/config\"\n\t\"github.com/infracost/infracost/internal/logging\"\n\t\"github.com/infracost/infracost/internal/schema\"\n)"
        ],
        "reference_api": [
            "append",
            "filterResource",
            "sort.Slice"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "filterResource",
                "code": "func filterResource(rd *schema.ResourceData, al allowList) policy2Resource {\n\tvar tagsPtr *[]policy2Tag\n\tif rd.Tags != nil {\n\t\ttags := make([]policy2Tag, 0, len(*rd.Tags))\n\t\tfor k, v := range *rd.Tags {\n\t\t\ttags = append(tags, policy2Tag{Key: k, Value: v})\n\t\t}\n\t\tsort.Slice(tags, func(i, j int) bool {\n\t\t\treturn tags[i].Key < tags[j].Key\n\t\t})\n\n\t\ttagsPtr = &tags\n\t}\n\n\tvar propagatedTagsPtr *[]policy2Tag\n\tif rd.TagPropagation != nil && rd.TagPropagation.Tags != nil {\n\t\ttags := make([]policy2Tag, 0, len(*rd.Tags))\n\t\tfor k, v := range *rd.TagPropagation.Tags {\n\t\t\ttags = append(tags, policy2Tag{Key: k, Value: v})\n\t\t}\n\t\tsort.Slice(tags, func(i, j int) bool {\n\t\t\treturn tags[i].Key < tags[j].Key\n\t\t})\n\t\tpropagatedTagsPtr = &tags\n\t}\n\n\t// make sure the keys in the values json are sorted so we get consistent policyShas\n\tvaluesJSON, err := jsonSorted.Marshal(filterValues(rd.RawValues, al))\n\tif err != nil {\n\t\tlogging.Logger.Debug().Err(err).Str(\"address\", rd.Address).Msg(\"Failed to marshal filtered values\")\n\t}\n\n\treferences := make([]policy2Reference, 0, len(rd.ReferencesMap))\n\tfor k, refRds := range rd.ReferencesMap {\n\t\trefAddresses := make([]string, 0, len(refRds))\n\t\tfor _, refRd := range refRds {\n\t\t\trefAddresses = append(refAddresses, refRd.Address)\n\t\t}\n\t\treferences = append(references, policy2Reference{Key: k, Addresses: refAddresses})\n\t}\n\tsort.Slice(references, func(i, j int) bool {\n\t\treturn references[i].Key < references[j].Key\n\t})\n\n\tvar mdCalls []policy2InfracostMetadataCall\n\tfor _, c := range rd.Metadata[\"calls\"].Array() {\n\t\tmdCalls = append(mdCalls, policy2InfracostMetadataCall{\n\t\t\tBlockName: c.Get(\"blockName\").String(),\n\t\t\tEndLine:   c.Get(\"endLine\").Int(),\n\t\t\tFilename:  c.Get(\"filename\").String(),\n\t\t\tStartLine: c.Get(\"startLine\").Int(),\n\t\t})\n\t}\n\n\tchecksum := rd.Metadata[\"checksum\"].String()\n\n\tif checksum == \"\" {\n\t\t// this must be a plan json run.  calculate a checksum now.\n\t\tchecksum = calcChecksum(rd)\n\t}\n\n\tvar tagPropagation *TagPropagation\n\tif rd.TagPropagation != nil {\n\t\ttagPropagation = &TagPropagation{\n\t\t\tTo:                    rd.TagPropagation.To,\n\t\t\tFrom:                  rd.TagPropagation.From,\n\t\t\tTags:                  propagatedTagsPtr,\n\t\t\tAttribute:             rd.TagPropagation.Attribute,\n\t\t\tHasRequiredAttributes: rd.TagPropagation.HasRequiredAttributes,\n\t\t}\n\t}\n\n\treturn policy2Resource{\n\t\tResourceType:   rd.Type,\n\t\tProviderName:   rd.ProviderName,\n\t\tAddress:        rd.Address,\n\t\tTags:           tagsPtr,\n\t\tTagPropagation: tagPropagation,\n\t\tValues:         valuesJSON,\n\t\tReferences:     references,\n\t\tMetadata: policy2InfracostMetadata{\n\t\t\tCalls:          mdCalls,\n\t\t\tChecksum:       checksum,\n\t\t\tEndLine:        rd.Metadata[\"endLine\"].Int(),\n\t\t\tFilename:       rd.Metadata[\"filename\"].String(),\n\t\t\tStartLine:      rd.Metadata[\"startLine\"].Int(),\n\t\t\tModuleFilename: rd.Metadata[\"moduleFilename\"].String(),\n\t\t},\n\t\tRegion: rd.Region,\n\t}\n}"
            }
        ],
        "third_party": [
            "append",
            "sort.Slice"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func newCache(ctx *config.RunContext) *lru.TwoQueueCache[uint64, cacheValue]",
        "start_line": "167",
        "end_line": "174",
        "file_path": "internal/apiclient/pricing.go",
        "docstring": "The function newCache initializes a new LRU (Least Recently Used) TwoQueueCache for caching pricing data. It takes a RunContext object as input, which contains configuration details. The function sets the cache object limit to a default of 1000, but if the PricingCacheObjectSize in the configuration is greater than 0, it uses that value instead. It then creates and returns a new TwoQueueCache with the specified object limit.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0e32e938784b",
        "ground_truth": "func newCache(ctx *config.RunContext) *lru.TwoQueueCache[uint64, cacheValue] {\n objectLimit := 1000\n if ctx.Config.PricingCacheObjectSize > 0 {\n  objectLimit = ctx.Config.PricingCacheObjectSize\n }\n l, _ := lru.New2Q[uint64, cacheValue](objectLimit)\n return l\n}",
        "import_statements": [
            "import (\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/gob\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/hashicorp/go-retryablehttp\"\n\tlru \"github.com/hashicorp/golang-lru/v2\"\n\t\"github.com/mitchellh/hashstructure/v2\"\n\n\t\"github.com/infracost/infracost/internal/config\"\n\t\"github.com/infracost/infracost/internal/logging\"\n\t\"github.com/infracost/infracost/internal/schema\"\n\n\t\"github.com/tidwall/gjson\"\n)"
        ],
        "reference_api": [],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func unflattenUsageKey(attribs map[string]interface{}, usageKey string, value string)",
        "start_line": "247",
        "end_line": "265",
        "file_path": "internal/apiclient/usage.go",
        "docstring": "The unflattenUsageKey function takes a map, a usage key, and a value, and adds the value to the map in a nested structure based on the usage key. It splits the usage key on the first dot to determine the top-level key and any remaining sub-keys. If there are no sub-keys, it adds the value directly to the map. If there are sub-keys, it checks if the top-level key already exists in the map. If it does, it retrieves the associated map; if not, it creates a new map. It then recursively calls itself with the sub-keys and value to populate the nested structure.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4b992e40c5c7",
        "ground_truth": "func unflattenUsageKey(attribs map[string]interface{}, usageKey string, value string) {\n split := strings.SplitN(usageKey, \".\", 2)\n if len(split) <= 1 {\n  attribs[usageKey] = value\n  return\n }\n  var childAttribs map[string]interface{}\n if val, ok := attribs[split[0]]; ok {\n  childAttribs = val.(map[string]interface{})\n } else {\n  // sub attrib map doesn't already exist so add it to the parent\n  childAttribs = make(map[string]interface{})\n  attribs[split[0]] = childAttribs\n }\n  // populate the value in the childMap (recursively, in case there are multiple \".\")\n unflattenUsageKey(childAttribs, split[1], value)\n}",
        "import_statements": [
            "import (\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"os\"\n\t\"strings\"\n\t\"time\"\n\n\tjson \"github.com/json-iterator/go\"\n\n\t\"github.com/hashicorp/go-cleanhttp\"\n\t\"github.com/tidwall/gjson\"\n\n\t\"github.com/infracost/infracost/internal/config\"\n\t\"github.com/infracost/infracost/internal/logging\"\n\t\"github.com/infracost/infracost/internal/schema\"\n)"
        ],
        "reference_api": [
            "unflattenUsageKey",
            "len",
            "make",
            "strings.SplitN"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "unflattenUsageKey",
                "code": "func unflattenUsageKey(attribs map[string]interface{}, usageKey string, value string) {\n\tsplit := strings.SplitN(usageKey, \".\", 2)\n\tif len(split) <= 1 {\n\t\tattribs[usageKey] = value\n\t\treturn\n\t}\n\n\tvar childAttribs map[string]interface{}\n\tif val, ok := attribs[split[0]]; ok {\n\t\tchildAttribs = val.(map[string]interface{})\n\t} else {\n\t\t// sub attrib map doesn't already exist so add it to the parent\n\t\tchildAttribs = make(map[string]interface{})\n\t\tattribs[split[0]] = childAttribs\n\t}\n\n\t// populate the value in the childMap (recursively, in case there are multiple \".\")\n\tunflattenUsageKey(childAttribs, split[1], value)\n}"
            }
        ],
        "third_party": [
            "len",
            "make"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func newAzureReposAPIClient(ctx context.Context, token string) (*http.Client, error) ",
        "start_line": "89",
        "end_line": "108",
        "file_path": "internal/comment/azure_repos.go",
        "docstring": "The newAzureReposAPIClient function creates and returns a new HTTP client for Azure Repos API authentication. It takes a context and a token as parameters. If the token length matches a predefined Azure Personal Access Token (PAT) length, it encodes the token in base64 and sets the token type to \"Basic.\" Otherwise, it sets the token type to \"Bearer.\" It then creates an OAuth2 static token source with the access token and token type, and uses this to create and return a new HTTP client configured with the OAuth2 token source.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "14e404acb14c",
        "ground_truth": "func newAzureReposAPIClient(ctx context.Context, token string) (*http.Client, error) {\n accessToken, tokenType := token, \"Bearer\"\n  if len(token) == azurePATLength {\n  accessToken = base64.StdEncoding.EncodeToString(\n   []byte(fmt.Sprintf(\":%s\", accessToken)),\n  )\n  tokenType = \"Basic\"\n }\n  ts := oauth2.StaticTokenSource(\n  &oauth2.Token{\n   AccessToken: accessToken,\n   TokenType:   tokenType,\n  },\n )\n httpClient := oauth2.NewClient(ctx, ts)\n  return httpClient, nil\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/oauth2\"\n)"
        ],
        "reference_api": [
            "oauth2.StaticTokenSource",
            "len",
            "oauth2.NewClient",
            "base64.StdEncoding.EncodeToString",
            "fmt.Sprintf"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "oauth2.StaticTokenSource",
            "len",
            "oauth2.NewClient"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func buildAzureAPIURL(repoURL string) (string, error)",
        "start_line": "111",
        "end_line": "131",
        "file_path": "internal/comment/azure_repos.go",
        "docstring": "The buildAzureAPIURL function constructs an Azure API URL from a given repository URL. It first parses the repo URL and splits the path to extract the organization and project details. It validates the URL format and removes any user information that might cause authentication issues. The function then formats the path to include the necessary API segments for accessing the Git repository and ensures it ends with a slash. Finally, it returns the constructed API URL as a string or an error if any issues occur during processing.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e7bc31e46d1b",
        "ground_truth": "func buildAzureAPIURL(repoURL string) (string, error) {\n apiURL, err := url.Parse(repoURL)\n if err != nil {\n  return \"\", fmt.Errorf(\"error parsing repo URL %w\", err)\n }\n  urlParts := strings.Split(apiURL.Path, \"_git/\")\n if len(urlParts) != 2 {\n  return \"\", fmt.Errorf(\"Invalid repo URL format %s. Expected https://dev.azure.com/org/project/_git/repo/\", repoURL)\n }\n  // The URL can contain `org@` username part. If it's present in the API URL,\n // requests may result with 401 status even with the provided token.\n apiURL.User = nil\n apiURL.Path = fmt.Sprintf(\"%s_apis/git/repositories/%s\", urlParts[0], urlParts[1])\n if !strings.HasSuffix(apiURL.Path, \"/\") {\n  apiURL.Path += \"/\"\n }\n  return apiURL.String(), nil\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/oauth2\"\n)"
        ],
        "reference_api": [
            "fmt.Errorf",
            "strings.HasSuffix",
            "len",
            "strings.Split",
            "url.Parse",
            "fmt.Sprintf",
            "apiURL.String"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "len",
            "url.Parse",
            "apiURL.String"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func (h *azureReposPRHandler) CallUpdateComment(ctx context.Context, comment Comment, body string) error",
        "start_line": "304",
        "end_line": "329",
        "file_path": "internal/comment/azure_repos.go",
        "docstring": "The CallUpdateComment function in the azureReposPRHandler structure updates a comment on Azure Repos using a PATCH request. It takes the context, a Comment object, and the new body content as parameters. First, it marshals the new content into JSON format. Then, it constructs the request URL using the comment's reference and creates an HTTP PATCH request with the JSON payload. The Content-Type header is set to \"application/json\". The function executes the request using an HTTP client and ensures that the response body is closed if it is not nil. It returns any error encountered during the process.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "95040cd9d65b",
        "ground_truth": "func (h *azureReposPRHandler) CallUpdateComment(ctx context.Context, comment Comment, body string) error {\n reqData, err := json.Marshal(map[string]interface{}{\n  \"content\":         body,\n  \"parentCommentId\": 0,\n  \"commentType\":     1,\n })\n if err != nil {\n  return errors.Wrap(err, \"Error marshaling comment body\")\n }\n  url := fmt.Sprintf(\"%s?api-version=6.0\", comment.Ref())\n  req, err := http.NewRequest(\"PATCH\", url, bytes.NewBuffer(reqData))\n if err != nil {\n  return errors.Wrap(err, \"Error creating request\")\n }\n req.Header.Set(\"Content-Type\", \"application/json\")\n  res, err := h.httpClient.Do(req)\n  if res.Body != nil {\n  defer res.Body.Close()\n }\n  return err\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/oauth2\"\n)"
        ],
        "reference_api": [
            "bytes.NewBuffer",
            "json.Marshal",
            "h.httpClient.Do",
            "errors.Wrap",
            "res.Body.Close",
            "req.Header.Set",
            "fmt.Sprintf",
            "comment.Ref",
            "http.NewRequest"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "bytes.NewBuffer",
            "h.httpClient.Do",
            "errors.Wrap",
            "res.Body.Close",
            "req.Header.Set",
            "comment.Ref"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func newBitbucketAPIClient(ctx context.Context, token string) (*http.Client, error) ",
        "start_line": "102",
        "end_line": "119",
        "file_path": "internal/comment/bitbucket.go",
        "docstring": "The function newBitbucketAPIClient creates a new HTTP client for interacting with the Bitbucket API using OAuth2 authentication. It takes a context and a token as input. If the token contains a colon, it is encoded in base64 and the token type is set to \"Basic\". Otherwise, the token type is set to \"Bearer\". The function then creates a static token source with the provided access token and token type, and returns an HTTP client configured with this token source.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "833eb58bda7d",
        "ground_truth": "func newBitbucketAPIClient(ctx context.Context, token string) (*http.Client, error) {\n accessToken, tokenType := token, \"Bearer\"\n  if strings.Contains(token, \":\") {\n  accessToken = base64.StdEncoding.EncodeToString([]byte(accessToken))\n  tokenType = \"Basic\"\n }\n  ts := oauth2.StaticTokenSource(\n  &oauth2.Token{\n   AccessToken: accessToken,\n   TokenType:   tokenType,\n  },\n )\n httpClient := oauth2.NewClient(ctx, ts)\n  return httpClient, nil\n}",
        "import_statements": [
            "import (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/oauth2\"\n)"
        ],
        "reference_api": [
            "oauth2.StaticTokenSource",
            "oauth2.NewClient",
            "strings.Contains",
            "base64.StdEncoding.EncodeToString"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "oauth2.StaticTokenSource",
            "oauth2.NewClient"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "infracost/infracost",
        "function_declaration": "func credFromJSON(filename, host string) (string, error) ",
        "start_line": "91",
        "end_line": "112",
        "file_path": "internal/credentials/terraform.go",
        "docstring": "The function credFromJSON reads a JSON file specified by filename and retrieves a token for a given host. It first reads the file's content and unmarshals the JSON data into a struct. The struct contains a map of credentials with hosts as keys and tokens as values. If the token for the specified host is found, the function returns it; otherwise, it returns an empty string. Errors during file reading or JSON unmarshalling are propagated to the caller.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4cb13923ab49",
        "ground_truth": "func credFromJSON(filename, host string) (string, error) {\n data, err := os.ReadFile(filename)\n if err != nil {\n  return \"\", err\n }\n  var conf struct {\n  Credentials map[string]struct {\n   Token string `json:\"token\"`\n  } `json:\"credentials\"`\n }\n err = json.Unmarshal(data, &conf)\n if err != nil {\n  return \"\", err\n }\n  if hostCred, ok := conf.Credentials[host]; ok {\n  return hostCred.Token, nil\n }\n  return \"\", nil\n}",
        "import_statements": [
            "import (\n\t\"encoding/json\"\n\t\"errors\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"runtime\"\n\n\t\"github.com/hashicorp/hcl/v2/gohcl\"\n\t\"github.com/hashicorp/hcl/v2/hclparse\"\n\t\"github.com/mitchellh/go-homedir\"\n\n\t\"github.com/infracost/infracost/internal/logging\"\n)"
        ],
        "reference_api": [
            "os.ReadFile",
            "json.Unmarshal"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "azure",
        "owner/repo": "Netflix/metaflow",
        "function_declaration": "def _find_binary_reader(stream)",
        "start_line": "308",
        "end_line": "321",
        "file_path": "metaflow/_vendor/click/_compat.py",
        "docstring": "The function _find_binary_reader checks if the given stream is a binary reader. If it is, the function returns the stream. If not, it attempts to access the buffer attribute of the stream and checks if the buffer is a binary reader. If the buffer is a binary reader, it returns the buffer.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7c956872e231",
        "ground_truth": "def _find_binary_reader(stream):\n    # We need to figure out if the given stream is already binary.\n    # This can happen because the official docs recommend detaching\n    # the streams to get binary streams.  Some code might do this, so\n    # we need to deal with this case explicitly.\n    if _is_binary_reader(stream, False):\n        return stream\n    buf = getattr(stream, \"buffer\", None)\n    # Same situation here; this time we assume that the buffer is\n    # actually binary in case it's closed.\n    if buf is not None and _is_binary_reader(buf, True):\n        return buf",
        "import_statements": [
            "import codecs",
            "import io",
            "import os",
            "import re",
            "import sys",
            "from weakref import WeakKeyDictionary"
        ],
        "reference_api": [
            "_is_binary_reader",
            "getattr"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "_is_binary_reader",
            "_is_binary_reader"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "Netflix/metaflow",
        "function_declaration": "def _find_binary_writer(stream)",
        "start_line": "323",
        "end_line": "336",
        "file_path": "metaflow/_vendor/click/_compat.py",
        "docstring": "The function _find_binary_writer attempts to identify and return a binary writer stream. It first checks if the provided stream is a binary writer using the _is_binary_writer function. If it is not, it then checks if the stream has a 'buffer' attribute. If the buffer exists and is confirmed to be a binary writer, the function returns this buffer.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b959e0eda105",
        "ground_truth": "def _find_binary_writer(stream):\n    # We need to figure out if the given stream is already binary.\n    # This can happen because the official docs recommend detatching\n    # the streams to get binary streams.  Some code might do this, so\n    # we need to deal with this case explicitly.\n    if _is_binary_writer(stream, False):\n        return stream\n    buf = getattr(stream, \"buffer\", None)\n    # Same situation here; this time we assume that the buffer is\n    # actually binary in case it's closed.\n    if buf is not None and _is_binary_writer(buf, True):\n        return buf",
        "import_statements": [
            "import codecs",
            "import io",
            "import os",
            "import re",
            "import sys",
            "from weakref import WeakKeyDictionary"
        ],
        "reference_api": [
            "getattr",
            "_is_binary_writer"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "_is_binary_writer",
            "_is_binary_writer"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "Netflix/metaflow",
        "function_declaration": "def _force_correct_text_stream(\n        text_stream,\n        encoding,\n        errors,\n        is_binary,\n        find_binary,\n        force_readable=False,\n        force_writable=False,\n    )",
        "start_line": "362",
        "end_line": "402",
        "file_path": "metaflow/_vendor/click/_compat.py",
        "docstring": "The function _force_correct_text_stream ensures that a given text stream is correctly configured with the specified encoding and error handling. It checks if the text stream is binary and retrieves the underlying binary reader if necessary. If the stream is already compatible with the desired configuration, it returns the stream as is. Otherwise, it constructs and returns a new text stream with the specified encoding, error handling, and optional force-read/write capabilities.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "95049b50cb13",
        "ground_truth": "def _force_correct_text_stream(\n    text_stream,\n    encoding,\n    errors,\n    is_binary,\n    find_binary,\n    force_readable=False,\n    force_writable=False,\n):\n    if is_binary(text_stream, False):\n        binary_reader = text_stream\n    else:\n        # If the stream looks compatible, and won't default to a\n        # misconfigured ascii encoding, return it as-is.\n        if _is_compatible_text_stream(text_stream, encoding, errors) and not (\n            encoding is None and _stream_is_misconfigured(text_stream)\n        ):\n            return text_stream\n        # Otherwise, get the underlying binary reader.\n        binary_reader = find_binary(text_stream)\n        # If that's not possible, silently use the original reader\n        # and get mojibake instead of exceptions.\n        if binary_reader is None:\n            return text_stream\n    # Default errors to replace instead of strict in order to get\n    # something that works.\n    if errors is None:\n        errors = \"replace\"\n    # Wrap the binary stream in a text stream with the correct\n    # encoding parameters.\n    return _make_text_stream(\n        binary_reader,\n        encoding,\n        errors,\n        force_readable=force_readable,\n        force_writable=force_writable,\n    )",
        "import_statements": [
            "import codecs",
            "import io",
            "import os",
            "import re",
            "import sys",
            "from weakref import WeakKeyDictionary"
        ],
        "reference_api": [
            "find_binary",
            "_is_compatible_text_stream",
            "is_binary",
            "_make_text_stream",
            "_stream_is_misconfigured"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_make_text_stream",
                "code": "def _make_text_stream(\n    stream, encoding, errors, force_readable=False, force_writable=False\n):\n    if encoding is None:\n        encoding = get_best_encoding(stream)\n    if errors is None:\n        errors = \"replace\"\n    return _NonClosingTextIOWrapper(\n        stream,\n        encoding,\n        errors,\n        line_buffering=True,\n        force_readable=force_readable,\n        force_writable=force_writable,\n    )"
            }
        ],
        "third_party": [
            "is_binary",
            "_is_compatible_text_stream",
            "_stream_is_misconfigured",
            "find_binary"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "Netflix/metaflow",
        "function_declaration": "def format_progress_line(self)",
        "start_line": "202",
        "end_line": "227",
        "file_path": "metaflow/_vendor/click/_termui_impl.py",
        "docstring": "The function format_progress_line constructs a formatted progress line for display. It collects various bits of information based on the object's attributes, such as whether to show the percentage completed, the current position, the estimated time of arrival (ETA), and any custom item information. These elements are formatted and appended to a list. The function then combines these bits into a final formatted string using a predefined bar template and returns this string after stripping any trailing whitespace.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "937c65460092",
        "ground_truth": "def format_progress_line(self):\n    show_percent = self.show_percent\n    info_bits = []\n    if self.length_known and show_percent is None:\n        show_percent = not self.show_pos\n    if self.show_pos:\n        info_bits.append(self.format_pos())\n    if show_percent:\n        info_bits.append(self.format_pct())\n    if self.show_eta and self.eta_known and not self.finished:\n        info_bits.append(self.format_eta())\n    if self.item_show_func is not None:\n        item_info = self.item_show_func(self.current_item)\n        if item_info is not None:\n            info_bits.append(item_info)\n    return (\n        self.bar_template\n        % {\n            \"label\": self.label,\n            \"bar\": self.format_bar(),\n            \"info\": self.info_sep.join(info_bits),\n        }\n    ).rstrip()",
        "import_statements": [
            "import contextlib",
            "import math",
            "import os",
            "import sys",
            "import time"
        ],
        "reference_api": [
            "join",
            "self.item_show_func",
            "self.format_pos",
            "rstrip",
            "self.format_eta",
            "self.format_pct",
            "self.format_bar",
            "info_bits.append"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.format_pos",
                "code": "def format_pos(self):\n        pos = str(self.pos)\n        if self.length_known:\n            pos += \"/{}\".format(self.length)\n        return pos"
            },
            {
                "name": "self.format_pct",
                "code": "def format_pct(self):\n        return \"{: 4}%\".format(int(self.pct * 100))[1:]"
            },
            {
                "name": "self.format_eta",
                "code": "def format_eta(self):\n        if self.eta_known:\n            t = int(self.eta)\n            seconds = t % 60\n            t //= 60\n            minutes = t % 60\n            t //= 60\n            hours = t % 24\n            t //= 24\n            if t > 0:\n                return \"{}d {:02}:{:02}:{:02}\".format(t, hours, minutes, seconds)\n            else:\n                return \"{:02}:{:02}:{:02}\".format(hours, minutes, seconds)\n        return \"\""
            },
            {
                "name": "self.format_bar",
                "code": "def format_bar(self):\n        if self.length_known:\n            bar_length = int(self.pct * self.width)\n            bar = self.fill_char * bar_length\n            bar += self.empty_char * (self.width - bar_length)\n        elif self.finished:\n            bar = self.fill_char * self.width\n        else:\n            bar = list(self.empty_char * (self.width or 1))\n            if self.time_per_iteration != 0:\n                bar[\n                    int(\n                        (math.cos(self.pos * self.time_per_iteration) / 2.0 + 0.5)\n                        * self.width\n                    )\n                ] = self.fill_char\n            bar = \"\".join(bar)\n        return bar"
            }
        ],
        "third_party": [
            "info_bits.append",
            "info_bits.append",
            "info_bits.append",
            "self.item_show_func",
            "info_bits.append",
            "rstrip",
            "join"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "Netflix/metaflow",
        "function_declaration": "def _tempfilepager(generator, cmd, color)",
        "start_line": "404",
        "end_line": "419",
        "file_path": "metaflow/_vendor/click/_termui_impl.py",
        "docstring": "The function _tempfilepager takes a generator, a command, and a boolean color flag to page through text by invoking a program on a temporary file. It creates a temporary file and joins the generated text into a single string. If color is False, it removes ANSI escape sequences from the text. The text is then written to the temporary file using the best encoding for the system's standard output. The specified command is executed on the temporary file, and the file is deleted afterward.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "45caffb1f38e",
        "ground_truth": "def _tempfilepager(generator, cmd, color):\n    \"\"\"Page through text by invoking a program on a temporary file.\"\"\"\n    import tempfile\n     filename = tempfile.mktemp()\n    # TODO: This never terminates if the passed generator never terminates.\n    text = \"\".join(generator)\n    if not color:\n        text = strip_ansi(text)\n    encoding = get_best_encoding(sys.stdout)\n    with open_stream(filename, \"wb\")[0] as f:\n        f.write(text.encode(encoding))\n    try:\n        os.system('{} \"{}\"'.format(cmd, filename))\n    finally:\n        os.unlink(filename)",
        "import_statements": [
            "import contextlib",
            "import math",
            "import os",
            "import sys",
            "import time"
        ],
        "reference_api": [
            "join",
            "f.write",
            "os.unlink",
            "get_best_encoding",
            "open_stream",
            "text.encode",
            "os.system",
            "strip_ansi",
            "tempfile.mktemp",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "join",
            "strip_ansi",
            "get_best_encoding",
            "open_stream",
            "f.write",
            "text.encode"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "Netflix/metaflow",
        "function_declaration": "def _bashcomplete(cmd, prog_name, complete_var=None)",
        "start_line": "62",
        "end_line": "73",
        "file_path": "metaflow/_vendor/click/core.py",
        "docstring": "The function _bashcomplete is an internal handler for bash completion support. It checks an environment variable, derived from the program name, to see if bash completion is enabled. If the variable is set, it calls a bashcomplete function with the command, program name, completion variable, and instructions. If bashcomplete returns True, the function exits quickly with a status code of 1.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "25acaca1b6e0",
        "ground_truth": "def _bashcomplete(cmd, prog_name, complete_var=None):\n    \"\"\"Internal handler for the bash completion support.\"\"\"\n    if complete_var is None:\n        complete_var = \"_{}_COMPLETE\".format(prog_name.replace(\"-\", \"_\").upper())\n    complete_instr = os.environ.get(complete_var)\n    if not complete_instr:\n        return\n     from ._bashcomplete import bashcomplete\n     if bashcomplete(cmd, prog_name, complete_var, complete_instr):\n        fast_exit(1)",
        "import_statements": [
            "import errno",
            "import inspect",
            "import os",
            "import sys",
            "from contextlib import contextmanager",
            "from functools import update_wrapper",
            "from itertools import repeat"
        ],
        "reference_api": [
            "get",
            "bashcomplete",
            "prog_name.replace",
            "fast_exit",
            "upper",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "fast_exit",
                "code": "def fast_exit(code):\n    \"\"\"Exit without garbage collection, this speeds up exit by about 10ms for\n    things like bash completion.\n    \"\"\"\n    sys.stdout.flush()\n    sys.stderr.flush()\n    os._exit(code)"
            }
        ],
        "third_party": [
            "upper",
            "prog_name.replace",
            "get",
            "bashcomplete"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "Netflix/metaflow",
        "function_declaration": "def write_usage(self, prog, args=\"\", prefix=\"Usage: \")",
        "start_line": "130",
        "end_line": "162",
        "file_path": "metaflow/_vendor/click/formatting.py",
        "docstring": "The function write_usage formats and writes the usage information for a program. It accepts the program name (prog), arguments (args), and an optional prefix (default is \"Usage: \"). It calculates the prefix width and text width based on current indentation. If the text width allows, it writes the usage prefix followed by arguments on the same line. Otherwise, it writes the usage prefix on one line and the arguments on the next line with proper indentation. The formatted text is then written using the write method and wrapped appropriately to fit within the specified width.\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "db0dc4014cc8",
        "ground_truth": "def write_usage(self, prog, args=\"\", prefix=\"Usage: \"):\n    \"\"\"Writes a usage line into the buffer.\n    :param prog: the program name.\n    :param args: whitespace separated list of arguments.\n    :param prefix: the prefix for the first line.\n    \"\"\"\n    usage_prefix = \"{:>{w}}{} \".format(prefix, prog, w=self.current_indent)\n    text_width = self.width - self.current_indent\n    if text_width >= (term_len(usage_prefix) + 20):\n        # The arguments will fit to the right of the prefix.\n        indent = \" \" * term_len(usage_prefix)\n        self.write(\n            wrap_text(\n                args,\n                text_width,\n                initial_indent=usage_prefix,\n                subsequent_indent=indent,\n            )\n        )\n    else:\n        # The prefix is too long, put the arguments on the next line.\n        self.write(usage_prefix)\n        self.write(\"\\n\")\n        indent = \" \" * (max(self.current_indent, term_len(prefix)) + 4)\n        self.write(\n            wrap_text(\n                args, text_width, initial_indent=indent, subsequent_indent=indent\n            )\n        )\n    self.write(\"\\n\")",
        "import_statements": [
            "from contextlib import contextmanager"
        ],
        "reference_api": [
            "wrap_text",
            "self.write",
            "term_len",
            "max",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.write",
                "code": "def write(self, string):\n        \"\"\"Writes a unicode string into the internal buffer.\"\"\"\n        self.buffer.append(string)"
            },
            {
                "name": "wrap_text",
                "code": "def wrap_text(\n    text, width=78, initial_indent=\"\", subsequent_indent=\"\", preserve_paragraphs=False\n):\n    \"\"\"A helper function that intelligently wraps text.  By default, it\n    assumes that it operates on a single paragraph of text but if the\n    `preserve_paragraphs` parameter is provided it will intelligently\n    handle paragraphs (defined by two empty lines).\n\n    If paragraphs are handled, a paragraph can be prefixed with an empty\n    line containing the ``\\\\b`` character (``\\\\x08``) to indicate that\n    no rewrapping should happen in that block.\n\n    :param text: the text that should be rewrapped.\n    :param width: the maximum width for the text.\n    :param initial_indent: the initial indent that should be placed on the\n                           first line as a string.\n    :param subsequent_indent: the indent string that should be placed on\n                              each consecutive line.\n    :param preserve_paragraphs: if this flag is set then the wrapping will\n                                intelligently handle paragraphs.\n    \"\"\"\n    from ._textwrap import TextWrapper\n\n    text = text.expandtabs()\n    wrapper = TextWrapper(\n        width,\n        initial_indent=initial_indent,\n        subsequent_indent=subsequent_indent,\n        replace_whitespace=False,\n    )\n    if not preserve_paragraphs:\n        return wrapper.fill(text)\n\n    p = []\n    buf = []\n    indent = None\n\n    def _flush_par():\n        if not buf:\n            return\n        if buf[0].strip() == \"\\b\":\n            p.append((indent or 0, True, \"\\n\".join(buf[1:])))\n        else:\n            p.append((indent or 0, False, \" \".join(buf)))\n        del buf[:]\n\n    for line in text.splitlines():\n        if not line:\n            _flush_par()\n            indent = None\n        else:\n            if indent is None:\n                orig_len = term_len(line)\n                line = line.lstrip()\n                indent = orig_len - term_len(line)\n            buf.append(line)\n    _flush_par()\n\n    rv = []\n    for indent, raw, text in p:\n        with wrapper.extra_indent(\" \" * indent):\n            if raw:\n                rv.append(wrapper.indent_only(text))\n            else:\n                rv.append(wrapper.fill(text))\n\n    return \"\\n\\n\".join(rv)"
            },
            {
                "name": "self.write",
                "code": "def write(self, string):\n        \"\"\"Writes a unicode string into the internal buffer.\"\"\"\n        self.buffer.append(string)"
            },
            {
                "name": "self.write",
                "code": "def write(self, string):\n        \"\"\"Writes a unicode string into the internal buffer.\"\"\"\n        self.buffer.append(string)"
            },
            {
                "name": "self.write",
                "code": "def write(self, string):\n        \"\"\"Writes a unicode string into the internal buffer.\"\"\"\n        self.buffer.append(string)"
            },
            {
                "name": "wrap_text",
                "code": "def wrap_text(\n    text, width=78, initial_indent=\"\", subsequent_indent=\"\", preserve_paragraphs=False\n):\n    \"\"\"A helper function that intelligently wraps text.  By default, it\n    assumes that it operates on a single paragraph of text but if the\n    `preserve_paragraphs` parameter is provided it will intelligently\n    handle paragraphs (defined by two empty lines).\n\n    If paragraphs are handled, a paragraph can be prefixed with an empty\n    line containing the ``\\\\b`` character (``\\\\x08``) to indicate that\n    no rewrapping should happen in that block.\n\n    :param text: the text that should be rewrapped.\n    :param width: the maximum width for the text.\n    :param initial_indent: the initial indent that should be placed on the\n                           first line as a string.\n    :param subsequent_indent: the indent string that should be placed on\n                              each consecutive line.\n    :param preserve_paragraphs: if this flag is set then the wrapping will\n                                intelligently handle paragraphs.\n    \"\"\"\n    from ._textwrap import TextWrapper\n\n    text = text.expandtabs()\n    wrapper = TextWrapper(\n        width,\n        initial_indent=initial_indent,\n        subsequent_indent=subsequent_indent,\n        replace_whitespace=False,\n    )\n    if not preserve_paragraphs:\n        return wrapper.fill(text)\n\n    p = []\n    buf = []\n    indent = None\n\n    def _flush_par():\n        if not buf:\n            return\n        if buf[0].strip() == \"\\b\":\n            p.append((indent or 0, True, \"\\n\".join(buf[1:])))\n        else:\n            p.append((indent or 0, False, \" \".join(buf)))\n        del buf[:]\n\n    for line in text.splitlines():\n        if not line:\n            _flush_par()\n            indent = None\n        else:\n            if indent is None:\n                orig_len = term_len(line)\n                line = line.lstrip()\n                indent = orig_len - term_len(line)\n            buf.append(line)\n    _flush_par()\n\n    rv = []\n    for indent, raw, text in p:\n        with wrapper.extra_indent(\" \" * indent):\n            if raw:\n                rv.append(wrapper.indent_only(text))\n            else:\n                rv.append(wrapper.fill(text))\n\n    return \"\\n\\n\".join(rv)"
            },
            {
                "name": "self.write",
                "code": "def write(self, string):\n        \"\"\"Writes a unicode string into the internal buffer.\"\"\"\n        self.buffer.append(string)"
            }
        ],
        "third_party": [
            "term_len",
            "term_len",
            "term_len"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "microsoft/azuredatastudio",
        "function_declaration": "function findNodeModulesFiles(location, inNodeModules, result)",
        "start_line": "14",
        "end_line": "37",
        "file_path": "build/azure-pipelines/common/listNodeModules.js",
        "docstring": "The function findNodeModulesFiles recursively searches for files within the 'node_modules' directories starting from a specified location. It reads directory entries, skips specific directories ('out', 'src', '.git', '.build'), and checks if each entry is a directory or file. If an entry is a directory, the function calls itself recursively. If an entry is a file within a 'node_modules' directory, it adds the file path to the result list. Errors during the stat check are caught and ignored, allowing the function to continue processing other entries.",
        "language": "JavaScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bc176443892f",
        "ground_truth": "function findNodeModulesFiles(location, inNodeModules, result) {\n    const entries = fs.readdirSync(path.join(ROOT, location));\n    for (const entry of entries) {\n        const entryPath = `${location}/${entry}`;\n        if (/(^\\/out)|(^\\/src$)|(^\\/.git$)|(^\\/.build$)/.test(entryPath)) {\n            continue;\n        }\n        let stat;\n        try {\n            stat = fs.statSync(path.join(ROOT, entryPath));\n        }\n        catch (err) {\n            continue;\n        }\n        if (stat.isDirectory()) {\n            findNodeModulesFiles(entryPath, inNodeModules || (entry === 'node_modules'), result);\n        }\n        else {\n            if (inNodeModules) {\n                result.push(entryPath.substr(1));\n            }\n        }\n    }\n}",
        "import_statements": [],
        "reference_api": [
            "stat.isDirectory",
            "result.push",
            "fs.readdirSync",
            "fs.statSync",
            "/(^\\/out)|(^\\/src$)|(^\\/.git$)|(^\\/.build$)/.test",
            "entryPath.substr",
            "findNodeModulesFiles",
            "path.join"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "findNodeModulesFiles",
                "code": "function findNodeModulesFiles(location, inNodeModules, result) {\n    const entries = fs.readdirSync(path.join(ROOT, location));\n    for (const entry of entries) {\n        const entryPath = `${location}/${entry}`;\n        if (/(^\\/out)|(^\\/src$)|(^\\/.git$)|(^\\/.build$)/.test(entryPath)) {\n            continue;\n        }\n        let stat;\n        try {\n            stat = fs.statSync(path.join(ROOT, entryPath));\n        }\n        catch (err) {\n            continue;\n        }\n        if (stat.isDirectory()) {\n            findNodeModulesFiles(entryPath, inNodeModules || (entry === 'node_modules'), result);\n        }\n        else {\n            if (inNodeModules) {\n                result.push(entryPath.substr(1));\n            }\n        }\n    }\n}"
            }
        ],
        "third_party": [
            "stat.isDirectory",
            "result.push",
            "fs.readdirSync",
            "fs.statSync",
            "/(^\\/out)|(^\\/src$)|(^\\/.git$)|(^\\/.build$)/.test",
            "entryPath.substr",
            "path.join"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "node-formidable/formidable",
        "function_declaration": "const decorateForceSequential = function (promiseCreator)",
        "start_line": "48",
        "end_line": "66",
        "file_path": "src/Formidable.js",
        "docstring": "The decorateForceSequential function ensures that a given function returning a promise executes sequentially. It maintains a 'lastPromise' variable to track the last promise created. When the returned function is called, it creates a new promise ('lastPromise') before awaiting the previous one, ensuring sequential execution. It then calls the original promise-creating function and resolves or rejects 'lastPromise' based on the outcome of the current promise. This mechanism guarantees that each call waits for the previous one to complete before starting.\n",
        "language": "JavaScript",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c69bc58cc41e",
        "ground_truth": "const decorateForceSequential = function (promiseCreator) {\n  /* forces a function that returns a promise to be sequential\n  useful for fs  for example */\n  let lastPromise = Promise.resolve();\n  return async function (...x) {\n      const promiseWeAreWaitingFor = lastPromise;\n      let currentPromise;\n      let callback;\n      // we need to change lastPromise before await anything,\n      // otherwise 2 calls might wait the same thing\n      lastPromise = new Promise(function (resolve) {\n          callback = resolve;\n      });\n      await promiseWeAreWaitingFor;\n      currentPromise = promiseCreator(...x);\n      currentPromise.then(callback).catch(callback);\n      return currentPromise;\n  };\n};",
        "import_statements": [
            "node:os",
            "node:path",
            "node:fs/promises",
            "node:events",
            "node:string_decoder",
            "hexoid",
            "once",
            "dezalgo",
            "./plugins/index.js",
            "./PersistentFile.js",
            "./VolatileFile.js",
            "./parsers/Dummy.js",
            "./parsers/Multipart.js",
            "./FormidableError.js",
            "./FormidableError.js"
        ],
        "reference_api": [
            "currentPromise.then",
            "Promise.resolve",
            "promiseCreator",
            "currentPromise.then(callback).catch"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "currentPromise.then",
            "promiseCreator",
            "currentPromise.then(callback).catch"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "bridgecrewio/checkov",
        "function_declaration": "def extract_images_from_azurerm_batch_pool(resource: dict[str, Any]) -> list[str]:",
        "start_line": "21",
        "end_line": "31",
        "file_path": "checkov/bicep/image_referencer/provider/azure.py",
        "docstring": "The extract_images_from_azurerm_batch_pool function extracts image names from an Azure Batch pool resource. It initializes an empty list 'image_names' and retrieves the container image names from the resource's dictionary using the specified key path. If the retrieved containers are a list, it extends 'image_names' with the container names, ensuring each is a string. The function then returns the list of image names.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "498bf7ab47f4",
        "ground_truth": "def extract_images_from_azurerm_batch_pool(resource: dict[str, Any]) -> list[str]:\n    image_names: list[str] = []\n     containers = find_in_dict(\n        input_dict=resource,\n        key_path=\"properties/virtualMachineConfiguration/containerConfiguration/containerImageNames\",\n    )\n    if isinstance(containers, list):\n        image_names.extend(container for container in containers if isinstance(container, str))\n     return image_names",
        "import_statements": [
            "from typing import TYPE_CHECKING, Any",
            "from checkov.bicep.image_referencer.base_provider import BaseBicepProvider",
            "from checkov.common.util.data_structures_utils import find_in_dict",
            "from checkov.common.util.type_forcers import force_list"
        ],
        "reference_api": [
            "find_in_dict",
            "image_names.extend",
            "isinstance"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "find_in_dict",
                "code": "def find_in_dict(input_dict: dict[str, Any], key_path: str) -> Any:\n    \"\"\"Tries to retrieve the value under the given 'key_path', otherwise returns None.\"\"\"\n\n    value: Any = input_dict\n    key_list = key_path.split(\"/\")\n\n    try:\n        for key in key_list:\n            if key.startswith(\"[\") and key.endswith(\"]\"):\n                if isinstance(value, list):\n                    idx = int(key[1:-1])\n                    value = value[idx]\n                    continue\n                else:\n                    return None\n\n            value = value.get(key)\n            if value is None:\n                return None\n    except (AttributeError, IndexError, KeyError, TypeError, ValueError):\n        logging.debug(f\"Could not find {key_path} in dict\")\n        return None\n\n    return value"
            }
        ],
        "third_party": [
            "image_names.extend"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "bridgecrewio/checkov",
        "function_declaration": "def extract_images_from_azurerm_web_app(resource: dict[str, Any]) -> list[str]",
        "start_line": "55",
        "end_line": "65",
        "file_path": "checkov/bicep/image_referencer/provider/azure.py",
        "docstring": "The extract_images_from_azurerm_web_app function retrieves a list of image names from an Azure Resource Manager (ARM) web app resource dictionary. It searches for containers within the resource's properties template. If containers are found, it iterates through them, extracting the image name from each container and adding it to the image_names list. The function ensures the image names are strings before appending them to the list. The final list of image names is then returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1c50195102cd",
        "ground_truth": "def extract_images_from_azurerm_web_app(resource: dict[str, Any]) -> list[str]:\n    image_names: list[str] = []\n     containers = find_in_dict(input_dict=resource, key_path=\"properties/template/containers\")\n    if containers:\n        for container in force_list(containers):\n            name = container.get(\"image\")\n            if name and isinstance(name, str):\n                image_names.append(name)\n     return image_names",
        "import_statements": [
            "from typing import TYPE_CHECKING, Any",
            "from checkov.bicep.image_referencer.base_provider import BaseBicepProvider",
            "from checkov.common.util.data_structures_utils import find_in_dict",
            "from checkov.common.util.type_forcers import force_list"
        ],
        "reference_api": [
            "container.get",
            "force_list",
            "isinstance",
            "image_names.append",
            "find_in_dict"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "find_in_dict",
                "code": "def find_in_dict(input_dict: dict[str, Any], key_path: str) -> Any:\n    \"\"\"Tries to retrieve the value under the given 'key_path', otherwise returns None.\"\"\"\n\n    value: Any = input_dict\n    key_list = key_path.split(\"/\")\n\n    try:\n        for key in key_list:\n            if key.startswith(\"[\") and key.endswith(\"]\"):\n                if isinstance(value, list):\n                    idx = int(key[1:-1])\n                    value = value[idx]\n                    continue\n                else:\n                    return None\n\n            value = value.get(key)\n            if value is None:\n                return None\n    except (AttributeError, IndexError, KeyError, TypeError, ValueError):\n        logging.debug(f\"Could not find {key_path} in dict\")\n        return None\n\n    return value"
            },
            {
                "name": "force_list",
                "code": "def force_list(var: T | list[T]) -> list[T]:\n    if not isinstance(var, list):\n        return [var]\n    return var"
            }
        ],
        "third_party": [
            "container.get",
            "image_names.append"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "bridgecrewio/checkov",
        "function_declaration": "def scan_resource_conf(self, conf: dict[str, list[Any]]) -> CheckResult",
        "start_line": "24",
        "end_line": "34",
        "file_path": "checkov/terraform/checks/resource/azure/AzureSearchSLAIndex.py",
        "docstring": "The function scan_resource_conf evaluates the configuration of a resource to determine its compliance. It first notes that it evaluates the \"replica_count\" key. It retrieves the \"replica_count\" value from the configuration, which is expected to be a list. If the list's first element is not an integer, it returns CheckResult.UNKNOWN. If the integer value is 3 or more, it returns CheckResult.PASSED. Otherwise, it returns CheckResult.FAILED.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "31c119b0bb52",
        "ground_truth": "def scan_resource_conf(self, conf: dict[str, list[Any]]) -> CheckResult:\n    self.evaluated_keys = [\"replica_count\"]\n    replica_count = conf.get(\"replica_count\")\n    if replica_count and isinstance(replica_count, list):\n        if not isinstance(replica_count[0], int):\n            return CheckResult.UNKNOWN\n        if replica_count[0] >= 3:\n            return CheckResult.PASSED\n    return CheckResult.FAILED",
        "import_statements": [
            "from typing import Any",
            "from checkov.common.models.enums import CheckCategories, CheckResult",
            "from checkov.terraform.checks.resource.base_resource_check import BaseResourceCheck"
        ],
        "reference_api": [
            "isinstance",
            "conf.get"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "conf.get"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "bridgecrewio/checkov",
        "function_declaration": "def _create_block_vertices(self, file_path: str, block: dict[str, Any], prefix: str = \"\") -> None",
        "start_line": "113",
        "end_line": "134",
        "file_path": "checkov/ansible/graph_builder/local_graph.py",
        "docstring": "The _create_block_vertices function creates vertices for a given block in a file. It extracts the block name or assigns \"unknown\" if it doesn't exist. The block's configuration and attributes are copied and modified to include the block's resource type, excluding the block content itself. A new Block object is created with the block's name, configuration, file path, block type, attributes, an ID prefixed with the provided string, and the source. This Block object is then appended to the vertices list.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2b2640b5cb5a",
        "ground_truth": "def _create_block_vertices(self, file_path: str, block: dict[str, Any], prefix: str = \"\") -> None:\n    \"\"\"Creates block vertices\"\"\"\n    # grab the block name, if it exists\n    block_name = block.get(\"name\") or \"unknown\"\n    config = block\n    attributes = pickle_deepcopy(config)\n    attributes[CustomAttributes.RESOURCE_TYPE] = ResourceType.BLOCK\n    del attributes[ResourceType.BLOCK]  # the real block content are tasks, which have their own vertices\n    self.vertices.append(\n        Block(\n            name=f\"{ResourceType.BLOCK}.{block_name}\",\n            config=config,\n            path=file_path,\n            block_type=BlockType.RESOURCE,\n            attributes=attributes,\n            id=f\"{prefix}{block_name}\",\n            source=self.source,\n        )\n    )",
        "import_statements": [
            "import logging",
            "from pathlib import Path",
            "from typing import Any",
            "from checkov.common.graph.graph_builder import CustomAttributes",
            "from checkov.common.graph.graph_builder.consts import GraphSource, SELF_REFERENCE",
            "from checkov.common.graph.graph_builder.graph_components.block_types import BlockType",
            "from checkov.common.graph.graph_builder.graph_components.blocks import Block",
            "from checkov.common.runners.graph_builder.local_graph import ObjectLocalGraph",
            "from checkov.common.util.consts import START_LINE, END_LINE",
            "from checkov.ansible.graph_builder.graph_components.resource_types import ResourceType",
            "from checkov.ansible.utils import get_scannable_file_paths, TASK_RESERVED_KEYWORDS, parse_file",
            "from checkov.common.util.data_structures_utils import pickle_deepcopy"
        ],
        "reference_api": [
            "append",
            "Block",
            "pickle_deepcopy",
            "block.get"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "pickle_deepcopy",
                "code": "def pickle_deepcopy(obj: _T) -> _T:\n    \"\"\"More performant version of the built-in deepcopy\"\"\"\n\n    return cast(\"_T\", pickle.loads(pickle.dumps(obj, pickle.HIGHEST_PROTOCOL)))  # nosec"
            }
        ],
        "third_party": [
            "block.get",
            "append",
            "Block"
        ]
    },
    {
        "subclass": "azure",
        "owner/repo": "bridgecrewio/checkov",
        "function_declaration": "def _create_vertices(self) -> None",
        "start_line": "24",
        "end_line": "37",
        "file_path": "checkov/ansible/graph_builder/local_graph.py",
        "docstring": "The `_create_vertices` function processes definitions to create vertices. It iterates through items in the `self.definitions` dictionary. If an item is not a list, it logs a debug message and continues to the next item. For each code block in the list, it checks if the code block contains tasks. If tasks are present, it processes each task using `_process_blocks`. If tasks are not present, it processes the entire code block using `_process_blocks`.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "39e1df8f3d80",
        "ground_truth": "def _create_vertices(self) -> None:\n    for file_path, definition in self.definitions.items():\n        if not isinstance(definition, list):\n            logging.debug(f\"definition of file {file_path} has the wrong type {type(definition)}\")\n            continue\n        file_path = str(file_path)\n        for code_block in definition:\n            if ResourceType.TASKS in code_block:\n                for task in code_block[ResourceType.TASKS]:\n                    self._process_blocks(file_path=file_path, task=task)\n            else:\n                self._process_blocks(file_path=file_path, task=code_block)",
        "import_statements": [
            "import logging",
            "from pathlib import Path",
            "from typing import Any",
            "from checkov.common.graph.graph_builder import CustomAttributes",
            "from checkov.common.graph.graph_builder.consts import GraphSource, SELF_REFERENCE",
            "from checkov.common.graph.graph_builder.graph_components.block_types import BlockType",
            "from checkov.common.graph.graph_builder.graph_components.blocks import Block",
            "from checkov.common.runners.graph_builder.local_graph import ObjectLocalGraph",
            "from checkov.common.util.consts import START_LINE, END_LINE",
            "from checkov.ansible.graph_builder.graph_components.resource_types import ResourceType",
            "from checkov.ansible.utils import get_scannable_file_paths, TASK_RESERVED_KEYWORDS, parse_file",
            "from checkov.common.util.data_structures_utils import pickle_deepcopy"
        ],
        "reference_api": [
            "self._process_blocks",
            "type",
            "isinstance",
            "logging.debug",
            "items",
            "str"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._process_blocks",
                "code": "def _process_blocks(self, file_path: str, task: Any, prefix: str = \"\") -> None:\n        \"\"\"Checks for possible block usage\"\"\"\n\n        if not task or not isinstance(task, dict):\n            return\n\n        if ResourceType.BLOCK in task and isinstance(task[ResourceType.BLOCK], list):\n            prefix += f\"{ResourceType.BLOCK}.\"  # with each nested level an extra block prefix is added\n            self._create_block_vertices(file_path=file_path, block=task, prefix=prefix)\n\n            for block_task in task[ResourceType.BLOCK]:\n                self._process_blocks(file_path=file_path, task=block_task, prefix=prefix)\n        else:\n            self._create_tasks_vertices(file_path=file_path, task=task, prefix=prefix)"
            },
            {
                "name": "self._process_blocks",
                "code": "def _process_blocks(self, file_path: str, task: Any, prefix: str = \"\") -> None:\n        \"\"\"Checks for possible block usage\"\"\"\n\n        if not task or not isinstance(task, dict):\n            return\n\n        if ResourceType.BLOCK in task and isinstance(task[ResourceType.BLOCK], list):\n            prefix += f\"{ResourceType.BLOCK}.\"  # with each nested level an extra block prefix is added\n            self._create_block_vertices(file_path=file_path, block=task, prefix=prefix)\n\n            for block_task in task[ResourceType.BLOCK]:\n                self._process_blocks(file_path=file_path, task=block_task, prefix=prefix)\n        else:\n            self._create_tasks_vertices(file_path=file_path, task=task, prefix=prefix)"
            }
        ],
        "third_party": [
            "items"
        ]
    }
]