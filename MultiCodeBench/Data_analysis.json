[
    {
        "subclass": "scikit-learn",
        "owner/repo": "adap/flower",
        "file_path": "examples/quickstart-sklearn-tabular/utils.py",
        "function_declaration": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression",
        "start_line": "23",
        "end_line": "28",
        "docstring": "# This function sets the parameters of a scikit-learn LogisticRegression model.\\nIt assigns the model's coefficients from the first element of the provided parameters.\\nIf the model is configured to fit an intercept, it also assigns the intercept from the second element of the parameters.\\nThe function returns the updated LogisticRegression model.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "58936b2a621b",
        "ground_truth": "def set_model_params(model: LogisticRegression, params: NDArrays) -> LogisticRegression:\n    \"\"\"Set the parameters of a sklean LogisticRegression model.\"\"\"\n    model.coef_ = params[0]\n    if model.fit_intercept:\n        model.intercept_ = params[1]\n    return model",
        "import_statements": [
            "from typing import List, Tuple, Dict",
            "from sklearn.linear_model import LogisticRegression",
            "from flwr.common import NDArrays, Metrics, Scalar"
        ],
        "reference_api": [],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "adap/flower",
        "file_path": "baselines/fedmeta/fedmeta/dataset_preparation.py",
        "function_declaration": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]",
        "start_line": "49",
        "end_line": "75",
        "docstring": "# This function splits data and labels into training and testing sets based on a specified support ratio.\\nIt uses the train_test_split function with stratification to ensure the label distribution is preserved across both sets.\\nA random state of 42 is used for reproducibility.\\nThe function returns four lists: x_train, x_test, y_train, and y_test.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "83b142d9d251",
        "ground_truth": "def support_query_split(\n    data,\n    label,\n    support_ratio: float,\n) -> Tuple[List, List, List, List]:\n    \"\"\"Separate support set and query set.\n     Parameters\n    ----------\n        data: DefaultDict,\n            Raw all Datasets\n        label: List,\n            Raw all Labels\n        support_ratio : float\n            The ratio of Support set for each client.(between 0 and 1)\n            by default 0.2\n     Returns\n    -------\n    Tuple[List, List, List, List]\n        Support set and query set classification of data and labels\n    \"\"\"\n    x_train, x_test, y_train, y_test = train_test_split(\n        data, label, train_size=support_ratio, stratify=label, random_state=42\n    )\n     return x_train, x_test, y_train, y_test",
        "import_statements": [
            "import json",
            "import os",
            "from collections import defaultdict",
            "from typing import Any, DefaultDict, Dict, List, Tuple",
            "from sklearn.model_selection import train_test_split"
        ],
        "reference_api": [
            "train_test_split"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "train_test_split"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "adap/flower",
        "file_path": "examples/quickstart-jax/jax_training.py",
        "function_declaration": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n)",
        "start_line": "20",
        "end_line": "26",
        "docstring": "# This function loads a regression dataset and splits it into training and testing sets.\\nIt generates the dataset using make_regression with 3 features and a fixed random state for reproducibility.\\nThe data is then split into training and testing sets using train_test_split.\\nThe function returns four lists: X (training data), y (training labels), X_test (testing data), and y_test (testing labels).",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5fd02b540f61",
        "ground_truth": "def load_data() -> (\n    Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray]]\n):\n    # Load dataset\n    X, y = make_regression(n_features=3, random_state=0)\n    X, X_test, y, y_test = train_test_split(X, y)\n    return X, y, X_test, y_test",
        "import_statements": [
            "from typing import Dict, List, Tuple, Callable",
            "import jax",
            "from sklearn.datasets import make_regression",
            "from sklearn.model_selection import train_test_split"
        ],
        "reference_api": [
            "make_regression",
            "train_test_split"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "make_regression",
            "train_test_split"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "adap/flower",
        "file_path": "baselines/fedpft/fedpft/utils.py",
        "function_declaration": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters",
        "start_line": "27",
        "end_line": "35",
        "docstring": "# This function converts a NumPy ndarray to a GMMParameters object.\\nIt takes an array of ndarrays as input and maps its elements to the corresponding attributes of the GMMParameters class.\\nThe attributes include label, means, weights, covariances, and num_samples.\\nThe function returns the constructed GMMParameters object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ad958cb31a57",
        "ground_truth": "def ndarrays_to_gmmparam(ndarrays: NDArray) -> GMMParameters:\n    \"\"\"Convert NumPy ndarray to GMM object.\"\"\"\n    return GMMParameters(\n        label=ndarrays[0],\n        means=ndarrays[1],\n        weights=ndarrays[2],\n        covariances=ndarrays[3],\n        num_samples=ndarrays[4],\n    )",
        "import_statements": [
            "from dataclasses import dataclass",
            "from typing import List",
            "from numpy.typing import NDArray",
            "from sklearn.mixture import GaussianMixture"
        ],
        "reference_api": [
            "GMMParameters"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "GMMParameters"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "adap/flower",
        "file_path": "baselines/hfedxgboost/hfedxgboost/dataset_preparation.py",
        "function_declaration": "def datafiles_fusion(data_paths):",
        "start_line": "174",
        "end_line": "196",
        "docstring": "# This function fuses multiple data files into a single dataset.\\nIt begins by loading the first data file using load_svmlight_file, converting the features to a dense array and storing the labels.\\nFor each subsequent data file, it loads the data ensuring the number of features matches the first file, then concatenates the new features and labels to the existing arrays.\\nThe function returns the combined feature array X and label array Y.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "061d5a5b9d56",
        "ground_truth": "def datafiles_fusion(data_paths):\n    \"\"\"Merge (if necessary) the data files and returns the features and labels.\n     Parmetres:\n        data_paths: List[Dataset Pathes]\n            - The pathes for the data that will be used in train and test,\n            with train of full dataset in index 0\n    Returns:\n        X: Numpy array\n            - The full features of the dataset.\n        y: Numpy array\n            - The full labels of the dataset.\n    \"\"\"\n    data = load_svmlight_file(data_paths[0], zero_based=False)\n    X = data[0].toarray()\n    Y = data[1]\n    for i in range(1, len(data_paths)):\n        data = load_svmlight_file(\n            data_paths[i], zero_based=False, n_features=X.shape[1]\n        )\n        X = np.concatenate((X, data[0].toarray()), axis=0)\n        Y = np.concatenate((Y, data[1]), axis=0)\n    return X, Y",
        "import_statements": [
            "import bz2",
            "import os",
            "import shutil",
            "import urllib.request",
            "from typing import Optional",
            "from sklearn.datasets import load_svmlight_file"
        ],
        "reference_api": [
            "toarray",
            "len",
            "load_svmlight_file",
            "np.concatenate",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "load_svmlight_file",
            "toarray",
            "load_svmlight_file",
            "np.concatenate",
            "toarray",
            "np.concatenate"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "adap/flower",
        "file_path": "examples/custom-metrics/client.py",
        "function_declaration": "def eval_learning(y_test, y_pred)",
        "start_line": "31",
        "end_line": "38",
        "docstring": "# This function evaluates the performance of a learning model by calculating four metrics: accuracy, recall, precision, and F1 score.\\nIt takes the true labels (y_test) and the predicted labels (y_pred) as input.\\nThe recall, precision, and F1 score are calculated with the \"micro\" average to handle multi-class classification.\\nThe function returns the calculated accuracy, recall, precision, and F1 score.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "977d3bd5d5a3",
        "ground_truth": "def eval_learning(y_test, y_pred):\n    acc = accuracy_score(y_test, y_pred)\n    rec = recall_score(\n        y_test, y_pred, average=\"micro\"\n    )  # average argument required for multi-class\n    prec = precision_score(y_test, y_pred, average=\"micro\")\n    f1 = f1_score(y_test, y_pred, average=\"micro\")\n    return acc, rec, prec, f1",
        "import_statements": [
            "import os",
            "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score",
            "from flwr_datasets import FederatedDataset"
        ],
        "reference_api": [
            "precision_score",
            "recall_score",
            "accuracy_score",
            "f1_score"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "accuracy_score",
            "recall_score",
            "precision_score",
            "f1_score"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "adap/flower",
        "file_path": "examples/fl-tabular/task.py",
        "function_declaration": "def train(model, train_loader, num_epochs=1)",
        "start_line": "68",
        "end_line": "78",
        "docstring": "# This function trains a given model using a specified data loader for a set number of epochs.\\nIt uses the binary cross-entropy loss function and the Adam optimizer with a learning rate of 0.001.\\nThe model is set to training mode, and for each epoch, it iterates over batches of data from the train_loader.\\nFor each batch, it performs a forward pass to compute the outputs, calculates the loss, performs backpropagation, and updates the model parameters using the optimizer.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f4c838f1f092",
        "ground_truth": "def train(model, train_loader, num_epochs=1):\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()",
        "import_statements": [
            "import torch",
            "from torch.utils.data import TensorDataset, DataLoader",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.preprocessing import StandardScaler, OrdinalEncoder",
            "from sklearn.compose import ColumnTransformer",
            "from sklearn.pipeline import Pipeline",
            "from collections import OrderedDict",
            "from flwr_datasets import FederatedDataset"
        ],
        "reference_api": [
            "loss.backward",
            "nn.BCELoss",
            "optim.Adam",
            "optimizer.step",
            "model.parameters",
            "optimizer.zero_grad",
            "model",
            "model.train",
            "criterion",
            "range"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "model.train",
                "code": "def train(model, train_loader, num_epochs=1):\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    model.train()\n    for epoch in range(num_epochs):\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()"
            }
        ],
        "third_party": [
            "nn.BCELoss",
            "optim.Adam",
            "model.parameters",
            "optimizer.zero_grad",
            "model",
            "criterion",
            "loss.backward",
            "optimizer.step"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "adap/flower",
        "file_path": "baselines/dasha/dasha/dataset.py",
        "function_declaration": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset",
        "start_line": "23",
        "end_line": "42",
        "docstring": "# This function loads a LIBSVM dataset based on the configuration provided in cfg.\\nIt first ensures that the dataset type is LIBSVM and retrieves the path and name of the dataset from the configuration.\\nThe data and labels are loaded using the load_svmlight_file function and converted to a dense array of type float32.\\nThe function prints the unique labels with their counts and the shape of the features.\\nFor the MUSHROOMS dataset, labels are remapped so that label 1 becomes 0 and all other labels become 1.\\nIf the dataset name is not MUSHROOMS, a RuntimeError is raised.\\nFinally, the function creates and returns a TensorDataset containing the data and labels.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "35068c779c02",
        "ground_truth": "def _load_libsvm_dataset(cfg: DictConfig) -> Dataset:\n    assert cfg.dataset.type == DatasetType.LIBSVM.value\n    path_to_dataset = cfg.dataset.path_to_dataset\n    dataset_name = cfg.dataset.dataset_name\n    data, labels = load_svmlight_file(  # pylint: disable=unbalanced-tuple-unpacking\n        train_dataset_path(path_to_dataset, dataset_name)\n    )\n    data = data.toarray().astype(np.float32)\n    print(\"Original labels: {}\".format(np.unique(labels, return_counts=True)))\n    print(\"Features Shape: {}\".format(data.shape))\n    if dataset_name == LIBSVMDatasetName.MUSHROOMS.value:\n        labels = labels.astype(np.int64)\n        remap_labels = np.zeros_like(labels)\n        remap_labels[labels == 1] = 0\n        remap_labels[labels != 1] = 1\n        labels = remap_labels\n    else:\n        raise RuntimeError(\"Wrong dataset\")\n    dataset = data_utils.TensorDataset(torch.Tensor(data), torch.Tensor(labels))\n    return dataset",
        "import_statements": [
            "from enum import Enum",
            "import torch",
            "import torchvision",
            "from omegaconf import DictConfig",
            "from sklearn.datasets import load_svmlight_file",
            "from torch.utils.data import Dataset",
            "from dasha.dataset_preparation import DatasetType, train_dataset_path"
        ],
        "reference_api": [
            "RuntimeError",
            "train_dataset_path",
            "print",
            "np.zeros_like",
            "data_utils.TensorDataset",
            "labels.astype",
            "data.toarray",
            "load_svmlight_file",
            "np.unique",
            "astype",
            "torch.Tensor",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "load_svmlight_file",
            "train_dataset_path",
            "astype",
            "data.toarray",
            "np.unique",
            "labels.astype",
            "np.zeros_like",
            "data_utils.TensorDataset",
            "torch.Tensor",
            "torch.Tensor"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "ageron/handson-ml",
        "file_path": "future_encoders.py",
        "function_declaration": "def _fit_transform_one(transformer, X, y, weight, **fit_params)",
        "start_line": "55",
        "end_line": "63",
        "docstring": "# This function applies a transformation to data using a specified transformer.\\nIt first checks if the transformer has a fit_transform method and uses it to fit and transform the data X and y with optional fit parameters.\\nIf fit_transform is not available, it separately fits the transformer and then transforms the data.\\nIf a weight is provided, the transformed result is multiplied by this weight.\\nThe function returns the transformed data and the fitted transformer.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d602a96e571e",
        "ground_truth": "def _fit_transform_one(transformer, X, y, weight, **fit_params):\n    if hasattr(transformer, 'fit_transform'):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer",
        "import_statements": [
            "import numbers",
            "import warnings",
            "from scipy import sparse",
            "from sklearn.base import clone, BaseEstimator, TransformerMixin",
            "from sklearn.externals import six",
            "from sklearn.utils import Bunch, check_array",
            "from sklearn.externals.joblib.parallel import delayed, Parallel",
            "from sklearn.utils.metaestimators import _BaseComposition",
            "from sklearn.utils.validation import check_is_fitted, FLOAT_DTYPES",
            "from sklearn.pipeline import _name_estimators",
            "from sklearn.preprocessing import FunctionTransformer",
            "from sklearn.preprocessing.label import LabelEncoder",
            "from itertools import chain"
        ],
        "reference_api": [
            "hasattr",
            "transformer.fit",
            "transform",
            "transformer.fit_transform"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "transformer.fit_transform",
                "code": "def fit_transform(self, X, y=None):\n        \"\"\"Fit OneHotEncoder to X, then transform X.\n\n        Equivalent to self.fit(X).transform(X), but more convenient and more\n        efficient. See fit for the parameters, transform for the return value.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            Input array of type int.\n        \"\"\"\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        self._handle_deprecations(X)\n\n        if self._legacy_mode:\n            return _transform_selected(X, self._legacy_fit_transform,\n                                       self._deprecated_categorical_features,\n                                       copy=True)\n        else:\n            return self.fit(X).transform(X)"
            },
            {
                "name": "transform",
                "code": "def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array\n            Transformed input.\n        \"\"\"\n        if not self._legacy_mode:\n            return self._transform_new(X)\n        else:\n            return _transform_selected(X, self._legacy_transform,\n                                       self._deprecated_categorical_features,\n                                       copy=True)"
            },
            {
                "name": "transformer.fit",
                "code": "def fit(self, X, y=None):\n        \"\"\"Fit OneHotEncoder to X.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n\n        Returns\n        -------\n        self\n        \"\"\"\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        self._handle_deprecations(X)\n\n        if self._legacy_mode:\n            # TODO not with _transform_selected ??\n            self._legacy_fit_transform(X)\n            return self\n        else:\n            self._fit(X, handle_unknown=self.handle_unknown)\n            return self"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "ageron/handson-ml",
        "file_path": "future_encoders.py",
        "function_declaration": "def _transform_selected(X, transform, selected=\"all\", copy=True)",
        "start_line": "103",
        "end_line": "152",
        "docstring": "# This function applies a transformation to selected features of an array X.\\nIt first checks and ensures that X is a valid array, accepting sparse 'csc' format if specified.\\nIf all features are to be transformed, it applies the transformation to the entire array.\\nIf no features are selected, it returns X unchanged.\\nFor specific selected features, it identifies the selected and non-selected features, applies the transformation to the selected features, and then combines the transformed and non-transformed features.\\nThe function handles both sparse and dense arrays, returning the combined result appropriately.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fcf88a773fd8",
        "ground_truth": "def _transform_selected(X, transform, selected=\"all\", copy=True):\n    \"\"\"Apply a transform function to portion of selected features\n     Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        Dense array or sparse matrix.\n     transform : callable\n        A callable transform(X) -> X_transformed\n     copy : boolean, optional\n        Copy X even if it could be avoided.\n     selected: \"all\" or array of indices or mask\n        Specify which features to apply the transform to.\n     Returns\n    -------\n    X : array or sparse matrix, shape=(n_samples, n_features_new)\n    \"\"\"\n    X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)\n     if isinstance(selected, six.string_types) and selected == \"all\":\n        return transform(X)\n     if len(selected) == 0:\n        return X\n     n_features = X.shape[1]\n    ind = np.arange(n_features)\n    sel = np.zeros(n_features, dtype=bool)\n    sel[np.asarray(selected)] = True\n    not_sel = np.logical_not(sel)\n    n_selected = np.sum(sel)\n     if n_selected == 0:\n        # No features selected.\n        return X\n    elif n_selected == n_features:\n        # All features selected.\n        return transform(X)\n    else:\n        X_sel = transform(X[:, ind[sel]])\n        X_not_sel = X[:, ind[not_sel]]\n         if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):\n            return sparse.hstack((X_sel, X_not_sel))\n        else:\n            return np.hstack((X_sel, X_not_sel))",
        "import_statements": [
            "import numbers",
            "import warnings",
            "from scipy import sparse",
            "from sklearn.base import clone, BaseEstimator, TransformerMixin",
            "from sklearn.externals import six",
            "from sklearn.utils import Bunch, check_array",
            "from sklearn.externals.joblib.parallel import delayed, Parallel",
            "from sklearn.utils.metaestimators import _BaseComposition",
            "from sklearn.utils.validation import check_is_fitted, FLOAT_DTYPES",
            "from sklearn.pipeline import _name_estimators",
            "from sklearn.preprocessing import FunctionTransformer",
            "from sklearn.preprocessing.label import LabelEncoder",
            "from itertools import chain"
        ],
        "reference_api": [
            "np.logical_not",
            "np.asarray",
            "sparse.hstack",
            "np.sum",
            "len",
            "np.hstack",
            "isinstance",
            "np.zeros",
            "sparse.issparse",
            "check_array",
            "transform",
            "np.arange"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "transform",
                "code": "def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array\n            Transformed input.\n        \"\"\"\n        if not self._legacy_mode:\n            return self._transform_new(X)\n        else:\n            return _transform_selected(X, self._legacy_transform,\n                                       self._deprecated_categorical_features,\n                                       copy=True)"
            },
            {
                "name": "transform",
                "code": "def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array\n            Transformed input.\n        \"\"\"\n        if not self._legacy_mode:\n            return self._transform_new(X)\n        else:\n            return _transform_selected(X, self._legacy_transform,\n                                       self._deprecated_categorical_features,\n                                       copy=True)"
            },
            {
                "name": "transform",
                "code": "def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array\n            Transformed input.\n        \"\"\"\n        if not self._legacy_mode:\n            return self._transform_new(X)\n        else:\n            return _transform_selected(X, self._legacy_transform,\n                                       self._deprecated_categorical_features,\n                                       copy=True)"
            }
        ],
        "third_party": [
            "check_array",
            "np.arange",
            "np.zeros",
            "np.asarray",
            "np.logical_not",
            "np.sum",
            "sparse.issparse",
            "sparse.issparse",
            "sparse.hstack",
            "np.hstack"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "ageron/handson-ml",
        "file_path": "future_encoders.py",
        "function_declaration": "def _transform_new(self, X)",
        "start_line": "676",
        "end_line": "704",
        "docstring": "# This function transforms a categorical input array X into a sparse matrix format, suitable for encoding categorical features.\\nIt first checks the input array X and adjusts its data type if necessary.\\nIt then retrieves the number of samples and features in X.\\nUsing a helper function, it transforms the input array into integer-encoded values and generates a mask for valid entries.\\nThe function calculates the number of categories for each feature and computes feature indices.\\nIt constructs indices and indptr arrays for creating a sparse matrix.\\nThe output is a sparse CSR matrix with binary data, which can be converted to a dense array if the sparse attribute is set to False.\\nThe function returns the transformed data.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c9dfb00047d1",
        "ground_truth": "def _transform_new(self, X):\n    \"\"\"New implementation assuming categorical input\"\"\"\n    X_temp = check_array(X, dtype=None)\n    if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n        X = check_array(X, dtype=np.object)\n    else:\n        X = X_temp\n    n_samples, n_features = X.shape\n    X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n    mask = X_mask.ravel()\n    n_values = [cats.shape[0] for cats in self.categories_]\n    n_values = np.array([0] + n_values)\n    feature_indices = np.cumsum(n_values)\n    indices = (X_int + feature_indices[:-1]).ravel()[mask]\n    indptr = X_mask.sum(axis=1).cumsum()\n    indptr = np.insert(indptr, 0, 0)\n    data = np.ones(n_samples * n_features)[mask]\n    out = sparse.csr_matrix((data, indices, indptr),\n                            shape=(n_samples, feature_indices[-1]),\n                            dtype=self.dtype)\n    if not self.sparse:\n        return out.toarray()\n    else:\n        return out",
        "import_statements": [
            "import numbers",
            "import warnings",
            "from scipy import sparse",
            "from sklearn.base import clone, BaseEstimator, TransformerMixin",
            "from sklearn.externals import six",
            "from sklearn.utils import Bunch, check_array",
            "from sklearn.externals.joblib.parallel import delayed, Parallel",
            "from sklearn.utils.metaestimators import _BaseComposition",
            "from sklearn.utils.validation import check_is_fitted, FLOAT_DTYPES",
            "from sklearn.pipeline import _name_estimators",
            "from sklearn.preprocessing import FunctionTransformer",
            "from sklearn.preprocessing.label import LabelEncoder",
            "from itertools import chain"
        ],
        "reference_api": [
            "self._transform",
            "cumsum",
            "hasattr",
            "np.ones",
            "np.cumsum",
            "ravel",
            "X_mask.ravel",
            "check_array",
            "np.insert",
            "out.toarray",
            "sparse.csr_matrix",
            "np.issubdtype",
            "X_mask.sum",
            "np.array"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._transform",
                "code": "def _transform(self, X, handle_unknown='error'):\n\n        X_temp = check_array(X, dtype=None)\n        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n            X = check_array(X, dtype=np.object)\n        else:\n            X = X_temp\n\n        _, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            Xi = X[:, i]\n            valid_mask = np.in1d(Xi, self.categories_[i])\n\n            if not np.all(valid_mask):\n                if handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    Xi = Xi.copy()\n                    Xi[~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(Xi)\n\n        return X_int, X_mask"
            }
        ],
        "third_party": [
            "check_array",
            "np.issubdtype",
            "check_array",
            "X_mask.ravel",
            "np.array",
            "np.cumsum",
            "ravel",
            "cumsum",
            "X_mask.sum",
            "np.insert",
            "np.ones",
            "sparse.csr_matrix",
            "out.toarray"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "common/src/autogluon/common/space.py",
        "function_declaration": "def convert_to_sklearn(self)",
        "start_line": "119",
        "end_line": "126",
        "docstring": "# This function converts the current object into a scikit-learn compatible sampler.\\nIt imports the necessary distributions from scipy.stats.\\nIf the log attribute is True, it creates a log-uniform sampler with the specified lower and upper bounds.\\nIf log is False, it creates a uniform sampler with the specified range.\\nThe function returns the created sampler.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fcde391a8972",
        "ground_truth": "def convert_to_sklearn(self):\n    from scipy.stats import loguniform, uniform\n    if self.log:\n        sampler = loguniform(self.lower, self.upper)\n    else:\n        sampler = uniform(self.lower, self.upper - self.lower)\n    return sampler",
        "import_statements": [],
        "reference_api": [
            "loguniform",
            "uniform"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "loguniform",
            "uniform"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "eda/src/autogluon/eda/visualization/interaction.py",
        "function_declaration": "def _render(self, state: AnalysisState) -> None",
        "start_line": "526",
        "end_line": "551",
        "docstring": "# This function generates and displays Partial Dependence Plots (PDP) based on the provided analysis state.\\nIt retrieves additional arguments, figure arguments, and features to be plotted.\\nIf headers are specified, it renders the header for the PDP section.\\nA figure and axes are created using plt.subplots with the specified figure arguments.\\nThe function prepares keyword arguments by merging additional arguments and predefined keyword arguments.\\nIt filters the data for non-null values if two-way interaction plots are enabled.\\nPartialDependenceDisplay.from_estimator is used to create PDPs from the model and data, plotting them on the specified axes.\\nThe layout is adjusted with tight_layout for better spacing, and the plots are displayed using plt.show().",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5d5413adcca6",
        "ground_truth": "def _render(self, state: AnalysisState) -> None:\n    additional_kwargs, fig_args, features = self._get_args()\n    if self.headers:\n        self.render_header_if_needed(state, \"Partial Dependence Plots\")\n    fig, axs = plt.subplots(**fig_args)\n    kwargs = {**additional_kwargs, **self._kwargs}\n    data = state.pdp_data\n    if self.two_way:\n        for f in self.features[:-1]:\n            data = data[data[f].notna()]\n    PartialDependenceDisplay.from_estimator(\n        _SklearnAutoGluonWrapper(state.model),\n        data,\n        self.features,\n        ax=axs.ravel()[: len(features)],\n        target=self.target,\n        subsample=self.sample,\n        **kwargs,\n    )\n    plt.tight_layout(h_pad=0.3, w_pad=0.5)\n    plt.show()",
        "import_statements": [
            "from abc import ABC, abstractmethod",
            "from typing import Any, Dict, List, Optional, Tuple, Type, Union",
            "from scipy import stats",
            "from scipy.cluster import hierarchy as hc",
            "from sklearn.inspection import PartialDependenceDisplay",
            "from autogluon.common.features.types import R_BOOL, R_CATEGORY, R_DATETIME, R_FLOAT, R_INT, R_OBJECT"
        ],
        "reference_api": [
            "plt.subplots",
            "axs.ravel",
            "notna",
            "plt.show",
            "PartialDependenceDisplay.from_estimator",
            "len",
            "plt.tight_layout",
            "self.render_header_if_needed",
            "_SklearnAutoGluonWrapper",
            "self._get_args"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_args",
                "code": "def _get_args(self):\n        n = len(self.features)\n        cols = self.MAX_CHARTS_PER_ROW if n > self.MAX_CHARTS_PER_ROW else n\n        rows = int(np.ceil(n / cols))\n        if self.fig_args is None:\n            self.fig_args = {}\n        kind = \"both\"\n        additional_kwargs: Dict[str, Any] = {}\n        features = self.features\n\n        if self.two_way:\n            fig_args = {**dict(nrows=1, ncols=3, figsize=(12, 3)), **self.fig_args}\n            if len(self.features) == 2:\n                kind = \"average\"\n            features.append(features.copy())\n        else:\n            fig_args = {**dict(nrows=rows, ncols=cols, figsize=(12, 3 * rows)), **self.fig_args}\n            additional_kwargs[\"pd_line_kw\"] = {\"color\": \"red\"}\n            additional_kwargs[\"ice_lines_kw\"] = {\"color\": \"blue\"}\n        additional_kwargs[\"kind\"] = kind\n\n        return additional_kwargs, fig_args, features"
            }
        ],
        "third_party": [
            "self.render_header_if_needed",
            "plt.subplots",
            "notna",
            "PartialDependenceDisplay.from_estimator",
            "_SklearnAutoGluonWrapper",
            "axs.ravel",
            "plt.tight_layout",
            "plt.show"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "tabular/src/autogluon/tabular/models/rf/rf_model.py",
        "function_declaration": "def _preprocess(self, X, **kwargs)",
        "start_line": "69",
        "end_line": "78",
        "docstring": "# This function preprocesses the input data X by first calling the superclass's _preprocess method.\\nIf a feature generator is not already initialized, it creates an instance of LabelEncoderFeatureGenerator with verbosity set to 0 and fits it to the data.\\nIf the feature generator has identified features, the function copies X and applies the feature generator's transformations to these features.\\nIt then fills any missing values in X with 0 and converts the data to a NumPy array with dtype float32.\\nThe function returns the preprocessed data.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "db8aac29180b",
        "ground_truth": "def _preprocess(self, X, **kwargs):\n    X = super()._preprocess(X, **kwargs)\n    if self._feature_generator is None:\n        self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n        self._feature_generator.fit(X=X)\n    if self._feature_generator.features_in:\n        X = X.copy()\n        X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n    X = X.fillna(0).to_numpy(dtype=np.float32)\n    return X",
        "import_statements": [
            "import logging",
            "import math",
            "import os",
            "import pickle",
            "import sys",
            "import time",
            "from autogluon.common.features.types import R_BOOL, R_CATEGORY, R_FLOAT, R_INT",
            "from autogluon.common.utils.resource_utils import ResourceManager",
            "from autogluon.core.constants import MULTICLASS, QUANTILE, REGRESSION, SOFTCLASS",
            "from autogluon.core.models import AbstractModel",
            "from autogluon.core.utils.exceptions import NotEnoughMemoryError, TimeLimitExceeded",
            "from autogluon.core.utils.utils import normalize_pred_probas",
            "from autogluon.features.generators import LabelEncoderFeatureGenerator"
        ],
        "reference_api": [
            "super",
            "fit",
            "X.fillna",
            "to_numpy",
            "LabelEncoderFeatureGenerator",
            "X.copy",
            "transform",
            "_preprocess"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_preprocess",
                "code": "def _preprocess(self, X, **kwargs):\n        X = super()._preprocess(X, **kwargs)\n        if self._feature_generator is None:\n            self._feature_generator = LabelEncoderFeatureGenerator(verbosity=0)\n            self._feature_generator.fit(X=X)\n        if self._feature_generator.features_in:\n            X = X.copy()\n            X[self._feature_generator.features_in] = self._feature_generator.transform(X=X)\n        X = X.fillna(0).to_numpy(dtype=np.float32)\n        return X"
            }
        ],
        "third_party": [
            "LabelEncoderFeatureGenerator",
            "fit",
            "X.copy",
            "transform",
            "to_numpy",
            "X.fillna"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "tabular/src/autogluon/tabular/models/rf/rf_model.py",
        "function_declaration": "def _estimate_memory_usage(self, X, **kwargs)",
        "start_line": "126",
        "end_line": "136",
        "docstring": "# This function estimates the minimum memory usage required for a model based on the input data X.\\nIt retrieves model parameters and determines the final number of estimators.\\nA minimum of 40 estimators is set if the final number is less than 40 or if a search space is defined.\\nIt calculates the number of trees per estimator and estimates the bytes used per estimator based on the size of X, with an adjustment factor.\\nThe expected minimum memory usage is calculated by multiplying the bytes per estimator by the minimum number of estimators.\\nThe function returns the estimated minimum memory usage.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d4eb45e6d013",
        "ground_truth": "def _estimate_memory_usage(self, X, **kwargs):\n    params = self._get_model_params()\n    n_estimators_final = params[\"n_estimators\"]\n    if isinstance(n_estimators_final, int):\n        n_estimators_minimum = min(40, n_estimators_final)\n    else:  # if search space\n        n_estimators_minimum = 40\n    num_trees_per_estimator = self._get_num_trees_per_estimator()\n    bytes_per_estimator = num_trees_per_estimator * len(X) / 60000 * 1e6  # Underestimates by 3x on ExtraTrees\n    expected_min_memory_usage = bytes_per_estimator * n_estimators_minimum\n    return expected_min_memory_usage",
        "import_statements": [
            "import logging",
            "import math",
            "import os",
            "import pickle",
            "import sys",
            "import time",
            "from autogluon.common.features.types import R_BOOL, R_CATEGORY, R_FLOAT, R_INT",
            "from autogluon.common.utils.resource_utils import ResourceManager",
            "from autogluon.core.constants import MULTICLASS, QUANTILE, REGRESSION, SOFTCLASS",
            "from autogluon.core.models import AbstractModel",
            "from autogluon.core.utils.exceptions import NotEnoughMemoryError, TimeLimitExceeded",
            "from autogluon.core.utils.utils import normalize_pred_probas",
            "from autogluon.features.generators import LabelEncoderFeatureGenerator"
        ],
        "reference_api": [
            "min",
            "self._get_model_params",
            "self._get_num_trees_per_estimator",
            "len",
            "isinstance"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_num_trees_per_estimator",
                "code": "def _get_num_trees_per_estimator(self):\n        # Very rough guess to size of a single tree before training\n        if self.problem_type in [MULTICLASS, SOFTCLASS]:\n            if self.num_classes is None:\n                num_trees_per_estimator = 10  # Guess since it wasn't passed in, could also check y for a better value\n            else:\n                num_trees_per_estimator = self.num_classes\n        else:\n            num_trees_per_estimator = 1\n        return num_trees_per_estimator"
            }
        ],
        "third_party": [
            "self._get_model_params"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "features/src/autogluon/features/vectorizers.py",
        "function_declaration": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size)",
        "start_line": "19",
        "end_line": "24",
        "docstring": "# This function downsizes the vocabulary of a vectorizer based on n-gram frequencies and a specified vocabulary size.\\nIt creates a Counter object from the n-gram frequency data and identifies the top n most common n-grams according to the specified vocabulary size.\\nThe names of these top n n-grams are sorted and used to create a new vocabulary dictionary, mapping each n-gram to a unique index.\\nThe vectorizer's vocabulary is then updated with this new, downsized vocabulary.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6923cb007f4c",
        "ground_truth": "def downscale_vectorizer(vectorizer, ngram_freq, vocab_size):\n    counter = Counter(ngram_freq)\n    top_n = counter.most_common(vocab_size)\n    top_n_names = sorted([name for name, _ in top_n])\n    new_vocab = {name: i for i, name in enumerate(top_n_names)}\n    vectorizer.vocabulary_ = new_vocab",
        "import_statements": [
            "from collections import Counter",
            "from sklearn.feature_extraction.text import CountVectorizer"
        ],
        "reference_api": [
            "sorted",
            "enumerate",
            "Counter",
            "counter.most_common"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "Counter",
            "counter.most_common"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "tabular/src/autogluon/tabular/models/lr/lr_model.py",
        "function_declaration": " def _select_continuous(self, df, features)",
        "start_line": "274",
        "end_line": "284",
        "docstring": "# This function categorizes features in a DataFrame as either continuous or skewed based on their skewness.\\nIt initializes a dictionary to store these feature types and retrieves the skewness threshold from the parameters.\\nFor each feature, it calculates the skewness and compares it to the threshold.\\nFeatures with skewness exceeding the threshold are classified as \"skewed\", while others are classified as \"continuous\".\\nThe function returns a dictionary with lists of continuous and skewed features.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0e837b2d8088",
        "ground_truth": "def _select_continuous(self, df, features):\n    # continuous = numeric features to rescale\n    # skewed = features to which we will apply power (ie. log / box-cox) transform before normalization\n    types_of_features = defaultdict(list)\n    skew_threshold = self.params[\"proc.skew_threshold\"]\n    for feature in features:\n        if skew_threshold is not None and (np.abs(df[feature].skew()) > self.params[\"proc.skew_threshold\"]):\n            types_of_features[\"skewed\"].append(feature)\n        else:\n            types_of_features[\"continuous\"].append(feature)\n    return types_of_features",
        "import_statements": [
            "import logging",
            "import re",
            "import time",
            "import warnings",
            "from collections import defaultdict",
            "from sklearn.compose import ColumnTransformer",
            "from sklearn.feature_extraction.text import TfidfVectorizer",
            "from sklearn.impute import SimpleImputer",
            "from sklearn.pipeline import Pipeline",
            "from sklearn.preprocessing import QuantileTransformer, StandardScaler",
            "from autogluon.common.features.types import R_BOOL, R_CATEGORY, R_FLOAT, R_INT, R_OBJECT, S_BOOL, S_TEXT_AS_CATEGORY",
            "from autogluon.common.utils.log_utils import fix_sklearnex_logging_if_kaggle",
            "from autogluon.core.constants import BINARY, REGRESSION",
            "from autogluon.core.models import AbstractModel",
            "from autogluon.core.utils.exceptions import TimeLimitExceeded"
        ],
        "reference_api": [
            "append",
            "np.abs",
            "skew",
            "defaultdict"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "defaultdict",
            "np.abs",
            "skew",
            "append",
            "append"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "tabular/src/autogluon/tabular/models/knn/knn_utils.py",
        "function_declaration": "def _get_weights(dist, weights):",
        "start_line": "25",
        "end_line": "42",
        "docstring": "# This function calculates weights based on distances and a specified weights parameter.\\nIf the weights parameter is None or \"uniform\", it returns None, indicating no weights.\\nIf the weights parameter is \"distance\", it computes the inverse of the distances, handling any infinite values by masking and adjusting the rows accordingly.\\nIf the weights parameter is a callable function, it applies this function to the distances and returns the result.\\nIf the weights parameter is not recognized, it raises a ValueError indicating that the weights should be 'uniform', 'distance', or a callable function.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bb569c10f975",
        "ground_truth": "def _get_weights(dist, weights):\n    \"\"\"Get the weights from an array of distances and a parameter weights\"\"\"\n    if weights in (None, \"uniform\"):\n        return None\n    elif weights == \"distance\":\n        # if user attempts to classify a point that was zero distance from one\n        # or more training points, those training points are weighted as 1.0\n        # and the other points as 0.0\n        with np.errstate(divide=\"ignore\"):\n            dist = 1.0 / dist\n        inf_mask = np.isinf(dist)\n        inf_row = np.any(inf_mask, axis=1)\n        dist[inf_row] = inf_mask[inf_row]\n        return dist\n    elif callable(weights):\n        return weights(dist)\n    else:\n        raise ValueError(\"weights not recognized: should be 'uniform', 'distance', or a callable function\")",
        "import_statements": [
            "import logging",
            "from pandas import DataFrame",
            "from scipy.stats import mode",
            "from sklearn.utils.extmath import weighted_mode",
            "from autogluon.common.utils.try_import import try_import_faiss"
        ],
        "reference_api": [
            "weights",
            "ValueError",
            "np.errstate",
            "np.isinf",
            "np.any",
            "callable"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.errstate",
            "np.isinf",
            "np.any",
            "weights"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "tabular/src/autogluon/tabular/models/knn/_knn_loo_variants.py",
        "function_declaration": "def predict_loo(self)",
        "start_line": "107",
        "end_line": "136",
        "docstring": "# This function performs leave-one-out (LOO) prediction using k-nearest neighbors.\\nIt first computes the distances and indices of the k-nearest neighbors using the kneighbors method.\\nWeights for the neighbors are obtained using the _get_weights function.\\nIf the target variable _y is one-dimensional, it is reshaped to a two-dimensional array.\\nPredictions are calculated by averaging the neighbor target values, either unweighted or weighted, depending on the weights parameter.\\nThe weighted prediction involves summing the product of neighbor target values and their corresponding weights, divided by the sum of the weights.\\nIf the original target variable _y was one-dimensional, the predictions are flattened back to a one-dimensional array.\\nThe function returns the leave-one-out predictions.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "12aaa9c78402",
        "ground_truth": "def predict_loo(self):\n    \"\"\"Predict the target for the training data via leave-one-out.\n    Returns\n    -------\n    y : ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int\n        Target values.\n    \"\"\"\n    neigh_dist, neigh_ind = self.kneighbors()\n    weights = _get_weights(neigh_dist, self.weights)\n    _y = self._y\n    if _y.ndim == 1:\n        _y = _y.reshape((-1, 1))\n    if weights is None:\n        y_pred = np.mean(_y[neigh_ind], axis=1)\n    else:\n        y_pred = np.empty((len(neigh_dist), _y.shape[1]), dtype=np.float64)\n        denom = np.sum(weights, axis=1)\n        for j in range(_y.shape[1]):\n            num = np.sum(_y[neigh_ind, j] * weights, axis=1)\n            y_pred[:, j] = num / denom\n    if self._y.ndim == 1:\n        y_pred = y_pred.ravel()\n    return y_pred",
        "import_statements": [
            "import logging",
            "from scipy import stats",
            "from sklearn.neighbors._base import _get_weights",
            "from sklearn.utils.extmath import weighted_mode"
        ],
        "reference_api": [
            "y_pred.ravel",
            "self.kneighbors",
            "len",
            "np.sum",
            "np.mean",
            "_y.reshape",
            "_get_weights",
            "np.empty",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self.kneighbors",
            "_get_weights",
            "_y.reshape",
            "np.mean",
            "np.empty",
            "np.sum",
            "np.sum",
            "y_pred.ravel"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "tabular/src/autogluon/tabular/models/fastainn/quantile_helpers.py",
        "function_declaration": "def isotonic(input_data, quantile_list)",
        "start_line": "8",
        "end_line": "14",
        "docstring": "# This function applies isotonic regression to each row of the input data based on the specified quantile list.\\nIt first reshapes the quantile list into a one-dimensional array and determines the batch size from the input data's first dimension.\\nFor each row in the input data, it fits and transforms the data using IsotonicRegression with the given quantile list.\\nThe transformed data for each row is collected and stacked into a new array, which is then returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b316687d9e9b",
        "ground_truth": "def isotonic(input_data, quantile_list):\n    quantile_list = np.array(quantile_list).reshape(-1)\n    batch_size = input_data.shape[0]\n    new_output_data = []\n    for i in range(batch_size):\n        new_output_data.append(IsotonicRegression().fit_transform(quantile_list, input_data[i]))\n    return np.stack(new_output_data, 0)",
        "import_statements": [
            "import torch",
            "from sklearn.isotonic import IsotonicRegression"
        ],
        "reference_api": [
            "new_output_data.append",
            "fit_transform",
            "reshape",
            "np.array",
            "np.stack",
            "IsotonicRegression",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "reshape",
            "np.array",
            "new_output_data.append",
            "fit_transform",
            "IsotonicRegression",
            "np.stack"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "features/src/autogluon/features/generators/text_ngram.py",
        "function_declaration": "def _train_vectorizer(text_data: list, vectorizer)",
        "start_line": "271",
        "end_line": "276",
        "docstring": "# This function trains a vectorizer on the provided text data.\\nIt fits the vectorizer to the text data and transforms it into a matrix representation.\\nTo reduce the object size significantly, it sets the stop_words_ attribute of the vectorizer to None, which does not affect usability.\\nThe function returns the trained vectorizer and the transformed matrix.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "17566e088b4e",
        "ground_truth": "def _train_vectorizer(text_data: list, vectorizer):\n    # TODO: Consider upgrading to pandas 0.25.0 to benefit from sparse attribute improvements / bug fixes!\n    #  https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.25.0.html\n    transform_matrix = vectorizer.fit_transform(text_data)\n    vectorizer.stop_words_ = None  # Reduces object size by 100x+ on large datasets, no effect on usability\n    return vectorizer, transform_matrix",
        "import_statements": [
            "import copy",
            "import logging",
            "import traceback",
            "from pandas import DataFrame, Series",
            "from sklearn.feature_selection import SelectKBest, f_classif, f_regression",
            "from autogluon.common.features.types import S_IMAGE_BYTEARRAY, S_IMAGE_PATH, S_TEXT, S_TEXT_NGRAM",
            "from autogluon.common.utils.lite import disable_if_lite_mode"
        ],
        "reference_api": [
            "vectorizer.fit_transform"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "vectorizer.fit_transform"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "autogluon/autogluon",
        "file_path": "examples/automm/TCGA_cancer_survival/example_cancer_survival.py",
        "function_declaration": "def preprocess(df, test_size, shuffle)",
        "start_line": "62",
        "end_line": "77",
        "docstring": "# This function preprocesses a dataframe by cleaning and splitting it into training and testing sets.\\nIt first removes rows with missing entries marked by \"'--\".\\nColumns with unique values less than or equal to one or containing \"id\" in their name are dropped.\\nShortcut columns like \"days_to_death\" and \"year_of_death\" are also removed.\\nFinally, the dataframe is split into training and testing sets using train_test_split with specified test_size and shuffle parameters.\\nThe function returns the training and testing dataframes.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "be23c0afc2d6",
        "ground_truth": "def preprocess(df, test_size, shuffle):\n    N, _ = df.shape\n     df = df[df != \"'--\"]  # Replace missing entries with nan\n    n_unique = df.nunique()\n     for col, n in n_unique.items():\n        if \"id\" in col or n <= 1:\n            df.drop(col, axis=1, inplace=True)\n     shortcut_col = [\"days_to_death\", \"year_of_death\"] # Shortcut columns should be removed\n    for col in shortcut_col:\n        df.drop(col, axis=1, inplace=True)\n     df_train, df_test = train_test_split(df, test_size=test_size, shuffle=shuffle)\n    return df_train, df_test",
        "import_statements": [
            "import argparse",
            "import os",
            "import random",
            "from autogluon.tabular import TabularPredictor, TabularDataset",
            "from autogluon.tabular.configs.hyperparameter_configs import get_hyperparameter_config",
            "from sklearn.model_selection import train_test_split",
            "from autogluon.multimodal.utils import download",
            "import warnings"
        ],
        "reference_api": [
            "train_test_split",
            "df.nunique",
            "df.drop",
            "n_unique.items"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "df.nunique",
            "n_unique.items",
            "df.drop",
            "df.drop",
            "train_test_split"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "automl/auto-sklearn",
        "file_path": "autosklearn/pipeline/util.py",
        "function_declaration": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False)",
        "start_line": "160",
        "end_line": "177",
        "docstring": "# This function tests a classifier's iterative fitting process on a specified dataset, which defaults to the iris dataset.\\nIt first loads the training and testing data, with an option to make the data sparse.\\nIt retrieves the classifier's hyperparameter search space and obtains the default configuration.\\nThe classifier is initialized with this default configuration and a random state, then fitted iteratively to the training data for an initial two iterations with refitting enabled.\\nThe function continues iterative fitting, doubling the number of iterations each time, until the classifier is fully fitted.\\nFinally, it makes predictions on the test data and returns the predictions, true test labels, and the fitted classifier.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6a82997a8c59",
        "ground_truth": "def _test_classifier_iterative_fit(classifier, dataset=\"iris\", sparse=False):\n    X_train, Y_train, X_test, Y_test = get_dataset(dataset=dataset, make_sparse=sparse)\n    configuration_space = classifier.get_hyperparameter_search_space(\n        dataset_properties={\"sparse\": sparse}\n    )\n    default_config = configuration_space.get_default_configuration()\n     classifier = classifier(random_state=0, **default_config)\n    classifier.iterative_fit(X_train, Y_train, n_iter=2, refit=True)\n     iteration = 2\n    while not classifier.configuration_fully_fitted():\n        n_iter = int(2**iteration / 2)\n        classifier.iterative_fit(X_train, Y_train, n_iter=n_iter)\n        iteration += 1\n     predictions = classifier.predict(X_test)\n    return predictions, Y_test, classifier",
        "import_statements": [
            "import importlib",
            "import inspect",
            "import os",
            "import pkgutil",
            "import scipy.sparse",
            "import sklearn",
            "import sklearn.base",
            "import sklearn.datasets",
            "import unittest"
        ],
        "reference_api": [
            "classifier.configuration_fully_fitted",
            "get_dataset",
            "classifier.get_hyperparameter_search_space",
            "classifier.iterative_fit",
            "int",
            "classifier.predict",
            "classifier",
            "configuration_space.get_default_configuration"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "get_dataset",
                "code": "def get_dataset(\n    dataset=\"iris\",\n    make_sparse=False,\n    add_NaNs=False,\n    train_size_maximum=150,\n    make_multilabel=False,\n    make_binary=False,\n    return_target_as_string=False,\n):\n    iris = getattr(sklearn.datasets, \"load_%s\" % dataset)()\n    X = iris.data.astype(np.float32)\n    Y = iris.target\n\n    if return_target_as_string:\n        if make_binary or make_multilabel or (len(Y.shape) > 1 and Y.shape[1] > 1):\n            raise NotImplementedError()\n        Y = np.array([iris.target_names[y] for y in Y])\n\n    rs = np.random.RandomState(42)\n    indices = np.arange(X.shape[0])\n    train_size = min(int(len(indices) / 3.0 * 2.0), train_size_maximum)\n    rs.shuffle(indices)\n    X = X[indices]\n    Y = Y[indices]\n    X_train = X[:train_size]\n    Y_train = Y[:train_size]\n    X_test = X[train_size:]\n    Y_test = Y[train_size:]\n\n    if add_NaNs:\n        mask = rs.choice([True, False], size=(X_train.shape))\n        X_train[mask] = np.NaN\n\n    if make_sparse:\n        X_train[:, 0] = 0\n        X_train[rs.random_sample(X_train.shape) > 0.5] = 0\n        X_train = scipy.sparse.csc_matrix(X_train)\n        X_train.eliminate_zeros()\n        X_test[:, 0] = 0\n        X_test[rs.random_sample(X_test.shape) > 0.5] = 0\n        X_test = scipy.sparse.csc_matrix(X_test)\n        X_test.eliminate_zeros()\n\n    if make_binary and make_multilabel:\n        raise ValueError(\n            \"Can convert dataset only to one of the two \"\n            \"options binary or multilabel!\"\n        )\n\n    if make_binary:\n        Y_train[Y_train > 1] = 1\n        Y_test[Y_test > 1] = 1\n\n    if make_multilabel:\n        num_classes = len(np.unique(Y))\n        Y_train_ = np.zeros((Y_train.shape[0], num_classes))\n        for i in range(Y_train.shape[0]):\n            Y_train_[i, Y_train[i]] = 1\n        Y_train = Y_train_\n        Y_test_ = np.zeros((Y_test.shape[0], num_classes))\n        for i in range(Y_test.shape[0]):\n            Y_test_[i, Y_test[i]] = 1\n        Y_test = Y_test_\n\n    return X_train, Y_train, X_test, Y_test"
            }
        ],
        "third_party": [
            "classifier.get_hyperparameter_search_space",
            "configuration_space.get_default_configuration",
            "classifier",
            "classifier.iterative_fit",
            "classifier.configuration_fully_fitted",
            "classifier.iterative_fit",
            "classifier.predict"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "automl/auto-sklearn",
        "file_path": "autosklearn/automl.py",
        "function_declaration": "def _load_models(self)",
        "start_line": "1593",
        "end_line": "1635",
        "docstring": "# This function loads models based on the specified ensemble class and resampling strategy.\\nIf an ensemble class is defined, it loads the ensemble using the backend and a seed.\\nIf the ensemble is not loaded and certain evaluator output conditions and resampling strategies are met, it loads the best individual model instead.\\nIf the ensemble is successfully loaded, it retrieves model identifiers and loads the corresponding models using the backend.\\nFor specific resampling strategies, it also loads cross-validated models; otherwise, it sets the cross-validated models to None.\\nIf no ensemble is loaded, it initializes empty lists for models and cross-validated models.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d0d8450c4c17",
        "ground_truth": "def _load_models(self):\n    if self._ensemble_class is not None:\n        self.ensemble_ = self._backend.load_ensemble(self._seed)\n    else:\n        self.ensemble_ = None\n    # If no ensemble is loaded, try to get the best performing model.\n    # This is triggered if\n    # 1. self._ensemble_class is None (see if-statement above)\n    # 2. if the ensemble builder crashed and no ensemble is available\n    # 3. if the ensemble cannot be built because of arguments passed\n    #    by the user (disable_evaluator_output and\n    #    resampling_strategy)\n    if (\n        not self.ensemble_\n        and not (\n            self._disable_evaluator_output is True\n            or (\n                isinstance(self._disable_evaluator_output, list)\n                and \"model\" in self._disable_evaluator_output\n            )\n        )\n        and self._resampling_strategy\n        not in (\n            \"partial-cv\",\n            \"partial-cv-iterative-fit\",\n        )\n    ):\n        self.ensemble_ = self._load_best_individual_model()\n    if self.ensemble_:\n        identifiers = self.ensemble_.get_selected_model_identifiers()\n        self.models_ = self._backend.load_models_by_identifiers(identifiers)\n        if self._resampling_strategy in (\"cv\", \"cv-iterative-fit\"):\n            self.cv_models_ = self._backend.load_cv_models_by_identifiers(\n                identifiers\n            )\n        else:\n            self.cv_models_ = None\n    else:\n        self.models_ = []\n        self.cv_models_ = []",
        "import_statements": [
            "from typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n)",
            "import copy",
            "import io",
            "import itertools",
            "import json",
            "import logging.handlers",
            "import multiprocessing",
            "import os",
            "import platform",
            "import sys",
            "import time",
            "import types",
            "import uuid",
            "import warnings",
            "import distro",
            "import joblib",
            "import pkg_resources",
            "import scipy.stats",
            "import sklearn.utils",
            "from ConfigSpace.configuration_space import Configuration, ConfigurationSpace",
            "from ConfigSpace.read_and_write import json as cs_json",
            "from dask.distributed import Client",
            "from scipy.sparse import spmatrix",
            "from sklearn.base import BaseEstimator",
            "from sklearn.dummy import DummyClassifier, DummyRegressor",
            "from sklearn.ensemble import VotingClassifier, VotingRegressor",
            "from sklearn.metrics._classification import type_of_target",
            "from sklearn.model_selection._split import (\n    BaseCrossValidator,\n    BaseShuffleSplit,\n    _RepeatedSplits,\n)",
            "from sklearn.pipeline import Pipeline",
            "from sklearn.utils import check_random_state",
            "from sklearn.utils.validation import check_is_fitted",
            "from smac.callbacks import IncorporateRunResultCallback",
            "from smac.runhistory.runhistory import RunInfo, RunValue",
            "from smac.stats.stats import Stats",
            "from smac.tae import StatusType",
            "from typing_extensions import Literal",
            "from autosklearn.automl_common.common.utils.backend import Backend, create",
            "from autosklearn.constants import (\n    BINARY_CLASSIFICATION,\n    CLASSIFICATION_TASKS,\n    MULTICLASS_CLASSIFICATION,\n    MULTILABEL_CLASSIFICATION,\n    MULTIOUTPUT_REGRESSION,\n    REGRESSION,\n    REGRESSION_TASKS,\n)",
            "from autosklearn.data.validation import (\n    SUPPORTED_FEAT_TYPES,\n    SUPPORTED_TARGET_TYPES,\n    InputValidator,\n    convert_if_sparse,\n)",
            "from autosklearn.data.xy_data_manager import XYDataManager",
            "from autosklearn.ensemble_building import EnsembleBuilderManager",
            "from autosklearn.ensembles.abstract_ensemble import (\n    AbstractEnsemble,\n    AbstractMultiObjectiveEnsemble,\n)",
            "from autosklearn.ensembles.ensemble_selection import EnsembleSelection",
            "from autosklearn.ensembles.singlebest_ensemble import SingleBestFromRunhistory",
            "from autosklearn.evaluation import ExecuteTaFuncWithQueue, get_cost_of_crash",
            "from autosklearn.evaluation.abstract_evaluator import _fit_and_suppress_warnings",
            "from autosklearn.evaluation.train_evaluator import TrainEvaluator, _fit_with_budget",
            "from autosklearn.metrics import (\n    Scorer,\n    _validate_metrics,\n    compute_single_metric,\n    default_metric_for_task,\n)",
            "from autosklearn.pipeline.base import BasePipeline",
            "from autosklearn.pipeline.components.classification import ClassifierChoice",
            "from autosklearn.pipeline.components.data_preprocessing.categorical_encoding import (\n    OHEChoice,\n)",
            "from autosklearn.pipeline.components.data_preprocessing.minority_coalescense import (\n    CoalescenseChoice,\n)",
            "from autosklearn.pipeline.components.data_preprocessing.rescaling import RescalingChoice",
            "from autosklearn.pipeline.components.feature_preprocessing import (\n    FeaturePreprocessorChoice,\n)",
            "from autosklearn.pipeline.components.regression import RegressorChoice",
            "from autosklearn.smbo import AutoMLSMBO",
            "from autosklearn.util import RE_PATTERN, pipeline",
            "from autosklearn.util.dask import Dask, LocalDask, UserDask",
            "from autosklearn.util.data import (\n    DatasetCompressionSpec,\n    default_dataset_compression_arg,\n    reduce_dataset_size_if_too_large,\n    supported_precision_reductions,\n    validate_dataset_compression_arg,\n)",
            "from autosklearn.util.logging_ import (\n    PicklableClientLogger,\n    get_named_client_logger,\n    setup_logger,\n    start_log_server,\n    warnings_to,\n)",
            "from autosklearn.util.parallel import preload_modules",
            "from autosklearn.util.progress_bar import ProgressBar",
            "from autosklearn.util.smac_wrap import SMACCallback, SmacRunCallback",
            "from autosklearn.util.stopwatch import StopWatch",
            "import unittest.mock"
        ],
        "reference_api": [
            "load_ensemble",
            "load_cv_models_by_identifiers",
            "isinstance",
            "load_models_by_identifiers",
            "self._load_best_individual_model",
            "get_selected_model_identifiers"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._load_best_individual_model",
                "code": "def _load_best_individual_model(self):\n        \"\"\"\n        In case of failure during ensemble building,\n        this method returns the single best model found\n        by AutoML.\n        This is a robust mechanism to be able to predict,\n        even though no ensemble was found by ensemble builder.\n        It is also used to load the single best model in case\n        the user does not want to build an ensemble.\n        \"\"\"\n        # We also require that the model is fit and a task is defined\n        if not self._task:\n            return None\n\n        # SingleBest contains the best model found by AutoML\n        ensemble = SingleBestFromRunhistory(\n            metrics=self._metrics,\n            task_type=self._task,\n            seed=self._seed,\n            run_history=self.runhistory_,\n            backend=self._backend,\n            random_state=self._seed,\n        )\n        self._logger.warning(\n            \"No valid ensemble was created. Please check the log\"\n            \"file for errors. Default to the best individual estimator:{}\".format(\n                ensemble.get_identifiers_with_weights()[0][0]\n            )\n        )\n        return ensemble"
            },
            {
                "name": "get_selected_model_identifiers",
                "code": "def get_selected_model_identifiers(self) -> List[Tuple[int, int, float]]:\n        output = []\n\n        for i, weight in enumerate(self.weights_):\n            identifier = self.identifiers_[i]\n            if weight > 0.0:\n                output.append(identifier)\n\n        return output"
            }
        ],
        "third_party": [
            "load_ensemble",
            "load_models_by_identifiers",
            "load_cv_models_by_identifiers"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "automl/auto-sklearn",
        "file_path": "scripts/2015_nips_paper/run/run_auto_sklearn.py",
        "function_declaration": "def load_task(task_id)",
        "start_line": "14",
        "end_line": "36",
        "docstring": "# This function loads data for a given task ID using the OpenML library.\\nIt retrieves the task and extracts features (X) and labels (y).\\nIt obtains train-test split indices from the task and uses them to create training and testing sets for both features and labels.\\nThe function fetches the dataset associated with the task to determine the categorical or numerical nature of each feature.\\nIt maps unique label values to integer indices for both training and testing labels.\\nThe function returns the training features, training labels, testing features, testing labels, and a list indicating whether each feature is categorical or numerical.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ef71b1cddb50",
        "ground_truth": "def load_task(task_id):\n    \"\"\"Function used for loading data.\"\"\"\n    task = openml.tasks.get_task(task_id)\n    X, y = task.get_X_and_y()\n    train_indices, test_indices = task.get_train_test_split_indices()\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    dataset = openml.datasets.get_dataset(task.dataset_id)\n    _, _, cat = dataset.get_data(\n        return_categorical_indicator=True, target=task.target_name\n    )\n    del _\n    del dataset\n    cat = [\"categorical\" if c else \"numerical\" for c in cat]\n     unique = np.unique(y_train)\n    mapping = {unique_value: i for i, unique_value in enumerate(unique)}\n    y_train = np.array([mapping[value] for value in y_train])\n    y_test = np.array([mapping[value] for value in y_test])\n     return X_train, y_train, X_test, y_test, cat",
        "import_statements": [
            "import os",
            "import argparse",
            "import openml",
            "from autosklearn.classification import AutoSklearnClassifier",
            "from autosklearn.metrics import balanced_accuracy",
            "from remove_dataset_from_metadata import remove_dataset",
            "import score_ensemble"
        ],
        "reference_api": [
            "task.get_X_and_y",
            "get_dataset",
            "get_task",
            "dataset.get_data",
            "np.unique",
            "task.get_train_test_split_indices",
            "enumerate",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get_task",
            "task.get_X_and_y",
            "task.get_train_test_split_indices",
            "get_dataset",
            "dataset.get_data",
            "np.unique",
            "np.array",
            "np.array"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "automl/auto-sklearn",
        "file_path": "autosklearn/data/validation.py",
        "function_declaration": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]",
        "start_line": "17",
        "end_line": "41",
        "docstring": "# This function converts a sparse matrix input into a dense format.\\nIf the input y is a sparse matrix, it is converted to a dense array using the toarray method.\\nFor one-dimensional sparse data, the resulting array is flattened before returning.\\nIf the input y is not a sparse matrix, it is returned unchanged.\\nThe function supports returning the data as a NumPy array, list, pandas DataFrame, or pandas Series.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "90bd6edbcc9e",
        "ground_truth": "def convert_if_sparse(\n    y: SUPPORTED_TARGET_TYPES,\n) -> Union[np.ndarray, List, pd.DataFrame, pd.Series]:\n    \"\"\"If the labels `y` are sparse, it will convert it to its dense representation\n     Parameters\n    ----------\n    y: {array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs)\n        The labels to 'densify' if sparse\n     Returns\n    -------\n    np.ndarray of shape (n_samples, ) or (n_samples, n_outputs)\n    \"\"\"\n    if isinstance(y, spmatrix):\n        y_ = y.toarray()\n         # For sparse one dimensional data, y.toarray will return [[1], [2], [3], ...]\n        # We need to flatten this before returning it\n        if y_.shape[0] == 1:\n            y_ = y_.flatten()\n    else:\n        y_ = y\n     return y_",
        "import_statements": [
            "from typing import List, Optional, Tuple, Union",
            "import logging",
            "from scipy.sparse import spmatrix",
            "from sklearn.base import BaseEstimator",
            "from sklearn.exceptions import NotFittedError",
            "from autosklearn.data.feature_validator import SUPPORTED_FEAT_TYPES, FeatureValidator",
            "from autosklearn.data.target_validator import SUPPORTED_TARGET_TYPES, TargetValidator",
            "from autosklearn.util.logging_ import get_named_client_logger"
        ],
        "reference_api": [
            "y_.flatten",
            "y.toarray",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "y.toarray",
            "y_.flatten"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "automl/auto-sklearn",
        "file_path": "autosklearn/ensembles/ensemble_selection.py",
        "function_declaration": "def _calculate_weights(self) -> None",
        "start_line": "275",
        "end_line": "288",
        "docstring": "# This function calculates the weights for ensemble members based on their occurrence frequency in the ensemble.\\nIt counts the occurrences of each ensemble member index and initializes a weights array with zeros, having a length equal to the number of input models.\\nFor each ensemble member, it calculates the weight as the frequency divided by the ensemble size and assigns it to the corresponding index in the weights array.\\nIf the sum of weights is less than 1, the weights are normalized to ensure their sum equals 1.\\nThe calculated weights are then stored in the instance variable self.weights_.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5728e3d4f914",
        "ground_truth": "def _calculate_weights(self) -> None:\n    ensemble_members = Counter(self.indices_).most_common()\n    weights = np.zeros(\n        (self.num_input_models_,),\n        dtype=np.float64,\n    )\n    for ensemble_member in ensemble_members:\n        weight = float(ensemble_member[1]) / self.ensemble_size\n        weights[ensemble_member[0]] = weight\n    if np.sum(weights) < 1:\n        weights = weights / np.sum(weights)\n    self.weights_ = weights",
        "import_statements": [
            "from typing import Dict, List, Sequence, Tuple, Union",
            "import random",
            "import warnings",
            "from collections import Counter",
            "from sklearn.utils import check_random_state",
            "from autosklearn.automl_common.common.utils.backend import Backend",
            "from autosklearn.constants import TASK_TYPES",
            "from autosklearn.data.validation import SUPPORTED_FEAT_TYPES",
            "from autosklearn.ensemble_building.run import Run",
            "from autosklearn.ensembles.abstract_ensemble import AbstractEnsemble",
            "from autosklearn.metrics import Scorer, calculate_losses",
            "from autosklearn.pipeline.base import BasePipeline"
        ],
        "reference_api": [
            "float",
            "np.sum",
            "np.zeros",
            "most_common",
            "Counter"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "most_common",
            "Counter",
            "np.zeros",
            "np.sum",
            "np.sum"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "automl/auto-sklearn",
        "file_path": "autosklearn/estimators.py",
        "function_declaration": "def predict_proba(self, X, batch_size=None, n_jobs=1)",
        "start_line": "1485",
        "end_line": "1516",
        "docstring": "# This function predicts class probabilities for input data X, optionally using specified batch size and number of jobs.\\nIt calls the superclass's predict_proba method to obtain the predicted probabilities.\\nIf the target type is not \"multilabel-indicator\", it asserts that the sum of probabilities for each instance is 1.\\nIt also asserts that all probability values lie between 0 and 1.\\nThe function returns the predicted probabilities.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4f9c4799fd8f",
        "ground_truth": "def predict_proba(self, X, batch_size=None, n_jobs=1):\n    \"\"\"Predict probabilities of classes for all samples X.\n    Parameters\n    ----------\n    X : array-like or sparse matrix of shape = [n_samples, n_features]\n    batch_size : int (optional)\n        Number of data points to predict for (predicts all points at once\n        if ``None``.\n    n_jobs : int\n    Returns\n    -------\n    y : array of shape = [n_samples, n_classes] or [n_samples, n_labels]\n        The predicted class probabilities.\n    \"\"\"\n    pred_proba = super().predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)\n    # Check if all probabilities sum up to 1.\n    # Assert only if target type is not multilabel-indicator.\n    if self.target_type not in [\"multilabel-indicator\"]:\n        assert np.allclose(\n            np.sum(pred_proba, axis=1), np.ones_like(pred_proba[:, 0])\n        ), \"prediction probability does not sum up to 1!\"\n    # Check that all probability values lie between 0 and 1.\n    assert (pred_proba >= 0).all() and (\n        pred_proba <= 1\n    ).all(), \"found prediction probability value outside of [0, 1]!\"\n    return pred_proba",
        "import_statements": [
            "from typing import (\n    Any,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n)",
            "import warnings",
            "import dask.distributed",
            "import joblib",
            "from ConfigSpace.configuration_space import Configuration, ConfigurationSpace",
            "from scipy.sparse import spmatrix",
            "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin",
            "from sklearn.ensemble import VotingClassifier, VotingRegressor",
            "from sklearn.utils.multiclass import type_of_target",
            "from smac.runhistory.runhistory import RunInfo, RunValue",
            "from typing_extensions import Literal",
            "from autosklearn.automl import AutoML, AutoMLClassifier, AutoMLRegressor",
            "from autosklearn.data.validation import (\n    SUPPORTED_FEAT_TYPES,\n    SUPPORTED_TARGET_TYPES,\n    convert_if_sparse,\n)",
            "from autosklearn.ensembles.abstract_ensemble import AbstractEnsemble",
            "from autosklearn.ensembles.ensemble_selection import EnsembleSelection",
            "from autosklearn.ensembles.multiobjective_dummy_ensemble import (\n    MultiObjectiveDummyEnsemble,\n)",
            "from autosklearn.metrics import Scorer",
            "from autosklearn.pipeline.base import BasePipeline",
            "from autosklearn.util.smac_wrap import SMACCallback"
        ],
        "reference_api": [
            "super",
            "np.allclose",
            "np.sum",
            "np.ones_like",
            "all",
            "predict_proba"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "predict_proba",
                "code": "def predict_proba(self, X, batch_size=None, n_jobs=1):\n        return self.automl_.predict_proba(X, batch_size=batch_size, n_jobs=n_jobs)"
            }
        ],
        "third_party": [
            "np.allclose",
            "np.sum",
            "np.ones_like"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "automl/auto-sklearn",
        "file_path": "autosklearn/util/data.py",
        "function_declaration": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:",
        "start_line": "213",
        "end_line": "223",
        "docstring": "# This function converts a binary matrix Ybin to a numeric array.\\nIf Ybin is not a one-dimensional array, it calculates the dot product of Ybin with a range of integers corresponding to its second dimension's size, effectively converting binary class indicators to numeric class labels.\\nThe function returns the resulting numeric array.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a9a13150aebe",
        "ground_truth": "def convert_to_num(Ybin: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert binary targets to numeric vector\n    typically classification target values\n    :param Ybin:\n    :return:\n    \"\"\"\n    result = np.array(Ybin)\n    if len(Ybin.shape) != 1:\n        result = np.dot(Ybin, range(Ybin.shape[1]))\n    return result",
        "import_statements": [
            "from typing import (\n    Any,\n    Dict,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)",
            "import warnings",
            "from scipy.sparse import spmatrix",
            "from sklearn.model_selection import train_test_split",
            "from autosklearn.evaluation.splitter import CustomStratifiedShuffleSplit"
        ],
        "reference_api": [
            "np.dot",
            "range",
            "len",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.array",
            "np.dot"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "automl/auto-sklearn",
        "file_path": "autosklearn/pipeline/classification.py",
        "function_declaration": "def fit_transformer(self, X, y, fit_params=None)",
        "start_line": "99",
        "end_line": "125",
        "docstring": "# This function fits a transformer to the data X and labels y, applying any provided fit parameters.\\nIf no fit parameters are provided, it initializes an empty dictionary.\\nIf the balancing strategy in the configuration is set to \"weighting\", it creates a Balancing object and obtains weights for the classifier and feature preprocessor choices.\\nIt updates the initialization parameters and sets the hyperparameters with the updated configuration and initialization parameters.\\nAny additional fit parameters obtained are merged with the existing fit parameters.\\nThe superclass's fit_transformer method is called with the data, labels, and fit parameters.\\nThe function returns the transformed data and the fit parameters.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e97d38e62f81",
        "ground_truth": "def fit_transformer(self, X, y, fit_params=None):\n    if fit_params is None:\n        fit_params = {}\n    if self.config[\"balancing:strategy\"] == \"weighting\":\n        balancing = Balancing(strategy=\"weighting\")\n        _init_params, _fit_params = balancing.get_weights(\n            y,\n            self.config[\"classifier:__choice__\"],\n            self.config[\"feature_preprocessor:__choice__\"],\n            {},\n            {},\n        )\n        _init_params.update(self.init_params)\n        self.set_hyperparameters(\n            feat_type=self.feat_type,\n            configuration=self.config,\n            init_params=_init_params,\n        )\n        if _fit_params is not None:\n            fit_params.update(_fit_params)\n    X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n    return X, fit_params",
        "import_statements": [
            "from typing import Optional, Union",
            "import copy",
            "from itertools import product",
            "from ConfigSpace.configuration_space import Configuration, ConfigurationSpace",
            "from ConfigSpace.forbidden import ForbiddenAndConjunction, ForbiddenEqualsClause",
            "from sklearn.base import ClassifierMixin",
            "from autosklearn.askl_typing import FEAT_TYPE_TYPE",
            "from autosklearn.pipeline.base import BasePipeline",
            "from autosklearn.pipeline.components.classification import ClassifierChoice",
            "from autosklearn.pipeline.components.data_preprocessing import DataPreprocessorChoice",
            "from autosklearn.pipeline.components.data_preprocessing.balancing.balancing import (\n    Balancing,\n)",
            "from autosklearn.pipeline.components.feature_preprocessing import (\n    FeaturePreprocessorChoice,\n)",
            "from autosklearn.pipeline.constants import SPARSE"
        ],
        "reference_api": [
            "super",
            "self.set_hyperparameters",
            "_init_params.update",
            "fit_transformer",
            "Balancing",
            "fit_params.update",
            "balancing.get_weights"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "balancing.get_weights",
                "code": "def get_weights(\n        self,\n        Y: PIPELINE_DATA_DTYPE,\n        classifier: BaseEstimator,\n        preprocessor: BaseEstimator,\n        init_params: Optional[Dict[str, Any]],\n        fit_params: Optional[Dict[str, Any]],\n    ) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:\n        if init_params is None:\n            init_params = {}\n\n        if fit_params is None:\n            fit_params = {}\n\n        # Classifiers which require sample weights:\n        # We can have adaboost in here, because in the fit method,\n        # the sample weights are normalized:\n        # https://github.com/scikit-learn/scikit-learn/blob/0.15.X/sklearn/ensemble/weight_boosting.py#L121\n        # Have RF and ET in here because they emit a warning if class_weights\n        #  are used together with warmstarts\n        clf_ = [\n            \"adaboost\",\n            \"random_forest\",\n            \"extra_trees\",\n            \"sgd\",\n            \"passive_aggressive\",\n            \"gradient_boosting\",\n        ]\n        pre_: List[str] = []\n        if classifier in clf_ or preprocessor in pre_:\n            if len(Y.shape) > 1:\n                offsets = [2**i for i in range(Y.shape[1])]\n                Y_ = np.sum(Y * offsets, axis=1)\n            else:\n                Y_ = Y\n\n            unique, counts = np.unique(Y_, return_counts=True)\n            # This will result in an average weight of 1!\n            cw = 1 / (counts / np.sum(counts)) / 2\n            if len(Y.shape) == 2:\n                cw /= Y.shape[1]\n\n            sample_weights = np.ones(Y_.shape)\n\n            for i, ue in enumerate(unique):\n                mask = Y_ == ue\n                sample_weights[mask] *= cw[i]\n\n            if classifier in clf_:\n                fit_params[\"classifier:sample_weight\"] = sample_weights\n            if preprocessor in pre_:\n                fit_params[\"feature_preprocessor:sample_weight\"] = sample_weights\n\n        # Classifiers which can adjust sample weights themselves via the\n        # argument `class_weight`\n        clf_ = [\"decision_tree\", \"liblinear_svc\", \"libsvm_svc\"]\n        pre_ = [\"liblinear_svc_preprocessor\", \"extra_trees_preproc_for_classification\"]\n        if classifier in clf_:\n            init_params[\"classifier:class_weight\"] = \"balanced\"\n        if preprocessor in pre_:\n            init_params[\"feature_preprocessor:class_weight\"] = \"balanced\"\n\n        clf_ = [\"ridge\"]\n        if classifier in clf_:\n            class_weights = {}\n\n            unique, counts = np.unique(Y, return_counts=True)\n            cw = 1.0 / counts\n            cw = cw / np.mean(cw)\n\n            for i, ue in enumerate(unique):\n                class_weights[ue] = cw[i]\n\n            if classifier in clf_:\n                init_params[\"classifier:class_weight\"] = class_weights\n\n        return init_params, fit_params"
            },
            {
                "name": "self.set_hyperparameters",
                "code": "def set_hyperparameters(\n        self,\n        configuration,\n        feat_type: Optional[FEAT_TYPE_TYPE] = None,\n        init_params=None,\n    ):\n        self.config = configuration\n\n        for node_idx, n_ in enumerate(self.steps):\n            node_name, node = n_\n\n            sub_configuration_space = node.get_hyperparameter_search_space(\n                feat_type=feat_type, dataset_properties=self.dataset_properties\n            )\n            sub_config_dict = {}\n            for param in configuration:\n                if param.startswith(\"%s:\" % node_name):\n                    value = configuration[param]\n                    new_name = param.replace(\"%s:\" % node_name, \"\", 1)\n                    sub_config_dict[new_name] = value\n\n            sub_configuration = Configuration(\n                sub_configuration_space, values=sub_config_dict\n            )\n\n            if init_params is not None:\n                sub_init_params_dict = {}\n                for param in init_params:\n                    if param.startswith(\"%s:\" % node_name):\n                        value = init_params[param]\n                        new_name = param.replace(\"%s:\" % node_name, \"\", 1)\n                        sub_init_params_dict[new_name] = value\n            else:\n                sub_init_params_dict = None\n\n            if isinstance(\n                node, (AutoSklearnChoice, AutoSklearnComponent, BasePipeline)\n            ):\n                node.set_hyperparameters(\n                    feat_type=feat_type,\n                    configuration=sub_configuration,\n                    init_params=sub_init_params_dict,\n                )\n            else:\n                raise NotImplementedError(\"Not supported yet!\")\n\n        # In-code check to make sure init params\n        # is checked after pipeline creation\n        self._check_init_params_honored(init_params)\n\n        return self"
            },
            {
                "name": "fit_transformer",
                "code": "def fit_transformer(self, X, y, fit_params=None):\n\n        if fit_params is None:\n            fit_params = {}\n\n        if self.config[\"balancing:strategy\"] == \"weighting\":\n            balancing = Balancing(strategy=\"weighting\")\n            _init_params, _fit_params = balancing.get_weights(\n                y,\n                self.config[\"classifier:__choice__\"],\n                self.config[\"feature_preprocessor:__choice__\"],\n                {},\n                {},\n            )\n            _init_params.update(self.init_params)\n            self.set_hyperparameters(\n                feat_type=self.feat_type,\n                configuration=self.config,\n                init_params=_init_params,\n            )\n\n            if _fit_params is not None:\n                fit_params.update(_fit_params)\n\n        X, fit_params = super().fit_transformer(X, y, fit_params=fit_params)\n\n        return X, fit_params"
            }
        ],
        "third_party": [
            "Balancing",
            "_init_params.update",
            "fit_params.update"
        ]
    },
    {
        "subclass": "statsmodels",
        "owner/repo": "BayesWitnesses/m2cgen",
        "file_path": "tests/assemblers/test_linear_statsmodels.py",
        "function_declaration": "def test_glm_identity_link_func()",
        "start_line": "413",
        "end_line": "433",
        "docstring": "# This function tests the Generalized Linear Model (GLM) with an identity link function.\\nIt wraps the GLM from the Statsmodels library using a custom wrapper and sets it up with a Tweedie family and Power(1) link.\\nThe model is fitted with a small dataset.\\nAn assembler is used to convert the fitted model into an expression representation.\\nThe test compares the assembled expression to an expected expression using an assertion to ensure they match.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b1aaad80a9c4",
        "ground_truth": "def test_glm_identity_link_func():\n    estimator = utils.StatsmodelsSklearnLikeWrapper(\n        sm.GLM,\n        dict(init=dict(\n            family=sm.families.Tweedie(\n                sm.families.links.Power(1))),\n             fit=dict(maxiter=1)))\n    estimator = estimator.fit([[1], [2], [3]], [0.1, 0.2, 0.2])\n     assembler = assemblers.StatsmodelsModelAssemblerSelector(estimator)\n    actual = assembler.assemble()\n     expected = ast.BinNumExpr(\n        ast.NumVal(0.0),\n        ast.BinNumExpr(\n            ast.FeatureRef(0),\n            ast.NumVal(0.0791304348),\n            ast.BinNumOpType.MUL),\n        ast.BinNumOpType.ADD)\n     assert utils.cmp_exprs(actual, expected)",
        "import_statements": [
            "import pytest",
            "from statsmodels.regression.process_regression import ProcessMLE",
            "from m2cgen import assemblers, ast",
            "from tests import utils"
        ],
        "reference_api": [
            "estimator.fit",
            "utils.cmp_exprs",
            "Tweedie",
            "dict",
            "utils.StatsmodelsSklearnLikeWrapper",
            "assemblers.StatsmodelsModelAssemblerSelector",
            "assembler.assemble",
            "ast.BinNumExpr",
            "Power",
            "ast.NumVal",
            "ast.FeatureRef"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "utils.StatsmodelsSklearnLikeWrapper",
            "Tweedie",
            "Power",
            "estimator.fit",
            "assemblers.StatsmodelsModelAssemblerSelector",
            "assembler.assemble",
            "utils.cmp_exprs"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "biolab/orange3",
        "file_path": "Orange/data/aggregate.py",
        "function_declaration": "def _compute_aggregation(\n        self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n    ) -> pd.Series",
        "start_line": "89",
        "end_line": "98",
        "docstring": "# This function computes an aggregation for a specified column in a grouped DataFrame.\\nIt handles named aggregation to avoid conflicts with column names when resetting the index.\\nIf the aggregation is provided as a tuple, it extracts the name and function; otherwise, it determines the name from the string or function name.\\nThe function constructs a new column name based on the original column name and the aggregation name.\\nIt returns the result of applying the aggregation to the grouped DataFrame, with the new column name.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bcab63e592d0",
        "ground_truth": "def _compute_aggregation(\n    self, col: Variable, agg: Union[str, Callable, Tuple[str, Union[str, Callable]]]\n) -> pd.Series:\n    # use named aggregation to avoid issues with same column names when reset_index\n    if isinstance(agg, tuple):\n        name, agg = agg\n    else:\n        name = agg if isinstance(agg, str) else agg.__name__\n    col_name = f\"{col.name} - {name}\"\n    return self.group_by[col.name].agg(**{col_name: agg})",
        "import_statements": [
            "from functools import lru_cache",
            "from typing import Callable, Dict, List, Tuple, Union",
            "from Orange.data import Domain, Table, Variable, table_from_frame, table_to_frame",
            "from Orange.util import dummy_callback"
        ],
        "reference_api": [
            "agg",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "agg"
        ]
    },
    {
        "subclass": "seaborn",
        "owner/repo": "c60evaporator/seaborn-analyzer",
        "file_path": "seaborn_analyzer/custom_pair_plot.py",
        "function_declaration": "def _corrfunc(self, x, y, **kws)",
        "start_line": "16",
        "end_line": "36",
        "docstring": "# This function calculates and annotates the Pearson correlation coefficient between two variables, x and y.\\nIt handles optional hue groups, excluding rows where x or y is NaN.\\nIt computes the correlation coefficient and adjusts the font size based on the number of hue groups and the correlation value.\\nIt retrieves the appropriate axis and annotates it with the correlation coefficient, adjusting the position based on the hue group index.\\nThe function is designed to work with different versions of seaborn, handling both old and new keyword arguments for axes.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "114d623dae7b",
        "ground_truth": "def _corrfunc(self, x, y, **kws):\n    if self.hue_names is None:\n        labelnum=0\n        hue_num = 0\n    else:\n        labelnum=self.hue_names.index(kws[\"label\"])\n        hue_num = len(self.hue_names)\n    #x\u307e\u305f\u306fy\u304cNaN\u306e\u884c\u3092\u9664\u5916\n    mask = ~np.logical_or(np.isnan(x), np.isnan(y))\n    x, y = x[mask], y[mask]\n    #\u76f8\u95a2\u4fc2\u6570\u7b97\u51fa\uff060.4\u3054\u3068\u306b\u30d5\u30a9\u30f3\u30c8\u30b5\u30a4\u30ba\u62e1\u5927\n    r, _ = stats.pearsonr(x, y)\n    fsize = min(9, 45/hue_num) + min(4.5, 22.5/hue_num) * np.ceil(abs(r)/0.4)\n    fsize = min(9, 45/hue_num) if np.isnan(fsize) else fsize\n    #\u8a72\u5f53\u30de\u30b9\u306eax\u3092\u53d6\u5f97\n    if 'ax' in kws.keys():  # seaborn 0.11.1\u4ee5\u964d\n        ax = kws['ax']\n    else:  # seaborn 0.11.0\u4ee5\u524d\n        ax = plt.gca()\n    #\u65e2\u306b\u8868\u793a\u3057\u305fhue\u306e\u5206\u3060\u3051\u4e0b\u306b\u3055\u3052\u3066\u76f8\u95a2\u4fc2\u6570\u8868\u793a\n    ax.annotate(\"r={:.2f}\".format(r), xy=(.1, .65-min(.15,.75/hue_num)*labelnum), xycoords=ax.transAxes, size=fsize, color=kws[\"color\"])",
        "import_statements": [
            "from scipy import stats"
        ],
        "reference_api": [
            "index",
            "np.logical_or",
            "min",
            "np.isnan",
            "ax.annotate",
            "len",
            "abs",
            "np.ceil",
            "kws.keys",
            "stats.pearsonr",
            "plt.gca",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "index",
            "np.logical_or",
            "np.isnan",
            "np.isnan",
            "stats.pearsonr",
            "np.ceil",
            "np.isnan",
            "kws.keys",
            "plt.gca",
            "ax.annotate"
        ]
    },
    {
        "subclass": "seaborn",
        "owner/repo": "c60evaporator/seaborn-analyzer",
        "file_path": "seaborn_analyzer/custom_hist_plot.py",
        "function_declaration": "def _round_digits(src: float, rounddigit: int = None, method='decimal')",
        "start_line": "65",
        "end_line": "85",
        "docstring": "# This function rounds a given floating-point number src to a specified number of digits using one of three methods.\\nIf the method is 'decimal', it uses Python's built-in round function.\\nIf the method is 'sig', it uses the decimal module to set the precision and create a decimal representation of the number.\\nIf the method is 'format', it formats the number as a string with a specified number of significant digits.\\nThe function returns the rounded number according to the chosen method.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4c1ade308953",
        "ground_truth": "def _round_digits(src: float, rounddigit: int = None, method='decimal'):\n    \"\"\"\n    \u6307\u5b9a\u6841\u6570\u3067\u5c0f\u6570\u3092\u4e38\u3081\u308b\n    Parameters\n    ----------\n    src : float\n        \u4e38\u3081\u5bfe\u8c61\u306e\u6570\u5024\n    rounddigit : int\n        \u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u7dda\u306e\u8868\u793a\u7bc4\u56f2\uff08\u6a19\u6e96\u504f\u5dee\u306e\u4f55\u500d\u307e\u3067\u8868\u793a\u3059\u308b\u304b\u6307\u5b9a\uff09\n    method : int\n        \u6841\u6570\u6c7a\u5b9a\u624b\u6cd5\uff08'decimal':\u5c0f\u6570\u70b9\u4ee5\u4e0b, 'sig':\u6709\u52b9\u6570\u5b57(Decimal\u6307\u5b9a), 'format':format\u3067\u6709\u52b9\u6841\u6570\u6307\u5b9a\uff09\n    \"\"\"\n    if method == 'decimal':\n        return round(src, rounddigit)\n    elif method == 'sig':\n        with decimal.localcontext() as ctx:\n            ctx.prec = rounddigit\n            return ctx.create_decimal(src)\n    elif method == 'format':\n        return '{:.{width}g}'.format(src, width=rounddigit)",
        "import_statements": [
            "from typing import Dict",
            "from scipy import stats",
            "from scipy.stats import distributions",
            "import decimal"
        ],
        "reference_api": [
            "ctx.create_decimal",
            "format",
            "decimal.localcontext",
            "round"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ctx.create_decimal"
        ]
    },
    {
        "subclass": "seaborn",
        "owner/repo": "c60evaporator/seaborn-analyzer",
        "file_path": "seaborn_analyzer/custom_hist_plot.py",
        "function_declaration": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal')",
        "start_line": "88",
        "end_line": "107",
        "docstring": "# This function rounds the values in a dictionary to a specified number of digits using a chosen method.\\nIt iterates through the input dictionary srcdict, and for each floating-point value, it rounds the value using the _round_digits method if rounddigit is provided.\\nThe method can be 'decimal', 'sig', or 'format'.\\nIf the value is not a float or no rounding is specified, the original value is retained.\\nThe function returns a new dictionary with the rounded values.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a9923e678320",
        "ground_truth": "def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal'):\n    \"\"\"\n    \u6307\u5b9a\u6841\u6570\u3067Dict\u306e\u5024\u3092\u4e38\u3081\u308b\n    Parameters\n    ----------\n    srcdict : dict[str, float]\n        \u4e38\u3081\u5bfe\u8c61\u306eDict\n    rounddigit : int\n        \u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u7dda\u306e\u8868\u793a\u7bc4\u56f2\uff08\u6a19\u6e96\u504f\u5dee\u306e\u4f55\u500d\u307e\u3067\u8868\u793a\u3059\u308b\u304b\u6307\u5b9a\uff09\n    method : int\n        \u6841\u6570\u6c7a\u5b9a\u624b\u6cd5\uff08'decimal':\u5c0f\u6570\u70b9\u4ee5\u4e0b, 'sig':\u6709\u52b9\u6570\u5b57(Decimal\u6307\u5b9a), 'format':format\u3067\u6709\u52b9\u6841\u6570\u6307\u5b9a\uff09\n    \"\"\"\n    dstdict = {}\n    for k, v in srcdict.items():\n        if rounddigit is not None and isinstance(v, float):\n            dstdict[k] = cls._round_digits(v, rounddigit=rounddigit, method=method)\n        else:\n            dstdict[k] = v\n    return dstdict",
        "import_statements": [
            "from typing import Dict",
            "from scipy import stats",
            "from scipy.stats import distributions",
            "import decimal"
        ],
        "reference_api": [
            "srcdict.items",
            "cls._round_digits",
            "isinstance"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "cls._round_digits",
                "code": "     ctx.prec = rounddigit\n                return ctx.create_decimal(src)\n        elif method == 'format':\n            return '{:.{width}g}'.format(src, width=rounddigit)\n\n    @classmethod\n    def _round_dict_digits(cls, srcdict: Dict[str, float], rounddigit: int = None, method='decimal'):\n        \"\"\"\n        \u6307\u5b9a\u6841\u6570\u3067Dict\u306e\u5024\u3092\u4e38\u3081\u308b\n\n        Parameters\n        ----------\n        srcdict : dict[str, float]\n            \u4e38\u3081\u5bfe\u8c61\u306eDict\n        rounddigit : int\n            \u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\u7dda\u306e\u8868\u793a\u7bc4\u56f2\uff08\u6a19\u6e96\u504f\u5dee\u306e\u4f55\u500d\u307e\u3067\u8868\u793a\u3059\u308b\u304b\u6307\u5b9a\uff09\n        method : int\n            \u6841\u6570\u6c7a\u5b9a\u624b\u6cd5\uff08'decimal':\u5c0f\u6570\u70b9\u4ee5\u4e0b, 'sig':\u6709\u52b9\u6570\u5b57(Decimal\u6307\u5b9a), 'format':format\u3067\u6709\u52b9\u6841\u6570\u6307\u5b9a\uff09\n        \"\"\"\n        dstdict = {}\n        for k, v in srcdict.items():\n            if rounddigit is not None and isinstance(v, float):\n                dstdict[k] = cls._round_digits(v, rounddigit=rounddigit, method=method)\n            else:\n                "
            }
        ],
        "third_party": [
            "srcdict.items"
        ]
    },
    {
        "subclass": "seaborn",
        "owner/repo": "c60evaporator/seaborn-analyzer",
        "file_path": "seaborn_analyzer/custom_reg_plot.py",
        "function_declaration": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws)",
        "start_line": "174",
        "end_line": "191",
        "docstring": "# This function creates a scatter plot from numpy arrays using Seaborn.\\nIt first combines the x and y values into a DataFrame with specified column names.\\nIf hue_data is provided, it adds a hue field to the DataFrame for color-coding the points.\\nThe function then plots the scatter plot using Seaborn's scatterplot function, applying any additional keyword arguments for the plot and legend.\\nIf no title is specified for the legend, it sets the title to the hue field name.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b6106e079dc0",
        "ground_truth": "def _scatterplot_ndarray(cls, x, x_name, y, y_name, hue_data, hue_name, ax, scatter_kws, legend_kws):\n    \"\"\"\n    np.ndarray\u3092\u5165\u529b\u3068\u3057\u3066\u6563\u5e03\u56f3\u8868\u793a(scatterplot)\n    \"\"\"\n    # X\u5024\u3068Y\u5024\u3092\u5408\u4f53\u3057\u3066DataFrame\u5316\n    data = np.stack([x, y], axis=1)\n    data = pd.DataFrame(data, columns=[x_name, y_name])\n    # \u8272\u5206\u3051\u6307\u5b9a\u3057\u3066\u3044\u308b\u3068\u304d\u3001\u8272\u5206\u3051\u7528\u306e\u30d5\u30a3\u30fc\u30eb\u30c9\u3092\u8ffd\u52a0\n    if hue_data is not None:\n        if hue_name is None:\n            hue_name = 'hue'\n        data[hue_name] = pd.Series(hue_data)\n    # \u6563\u5e03\u56f3\u30d7\u30ed\u30c3\u30c8\n    sns.scatterplot(x=x_name, y=y_name, data=data, ax=ax, hue=hue_name, **scatter_kws)\n    # \u51e1\u4f8b\u8ffd\u52a0\n    if 'title' not in legend_kws.keys():\n        legend_kws['title'] = hue_name \n    ax.legend(**legend_kws)",
        "import_statements": [
            "from typing import List, Dict",
            "import numbers",
            "from scipy import stats",
            "from sklearn.linear_model import LinearRegression",
            "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error, mean_absolute_percentage_error",
            "from sklearn.model_selection import KFold, LeaveOneOut, GroupKFold, LeaveOneGroupOut",
            "from sklearn.pipeline import Pipeline",
            "from sklearn.base import is_classifier",
            "import decimal"
        ],
        "reference_api": [
            "ax.legend",
            "sns.scatterplot",
            "pd.Series",
            "legend_kws.keys",
            "np.stack",
            "pd.DataFrame"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.stack",
            "pd.DataFrame",
            "pd.Series",
            "sns.scatterplot",
            "legend_kws.keys",
            "ax.legend"
        ]
    },
    {
        "subclass": "statsmodels",
        "owner/repo": "carlomazzaferro/scikit-hts",
        "file_path": "hts/model/ar.py",
        "function_declaration": "def fit(self, **fit_args) -> \"TimeSeriesModel\"",
        "start_line": "43",
        "end_line": "54",
        "docstring": "# This function fits a time series model using the data associated with the current node.\\nIt retrieves the main time series data and checks for any exogenous variables specified in the node.\\nWarnings related to user actions and convergence are temporarily suppressed during the fitting process.\\nThe model is fitted using the endogenous and exogenous data, along with any additional fitting arguments provided.\\nThe function returns the fitted time series model object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "257e0aa5dd4d",
        "ground_truth": "def fit(self, **fit_args) -> \"TimeSeriesModel\":\n    as_df = self.node.item\n    end = self._get_transformed_data(as_series=True)\n    if self.node.exogenous:\n        ex = as_df[self.node.exogenous]\n    else:\n        ex = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n        self.model = self.model.fit(y=end, X=ex, **fit_args)\n    return self",
        "import_statements": [
            "import logging",
            "import warnings",
            "import pandas",
            "from statsmodels.tools.sm_exceptions import ConvergenceWarning",
            "from hts._t import ModelT",
            "from hts.hierarchy import HierarchyTree",
            "from hts.model.base import TimeSeriesModel"
        ],
        "reference_api": [
            "warnings.catch_warnings",
            "fit",
            "self._get_transformed_data",
            "warnings.filterwarnings"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_transformed_data",
                "code": "def _get_transformed_data(\n        self, as_series: bool = False\n    ) -> Union[pandas.DataFrame, pandas.Series]:\n        key = self.node.key\n        value = self.node.item\n        transformed = self.transform_function.transform(value[key])\n        if as_series:\n            return pandas.Series(transformed)\n        else:\n            return pandas.DataFrame({key: transformed})"
            },
            {
                "name": "fit",
                "code": "def fit(self, **fit_args) -> \"TimeSeriesModel\":\n        as_df = self.node.item\n        end = self._get_transformed_data(as_series=True)\n        if self.node.exogenous:\n            ex = as_df[self.node.exogenous]\n        else:\n            ex = None\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=UserWarning)\n            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n            self.model = self.model.fit(y=end, X=ex, **fit_args)\n        return self"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "statsmodels",
        "owner/repo": "carlomazzaferro/scikit-hts",
        "file_path": "hts/model/base.py",
        "function_declaration": "def create_model(self, **kwargs)",
        "start_line": "91",
        "end_line": "118",
        "docstring": "# This function creates a time series model based on the specified model type.\\nIf the model type is 'holt_winters', it retrieves the transformed data and creates an ExponentialSmoothing model.\\nIf the model type is 'auto_arima', it attempts to import AutoARIMA from pmdarima and create an AutoARIMA model with the given arguments.\\nIf pmdarima is not installed, it logs an error and exits.\\nIf the model type is 'sarimax', it retrieves the transformed data, checks for exogenous variables, and creates a SARIMAX model.\\nIf the model type is not recognized, it raises an exception.\\nThe function returns the created model.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6446de2223e8",
        "ground_truth": "def create_model(self, **kwargs):\n    if self.kind == ModelT.holt_winters.name:\n        data = self._get_transformed_data()\n        model = ExponentialSmoothing(endog=data, **kwargs)\n    elif self.kind == ModelT.auto_arima.name:\n        try:\n            from pmdarima import AutoARIMA\n        except ImportError:  # pragma: no cover\n            logger.error(\n                \"pmdarima not installed, so auto_arima won't work. Exiting.\"\n                \"Install it with: pip install scikit-hts[auto_arima]\"\n            )\n            return\n        model = AutoARIMA(**kwargs)\n    elif self.kind == ModelT.sarimax.name:\n        as_df = self.node.item\n        end = self._get_transformed_data(as_series=True)\n        if self.node.exogenous:\n            ex = as_df[self.node.exogenous]\n        else:\n            ex = None\n        model = SARIMAX(endog=end, exog=ex, **kwargs)\n    else:\n        raise\n    return model",
        "import_statements": [
            "import logging",
            "from typing import NamedTuple, Union",
            "import numpy",
            "import pandas",
            "from statsmodels.tsa.holtwinters import ExponentialSmoothing",
            "from statsmodels.tsa.statespace.sarimax import SARIMAX",
            "from hts._t import ModelT, NAryTreeT, TimeSeriesModelT, TransformT",
            "from hts.core.exceptions import InvalidArgumentException",
            "from hts.hierarchy import HierarchyTree",
            "from hts.transforms import BoxCoxTransformer, FunctionTransformer"
        ],
        "reference_api": [
            "self._get_transformed_data",
            "AutoARIMA",
            "ExponentialSmoothing",
            "SARIMAX",
            "logger.error"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_transformed_data",
                "code": "def _get_transformed_data(\n        self, as_series: bool = False\n    ) -> Union[pandas.DataFrame, pandas.Series]:\n        key = self.node.key\n        value = self.node.item\n        transformed = self.transform_function.transform(value[key])\n        if as_series:\n            return pandas.Series(transformed)\n        else:\n            return pandas.DataFrame({key: transformed})"
            },
            {
                "name": "self._get_transformed_data",
                "code": "def _get_transformed_data(\n        self, as_series: bool = False\n    ) -> Union[pandas.DataFrame, pandas.Series]:\n        key = self.node.key\n        value = self.node.item\n        transformed = self.transform_function.transform(value[key])\n        if as_series:\n            return pandas.Series(transformed)\n        else:\n            return pandas.DataFrame({key: transformed})"
            }
        ],
        "third_party": [
            "ExponentialSmoothing",
            "AutoARIMA",
            "SARIMAX"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "chainer/chainer",
        "file_path": "chainerx/_fallback_workarounds.py",
        "function_declaration": "def _to_chx(array)",
        "start_line": "92",
        "end_line": "99",
        "docstring": "# This function converts a given array to a chainerx.ndarray.\\nIf the input array is a numpy.ndarray, it converts it using the _from_numpy function.\\nIf the input array is a cupy.ndarray and the cupy module is available, it converts it using the _from_cupy function.\\nFor objects with other types, the function returns them unchanged.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b2f67820e27f",
        "ground_truth": "def _to_chx(array):\n    # Converts numpy/cupy.ndarray to chainerx.ndarray.\n    # Objects with other types are kept intact.\n    if isinstance(array, numpy.ndarray):\n        return _from_numpy(array)\n    elif cupy is not None and isinstance(array, cupy.ndarray):\n        return _from_cupy(array)\n    return array",
        "import_statements": [
            "import numpy",
            "import chainerx"
        ],
        "reference_api": [
            "_from_cupy",
            "_from_numpy",
            "isinstance"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_from_numpy",
                "code": "def _from_numpy(array):\n    assert isinstance(array, numpy.ndarray)\n    return chainerx.array(array, copy=False)"
            },
            {
                "name": "_from_cupy",
                "code": "def _from_cupy(array):\n    assert cupy is not None\n    assert isinstance(array, cupy.ndarray)\n    device = chainerx.get_device('cuda', array.device.id)\n    return chainerx._core._fromrawpointer(\n        array.data.mem.ptr,\n        array.shape,\n        array.dtype,\n        array.strides,\n        device,\n        array.data.ptr - array.data.mem.ptr,\n        array)"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "numpy",
        "owner/repo": "chainer/chainer",
        "file_path": "chainer/functions/array/split_axis.py",
        "function_declaration": "def _fix_numpy_split(ys, x, indices_or_sections, axis)",
        "start_line": "16",
        "end_line": "29",
        "docstring": "# This function ensures compatibility of the output of np.split with numpy versions >= 1.11.\\nIt checks if all arrays in the input list ys have the same number of dimensions as the array x.\\nIf not, it calculates the expected shapes for the split segments using a temporary split on an empty array with the same shape as x along the specified axis.\\nFor each segment, if the dimensions do not match, it reshapes the segment to the expected shape based on the calculated split sizes.\\nThe function returns the modified list of split arrays.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b96f7d9bcc95",
        "ground_truth": "def _fix_numpy_split(ys, x, indices_or_sections, axis):\n    \"\"\"Make the output of np.split compatible with numpy >= 1.11\"\"\"\n    if all(y.ndim == x.ndim for y in ys):\n        return ys\n    tmp = [len(t) for t in numpy.split(\n        numpy.empty(x.shape[axis], dtype=numpy.int8), indices_or_sections, 0)]\n    shape = list(x.shape)\n    for i, t in enumerate(tmp):\n        y = ys[i]\n        if y.ndim != x.ndim:\n            assert y.size == 0\n            shape[axis] = t\n            ys[i] = y.reshape(shape)\n    return ys",
        "import_statements": [
            "import numpy",
            "import six",
            "import chainer",
            "from chainer import backend",
            "from chainer.backends import intel64",
            "from chainer import function_node",
            "from chainer.utils import collections_abc",
            "from chainer.utils import type_check",
            "import chainerx"
        ],
        "reference_api": [
            "list",
            "y.reshape",
            "numpy.empty",
            "len",
            "numpy.split",
            "all",
            "enumerate"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "numpy.split",
            "numpy.empty",
            "y.reshape"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "chainer/chainer",
        "file_path": "examples/chainermn/seq2seq/seq2seq.py",
        "function_declaration": "def to_device_batch(batch):",
        "start_line": "143",
        "end_line": "155",
        "docstring": "# This function transfers a batch of data to a specified device.\\nIf the device is None, it returns the batch unchanged.\\nIf the device is a negative number, it transfers each element in the batch to the specified device using chainer.dataset.to_device.\\nFor positive device values, it determines the array module (NumPy or CuPy) for the batch, concatenates the batch along the first axis, and calculates the split sections.\\nThe concatenated batch is then transferred to the specified device and split back into the original batch sections using CuPy.\\nThe function returns the batch transferred to the specified device.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cf39fb123842",
        "ground_truth": "def to_device_batch(batch):\n    if device is None:\n        return batch\n    elif device < 0:\n        return [chainer.dataset.to_device(device, x) for x in batch]\n    else:\n        xp = cuda.cupy.get_array_module(*batch)\n        concat = xp.concatenate(batch, axis=0)\n        sections = numpy.cumsum(\n            [len(x) for x in batch[:-1]], dtype=numpy.int32)\n        concat_dev = chainer.dataset.to_device(device, concat)\n        batch_dev = cuda.cupy.split(concat_dev, sections)\n        return batch_dev",
        "import_statements": [
            "import argparse",
            "import math",
            "import os.path",
            "import pickle",
            "import re",
            "import sys",
            "import time",
            "from nltk.translate import bleu_score",
            "import numpy",
            "import six",
            "import chainer",
            "from chainer import cuda",
            "from chainer import reporter",
            "from chainer import training",
            "from chainer.training import extensions",
            "import chainermn",
            "import europal"
        ],
        "reference_api": [
            "to_device",
            "xp.concatenate",
            "len",
            "split",
            "get_array_module",
            "numpy.cumsum"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "to_device",
            "get_array_module",
            "xp.concatenate",
            "numpy.cumsum",
            "to_device",
            "split"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "chainer/chainer",
        "file_path": "examples/dcgan/visualize.py",
        "function_declaration": "def make_image(trainer)",
        "start_line": "15",
        "end_line": "36",
        "docstring": "# This function generates and saves an image based on the current state of a trainer.\\nIt sets a random seed and determines the number of images to generate.\\nIt creates a latent variable z, and generates images using a generator model with the training mode turned off.\\nThe generated images are moved to the CPU, scaled to a [0, 255] range, and converted to uint8 type.\\nThe images are reshaped and transposed to form a grid of images.\\nThe function creates a directory for saving the preview image if it doesn't exist and saves the generated image as a PNG file named according to the current training iteration.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "845c744d897c",
        "ground_truth": "def make_image(trainer):\n    np.random.seed(seed)\n    n_images = rows * cols\n    xp = gen.xp\n    z = Variable(xp.asarray(gen.make_hidden(n_images)))\n    with chainer.using_config('train', False):\n        x = gen(z)\n    x = chainer.backends.cuda.to_cpu(x.array)\n    np.random.seed()\n    x = np.asarray(np.clip(x * 255, 0.0, 255.0), dtype=np.uint8)\n    _, _, H, W = x.shape\n    x = x.reshape((rows, cols, 3, H, W))\n    x = x.transpose(0, 3, 1, 4, 2)\n    x = x.reshape((rows * H, cols * W, 3))\n    preview_dir = '{}/preview'.format(dst)\n    preview_path = preview_dir +\\\n        '/image{:0>8}.png'.format(trainer.updater.iteration)\n    if not os.path.exists(preview_dir):\n        os.makedirs(preview_dir)\n    Image.fromarray(x).save(preview_path)",
        "import_statements": [
            "import os",
            "from PIL import Image",
            "import chainer",
            "import chainer.backends.cuda",
            "from chainer import Variable"
        ],
        "reference_api": [
            "save",
            "exists",
            "gen.make_hidden",
            "os.makedirs",
            "np.asarray",
            "x.reshape",
            "x.transpose",
            "xp.asarray",
            "np.clip",
            "seed",
            "Variable",
            "gen",
            "to_cpu",
            "chainer.using_config",
            "Image.fromarray",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "seed",
            "Variable",
            "xp.asarray",
            "gen.make_hidden",
            "chainer.using_config",
            "gen",
            "to_cpu",
            "seed",
            "np.asarray",
            "np.clip",
            "x.reshape",
            "x.transpose",
            "x.reshape",
            "exists",
            "save",
            "Image.fromarray"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "cupy/cupy",
        "file_path": "cupyx/jit/_cuda_typerules.py",
        "function_declaration": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool",
        "start_line": "141",
        "end_line": "144",
        "docstring": "# This function checks if a data type conversion is possible on CUDA-enabled devices.\\nIt converts the input data types from_dtype and to_dtype to their numpy dtype equivalents.\\nIt then compares the type characters of the data types using their positions in a predefined string _typechars.\\nThe function returns True if the from_dtype can be cast to the to_dtype based on their positions in _typechars; otherwise, it returns False.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fc2162d43aee",
        "ground_truth": "def _cuda_can_cast(from_dtype: npt.DTypeLike, to_dtype: npt.DTypeLike) -> bool:\n    from_dtype = numpy.dtype(from_dtype)\n    to_dtype = numpy.dtype(to_dtype)\n    return _typechars.find(from_dtype.char) <= _typechars.find(to_dtype.char)",
        "import_statements": [
            "import ast",
            "from typing import Any, Callable, Mapping, Optional, Tuple, Type",
            "import numpy",
            "import operator",
            "import cupy",
            "from cupy._logic import ops",
            "from cupy._math import arithmetic",
            "from cupy._logic import comparison",
            "from cupy._binary import elementwise",
            "from cupy import _core",
            "from cupyx.jit import _cuda_types"
        ],
        "reference_api": [
            "_typechars.find",
            "numpy.dtype"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "numpy.dtype",
            "numpy.dtype",
            "_typechars.find",
            "_typechars.find"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "DistrictDataLabs/yellowbrick",
        "file_path": "yellowbrick/draw.py",
        "function_declaration": "def manual_legend(g, labels, colors, **legend_kwargs)",
        "start_line": "34",
        "end_line": "94",
        "docstring": "# This function manually creates a legend for a given plot using specified labels and colors.\\nIt first obtains the matplotlib Axes object from the input, which can be a Visualizer, None, or an Axes object.\\nThe function checks that the lengths of the labels and colors lists match, raising an error if they do not.\\nIt creates legend handles by pairing each label with its corresponding color using patches.Patch.\\nFinally, it returns the legend object created with the specified legend handles and additional keyword arguments.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5b8e2d5e6e12",
        "ground_truth": "def manual_legend(g, labels, colors, **legend_kwargs):\n    \"\"\"\n    Adds a manual legend for a scatter plot to the visualizer where the labels\n    and associated colors are drawn with circle patches instead of determining\n    them from the labels of the artist objects on the axes. This helper is\n    used either when there are a lot of duplicate labels, no labeled artists,\n    or when the color of the legend doesn't exactly match the color in the\n    figure (e.g. because of the use of transparency).\n     Parameters\n    ----------\n    g : Visualizer or Axes object\n        The graph to draw the legend on, either a Visualizer or a matplotlib\n        Axes object. If None, the current axes are drawn on, but this is not\n        recommended.\n     labels : list of str\n        The text labels to associate with the legend. Note that the labels\n        will be added to the legend in the order specified.\n     colors : list of colors\n        A list of any valid matplotlib color reference. The number of colors\n        specified must be equal to the number of labels.\n     legend_kwargs : dict\n        Any additional keyword arguments to pass to the legend.\n     Returns\n    -------\n    legend: Legend artist\n        The artist created by the ax.legend() call, returned for further\n        manipulation if required by the caller.\n     Notes\n    -----\n    Right now this method simply draws the patches as rectangles and cannot\n    take into account the line or scatter plot properties (e.g. line style or\n    marker style). It is possible to add Line2D patches to the artist that do\n    add manual styles like this, which we can explore in the future.\n     .. seealso:: https://matplotlib.org/gallery/text_labels_and_annotations/custom_legends.html\n    \"\"\"\n    # Get access to the matplotlib Axes\n    if isinstance(g, Visualizer):\n        g = g.ax\n    elif g is None:\n        g = plt.gca()\n     # Ensure that labels and colors are the same length to prevent odd behavior.\n    if len(colors) != len(labels):\n        raise YellowbrickValueError(\n            \"please specify the same number of colors as labels!\"\n        )\n     # Create the legend handles with the associated colors and labels\n    handles = [\n        patches.Patch(color=color, label=label) for color, label in zip(colors, labels)\n    ]\n     # Return the Legend artist\n    return g.legend(handles=handles, **legend_kwargs)",
        "import_statements": [
            "from matplotlib import patches"
        ],
        "reference_api": [
            "len",
            "patches.Patch",
            "isinstance",
            "YellowbrickValueError",
            "zip",
            "g.legend",
            "plt.gca"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "plt.gca",
            "YellowbrickValueError",
            "patches.Patch",
            "g.legend"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "DistrictDataLabs/yellowbrick",
        "file_path": "docs/api/features/pcoords_benchmark.py",
        "function_declaration": "def pcoords_time(X, y, fast=True)",
        "start_line": "10",
        "end_line": "22",
        "docstring": "# This function measures the time taken to fit and transform data using the ParallelCoordinates visualizer.\\nIt creates a plot axis and initializes the ParallelCoordinates visualizer with the specified speed setting.\\nThe function records the start time, fits and transforms the input data X and y with the visualizer, and calculates the elapsed time.\\nIt then clears and closes the current plot to free up resources.\\nThe function returns the elapsed time for the fit and transform process.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "494e7f719eb6",
        "ground_truth": "def pcoords_time(X, y, fast=True):\n    _, ax = plt.subplots()\n    oz = ParallelCoordinates(fast=fast, ax=ax)\n    start = time.time()\n    oz.fit_transform(X, y)\n    delta = time.time() - start\n    plt.cla()  # clear current axis\n    plt.clf()  # clear current figure\n    plt.close(\"all\")  # close all existing plots\n    return delta",
        "import_statements": [
            "import time",
            "from sklearn.datasets import load_iris",
            "from yellowbrick.features import ParallelCoordinates"
        ],
        "reference_api": [
            "plt.subplots",
            "plt.clf",
            "plt.close",
            "oz.fit_transform",
            "ParallelCoordinates",
            "plt.cla",
            "time.time"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "plt.subplots",
            "ParallelCoordinates",
            "oz.fit_transform",
            "plt.cla",
            "plt.clf",
            "plt.close"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "DistrictDataLabs/yellowbrick",
        "file_path": "yellowbrick/features/pcoords.py",
        "function_declaration": "def draw_instances(self, X, y, **kwargs):",
        "start_line": "432",
        "end_line": "465",
        "docstring": "# This function draws instances of data on a plot using provided features X and labels y.\\nIt retrieves the alpha transparency value from the object's attributes or defaults to 0.25.\\nFor each instance, it extracts the feature vector and corresponding label, determines the color based on the label, and plots the feature vector against a predefined increment sequence on the axis.\\nThe function returns the axis object with the plotted instances.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "db4143bc029f",
        "ground_truth": "def draw_instances(self, X, y, **kwargs):\n    \"\"\"\n    Draw the instances colored by the target y such that each line is a\n    single instance. This is the \"slow\" mode of drawing, since each\n    instance has to be drawn individually. However, in so doing, the\n    density of instances in braids is more apparent since lines have an\n    independent alpha that is compounded in the figure.\n    This is the default method of drawing.\n    Parameters\n    ----------\n    X : ndarray of shape n x m\n        A matrix of n instances with m features\n    y : ndarray of length n\n        An array or series of target or class values\n    Notes\n    -----\n    This method can be used to draw additional instances onto the parallel\n    coordinates before the figure is finalized.\n    \"\"\"\n    # Get alpha from param or default\n    alpha = self.alpha or 0.25\n    for idx in range(len(X)):\n        Xi = X[idx]\n        yi = y[idx]\n        color = self.get_colors([yi])[0]\n        self.ax.plot(self._increments, Xi, color=color, alpha=alpha, **kwargs)\n    return self.ax",
        "import_statements": [
            "from numpy.random import RandomState",
            "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler",
            "from sklearn.preprocessing import Normalizer, StandardScaler",
            "from yellowbrick.draw import manual_legend",
            "from yellowbrick.features.base import DataVisualizer",
            "from yellowbrick.utils import is_dataframe, is_series",
            "from yellowbrick.exceptions import YellowbrickTypeError, YellowbrickValueError"
        ],
        "reference_api": [
            "self.get_colors",
            "plot",
            "len",
            "range"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.get_colors",
                "code": "def get_colors(self, y):\n        \"\"\"\n        Returns the color for the specified value(s) of y based on the learned\n        colors property for any specified target type.\n\n        Parameters\n        ----------\n        y : array-like\n            The values of y to get the associated colors for.\n\n        Returns\n        -------\n        colors : list\n            Returns a list of colors for each value in y.\n        \"\"\"\n        if self._colors is None:\n            raise NotFitted(\"cannot determine colors on unfitted visualizer\")\n\n        if self._target_color_type == TargetType.SINGLE:\n            return [self._colors] * len(y)\n\n        if self._target_color_type == TargetType.DISCRETE:\n            try:\n                # Use the label encoder to get the class name (or use the value\n                # if the label is not mapped in the encoder) then use the class\n                # name to get the color from the color map.\n                return [self._colors[self._label_encoder.get(yi, yi)] for yi in y]\n            except KeyError:\n                unknown = set(y) - set(self._label_encoder.keys())\n                unknown = \", \".join([\"'{}'\".format(uk) for uk in unknown])\n                raise YellowbrickKeyError(\n                    \"could not determine color for classes {}\".format(unknown)\n                )\n\n        if self._target_color_type == TargetType.CONTINUOUS:\n            # Normalize values into target range and compute colors from colormap\n            norm = Normalize(*self.range_)\n            return self._colors(norm(y))\n\n        # This is a developer error, we should never get here!\n        raise YellowbrickValueError(\n            \"unknown target color type '{}'\".format(self._target_color_type)\n        )"
            }
        ],
        "third_party": [
            "plot"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "DistrictDataLabs/yellowbrick",
        "file_path": "yellowbrick/features/pcoords.py",
        "function_declaration": "def draw_classes(self, X, y, **kwargs):",
        "start_line": "467",
        "end_line": "512",
        "docstring": "# This function draws class-specific line plots for the provided data X and labels y on a given axis.\\nIt sets the alpha transparency value from the object's attributes or defaults to 0.5.\\nIt adds a column of ones to X to separate the instances visually and appends None to the increments list to create breaks between instances.\\nThe function identifies unique class labels in y and iterates over them.\\nFor each class, it determines the color, selects the instances of X belonging to that class, and plots them as a single line plot with the specified color and alpha transparency.\\nThe function returns the axis object with the plotted classes.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "676433312ffe",
        "ground_truth": "def draw_classes(self, X, y, **kwargs):\n    \"\"\"\n    Draw the instances colored by the target y such that each line is a\n    single class. This is the \"fast\" mode of drawing, since the number of\n    lines drawn equals the number of classes, rather than the number of\n    instances. However, this drawing method sacrifices inter-class density\n    of points using the alpha parameter.\n    Parameters\n    ----------\n    X : ndarray of shape n x m\n        A matrix of n instances with m features\n    y : ndarray of length n\n        An array or series of target or class values\n    \"\"\"\n    # Get alpha from param or default\n    alpha = self.alpha or 0.5\n    # Prepare to flatten data within each class:\n    #   introduce separation between individual data points using None in\n    #   x-values and arbitrary value (one) in y-values\n    X_separated = np.hstack([X, np.ones((X.shape[0], 1))])\n    increments_separated = self._increments.tolist()\n    increments_separated.append(None)\n    # Get the classes that exist in the dataset, y\n    y_values = np.unique(y)\n    # Plot each class as a single line plot\n    for yi in y_values:\n        color = self.get_colors([yi])[0]\n        X_in_class = X_separated[y == yi, :]\n        increments_in_class = increments_separated * len(X_in_class)\n        if len(X_in_class) > 0:\n            self.ax.plot(\n                increments_in_class,\n                X_in_class.flatten(),\n                linewidth=1,\n                color=color,\n                alpha=alpha,\n                **kwargs\n            )\n    return self.ax",
        "import_statements": [
            "from numpy.random import RandomState",
            "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler",
            "from sklearn.preprocessing import Normalizer, StandardScaler",
            "from yellowbrick.draw import manual_legend",
            "from yellowbrick.features.base import DataVisualizer",
            "from yellowbrick.utils import is_dataframe, is_series",
            "from yellowbrick.exceptions import YellowbrickTypeError, YellowbrickValueError"
        ],
        "reference_api": [
            "self.get_colors",
            "np.ones",
            "tolist",
            "increments_separated.append",
            "np.hstack",
            "len",
            "np.unique",
            "plot",
            "X_in_class.flatten"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.get_colors",
                "code": "def get_colors(self, y):\n        \"\"\"\n        Returns the color for the specified value(s) of y based on the learned\n        colors property for any specified target type.\n\n        Parameters\n        ----------\n        y : array-like\n            The values of y to get the associated colors for.\n\n        Returns\n        -------\n        colors : list\n            Returns a list of colors for each value in y.\n        \"\"\"\n        if self._colors is None:\n            raise NotFitted(\"cannot determine colors on unfitted visualizer\")\n\n        if self._target_color_type == TargetType.SINGLE:\n            return [self._colors] * len(y)\n\n        if self._target_color_type == TargetType.DISCRETE:\n            try:\n                # Use the label encoder to get the class name (or use the value\n                # if the label is not mapped in the encoder) then use the class\n                # name to get the color from the color map.\n                return [self._colors[self._label_encoder.get(yi, yi)] for yi in y]\n            except KeyError:\n                unknown = set(y) - set(self._label_encoder.keys())\n                unknown = \", \".join([\"'{}'\".format(uk) for uk in unknown])\n                raise YellowbrickKeyError(\n                    \"could not determine color for classes {}\".format(unknown)\n                )\n\n        if self._target_color_type == TargetType.CONTINUOUS:\n            # Normalize values into target range and compute colors from colormap\n            norm = Normalize(*self.range_)\n            return self._colors(norm(y))\n\n        # This is a developer error, we should never get here!\n        raise YellowbrickValueError(\n            \"unknown target color type '{}'\".format(self._target_color_type)\n        )"
            }
        ],
        "third_party": [
            "np.hstack",
            "np.ones",
            "tolist",
            "increments_separated.append",
            "np.unique",
            "plot",
            "X_in_class.flatten"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "DistrictDataLabs/yellowbrick",
        "file_path": "paper/figures/figures.py",
        "function_declaration": "def feature_analysis(fname=\"feature_analysis.png\")",
        "start_line": "78",
        "end_line": "101",
        "docstring": "# This function creates and saves a feature analysis figure consisting of two subplots.\\nIt sets up a side-by-side axes grid with a specified figure size.\\nOn the left subplot, it performs RadViz analysis using occupancy data, fitting and finalizing the visualization.\\nOn the right subplot, it performs Rank2D analysis using concrete data, fitting, transforming, and finalizing the visualization.\\nFinally, it saves the resulting figure to a specified file path.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9d297b12a673",
        "ground_truth": "def feature_analysis(fname=\"feature_analysis.png\"):\n    \"\"\"\n    Create figures for feature analysis\n    \"\"\"\n     # Create side-by-side axes grid\n    _, axes = plt.subplots(ncols=2, figsize=(18, 6))\n     # Draw RadViz on the left\n    data = load_occupancy(split=False)\n    oz = RadViz(ax=axes[0], classes=[\"unoccupied\", \"occupied\"])\n    oz.fit(data.X, data.y)\n    oz.finalize()\n     # Draw Rank2D on the right\n    data = load_concrete(split=False)\n    oz = Rank2D(ax=axes[1])\n    oz.fit_transform(data.X, data.y)\n    oz.finalize()\n     # Save figure\n    path = os.path.join(FIGURES, fname)\n    plt.tight_layout()\n    plt.savefig(path)",
        "import_statements": [
            "import os",
            "import argparse",
            "from yellowbrick.features import Rank2D, RadViz",
            "from yellowbrick.model_selection import LearningCurve",
            "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer",
            "from yellowbrick.classifier import ClassificationReport, DiscriminationThreshold",
            "from yellowbrick.regressor import ResidualsPlot, PredictionError, AlphaSelection",
            "from collections import namedtuple",
            "from sklearn.datasets import make_blobs",
            "from sklearn.naive_bayes import MultinomialNB",
            "from sklearn.ensemble import RandomForestRegressor",
            "from sklearn.cluster import MiniBatchKMeans, Birch",
            "from sklearn.model_selection import train_test_split as tts",
            "from sklearn.linear_model import LassoCV, RidgeCV, LogisticRegression"
        ],
        "reference_api": [
            "plt.subplots",
            "join",
            "RadViz",
            "plt.savefig",
            "load_concrete",
            "oz.fit",
            "Rank2D",
            "plt.tight_layout",
            "oz.fit_transform",
            "load_occupancy",
            "oz.finalize"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "load_occupancy",
                "code": "def load_occupancy(split=False):\n    \"\"\"\n    Create a dataset for the specified yb dataset\n    \"\"\"\n    path = os.path.join(DATA, \"occupancy\", \"occupancy.csv\")\n    data = pd.read_csv(path)\n\n    X = data[[\"temperature\", \"relative humidity\", \"light\", \"C02\", \"humidity\"]]\n    y = data[\"occupancy\"]\n    return _make_dataset(X, y, split)"
            },
            {
                "name": "load_concrete",
                "code": "def load_concrete(split=False):\n    path = os.path.join(DATA, \"concrete\", \"concrete.csv\")\n    data = pd.read_csv(path)\n\n    X = data[[\"cement\", \"slag\", \"ash\", \"water\", \"splast\", \"coarse\", \"fine\", \"age\"]]\n    y = data[\"strength\"]\n    return _make_dataset(X, y, split)"
            }
        ],
        "third_party": [
            "plt.subplots",
            "RadViz",
            "oz.fit",
            "oz.finalize",
            "Rank2D",
            "oz.fit_transform",
            "oz.finalize",
            "join",
            "plt.tight_layout",
            "plt.savefig"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "DistrictDataLabs/yellowbrick",
        "file_path": "paper/figures/figures.py",
        "function_declaration": "def regression(fname=\"regression.png\")",
        "start_line": "104",
        "end_line": "127",
        "docstring": "# This function generates and saves regression model evaluation figures.\\nIt creates a figure with two subplots of size 18x6 inches and defines a range of alpha values for regularization.\\nThe function loads the concrete dataset and splits it into training and testing sets.\\nIn the first subplot, it plots the prediction error using LassoCV with the specified alphas, fitting the model on the training data and scoring it on the testing data.\\nIn the second subplot, it plots the residuals using RidgeCV with the same alphas, fitting and scoring in a similar manner.\\nFinally, it adjusts the layout, saves the figure to a specified path, and returns the path.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fd1b4b9cd7ef",
        "ground_truth": "def regression(fname=\"regression.png\"):\n    \"\"\"\n    Create figures for regression models\n    \"\"\"\n    _, axes = plt.subplots(ncols=2, figsize=(18, 6))\n    alphas = np.logspace(-10, 1, 300)\n    data = load_concrete(split=True)\n     # Plot prediction error in the middle\n    oz = PredictionError(LassoCV(alphas=alphas), ax=axes[0])\n    oz.fit(data.X.train, data.y.train)\n    oz.score(data.X.test, data.y.test)\n    oz.finalize()\n     # Plot residuals on the right\n    oz = ResidualsPlot(RidgeCV(alphas=alphas), ax=axes[1])\n    oz.fit(data.X.train, data.y.train)\n    oz.score(data.X.test, data.y.test)\n    oz.finalize()\n     # Save figure\n    path = os.path.join(FIGURES, fname)\n    plt.tight_layout()\n    plt.savefig(path)",
        "import_statements": [
            "import os",
            "import argparse",
            "from yellowbrick.features import Rank2D, RadViz",
            "from yellowbrick.model_selection import LearningCurve",
            "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer",
            "from yellowbrick.classifier import ClassificationReport, DiscriminationThreshold",
            "from yellowbrick.regressor import ResidualsPlot, PredictionError, AlphaSelection",
            "from collections import namedtuple",
            "from sklearn.datasets import make_blobs",
            "from sklearn.naive_bayes import MultinomialNB",
            "from sklearn.ensemble import RandomForestRegressor",
            "from sklearn.cluster import MiniBatchKMeans, Birch",
            "from sklearn.model_selection import train_test_split as tts",
            "from sklearn.linear_model import LassoCV, RidgeCV, LogisticRegression"
        ],
        "reference_api": [
            "plt.subplots",
            "join",
            "oz.score",
            "plt.savefig",
            "PredictionError",
            "load_concrete",
            "np.logspace",
            "RidgeCV",
            "oz.finalize",
            "plt.tight_layout",
            "LassoCV",
            "ResidualsPlot",
            "oz.fit"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "load_concrete",
                "code": "def load_concrete(split=False):\n    path = os.path.join(DATA, \"concrete\", \"concrete.csv\")\n    data = pd.read_csv(path)\n\n    X = data[[\"cement\", \"slag\", \"ash\", \"water\", \"splast\", \"coarse\", \"fine\", \"age\"]]\n    y = data[\"strength\"]\n    return _make_dataset(X, y, split)"
            }
        ],
        "third_party": [
            "plt.subplots",
            "np.logspace",
            "PredictionError",
            "LassoCV",
            "oz.fit",
            "oz.score",
            "oz.finalize",
            "ResidualsPlot",
            "RidgeCV",
            "oz.fit",
            "oz.score",
            "oz.finalize",
            "join",
            "plt.tight_layout",
            "plt.savefig"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "DistrictDataLabs/yellowbrick",
        "file_path": "paper/figures/figures.py",
        "function_declaration": "def classification(fname=\"classification.png\")",
        "start_line": "130",
        "end_line": "151",
        "docstring": "# This function generates and saves a classification visualization composed of two plots side-by-side.\\nIt creates a grid of two axes with a specified figure size.\\nOn the left axis, it loads spam dataset with a train-test split, fits a MultinomialNB classifier, and displays a ClassificationReport.\\nOn the right axis, it loads the entire spam dataset, fits a LogisticRegression model, and displays a DiscriminationThreshold plot.\\nBoth visualizations are finalized, the layout is adjusted, and the combined figure is saved to a specified file path.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "627126918f88",
        "ground_truth": "def classification(fname=\"classification.png\"):\n     # Create side-by-side axes grid\n    _, axes = plt.subplots(ncols=2, figsize=(18, 6))\n     # Add ClassificationReport to the reft\n    data = load_spam(split=True)\n    oz = ClassificationReport(MultinomialNB(), classes=[\"ham\", \"spam\"], ax=axes[0])\n    oz.fit(data.X.train, data.y.train)\n    oz.score(data.X.test, data.y.test)\n    oz.finalize()\n     # Add DiscriminationThreshold to the right\n    data = load_spam(split=False)\n    oz = DiscriminationThreshold(LogisticRegression(), ax=axes[1])\n    oz.fit(data.X, data.y)\n    oz.finalize()\n     # Save figure\n    path = os.path.join(FIGURES, fname)\n    plt.tight_layout()\n    plt.savefig(path)",
        "import_statements": [
            "import os",
            "import argparse",
            "from yellowbrick.features import Rank2D, RadViz",
            "from yellowbrick.model_selection import LearningCurve",
            "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer",
            "from yellowbrick.classifier import ClassificationReport, DiscriminationThreshold",
            "from yellowbrick.regressor import ResidualsPlot, PredictionError, AlphaSelection",
            "from collections import namedtuple",
            "from sklearn.datasets import make_blobs",
            "from sklearn.naive_bayes import MultinomialNB",
            "from sklearn.ensemble import RandomForestRegressor",
            "from sklearn.cluster import MiniBatchKMeans, Birch",
            "from sklearn.model_selection import train_test_split as tts",
            "from sklearn.linear_model import LassoCV, RidgeCV, LogisticRegression"
        ],
        "reference_api": [
            "plt.subplots",
            "join",
            "oz.score",
            "plt.savefig",
            "load_spam",
            "ClassificationReport",
            "oz.finalize",
            "plt.tight_layout",
            "LogisticRegression",
            "DiscriminationThreshold",
            "MultinomialNB",
            "oz.fit"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "load_spam",
                "code": "def load_spam(split=False):\n    path = os.path.join(DATA, \"spam\", \"spam.csv\")\n    data = pd.read_csv(path)\n\n    target = \"is_spam\"\n    features = [col for col in data.columns if col != target]\n\n    X = data[features]\n    y = data[target]\n    return _make_dataset(X, y, split)"
            },
            {
                "name": "load_spam",
                "code": "def load_spam(split=False):\n    path = os.path.join(DATA, \"spam\", \"spam.csv\")\n    data = pd.read_csv(path)\n\n    target = \"is_spam\"\n    features = [col for col in data.columns if col != target]\n\n    X = data[features]\n    y = data[target]\n    return _make_dataset(X, y, split)"
            }
        ],
        "third_party": [
            "plt.subplots",
            "ClassificationReport",
            "MultinomialNB",
            "oz.fit",
            "oz.score",
            "oz.finalize",
            "DiscriminationThreshold",
            "LogisticRegression",
            "oz.fit",
            "oz.finalize",
            "join",
            "plt.tight_layout",
            "plt.savefig"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "DistrictDataLabs/yellowbrick",
        "file_path": "yellowbrick/style/colors.py",
        "function_declaration": "def get_color_cycle()",
        "start_line": "43",
        "end_line": "56",
        "docstring": "# This function retrieves the current color cycle from matplotlib.\\nIf the matplotlib version is 1.5.0 or greater, it attempts to get the color cycle from the \"axes.prop_cycle\" parameter.\\nIf successful, it returns a list of colors from this cycle.\\nIf there is a KeyError or if the matplotlib version is below 1.5.0, it falls back to returning the color cycle from the \"axes.color_cycle\" parameter.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "318d3a674694",
        "ground_truth": "def get_color_cycle():\n    \"\"\"\n    Returns the current color cycle from matplotlib.\n    \"\"\"\n    if mpl_ge_150:\n        cyl = mpl.rcParams[\"axes.prop_cycle\"]\n        # matplotlib 1.5 verifies that axes.prop_cycle *is* a cycler\n        # but no garuantee that there's a `color` key.\n        # so users could have a custom rcParams w/ no color...\n        try:\n            return [x[\"color\"] for x in cyl]\n        except KeyError:\n            pass  # just return axes.color style below\n    return mpl.rcParams[\"axes.color_cycle\"]",
        "import_statements": [
            "import random",
            "import warnings",
            "from copy import copy",
            "from yellowbrick.exceptions import YellowbrickValueError",
            "from distutils.version import LooseVersion"
        ],
        "reference_api": [],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "DistrictDataLabs/yellowbrick",
        "file_path": "yellowbrick/utils/kneed.py",
        "function_declaration": "def plot_knee_normalized(\n        self,\n    )",
        "start_line": "262",
        "end_line": "281",
        "docstring": "# This function plots a normalized curve along with a distance curve and the knee point if it exists.\\nIt imports the matplotlib.pyplot module for plotting.\\nThe plot is created with a figure size of 8x8 inches.\\nIt plots the normalized x and y values, followed by the distance curve in red.\\nThe x and y ticks are set at intervals of 0.1 based on the minimum and maximum values of the respective data.\\nA vertical line is drawn at the knee point to highlight it on the plot.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f3fac557334c",
        "ground_truth": "def plot_knee_normalized(\n    self,\n):\n    \"\"\"\n    Plots the normalized curve, the distance curve (x_distance, y_normalized) and the\n    knee, if it exists.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x_normalized, self.y_normalized)\n    plt.plot(self.x_difference, self.y_difference, \"r\")\n    plt.xticks(\n        np.arange(self.x_normalized.min(), self.x_normalized.max() + 0.1, 0.1)\n    )\n    plt.yticks(\n        np.arange(self.y_difference.min(), self.y_normalized.max() + 0.1, 0.1)\n    )\n    plt.vlines(self.norm_knee, plt.ylim()[0], plt.ylim()[1])",
        "import_statements": [
            "from scipy import interpolate",
            "from scipy.signal import argrelextrema",
            "import warnings",
            "from yellowbrick.exceptions import YellowbrickWarning"
        ],
        "reference_api": [
            "min",
            "plt.figure",
            "plt.ylim",
            "plt.vlines",
            "plt.yticks",
            "plt.xticks",
            "max",
            "np.arange",
            "plt.plot"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "plt.figure",
            "plt.plot",
            "plt.plot",
            "plt.xticks",
            "np.arange",
            "plt.yticks",
            "np.arange",
            "plt.vlines",
            "plt.ylim",
            "plt.ylim"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "DistrictDataLabs/yellowbrick",
        "file_path": "yellowbrick/utils/kneed.py",
        "function_declaration": "def plot_knee(\n        self,\n    )",
        "start_line": "283",
        "end_line": "294",
        "docstring": "# This function plots a curve and its knee point, if it exists.\\nIt uses matplotlib to create a plot with a specified figure size of 8x8 inches.\\nThe function plots the data points defined by self.x and self.y.\\nIt then draws a vertical line at the knee point along the y-axis limits.\\nThe plot is displayed with the curve and the knee point highlighted.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b2d48aae4fce",
        "ground_truth": "def plot_knee(\n    self,\n):\n    \"\"\"\n    Plot the curve and the knee, if it exists\n    \"\"\"\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(8, 8))\n    plt.plot(self.x, self.y)\n    plt.vlines(self.knee, plt.ylim()[0], plt.ylim()[1])",
        "import_statements": [
            "from scipy import interpolate",
            "from scipy.signal import argrelextrema",
            "import warnings",
            "from yellowbrick.exceptions import YellowbrickWarning"
        ],
        "reference_api": [
            "plt.ylim",
            "plt.vlines",
            "plt.plot",
            "plt.figure"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "plt.figure",
            "plt.plot",
            "plt.vlines",
            "plt.ylim",
            "plt.ylim"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "donnemartin/data-science-ipython-notebooks",
        "file_path": "scikit-learn/fig_code/figures.py",
        "function_declaration": "def plot_pca_interactive(data, n_components=6)",
        "start_line": "222",
        "end_line": "233",
        "docstring": "# This function performs PCA on the given data and provides an interactive plot to visualize the PCA components.\\nIt imports the necessary PCA module from sklearn and interact from IPython widgets.\\nThe function applies PCA to the data with a specified number of components and transforms the data accordingly.\\nAn internal function, show_decomp, is defined to plot the original data and its PCA components for a selected index.\\nThe interact function creates an interactive widget to select the index, updating the plot to show the PCA decomposition for the chosen data instance.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4450ba2384c9",
        "ground_truth": "def plot_pca_interactive(data, n_components=6):\n    from sklearn.decomposition import PCA\n    from IPython.html.widgets import interact\n     pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n     def show_decomp(i=0):\n        plot_image_components(data[i], Xproj[i],\n                              pca.mean_, pca.components_)\n         interact(show_decomp, i=(0, data.shape[0] - 1));",
        "import_statements": [
            "import warnings"
        ],
        "reference_api": [
            "plot_image_components"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "plot_image_components",
                "code": "def plot_image_components(x, coefficients=None, mean=0, components=None,\n                          imshape=(8, 8), n_components=6, fontsize=12):\n    if coefficients is None:\n        coefficients = x\n        \n    if components is None:\n        components = np.eye(len(coefficients), len(x))\n        \n    mean = np.zeros_like(x) + mean\n        \n\n    fig = plt.figure(figsize=(1.2 * (5 + n_components), 1.2 * 2))\n    g = plt.GridSpec(2, 5 + n_components, hspace=0.3)\n\n    def show(i, j, x, title=None):\n        ax = fig.add_subplot(g[i, j], xticks=[], yticks=[])\n        ax.imshow(x.reshape(imshape), interpolation='nearest')\n        if title:\n            ax.set_title(title, fontsize=fontsize)\n\n    show(slice(2), slice(2), x, \"True\")\n\n    approx = mean.copy()\n    show(0, 2, np.zeros_like(x) + mean, r'$\\mu$')\n    show(1, 2, approx, r'$1 \\cdot \\mu$')\n\n    for i in range(0, n_components):\n        approx = approx + coefficients[i] * components[i]\n        show(0, i + 3, components[i], r'$c_{0}$'.format(i + 1))\n        show(1, i + 3, approx,\n             r\"${0:.2f} \\cdot c_{1}$\".format(coefficients[i], i + 1))\n        plt.gca().text(0, 1.05, '$+$', ha='right', va='bottom',\n                       transform=plt.gca().transAxes, fontsize=fontsize)\n\n    show(slice(2), slice(-2, None), approx, \"Approx\")"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "donnemartin/data-science-ipython-notebooks",
        "file_path": "scikit-learn/fig_code/helpers.py",
        "function_declaration": "def plot_iris_knn()",
        "start_line": "14",
        "end_line": "38",
        "docstring": "# This function plots the decision boundaries of a k-Nearest Neighbors (kNN) classifier on the Iris dataset using the first two features.\\nIt loads the Iris dataset and extracts the first two features and the target labels.\\nA kNN classifier with 5 neighbors is trained on this data.\\nThe function defines a mesh grid over the feature space and predicts the class for each point in the grid using the trained kNN model.\\nThe predictions are reshaped and displayed as a color plot representing the decision boundaries.\\nAdditionally, the training points are plotted on top of the decision boundary plot with labeled axes for sepal length and sepal width.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b09a163f3ff1",
        "ground_truth": "def plot_iris_knn():\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features. We could\n                        # avoid this ugly slicing by using a two-dim dataset\n    y = iris.target\n     knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n    knn.fit(X, y)\n     x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n     # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    pl.figure()\n    pl.pcolormesh(xx, yy, Z, cmap=cmap_light)\n     # Plot also the training points\n    pl.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    pl.xlabel('sepal length (cm)')\n    pl.ylabel('sepal width (cm)')\n    pl.axis('tight')",
        "import_statements": [
            "from sklearn import neighbors, datasets, linear_model",
            "from matplotlib.colors import ListedColormap"
        ],
        "reference_api": [
            "np.meshgrid",
            "min",
            "knn.predict",
            "yy.ravel",
            "xx.ravel",
            "pl.figure",
            "pl.axis",
            "datasets.load_iris",
            "neighbors.KNeighborsClassifier",
            "pl.xlabel",
            "knn.fit",
            "Z.reshape",
            "pl.ylabel",
            "max",
            "pl.pcolormesh",
            "np.linspace",
            "pl.scatter"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "datasets.load_iris",
            "neighbors.KNeighborsClassifier",
            "knn.fit",
            "np.meshgrid",
            "np.linspace",
            "np.linspace",
            "knn.predict",
            "xx.ravel",
            "yy.ravel",
            "Z.reshape",
            "pl.figure",
            "pl.pcolormesh",
            "pl.scatter",
            "pl.xlabel",
            "pl.ylabel",
            "pl.axis"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "donnemartin/data-science-ipython-notebooks",
        "file_path": "scikit-learn/fig_code/sgd_separator.py",
        "function_declaration": "def plot_sgd_separator()",
        "start_line": "6",
        "end_line": "35",
        "docstring": "# This function plots the decision boundary of an SGD classifier.\\nIt creates 50 separable data points using the make_blobs function with two centers and a specified standard deviation.\\nThe SGD classifier is instantiated with a hinge loss function, an alpha value of 0.01, 200 iterations, and fit_intercept set to True, then fitted to the generated data.\\nThe function prepares a meshgrid of values for plotting the decision boundary and evaluates the decision function at each point in the grid.\\nIt uses contour plotting to visualize the decision boundary at levels -1.0, 0.0, and 1.0 with different line styles.\\nFinally, it scatters the original data points on the plot and sets the axis to 'tight'.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8eaee3207c09",
        "ground_truth": "def plot_sgd_separator():\n    # we create 50 separable points\n    X, Y = make_blobs(n_samples=50, centers=2,\n                      random_state=0, cluster_std=0.60)\n     # fit the model\n    clf = SGDClassifier(loss=\"hinge\", alpha=0.01,\n                        n_iter=200, fit_intercept=True)\n    clf.fit(X, Y)\n     # plot the line, the points, and the nearest vectors to the plane\n    xx = np.linspace(-1, 5, 10)\n    yy = np.linspace(-1, 5, 10)\n     X1, X2 = np.meshgrid(xx, yy)\n    Z = np.empty(X1.shape)\n    for (i, j), val in np.ndenumerate(X1):\n        x1 = val\n        x2 = X2[i, j]\n        p = clf.decision_function([x1, x2])\n        Z[i, j] = p[0]\n    levels = [-1.0, 0.0, 1.0]\n    linestyles = ['dashed', 'solid', 'dashed']\n    colors = 'k'\n     ax = plt.axes()\n    ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n    ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n     ax.axis('tight')",
        "import_statements": [
            "from sklearn.linear_model import SGDClassifier",
            "from sklearn.datasets.samples_generator import make_blobs"
        ],
        "reference_api": [
            "np.meshgrid",
            "plt.axes",
            "ax.axis",
            "np.ndenumerate",
            "clf.decision_function",
            "clf.fit",
            "make_blobs",
            "SGDClassifier",
            "np.linspace",
            "ax.scatter",
            "ax.contour",
            "np.empty"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "make_blobs",
            "SGDClassifier",
            "clf.fit",
            "np.linspace",
            "np.linspace",
            "np.meshgrid",
            "np.empty",
            "np.ndenumerate",
            "clf.decision_function",
            "plt.axes",
            "ax.contour",
            "ax.scatter",
            "ax.axis"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "donnemartin/data-science-ipython-notebooks",
        "file_path": "scikit-learn/fig_code/svm_gui.py",
        "function_declaration": "def dump_svmlight_file(self, file)",
        "start_line": "69",
        "end_line": "73",
        "docstring": "# This function exports data to a file in the SVMLight format.\\nIt converts the object's data attribute to a NumPy array, then separates the first two columns as features (X) and the third column as labels (y).\\nIt uses the dump_svmlight_file function to save the features and labels to the specified file.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8b54f999b4c0",
        "ground_truth": "def dump_svmlight_file(self, file):\n    data = np.array(self.data)\n    X = data[:, 0:2]\n    y = data[:, 2]\n    dump_svmlight_file(X, y, file)",
        "import_statements": [
            "import matplotlib",
            "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg",
            "from matplotlib.backends.backend_tkagg import NavigationToolbar2TkAgg",
            "from matplotlib.figure import Figure",
            "from matplotlib.contour import ContourSet",
            "import sys",
            "from sklearn import svm",
            "from sklearn.datasets import dump_svmlight_file",
            "from sklearn.externals.six.moves import xrange"
        ],
        "reference_api": [
            "dump_svmlight_file",
            "np.array"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "dump_svmlight_file",
                "code": "def dump_svmlight_file(self, file):\n        data = np.array(self.data)\n        X = data[:, 0:2]\n        y = data[:, 2]\n        dump_svmlight_file(X, y, file)"
            }
        ],
        "third_party": [
            "np.array"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "donnemartin/data-science-ipython-notebooks",
        "file_path": "scikit-learn/fig_code/svm_gui.py",
        "function_declaration": "def fit(self)",
        "start_line": "84",
        "end_line": "110",
        "docstring": "# This function fits a model using the SVM algorithm based on the data and parameters provided in the object's attributes.\\nIt first prints a message indicating the fitting process has started.\\nThe training data is extracted and split into features X and labels y.\\nSVM parameters such as complexity, gamma, coef0, degree, and kernel are retrieved from the object's attributes.\\nDepending on whether the labels are all the same, it initializes either a OneClassSVM or a regular SVC classifier with the specified kernel and parameters.\\nThe classifier is then fitted to the data.\\nIf the classifier has a score method, it prints the accuracy of the model on the training data.\\nThe decision surface is calculated and stored in the model's attributes.\\nThe model's surface type is updated, and a flag indicating the model has been fitted is set to True.\\nFinally, it triggers an event indicating that the model's surface has changed.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2419013adc6e",
        "ground_truth": "def fit(self):\n    print(\"fit the model\")\n    train = np.array(self.model.data)\n    X = train[:, 0:2]\n    y = train[:, 2]\n    C = float(self.complexity.get())\n    gamma = float(self.gamma.get())\n    coef0 = float(self.coef0.get())\n    degree = int(self.degree.get())\n    kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n    if len(np.unique(y)) == 1:\n        clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                              gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X)\n    else:\n        clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                      gamma=gamma, coef0=coef0, degree=degree)\n        clf.fit(X, y)\n    if hasattr(clf, 'score'):\n        print(\"Accuracy:\", clf.score(X, y) * 100)\n    X1, X2, Z = self.decision_surface(clf)\n    self.model.clf = clf\n    self.model.set_surface((X1, X2, Z))\n    self.model.surface_type = self.surface_type.get()\n    self.fitted = True\n    self.model.changed(\"surface\")",
        "import_statements": [
            "import matplotlib",
            "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg",
            "from matplotlib.backends.backend_tkagg import NavigationToolbar2TkAgg",
            "from matplotlib.figure import Figure",
            "from matplotlib.contour import ContourSet",
            "import sys",
            "from sklearn import svm",
            "from sklearn.datasets import dump_svmlight_file",
            "from sklearn.externals.six.moves import xrange"
        ],
        "reference_api": [
            "print",
            "svm.SVC",
            "set_surface",
            "float",
            "hasattr",
            "int",
            "len",
            "clf.fit",
            "np.unique",
            "get",
            "clf.score",
            "svm.OneClassSVM",
            "self.decision_surface",
            "changed",
            "np.array"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "clf.fit",
                "code": "def fit(self):\n        print(\"fit the model\")\n        train = np.array(self.model.data)\n        X = train[:, 0:2]\n        y = train[:, 2]\n\n        C = float(self.complexity.get())\n        gamma = float(self.gamma.get())\n        coef0 = float(self.coef0.get())\n        degree = int(self.degree.get())\n        kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n        if len(np.unique(y)) == 1:\n            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                                  gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X)\n        else:\n            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                          gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X, y)\n        if hasattr(clf, 'score'):\n            print(\"Accuracy:\", clf.score(X, y) * 100)\n        X1, X2, Z = self.decision_surface(clf)\n        self.model.clf = clf\n        self.model.set_surface((X1, X2, Z))\n        self.model.surface_type = self.surface_type.get()\n        self.fitted = True\n        self.model.changed(\"surface\")"
            },
            {
                "name": "clf.fit",
                "code": "def fit(self):\n        print(\"fit the model\")\n        train = np.array(self.model.data)\n        X = train[:, 0:2]\n        y = train[:, 2]\n\n        C = float(self.complexity.get())\n        gamma = float(self.gamma.get())\n        coef0 = float(self.coef0.get())\n        degree = int(self.degree.get())\n        kernel_map = {0: \"linear\", 1: \"rbf\", 2: \"poly\"}\n        if len(np.unique(y)) == 1:\n            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                                  gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X)\n        else:\n            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                          gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X, y)\n        if hasattr(clf, 'score'):\n            print(\"Accuracy:\", clf.score(X, y) * 100)\n        X1, X2, Z = self.decision_surface(clf)\n        self.model.clf = clf\n        self.model.set_surface((X1, X2, Z))\n        self.model.surface_type = self.surface_type.get()\n        self.fitted = True\n        self.model.changed(\"surface\")"
            },
            {
                "name": "self.decision_surface",
                "code": "def decision_surface(self, cls):\n        delta = 1\n        x = np.arange(x_min, x_max + delta, delta)\n        y = np.arange(y_min, y_max + delta, delta)\n        X1, X2 = np.meshgrid(x, y)\n        Z = cls.decision_function(np.c_[X1.ravel(), X2.ravel()])\n        Z = Z.reshape(X1.shape)\n        return X1, X2, Z"
            },
            {
                "name": "set_surface",
                "code": "def set_surface(self, surface):\n        self.surface = surface"
            },
            {
                "name": "changed",
                "code": "def changed(self, event):\n        \"\"\"Notify the observers. \"\"\"\n        for observer in self.observers:\n            observer.update(event, self)"
            }
        ],
        "third_party": [
            "np.array",
            "get",
            "get",
            "get",
            "get",
            "np.unique",
            "svm.OneClassSVM",
            "get",
            "svm.SVC",
            "get",
            "clf.score",
            "get"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/operator_utils.py",
        "function_declaration": "def source_decode(sourcecode, verbose=0)",
        "start_line": "47",
        "end_line": "90",
        "docstring": "# This function attempts to import and decode a given source code string.\\nIt splits the source code into its module and object components.\\nIf the source code starts with \"tpot.\", it imports the object from the corresponding module, removing the \"tpot.\" prefix.\\nOtherwise, it imports the object from the module directly.\\nIf the import fails and the verbose level is higher than 2, it raises an ImportError with the error details.\\nIf verbose is not higher than 2, it prints a warning message.\\nThe function returns the import string, object string, and the imported object (or None if the import failed).",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e887fd2a8513",
        "ground_truth": "def source_decode(sourcecode, verbose=0):\n    \"\"\"Decode operator source and import operator class.\n     Parameters\n    ----------\n    sourcecode: string\n        a string of operator source (e.g 'sklearn.feature_selection.RFE')\n    verbose: int, optional (default: 0)\n        How much information TPOT communicates while it's running.\n        0 = none, 1 = minimal, 2 = high, 3 = all.\n        if verbose > 2 then ImportError will rasie during initialization\n      Returns\n    -------\n    import_str: string\n        a string of operator class source (e.g. 'sklearn.feature_selection')\n    op_str: string\n        a string of operator class (e.g. 'RFE')\n    op_obj: object\n        operator class (e.g. RFE)\n     \"\"\"\n    tmp_path = sourcecode.split(\".\")\n    op_str = tmp_path.pop()\n    import_str = \".\".join(tmp_path)\n    try:\n        if sourcecode.startswith(\"tpot.\"):\n            exec(\"from {} import {}\".format(import_str[4:], op_str))\n        else:\n            exec(\"from {} import {}\".format(import_str, op_str))\n        op_obj = eval(op_str)\n    except Exception as e:\n        if verbose > 2:\n            raise ImportError(\"Error: could not import {}.\\n{}\".format(sourcecode, e))\n        else:\n            print(\n                \"Warning: {} is not available and will not be used by TPOT.\".format(\n                    sourcecode\n                )\n            )\n        op_obj = None\n     return import_str, op_str, op_obj",
        "import_statements": [
            "from sklearn.base import BaseEstimator, is_classifier, is_regressor",
            "from sklearn.gaussian_process.kernels import Kernel",
            "import inspect"
        ],
        "reference_api": [
            "join",
            "eval",
            "exec",
            "print",
            "sourcecode.startswith",
            "sourcecode.split",
            "ImportError",
            "tmp_path.pop",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "sourcecode.split",
            "tmp_path.pop",
            "join",
            "sourcecode.startswith"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/operator_utils.py",
        "function_declaration": "def set_sample_weight(pipeline_steps, sample_weight=None)",
        "start_line": "93",
        "end_line": "118",
        "docstring": "# This function sets sample weights for the steps in a machine learning pipeline.\\nIt initializes an empty dictionary for sample weights and checks if sample_weight is not None.\\nFor each step in the pipeline, it inspects the fit method to see if it accepts a \"sample_weight\" argument.\\nIf it does, it constructs a key by combining the step name and \"sample_weight\", and adds it to the dictionary.\\nThe function returns the sample weight dictionary if it is not empty, otherwise, it returns None.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5d6f629c3363",
        "ground_truth": "def set_sample_weight(pipeline_steps, sample_weight=None):\n    \"\"\"Recursively iterates through all objects in the pipeline and sets sample weight.\n     Parameters\n    ----------\n    pipeline_steps: array-like\n        List of (str, obj) tuples from a scikit-learn pipeline or related object\n    sample_weight: array-like\n        List of sample weight\n    Returns\n    -------\n    sample_weight_dict:\n        A dictionary of sample_weight\n     \"\"\"\n    sample_weight_dict = {}\n    if not isinstance(sample_weight, type(None)):\n        for (pname, obj) in pipeline_steps:\n            if inspect.getargspec(obj.fit).args.count(\"sample_weight\"):\n                step_sw = pname + \"__sample_weight\"\n                sample_weight_dict[step_sw] = sample_weight\n     if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None",
        "import_statements": [
            "from sklearn.base import BaseEstimator, is_classifier, is_regressor",
            "from sklearn.gaussian_process.kernels import Kernel",
            "import inspect"
        ],
        "reference_api": [
            "inspect.getargspec",
            "count",
            "type",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "count"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/base.py",
        "function_declaration": "def _summary_of_best_pipeline(self, features, target):",
        "start_line": "967",
        "end_line": "1020",
        "docstring": "# This function provides a summary of the best pipeline identified by TPOT optimization.\\nIt first checks if an optimized pipeline exists, raising a RuntimeError with a descriptive message if not.\\nIf an optimized pipeline is available, it compiles and fits the pipeline using the provided features and target, while ignoring warnings.\\nIf verbosity is set to 1 or 2, it prints the best pipeline string, adding extra spacing if verbosity is 2 or higher.\\nThe function also compiles and fits the entire Pareto front of pipelines, storing them as fitted models for convenience.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cae17d519818",
        "ground_truth": "def _summary_of_best_pipeline(self, features, target):\n    \"\"\"Print out best pipeline at the end of optimization process.\n    Parameters\n    ----------\n    features: array-like {n_samples, n_features}\n        Feature matrix\n    target: array-like {n_samples}\n        List of class labels for prediction\n    Returns\n    -------\n    self: object\n        Returns a copy of the fitted TPOT object\n    \"\"\"\n    if not self._optimized_pipeline:\n        raise RuntimeError(\n            \"There was an error in the TPOT optimization process. \"\n            \"This could be because the data was not formatted \"\n            \"properly (e.g. nan values became a third class), or \"\n            \"because data for a regression problem was provided \"\n            \"to the TPOTClassifier object. Please make sure you \"\n            \"passed the data to TPOT correctly.\"\n        )\n    else:\n        self.fitted_pipeline_ = self._toolbox.compile(expr=self._optimized_pipeline)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            self.fitted_pipeline_.fit(features, target)\n        if self.verbosity in [1, 2]:\n            # Add an extra line of spacing if the progress bar was used\n            if self.verbosity >= 2:\n                print(\"\")\n            optimized_pipeline_str = self.clean_pipeline_string(\n                self._optimized_pipeline\n            )\n            print(\"Best pipeline:\", optimized_pipeline_str)\n        # Store and fit the entire Pareto front as fitted models for convenience\n        self.pareto_front_fitted_pipelines_ = {}\n        for pipeline in self._pareto_front.items:\n            self.pareto_front_fitted_pipelines_[\n                str(pipeline)\n            ] = self._toolbox.compile(expr=pipeline)\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                self.pareto_front_fitted_pipelines_[str(pipeline)].fit(\n                    features, target\n                )",
        "import_statements": [
            "import random",
            "import inspect",
            "import warnings",
            "import sys",
            "from functools import partial",
            "from datetime import datetime",
            "from multiprocessing import cpu_count",
            "import os",
            "import re",
            "import errno",
            "from tempfile import mkdtemp",
            "from shutil import rmtree",
            "import types",
            "from pandas import DataFrame",
            "from scipy import sparse",
            "import deap",
            "from deap import base, creator, tools, gp",
            "from copy import copy, deepcopy",
            "from sklearn.base import BaseEstimator",
            "from sklearn.utils import check_X_y, check_consistent_length, check_array",
            "from sklearn.pipeline import make_union, make_pipeline",
            "from sklearn.preprocessing import FunctionTransformer",
            "from sklearn.impute import SimpleImputer",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.model_selection._split import check_cv",
            "from sklearn.utils.metaestimators import available_if",
            "from joblib import Parallel, delayed, Memory",
            "from update_checker import update_check"
        ],
        "reference_api": [
            "RuntimeError",
            "fit",
            "print",
            "compile",
            "warnings.catch_warnings",
            "warnings.simplefilter",
            "self.clean_pipeline_string",
            "str"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "fit",
                "code": "def fit(self, features, target, sample_weight=None, groups=None):\n        \"\"\"Fit an optimized machine learning pipeline.\n\n        Uses genetic programming to optimize a machine learning pipeline that\n        maximizes score on the provided features and target. Performs internal\n        k-fold cross-validaton to avoid overfitting on the provided data. The\n        best pipeline is then trained on the entire set of provided samples.\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix\n\n            TPOT and all scikit-learn algorithms assume that the features will be numerical\n            and there will be no missing values. As such, when a feature matrix is provided\n            to TPOT, all missing values will automatically be replaced (i.e., imputed) using\n            median value imputation.\n\n            If you wish to use a different imputation strategy than median imputation, please\n            make sure to apply imputation to your feature set prior to passing it to TPOT.\n        target: array-like {n_samples}\n            List of class labels for prediction\n        sample_weight: array-like {n_samples}, optional\n            Per-sample weights. Higher weights indicate more importance. If specified,\n            sample_weight will be passed to any pipeline element whose fit() function accepts\n            a sample_weight argument. By default, using sample_weight does not affect tpot's\n            scoring functions, which determine preferences between pipelines.\n        groups: array-like, with shape {n_samples, }, optional\n            Group labels for the samples used when performing cross-validation.\n            This parameter should only be used in conjunction with sklearn's Group cross-validation\n            functions, such as sklearn.model_selection.GroupKFold\n\n        Returns\n        -------\n        self: object\n            Returns a copy of the fitted TPOT object\n\n        \"\"\"\n        self._fit_init()\n        features, target = self._check_dataset(features, target, sample_weight)\n\n        self._init_pretest(features, target)\n\n        # Randomly collect a subsample of training samples for pipeline optimization process.\n        if self.subsample < 1.0:\n            features, _, target, _ = train_test_split(\n                features,\n                target,\n                train_size=self.subsample,\n                test_size=None,\n                random_state=self.random_state,\n            )\n            # Raise a warning message if the training size is less than 1500 when subsample is not default value\n            if features.shape[0] < 1500:\n                print(\n                    \"Warning: Although subsample can accelerate pipeline optimization process, \"\n                    \"too small training sample size may cause unpredictable effect on maximizing \"\n                    \"score in pipeline optimization process. Increasing subsample ratio may get \"\n                    \"a more reasonable outcome from optimization process in TPOT.\"\n                )\n\n        # Set the seed for the GP run\n        if self.random_state is not None:\n            random.seed(self.random_state)  # deap uses random\n            np.random.seed(self.random_state)\n\n        self._start_datetime = datetime.now()\n        self._last_pipeline_write = self._start_datetime\n        self._toolbox.register(\n            \"evaluate\",\n            self._evaluate_individuals,\n            features=features,\n            target=target,\n            sample_weight=sample_weight,\n            groups=groups,\n        )\n\n        # assign population, self._pop can only be not None if warm_start is enabled\n        if not self._pop:\n            self._pop = self._toolbox.population(n=self.population_size)\n\n        def pareto_eq(ind1, ind2):\n            \"\"\"Determine whether two individuals are equal on the Pareto front.\n\n            Parameters\n            ----------\n            ind1: DEAP individual from the GP population\n                First individual to compare\n            ind2: DEAP individual from the GP population\n                Second individual to compare\n\n            Returns\n            ----------\n            individuals_equal: bool\n                Boolean indicating whether the two individuals are equal on\n                the Pareto front\n\n            \"\"\"\n            return np.allclose(ind1.fitness.values, ind2.fitness.values)\n\n        # Generate new pareto front if it doesn't already exist for warm start\n        if not self.warm_start or not self._pareto_front:\n            self._pareto_front = tools.ParetoFront(similar=pareto_eq)\n\n        # Set lambda_ (offspring size in GP) equal to population_size by default\n        if not self.offspring_size:\n            self._lambda = self.population_size\n        else:\n            self._lambda = self.offspring_size\n\n        # Start the progress bar\n        if self.max_time_mins:\n            total_evals = self.population_size\n        else:\n            total_evals = self._lambda * self.generations + self.population_size\n\n        self._pbar = tqdm(\n            total=total_evals,\n            unit=\"pipeline\",\n            leave=False,\n            file=self.log_file_,\n            disable=not (self.verbosity >= 2),\n            desc=\"Optimization Progress\",\n        )\n\n        try:\n            with warnings.catch_warnings():\n                self._setup_memory()\n                warnings.simplefilter(\"ignore\")\n                self._pop, _ = eaMuPlusLambda(\n                    population=self._pop,\n                    toolbox=self._toolbox,\n                    mu=self.population_size,\n                    lambda_=self._lambda,\n                    cxpb=self.crossover_rate,\n                    mutpb=self.mutation_rate,\n                    ngen=self.generations,\n                    pbar=self._pbar,\n                    halloffame=self._pareto_front,\n                    verbose=self.verbosity,\n                    per_generation_function=self._check_periodic_pipeline,\n                    log_file=self.log_file_,\n                )\n\n        # Allow for certain exceptions to signal a premature fit() cancellation\n        except (KeyboardInterrupt, SystemExit, StopIteration) as e:\n            if self.verbosity > 0:\n                self._pbar.write(\"\", file=self.log_file_)\n                self._pbar.write(\n                    \"{}\\nTPOT closed prematurely. Will use the current best pipeline.\".format(\n                        e\n                    ),\n                    file=self.log_file_,\n                )\n        finally:\n            # clean population for the next call if warm_start=False\n            if not self.warm_start:\n                self._pop = []\n            # keep trying 10 times in case weird things happened like multiple CTRL+C or exceptions\n            attempts = 10\n            for attempt in range(attempts):\n                try:\n                    # Close the progress bar\n                    # Standard truthiness checks won't work for tqdm\n                    if not isinstance(self._pbar, type(None)):\n                        self._pbar.close()\n\n                    self._update_top_pipeline()\n                    self._summary_of_best_pipeline(features, target)\n                    # Delete the temporary cache before exiting\n                    self._cleanup_memory()\n                    break\n\n                except (KeyboardInterrupt, SystemExit, Exception) as e:\n                    # raise the exception if it's our last attempt\n                    if attempt == (attempts - 1):\n                        raise e\n            return self"
            },
            {
                "name": "self.clean_pipeline_string",
                "code": "def clean_pipeline_string(self, individual):\n        \"\"\"Provide a string of the individual without the parameter prefixes.\n\n        Parameters\n        ----------\n        individual: individual\n            Individual which should be represented by a pretty string\n\n        Returns\n        -------\n        A string like str(individual), but with parameter prefixes removed.\n\n        \"\"\"\n        dirty_string = str(individual)\n        # There are many parameter prefixes in the pipeline strings, used solely for\n        # making the terminal name unique, eg. LinearSVC__.\n        parameter_prefixes = [\n            (m.start(), m.end()) for m in re.finditer(\", [\\w]+__\", dirty_string)\n        ]\n        # We handle them in reverse so we do not mess up indices\n        pretty = dirty_string\n        for (start, end) in reversed(parameter_prefixes):\n            pretty = pretty[: start + 2] + pretty[end:]\n\n        return pretty"
            },
            {
                "name": "fit",
                "code": "def fit(self, features, target, sample_weight=None, groups=None):\n        \"\"\"Fit an optimized machine learning pipeline.\n\n        Uses genetic programming to optimize a machine learning pipeline that\n        maximizes score on the provided features and target. Performs internal\n        k-fold cross-validaton to avoid overfitting on the provided data. The\n        best pipeline is then trained on the entire set of provided samples.\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix\n\n            TPOT and all scikit-learn algorithms assume that the features will be numerical\n            and there will be no missing values. As such, when a feature matrix is provided\n            to TPOT, all missing values will automatically be replaced (i.e., imputed) using\n            median value imputation.\n\n            If you wish to use a different imputation strategy than median imputation, please\n            make sure to apply imputation to your feature set prior to passing it to TPOT.\n        target: array-like {n_samples}\n            List of class labels for prediction\n        sample_weight: array-like {n_samples}, optional\n            Per-sample weights. Higher weights indicate more importance. If specified,\n            sample_weight will be passed to any pipeline element whose fit() function accepts\n            a sample_weight argument. By default, using sample_weight does not affect tpot's\n            scoring functions, which determine preferences between pipelines.\n        groups: array-like, with shape {n_samples, }, optional\n            Group labels for the samples used when performing cross-validation.\n            This parameter should only be used in conjunction with sklearn's Group cross-validation\n            functions, such as sklearn.model_selection.GroupKFold\n\n        Returns\n        -------\n        self: object\n            Returns a copy of the fitted TPOT object\n\n        \"\"\"\n        self._fit_init()\n        features, target = self._check_dataset(features, target, sample_weight)\n\n        self._init_pretest(features, target)\n\n        # Randomly collect a subsample of training samples for pipeline optimization process.\n        if self.subsample < 1.0:\n            features, _, target, _ = train_test_split(\n                features,\n                target,\n                train_size=self.subsample,\n                test_size=None,\n                random_state=self.random_state,\n            )\n            # Raise a warning message if the training size is less than 1500 when subsample is not default value\n            if features.shape[0] < 1500:\n                print(\n                    \"Warning: Although subsample can accelerate pipeline optimization process, \"\n                    \"too small training sample size may cause unpredictable effect on maximizing \"\n                    \"score in pipeline optimization process. Increasing subsample ratio may get \"\n                    \"a more reasonable outcome from optimization process in TPOT.\"\n                )\n\n        # Set the seed for the GP run\n        if self.random_state is not None:\n            random.seed(self.random_state)  # deap uses random\n            np.random.seed(self.random_state)\n\n        self._start_datetime = datetime.now()\n        self._last_pipeline_write = self._start_datetime\n        self._toolbox.register(\n            \"evaluate\",\n            self._evaluate_individuals,\n            features=features,\n            target=target,\n            sample_weight=sample_weight,\n            groups=groups,\n        )\n\n        # assign population, self._pop can only be not None if warm_start is enabled\n        if not self._pop:\n            self._pop = self._toolbox.population(n=self.population_size)\n\n        def pareto_eq(ind1, ind2):\n            \"\"\"Determine whether two individuals are equal on the Pareto front.\n\n            Parameters\n            ----------\n            ind1: DEAP individual from the GP population\n                First individual to compare\n            ind2: DEAP individual from the GP population\n                Second individual to compare\n\n            Returns\n            ----------\n            individuals_equal: bool\n                Boolean indicating whether the two individuals are equal on\n                the Pareto front\n\n            \"\"\"\n            return np.allclose(ind1.fitness.values, ind2.fitness.values)\n\n        # Generate new pareto front if it doesn't already exist for warm start\n        if not self.warm_start or not self._pareto_front:\n            self._pareto_front = tools.ParetoFront(similar=pareto_eq)\n\n        # Set lambda_ (offspring size in GP) equal to population_size by default\n        if not self.offspring_size:\n            self._lambda = self.population_size\n        else:\n            self._lambda = self.offspring_size\n\n        # Start the progress bar\n        if self.max_time_mins:\n            total_evals = self.population_size\n        else:\n            total_evals = self._lambda * self.generations + self.population_size\n\n        self._pbar = tqdm(\n            total=total_evals,\n            unit=\"pipeline\",\n            leave=False,\n            file=self.log_file_,\n            disable=not (self.verbosity >= 2),\n            desc=\"Optimization Progress\",\n        )\n\n        try:\n            with warnings.catch_warnings():\n                self._setup_memory()\n                warnings.simplefilter(\"ignore\")\n                self._pop, _ = eaMuPlusLambda(\n                    population=self._pop,\n                    toolbox=self._toolbox,\n                    mu=self.population_size,\n                    lambda_=self._lambda,\n                    cxpb=self.crossover_rate,\n                    mutpb=self.mutation_rate,\n                    ngen=self.generations,\n                    pbar=self._pbar,\n                    halloffame=self._pareto_front,\n                    verbose=self.verbosity,\n                    per_generation_function=self._check_periodic_pipeline,\n                    log_file=self.log_file_,\n                )\n\n        # Allow for certain exceptions to signal a premature fit() cancellation\n        except (KeyboardInterrupt, SystemExit, StopIteration) as e:\n            if self.verbosity > 0:\n                self._pbar.write(\"\", file=self.log_file_)\n                self._pbar.write(\n                    \"{}\\nTPOT closed prematurely. Will use the current best pipeline.\".format(\n                        e\n                    ),\n                    file=self.log_file_,\n                )\n        finally:\n            # clean population for the next call if warm_start=False\n            if not self.warm_start:\n                self._pop = []\n            # keep trying 10 times in case weird things happened like multiple CTRL+C or exceptions\n            attempts = 10\n            for attempt in range(attempts):\n                try:\n                    # Close the progress bar\n                    # Standard truthiness checks won't work for tqdm\n                    if not isinstance(self._pbar, type(None)):\n                        self._pbar.close()\n\n                    self._update_top_pipeline()\n                    self._summary_of_best_pipeline(features, target)\n                    # Delete the temporary cache before exiting\n                    self._cleanup_memory()\n                    break\n\n                except (KeyboardInterrupt, SystemExit, Exception) as e:\n                    # raise the exception if it's our last attempt\n                    if attempt == (attempts - 1):\n                        raise e\n            return self"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/base.py",
        "function_declaration": "def score(self, testing_features, testing_target)",
        "start_line": "1071",
        "end_line": "1111",
        "docstring": "# This function evaluates the performance of a fitted pipeline on testing data.\\nIt first checks if the pipeline has been optimized and raises an error if not.\\nThe testing features and target are validated using a helper method.\\nDepending on the type of scoring function provided, it either retrieves a scorer from the predefined SCORERS dictionary or uses the provided callable scorer.\\nThe function then calculates and returns the score by applying the scorer to the fitted pipeline, testing features, and testing target.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "638683d78167",
        "ground_truth": "def score(self, testing_features, testing_target):\n    \"\"\"Return the score on the given testing data using the user-specified scoring function.\n    Parameters\n    ----------\n    testing_features: array-like {n_samples, n_features}\n        Feature matrix of the testing set\n    testing_target: array-like {n_samples}\n        List of class labels for prediction in the testing set\n    Returns\n    -------\n    accuracy_score: float\n        The estimated test set accuracy\n    \"\"\"\n    if self.fitted_pipeline_ is None:\n        raise RuntimeError(\n            \"A pipeline has not yet been optimized. Please call fit() first.\"\n        )\n    testing_features, testing_target = self._check_dataset(\n        testing_features, testing_target, sample_weight=None\n    )\n    # If the scoring function is a string, we must adjust to use the sklearn\n    # scoring interface\n    if isinstance(self.scoring_function, str):\n        scorer = SCORERS[self.scoring_function]\n    elif callable(self.scoring_function):\n        scorer = self.scoring_function\n    else:\n        raise RuntimeError(\n            \"The scoring function should either be the name of a scikit-learn scorer or a scorer object\"\n        )\n    score = scorer(\n        self.fitted_pipeline_,\n        testing_features.astype(np.float64),\n        testing_target.astype(np.float64),\n    )\n    return score",
        "import_statements": [
            "import random",
            "import inspect",
            "import warnings",
            "import sys",
            "from functools import partial",
            "from datetime import datetime",
            "from multiprocessing import cpu_count",
            "import os",
            "import re",
            "import errno",
            "from tempfile import mkdtemp",
            "from shutil import rmtree",
            "import types",
            "from pandas import DataFrame",
            "from scipy import sparse",
            "import deap",
            "from deap import base, creator, tools, gp",
            "from copy import copy, deepcopy",
            "from sklearn.base import BaseEstimator",
            "from sklearn.utils import check_X_y, check_consistent_length, check_array",
            "from sklearn.pipeline import make_union, make_pipeline",
            "from sklearn.preprocessing import FunctionTransformer",
            "from sklearn.impute import SimpleImputer",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.model_selection._split import check_cv",
            "from sklearn.utils.metaestimators import available_if",
            "from joblib import Parallel, delayed, Memory",
            "from update_checker import update_check"
        ],
        "reference_api": [
            "RuntimeError",
            "scorer",
            "testing_features.astype",
            "isinstance",
            "callable",
            "self._check_dataset",
            "testing_target.astype"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._check_dataset",
                "code": "def _check_dataset(self, features, target, sample_weight=None):\n        \"\"\"Check if a dataset has a valid feature set and labels.\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix\n        target: array-like {n_samples} or None\n            List of class labels for prediction\n        sample_weight: array-like {n_samples} (optional)\n            List of weights indicating relative importance\n        Returns\n        -------\n        (features, target)\n        \"\"\"\n        # Check sample_weight\n        if sample_weight is not None:\n            try:\n                sample_weight = np.array(sample_weight).astype(\"float\")\n            except ValueError as e:\n                raise ValueError(\n                    \"sample_weight could not be converted to float array: %s\" % e\n                )\n            if np.any(np.isnan(sample_weight)):\n                raise ValueError(\"sample_weight contained NaN values.\")\n            try:\n                check_consistent_length(sample_weight, target)\n            except ValueError as e:\n                raise ValueError(\n                    \"sample_weight dimensions did not match target: %s\" % e\n                )\n\n        # If features is a sparse matrix, do not apply imputation\n        if sparse.issparse(features):\n            if self.config_dict in [None, \"TPOT light\", \"TPOT MDR\"]:\n                raise ValueError(\n                    \"Not all operators in {} supports sparse matrix. \"\n                    'Please use \"TPOT sparse\" for sparse matrix.'.format(\n                        self.config_dict\n                    )\n                )\n            elif self.config_dict != \"TPOT sparse\":\n                print(\n                    \"Warning: Since the input matrix is a sparse matrix, please makes sure all the operators in the \"\n                    \"customized config dictionary supports sparse matriies.\"\n                )\n        else:\n            if isinstance(features, np.ndarray):\n                if np.any(np.isnan(features)):\n                    self._imputed = True\n            elif isinstance(features, DataFrame):\n                if features.isnull().values.any():\n                    self._imputed = True\n\n            if self._imputed:\n                features = self._impute_values(features)\n\n        try:\n            if target is not None:\n                X, y = check_X_y(features, target, accept_sparse=True, dtype=None)\n                if self._imputed:\n                    return X, y\n                else:\n                    return features, target\n            else:\n                X = check_array(features, accept_sparse=True, dtype=None)\n                if self._imputed:\n                    return X\n                else:\n                    return features\n        except (AssertionError, ValueError):\n            raise ValueError(\n                \"Error: Input data is not in a valid format. Please confirm \"\n                \"that the input data is scikit-learn compatible. For example, \"\n                \"the features must be a 2-D array and target labels must be a \"\n                \"1-D array.\"\n            )"
            }
        ],
        "third_party": [
            "scorer",
            "testing_features.astype",
            "testing_target.astype"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/base.py",
        "function_declaration": " def _impute_values(self, features)",
        "start_line": "1308",
        "end_line": "1327",
        "docstring": "# This function imputes missing values in a feature set.\\nIf verbosity is greater than 1, it prints a message indicating the imputation process.\\nIf the imputer has not been fitted yet, it creates a SimpleImputer with the \"median\" strategy and fits it to the features.\\nThe function then transforms the features using the fitted imputer and returns the imputed feature set.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a04382560333",
        "ground_truth": "def _impute_values(self, features):\n    \"\"\"Impute missing values in a feature set.\n    Parameters\n    ----------\n    features: array-like {n_samples, n_features}\n        A feature matrix\n    Returns\n    -------\n    array-like {n_samples, n_features}\n    \"\"\"\n    if self.verbosity > 1:\n        print(\"Imputing missing values in feature set\")\n    if self._fitted_imputer is None:\n        self._fitted_imputer = SimpleImputer(strategy=\"median\")\n        self._fitted_imputer.fit(features)\n    return self._fitted_imputer.transform(features)",
        "import_statements": [
            "import random",
            "import inspect",
            "import warnings",
            "import sys",
            "from functools import partial",
            "from datetime import datetime",
            "from multiprocessing import cpu_count",
            "import os",
            "import re",
            "import errno",
            "from tempfile import mkdtemp",
            "from shutil import rmtree",
            "import types",
            "from pandas import DataFrame",
            "from scipy import sparse",
            "import deap",
            "from deap import base, creator, tools, gp",
            "from copy import copy, deepcopy",
            "from sklearn.base import BaseEstimator",
            "from sklearn.utils import check_X_y, check_consistent_length, check_array",
            "from sklearn.pipeline import make_union, make_pipeline",
            "from sklearn.preprocessing import FunctionTransformer",
            "from sklearn.impute import SimpleImputer",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.model_selection._split import check_cv",
            "from sklearn.utils.metaestimators import available_if",
            "from joblib import Parallel, delayed, Memory",
            "from update_checker import update_check"
        ],
        "reference_api": [
            "fit",
            "print",
            "transform",
            "SimpleImputer"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "fit",
                "code": "def fit(self, features, target, sample_weight=None, groups=None):\n        \"\"\"Fit an optimized machine learning pipeline.\n\n        Uses genetic programming to optimize a machine learning pipeline that\n        maximizes score on the provided features and target. Performs internal\n        k-fold cross-validaton to avoid overfitting on the provided data. The\n        best pipeline is then trained on the entire set of provided samples.\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix\n\n            TPOT and all scikit-learn algorithms assume that the features will be numerical\n            and there will be no missing values. As such, when a feature matrix is provided\n            to TPOT, all missing values will automatically be replaced (i.e., imputed) using\n            median value imputation.\n\n            If you wish to use a different imputation strategy than median imputation, please\n            make sure to apply imputation to your feature set prior to passing it to TPOT.\n        target: array-like {n_samples}\n            List of class labels for prediction\n        sample_weight: array-like {n_samples}, optional\n            Per-sample weights. Higher weights indicate more importance. If specified,\n            sample_weight will be passed to any pipeline element whose fit() function accepts\n            a sample_weight argument. By default, using sample_weight does not affect tpot's\n            scoring functions, which determine preferences between pipelines.\n        groups: array-like, with shape {n_samples, }, optional\n            Group labels for the samples used when performing cross-validation.\n            This parameter should only be used in conjunction with sklearn's Group cross-validation\n            functions, such as sklearn.model_selection.GroupKFold\n\n        Returns\n        -------\n        self: object\n            Returns a copy of the fitted TPOT object\n\n        \"\"\"\n        self._fit_init()\n        features, target = self._check_dataset(features, target, sample_weight)\n\n        self._init_pretest(features, target)\n\n        # Randomly collect a subsample of training samples for pipeline optimization process.\n        if self.subsample < 1.0:\n            features, _, target, _ = train_test_split(\n                features,\n                target,\n                train_size=self.subsample,\n                test_size=None,\n                random_state=self.random_state,\n            )\n            # Raise a warning message if the training size is less than 1500 when subsample is not default value\n            if features.shape[0] < 1500:\n                print(\n                    \"Warning: Although subsample can accelerate pipeline optimization process, \"\n                    \"too small training sample size may cause unpredictable effect on maximizing \"\n                    \"score in pipeline optimization process. Increasing subsample ratio may get \"\n                    \"a more reasonable outcome from optimization process in TPOT.\"\n                )\n\n        # Set the seed for the GP run\n        if self.random_state is not None:\n            random.seed(self.random_state)  # deap uses random\n            np.random.seed(self.random_state)\n\n        self._start_datetime = datetime.now()\n        self._last_pipeline_write = self._start_datetime\n        self._toolbox.register(\n            \"evaluate\",\n            self._evaluate_individuals,\n            features=features,\n            target=target,\n            sample_weight=sample_weight,\n            groups=groups,\n        )\n\n        # assign population, self._pop can only be not None if warm_start is enabled\n        if not self._pop:\n            self._pop = self._toolbox.population(n=self.population_size)\n\n        def pareto_eq(ind1, ind2):\n            \"\"\"Determine whether two individuals are equal on the Pareto front.\n\n            Parameters\n            ----------\n            ind1: DEAP individual from the GP population\n                First individual to compare\n            ind2: DEAP individual from the GP population\n                Second individual to compare\n\n            Returns\n            ----------\n            individuals_equal: bool\n                Boolean indicating whether the two individuals are equal on\n                the Pareto front\n\n            \"\"\"\n            return np.allclose(ind1.fitness.values, ind2.fitness.values)\n\n        # Generate new pareto front if it doesn't already exist for warm start\n        if not self.warm_start or not self._pareto_front:\n            self._pareto_front = tools.ParetoFront(similar=pareto_eq)\n\n        # Set lambda_ (offspring size in GP) equal to population_size by default\n        if not self.offspring_size:\n            self._lambda = self.population_size\n        else:\n            self._lambda = self.offspring_size\n\n        # Start the progress bar\n        if self.max_time_mins:\n            total_evals = self.population_size\n        else:\n            total_evals = self._lambda * self.generations + self.population_size\n\n        self._pbar = tqdm(\n            total=total_evals,\n            unit=\"pipeline\",\n            leave=False,\n            file=self.log_file_,\n            disable=not (self.verbosity >= 2),\n            desc=\"Optimization Progress\",\n        )\n\n        try:\n            with warnings.catch_warnings():\n                self._setup_memory()\n                warnings.simplefilter(\"ignore\")\n                self._pop, _ = eaMuPlusLambda(\n                    population=self._pop,\n                    toolbox=self._toolbox,\n                    mu=self.population_size,\n                    lambda_=self._lambda,\n                    cxpb=self.crossover_rate,\n                    mutpb=self.mutation_rate,\n                    ngen=self.generations,\n                    pbar=self._pbar,\n                    halloffame=self._pareto_front,\n                    verbose=self.verbosity,\n                    per_generation_function=self._check_periodic_pipeline,\n                    log_file=self.log_file_,\n                )\n\n        # Allow for certain exceptions to signal a premature fit() cancellation\n        except (KeyboardInterrupt, SystemExit, StopIteration) as e:\n            if self.verbosity > 0:\n                self._pbar.write(\"\", file=self.log_file_)\n                self._pbar.write(\n                    \"{}\\nTPOT closed prematurely. Will use the current best pipeline.\".format(\n                        e\n                    ),\n                    file=self.log_file_,\n                )\n        finally:\n            # clean population for the next call if warm_start=False\n            if not self.warm_start:\n                self._pop = []\n            # keep trying 10 times in case weird things happened like multiple CTRL+C or exceptions\n            attempts = 10\n            for attempt in range(attempts):\n                try:\n                    # Close the progress bar\n                    # Standard truthiness checks won't work for tqdm\n                    if not isinstance(self._pbar, type(None)):\n                        self._pbar.close()\n\n                    self._update_top_pipeline()\n                    self._summary_of_best_pipeline(features, target)\n                    # Delete the temporary cache before exiting\n                    self._cleanup_memory()\n                    break\n\n                except (KeyboardInterrupt, SystemExit, Exception) as e:\n                    # raise the exception if it's our last attempt\n                    if attempt == (attempts - 1):\n                        raise e\n            return self"
            }
        ],
        "third_party": [
            "SimpleImputer",
            "transform"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/base.py",
        "function_declaration": "def _compile_to_sklearn(self, expr)",
        "start_line": "1407",
        "end_line": "1429",
        "docstring": "The function _compile_to_sklearn(self, expr) converts an expression into a scikit-learn pipeline. It first generates the pipeline code string from the expression and a predefined set of operators. Then, it evaluates this string within a specific context to obtain the scikit-learn pipeline object. The pipeline's memory attribute is set, and if a random state is specified, it recursively sets the random_state parameter for each step in the pipeline. Finally, the configured scikit-learn pipeline is returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0043d411c332",
        "ground_truth": "def _compile_to_sklearn(self, expr):\n    \"\"\"Compile a DEAP pipeline into a sklearn pipeline.\n    Parameters\n    ----------\n    expr: DEAP individual\n        The DEAP pipeline to be compiled\n    Returns\n    -------\n    sklearn_pipeline: sklearn.pipeline.Pipeline\n    \"\"\"\n    sklearn_pipeline_str = generate_pipeline_code(\n        expr_to_tree(expr, self._pset), self.operators\n    )\n    sklearn_pipeline = eval(sklearn_pipeline_str, self.operators_context)\n    sklearn_pipeline.memory = self._memory\n    if self.random_state is not None:\n        # Fix random state when the operator allows\n        set_param_recursive(\n            sklearn_pipeline.steps, \"random_state\", self.random_state\n        )\n    return sklearn_pipeline",
        "import_statements": [
            "import random",
            "import inspect",
            "import warnings",
            "import sys",
            "from functools import partial",
            "from datetime import datetime",
            "from multiprocessing import cpu_count",
            "import os",
            "import re",
            "import errno",
            "from tempfile import mkdtemp",
            "from shutil import rmtree",
            "import types",
            "from pandas import DataFrame",
            "from scipy import sparse",
            "import deap",
            "from deap import base, creator, tools, gp",
            "from copy import copy, deepcopy",
            "from sklearn.base import BaseEstimator",
            "from sklearn.utils import check_X_y, check_consistent_length, check_array",
            "from sklearn.pipeline import make_union, make_pipeline",
            "from sklearn.preprocessing import FunctionTransformer",
            "from sklearn.impute import SimpleImputer",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.model_selection._split import check_cv",
            "from sklearn.utils.metaestimators import available_if",
            "from joblib import Parallel, delayed, Memory",
            "from update_checker import update_check"
        ],
        "reference_api": [
            "expr_to_tree",
            "generate_pipeline_code",
            "eval",
            "set_param_recursive"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "generate_pipeline_code",
            "expr_to_tree",
            "set_param_recursive"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/gp_deap.py",
        "function_declaration": "def pick_two_individuals_eligible_for_crossover(population)",
        "start_line": "40",
        "end_line": "72",
        "docstring": "The function pick_two_individuals_eligible_for_crossover(population) selects two individuals from a given population that are eligible for crossover. It first creates sets of primitive nodes for each individual and converts each individual to a string representation. It then identifies pairs of individuals that share at least one primitive and are not identical. Eligible pairs are considered in both possible orders. If no eligible pairs are found, the function returns None for both individuals. Otherwise, it randomly selects one of the eligible pairs and returns the corresponding individuals from the population.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ca0ce462aa76",
        "ground_truth": "def pick_two_individuals_eligible_for_crossover(population):\n    \"\"\"Pick two individuals from the population which can do crossover, that is, they share a primitive.\n     Parameters\n    ----------\n    population: array of individuals\n     Returns\n    ----------\n    tuple: (individual, individual)\n        Two individuals which are not the same, but share at least one primitive.\n        Alternatively, if no such pair exists in the population, (None, None) is returned instead.\n    \"\"\"\n    primitives_by_ind = [set([node.name for node in ind if isinstance(node, gp.Primitive)])\n                         for ind in population]\n    pop_as_str = [str(ind) for ind in population]\n     eligible_pairs = [(i, i+1+j) for i, ind1_prims in enumerate(primitives_by_ind)\n                                 for j, ind2_prims in enumerate(primitives_by_ind[i+1:])\n                                 if not ind1_prims.isdisjoint(ind2_prims) and\n                                    pop_as_str[i] != pop_as_str[i+1+j]]\n     # Pairs are eligible in both orders, this ensures that both orders are considered\n    eligible_pairs += [(j, i) for (i, j) in eligible_pairs]\n     if not eligible_pairs:\n        # If there are no eligible pairs, the caller should decide what to do\n        return None, None\n     pair = np.random.randint(0, len(eligible_pairs))\n    idx1, idx2 = eligible_pairs[pair]\n     return population[idx1], population[idx2]",
        "import_statements": [
            "from deap import tools, gp",
            "from inspect import isclass",
            "from sklearn.utils import indexable",
            "from sklearn.metrics import check_scoring",
            "from sklearn.model_selection._validation import _fit_and_score",
            "from sklearn.base import clone",
            "from collections import defaultdict",
            "import warnings",
            "from stopit import threading_timeoutable, TimeoutException"
        ],
        "reference_api": [
            "len",
            "set",
            "isinstance",
            "randint",
            "ind1_prims.isdisjoint",
            "str",
            "enumerate"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ind1_prims.isdisjoint",
            "randint"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/export_utils.py",
        "function_declaration": "def generate_export_pipeline_code(pipeline_tree, operators)",
        "start_line": "372",
        "end_line": "396",
        "docstring": "The function generate_export_pipeline_code(pipeline_tree, operators) generates the Python code for a scikit-learn pipeline from a given pipeline tree and set of operators. It processes the operators to create the steps of the pipeline and determines the number of steps. If the pipeline has more than one step, it formats the steps into a make_pipeline call with indentation. If there is only one step, it formats it directly without make_pipeline. The resulting code as a string is then returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4b03c843f506",
        "ground_truth": "def generate_export_pipeline_code(pipeline_tree, operators):\n    \"\"\"Generate code specific to the construction of the sklearn Pipeline for export_pipeline.\n     Parameters\n    ----------\n    pipeline_tree: list\n        List of operators in the current optimized pipeline\n     Returns\n    -------\n    Source code for the sklearn pipeline\n     \"\"\"\n    steps = _process_operator(pipeline_tree, operators)\n    # number of steps in a pipeline\n    num_step = len(steps)\n    if num_step > 1:\n        pipeline_text = \"make_pipeline(\\n{STEPS}\\n)\".format(\n            STEPS=_indent(\",\\n\".join(steps), 4)\n        )\n    # only one operator (root = True)\n    else:\n        pipeline_text = \"{STEPS}\".format(STEPS=_indent(\",\\n\".join(steps), 0))\n     return pipeline_text",
        "import_statements": [
            "import deap"
        ],
        "reference_api": [
            "join",
            "_indent",
            "len",
            "_process_operator",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_process_operator",
                "code": "def _process_operator(operator, operators, depth=0):\n    steps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name != \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps"
            },
            {
                "name": "_indent",
                "code": "def _indent(text, amount):\n    \"\"\"Indent a multiline string by some number of spaces.\n\n    Parameters\n    ----------\n    text: str\n        The text to be indented\n    amount: int\n        The number of spaces to indent the text\n\n    Returns\n    -------\n    indented_text\n\n    \"\"\"\n    indentation = amount * \" \"\n    return indentation + (\"\\n\" + indentation).join(text.split(\"\\n\"))"
            },
            {
                "name": "_indent",
                "code": "def _indent(text, amount):\n    \"\"\"Indent a multiline string by some number of spaces.\n\n    Parameters\n    ----------\n    text: str\n        The text to be indented\n    amount: int\n        The number of spaces to indent the text\n\n    Returns\n    -------\n    indented_text\n\n    \"\"\"\n    indentation = amount * \" \"\n    return indentation + (\"\\n\" + indentation).join(text.split(\"\\n\"))"
            }
        ],
        "third_party": [
            "join",
            "join"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/export_utils.py",
        "function_declaration": "def _process_operator(operator, operators, depth=0)",
        "start_line": "399",
        "end_line": "422",
        "docstring": "The function _process_operator(operator, operators, depth=0) recursively processes a given operator to generate a sequence of steps for a pipeline. It first identifies the operator's name and checks if it is a \"CombineDFs\" operation, in which case it appends the result of combining dataframes. For other operations, it retrieves the corresponding TPOT operator and processes any input operators recursively if they are not the initial input matrix. If the operator is a root operator and the depth is greater than zero, it wraps the operator in a StackingEstimator. The function returns the list of generated steps for the pipeline.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e9aec2ee39a4",
        "ground_truth": "def _process_operator(operator, operators, depth=0):\n    steps = []\n    op_name = operator[0]\n     if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n         if input_name != \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n         # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps",
        "import_statements": [
            "import deap"
        ],
        "reference_api": [
            "steps.append",
            "steps.extend",
            "_process_operator",
            "tpot_op.export",
            "_combine_dfs",
            "get_by_name",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_combine_dfs",
                "code": "def _combine_dfs(left, right, operators):\n    def _make_branch(branch):\n        if branch == \"input_matrix\":\n            return \"FunctionTransformer(copy)\"\n        elif branch[0] == \"CombineDFs\":\n            return _combine_dfs(branch[1], branch[2], operators)\n        elif branch[1] == \"input_matrix\":  # If depth of branch == 1\n            tpot_op = get_by_name(branch[0], operators)\n\n            if tpot_op.root:\n                return \"StackingEstimator(estimator={})\".format(\n                    _process_operator(branch, operators)[0]\n                )\n            else:\n                return _process_operator(branch, operators)[0]\n        else:  # We're going to have to make a pipeline\n            tpot_op = get_by_name(branch[0], operators)\n\n            if tpot_op.root:\n                return \"StackingEstimator(estimator={})\".format(\n                    generate_pipeline_code(branch, operators)\n                )\n            else:\n                return generate_pipeline_code(branch, operators)\n\n    return \"make_union(\\n{},\\n{}\\n)\".format(\n        _indent(_make_branch(left), 4), _indent(_make_branch(right), 4)\n    )"
            },
            {
                "name": "get_by_name",
                "code": "def get_by_name(opname, operators):\n    \"\"\"Return operator class instance by name.\n\n    Parameters\n    ----------\n    opname: str\n        Name of the sklearn class that belongs to a TPOT operator\n    operators: list\n        List of operator classes from operator library\n\n    Returns\n    -------\n    ret_op_class: class\n        An operator class\n\n    \"\"\"\n    ret_op_classes = [op for op in operators if op.__name__ == opname]\n\n    if len(ret_op_classes) == 0:\n        raise TypeError(\n            \"Cannot found operator {} in operator dictionary\".format(opname)\n        )\n    elif len(ret_op_classes) > 1:\n        raise ValueError(\n            \"Found duplicate operators {} in operator dictionary. Please check \"\n            \"your dictionary file.\".format(opname)\n        )\n    ret_op_class = ret_op_classes[0]\n    return ret_op_class"
            },
            {
                "name": "_process_operator",
                "code": "def _process_operator(operator, operators, depth=0):\n    steps = []\n    op_name = operator[0]\n\n    if op_name == \"CombineDFs\":\n        steps.append(_combine_dfs(operator[1], operator[2], operators))\n    else:\n        input_name, args = operator[1], operator[2:]\n        tpot_op = get_by_name(op_name, operators)\n\n        if input_name != \"input_matrix\":\n            steps.extend(_process_operator(input_name, operators, depth + 1))\n\n        # If the step is an estimator and is not the last step then we must\n        # add its guess as synthetic feature(s)\n        # classification prediction for both regression and classification\n        # classification probabilities for classification if available\n        if tpot_op.root and depth > 0:\n            steps.append(\n                \"StackingEstimator(estimator={})\".format(tpot_op.export(*args))\n            )\n        else:\n            steps.append(tpot_op.export(*args))\n    return steps"
            }
        ],
        "third_party": [
            "steps.append",
            "steps.extend",
            "steps.append",
            "tpot_op.export",
            "steps.append",
            "tpot_op.export"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/builtins/feature_transformers.py",
        "function_declaration": "def transform(self, X)",
        "start_line": "63",
        "end_line": "83",
        "docstring": "This function selects categorical features from the input data `X` based on a threshold.\\nIf no categorical features are found, it raises a `ValueError`.\\nIf categorical features are selected, it applies one-hot encoding to these features and returns the transformed data.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0513bf969510",
        "ground_truth": "def transform(self, X):\n    \"\"\"Select categorical features and transform them using OneHotEncoder.\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components is the number of components.\n    Returns\n    -------\n    array-like, {n_samples, n_components}\n    \"\"\"\n    selected = auto_select_categorical_features(X, threshold=self.threshold)\n    X_sel, _, n_selected, _ = _X_selected(X, selected)\n    if n_selected == 0:\n        # No features selected.\n        raise ValueError('No categorical feature was found!')\n    else:\n        ohe = OneHotEncoder(categorical_features='all', sparse=False, minimum_fraction=self.minimum_fraction)\n        return ohe.fit_transform(X_sel)",
        "import_statements": [
            "from sklearn.base import BaseEstimator, TransformerMixin",
            "from sklearn.utils import check_array",
            "from sklearn.decomposition import PCA"
        ],
        "reference_api": [
            "auto_select_categorical_features",
            "ValueError",
            "OneHotEncoder",
            "ohe.fit_transform",
            "_X_selected"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "auto_select_categorical_features",
            "_X_selected",
            "OneHotEncoder",
            "ohe.fit_transform"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/metrics.py",
        "function_declaration": "def balanced_accuracy(y_true, y_pred)",
        "start_line": "30",
        "end_line": "69",
        "docstring": "The function balanced_accuracy(y_true, y_pred) computes the balanced accuracy for a set of true and predicted labels. It first identifies all unique classes present in the true and predicted labels. For each class, it calculates the sensitivity and specificity, ensuring both metrics are accounted for even if a class is underrepresented. Sensitivity is the ratio of true positives to the total actual positives, while specificity is the ratio of true negatives to the total actual negatives. If the true labels consist of only one class, specificity is set to 1. The class accuracy is then the average of sensitivity and specificity. The function returns the mean of these class accuracies, providing a balanced accuracy metric that accounts for class imbalances.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5b1bcf42ca9e",
        "ground_truth": "def balanced_accuracy(y_true, y_pred):\n    \"\"\"Default scoring function: balanced accuracy.\n     Balanced accuracy computes each class' accuracy on a per-class basis using a\n    one-vs-rest encoding, then computes an unweighted average of the class accuracies.\n     Parameters\n    ----------\n    y_true: numpy.ndarray {n_samples}\n        True class labels\n    y_pred: numpy.ndarray {n_samples}\n        Predicted class labels by the estimator\n     Returns\n    -------\n    fitness: float\n        Returns a float value indicating the individual's balanced accuracy\n        0.5 is as good as chance, and 1.0 is perfect predictive accuracy\n    \"\"\"\n    all_classes = np.unique(np.append(y_true, y_pred))\n    all_class_accuracies = []\n    for this_class in all_classes:\n        this_class_sensitivity = 0.\n        this_class_specificity = 0.\n         if sum(y_true == this_class) != 0:\n            this_class_sensitivity = \\\n                float(sum((y_pred == this_class) & (y_true == this_class))) /\\\n                float(sum((y_true == this_class)))\n        if sum(y_true != this_class) != 0:\n            this_class_specificity = \\\n                float(sum((y_pred != this_class) & (y_true != this_class))) /\\\n                float(sum((y_true != this_class)))\n        else: # in rase case, y_true has only 1 class then specificity should be 1\n            this_class_specificity = 1.\n         this_class_accuracy = (this_class_sensitivity + this_class_specificity) / 2.\n        all_class_accuracies.append(this_class_accuracy)\n     return np.mean(all_class_accuracies)",
        "import_statements": [
            "from sklearn.metrics import get_scorer, get_scorer_names, make_scorer"
        ],
        "reference_api": [
            "float",
            "sum",
            "np.unique",
            "np.mean",
            "all_class_accuracies.append",
            "np.append"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.unique",
            "np.append",
            "all_class_accuracies.append",
            "np.mean"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/builtins/zero_count.py",
        "function_declaration": "def transform(self, X, y=None)",
        "start_line": "38",
        "end_line": "66",
        "docstring": "The function \\\"transform\\\" processes the input array X by first verifying it with check_array. It then calculates the number of non-zero elements per row and the number of zero elements per row. These two new columns are added to the beginning of the transformed array. The function returns the modified array with the additional columns representing non-zero and zero element counts per row.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "19a9ad39de0a",
        "ground_truth": "def transform(self, X, y=None):\n    \"\"\"Transform data by adding two virtual features.\n    Parameters\n    ----------\n    X: numpy ndarray, {n_samples, n_components}\n        New data, where n_samples is the number of samples and n_components\n        is the number of components.\n    y: None\n        Unused\n    Returns\n    -------\n    X_transformed: array-like, shape (n_samples, n_features)\n        The transformed feature set\n    \"\"\"\n    X = check_array(X)\n    n_features = X.shape[1]\n    X_transformed = np.copy(X)\n    non_zero_vector = np.count_nonzero(X_transformed, axis=1)\n    non_zero = np.reshape(non_zero_vector, (-1, 1))\n    zero_col = np.reshape(n_features - non_zero_vector, (-1, 1))\n    X_transformed = np.hstack((non_zero, X_transformed))\n    X_transformed = np.hstack((zero_col, X_transformed))\n    return X_transformed",
        "import_statements": [
            "from sklearn.base import BaseEstimator, TransformerMixin",
            "from sklearn.utils import check_array"
        ],
        "reference_api": [
            "np.hstack",
            "check_array",
            "np.count_nonzero",
            "np.reshape",
            "np.copy"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "check_array",
            "np.copy",
            "np.count_nonzero",
            "np.reshape",
            "np.reshape",
            "np.hstack",
            "np.hstack"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "EpistasisLab/tpot",
        "file_path": "tpot/tpot.py",
        "function_declaration": "def _init_pretest(self, features, target)",
        "start_line": "44",
        "end_line": "68",
        "docstring": "The function _init_pretest(self, features, target) initializes a pretest dataset.\\nIt calculates the number of unique target classes and ensures the training size is at least this number.\\nUsing train_test_split, it splits the features and target into pretest_X and pretest_y with a specified random state.\\nIf all target classes are not included in pretest_y, it adjusts pretest_y to include at least one example from each class by indexing unique target values.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5e8db0b5d447",
        "ground_truth": "def _init_pretest(self, features, target):\n    \"\"\"Set the sample of data used to verify pipelines work\n    with the passed data set.\n    This is not intend for anything other than perfunctory dataset\n    pipeline compatibility testing\n    \"\"\"\n    num_unique_target = len(np.unique(target))\n    # make sure train_size is at least num_unique_target\n    train_size=max(min(50,int(0.9*features.shape[0])), num_unique_target)\n    self.pretest_X, _, self.pretest_y, _ = \\\n            train_test_split(\n                            features,\n                            target,\n                            random_state=self.random_state,\n                            test_size=None,\n                            train_size=train_size\n                            )\n    #Make sure there is a least one example from each class\n    #for this evaluative test sample\n    if not np.array_equal(np.unique(target), np.unique(self.pretest_y)):\n        unique_target_idx = np.unique(target,return_index=True)[1]\n        self.pretest_y[0:unique_target_idx.shape[0]] = \\\n                _safe_indexing(target, unique_target_idx)",
        "import_statements": [
            "from sklearn.model_selection import train_test_split",
            "from sklearn.utils import _safe_indexing"
        ],
        "reference_api": [
            "min",
            "train_test_split",
            "int",
            "len",
            "np.unique",
            "_safe_indexing",
            "max",
            "np.array_equal"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.unique",
            "train_test_split",
            "np.array_equal",
            "np.unique",
            "np.unique",
            "np.unique",
            "_safe_indexing"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "google/trax",
        "file_path": "trax/tf_numpy/numpy_impl/utils.py",
        "function_declaration": "def _to_numpy_type(dtype)",
        "start_line": "58",
        "end_line": "69",
        "docstring": "This function converts the given data type to a NumPy data type.\\nIf the input is a TensorFlow data type, it returns the corresponding NumPy data type.\\nIf the input is already a NumPy data type, it returns it as is.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5e3d48667396",
        "ground_truth": "def _to_numpy_type(dtype):\n  \"\"\"Converts a native python or TF DType to numpy type.\n   Args:\n    dtype: Could be a python type, a numpy type or a TF DType.\n   Returns:\n    A NumPy `dtype`.\n  \"\"\"\n  if isinstance(dtype, tf.DType):\n    return dtype.as_numpy_dtype\n  return np.dtype(dtype)",
        "import_statements": [
            "import funcsigs",
            "from trax.tf_numpy.numpy_impl import arrays",
            "from trax.tf_numpy.numpy_impl import dtypes"
        ],
        "reference_api": [
            "np.dtype",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.dtype"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "google/trax",
        "file_path": "trax/fastmath/jax.py",
        "function_declaration": "def _dataset_as_numpy(ds, batch_size=None)",
        "start_line": "139",
        "end_line": "155",
        "docstring": "This function converts a TensorFlow dataset to a NumPy format by batching the dataset and iterating over the batches.\\nIf no batching is requested or the required batching function is unavailable, it falls back to a default method.\\nThe function ensures compatibility with both TensorFlow 1.X and later versions by using a try-except block to handle the presence of the batching function.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8f1120fdd463",
        "ground_truth": "def _dataset_as_numpy(ds, batch_size=None):\n  \"\"\"Speed up tfds.as_numpy by batching and then iterating over the batches.\"\"\"\n  batch_size = batch_size or 1\n  try:  # Check that dense_to_ragged_batch exists.\n    if batch_size < 2:  # Fall back to default if no batching requested.\n      raise AttributeError\n    ds_batch = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size))\n    for example in tfds.as_numpy(ds_batch):\n      flat_example = tnp.tree_flatten(example)\n      np_flat_example = [_to_numpy(x) for x in flat_example]\n      for single_example_flat in zip(*np_flat_example):\n        single_example, _ = tnp.tree_unflatten(single_example_flat, example)\n        yield single_example\n  except AttributeError:\n    # In TF 1.X there is not dense_to_ragged_batch: fallback.\n    for example in tfds.as_numpy(ds):\n      yield example",
        "import_statements": [
            "import functools",
            "import jax",
            "from jax import lax",
            "from jax import random as jax_random",
            "from trax.fastmath import numpy as tnp",
            "from trax.shapes import signature"
        ],
        "reference_api": [
            "tnp.tree_unflatten",
            "_to_numpy",
            "dense_to_ragged_batch",
            "tnp.tree_flatten",
            "ds.apply",
            "zip",
            "tfds.as_numpy"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_to_numpy",
                "code": "def _to_numpy(x):\n  \"\"\"Converts non-NumPy tensors to NumPy arrays.\"\"\"\n  return x if isinstance(x, np.ndarray) else x.numpy()"
            }
        ],
        "third_party": [
            "ds.apply",
            "dense_to_ragged_batch",
            "tfds.as_numpy",
            "tnp.tree_flatten",
            "tnp.tree_unflatten",
            "tfds.as_numpy"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "google/trax",
        "file_path": "trax/fastmath/numpy.py",
        "function_declaration": "def get_prng(seed)",
        "start_line": "23",
        "end_line": "30",
        "docstring": "This function generates a JAX-compatible pseudo-random number generator (PRNG) key from a given seed.\\nIt ensures the seed is a scalar and splits it into two 32-bit unsigned integers.\\nThese integers are then concatenated to form the PRNG key.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "282271955871",
        "ground_truth": "def get_prng(seed):\n  \"\"\"JAX-compatible way of getting PRNG seeds.\"\"\"\n  if np.shape(seed):\n    raise TypeError('PRNGKey seed must be a scalar.')\n  convert = lambda k: np.reshape(np.asarray(k, np.uint32), [1])\n  k1 = convert(np.bitwise_and(np.right_shift(seed, 32), 0xFFFFFFFF))\n  k2 = convert(np.bitwise_and(seed, 0xFFFFFFFF))\n  return np.concatenate([k1, k2], 0)",
        "import_statements": [
            "from scipy.special import logsumexp",
            "from trax.shapes import signature"
        ],
        "reference_api": [
            "np.right_shift",
            "np.shape",
            "np.asarray",
            "np.bitwise_and",
            "TypeError",
            "convert",
            "np.reshape",
            "np.concatenate"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.shape",
            "np.reshape",
            "np.asarray",
            "convert",
            "np.bitwise_and",
            "np.right_shift",
            "convert",
            "np.bitwise_and",
            "np.concatenate"
        ]
    },
    {
        "subclass": "dask",
        "owner/repo": "ibis-project/ibis",
        "file_path": "ibis/backends/dask/convert.py",
        "function_declaration": "def convert_Timestamp(cls, s, dtype, pandas_type)",
        "start_line": "59",
        "end_line": "67",
        "docstring": "The function convert_Timestamp(cls, s, dtype, pandas_type) converts a given series s to a timestamp with a specific timezone based on its dtype.\\nIf s is a pd.DatetimeTZDtype, it converts the timezone using tz_convert.\\nIf s is a datetime64 type, it localizes the timezone using tz_localize.\\nIf s is numeric, it converts it to datetime and localizes the timezone.\\nOtherwise, it converts s to datetime with UTC and then localizes the timezone.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "42234dccce9a",
        "ground_truth": "def convert_Timestamp(cls, s, dtype, pandas_type):\n    if isinstance(s.dtype, pd.DatetimeTZDtype):\n        return s.dt.tz_convert(dtype.timezone)\n    elif pdt.is_datetime64_dtype(s.dtype):\n        return s.dt.tz_localize(dtype.timezone)\n    elif pdt.is_numeric_dtype(s.dtype):\n        return dd.to_datetime(s, unit=\"s\").dt.tz_localize(dtype.timezone)\n    else:\n        return dd.to_datetime(s, utc=True).dt.tz_localize(dtype.timezone)",
        "import_statements": [
            "from ibis.backends.pandas.convert import PandasConverter",
            "from ibis.formats.pandas import DataMapper, PandasType"
        ],
        "reference_api": [
            "pdt.is_datetime64_dtype",
            "dd.to_datetime",
            "tz_localize",
            "isinstance",
            "pdt.is_numeric_dtype",
            "tz_convert"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tz_convert",
            "pdt.is_datetime64_dtype",
            "tz_localize",
            "pdt.is_numeric_dtype",
            "tz_localize",
            "dd.to_datetime",
            "tz_localize",
            "dd.to_datetime"
        ]
    },
    {
        "subclass": "dask",
        "owner/repo": "ibis-project/ibis",
        "file_path": "ibis/backends/dask/executor.py",
        "function_declaration": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound)",
        "start_line": "132",
        "end_line": "143",
        "docstring": "This function processes a time-based operation on a Dask DataFrame.\\nIt checks if the data is timezone-aware and converts it to UTC if necessary.\\nIt then creates a boolean indexer to identify rows within the specified time range.\\nThe result is a Dask array where the identified rows are marked as True, and this array is converted back to a Dask DataFrame for the final output.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9cee7388879b",
        "ground_truth": "def visit(cls, op: ops.BetweenTime, arg, lower_bound, upper_bound):\n    if getattr(arg.dtype, \"tz\", None) is not None:\n        localized = arg.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n    else:\n        localized = arg\n    time = localized.dt.time.astype(str)\n    indexer = ((time >= lower_bound) & (time <= upper_bound)).to_dask_array(True)\n    result = da.zeros(len(arg), dtype=np.bool_)\n    result[indexer] = True\n    return dd.from_array(result)",
        "import_statements": [
            "import operator",
            "from functools import reduce",
            "from packaging.version import parse as vparse",
            "from ibis.backends.dask.convert import DaskConverter",
            "from ibis.backends.dask.helpers import (\n    DaskUtils,\n    add_globally_consecutive_column,\n)",
            "from ibis.backends.pandas.executor import PandasExecutor",
            "from ibis.backends.pandas.rewrites import (\n    PandasAggregate,\n    PandasJoin,\n    PandasLimit,\n    PandasResetIndex,\n    PandasScalarSubquery,\n    PandasWindowFrame,\n    PandasWindowFunction,\n    plan,\n)",
            "from ibis.common.exceptions import UnboundExpressionError, UnsupportedOperationError",
            "from ibis.formats.pandas import PandasData, PandasType",
            "from ibis.util import gen_name"
        ],
        "reference_api": [
            "getattr",
            "to_dask_array",
            "len",
            "dd.from_array",
            "tz_localize",
            "da.zeros",
            "tz_convert",
            "astype"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tz_localize",
            "tz_convert",
            "astype",
            "to_dask_array",
            "da.zeros",
            "dd.from_array"
        ]
    },
    {
        "subclass": "dask",
        "owner/repo": "ibis-project/ibis",
        "file_path": "ibis/backends/dask/helpers.py",
        "function_declaration": "def asseries(cls, value, like=None)",
        "start_line": "32",
        "end_line": "59",
        "docstring": "The function asseries(cls, value, like=None) ensures that the input value is converted into a pandas Series object, broadcasting it if necessary.\\nIf the value is a Dask Series, it returns it directly.\\nIf the value is a Dask scalar, it creates a Dask array from the scalar and then a Dask Series from the array.\\nIf the value is a pandas Series, it converts it into a Dask Series with one partition.\\nIf a 'like' object is provided, it creates a Series based on the structure of the 'like' object, handling tuples, lists, and dictionaries appropriately.\\nIf none of these conditions are met, it converts the value into a single-partition Dask Series.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0ab1cc8ccf71",
        "ground_truth": "def asseries(cls, value, like=None):\n    \"\"\"Ensure that value is a pandas Series object, broadcast if necessary.\"\"\"\n    if isinstance(value, dd.Series):\n        return value\n    elif isinstance(value, dd.core.Scalar):\n        # Create a Dask array from the Dask scalar\n        try:\n            dtype = value.dtype\n        except AttributeError:\n            #     @property\n            #     def dtype(self):\n            # >       return self._meta.dtype\n            # E       AttributeError: 'Timestamp' object has no attribute 'dtype'\n            dtype = object\n        array = da.from_delayed(value.to_delayed(), (1,), dtype=dtype)\n        # Create a Dask series from the Dask array\n        return dd.from_array(array)\n    elif isinstance(value, pd.Series):\n        return dd.from_pandas(value, npartitions=1)\n    elif like is not None:\n        if isinstance(value, (tuple, list, dict)):\n            fn = lambda df: pd.Series([value] * len(df), index=df.index)\n        else:\n            fn = lambda df: pd.Series(value, index=df.index)\n        return like.map_partitions(fn)\n    else:\n        return dd.from_pandas(pd.Series([value]), npartitions=1)",
        "import_statements": [
            "from typing import TYPE_CHECKING",
            "from ibis.backends.pandas.helpers import PandasUtils"
        ],
        "reference_api": [
            "da.from_delayed",
            "dd.from_array",
            "len",
            "isinstance",
            "like.map_partitions",
            "pd.Series",
            "dd.from_pandas",
            "value.to_delayed"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "da.from_delayed",
            "value.to_delayed",
            "dd.from_array",
            "dd.from_pandas",
            "pd.Series",
            "pd.Series",
            "like.map_partitions",
            "dd.from_pandas",
            "pd.Series"
        ]
    },
    {
        "subclass": "dask",
        "owner/repo": "ibis-project/ibis",
        "file_path": "ibis/backends/dask/__init__.py",
        "function_declaration": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None",
        "start_line": "29",
        "end_line": "60",
        "docstring": "This function connects to a given dictionary of data frames.\\nIf the dictionary is not provided, it initializes an empty dictionary.\\nIt validates that each value in the dictionary is either a Dask or Pandas DataFrame.\\nIf any value is not a valid DataFrame, it raises a TypeError.\\nFinally, it calls the superclass's connect method with the validated dictionary.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "926220a06a0c",
        "ground_truth": "def do_connect(\n    self,\n    dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n) -> None:\n    \"\"\"Construct a Dask backend client from a dictionary of data sources.\n    Parameters\n    ----------\n    dictionary\n        An optional mapping from `str` table names to Dask DataFrames.\n    Examples\n    --------\n    >>> import ibis\n    >>> import dask.dataframe as dd\n    >>> data = {\n    ...     \"t\": dd.read_parquet(\"path/to/file.parquet\"),\n    ...     \"s\": dd.read_csv(\"path/to/file.csv\"),\n    ... }\n    >>> ibis.dask.connect(data)\n    \"\"\"\n    if dictionary is None:\n        dictionary = {}\n    for k, v in dictionary.items():\n        if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n            raise TypeError(\n                f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                f\" got an instance of '{type(v).__name__}' instead.\"\n            )\n    super().do_connect(dictionary)",
        "import_statements": [
            "from typing import TYPE_CHECKING, Any",
            "import dask",
            "from ibis import util",
            "from ibis.backends import NoUrl",
            "from ibis.backends.pandas import BasePandasBackend",
            "from ibis.formats.pandas import PandasData"
        ],
        "reference_api": [
            "super",
            "TypeError",
            "type",
            "isinstance",
            "dictionary.items",
            "do_connect"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "do_connect",
                "code": "def do_connect(\n        self,\n        dictionary: MutableMapping[str, dd.DataFrame] | None = None,\n    ) -> None:\n        \"\"\"Construct a Dask backend client from a dictionary of data sources.\n\n        Parameters\n        ----------\n        dictionary\n            An optional mapping from `str` table names to Dask DataFrames.\n\n        Examples\n        --------\n        >>> import ibis\n        >>> import dask.dataframe as dd\n        >>> data = {\n        ...     \"t\": dd.read_parquet(\"path/to/file.parquet\"),\n        ...     \"s\": dd.read_csv(\"path/to/file.csv\"),\n        ... }\n        >>> ibis.dask.connect(data)\n\n        \"\"\"\n        if dictionary is None:\n            dictionary = {}\n\n        for k, v in dictionary.items():\n            if not isinstance(v, (dd.DataFrame, pd.DataFrame)):\n                raise TypeError(\n                    f\"Expected an instance of 'dask.dataframe.DataFrame' for {k!r},\"\n                    f\" got an instance of '{type(v).__name__}' instead.\"\n                )\n        super().do_connect(dictionary)"
            }
        ],
        "third_party": [
            "dictionary.items"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "IntelPython/scikit-learn_bench",
        "file_path": "sklbench/benchmarks/sklearn_estimator.py",
        "function_declaration": "def get_estimator(library_name: str, estimator_name: str)",
        "start_line": "52",
        "end_line": "71",
        "docstring": "The function get_estimator takes a library name and an estimator name as arguments.\\nIt retrieves a map of classes from the specified library module using the get_module_members function.\\nIf the estimator name is not found in the classes map, it raises a ValueError indicating the estimator could not be found in the module.\\nIf multiple classes match the estimator name, it logs a debug message with the list of matching estimators and a warning message indicating that multiple classes were found and the first one will be used.\\nThe function then checks if the selected estimator class is a subclass of sklearn's BaseEstimator and logs an informational message if it is not.\\nFinally, it returns the selected estimator class.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "95afc38c9125",
        "ground_truth": "def get_estimator(library_name: str, estimator_name: str):\n    classes_map, _ = get_module_members(library_name.split(\".\"))\n    if estimator_name not in classes_map:\n        raise ValueError(\n            f\"Unable to find {estimator_name} estimator in {library_name} module.\"\n        )\n    if len(classes_map[estimator_name]) != 1:\n        logger.debug(\n            f'List of estimator with name \"{estimator_name}\": '\n            f\"{classes_map[estimator_name]}\"\n        )\n        logger.warning(\n            f\"Found {len(classes_map[estimator_name])} classes for \"\n            f'\"{estimator_name}\" estimator name. '\n            f\"Using first {classes_map[estimator_name][0]}.\"\n        )\n    estimator = classes_map[estimator_name][0]\n    if not issubclass(estimator, BaseEstimator):\n        logger.info(f\"{estimator} estimator is not derived from sklearn's BaseEstimator\")\n    return estimator",
        "import_statements": [
            "import inspect",
            "import io",
            "import json",
            "import logging",
            "import os",
            "from importlib.metadata import PackageNotFoundError, version",
            "from typing import Dict, List, Union",
            "from sklearn.base import BaseEstimator",
            "from sklearn.metrics import (\n    accuracy_score,\n    balanced_accuracy_score,\n    completeness_score,\n    davies_bouldin_score,\n    homogeneity_score,\n    log_loss,\n    mean_squared_error,\n    r2_score,\n    roc_auc_score,\n)"
        ],
        "reference_api": [
            "get_module_members",
            "issubclass",
            "ValueError",
            "len",
            "logger.warning",
            "logger.info",
            "logger.debug",
            "library_name.split"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get_module_members",
            "library_name.split"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "IntelPython/scikit-learn_bench",
        "file_path": "sklbench/benchmarks/sklearn_estimator.py",
        "function_declaration": "def estimator_to_task(estimator_name: str) -> str",
        "start_line": "89",
        "end_line": "100",
        "docstring": "This function maps an estimator name to a machine learning task by checking its postfix against a predefined mapping.\\nIt reads the mapping from a JSON file and returns the corresponding task if a match is found.\\nIf no match is found, it returns \"unknown\".",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9562a9cc043b",
        "ground_truth": "def estimator_to_task(estimator_name: str) -> str:\n    \"\"\"Maps estimator name to machine learning task based on listed estimator postfixes\"\"\"\n    with open(\n        os.path.join(\n            os.path.abspath(os.path.dirname(__file__)), \"estimator_task_map.json\"\n        )\n    ) as map_file:\n        estimator_to_task_map = json.load(map_file)\n    for task, postfixes_list in estimator_to_task_map.items():\n        if any(map(lambda postfix: estimator_name.endswith(postfix), postfixes_list)):\n            return task\n    return \"unknown\"",
        "import_statements": [
            "import inspect",
            "import io",
            "import json",
            "import logging",
            "import os",
            "from importlib.metadata import PackageNotFoundError, version",
            "from typing import Dict, List, Union",
            "from sklearn.base import BaseEstimator",
            "from sklearn.metrics import (\n    accuracy_score,\n    balanced_accuracy_score,\n    completeness_score,\n    davies_bouldin_score,\n    homogeneity_score,\n    log_loss,\n    mean_squared_error,\n    r2_score,\n    roc_auc_score,\n)"
        ],
        "reference_api": [
            "join",
            "any",
            "estimator_to_task_map.items",
            "dirname",
            "map",
            "estimator_name.endswith",
            "abspath",
            "open",
            "json.load"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "join",
            "abspath",
            "dirname",
            "estimator_to_task_map.items",
            "estimator_name.endswith"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "IntelPython/scikit-learn_bench",
        "file_path": "sklbench/datasets/common.py",
        "function_declaration": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict",
        "start_line": "74",
        "end_line": "87",
        "docstring": "The function load_data_from_cache takes a directory path and a prefix string, retrieves filenames starting with the prefix, and loads data from those files into a dictionary. It skips files with a .json extension and processes other files by extracting the component and file extension from the filename. The function then loads the file content using another function and stores it in the dictionary under the component name as the key. Finally, it returns the dictionary containing the loaded data.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2f50195d043a",
        "ground_truth": "def load_data_from_cache(data_cache: str, data_name: str) -> Dict:\n    # data filename format:\n    # {data_name}_{data_component}.{file_ext}\n    data_filenames = get_filenames_by_prefix(data_cache, data_name)\n    data = dict()\n    for data_filename in data_filenames:\n        if data_filename.endswith(\".json\"):\n            continue\n        postfix = data_filename.replace(data_name, \"\")[1:]\n        component, file_ext = postfix.split(\".\", 1)\n        data[component] = load_data_file(\n            os.path.join(data_cache, data_filename), file_ext\n        )\n    return data",
        "import_statements": [
            "import json",
            "import os",
            "import re",
            "from typing import Dict, List, Union",
            "from scipy.sparse import csr_matrix",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder"
        ],
        "reference_api": [
            "join",
            "postfix.split",
            "data_filename.replace",
            "load_data_file",
            "dict",
            "data_filename.endswith",
            "get_filenames_by_prefix"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "get_filenames_by_prefix",
                "code": "def get_filenames_by_prefix(directory: str, prefix: str) -> List[str]:\n    assert os.path.isdir(directory)\n    prefix_expr = get_expr_by_prefix(prefix)\n    return list(\n        filter(lambda x: re.search(prefix_expr, x) is not None, os.listdir(directory))\n    )"
            },
            {
                "name": "load_data_file",
                "code": "def load_data_file(filepath, extension):\n    if extension == \"parq\":\n        data = pd.read_parquet(filepath)\n    elif extension.endswith(\"npz\"):\n        npz_content = np.load(filepath)\n        if extension == \"npz\":\n            data = npz_content[\"arr_0\"]\n        elif extension == \"csr.npz\":\n            data = csr_matrix(\n                tuple(npz_content[attr] for attr in [\"data\", \"indices\", \"indptr\"])\n            )\n        else:\n            raise ValueError(f'Unknown npz subextension \"{extension}\"')\n        npz_content.close()\n    else:\n        raise ValueError(f'Unknown extension \"{extension}\"')\n    return data"
            }
        ],
        "third_party": [
            "data_filename.endswith",
            "data_filename.replace",
            "postfix.split",
            "join"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "IntelPython/scikit-learn_bench",
        "file_path": "sklbench/datasets/downloaders.py",
        "function_declaration": "def retrieve(url: str, filename: str) -> None",
        "start_line": "34",
        "end_line": "52",
        "docstring": "The function retrieve takes a URL and a filename as arguments.\\nIt checks if the specified file already exists using os.path.isfile; if it does, the function returns immediately.\\nIf the URL starts with \"http\", it attempts to download the content from the URL using the requests library.\\nIf the response status code is not 200, it raises an AssertionError indicating the download failed and includes the status code.\\nIt then reads the total content length from the response headers and sets a block size for reading the data.\\nThe function writes the content to the specified file in chunks, updating the written data size.\\nAfter the download, it verifies if the total downloaded size matches the expected content length, raising an AssertionError if there is a discrepancy.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e0281206029e",
        "ground_truth": "def retrieve(url: str, filename: str) -> None:\n    if os.path.isfile(filename):\n        return\n    elif url.startswith(\"http\"):\n        response = requests.get(url, stream=True)\n        if response.status_code != 200:\n            raise AssertionError(\n                f\"Failed to download from {url}.\\n\"\n                f\"Response returned status code {response.status_code}\"\n            )\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 8192\n        n = 0\n        with open(filename, \"wb+\") as datafile:\n            for data in response.iter_content(block_size):\n                n += len(data) / 1024\n                datafile.write(data)\n        if total_size != 0 and n != total_size / 1024:\n            raise AssertionError(\"Some content was present but not downloaded/written\")",
        "import_statements": [
            "import os",
            "from typing import Callable, List, Union",
            "import requests",
            "from scipy.sparse import csr_matrix",
            "from sklearn.datasets import fetch_openml"
        ],
        "reference_api": [
            "url.startswith",
            "response.iter_content",
            "requests.get",
            "int",
            "AssertionError",
            "len",
            "datafile.write",
            "get",
            "isfile",
            "open"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "isfile",
            "url.startswith",
            "requests.get",
            "get",
            "response.iter_content",
            "datafile.write"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "IntelPython/scikit-learn_bench",
        "file_path": "sklbench/datasets/loaders.py",
        "function_declaration": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]",
        "start_line": "313",
        "end_line": "333",
        "docstring": "The function load_covtype loads the Covertype dataset, processes the target labels, and returns the data along with a description. It takes the dataset name, cache paths, and parameters as inputs. The function retrieves the dataset features and labels, adjusts the labels to be zero-indexed, and optionally converts them to binary based on a parameter. It then creates a description dictionary with the number of classes and default split parameters. Finally, it returns a tuple containing the dataset and its description.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a69a71cc2fe8",
        "ground_truth": "def load_covtype(\n    data_name: str, data_cache: str, raw_data_cache: str, dataset_params: Dict\n) -> Tuple[Dict, Dict]:\n    \"\"\"\n    Cover type dataset from UCI machine learning repository\n    https://archive.ics.uci.edu/ml/datasets/covertype\n     y contains 7 unique class labels from 1 to 7 inclusive.\n    Classification task. n_classes = 7.\n    \"\"\"\n    x, y = fetch_covtype(return_X_y=True, data_home=raw_data_cache)\n    y = y.astype(int) - 1\n    binary = dataset_params.get(\"binary\", False)\n    if binary:\n        y = (y > 2).astype(int)\n     data_desc = {\n        \"n_classes\": 2 if binary else 7,\n        \"default_split\": {\"test_size\": 0.2, \"random_state\": 77},\n    }\n    return {\"x\": x, \"y\": y}, data_desc",
        "import_statements": [
            "import os",
            "from typing import Dict, Tuple",
            "from scipy import sparse",
            "from sklearn.datasets import (\n    fetch_california_housing,\n    fetch_covtype,\n    load_digits,\n    load_svmlight_file,\n    make_blobs,\n    make_classification,\n    make_regression,\n)"
        ],
        "reference_api": [
            "y.astype",
            "fetch_covtype",
            "astype",
            "dataset_params.get"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "fetch_covtype",
            "y.astype",
            "dataset_params.get",
            "astype"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "IntelPython/scikit-learn_bench",
        "file_path": "sklbench/utils/common.py",
        "function_declaration": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List",
        "start_line": "107",
        "end_line": "119",
        "docstring": "The function flatten_list takes a list and an optional boolean argument ensure_type_homogeneity.\\nIt initializes an empty list to store the flattened values.\\nThe function iterates through each element in the input list.\\nIf an element is a list, it recursively calls itself to flatten the inner list and appends the flattened values to the output list.\\nIf an element is not a list, it directly appends the element to the output list.\\nIf ensure_type_homogeneity is True, it calls ensure_list_types_homogeneity on the output list to ensure all elements are of the same type.\\nFinally, it returns the flattened list.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "129cbd2529ef",
        "ground_truth": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List:\n    output_list = list()\n    # iteration with inner recursion\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list",
        "import_statements": [
            "import hashlib",
            "import importlib",
            "import inspect",
            "import json",
            "import re",
            "from pprint import pformat",
            "from shutil import get_terminal_size",
            "from typing import Any, Dict, List, Tuple, Union"
        ],
        "reference_api": [
            "list",
            "flatten_list",
            "isinstance",
            "output_list.append",
            "ensure_list_types_homogeneity"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "flatten_list",
                "code": "def flatten_list(input_list: List, ensure_type_homogeneity: bool = False) -> List:\n    output_list = list()\n    # iteration with inner recursion\n    for value in input_list:\n        if isinstance(value, list):\n            inner_flat_list = flatten_list(value)\n            for inner_value in inner_flat_list:\n                output_list.append(inner_value)\n        else:\n            output_list.append(value)\n    if ensure_type_homogeneity:\n        ensure_list_types_homogeneity(output_list)\n    return output_list"
            },
            {
                "name": "ensure_list_types_homogeneity",
                "code": "def ensure_list_types_homogeneity(input_list: List):\n    list_types = set([type(el) for el in input_list])\n    if len(list_types) != 1:\n        raise ValueError(\"List is not type homogeneous. \" f\"Existing types: {list_types}\")"
            }
        ],
        "third_party": [
            "output_list.append",
            "output_list.append"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/activations.py",
        "function_declaration": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n)",
        "start_line": "15",
        "end_line": "31",
        "docstring": "The function logit computes the logit (log-odds) of input array x. It takes x as a mandatory input, with optional parameters eps for clipping values, complex_mode to specify handling of complex numbers, and out for output array. If eps is None, values outside [0,1] are set to NaN. Otherwise, values are clipped to the range [eps, 1-eps]. The function returns the logit of x, preserving the input data type.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2d38c2c60b59",
        "ground_truth": "def logit(\n    x: np.ndarray,\n    /,\n    *,\n    eps: Optional[float] = None,\n    complex_mode: Literal[\"split\", \"magnitude\", \"jax\"] = \"jax\",\n    out: Optional[np.ndarray] = None,\n):\n    x_dtype = x.dtype\n    if eps is None:\n        x = np.where(np.logical_or(x > 1, x < 0), np.nan, x)\n    else:\n        x = np.clip(x, eps, 1 - eps)\n    ret = (np.log(x / (1 - x))).astype(x_dtype)\n    if np.isscalar(ret):\n        return np.array(ret)\n    return ret",
        "import_statements": [
            "from typing import Optional, Union, Literal",
            "import ivy",
            "from ivy.functional.backends.numpy.helpers import _scalar_output_to_0d_array",
            "from ivy.func_wrapper import (\n    with_unsupported_dtypes,\n)"
        ],
        "reference_api": [
            "np.where",
            "np.log",
            "np.logical_or",
            "np.isscalar",
            "np.clip",
            "astype",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.where",
            "np.logical_or",
            "np.clip",
            "astype",
            "np.log",
            "np.isscalar",
            "np.array"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/activations.py",
        "function_declaration": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray",
        "start_line": "195",
        "end_line": "201",
        "docstring": "The function hardshrink takes a NumPy array x, a threshold value lambd, and an optional output array out as arguments.\\nIt applies a hard thresholding operation to the input array x, setting values to zero if they are within the range [-lambd, lambd], and keeping the original values otherwise.\\nIf an output array out is provided, the function updates it in-place with the result and returns it with the same data type as the input array x.\\nIf no output array is provided, it returns the result with the same data type as the input array x.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "205e296f8621",
        "ground_truth": "def hardshrink(\n    x: np.ndarray, /, *, lambd: float = 0.5, out: Optional[np.ndarray] = None\n) -> np.ndarray:\n    ret = np.where(x > lambd, x, np.where(x < -lambd, x, 0))\n    if ivy.exists(out):\n        return ivy.inplace_update(out, ret).astype(x.dtype)\n    return ivy.astype(ret, x.dtype)",
        "import_statements": [
            "from typing import Optional, Union, Literal",
            "import ivy",
            "from ivy.functional.backends.numpy.helpers import _scalar_output_to_0d_array",
            "from ivy.func_wrapper import (\n    with_unsupported_dtypes,\n)"
        ],
        "reference_api": [
            "np.where",
            "ivy.inplace_update",
            "ivy.exists",
            "astype",
            "ivy.astype"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.where",
            "np.where",
            "ivy.exists",
            "astype",
            "ivy.inplace_update",
            "ivy.astype"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/creation.py",
        "function_declaration": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray",
        "start_line": "13",
        "end_line": "23",
        "docstring": "The function vorbis_window generates a Vorbis window for a given length and returns it as a NumPy array. It takes the window length, an optional data type, and an optional output array as inputs. The function calculates the window values using a specific mathematical formula involving sine functions and appends the rounded results to a list. Finally, it converts the list to a NumPy array with the specified data type and returns it.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5b133f6ebe2d",
        "ground_truth": "def vorbis_window(\n    window_length: np.ndarray,\n    *,\n    dtype: np.dtype = np.float32,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    result = []\n    for i in range(1, window_length * 2, 2):\n        temp = np.sin(ivy.pi / 2 * (np.sin(ivy.pi * i / (window_length * 2)) ** 2))\n        result.append(round(temp, 8))\n    return np.array(result, dtype=dtype)",
        "import_statements": [
            "from typing import Optional, Tuple, Sequence, Union",
            "import ivy"
        ],
        "reference_api": [
            "np.sin",
            "result.append",
            "round",
            "range",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.sin",
            "np.sin",
            "result.append",
            "np.array"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/creation.py",
        "function_declaration": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray",
        "start_line": "234",
        "end_line": "239",
        "docstring": "The function polyval evaluates a polynomial at specific values given by the coefficients and input array. It ensures precision by setting a precise mode and promoting data types of the coefficients and inputs to a common type. The function uses numpy's polyval to compute the polynomial values and then converts the result to an array with the promoted data type. Finally, it returns the computed array.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "473296f771a9",
        "ground_truth": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray:\n    with ivy.PreciseMode(True):\n        promoted_type = ivy.promote_types(ivy.dtype(coeffs[0]), ivy.dtype(x[0]))\n    result = np.polyval(coeffs, x)\n    result = np.asarray(result, np.dtype(promoted_type))\n    return result",
        "import_statements": [
            "from typing import Optional, Tuple, Sequence, Union",
            "import ivy"
        ],
        "reference_api": [
            "np.asarray",
            "ivy.PreciseMode",
            "ivy.dtype",
            "np.dtype",
            "ivy.promote_types",
            "np.polyval"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "np.polyval",
                "code": "def polyval(coeffs: np.ndarray, x: np.ndarray) -> np.ndarray:\n    with ivy.PreciseMode(True):\n        promoted_type = ivy.promote_types(ivy.dtype(coeffs[0]), ivy.dtype(x[0]))\n    result = np.polyval(coeffs, x)\n    result = np.asarray(result, np.dtype(promoted_type))\n    return result"
            }
        ],
        "third_party": [
            "ivy.PreciseMode",
            "ivy.promote_types",
            "ivy.dtype",
            "ivy.dtype",
            "np.asarray",
            "np.dtype"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/elementwise.py",
        "function_declaration": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray",
        "start_line": "12",
        "end_line": "22",
        "docstring": "The function amax computes the maximum value of a NumPy array x along a specified axis or axes.\\nIt accepts parameters for the axis or axes to operate on, whether to keep the dimensions of the original array, and an optional output array to store the result.\\nIf the axis parameter is a list, it is converted to a tuple.\\nThe function uses np.amax to calculate the maximum value, and ensures that if the result is a scalar, it is returned as a NumPy array.\\nOtherwise, the result is returned as is.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b8455221cb2a",
        "ground_truth": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret",
        "import_statements": [
            "from typing import Optional, Union, Tuple, List, Sequence",
            "import ivy",
            "from ivy import promote_types_of_inputs",
            "from ivy.functional.backends.numpy.helpers import _scalar_output_to_0d_array",
            "from ivy.func_wrapper import with_unsupported_dtypes"
        ],
        "reference_api": [
            "tuple",
            "np.asarray",
            "np.isscalar",
            "isinstance",
            "np.amax"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "np.amax",
                "code": "def amax(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Sequence[int]]] = None,\n    keepdims: bool = False,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    axis = tuple(axis) if isinstance(axis, list) else axis\n    ret = np.amax(a=x, axis=axis, out=out, keepdims=keepdims)\n    return np.asarray(ret) if np.isscalar(ret) else ret"
            }
        ],
        "third_party": [
            "np.asarray",
            "np.isscalar"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/elementwise.py",
        "function_declaration": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray",
        "start_line": "90",
        "end_line": "101",
        "docstring": "The function copysign takes two array-like inputs x1 and x2, along with an optional output array out.\\nIt promotes the types of x1 and x2 to a common type using promote_types_of_inputs.\\nIf x1 is not of a float data type, both x1 and x2 are cast to the default float data type.\\nThe function then applies the np.copysign operation, which assigns the sign of each element in x2 to the corresponding element in x1, and returns the result, optionally storing it in the provided output array out.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f72489f434a3",
        "ground_truth": "def copysign(\n    x1: npt.ArrayLike,\n    x2: npt.ArrayLike,\n    /,\n    *,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    x1, x2 = promote_types_of_inputs(x1, x2)\n    if not ivy.is_float_dtype(x1):\n        x1 = x1.astype(ivy.default_float_dtype(as_native=True))\n        x2 = x2.astype(ivy.default_float_dtype(as_native=True))\n    return np.copysign(x1, x2, out=out)",
        "import_statements": [
            "from typing import Optional, Union, Tuple, List, Sequence",
            "import ivy",
            "from ivy import promote_types_of_inputs",
            "from ivy.functional.backends.numpy.helpers import _scalar_output_to_0d_array",
            "from ivy.func_wrapper import with_unsupported_dtypes"
        ],
        "reference_api": [
            "ivy.is_float_dtype",
            "promote_types_of_inputs",
            "x1.astype",
            "ivy.default_float_dtype",
            "x2.astype",
            "np.copysign"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "promote_types_of_inputs",
            "ivy.is_float_dtype",
            "x1.astype",
            "ivy.default_float_dtype",
            "x2.astype",
            "ivy.default_float_dtype",
            "np.copysign"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/elementwise.py",
        "function_declaration": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray",
        "start_line": "108",
        "end_line": "122",
        "docstring": "The function count_nonzero counts the number of non-zero elements in an array along a specified axis, with options to keep dimensions, specify output data type, and use an output array. It converts axis lists to tuples and uses np.count_nonzero for counting. The function handles scalar results by converting them to an array with the specified dtype and returns the result.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "55c0d77e4dc2",
        "ground_truth": "def count_nonzero(\n    a: np.ndarray,\n    /,\n    *,\n    axis: Optional[Union[int, Tuple[int, ...]]] = None,\n    keepdims: bool = False,\n    dtype: Optional[np.dtype] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    ret = np.count_nonzero(a, axis=axis, keepdims=keepdims)\n    if np.isscalar(ret):\n        return np.array(ret, dtype=dtype)\n    return ret.astype(dtype)",
        "import_statements": [
            "from typing import Optional, Union, Tuple, List, Sequence",
            "import ivy",
            "from ivy import promote_types_of_inputs",
            "from ivy.functional.backends.numpy.helpers import _scalar_output_to_0d_array",
            "from ivy.func_wrapper import with_unsupported_dtypes"
        ],
        "reference_api": [
            "tuple",
            "ret.astype",
            "np.isscalar",
            "isinstance",
            "np.count_nonzero",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.count_nonzero",
            "np.isscalar",
            "np.array",
            "ret.astype"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/elementwise.py",
        "function_declaration": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]",
        "start_line": "273",
        "end_line": "283",
        "docstring": "The function gradient computes the numerical gradient of an input array along specified axes using given spacing and edge order. It accepts an array, an optional spacing parameter which can be an int, list, or tuple, an optional axis specification, and an edge order. If the spacing is a single value, it calls the gradient computation with that spacing. Otherwise, it unpacks and uses the spacing list or tuple. It returns the computed gradient as an array or a list of arrays.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "784bf7d24fc2",
        "ground_truth": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]:\n    if type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)",
        "import_statements": [
            "from typing import Optional, Union, Tuple, List, Sequence",
            "import ivy",
            "from ivy import promote_types_of_inputs",
            "from ivy.functional.backends.numpy.helpers import _scalar_output_to_0d_array",
            "from ivy.func_wrapper import with_unsupported_dtypes"
        ],
        "reference_api": [
            "np.gradient",
            "type"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "np.gradient",
                "code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]:\n    if type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)"
            },
            {
                "name": "np.gradient",
                "code": "def gradient(\n    x: np.ndarray,\n    /,\n    *,\n    spacing: Union[int, list, tuple] = 1,\n    axis: Optional[Union[int, list, tuple]] = None,\n    edge_order: int = 1,\n) -> Union[np.ndarray, List[np.ndarray]]:\n    if type(spacing) in (int, float):\n        return np.gradient(x, spacing, axis=axis, edge_order=edge_order)\n    return np.gradient(x, *spacing, axis=axis, edge_order=edge_order)"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/elementwise.py",
        "function_declaration": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray",
        "start_line": "286",
        "end_line": "293",
        "docstring": "The function xlogy takes two NumPy arrays x and y, along with an optional output array out.\\nIt promotes the types of x and y to a common type using promote_types_of_inputs.\\nIf all elements in x are zero, the function returns 0.0.\\nOtherwise, it computes the element-wise product of x and the natural logarithm of y, returning the result.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "efa0a51af6bc",
        "ground_truth": "def xlogy(\n    x: np.ndarray, y: np.ndarray, /, *, out: Optional[np.ndarray] = None\n) -> np.ndarray:\n    x, y = promote_types_of_inputs(x, y)\n    if (x == 0).all():\n        return 0.0\n    else:\n        return x * np.log(y)",
        "import_statements": [
            "from typing import Optional, Union, Tuple, List, Sequence",
            "import ivy",
            "from ivy import promote_types_of_inputs",
            "from ivy.functional.backends.numpy.helpers import _scalar_output_to_0d_array",
            "from ivy.func_wrapper import with_unsupported_dtypes"
        ],
        "reference_api": [
            "all",
            "np.log",
            "promote_types_of_inputs"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "promote_types_of_inputs",
            "np.log"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/layers.py",
        "function_declaration": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\")",
        "start_line": "24",
        "end_line": "30",
        "docstring": "The function _determine_depth_max_pooling adjusts the input tensor for depth-wise max pooling. It calls a helper function to get updated kernel size, strides, and a flag indicating if depth pooling is needed. If depth pooling is required, the input tensor is transposed to reorder its dimensions. The function returns the possibly transposed tensor, updated kernel size, strides, and the depth pooling flag.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ac1645284260",
        "ground_truth": "def _determine_depth_max_pooling(x, kernel, strides, dims, data_format=\"channel_last\"):\n    kernel, strides, depth_pooling = _depth_max_pooling_helper(\n        x.shape, kernel, strides, dims=dims, data_format=data_format\n    )\n    if depth_pooling:\n        x = np.transpose(x, (0, dims + 1, *range(1, dims + 1)))\n    return x, kernel, strides, depth_pooling",
        "import_statements": [
            "import math",
            "from typing import Optional, Union, Tuple, List, Literal, Sequence, Callable",
            "import ivy",
            "from ivy.functional.ivy.layers import (\n    _handle_padding,\n    _get_num_padded_values,\n    _validate_max_pool_params,\n    _depth_max_pooling_helper,\n)",
            "from ivy.functional.backends.numpy.layers import _add_dilations",
            "from ivy.functional.ivy.experimental.layers import (\n    _padding_ceil_mode,\n)",
            "from ivy.func_wrapper import with_supported_dtypes",
            "from ivy.func_wrapper import with_unsupported_dtypes"
        ],
        "reference_api": [
            "np.transpose",
            "_depth_max_pooling_helper",
            "range"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_depth_max_pooling_helper",
                "code": "channel_last\"\n):\n    # Determine depth pooling.\n    # We assume that the kernel and the data have the same data_format.\n    depth_pooling = False\n    CHANNEL_LAST = \"channel_last\"\n    channel_idx = -1 if data_format == CHANNEL_LAST else 1\n    if len(kernel) == dims + 2:\n        spatial_kernel = kernel[1:-1] if data_format == CHANNEL_LAST else kernel[2:]\n        if kernel[channel_idx] != 1:\n            depth_pooling = True\n            if any(i != 1 for i in spatial_kernel):\n                raise NotImplementedError(\n                    \"MaxPooling supports exactly one of pooling across\"\n                    \" depth or pooling across width/height.\"\n                )\n            if len(strides) != dims + 2 or strides[channel_idx] != kernel[channel_idx]:\n                raise NotImplementedError(\n                    \"Depthwise max pooling requires the depth window to equal the depth\"\n                    \" stride\"\n                )\n            if x_shape[channel_idx] % kernel[channel_idx] != 0:\n                raise NotImplementedError(\n                    \"Depthwise max pooling requires the depth window to evenly divide\"\n                    \" the input depth\"\n                )\n            kernel = [kernel[channel_idx], *[1] * (dims - 1)]\n            strides = [strides[channel_idx], *[1] * (dims - 1)]\n        else:\n            kernel = spatial_kernel\n            if len(strides) == dims + 2:\n                strides = strides[1:-1] if data_format == CHANNEL_LAST else strides[2:]\n    return kernel, strides, depth_pooling\n\n\ndef _deconv_length(dim_size, stride_size, kernel_size, padding, dilation=1):\n"
            }
        ],
        "third_party": [
            "np.transpose"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/layers.py",
        "function_declaration": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray",
        "start_line": "842",
        "end_line": "864",
        "docstring": "The function dropout1d applies 1D dropout regularization to a NumPy array x with a given probability prob.\\nIf training is True, it determines the shape of x and whether it is batched.\\nFor \"NCW\" data format, it transposes x accordingly.\\nA binary mask is generated using a binomial distribution, and x is scaled by 1/(1 - prob) where the mask is true, otherwise set to zero.\\nIf data_format is \"NCW\", it transposes the result back.\\nIf training is False, the function returns x unchanged.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "18ff0a4f7550",
        "ground_truth": "def dropout1d(\n    x: np.ndarray,\n    prob: float,\n    /,\n    *,\n    training: bool = True,\n    data_format: str = \"NWC\",\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    if training:\n        x_shape = x.shape\n        is_batched = len(x_shape) == 3\n        if data_format == \"NCW\":\n            perm = (0, 2, 1) if is_batched else (1, 0)\n            x = np.transpose(x, perm)\n            x_shape = x.shape\n        mask = np.random.binomial(1, 1 - prob, x_shape)\n        res = np.where(mask, x / (1 - prob), 0)\n        if data_format == \"NCW\":\n            res = np.transpose(res, perm)\n    else:\n        res = x\n    return res",
        "import_statements": [
            "import math",
            "from typing import Optional, Union, Tuple, List, Literal, Sequence, Callable",
            "import ivy",
            "from ivy.functional.ivy.layers import (\n    _handle_padding,\n    _get_num_padded_values,\n    _validate_max_pool_params,\n    _depth_max_pooling_helper,\n)",
            "from ivy.functional.backends.numpy.layers import _add_dilations",
            "from ivy.functional.ivy.experimental.layers import (\n    _padding_ceil_mode,\n)",
            "from ivy.func_wrapper import with_supported_dtypes",
            "from ivy.func_wrapper import with_unsupported_dtypes"
        ],
        "reference_api": [
            "np.transpose",
            "np.where",
            "len",
            "binomial"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.transpose",
            "binomial",
            "np.where",
            "np.transpose"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/manipulation.py",
        "function_declaration": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]",
        "start_line": "102",
        "end_line": "123",
        "docstring": "The function top_k retrieves the top k elements along a specified axis of a NumPy array. It accepts parameters to control whether the largest or smallest elements are selected, whether the result should be sorted, and allows for optional output storage. The function calculates the appropriate indices for the top k elements, extracts these elements from the input array, and returns both the values and their indices as a named tuple.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0ce45eb7cbd9",
        "ground_truth": "def top_k(\n    x: np.ndarray,\n    k: int,\n    /,\n    *,\n    axis: int = -1,\n    largest: bool = True,\n    sorted: bool = True,\n    out: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -> Tuple[np.ndarray, np.ndarray]:\n    k = min(k, x.shape[axis])\n    if not largest:\n        indices = np.argsort(x, axis=axis)\n        indices = np.take(indices, np.arange(k), axis=axis)\n    else:\n        indices = np.argsort(-x, axis=axis)\n        indices = np.take(indices, np.arange(k), axis=axis)\n    if not sorted:\n        indices = np.sort(indices, axis=axis)\n    topk_res = NamedTuple(\"top_k\", [(\"values\", np.ndarray), (\"indices\", np.ndarray)])\n    val = np.take_along_axis(x, indices, axis=axis)\n    return topk_res(val, indices)",
        "import_statements": [
            "from typing import (\n    Iterable,\n    Optional,\n    Union,\n    Sequence,\n    Tuple,\n    NamedTuple,\n    Literal,\n    Callable,\n    Any,\n    List,\n)",
            "from numbers import Number",
            "from collections import namedtuple",
            "import ivy",
            "from ivy.functional.backends.numpy.helpers import _scalar_output_to_0d_array",
            "from ivy.func_wrapper import with_supported_dtypes, handle_out_argument"
        ],
        "reference_api": [
            "min",
            "np.sort",
            "np.take_along_axis",
            "topk_res",
            "np.argsort",
            "NamedTuple",
            "np.arange",
            "np.take"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.argsort",
            "np.take",
            "np.arange",
            "np.argsort",
            "np.take",
            "np.arange",
            "np.sort",
            "NamedTuple",
            "np.take_along_axis",
            "topk_res"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/norms.py",
        "function_declaration": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray",
        "start_line": "8",
        "end_line": "21",
        "docstring": "The function l1_normalize takes a NumPy array x, an optional axis, and an optional output array out.\\nIf axis is None, it calculates the L1 norm by summing the absolute values of x reshaped into a 1D array, then creates a denormalization array of the same shape as x filled with this norm.\\nIf axis is specified, it calculates the L1 norm along the given axis and creates a denormalization array by dividing the norm by the absolute values of x plus a small epsilon to prevent division by zero.\\nThe function returns the element-wise division of x by the denormalization array.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1424f9541998",
        "ground_truth": "def l1_normalize(\n    x: np.ndarray,\n    /,\n    *,\n    axis: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    if axis is None:\n        norm = np.sum(np.abs(np.reshape(x, -1)))\n        denorm = norm * np.ones_like(x)\n    else:\n        norm = np.sum(np.abs(x), axis=axis, keepdims=True)\n        denorm = np.divide(norm, np.abs(x) + 1e-12)\n    return np.divide(x, denorm)",
        "import_statements": [
            "from typing import Optional",
            "from ivy.func_wrapper import with_unsupported_dtypes"
        ],
        "reference_api": [
            "np.abs",
            "np.sum",
            "np.ones_like",
            "np.divide",
            "np.reshape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.sum",
            "np.abs",
            "np.reshape",
            "np.ones_like",
            "np.sum",
            "np.abs",
            "np.divide",
            "np.abs",
            "np.divide"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/random.py",
        "function_declaration": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray",
        "start_line": "33",
        "end_line": "47",
        "docstring": "The function beta generates samples from a beta distribution given the parameters alpha and beta. It optionally accepts shape, device, dtype, seed, and out parameters. The function checks and determines the shape of the output, sets the random seed if provided, and returns an array of samples from the beta distribution with the specified dtype.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4c3a386a1f05",
        "ground_truth": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    shape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)",
        "import_statements": [
            "from typing import Optional, Union, Sequence",
            "import ivy",
            "from ivy.functional.ivy.random import (\n    _check_bounds_and_get_shape,\n    _check_shapes_broadcastable,\n)"
        ],
        "reference_api": [
            "seed",
            "beta",
            "_check_bounds_and_get_shape",
            "np.asarray"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_check_bounds_and_get_shape",
                "code": "def _check_bounds_and_get_shape(low, high, shape):\n    if shape is not None:\n        ivy.utils.assertions.check_all_or_any_fn(\n            low,\n            high,\n            fn=lambda x: isinstance(x, (int, float)),\n            type=\"all\",\n            message=\"low and high bounds must be numerics when shape is specified\",\n        )\n        return ivy.Shape(shape)\n\n    valid_types = (ivy.Array,)\n    if len(backend_stack) == 0:\n        valid_types += (ivy.current_backend().NativeArray,)\n    else:\n        valid_types += (ivy.NativeArray,)\n    if isinstance(low, valid_types):\n        if isinstance(high, valid_types):\n            ivy.utils.assertions.check_equal(\n                ivy.shape(low), ivy.shape(high), as_array=False\n            )\n        return ivy.shape(low)\n    if isinstance(high, valid_types):\n        return ivy.shape(high)\n    return ivy.Shape(())"
            },
            {
                "name": "beta",
                "code": "def beta(\n    alpha: Union[float, np.ndarray],\n    beta: Union[float, np.ndarray],\n    /,\n    *,\n    shape: Optional[Union[ivy.NativeShape, Sequence[int]]] = None,\n    device: Optional[str] = None,\n    dtype: Optional[np.dtype] = None,\n    seed: Optional[int] = None,\n    out: Optional[np.ndarray] = None,\n) -> np.ndarray:\n    shape = _check_bounds_and_get_shape(alpha, beta, shape).shape\n    if seed is not None:\n        np.random.seed(seed)\n    return np.asarray(np.random.beta(alpha, beta, shape), dtype=dtype)"
            }
        ],
        "third_party": [
            "seed",
            "np.asarray"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "ivy-llc/ivy",
        "file_path": "ivy/functional/backends/numpy/experimental/sorting.py",
        "function_declaration": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray",
        "start_line": "7",
        "end_line": "15",
        "docstring": "The function invert_permutation takes an array-like input x, which can be a NumPy array, list, or tuple.\\nIt computes the sorted indices of x and initializes an array inverse with the same shape as the sorted indices, filled with zeros.\\nIt then assigns to inverse the indices of the sorted elements of x.\\nFinally, it computes the inverse permutation by sorting the inverse array and returns it.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "271200a8b261",
        "ground_truth": "def invert_permutation(\n    x: Union[np.ndarray, list, tuple],\n    /,\n) -> np.ndarray:\n    sorted_indices = np.argsort(x)\n    inverse = np.zeros_like(sorted_indices)\n    inverse[sorted_indices] = np.arange(len(x))\n    inverse_permutation = np.argsort(inverse)\n    return inverse_permutation",
        "import_statements": [
            "from typing import Optional, Union"
        ],
        "reference_api": [
            "np.arange",
            "np.zeros_like",
            "len",
            "np.argsort"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.argsort",
            "np.zeros_like",
            "np.arange",
            "np.argsort"
        ]
    },
    {
        "subclass": "seaborn",
        "owner/repo": "jiuguangw/Agenoria",
        "file_path": "agenoria/plot_medical_charts.py",
        "function_declaration": "def plot_medical_charts() -> None",
        "start_line": "85",
        "end_line": "106",
        "docstring": "The function plot_medical_charts configures the plotting environment and generates a set of medical-related charts. It sets the style, creates a figure with subplots, and plots various medical data on these subplots, including total vomit per month, days between vomit, days in daycare, and doctor visits. Finally, it adjusts the layout of the subplots and exports the figure to a specified file location.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "040d73b37229",
        "ground_truth": "def plot_medical_charts() -> None:\n    register_matplotlib_converters()\n     # Style\n    sns.set(style=\"darkgrid\")\n    fig, axarr = plt.subplots(2, 3)\n     # Chart 1 - Total Vomit Per Month\n    plot_monthly_vomit(axarr[0, 0], misc_data)\n     # Chart 2 - Days Between Vomit\n    plot_days_between_vomit(axarr[0, 1], misc_data)\n     # Chart 3 - Days in Daycare\n    plot_daycare_days(axarr[0, 2], misc_data)\n     # Chart 4 - Doctor Visits\n    plot_doctor_visit_monthly(axarr[1, 0], misc_data)\n     # Export\n    fig.subplots_adjust(wspace=0.25, hspace=0.35)\n    export_figure(fig, config[\"output_data\"][\"output_medical_charts\"])",
        "import_statements": [
            "from pandas.plotting import register_matplotlib_converters",
            "from config import misc_data",
            "from config import param as config"
        ],
        "reference_api": [
            "plt.subplots",
            "plot_daycare_days",
            "plot_days_between_vomit",
            "plot_monthly_vomit",
            "export_figure",
            "sns.set",
            "plot_doctor_visit_monthly",
            "fig.subplots_adjust",
            "register_matplotlib_converters"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "plot_monthly_vomit",
                "code": "def plot_monthly_vomit(plot_object: plt.figure, data: pd.DataFrame) -> None:\n    # Group and compute sum by month. BMS gives 1st of month\n    vomit_monthly = data[\"Vomit\"].resample(\"BMS\").sum()\n\n    # Plot\n    plot_object.plot(vomit_monthly.index, vomit_monthly)\n    plot_object.set_title(\"Total Number of Vomits by Months\")\n    plot_object.set_ylabel(\"Total Number of Vomits\")\n    format_monthly_plot(\n        plot_object,\n        vomit_monthly.index[0],\n        vomit_monthly.index[-1],\n    )"
            },
            {
                "name": "plot_days_between_vomit",
                "code": "def plot_days_between_vomit(\n    plot_object: plt.figure,\n    data: pd.DataFrame,\n) -> None:\n    # Look up vomit days and compute gaps\n    vomit_days = data.loc[data[\"Vomit\"] == 1]\n    days_since_last_vomit = vomit_days[\"Date\"].diff() / np.timedelta64(1, \"D\")\n\n    # Plots\n    plot_object.plot(vomit_days[\"Date\"], days_since_last_vomit)\n    plot_object.set_title(\"Days Since Last Vomit\")\n    plot_object.set_xlabel(\"Date\")\n    plot_object.set_ylabel(\"Days Since Last Vomit\")\n    format_monthly_plot(plot_object, vomit_days.index[0], vomit_days.index[-1])"
            },
            {
                "name": "plot_daycare_days",
                "code": "def plot_daycare_days(plot_object: plt.figure, data: pd.DataFrame) -> None:\n    # Group and compute sum by month. BMS gives 1st of month\n    daycare_monthly = data[\"Daycare\"].resample(\"BMS\").sum()\n\n    # Plot\n    plot_object.plot(daycare_monthly.index, daycare_monthly)\n    plot_object.set_title(\"Number of Days in Daycare by Months\")\n    plot_object.set_ylabel(\"Number of Days\")\n    plot_object.yaxis.set_ticks(np.arange(0, 21, 2))\n    format_monthly_plot(\n        plot_object,\n        daycare_monthly.index[0],\n        daycare_monthly.index[-1],\n    )"
            },
            {
                "name": "plot_doctor_visit_monthly",
                "code": "def plot_doctor_visit_monthly(\n    plot_object: plt.figure,\n    data: pd.DataFrame,\n) -> None:\n    # Group and compute sum by month. BMS gives 1st of month\n    doctor_monthly = data[\"Doctor\"].resample(\"BMS\").sum()\n\n    # Plot\n    plot_object.plot(doctor_monthly.index, doctor_monthly)\n    plot_object.set_title(\"Total Number of Doctor Visits by Months\")\n    plot_object.set_ylabel(\"Total Number of Doctor Visits\")\n    plot_object.yaxis.set_ticks(np.arange(0, 5, 1))\n    format_monthly_plot(\n        plot_object,\n        doctor_monthly.index[0],\n        doctor_monthly.index[-1],\n    )"
            }
        ],
        "third_party": [
            "register_matplotlib_converters",
            "sns.set",
            "plt.subplots",
            "fig.subplots_adjust",
            "export_figure"
        ]
    },
    {
        "subclass": "seaborn",
        "owner/repo": "jiuguangw/Agenoria",
        "file_path": "agenoria/plot_24h_viz.py",
        "function_declaration": "def plot_feeding_24h_viz() -> None",
        "start_line": "108",
        "end_line": "153",
        "docstring": "The function plot_feeding_24h_viz visualizes 24-hour feeding data for bottle and solid feedings.\\nIt imports and extracts feeding data using parse_raw_data, sets up a plot with a dark grid style, and computes the offset from the birthday based on the last solid feeding date.\\nIt then creates a scatter plot of feeding times, using red for bottle feedings and blue for solid feedings, adding a legend to distinguish between the two.\\nThe function calculates the end date for the plot, formats the plot to display feeding data over a week in a 24-hour format, and exports the figure using export_figure.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7061f4d0c3fc",
        "ground_truth": "def plot_feeding_24h_viz() -> None:\n    # Import and extract feeding data\n    data_bottle = parse_raw_data(feeding_bottle_data, [\"Time of feeding\"])\n    data_solid = parse_raw_data(feeding_solid_data, [\"Time of feeding\"])\n     # Plot setup\n    sns.set(style=\"darkgrid\")\n    figure = plt.figure()\n    fig_ax = figure.add_subplot(111)\n     # Compute offset from birthday\n    offset = data_solid[\"Date\"].iloc[-1] - pd.Timestamp(\n        config[\"info\"][\"birthday\"],\n    )\n    offset = int(offset / np.timedelta64(1, \"D\"))  # Convert to day in int\n     # Plot\n    fig_ax.scatter(\n        data_bottle[\"day_number\"],\n        data_bottle[\"timestamp_hour\"],\n        s=25,\n        c=\"r\",\n    )\n    fig_ax.scatter(\n        data_solid[\"day_number\"] + offset,\n        data_solid[\"timestamp_hour\"],\n        s=25,\n        c=\"b\",\n    )\n     # Legend\n    red_patch = plot_patches.Patch(color=\"r\", label=\"Bottle Feeding\")\n    blue_patch = plot_patches.Patch(color=\"b\", label=\"Solid Feeding\")\n    plt.legend(handles=[red_patch, blue_patch])\n     # End date - one year or full\n    end_date = get_end_date(\n        data_bottle[\"day_number\"],\n        first_year_only=config[\"output_format\"][\"output_year_one_only\"],\n    )\n     # Format plot\n    format_24h_week_plot_horizontal(fig_ax, end_date, \"Feeding\")\n     # Export figure\n    export_figure(figure, config[\"output_data\"][\"output_feeding_viz\"])",
        "import_statements": [
            "from config import (\n    diaper_data,\n    feeding_bottle_data,\n    feeding_solid_data,\n    sleep_data,\n)",
            "from config import param as config"
        ],
        "reference_api": [
            "format_24h_week_plot_horizontal",
            "plt.legend",
            "plt.figure",
            "plot_patches.Patch",
            "pd.Timestamp",
            "int",
            "parse_raw_data",
            "figure.add_subplot",
            "get_end_date",
            "export_figure",
            "fig_ax.scatter",
            "sns.set",
            "np.timedelta64"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "parse_raw_data",
                "code": "def parse_raw_data(data: pd.DataFrame, key: list[str]) -> pd.DataFrame:\n    # Get start and end dates\n    start_date = data[\"Date\"].iloc[-1]\n\n    # Convert timesteamp to decimal hour\n    data[\"timestamp_hour\"] = data[key[0]].dt.hour + data[key[0]].dt.minute / 60\n\n    # Convert date to day number\n    data[\"day_number\"] = (data[\"Date\"] - start_date).dt.days + 1\n\n    return data"
            },
            {
                "name": "parse_raw_data",
                "code": "def parse_raw_data(data: pd.DataFrame, key: list[str]) -> pd.DataFrame:\n    # Get start and end dates\n    start_date = data[\"Date\"].iloc[-1]\n\n    # Convert timesteamp to decimal hour\n    data[\"timestamp_hour\"] = data[key[0]].dt.hour + data[key[0]].dt.minute / 60\n\n    # Convert date to day number\n    data[\"day_number\"] = (data[\"Date\"] - start_date).dt.days + 1\n\n    return data"
            },
            {
                "name": "get_end_date",
                "code": "def get_end_date(data: pd.DataFrame, *, first_year_only: bool) -> int:\n    # Assign the end date. Either 365 or actual day number.\n    return 365 if first_year_only else data.iloc[0]"
            }
        ],
        "third_party": [
            "sns.set",
            "plt.figure",
            "figure.add_subplot",
            "pd.Timestamp",
            "np.timedelta64",
            "fig_ax.scatter",
            "fig_ax.scatter",
            "plot_patches.Patch",
            "plot_patches.Patch",
            "plt.legend",
            "format_24h_week_plot_horizontal",
            "export_figure"
        ]
    },
    {
        "subclass": "seaborn",
        "owner/repo": "kochlisGit/ProphitBet-Soccer-Bets-Predictor",
        "file_path": "analysis/targets.py",
        "function_declaration": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):",
        "start_line": "28",
        "end_line": "31",
        "docstring": "The function plot creates a bar plot on the provided axis, showing target counts for a specified classification task. It first retrieves the target counts for the given task, then uses seaborn to generate a bar plot with target names on the x-axis and their corresponding counts on the y-axis.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6c1ffa63d122",
        "ground_truth": "def plot(self, ax, task: ClassificationTask = ClassificationTask.Result, **kwargs):\n    target_counts = self._get_target_counts(task=task)\n    columns = self._target_names[task]\n    sns.barplot(x=columns, y=target_counts, ax=ax)",
        "import_statements": [
            "from analysis.analyzer import FeatureAnalyzer",
            "from models.tasks import ClassificationTask"
        ],
        "reference_api": [
            "self._get_target_counts",
            "sns.barplot"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_target_counts",
                "code": "def _get_target_counts(self, task: ClassificationTask) -> (pd.Series, list[str]):\n        if task not in self._target_counts:\n            if task == ClassificationTask.Result:\n                self._target_counts[task] = self._input_df['Result'].value_counts()[['H', 'D', 'A']].values\n            elif task == ClassificationTask.Over:\n                self._target_counts[task] = ((self._input_df['HG'] + self._input_df['AG']) > 2.5).value_counts().values\n            else:\n                raise NotImplementedError(f'Not implemented target: {task.name}')\n\n        return self._target_counts[task]"
            }
        ],
        "third_party": [
            "sns.barplot"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "lanpa/tensorboardX",
        "file_path": "tensorboardX/utils.py",
        "function_declaration": "def render_to_rgb(figure)",
        "start_line": "21",
        "end_line": "37",
        "docstring": "The function render_to_rgb converts a Matplotlib figure to an RGB image.\\nIt creates a canvas from the figure using plt_backend_agg.FigureCanvasAgg and draws the figure on it.\\nThe RGBA buffer data from the canvas is converted to a NumPy array and reshaped into an image with width and height dimensions and 3 color channels (RGB).\\nThe function optionally closes the figure and returns the image in CHW format (channels, height, width).\\nIf a list of figures is provided, the function processes each figure and returns a stacked array of images.\\nIf a single figure is provided, it returns the corresponding image.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2f81a906bad5",
        "ground_truth": "def render_to_rgb(figure):\n    canvas = plt_backend_agg.FigureCanvasAgg(figure)\n    canvas.draw()\n    data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n    w, h = figure.canvas.get_width_height()\n    image_hwc = data.reshape([h, w, 4])[:, :, 0:3]\n    image_chw = np.moveaxis(image_hwc, source=2, destination=0)\n    if close:\n        plt.close(figure)\n    return image_chw\nif isinstance(figures, list):\n    images = [render_to_rgb(figure) for figure in figures]\n    return np.stack(images)\nelse:\n    image = render_to_rgb(figures)\n    return image",
        "import_statements": [],
        "reference_api": [
            "np.frombuffer",
            "canvas.buffer_rgba",
            "plt.close",
            "np.moveaxis",
            "canvas.draw",
            "plt_backend_agg.FigureCanvasAgg",
            "data.reshape",
            "get_width_height"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "plt_backend_agg.FigureCanvasAgg",
            "canvas.draw",
            "np.frombuffer",
            "canvas.buffer_rgba",
            "get_width_height",
            "data.reshape",
            "np.moveaxis",
            "plt.close"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "lanpa/tensorboardX",
        "file_path": "examples/demo_beholder.py",
        "function_declaration": "def beholder_pytorch()",
        "start_line": "35",
        "end_line": "48",
        "docstring": "The function beholder_pytorch repeatedly generates random tensors and updates a Beholder visualization with them. It runs a loop 1000 times, creating two lists of random tensors with associated names in each iteration. A Beholder object is instantiated with a specified log directory, and its update method is called with the generated tensors and a random frame. The function then sleeps briefly and prints the current iteration index.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c3037b01569c",
        "ground_truth": "def beholder_pytorch():\n    for i in range(1000):\n        fake_param = [tensor_and_name(np.random.randn(128, 768, 3), 'test' + str(i))\n                      for i in range(5)]\n        arrays = [tensor_and_name(np.random.randn(128, 768, 3), 'test' + str(i))\n                  for i in range(5)]\n        beholder = beholder_lib.Beholder(logdir=LOG_DIRECTORY)\n        beholder.update(\n            trainable=fake_param,\n            arrays=arrays,\n            frame=np.random.randn(128, 128),\n        )\n        time.sleep(0.1)\n        print(i)",
        "import_statements": [
            "import time",
            "from collections import namedtuple"
        ],
        "reference_api": [
            "print",
            "randn",
            "beholder_lib.Beholder",
            "tensor_and_name",
            "time.sleep",
            "str",
            "beholder.update",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tensor_and_name",
            "randn",
            "tensor_and_name",
            "randn",
            "beholder_lib.Beholder",
            "beholder.update",
            "randn"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "lanpa/tensorboardX",
        "file_path": "tensorboardX/comet_utils.py",
        "function_declaration": "def log_pr_data(self, tag, summary, num_thresholds, step=None)",
        "start_line": "323",
        "end_line": "349",
        "docstring": "The function log_pr_data takes a tag, summary, number of thresholds, and an optional step as arguments.\\nIt extracts tensor data from the summary and reshapes it based on its dimensions.\\nThe function computes thresholds as a list of values from 0 to 1 divided by the number of thresholds.\\nIt splits the tensor values into true positives (TP), false positives (FP), true negatives (TN), false negatives (FN), precision, and recall, flattening each into a list.\\nThese values, along with the thresholds and tag, are stored in a dictionary named pr_data.\\nFinally, it logs the pr_data using the log_asset_data method with the provided tag and optional step.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cd57b5de8a88",
        "ground_truth": "def log_pr_data(self, tag, summary, num_thresholds, step=None):\n    \"\"\"Logs a Precision-Recall Curve Data as an asset.\n    Args:\n    tag: An identifier for the PR curve\n    summary: TensorboardX Summary protocol buffer.\n    step: step value to record\n    \"\"\"\n    tensor_proto = summary.value[0].tensor\n    shape = [d.size for d in tensor_proto.tensor_shape.dim]\n    values = np.fromiter(tensor_proto.float_val, dtype=np.float32).reshape(shape)\n    thresholds = [1.0 / num_thresholds * i for i in range(num_thresholds)]\n    tp, fp, tn, fn, precision, recall = map(lambda x: x.flatten().tolist(), np.vsplit(values, values.shape[0]))\n    pr_data = {\n        'TP': tp,\n        'FP': fp,\n        'TN': tn,\n        'FN': fn,\n        'precision': precision,\n        'recall': recall,\n        'thresholds': thresholds,\n        'name': tag,\n    }\n    self.log_asset_data(pr_data, name=tag, step=step)",
        "import_statements": [
            "import logging",
            "import json",
            "import functools",
            "from io import BytesIO",
            "from google.protobuf.json_format import MessageToJson"
        ],
        "reference_api": [
            "x.flatten",
            "tolist",
            "map",
            "self.log_asset_data",
            "reshape",
            "np.fromiter",
            "np.vsplit",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "reshape",
            "np.fromiter",
            "tolist",
            "x.flatten",
            "np.vsplit",
            "self.log_asset_data"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "lanpa/tensorboardX",
        "file_path": "examples/chainer/plain_logger/data.py",
        "function_declaration": "def load_mnist(images, labels, num)",
        "start_line": "18",
        "end_line": "31",
        "docstring": "The function load_mnist reads a specified number of MNIST images and labels from compressed files and returns them as NumPy arrays. It initializes empty arrays for the data and target labels, then opens the image and label files using gzip. It skips the header bytes in both files and reads the image and label data byte by byte, storing them in the respective arrays. Finally, it returns the data and target arrays.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7c4ea9beecc9",
        "ground_truth": "def load_mnist(images, labels, num):\n    data = np.zeros(num * dim, dtype=np.uint8).reshape((num, dim))\n    target = np.zeros(num, dtype=np.uint8).reshape((num, ))\n     with gzip.open(images, 'rb') as f_images,\\\n            gzip.open(labels, 'rb') as f_labels:\n        f_images.read(16)\n        f_labels.read(8)\n        for i in six.moves.range(num):\n            target[i] = ord(f_labels.read(1))\n            for j in six.moves.range(dim):\n                data[i, j] = ord(f_images.read(1))\n     return data, target",
        "import_statements": [
            "import gzip",
            "import os",
            "import six",
            "from six.moves.urllib import request"
        ],
        "reference_api": [
            "f_labels.read",
            "np.zeros",
            "reshape",
            "gzip.open",
            "ord",
            "f_images.read",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "reshape",
            "np.zeros",
            "reshape",
            "np.zeros",
            "f_images.read",
            "f_labels.read",
            "f_labels.read",
            "f_images.read"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "lux-org/lux",
        "file_path": "lux/executor/PandasExecutor.py",
        "function_declaration": "def compute_stats(self, ldf: LuxDataFrame)",
        "start_line": "619",
        "end_line": "647",
        "docstring": "The function compute_stats initializes and computes several statistics for a LuxDataFrame (ldf).\\nIt sets up dictionaries for unique values, min-max values, and cardinality, and stores the length of the dataframe.\\nFor each attribute in the dataframe's columns, it computes unique values and cardinality.\\nIf the attribute is a timestamp, its string representation is used as the dictionary key.\\nFor numeric attributes, it also computes the minimum and maximum values.\\nIf the dataframe's index is not of integer type, it computes unique values and cardinality for the index as well.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bf4f1c828b25",
        "ground_truth": "def compute_stats(self, ldf: LuxDataFrame):\n    # precompute statistics\n    ldf.unique_values = {}\n    ldf._min_max = {}\n    ldf.cardinality = {}\n    ldf._length = len(ldf)\n    for attribute in ldf.columns:\n        if isinstance(attribute, pd._libs.tslibs.timestamps.Timestamp):\n            # If timestamp, make the dictionary keys the _repr_ (e.g., TimeStamp('2020-04-05 00.000')--> '2020-04-05')\n            attribute_repr = str(attribute._date_repr)\n        else:\n            attribute_repr = attribute\n        ldf.unique_values[attribute_repr] = list(ldf[attribute].unique())\n        ldf.cardinality[attribute_repr] = len(ldf.unique_values[attribute_repr])\n        if pd.api.types.is_float_dtype(ldf.dtypes[attribute]) or pd.api.types.is_integer_dtype(\n            ldf.dtypes[attribute]\n        ):\n            ldf._min_max[attribute_repr] = (\n                ldf[attribute].min(),\n                ldf[attribute].max(),\n            )\n    if not pd.api.types.is_integer_dtype(ldf.index):\n        index_column_name = ldf.index.name\n        ldf.unique_values[index_column_name] = list(ldf.index)\n        ldf.cardinality[index_column_name] = len(ldf.index)",
        "import_statements": [
            "from lux.vis.VisList import VisList",
            "from lux.vis.Vis import Vis",
            "from lux.core.frame import LuxDataFrame",
            "from lux.executor.Executor import Executor",
            "from lux.utils import utils",
            "from lux.utils.date_utils import is_datetime_series, is_timedelta64_series, timedelta64_to_float_seconds",
            "from lux.utils.utils import check_import_lux_widget, check_if_id_like, is_numeric_nan_column",
            "import warnings",
            "import lux",
            "from lux.utils.tracing_utils import LuxTracer"
        ],
        "reference_api": [
            "list",
            "min",
            "is_integer_dtype",
            "unique",
            "len",
            "isinstance",
            "is_float_dtype",
            "max",
            "str"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "unique",
            "is_float_dtype",
            "is_integer_dtype",
            "is_integer_dtype"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "lux-org/lux",
        "file_path": "lux/executor/SQLExecutor.py",
        "function_declaration": "def execute_preview(tbl: LuxSQLTable, preview_size=5)",
        "start_line": "28",
        "end_line": "31",
        "docstring": "The function execute_preview generates a preview of a SQL table using a specified query template and returns the result as a pandas DataFrame. It takes a LuxSQLTable object and an optional preview size parameter. The function formats a preview query with the table name and number of rows, executes the query using a predefined SQL connection, and retrieves the data into a DataFrame, which is then returned.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3e6fc3d2f47c",
        "ground_truth": "def execute_preview(tbl: LuxSQLTable, preview_size=5):\n    preview_query = lux.config.query_templates['preview_query']\n    output = pandas.read_sql(preview_query.format(table_name = tbl.table_name, num_rows = preview_size), lux.config.SQLconnection)\n    return output",
        "import_statements": [
            "import pandas",
            "from lux.vis.VisList import VisList",
            "from lux.vis.Vis import Vis",
            "from lux.core.sqltable import LuxSQLTable",
            "from lux.executor.Executor import Executor",
            "from lux.utils import utils",
            "from lux.utils.utils import check_import_lux_widget, check_if_id_like",
            "import lux",
            "import math"
        ],
        "reference_api": [
            "preview_query.format",
            "pandas.read_sql"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "pandas.read_sql",
            "preview_query.format"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "lux-org/lux",
        "file_path": "lux/executor/SQLExecutor.py",
        "function_declaration": "def execute_sampling(tbl: LuxSQLTable)",
        "start_line": "34",
        "end_line": "43",
        "docstring": "The function execute_sampling takes a LuxSQLTable object tbl as an argument.\\nIt retrieves sampling configuration parameters from lux.config, including SAMPLE_FLAG, SAMPLE_START, SAMPLE_CAP, and SAMPLE_FRAC.\\nIt executes a SQL query to determine the length of the table and calculates a limit based on a fraction of the table length.\\nUsing this limit, it formats and executes a sample query to retrieve a sample of rows from the table.\\nThe sampled rows are then stored in the _sampled attribute of the tbl object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b3dee87be4b8",
        "ground_truth": "def execute_sampling(tbl: LuxSQLTable):\n    SAMPLE_FLAG = lux.config.sampling\n    SAMPLE_START = lux.config.sampling_start\n    SAMPLE_CAP = lux.config.sampling_cap\n    SAMPLE_FRAC = 0.2\n    length_query = pandas.read_sql(lux.config.query_templates['length_query'].format(table_name = tbl.table_name, where_clause = \"\"),lux.config.SQLconnection,)\n    limit = int(list(length_query[\"length\"])[0]) * SAMPLE_FRAC\n    sample_query = lux.config.query_templates['sample_query'].format(table_name = tbl.table_name, where_clause = \"\", num_rows = str(int(limit)))\n    tbl._sampled = pandas.read_sql(sample_query, lux.config.SQLconnection)",
        "import_statements": [
            "import pandas",
            "from lux.vis.VisList import VisList",
            "from lux.vis.Vis import Vis",
            "from lux.core.sqltable import LuxSQLTable",
            "from lux.executor.Executor import Executor",
            "from lux.utils import utils",
            "from lux.utils.utils import check_import_lux_widget, check_if_id_like",
            "import lux",
            "import math"
        ],
        "reference_api": [
            "list",
            "int",
            "pandas.read_sql",
            "str",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "pandas.read_sql",
            "pandas.read_sql"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "marceloprates/prettymaps",
        "file_path": "prettymaps/draw.py",
        "function_declaration": "def draw_text(\n    params: Dict[str, dict],\n    background: BaseGeometry\n) -> None",
        "start_line": "555",
        "end_line": "590",
        "docstring": "The function draw_text places text on a plot with specified parameters and a background. It first overrides default text settings with provided parameters, extracting the text, x, and y values. The function then retrieves the background's bounds and interpolates the x and y values to fit within these bounds. Finally, it uses the plt.text method to draw the text at the calculated position with the remaining parameters.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "485253b1088a",
        "ground_truth": "def draw_text(params: Dict[str, dict], background: BaseGeometry) -> None:\n    \"\"\"\n    Draw text with content and matplotlib style parameters specified by 'params' dictionary.\n    params['text'] should contain the message to be drawn\n     Args:\n        params (Dict[str, dict]): matplotlib style parameters for drawing text. params['text'] should contain the message to be drawn.\n        background (BaseGeometry): Background layer\n    \"\"\"\n    # Override default osm_credit dict with provided parameters\n    params = override_params(\n        dict(\n            text=\"\\n\".join(\n                [\n                    \"data \u00c2\u00a9 OpenStreetMap contributors\",\n                    \"github.com/marceloprates/prettymaps\",\n                ]\n            ),\n            x=0,\n            y=1,\n            horizontalalignment=\"left\",\n            verticalalignment=\"top\",\n            bbox=dict(boxstyle=\"square\", fc=\"#fff\", ec=\"#000\"),\n            fontfamily=\"Ubuntu Mono\",\n        ),\n        params,\n    )\n    x, y, text = [params.pop(k) for k in [\"x\", \"y\", \"text\"]]\n     # Get background bounds\n    xmin, ymin, xmax, ymax = background.bounds\n     x = np.interp([x], [0, 1], [xmin, xmax])[0]\n    y = np.interp([y], [0, 1], [ymin, ymax])[0]\n     plt.text(x, y, text, **params)",
        "import_statements": [
            "import re",
            "import os",
            "import json",
            "import pathlib",
            "import warnings",
            "import matplotlib",
            "import shapely.ops",
            "import shapely.affinity",
            "from copy import deepcopy",
            "from dataclasses import dataclass",
            "from matplotlib import pyplot as plt",
            "from matplotlib.colors import hex2color",
            "from matplotlib.patches import Path, PathPatch",
            "from shapely.geometry.base import BaseGeometry",
            "from typing import Optional, Union, Tuple, List, Dict, Any, Iterable",
            "from shapely.geometry import (\n    Point,\n    LineString,\n    MultiLineString,\n    Polygon,\n    MultiPolygon,\n    GeometryCollection,\n    box,\n)"
        ],
        "reference_api": [
            "join",
            "plt.text",
            "dict",
            "np.interp",
            "override_params",
            "params.pop"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "override_params",
                "code": "def override_params(default_dict: dict, new_dict: dict) -> dict:\n    \"\"\"\n    Override parameters in 'default_dict' with additional parameters from 'new_dict'\n\n    Args:\n        default_dict (dict): Default dict to be overriden with 'new_dict' parameters\n        new_dict (dict): New dict to override 'default_dict' parameters\n\n    Returns:\n        dict: default_dict overriden with new_dict parameters\n    \"\"\"\n\n    final_dict = deepcopy(default_dict)\n\n    for key in new_dict.keys():\n        if type(new_dict[key]) == dict:\n            if key in final_dict:\n                final_dict[key] = override_params(final_dict[key], new_dict[key])\n            else:\n                final_dict[key] = new_dict[key]\n        else:\n            final_dict[key] = new_dict[key]\n\n    return final_dict"
            }
        ],
        "third_party": [
            "join",
            "params.pop",
            "np.interp",
            "np.interp",
            "plt.text"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "matplotlib/mplfinance",
        "file_path": "src/mplfinance/_helpers.py",
        "function_declaration": "def _determine_format_string( dates, datetime_format=None )",
        "start_line": "41",
        "end_line": "64",
        "docstring": "The function _determine_format_string takes a list of dates and an optional datetime_format.\\nIt calculates the average number of days between consecutive dates in the list.\\nIf datetime_format is provided, it returns this format.\\nIf the average days between points is less than 0.33, indicating intraday data, it checks if the data spans more than one day and sets the format string to '%b %d, %H:%M' or '%H:%M' accordingly.\\nFor daily or less frequent data, it checks if the data spans multiple years and sets the format string to '%Y-%b-%d' or '%b %d' as appropriate.\\nFinally, it returns the determined format string.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5f70a88b6b72",
        "ground_truth": "def _determine_format_string( dates, datetime_format=None ):\n    \"\"\"\n    Determine the datetime format string based on the averge number\n    of days between data points, or if the user passed in kwarg\n    datetime_format, use that as an override.\n    \"\"\"\n    avg_days_between_points = (dates[-1] - dates[0]) / float(len(dates))\n     if datetime_format is not None:\n        return datetime_format\n     # avgerage of 3 or more data points per day we will call intraday data:\n    if avg_days_between_points < 0.33:  # intraday\n        if mdates.num2date(dates[-1]).date() != mdates.num2date(dates[0]).date():\n            # intraday data for more than one day:\n            fmtstring = '%b %d, %H:%M'\n        else:  # intraday data for a single day\n            fmtstring = '%H:%M'\n    else:  # 'daily' data (or could be weekly, etc.)\n        if mdates.num2date(dates[-1]).date().year != mdates.num2date(dates[0]).date().year:\n           fmtstring = '%Y-%b-%d'\n        else:\n           fmtstring = '%b %d'\n    return fmtstring",
        "import_statements": [
            "import datetime"
        ],
        "reference_api": [
            "date",
            "len",
            "mdates.num2date",
            "float"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "date",
            "mdates.num2date",
            "date",
            "mdates.num2date",
            "date",
            "mdates.num2date",
            "date",
            "mdates.num2date"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "matplotlib/mplfinance",
        "file_path": "src/mplfinance/_helpers.py",
        "function_declaration": "def _mpf_to_rgba(c, alpha=None)",
        "start_line": "116",
        "end_line": "121",
        "docstring": "The function _mpf_to_rgba converts a color input to an RGBA tuple. It checks if the color input is in uint8 RGB or RGBA format and normalizes it if any RGB components are greater than 1. The normalization scales the RGB components to the range [0, 1]. If the input has an alpha component, it retains it. Finally, the function converts the processed color input to an RGBA tuple using a color conversion utility, optionally applying a specified alpha value.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f9589bf4666d",
        "ground_truth": "def _mpf_to_rgba(c, alpha=None):\n    cnew = c\n    if _is_uint8_rgb_or_rgba(c) and any(e>1 for e in c[:3]):\n        cnew = tuple([e/255. for e in c[:3]])\n        if len(c) == 4: cnew += c[3:]\n    return mcolors.to_rgba(cnew, alpha)",
        "import_statements": [
            "import datetime"
        ],
        "reference_api": [
            "any",
            "_is_uint8_rgb_or_rgba",
            "tuple",
            "len",
            "mcolors.to_rgba"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_is_uint8_rgb_or_rgba",
                "code": "def _is_uint8_rgb_or_rgba(tup):\n    \"\"\" Deterine if rgb or rgba is in (0-255) format:\n    Matplotlib expects rgb (and rgba) tuples to contain\n    three (or four) floats between 0.0 and 1.0 \n    \n    Some people express rgb as tuples of three integers\n    between 0 and 255.\n    (In rgba, alpha is still a float from 0.0 to 1.0)\n    \"\"\"\n    if isinstance(tup,str):  return False\n    if not np.iterable(tup): return False\n    L = len(tup)\n    if L < 3 or L > 4: return False\n    if L == 4 and (tup[3] < 0 or tup[3] > 1): return False\n    return not any([not isinstance(v,(int,np.unsignedinteger)) or v<0 or v>255 for v in tup[0:3]])"
            }
        ],
        "third_party": [
            "mcolors.to_rgba"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "matplotlib/mplfinance",
        "file_path": "src/mplfinance/_mplwraps.py",
        "function_declaration": "def subplots(self,*args,**kwargs)",
        "start_line": "103",
        "end_line": "121",
        "docstring": "The function subplots is a method that creates a set of subplots, handling custom styles.\\nIt accepts any number of positional and keyword arguments.\\nIf the 'style' keyword argument is provided or the object does not have an attribute 'mpfstyle', it applies the specified style using _check_for_and_apply_style and sets 'mpfstyle' attribute to the applied style.\\nOtherwise, it applies the existing 'mpfstyle'.\\nThe function then calls the subplots method of matplotlib's Figure class to create the subplots.\\nIt assigns the applied style to each of the created axes objects.\\nIf the result is a single Axes object, it assigns the style to it directly.\\nIf the result is an ndarray of Axes objects, it assigns the style to each Axes in the array.\\nIf the result is of an unexpected type, it raises a TypeError.\\nFinally, it returns the created subplots.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4202e378ab0b",
        "ground_truth": "def subplots(self,*args,**kwargs):\n     if 'style' in kwargs or not hasattr(self,'mpfstyle'):\n        style = _check_for_and_apply_style(kwargs)\n        self.mpfstyle = style\n    else:\n        style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n     axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n    if isinstance(axlist,mpl_axes.Axes):\n        axlist.mpfstyle = style\n    elif isinstance(axlist,np.ndarray):\n        for ax in axlist.flatten():\n            ax.mpfstyle = style\n    else:\n       raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                       'returned from \"matplotlib.figure.Figure.subplots()\"')\n    return axlist",
        "import_statements": [
            "from   mplfinance import _styles"
        ],
        "reference_api": [
            "axlist.flatten",
            "subplots",
            "hasattr",
            "TypeError",
            "dict",
            "isinstance",
            "type",
            "_check_for_and_apply_style",
            "str"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_check_for_and_apply_style",
                "code": "def _check_for_and_apply_style(kwargs):\n\n    if 'style' in kwargs:\n        style = kwargs['style']\n        del kwargs['style']\n    else:\n        style = 'default'\n\n    if not _styles._valid_mpf_style(style):\n        raise TypeError('Invalid mplfinance style')\n\n    if isinstance(style,str):\n        style = _styles._get_mpfstyle(style)\n\n    if isinstance(style,dict):\n        _styles._apply_mpfstyle(style)\n    else:\n        raise TypeError('style should be a `dict`; why is it not?')\n\n    return style"
            },
            {
                "name": "_check_for_and_apply_style",
                "code": "def _check_for_and_apply_style(kwargs):\n\n    if 'style' in kwargs:\n        style = kwargs['style']\n        del kwargs['style']\n    else:\n        style = 'default'\n\n    if not _styles._valid_mpf_style(style):\n        raise TypeError('Invalid mplfinance style')\n\n    if isinstance(style,str):\n        style = _styles._get_mpfstyle(style)\n\n    if isinstance(style,dict):\n        _styles._apply_mpfstyle(style)\n    else:\n        raise TypeError('style should be a `dict`; why is it not?')\n\n    return style"
            },
            {
                "name": "subplots",
                "code": "def subplots(self,*args,**kwargs):\n    \n        if 'style' in kwargs or not hasattr(self,'mpfstyle'):\n            style = _check_for_and_apply_style(kwargs)\n            self.mpfstyle = style\n        else:\n            style = _check_for_and_apply_style(dict(style=self.mpfstyle))\n    \n        axlist = mplfigure.Figure.subplots(self,*args,**kwargs)\n\n        if isinstance(axlist,mpl_axes.Axes):\n            axlist.mpfstyle = style\n        elif isinstance(axlist,np.ndarray):\n            for ax in axlist.flatten():\n                ax.mpfstyle = style\n        else:\n           raise TypeError('Unexpected type ('+str(type(axlist))+') '+\n                           'returned from \"matplotlib.figure.Figure.subplots()\"')\n        return axlist"
            }
        ],
        "third_party": [
            "axlist.flatten"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "matplotlib/mplfinance",
        "file_path": "src/mplfinance/plotting.py",
        "function_declaration": "def _adjust_figsize(fig,config)",
        "start_line": "998",
        "end_line": "1014",
        "docstring": "The function _adjust_figsize adjusts the size of a given figure based on a configuration dictionary. If the figure is not provided, it returns immediately. If the configuration does not specify a figure size, it calculates the size using the figure ratio, ensuring the aspect ratio is within a valid range. It scales the dimensions accordingly. If a figure size is specified in the configuration, it uses that size. Finally, it sets the figure size in inches.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3e2b6dfe4753",
        "ground_truth": "def _adjust_figsize(fig,config):\n    if fig is None:\n        return\n    if config['figsize'] is None:\n        w,h = config['figratio']\n        r = float(w)/float(h)\n        if r < 0.20 or r > 5.0:\n            raise ValueError('\"figratio\" (aspect ratio)  must be between 0.20 and 5.0 (but is '+str(r)+')')\n        default_scale = DEFAULT_FIGRATIO[1]/h\n        h *= default_scale\n        w *= default_scale\n        base      = (w,h)\n        figscale  = config['figscale']\n        fsize     = [d*figscale for d in base]\n    else:\n        fsize = config['figsize']\n    fig.set_size_inches(fsize)",
        "import_statements": [
            "import copy",
            "import io",
            "import os",
            "import math",
            "import warnings",
            "from itertools import cycle",
            "from mplfinance._utils import _construct_aline_collections",
            "from mplfinance._utils import _construct_hline_collections",
            "from mplfinance._utils import _construct_vline_collections",
            "from mplfinance._utils import _construct_tline_collections",
            "from mplfinance._utils import _construct_mpf_collections",
            "from mplfinance._utils import _construct_pnf_scatter",
            "from mplfinance._widths import _determine_width_config",
            "from mplfinance._utils import _updown_colors",
            "from mplfinance._utils import IntegerIndexDateTimeFormatter",
            "from mplfinance._utils import _mscatter",
            "from mplfinance._utils import _check_and_convert_xlim_configuration",
            "from mplfinance import _styles",
            "from mplfinance._arg_validators import _check_and_prepare_data, _mav_validator, _label_validator",
            "from mplfinance._arg_validators import _get_valid_plot_types, _fill_between_validator",
            "from mplfinance._arg_validators import _process_kwargs, _validate_vkwargs_dict",
            "from mplfinance._arg_validators import _kwarg_not_implemented, _bypass_kwarg_validation",
            "from mplfinance._arg_validators import _hlines_validator, _vlines_validator",
            "from mplfinance._arg_validators import _alines_validator, _tlines_validator",
            "from mplfinance._arg_validators import _scale_padding_validator, _yscale_validator",
            "from mplfinance._arg_validators import _valid_panel_id, _check_for_external_axes",
            "from mplfinance._arg_validators import _xlim_validator, _mco_validator, _is_marketcolor_object",
            "from mplfinance._panels import _build_panels",
            "from mplfinance._panels import _set_ticks_on_bottom_panel_only",
            "from mplfinance._helpers import _determine_format_string",
            "from mplfinance._helpers import _list_of_dict",
            "from mplfinance._helpers import _num_or_seq_of_num",
            "from mplfinance._helpers import _adjust_color_brightness"
        ],
        "reference_api": [
            "ValueError",
            "str",
            "fig.set_size_inches",
            "float"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "fig.set_size_inches"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "matplotlib/mplfinance",
        "file_path": "src/mplfinance/plotting.py",
        "function_declaration": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None)",
        "start_line": "1217",
        "end_line": "1247",
        "docstring": "The function _plot_mav plots moving averages on a given axis ax using provided configuration config, date data xdates, and price data prices.\\nIt determines the moving averages to plot from the config or apmav parameter.\\nIf the moving averages are specified as a dictionary, it extracts the shift values and periods.\\nThe function ensures at most 7 moving averages are plotted.\\nFor each moving average, it calculates the rolling mean of the prices, applies any shifts, and plots the result on the axis using the specified line width and color cycle from the config.\\nThe function returns a list of the calculated moving average prices.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "986e5804b941",
        "ground_truth": "def _plot_mav(ax,config,xdates,prices,apmav=None,apwidth=None):\n    style = config['style']\n    if apmav is not None:\n        mavgs = apmav\n    else:\n        mavgs = config['mav']\n    mavp_list = []\n    if mavgs is not None:\n        shift = None\n        if isinstance(mavgs,dict):\n            shift = mavgs['shift']\n            mavgs = mavgs['period']\n        if isinstance(mavgs,int):\n            mavgs = mavgs,      # convert to tuple\n        if len(mavgs) > 7:\n            mavgs = mavgs[0:7]  # take at most 7\n              mavc = config['_ma_color_cycle']\n         for idx,mav in enumerate(mavgs):\n            mean = pd.Series(prices).rolling(mav).mean()\n            if shift is not None:\n                mean = mean.shift(periods=shift[idx])\n            mavprices = mean.values\n            lw = config['_width_config']['line_width']\n            if mavc:\n                ax.plot(xdates, mavprices, linewidth=lw, color=next(mavc))\n            else:\n                ax.plot(xdates, mavprices, linewidth=lw)\n            mavp_list.append(mavprices)\n    return mavp_list",
        "import_statements": [
            "import copy",
            "import io",
            "import os",
            "import math",
            "import warnings",
            "from itertools import cycle",
            "from mplfinance._utils import _construct_aline_collections",
            "from mplfinance._utils import _construct_hline_collections",
            "from mplfinance._utils import _construct_vline_collections",
            "from mplfinance._utils import _construct_tline_collections",
            "from mplfinance._utils import _construct_mpf_collections",
            "from mplfinance._utils import _construct_pnf_scatter",
            "from mplfinance._widths import _determine_width_config",
            "from mplfinance._utils import _updown_colors",
            "from mplfinance._utils import IntegerIndexDateTimeFormatter",
            "from mplfinance._utils import _mscatter",
            "from mplfinance._utils import _check_and_convert_xlim_configuration",
            "from mplfinance import _styles",
            "from mplfinance._arg_validators import _check_and_prepare_data, _mav_validator, _label_validator",
            "from mplfinance._arg_validators import _get_valid_plot_types, _fill_between_validator",
            "from mplfinance._arg_validators import _process_kwargs, _validate_vkwargs_dict",
            "from mplfinance._arg_validators import _kwarg_not_implemented, _bypass_kwarg_validation",
            "from mplfinance._arg_validators import _hlines_validator, _vlines_validator",
            "from mplfinance._arg_validators import _alines_validator, _tlines_validator",
            "from mplfinance._arg_validators import _scale_padding_validator, _yscale_validator",
            "from mplfinance._arg_validators import _valid_panel_id, _check_for_external_axes",
            "from mplfinance._arg_validators import _xlim_validator, _mco_validator, _is_marketcolor_object",
            "from mplfinance._panels import _build_panels",
            "from mplfinance._panels import _set_ticks_on_bottom_panel_only",
            "from mplfinance._helpers import _determine_format_string",
            "from mplfinance._helpers import _list_of_dict",
            "from mplfinance._helpers import _num_or_seq_of_num",
            "from mplfinance._helpers import _adjust_color_brightness"
        ],
        "reference_api": [
            "mean.shift",
            "next",
            "ax.plot",
            "len",
            "isinstance",
            "mavp_list.append",
            "pd.Series",
            "mean",
            "enumerate",
            "rolling"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "ax.plot",
                "code": "def plot( data, **kwargs ):\n    \"\"\"\n    Given a Pandas DataFrame containing columns Open,High,Low,Close and optionally Volume\n    with a DatetimeIndex, plot the data.\n    Available plots include ohlc bars, candlestick, and line plots.\n    Also provide visually analysis in the form of common technical studies, such as:\n    moving averages, renko, etc.\n    Also provide ability to plot trading signals, and/or addtional user-defined data.\n    \"\"\"\n\n    config = _process_kwargs(kwargs, _valid_plot_kwargs())\n\n    # translate alias types:\n    config['type'] = _get_valid_plot_types(config['type'])\n    \n    dates,opens,highs,lows,closes,volumes = _check_and_prepare_data(data, config)\n\n    config['xlim'] = _check_and_convert_xlim_configuration(data, config)\n\n    if config['type'] in VALID_PMOVE_TYPES and config['addplot'] is not None:\n        err = \"`addplot` is not supported for `type='\" + config['type'] +\"'`\"\n        raise ValueError(err)\n\n    if config['marketcolor_overrides'] is not None:\n        if len(config['marketcolor_overrides']) != len(dates):\n            raise ValueError('`marketcolor_overrides` must be same length as dataframe.')\n\n    external_axes_mode = _check_for_external_axes(config)\n\n    if external_axes_mode:\n        if config['figscale'] is not None:\n            warnings.warn('\\n\\n ================================================================= '+\n                          '\\n\\n   WARNING: `figscale` has NO effect in External Axes Mode.'+\n                          '\\n\\n ================================================================ ',\n                          category=UserWarning)\n        if config['figratio'] is not None:\n            warnings.warn('\\n\\n ================================================================= '+\n                          '\\n\\n   WARNING: `figratio` has NO effect in External Axes Mode.'+\n                          '\\n\\n ================================================================ ',\n                          category=UserWarning)\n        if config['figsize'] is not None:\n            warnings.warn('\\n\\n ================================================================= '+\n                          '\\n\\n   WARNING: `figsize` has NO effect in External Axes Mode.'+\n                          '\\n\\n ================================================================ ',\n                          category=UserWarning)\n    else:\n        if config['figscale'] is None: config['figscale'] = 1.0\n        if config['figratio'] is None: config['figratio'] = DEFAULT_FIGRATIO\n\n    style = config['style']\n\n    if external_axes_mode and hasattr(config['ax'],'mpfstyle') and style is None:\n        style = config['ax'].mpfstyle\n    elif style is None:\n        style = 'default'\n\n    if isinstance(style,str):\n        style = _styles._get_mpfstyle(style)\n\n    config['style'] = style\n\n    if isinstance(style,dict):\n        if not external_axes_mode: _styles._apply_mpfstyle(style)\n    else:\n        raise TypeError('style should be a `dict`; why is it not?')\n\n    if config['mavcolors'] is not None:\n        config['_ma_color_cycle'] = cycle(config['mavcolors'])\n    elif style['mavcolors'] is not None:\n        config['_ma_color_cycle'] = cycle(style['mavcolors'])\n    else:\n        config['_ma_color_cycle'] = None\n\n    if not external_axes_mode:\n        fig = plt.figure()\n        _adjust_figsize(fig,config)\n    else:\n        fig = None\n\n    _adjust_fontsize(config)\n\n    if config['volume'] and volumes is None:\n        raise ValueError('Request for volume, but NO volume data.')\n\n    if external_axes_mode:\n        panels = None\n        axA1   = config['ax']\n        axA1.set_axisbelow(config['saxbelow'])\n        if config['volume']:\n            volumeAxes = config['volume']\n            volumeAxes.set_axisbelow(config['saxbelow'])\n    else:\n        panels = _build_panels(fig, config)\n        axA1 = panels.at[config['main_panel'],'axes'][0]\n        if config['volume']:\n            if config['volume_panel'] == config['main_panel']:\n                # ohlc and volume on same panel: move volume to secondary axes:\n                volumeAxes = panels.at[config['volume_panel'],'axes'][1]\n                volumeAxes.set_zorder(axA1.get_zorder()-0.1) # Make sure ohlc is above volume\n                axA1.patch.set_visible(False)                # Let volume show through\n                panels.at[config['main_panel'],'used2nd'] = True\n            else:\n                volumeAxes = panels.at[config['volume_panel'],'axes'][0]\n        else:\n            volumeAxes = None\n\n    fmtstring = _determine_format_string(dates, config['datetime_format'])\n\n    ptype = config['type'] \n\n    if config['show_nontrading']:\n        formatter = mdates.DateFormatter(fmtstring)\n        xdates = dates\n    else:\n        formatter = IntegerIndexDateTimeFormatter(dates, fmtstring)\n        xdates = np.arange(len(dates))\n\n    # Will have to handle widths config separately for PMOVE types ??\n    config['_width_config'] = _determine_width_config(xdates, config)\n\n    rwc = config['return_width_config']\n    if isinstance(rwc,dict) and len(rwc)==0:\n        config['return_width_config'].update(config['_width_config'])\n\n    collections = None\n    if ptype == 'line':\n        lw = config['_width_config']['line_width']\n        axA1.plot(xdates, closes, color=config['linecolor'], linewidth=lw)\n    elif ptype == 'pnf':\n        pnf_results = _construct_pnf_scatter(axA1,ptype,dates,xdates,opens,highs,lows,closes,volumes,config,style)\n    else:\n        collections =_construct_mpf_collections(ptype,dates,xdates,opens,highs,lows,closes,volumes,config,style)\n\n    if ptype == 'pnf':\n        volumes    = pnf_results['pnf_volumes']\n        pnf_values = pnf_results['pnf_values']\n        pnf_mdates = mdates.date2num(list(pnf_values.keys()))\n        formatter  = IntegerIndexDateTimeFormatter(pnf_mdates,fmtstring)\n        xdates     = pnf_results['pnf_xdates']\n    elif ptype == 'renko':\n        collections, renko_results = collections\n        volumes       = renko_results['volumes']\n        renko_dates   = renko_results['dates']\n        renko_values  = renko_results['values']\n        formatter = IntegerIndexDateTimeFormatter(renko_dates, fmtstring)\n        renko_avgvals = renko_values\n        renko_size    = renko_results['size']\n        xdates = np.arange(len(renko_dates))\n\n    if collections is not None:\n        for collection in collections:\n            axA1.add_collection(collection)\n\n    #formatter = IntegerIndexDateTimeFormatter(xdates, fmtstring)\n\n    if (ptype == 'pnf' and \n        (config['mav'] is not None or config['ema'] is not None)):\n        warnings.warn('\\n\\n ================================================================ '+\n                      '\\n\\n   MOVING Averages IGNORED for POINT and FIGURE PLOTS!'+\n                      '\\n\\n ================================================================ ',\n                      category=UserWarning)\n    elif ptype == 'renko':\n        mavprices = _plot_mav(axA1,config,xdates,renko_avgvals)\n        emaprices = _plot_ema(axA1,config,xdates,renko_avgvals)\n    else:\n        mavprices = _plot_mav(axA1,config,xdates,closes)\n        emaprices = _plot_ema(axA1,config,xdates,closes)\n\n    avg_dist_between_points = (xdates[-1] - xdates[0]) / float(len(xdates))\n    if not config['tight_layout']:\n        minx = xdates[0]  - avg_dist_between_points\n        maxx = xdates[-1] + avg_dist_between_points\n    else:\n        minx = xdates[0]  - (0.45 * avg_dist_between_points)\n        maxx = xdates[-1] + (0.45 * avg_dist_between_points)\n\n    if len(xdates) == 1:  # kludge special case\n        minx = minx - 0.75\n        maxx = maxx + 0.75\n\n    if ptype == 'renko':\n        _lows  = renko_avgvals \n        _highs = [value+renko_size for value in renko_avgvals]\n    else:\n        _lows  = lows\n        _highs = highs\n\n    if ptype == 'pnf':\n       miny, maxy = pnf_results['pnf_ylimits']\n    else:\n        miny = np.nanmin(_lows)\n        maxy = np.nanmax(_highs)\n\n    if config['ylim'] is not None:\n        axA1.set_ylim(config['ylim'][0], config['ylim'][1])\n    elif config['tight_layout']:\n        ydelta = 0.01 * (maxy-miny)\n        if miny > 0.0:\n            # don't let it go negative:\n            setminy = max(0.9*miny,miny-ydelta)\n        else:\n            setminy = miny-ydelta\n        axA1.set_ylim(setminy,maxy+ydelta)\n\n    if config['xlim'] is not None:\n        axA1.set_xlim(config['xlim'][0], config['xlim'][1])\n    elif config['tight_layout']:\n        axA1.set_xlim(minx,maxx)\n\n    if (config['ylim'] is None and\n        config['xlim'] is None and\n        not config['tight_layout']):\n        corners = (minx, miny), (maxx, maxy)\n        axA1.update_datalim(corners)\n\n    if config['return_calculated_values'] is not None:\n        retdict = config['return_calculated_values']\n        if ptype == 'renko':\n            retdict['renko_bricks' ] = renko_values\n            retdict['renko_dates'  ] = mdates.num2date(renko_dates)\n            retdict['renko_size'   ] = renko_size\n            retdict['renko_volumes'] = volumes if config['volume'] else None\n        elif ptype == 'pnf':\n            retdict['pnf_dates'    ] = mdates.num2date(pnf_mdates)\n            retdict['pnf_values'   ] = pnf_values\n            retdict['pnf_size'     ] = pnf_results['pnf_boxsize']\n            retdict['pnf_volumes'  ] = volumes[:len(pnf_values)] if config['volume'] else None\n        if config['mav'] is not None and ptype != 'pnf':\n            mav = config['mav']\n            if len(mav) != len(mavprices):\n                warnings.warn('len(mav)='+str(len(mav))+' BUT len(mavprices)='+str(len(mavprices)))\n            else:\n                for jj in range(0,len(mav)):     \n                    retdict['mav' + str(mav[jj])] = mavprices[jj]\n        if config['ema'] is not None and ptype != 'pnf':\n            ema = config['ema']\n            if len(ema) != len(emaprices):\n                warnings.warn('len(ema)='+str(len(ema))+' BUT len(emaprices)='+str(len(emaprices)))\n            else:\n                for jj in range(0, len(ema)):\n                    retdict['ema' + str(ema[jj])] = emaprices[jj]\n        retdict['minx'] = minx\n        retdict['maxx'] = maxx\n        retdict['miny'] = miny\n        retdict['maxy'] = maxy\n\n    # Note: these are NOT mutually exclusive, so the order of this\n    #       if/elif is important: VALID_PMOVE_TYPES must be first.\n    if ptype in VALID_PMOVE_TYPES:\n        pmove_dates = pnf_mdates if ptype == 'pnf' else renko_dates\n        dtix = pd.DatetimeIndex([dt for dt in mdates.num2date(pmove_dates)])\n    elif not config['show_nontrading']:\n        dtix = data.index\n    else:\n        dtix = None\n\n    line_collections = []\n    line_collections.append(_construct_aline_collections(config['alines'], dtix))\n    line_collections.append(_construct_hline_collections(config['hlines'], minx, maxx))\n    line_collections.append(_construct_vline_collections(config['vlines'], dtix, miny, maxy))\n    tlines = config['tlines']\n    if isinstance(tlines,(list,tuple)) and all([isinstance(item,dict) for item in tlines]):\n        pass\n    else:\n        tlines = [tlines,]\n    for tline_item in tlines:\n        line_collections.append(_construct_tline_collections(tline_item, dtix, dates, opens, highs, lows, closes))\n     \n    for collection in line_collections:\n        if collection is not None:\n            axA1.add_collection(collection)\n\n    datalen = len(xdates)\n    if config['volume']:\n        mc = style['marketcolors']\n        vup,vdown = mc['volume'].values()\n        #-- print('vup,vdown=',vup,vdown)\n        vcolors = _updown_colors(vup, vdown, opens, closes, use_prev_close=style['marketcolors']['vcdopcod'])\n        #-- print('len(vcolors),len(opens),len(closes)=',len(vcolors),len(opens),len(closes))\n        #-- print('vcolors=',vcolors)\n\n        w  = config['_width_config']['volume_width']\n        lw = config['_width_config']['volume_linewidth']\n\n        veup, vedown = mc['vcedge'].values()\n        if mc['volume'] == mc['vcedge']:\n            edgecolors = _adjust_color_brightness(vcolors,0.90)\n        elif veup != vedown:\n            edgecolors = _updown_colors(veup, vedown, opens, closes, use_prev_close=style['marketcolors']['vcdopcod'])\n        else: \n            edgecolors = veup \n\n        if config['volume_alpha']:\n           valp = config['volume_alpha']\n        elif 'volume_alpha' in mc:\n           valp = mc['volume_alpha']\n        else:\n           valp = 1.0\n        volumeAxes.bar(xdates,volumes,width=w,linewidth=lw,color=vcolors,ec=edgecolors,alpha=valp)\n        if config['volume_ylim'] is not None:\n            vymin = config['volume_ylim'][0]\n            vymax = config['volume_ylim'][1]\n        else:\n            vymin = 0.3 * np.nanmin(volumes)\n            vymax = 1.1 * np.nanmax(volumes)\n        volumeAxes.set_ylim(vymin,vymax)\n\n    xrotation = config['xrotation']\n    if not external_axes_mode:\n        _set_ticks_on_bottom_panel_only(panels,formatter,rotation=xrotation,\n                                        xlabel=config['xlabel'])\n    else:\n        axA1.tick_params(axis='x',rotation=xrotation)\n        axA1.xaxis.set_major_formatter(formatter)\n        axA1.set_xlabel(config['xlabel'])\n\n    if config['type'] == 'pnf':\n        pnf_xs = list(pnf_results['pnf_df'].XBox.values)\n        pnf_os = list(pnf_results['pnf_df'].OBox.values)\n        tick_vals = sorted( set(pnf_xs + pnf_os) )\n        axA1.set_yticks(tick_vals)\n        skip = int( round(len(xdates)/10.0, 0) )\n        skip = max(1,skip) # must be at least 1\n        tick_vals = [t for t in range(0-skip,len(xdates)+1,skip)]\n        #print('len(xdates)=',len(xdates),'len(pnf_mdates)=',len(pnf_mdates))\n        #print('skip=',skip,'\\nxdates=',xdates,'\\npnf_dates=',[str(d.date()) for d in mdates.num2date(pnf_mdates)])\n        axA1.set_xticks(tick_vals)\n\n    ysd = config['yscale']\n    if isinstance(ysd,dict):\n        yscale = ysd['yscale']\n        del      ysd['yscale']\n        axA1.set_yscale(yscale,**ysd)\n    elif isinstance(ysd,str):\n        axA1.set_yscale(ysd)\n \n\n    addplot = config['addplot']\n    if addplot is not None and ptype not in VALID_PMOVE_TYPES:\n        # NOTE: If in external_axes_mode, then all code relating\n        #       to panels and secondary_y becomes irrrelevant.\n        #       If the user wants something on a secondary_y then user should\n        #       determine that externally, and pass in the appropriate axes.\n\n        if not external_axes_mode:\n            # Calculate the Order of Magnitude Range ('mag')\n            # If addplot['secondary_y'] == 'auto', then: If the addplot['data']\n            # is out of the Order of Magnitude Range, then use secondary_y.\n\n            lo = math.log(max(math.fabs(np.nanmin(lows)),1e-7),10) - 0.5\n            hi = math.log(max(math.fabs(np.nanmax(highs)),1e-7),10) + 0.5\n\n            panels['mag'] = [None]*len(panels)  # create 'mag'nitude column\n\n            panels.at[config['main_panel'],'mag'] = {'lo':lo,'hi':hi} # update main panel magnitude range\n\n            if config['volume']:\n                lo = math.log(max(math.fabs(np.nanmin(volumes)),1e-7),10) - 0.5\n                hi = math.log(max(math.fabs(np.nanmax(volumes)),1e-7),10) + 0.5\n                panels.at[config['volume_panel'],'mag'] = {'lo':lo,'hi':hi}\n\n        if isinstance(addplot,dict):\n            addplot = [addplot,]   # make list of dict to be consistent\n\n        elif not _list_of_dict(addplot):\n            raise TypeError('addplot must be `dict`, or `list of dict`, NOT '+str(type(addplot)))\n        \n        contains_legend_label=[] # a list of axes that contains legend labels\n\n        for apdict in addplot:\n\n            panid = apdict['panel'] \n            if not external_axes_mode:\n                if   panid == 'main' : panid = 0  # for backwards compatibility\n                elif panid == 'lower': panid = 1  # for backwards compatibility\n                if apdict['y_on_right'] is not None:\n                    panels.at[panid,'y_on_right'] = apdict['y_on_right']\n            aptype = apdict['type']\n\n            if aptype == 'ohlc' or aptype == 'candle':\n                ax = _addplot_collections(panid,panels,apdict,xdates,config)\n                _addplot_apply_supplements(ax,apdict,xdates)\n            else:         \n                apdata = apdict['data']\n                if isinstance(apdata,list) and not isinstance(apdata[0],(float,int)):\n                    raise TypeError('apdata is list but NOT of float or int')\n                if isinstance(apdata,pd.DataFrame): \n                    havedf = True\n                else:\n                    havedf = False      # must be a single series or array\n                    apdata = [apdata,]  # make it iterable\n                if havedf and apdict['label']:\n                    if not isinstance(apdict['label'],(list,tuple,np.ndarray)):\n                       nlabels = 1\n                    else:\n                       nlabels = len(apdict['label'])\n                    ncolumns = len(apdata.columns)\n                    #print('nlabels=',nlabels,'ncolumns=',ncolumns)\n                    if nlabels < ncolumns:\n                        warnings.warn('\\n =======================================\\n'+\n                                      ' addplot MISMATCH between data and labels:\\n'+\n                                      ' have '+str(ncolumns)+' columns to plot \\n'+\n                                      ' BUT  '+str(nlabels)+' labels for them.\\n')\n                colcount = 0\n                for column in apdata:\n                    ydata = apdata.loc[:,column] if havedf else column\n                    ax = _addplot_columns(panid,panels,ydata,apdict,xdates,config,colcount)\n                    _addplot_apply_supplements(ax,apdict,xdates)\n                    colcount += 1\n                    if apdict['label']: # not supported for aptype == 'ohlc' or 'candle'\n                        contains_legend_label.append(ax)\n        for ax in set(contains_legend_label): # there might be duplicates\n            ax.legend()\n\n    # fill_between is NOT supported for external_axes_mode\n    # (caller can easily call ax.fill_between() themselves).\n    if config['fill_between'] is not None and not external_axes_mode:\n        fblist = copy.deepcopy(config['fill_between'])\n        if _num_or_seq_of_num(fblist):\n            fblist = [dict(y1=fblist),]\n        elif isinstance(fblist,dict):\n            fblist = [fblist,]\n        if not _list_of_dict(fblist):\n            raise TypeError('Bad type for `fill_between` specifier.')\n        for fb in fblist:\n            if 'x' in fb:\n                raise ValueError('fill_between dict may not contain `x`')\n            panid = config['main_panel']\n            if 'panel' in fb:\n                panid = fb['panel']\n                del fb['panel']\n            fb['x'] = xdates # add 'x' to caller's fb dict\n            ax = panels.at[panid,'axes'][0]\n            ax.fill_between(**fb)\n            \n    # put the primary axis on one side,\n    # and the twinx() on the \"other\" side:\n    if not external_axes_mode:\n        for panid,row in panels.iterrows():\n            ax = row['axes']\n            y_on_right = style['y_on_right'] if row['y_on_right'] is None else row['y_on_right']\n            _set_ylabels_side(ax[0],ax[1],y_on_right)\n    else:\n        y_on_right = style['y_on_right']\n        _set_ylabels_side(axA1,None,y_on_right)\n\n    # TODO: ================================================================\n    # TODO:  Investigate:\n    # TODO:  ===========\n    # TODO:  It appears to me that there may be some or significant overlap\n    # TODO:  between what the following functions actually do:\n    # TODO:  At the very least, all four of them appear to communicate \n    # TODO:  to matplotlib that the xaxis should be treated as dates:\n    # TODO:   ->  'ax.autoscale_view()'\n    # TODO:   ->  'ax.xaxis_dates()'\n    # TODO:   ->  'plt.autofmt_xdates()'\n    # TODO:   ->  'fig.autofmt_xdate()'\n    # TODO: ================================================================\n    \n\n    #if config['autofmt_xdate']:\n        #print('CALLING fig.autofmt_xdate()')\n        #fig.autofmt_xdate()\n\n    axA1.autoscale_view()  # Is this really necessary??\n                           # It appears to me, based on experience coding types 'ohlc' and 'candle'\n                           # for `addplot`, that this IS necessary when the only thing done to the\n                           # the axes is .add_collection().  (However, if ax.plot() .scatter() or\n                           # .bar() was called, then possibly this is not necessary; not entirely\n                           # sure, but it definitely was necessary to get 'ohlc' and 'candle' \n                           # working in `addplot`).\n\n    axA1.set_ylabel(config['ylabel'])\n\n    if config['volume']:\n        if external_axes_mode:\n            volumeAxes.tick_params(axis='x',rotation=xrotation)\n            volumeAxes.xaxis.set_major_formatter(formatter)\n\n        vscale = 'linear'\n        ysd = config['volume_yscale']\n        if isinstance(ysd,dict):\n            yscale = ysd['yscale']\n            del      ysd['yscale']\n            volumeAxes.set_yscale(yscale,**ysd)\n            vscale = yscale\n        elif isinstance(ysd,str):\n            volumeAxes.set_yscale(ysd)\n            vscale = ysd\n        offset = ''\n        if vscale == 'linear':\n            vxp = config['volume_exponent']\n            if vxp == 'legacy':\n                volumeAxes.figure.canvas.draw()  # This is needed to calculate offset\n                offset = volumeAxes.yaxis.get_major_formatter().get_offset()\n                if len(offset) > 0:\n                    offset = (' x '+offset)\n            elif isinstance(vxp,int) and vxp > 0:\n                volumeAxes.ticklabel_format(useOffset=False,scilimits=(vxp,vxp),axis='y')\n                offset = '  $10^{'+str(vxp)+'}$'\n            elif isinstance(vxp,int) and vxp == 0:\n                volumeAxes.ticklabel_format(useOffset=False,style='plain',axis='y')\n                offset = ''\n            else:\n                offset = ''\n                scilims = plt.rcParams['axes.formatter.limits']\n                if scilims[0] < scilims[1]:\n                    for power in (5,4,3,2,1):\n                        xp = scilims[1]*power\n                        if vymax >= 10.**xp:\n                            volumeAxes.ticklabel_format(useOffset=False,scilimits=(xp,xp),axis='y')\n                            offset = '  $10^{'+str(xp)+'}$'\n                            break\n                elif scilims[0] == scilims[1] and scilims[1] != 0:\n                    volumeAxes.ticklabel_format(useOffset=False,scilimits=scilims,axis='y')\n                    offset = ' $10^'+str(scilims[1])+'$'\n            volumeAxes.yaxis.offsetText.set_visible(False)\n\n        if config['ylabel_lower'] is None:\n            vol_label = 'Volume'+offset\n        else:\n            if len(offset) > 0:\n                offset = '\\n'+offset\n            vol_label = config['ylabel_lower'] + offset\n        volumeAxes.set_ylabel(vol_label)\n    \n    if config['title'] is not None:\n        if config['tight_layout']:\n            # IMPORTANT: `y=0.89` is based on the top of the top panel\n            #            being at 0.18+0.7 = 0.88.  See _panels.py\n            # If the value changes there, then it needs to change here.\n            title_kwargs = dict(va='bottom', y=0.89)\n        else:\n            title_kwargs = dict(va='center')\n        if isinstance(config['title'],dict):\n            title_dict = config['title']\n            if 'title' not in title_dict:\n                raise ValueError('Must have \"title\" entry in title dict')\n            else:\n                title = title_dict['title']\n                del title_dict['title']\n            title_kwargs.update(title_dict)  # allows override default values set by mplfinance above\n        else:\n            title = config['title']      # config['title'] is a string\n        fig.suptitle(title,**title_kwargs)\n\n    \n    if config['axtitle'] is not None:\n        axA1.set_title(config['axtitle'])\n\n    if not external_axes_mode:\n        for panid,row in panels.iterrows():\n            if not row['used2nd']:\n                row['axes'][1].set_visible(False)\n\n    if external_axes_mode:\n        return None\n\n    # Should we create a new kwarg to return a flattened axes list\n    # versus a list of tuples of primary and secondary axes?\n    # For now, for backwards compatibility, we flatten axes list:\n    axlist = [ax for axes in panels['axes'] for ax in axes]\n\n    if config['axisoff']:\n        for ax in axlist:\n            ax.set_axis_off()\n\n    if config['savefig'] is not None:\n        save = config['savefig']\n        if isinstance(save,dict):\n            if config['tight_layout'] and 'bbox_inches' not in save:\n                fig.savefig(**save,bbox_inches='tight')\n            else:\n                fig.savefig(**save)\n        else:\n            if config['tight_layout']:\n                fig.savefig(save,bbox_inches='tight')\n            else:\n                fig.savefig(save)\n        if config['closefig']: # True or 'auto'\n            plt.close(fig)\n    elif not config['returnfig']:\n        plt.show(block=config['block']) # https://stackoverflow.com/a/13361748/1639359\n        if config['closefig'] == True or (config['block'] and config['closefig']):\n            plt.close(fig)\n    \n    if config['returnfig']:\n        if config['closefig'] == True: plt.close(fig)\n        return (fig, axlist)\n\n    # rcp   = copy.deepcopy(plt.rcParams)\n    # rcpdf = rcParams_to_df(rcp)\n    # print('type(rcpdf)=',type(rcpdf))\n    # print('rcpdfhead(3)=',rcpdf.head(3))\n    # return # rcpdf"
            },
            {
                "name": "ax.plot",
                "code": "def plot( data, **kwargs ):\n    \"\"\"\n    Given a Pandas DataFrame containing columns Open,High,Low,Close and optionally Volume\n    with a DatetimeIndex, plot the data.\n    Available plots include ohlc bars, candlestick, and line plots.\n    Also provide visually analysis in the form of common technical studies, such as:\n    moving averages, renko, etc.\n    Also provide ability to plot trading signals, and/or addtional user-defined data.\n    \"\"\"\n\n    config = _process_kwargs(kwargs, _valid_plot_kwargs())\n\n    # translate alias types:\n    config['type'] = _get_valid_plot_types(config['type'])\n    \n    dates,opens,highs,lows,closes,volumes = _check_and_prepare_data(data, config)\n\n    config['xlim'] = _check_and_convert_xlim_configuration(data, config)\n\n    if config['type'] in VALID_PMOVE_TYPES and config['addplot'] is not None:\n        err = \"`addplot` is not supported for `type='\" + config['type'] +\"'`\"\n        raise ValueError(err)\n\n    if config['marketcolor_overrides'] is not None:\n        if len(config['marketcolor_overrides']) != len(dates):\n            raise ValueError('`marketcolor_overrides` must be same length as dataframe.')\n\n    external_axes_mode = _check_for_external_axes(config)\n\n    if external_axes_mode:\n        if config['figscale'] is not None:\n            warnings.warn('\\n\\n ================================================================= '+\n                          '\\n\\n   WARNING: `figscale` has NO effect in External Axes Mode.'+\n                          '\\n\\n ================================================================ ',\n                          category=UserWarning)\n        if config['figratio'] is not None:\n            warnings.warn('\\n\\n ================================================================= '+\n                          '\\n\\n   WARNING: `figratio` has NO effect in External Axes Mode.'+\n                          '\\n\\n ================================================================ ',\n                          category=UserWarning)\n        if config['figsize'] is not None:\n            warnings.warn('\\n\\n ================================================================= '+\n                          '\\n\\n   WARNING: `figsize` has NO effect in External Axes Mode.'+\n                          '\\n\\n ================================================================ ',\n                          category=UserWarning)\n    else:\n        if config['figscale'] is None: config['figscale'] = 1.0\n        if config['figratio'] is None: config['figratio'] = DEFAULT_FIGRATIO\n\n    style = config['style']\n\n    if external_axes_mode and hasattr(config['ax'],'mpfstyle') and style is None:\n        style = config['ax'].mpfstyle\n    elif style is None:\n        style = 'default'\n\n    if isinstance(style,str):\n        style = _styles._get_mpfstyle(style)\n\n    config['style'] = style\n\n    if isinstance(style,dict):\n        if not external_axes_mode: _styles._apply_mpfstyle(style)\n    else:\n        raise TypeError('style should be a `dict`; why is it not?')\n\n    if config['mavcolors'] is not None:\n        config['_ma_color_cycle'] = cycle(config['mavcolors'])\n    elif style['mavcolors'] is not None:\n        config['_ma_color_cycle'] = cycle(style['mavcolors'])\n    else:\n        config['_ma_color_cycle'] = None\n\n    if not external_axes_mode:\n        fig = plt.figure()\n        _adjust_figsize(fig,config)\n    else:\n        fig = None\n\n    _adjust_fontsize(config)\n\n    if config['volume'] and volumes is None:\n        raise ValueError('Request for volume, but NO volume data.')\n\n    if external_axes_mode:\n        panels = None\n        axA1   = config['ax']\n        axA1.set_axisbelow(config['saxbelow'])\n        if config['volume']:\n            volumeAxes = config['volume']\n            volumeAxes.set_axisbelow(config['saxbelow'])\n    else:\n        panels = _build_panels(fig, config)\n        axA1 = panels.at[config['main_panel'],'axes'][0]\n        if config['volume']:\n            if config['volume_panel'] == config['main_panel']:\n                # ohlc and volume on same panel: move volume to secondary axes:\n                volumeAxes = panels.at[config['volume_panel'],'axes'][1]\n                volumeAxes.set_zorder(axA1.get_zorder()-0.1) # Make sure ohlc is above volume\n                axA1.patch.set_visible(False)                # Let volume show through\n                panels.at[config['main_panel'],'used2nd'] = True\n            else:\n                volumeAxes = panels.at[config['volume_panel'],'axes'][0]\n        else:\n            volumeAxes = None\n\n    fmtstring = _determine_format_string(dates, config['datetime_format'])\n\n    ptype = config['type'] \n\n    if config['show_nontrading']:\n        formatter = mdates.DateFormatter(fmtstring)\n        xdates = dates\n    else:\n        formatter = IntegerIndexDateTimeFormatter(dates, fmtstring)\n        xdates = np.arange(len(dates))\n\n    # Will have to handle widths config separately for PMOVE types ??\n    config['_width_config'] = _determine_width_config(xdates, config)\n\n    rwc = config['return_width_config']\n    if isinstance(rwc,dict) and len(rwc)==0:\n        config['return_width_config'].update(config['_width_config'])\n\n    collections = None\n    if ptype == 'line':\n        lw = config['_width_config']['line_width']\n        axA1.plot(xdates, closes, color=config['linecolor'], linewidth=lw)\n    elif ptype == 'pnf':\n        pnf_results = _construct_pnf_scatter(axA1,ptype,dates,xdates,opens,highs,lows,closes,volumes,config,style)\n    else:\n        collections =_construct_mpf_collections(ptype,dates,xdates,opens,highs,lows,closes,volumes,config,style)\n\n    if ptype == 'pnf':\n        volumes    = pnf_results['pnf_volumes']\n        pnf_values = pnf_results['pnf_values']\n        pnf_mdates = mdates.date2num(list(pnf_values.keys()))\n        formatter  = IntegerIndexDateTimeFormatter(pnf_mdates,fmtstring)\n        xdates     = pnf_results['pnf_xdates']\n    elif ptype == 'renko':\n        collections, renko_results = collections\n        volumes       = renko_results['volumes']\n        renko_dates   = renko_results['dates']\n        renko_values  = renko_results['values']\n        formatter = IntegerIndexDateTimeFormatter(renko_dates, fmtstring)\n        renko_avgvals = renko_values\n        renko_size    = renko_results['size']\n        xdates = np.arange(len(renko_dates))\n\n    if collections is not None:\n        for collection in collections:\n            axA1.add_collection(collection)\n\n    #formatter = IntegerIndexDateTimeFormatter(xdates, fmtstring)\n\n    if (ptype == 'pnf' and \n        (config['mav'] is not None or config['ema'] is not None)):\n        warnings.warn('\\n\\n ================================================================ '+\n                      '\\n\\n   MOVING Averages IGNORED for POINT and FIGURE PLOTS!'+\n                      '\\n\\n ================================================================ ',\n                      category=UserWarning)\n    elif ptype == 'renko':\n        mavprices = _plot_mav(axA1,config,xdates,renko_avgvals)\n        emaprices = _plot_ema(axA1,config,xdates,renko_avgvals)\n    else:\n        mavprices = _plot_mav(axA1,config,xdates,closes)\n        emaprices = _plot_ema(axA1,config,xdates,closes)\n\n    avg_dist_between_points = (xdates[-1] - xdates[0]) / float(len(xdates))\n    if not config['tight_layout']:\n        minx = xdates[0]  - avg_dist_between_points\n        maxx = xdates[-1] + avg_dist_between_points\n    else:\n        minx = xdates[0]  - (0.45 * avg_dist_between_points)\n        maxx = xdates[-1] + (0.45 * avg_dist_between_points)\n\n    if len(xdates) == 1:  # kludge special case\n        minx = minx - 0.75\n        maxx = maxx + 0.75\n\n    if ptype == 'renko':\n        _lows  = renko_avgvals \n        _highs = [value+renko_size for value in renko_avgvals]\n    else:\n        _lows  = lows\n        _highs = highs\n\n    if ptype == 'pnf':\n       miny, maxy = pnf_results['pnf_ylimits']\n    else:\n        miny = np.nanmin(_lows)\n        maxy = np.nanmax(_highs)\n\n    if config['ylim'] is not None:\n        axA1.set_ylim(config['ylim'][0], config['ylim'][1])\n    elif config['tight_layout']:\n        ydelta = 0.01 * (maxy-miny)\n        if miny > 0.0:\n            # don't let it go negative:\n            setminy = max(0.9*miny,miny-ydelta)\n        else:\n            setminy = miny-ydelta\n        axA1.set_ylim(setminy,maxy+ydelta)\n\n    if config['xlim'] is not None:\n        axA1.set_xlim(config['xlim'][0], config['xlim'][1])\n    elif config['tight_layout']:\n        axA1.set_xlim(minx,maxx)\n\n    if (config['ylim'] is None and\n        config['xlim'] is None and\n        not config['tight_layout']):\n        corners = (minx, miny), (maxx, maxy)\n        axA1.update_datalim(corners)\n\n    if config['return_calculated_values'] is not None:\n        retdict = config['return_calculated_values']\n        if ptype == 'renko':\n            retdict['renko_bricks' ] = renko_values\n            retdict['renko_dates'  ] = mdates.num2date(renko_dates)\n            retdict['renko_size'   ] = renko_size\n            retdict['renko_volumes'] = volumes if config['volume'] else None\n        elif ptype == 'pnf':\n            retdict['pnf_dates'    ] = mdates.num2date(pnf_mdates)\n            retdict['pnf_values'   ] = pnf_values\n            retdict['pnf_size'     ] = pnf_results['pnf_boxsize']\n            retdict['pnf_volumes'  ] = volumes[:len(pnf_values)] if config['volume'] else None\n        if config['mav'] is not None and ptype != 'pnf':\n            mav = config['mav']\n            if len(mav) != len(mavprices):\n                warnings.warn('len(mav)='+str(len(mav))+' BUT len(mavprices)='+str(len(mavprices)))\n            else:\n                for jj in range(0,len(mav)):     \n                    retdict['mav' + str(mav[jj])] = mavprices[jj]\n        if config['ema'] is not None and ptype != 'pnf':\n            ema = config['ema']\n            if len(ema) != len(emaprices):\n                warnings.warn('len(ema)='+str(len(ema))+' BUT len(emaprices)='+str(len(emaprices)))\n            else:\n                for jj in range(0, len(ema)):\n                    retdict['ema' + str(ema[jj])] = emaprices[jj]\n        retdict['minx'] = minx\n        retdict['maxx'] = maxx\n        retdict['miny'] = miny\n        retdict['maxy'] = maxy\n\n    # Note: these are NOT mutually exclusive, so the order of this\n    #       if/elif is important: VALID_PMOVE_TYPES must be first.\n    if ptype in VALID_PMOVE_TYPES:\n        pmove_dates = pnf_mdates if ptype == 'pnf' else renko_dates\n        dtix = pd.DatetimeIndex([dt for dt in mdates.num2date(pmove_dates)])\n    elif not config['show_nontrading']:\n        dtix = data.index\n    else:\n        dtix = None\n\n    line_collections = []\n    line_collections.append(_construct_aline_collections(config['alines'], dtix))\n    line_collections.append(_construct_hline_collections(config['hlines'], minx, maxx))\n    line_collections.append(_construct_vline_collections(config['vlines'], dtix, miny, maxy))\n    tlines = config['tlines']\n    if isinstance(tlines,(list,tuple)) and all([isinstance(item,dict) for item in tlines]):\n        pass\n    else:\n        tlines = [tlines,]\n    for tline_item in tlines:\n        line_collections.append(_construct_tline_collections(tline_item, dtix, dates, opens, highs, lows, closes))\n     \n    for collection in line_collections:\n        if collection is not None:\n            axA1.add_collection(collection)\n\n    datalen = len(xdates)\n    if config['volume']:\n        mc = style['marketcolors']\n        vup,vdown = mc['volume'].values()\n        #-- print('vup,vdown=',vup,vdown)\n        vcolors = _updown_colors(vup, vdown, opens, closes, use_prev_close=style['marketcolors']['vcdopcod'])\n        #-- print('len(vcolors),len(opens),len(closes)=',len(vcolors),len(opens),len(closes))\n        #-- print('vcolors=',vcolors)\n\n        w  = config['_width_config']['volume_width']\n        lw = config['_width_config']['volume_linewidth']\n\n        veup, vedown = mc['vcedge'].values()\n        if mc['volume'] == mc['vcedge']:\n            edgecolors = _adjust_color_brightness(vcolors,0.90)\n        elif veup != vedown:\n            edgecolors = _updown_colors(veup, vedown, opens, closes, use_prev_close=style['marketcolors']['vcdopcod'])\n        else: \n            edgecolors = veup \n\n        if config['volume_alpha']:\n           valp = config['volume_alpha']\n        elif 'volume_alpha' in mc:\n           valp = mc['volume_alpha']\n        else:\n           valp = 1.0\n        volumeAxes.bar(xdates,volumes,width=w,linewidth=lw,color=vcolors,ec=edgecolors,alpha=valp)\n        if config['volume_ylim'] is not None:\n            vymin = config['volume_ylim'][0]\n            vymax = config['volume_ylim'][1]\n        else:\n            vymin = 0.3 * np.nanmin(volumes)\n            vymax = 1.1 * np.nanmax(volumes)\n        volumeAxes.set_ylim(vymin,vymax)\n\n    xrotation = config['xrotation']\n    if not external_axes_mode:\n        _set_ticks_on_bottom_panel_only(panels,formatter,rotation=xrotation,\n                                        xlabel=config['xlabel'])\n    else:\n        axA1.tick_params(axis='x',rotation=xrotation)\n        axA1.xaxis.set_major_formatter(formatter)\n        axA1.set_xlabel(config['xlabel'])\n\n    if config['type'] == 'pnf':\n        pnf_xs = list(pnf_results['pnf_df'].XBox.values)\n        pnf_os = list(pnf_results['pnf_df'].OBox.values)\n        tick_vals = sorted( set(pnf_xs + pnf_os) )\n        axA1.set_yticks(tick_vals)\n        skip = int( round(len(xdates)/10.0, 0) )\n        skip = max(1,skip) # must be at least 1\n        tick_vals = [t for t in range(0-skip,len(xdates)+1,skip)]\n        #print('len(xdates)=',len(xdates),'len(pnf_mdates)=',len(pnf_mdates))\n        #print('skip=',skip,'\\nxdates=',xdates,'\\npnf_dates=',[str(d.date()) for d in mdates.num2date(pnf_mdates)])\n        axA1.set_xticks(tick_vals)\n\n    ysd = config['yscale']\n    if isinstance(ysd,dict):\n        yscale = ysd['yscale']\n        del      ysd['yscale']\n        axA1.set_yscale(yscale,**ysd)\n    elif isinstance(ysd,str):\n        axA1.set_yscale(ysd)\n \n\n    addplot = config['addplot']\n    if addplot is not None and ptype not in VALID_PMOVE_TYPES:\n        # NOTE: If in external_axes_mode, then all code relating\n        #       to panels and secondary_y becomes irrrelevant.\n        #       If the user wants something on a secondary_y then user should\n        #       determine that externally, and pass in the appropriate axes.\n\n        if not external_axes_mode:\n            # Calculate the Order of Magnitude Range ('mag')\n            # If addplot['secondary_y'] == 'auto', then: If the addplot['data']\n            # is out of the Order of Magnitude Range, then use secondary_y.\n\n            lo = math.log(max(math.fabs(np.nanmin(lows)),1e-7),10) - 0.5\n            hi = math.log(max(math.fabs(np.nanmax(highs)),1e-7),10) + 0.5\n\n            panels['mag'] = [None]*len(panels)  # create 'mag'nitude column\n\n            panels.at[config['main_panel'],'mag'] = {'lo':lo,'hi':hi} # update main panel magnitude range\n\n            if config['volume']:\n                lo = math.log(max(math.fabs(np.nanmin(volumes)),1e-7),10) - 0.5\n                hi = math.log(max(math.fabs(np.nanmax(volumes)),1e-7),10) + 0.5\n                panels.at[config['volume_panel'],'mag'] = {'lo':lo,'hi':hi}\n\n        if isinstance(addplot,dict):\n            addplot = [addplot,]   # make list of dict to be consistent\n\n        elif not _list_of_dict(addplot):\n            raise TypeError('addplot must be `dict`, or `list of dict`, NOT '+str(type(addplot)))\n        \n        contains_legend_label=[] # a list of axes that contains legend labels\n\n        for apdict in addplot:\n\n            panid = apdict['panel'] \n            if not external_axes_mode:\n                if   panid == 'main' : panid = 0  # for backwards compatibility\n                elif panid == 'lower': panid = 1  # for backwards compatibility\n                if apdict['y_on_right'] is not None:\n                    panels.at[panid,'y_on_right'] = apdict['y_on_right']\n            aptype = apdict['type']\n\n            if aptype == 'ohlc' or aptype == 'candle':\n                ax = _addplot_collections(panid,panels,apdict,xdates,config)\n                _addplot_apply_supplements(ax,apdict,xdates)\n            else:         \n                apdata = apdict['data']\n                if isinstance(apdata,list) and not isinstance(apdata[0],(float,int)):\n                    raise TypeError('apdata is list but NOT of float or int')\n                if isinstance(apdata,pd.DataFrame): \n                    havedf = True\n                else:\n                    havedf = False      # must be a single series or array\n                    apdata = [apdata,]  # make it iterable\n                if havedf and apdict['label']:\n                    if not isinstance(apdict['label'],(list,tuple,np.ndarray)):\n                       nlabels = 1\n                    else:\n                       nlabels = len(apdict['label'])\n                    ncolumns = len(apdata.columns)\n                    #print('nlabels=',nlabels,'ncolumns=',ncolumns)\n                    if nlabels < ncolumns:\n                        warnings.warn('\\n =======================================\\n'+\n                                      ' addplot MISMATCH between data and labels:\\n'+\n                                      ' have '+str(ncolumns)+' columns to plot \\n'+\n                                      ' BUT  '+str(nlabels)+' labels for them.\\n')\n                colcount = 0\n                for column in apdata:\n                    ydata = apdata.loc[:,column] if havedf else column\n                    ax = _addplot_columns(panid,panels,ydata,apdict,xdates,config,colcount)\n                    _addplot_apply_supplements(ax,apdict,xdates)\n                    colcount += 1\n                    if apdict['label']: # not supported for aptype == 'ohlc' or 'candle'\n                        contains_legend_label.append(ax)\n        for ax in set(contains_legend_label): # there might be duplicates\n            ax.legend()\n\n    # fill_between is NOT supported for external_axes_mode\n    # (caller can easily call ax.fill_between() themselves).\n    if config['fill_between'] is not None and not external_axes_mode:\n        fblist = copy.deepcopy(config['fill_between'])\n        if _num_or_seq_of_num(fblist):\n            fblist = [dict(y1=fblist),]\n        elif isinstance(fblist,dict):\n            fblist = [fblist,]\n        if not _list_of_dict(fblist):\n            raise TypeError('Bad type for `fill_between` specifier.')\n        for fb in fblist:\n            if 'x' in fb:\n                raise ValueError('fill_between dict may not contain `x`')\n            panid = config['main_panel']\n            if 'panel' in fb:\n                panid = fb['panel']\n                del fb['panel']\n            fb['x'] = xdates # add 'x' to caller's fb dict\n            ax = panels.at[panid,'axes'][0]\n            ax.fill_between(**fb)\n            \n    # put the primary axis on one side,\n    # and the twinx() on the \"other\" side:\n    if not external_axes_mode:\n        for panid,row in panels.iterrows():\n            ax = row['axes']\n            y_on_right = style['y_on_right'] if row['y_on_right'] is None else row['y_on_right']\n            _set_ylabels_side(ax[0],ax[1],y_on_right)\n    else:\n        y_on_right = style['y_on_right']\n        _set_ylabels_side(axA1,None,y_on_right)\n\n    # TODO: ================================================================\n    # TODO:  Investigate:\n    # TODO:  ===========\n    # TODO:  It appears to me that there may be some or significant overlap\n    # TODO:  between what the following functions actually do:\n    # TODO:  At the very least, all four of them appear to communicate \n    # TODO:  to matplotlib that the xaxis should be treated as dates:\n    # TODO:   ->  'ax.autoscale_view()'\n    # TODO:   ->  'ax.xaxis_dates()'\n    # TODO:   ->  'plt.autofmt_xdates()'\n    # TODO:   ->  'fig.autofmt_xdate()'\n    # TODO: ================================================================\n    \n\n    #if config['autofmt_xdate']:\n        #print('CALLING fig.autofmt_xdate()')\n        #fig.autofmt_xdate()\n\n    axA1.autoscale_view()  # Is this really necessary??\n                           # It appears to me, based on experience coding types 'ohlc' and 'candle'\n                           # for `addplot`, that this IS necessary when the only thing done to the\n                           # the axes is .add_collection().  (However, if ax.plot() .scatter() or\n                           # .bar() was called, then possibly this is not necessary; not entirely\n                           # sure, but it definitely was necessary to get 'ohlc' and 'candle' \n                           # working in `addplot`).\n\n    axA1.set_ylabel(config['ylabel'])\n\n    if config['volume']:\n        if external_axes_mode:\n            volumeAxes.tick_params(axis='x',rotation=xrotation)\n            volumeAxes.xaxis.set_major_formatter(formatter)\n\n        vscale = 'linear'\n        ysd = config['volume_yscale']\n        if isinstance(ysd,dict):\n            yscale = ysd['yscale']\n            del      ysd['yscale']\n            volumeAxes.set_yscale(yscale,**ysd)\n            vscale = yscale\n        elif isinstance(ysd,str):\n            volumeAxes.set_yscale(ysd)\n            vscale = ysd\n        offset = ''\n        if vscale == 'linear':\n            vxp = config['volume_exponent']\n            if vxp == 'legacy':\n                volumeAxes.figure.canvas.draw()  # This is needed to calculate offset\n                offset = volumeAxes.yaxis.get_major_formatter().get_offset()\n                if len(offset) > 0:\n                    offset = (' x '+offset)\n            elif isinstance(vxp,int) and vxp > 0:\n                volumeAxes.ticklabel_format(useOffset=False,scilimits=(vxp,vxp),axis='y')\n                offset = '  $10^{'+str(vxp)+'}$'\n            elif isinstance(vxp,int) and vxp == 0:\n                volumeAxes.ticklabel_format(useOffset=False,style='plain',axis='y')\n                offset = ''\n            else:\n                offset = ''\n                scilims = plt.rcParams['axes.formatter.limits']\n                if scilims[0] < scilims[1]:\n                    for power in (5,4,3,2,1):\n                        xp = scilims[1]*power\n                        if vymax >= 10.**xp:\n                            volumeAxes.ticklabel_format(useOffset=False,scilimits=(xp,xp),axis='y')\n                            offset = '  $10^{'+str(xp)+'}$'\n                            break\n                elif scilims[0] == scilims[1] and scilims[1] != 0:\n                    volumeAxes.ticklabel_format(useOffset=False,scilimits=scilims,axis='y')\n                    offset = ' $10^'+str(scilims[1])+'$'\n            volumeAxes.yaxis.offsetText.set_visible(False)\n\n        if config['ylabel_lower'] is None:\n            vol_label = 'Volume'+offset\n        else:\n            if len(offset) > 0:\n                offset = '\\n'+offset\n            vol_label = config['ylabel_lower'] + offset\n        volumeAxes.set_ylabel(vol_label)\n    \n    if config['title'] is not None:\n        if config['tight_layout']:\n            # IMPORTANT: `y=0.89` is based on the top of the top panel\n            #            being at 0.18+0.7 = 0.88.  See _panels.py\n            # If the value changes there, then it needs to change here.\n            title_kwargs = dict(va='bottom', y=0.89)\n        else:\n            title_kwargs = dict(va='center')\n        if isinstance(config['title'],dict):\n            title_dict = config['title']\n            if 'title' not in title_dict:\n                raise ValueError('Must have \"title\" entry in title dict')\n            else:\n                title = title_dict['title']\n                del title_dict['title']\n            title_kwargs.update(title_dict)  # allows override default values set by mplfinance above\n        else:\n            title = config['title']      # config['title'] is a string\n        fig.suptitle(title,**title_kwargs)\n\n    \n    if config['axtitle'] is not None:\n        axA1.set_title(config['axtitle'])\n\n    if not external_axes_mode:\n        for panid,row in panels.iterrows():\n            if not row['used2nd']:\n                row['axes'][1].set_visible(False)\n\n    if external_axes_mode:\n        return None\n\n    # Should we create a new kwarg to return a flattened axes list\n    # versus a list of tuples of primary and secondary axes?\n    # For now, for backwards compatibility, we flatten axes list:\n    axlist = [ax for axes in panels['axes'] for ax in axes]\n\n    if config['axisoff']:\n        for ax in axlist:\n            ax.set_axis_off()\n\n    if config['savefig'] is not None:\n        save = config['savefig']\n        if isinstance(save,dict):\n            if config['tight_layout'] and 'bbox_inches' not in save:\n                fig.savefig(**save,bbox_inches='tight')\n            else:\n                fig.savefig(**save)\n        else:\n            if config['tight_layout']:\n                fig.savefig(save,bbox_inches='tight')\n            else:\n                fig.savefig(save)\n        if config['closefig']: # True or 'auto'\n            plt.close(fig)\n    elif not config['returnfig']:\n        plt.show(block=config['block']) # https://stackoverflow.com/a/13361748/1639359\n        if config['closefig'] == True or (config['block'] and config['closefig']):\n            plt.close(fig)\n    \n    if config['returnfig']:\n        if config['closefig'] == True: plt.close(fig)\n        return (fig, axlist)\n\n    # rcp   = copy.deepcopy(plt.rcParams)\n    # rcpdf = rcParams_to_df(rcp)\n    # print('type(rcpdf)=',type(rcpdf))\n    # print('rcpdfhead(3)=',rcpdf.head(3))\n    # return # rcpdf"
            }
        ],
        "third_party": [
            "mean",
            "rolling",
            "pd.Series",
            "mean.shift",
            "mavp_list.append"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "MegEngine/MegEngine",
        "file_path": "imperative/python/megengine/utils/network_node.py",
        "function_declaration": "def flops_conv(opnode: ConvolutionForward, inputs, outputs)",
        "start_line": "493",
        "end_line": "504",
        "docstring": "The function flops_conv calculates the number of floating-point operations (FLOPs) required for a convolution operation in a neural network. It takes as input a convolution operation node, input tensors, and output tensors. The function extracts the kernel height and width from the input tensor's shape, determines the number of input channels, and calculates the product of the output tensor's dimensions. It checks if the operation includes a bias term and computes the FLOPs by multiplying the total number of output elements by the number of input channels and the kernel size, adding the bias if present. It returns the computed FLOPs.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5308e20f3c90",
        "ground_truth": "def flops_conv(opnode: ConvolutionForward, inputs, outputs):\n    param_W_shape = inputs[1].shape\n    kh = param_W_shape[-2]\n    kw = param_W_shape[-1]\n    if len(param_W_shape) == 5:\n        num_input = param_W_shape[2]\n    else:\n        num_input = param_W_shape[1]\n    NCHW = np.prod(outputs[0].shape)\n    bias = 1 if isinstance(opnode, ConvBiasForward) else 0\n    # N x Cout x H x W x  (Cin x Kw x Kh)\n    return NCHW * (float(num_input * kw * kh) + bias)",
        "import_statements": [
            "import json",
            "import sys",
            "from typing import Sequence"
        ],
        "reference_api": [
            "np.prod",
            "float",
            "len",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.prod"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "MegEngine/MegEngine",
        "file_path": "imperative/python/megengine/xla/ir_utils.py",
        "function_declaration": "def mge_dtype_to_ir_type(mge_dtype)",
        "start_line": "289",
        "end_line": "295",
        "docstring": "The function mge_dtype_to_ir_type converts a given data type mge_dtype to its corresponding intermediate representation (IR) type.\\nIt first ensures that mge_dtype is a NumPy data type by converting it using np.dtype.\\nIt then asserts that the converted mge_dtype is indeed a NumPy data type, raising an error if it is not.\\nFinally, it retrieves the appropriate IR type factory from the _dtype_to_ir_type dictionary using mge_dtype and returns the result of calling this factory function.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "70811d3cf578",
        "ground_truth": "def mge_dtype_to_ir_type(mge_dtype):\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(\n        mge_dtype, np.dtype\n    ), f\"arg should be numpy dtype, but is {mge_dtype}\"\n    ir_type_factory = _dtype_to_ir_type[mge_dtype]\n    return ir_type_factory()",
        "import_statements": [
            "import io",
            "import textwrap",
            "from abc import ABC, abstractmethod",
            "from collections import defaultdict",
            "from functools import partial",
            "from typing import Any, Callable, Dict, Sequence, Tuple"
        ],
        "reference_api": [
            "np.dtype",
            "ir_type_factory",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.dtype",
            "ir_type_factory"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "MegEngine/MegEngine",
        "file_path": "imperative/python/megengine/xla/ir_utils.py",
        "function_declaration": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]",
        "start_line": "319",
        "end_line": "333",
        "docstring": "The function _numpy_array_constant converts a NumPy array into an intermediate representation (IR) constant. It optionally canonicalizes the array's data type, converts the dtype to an IR type, and handles special cases for boolean and bfloat16 types. The array is made contiguous in memory, and a dense elements attribute is created from it. Finally, it returns the result of an HLO ConstantOp initialized with this attribute.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a108c9ba2c22",
        "ground_truth": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]:\n    if canonicalize_types:\n        x = np.asarray(x, dtype.canonicalize_dtype(x.dtype))\n    element_type = mge_dtype_to_ir_type(x.dtype)\n    shape = x.shape\n    if x.dtype == np.bool_:\n        nelems = x.size\n        x = np.packbits(x, bitorder=\"little\")\n        if nelems == 1:\n            x = np.array(0 if x.item() == 0 else 0xFF, np.uint8)\n    elif x.dtype == dtype.bfloat16:\n        x = x.view(np.uint16)\n    x = np.ascontiguousarray(x)\n    attr = ir.DenseElementsAttr.get(x, type=element_type, shape=shape)\n    return (hlo.ConstantOp(attr).result,)",
        "import_statements": [
            "import io",
            "import textwrap",
            "from abc import ABC, abstractmethod",
            "from collections import defaultdict",
            "from functools import partial",
            "from typing import Any, Callable, Dict, Sequence, Tuple"
        ],
        "reference_api": [
            "np.ascontiguousarray",
            "x.item",
            "np.asarray",
            "hlo.ConstantOp",
            "mge_dtype_to_ir_type",
            "x.view",
            "get",
            "np.packbits",
            "dtype.canonicalize_dtype",
            "np.array"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "mge_dtype_to_ir_type",
                "code": "def mge_dtype_to_ir_type(mge_dtype):\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(\n        mge_dtype, np.dtype\n    ), f\"arg should be numpy dtype, but is {mge_dtype}\"\n    ir_type_factory = _dtype_to_ir_type[mge_dtype]\n    return ir_type_factory()"
            }
        ],
        "third_party": [
            "np.asarray",
            "dtype.canonicalize_dtype",
            "np.packbits",
            "np.array",
            "x.item",
            "x.view",
            "np.ascontiguousarray",
            "get",
            "hlo.ConstantOp"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "MegEngine/MegEngine",
        "file_path": "imperative/python/megengine/xla/ir_utils.py",
        "function_declaration": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]",
        "start_line": "336",
        "end_line": "360",
        "docstring": "The function _ndarray_constant_handler processes a NumPy array val and a flag canonicalize_types.\\nIf the array has any zero strides and is non-empty, it identifies the zero-stride and non-zero-stride axes.\\nIt collapses the array along the zero-stride axes and optionally canonicalizes its data type.\\nThen, it creates a broadcast operation for the collapsed array, matching the original shape and non-zero-stride axes, and returns the result.\\nIf the array does not have zero strides, it processes the array using _numpy_array_constant with the given canonicalize_types flag.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3345fe4d008b",
        "ground_truth": "def _ndarray_constant_handler(\n    val: np.ndarray, canonicalize_types\n) -> Sequence[ir.Value]:\n    if np.any(np.equal(0, val.strides)) and val.size > 0:\n        (zero_stride_axes,) = np.where(np.equal(0, val.strides))\n        (other_axes,) = np.where(np.not_equal(0, val.strides))\n        collapsed_val = val[\n            tuple(\n                0 if ax in zero_stride_axes else slice(None) for ax in range(val.ndim)\n            )\n        ]\n        if canonicalize_types:\n            collapsed_val = np.asarray(\n                collapsed_val, dtype.canonicalize_dtype(collapsed_val.dtype)\n            )\n        out = hlo.BroadcastInDimOp(\n            ir.RankedTensorType.get(\n                val.shape, mge_dtype_to_ir_type(collapsed_val.dtype)\n            ),\n            _numpy_array_constant(collapsed_val, canonicalize_types=False)[0],\n            dense_int_elements(other_axes),\n        ).result\n        return (out,)\n    else:\n        return _numpy_array_constant(val, canonicalize_types)",
        "import_statements": [
            "import io",
            "import textwrap",
            "from abc import ABC, abstractmethod",
            "from collections import defaultdict",
            "from functools import partial",
            "from typing import Any, Callable, Dict, Sequence, Tuple"
        ],
        "reference_api": [
            "dense_int_elements",
            "np.where",
            "_numpy_array_constant",
            "tuple",
            "np.asarray",
            "mge_dtype_to_ir_type",
            "np.equal",
            "hlo.BroadcastInDimOp",
            "get",
            "slice",
            "np.any",
            "dtype.canonicalize_dtype",
            "np.not_equal",
            "range"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "mge_dtype_to_ir_type",
                "code": "def mge_dtype_to_ir_type(mge_dtype):\n    mge_dtype = np.dtype(mge_dtype)\n    assert isinstance(\n        mge_dtype, np.dtype\n    ), f\"arg should be numpy dtype, but is {mge_dtype}\"\n    ir_type_factory = _dtype_to_ir_type[mge_dtype]\n    return ir_type_factory()"
            },
            {
                "name": "_numpy_array_constant",
                "code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]:\n    if canonicalize_types:\n        x = np.asarray(x, dtype.canonicalize_dtype(x.dtype))\n    element_type = mge_dtype_to_ir_type(x.dtype)\n    shape = x.shape\n    if x.dtype == np.bool_:\n        nelems = x.size\n        x = np.packbits(x, bitorder=\"little\")\n        if nelems == 1:\n            x = np.array(0 if x.item() == 0 else 0xFF, np.uint8)\n    elif x.dtype == dtype.bfloat16:\n        x = x.view(np.uint16)\n    x = np.ascontiguousarray(x)\n    attr = ir.DenseElementsAttr.get(x, type=element_type, shape=shape)\n    return (hlo.ConstantOp(attr).result,)"
            },
            {
                "name": "dense_int_elements",
                "code": "def dense_int_elements(xs) -> ir.DenseIntElementsAttr:\n    return ir.DenseIntElementsAttr.get(np.asarray(xs, np.int64))"
            },
            {
                "name": "_numpy_array_constant",
                "code": "def _numpy_array_constant(x: np.ndarray, canonicalize_types) -> Sequence[ir.Value]:\n    if canonicalize_types:\n        x = np.asarray(x, dtype.canonicalize_dtype(x.dtype))\n    element_type = mge_dtype_to_ir_type(x.dtype)\n    shape = x.shape\n    if x.dtype == np.bool_:\n        nelems = x.size\n        x = np.packbits(x, bitorder=\"little\")\n        if nelems == 1:\n            x = np.array(0 if x.item() == 0 else 0xFF, np.uint8)\n    elif x.dtype == dtype.bfloat16:\n        x = x.view(np.uint16)\n    x = np.ascontiguousarray(x)\n    attr = ir.DenseElementsAttr.get(x, type=element_type, shape=shape)\n    return (hlo.ConstantOp(attr).result,)"
            }
        ],
        "third_party": [
            "np.any",
            "np.equal",
            "np.where",
            "np.equal",
            "np.where",
            "np.not_equal",
            "np.asarray",
            "dtype.canonicalize_dtype",
            "hlo.BroadcastInDimOp",
            "get"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "MegEngine/MegEngine",
        "file_path": "imperative/python/megengine/xla/ir_utils.py",
        "function_declaration": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr",
        "start_line": "472",
        "end_line": "478",
        "docstring": "The function dense_bool_elements converts a sequence of boolean values into a packed bit array and returns a DenseElementsAttr object. It first packs the boolean array into bits using little-endian order. If the sequence has only one element, it adjusts the packed array to be either all zeros or all ones. Finally, it creates and returns a DenseElementsAttr object with the packed array, specifying it as a signless 1-bit integer type and setting the shape to the length of the input sequence.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "91b298678351",
        "ground_truth": "def dense_bool_elements(xs: Sequence[bool]) -> ir.DenseElementsAttr:\n    a = np.packbits(np.array(xs, np.bool_), bitorder=\"little\")\n    if len(xs) == 1:\n        a = np.array(0 if a.item() == 0 else 0xFF, np.uint8)\n    return ir.DenseElementsAttr.get(\n        a, type=ir.IntegerType.get_signless(1), shape=[len(xs)]\n    )",
        "import_statements": [
            "import io",
            "import textwrap",
            "from abc import ABC, abstractmethod",
            "from collections import defaultdict",
            "from functools import partial",
            "from typing import Any, Callable, Dict, Sequence, Tuple"
        ],
        "reference_api": [
            "get_signless",
            "len",
            "get",
            "np.packbits",
            "a.item",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.packbits",
            "np.array",
            "np.array",
            "a.item",
            "get",
            "get_signless"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "MegEngine/MegEngine",
        "file_path": "imperative/python/megengine/xla/rules/trivial.py",
        "function_declaration": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]])",
        "start_line": "27",
        "end_line": "41",
        "docstring": "The function create_tensor_lower takes a context ctx and a variable number of HLOTensor or sequences of HLOTensor as arguments.\\nIt asserts that the length of args, ctx.vars_in, and ctx.vars_out are all one.\\nThe function retrieves the input and output variables from the context and checks if the input variable has bound data.\\nIf bound data exists, it sets the values of both input and output variables in the module context.\\nThe function asserts that the shapes of the input and output variables are the same.\\nIt then converts the bound data of the output or input variable to a NumPy array with the output variable's data type.\\nIf neither variable has bound data, it raises an assertion error indicating only tensor creation from constants is supported.\\nFinally, it returns an HLOTensor initialized with the created data.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "06ebabf1e0ff",
        "ground_truth": "def create_tensor_lower(ctx, *args: Union[HLOTensor, Sequence[HLOTensor]]):\n    assert len(args) == len(ctx.vars_in) == len(ctx.vars_out) == 1\n    var_in, var_out = ctx.vars_in[0], ctx.vars_out[0]\n    if var_in.bound_data is not None:\n        ctx.module_context.set_value(var_in, var_in.bound_data)\n        ctx.module_context.set_value(var_out, var_in.bound_data)\n    assert var_in.shape == var_out.shape\n    if var_out.bound_data is not None:\n        data = np.asarray(var_out.bound_data, var_out.dtype)\n    elif var_in.bound_data is not None:\n        data = np.asarray(var_in.bound_data, var_out.dtype)\n    else:\n        assert False, \"only support create tensor from const now\"\n     return HLOTensor(data)",
        "import_statements": [
            "from typing import Sequence, Union"
        ],
        "reference_api": [
            "HLOTensor",
            "len",
            "set_value",
            "np.asarray"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "set_value",
            "set_value",
            "np.asarray",
            "np.asarray",
            "HLOTensor"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "MegEngine/MegEngine",
        "file_path": "imperative/python/megengine/xla/device.py",
        "function_declaration": "def _device_put_scalar(x, device)",
        "start_line": "32",
        "end_line": "38",
        "docstring": "The function _device_put_scalar transfers a scalar value to a specified device. It first converts the scalar to a numpy array, optionally determining the appropriate data type if not provided, using an internal helper function. The converted numpy array is then transferred to the specified device using another internal function. The function ensures that scalar values are appropriately handled and moved to the desired computational device.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6cd750fa0c8d",
        "ground_truth": "def _device_put_scalar(x, device):\n    def cvt_scalar_to_nparray(x, dtype=None):\n        if dtype is None and type(x) in _python_scalar_dtypes:\n            dtype = _scalar_type_to_dtype(type(x), x)\n        return np.asarray(x, dtype)\n     return _device_put_nparray(cvt_scalar_to_nparray(x), device)",
        "import_statements": [
            "from typing import Sequence, Tuple, Union"
        ],
        "reference_api": [
            "_scalar_type_to_dtype",
            "type",
            "np.asarray"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "_scalar_type_to_dtype",
            "np.asarray"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "MegEngine/MegEngine",
        "file_path": "imperative/python/megengine/xla/rules/utils.py",
        "function_declaration": "def _shape_equal(lhs_shape, rhs_shape)",
        "start_line": "45",
        "end_line": "65",
        "docstring": "The function _shape_equal checks if two shapes, lhs_shape and rhs_shape, are equal.\\nIt converts the shapes to lists if they are NumPy arrays.\\nThe function asserts that both shapes are either tuples or lists.\\nIf both shapes are empty, it returns True.\\nIt verifies that the first elements of the shapes are integers if the shapes are not empty.\\nIf the shapes have different lengths, it returns False.\\nIt iterates through the elements of both shapes, and if any corresponding elements are not equal, it returns False.\\nIf all checks pass, it returns True.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "140de7c2b6a8",
        "ground_truth": "def _shape_equal(lhs_shape, rhs_shape):\n    lhs_shape = lhs_shape.tolist() if isinstance(lhs_shape, np.ndarray) else lhs_shape\n    rhs_shape = rhs_shape.tolist() if isinstance(rhs_shape, np.ndarray) else rhs_shape\n    assert isinstance(lhs_shape, (tuple, list)) and isinstance(\n        rhs_shape, (tuple, list)\n    ), f\"lhs_shape: {lhs_shape}{type(lhs_shape)}, rhs_shape: {rhs_shape}{type(rhs_shape)}\"\n    if len(lhs_shape) == 0 and len(rhs_shape) == 0:\n        return True\n    if len(lhs_shape) != 0:\n        assert isinstance(lhs_shape[0], (int, np.int32)), f\"{lhs_shape}, {rhs_shape}\"\n    if len(rhs_shape) != 0:\n        assert isinstance(rhs_shape[0], (int, np.int32)), f\"{lhs_shape}, {rhs_shape}\"\n     if len(lhs_shape) != len(rhs_shape):\n        return False\n     for l, r in zip(lhs_shape, rhs_shape):\n        if l != r:\n            return False\n     return True",
        "import_statements": [
            "import warnings"
        ],
        "reference_api": [
            "len",
            "rhs_shape.tolist",
            "type",
            "isinstance",
            "lhs_shape.tolist",
            "zip"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "lhs_shape.tolist",
            "rhs_shape.tolist"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "modin-project/modin",
        "file_path": "examples/docker/modin-ray/plasticc.py",
        "function_declaration": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n)",
        "start_line": "134",
        "end_line": "156",
        "docstring": "The function read loads training and test datasets along with their metadata from CSV files. It takes filenames for the datasets and their metadata, as well as dictionaries specifying data types for the columns. The function reads the training set, test set, training metadata, and test metadata into pandas DataFrames using the specified data types. It temporarily removes the 'target' column type from the metadata dictionary while loading the test metadata, then restores it. Finally, it returns a tuple of the four DataFrames.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "55d0c71a259b",
        "ground_truth": "def read(\n    training_set_filename,\n    test_set_filename,\n    training_set_metadata_filename,\n    test_set_metadata_filename,\n    dtypes,\n    meta_dtypes,\n):\n    train = pd.read_csv(training_set_filename, dtype=dtypes)\n    test = pd.read_csv(\n        test_set_filename,\n        names=list(dtypes.keys()),\n        dtype=dtypes,\n        header=0,\n    )\n     train_meta = pd.read_csv(training_set_metadata_filename, dtype=meta_dtypes)\n    target = meta_dtypes.pop(\"target\")\n    test_meta = pd.read_csv(test_set_metadata_filename, dtype=meta_dtypes)\n    meta_dtypes[\"target\"] = target\n     dfs = (train, train_meta, test, test_meta)\n    return dfs",
        "import_statements": [
            "import sys",
            "import time",
            "from functools import partial",
            "import sklearnex",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.preprocessing import LabelEncoder"
        ],
        "reference_api": [
            "dtypes.keys",
            "meta_dtypes.pop",
            "list",
            "pd.read_csv"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "pd.read_csv",
            "pd.read_csv",
            "dtypes.keys",
            "pd.read_csv",
            "meta_dtypes.pop",
            "pd.read_csv"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "mwaskom/seaborn",
        "file_path": "seaborn/widgets.py",
        "function_declaration": "def _show_cmap(cmap)",
        "start_line": "37",
        "end_line": "44",
        "docstring": "The function _show_cmap displays a continuous matplotlib colormap.\\nIt first imports axes_style from rcmod to avoid a circular import.\\nUsing the white style from axes_style, it creates a figure and axis with specific dimensions.\\nIt removes the x and y ticks from the axis.\\nThen, it generates a 2D array with values linearly spaced between 0 and 1 and uses ax.pcolormesh to display the colormap.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9e5e02cd73e0",
        "ground_truth": "def _show_cmap(cmap):\n    \"\"\"Show a continuous matplotlib colormap.\"\"\"\n    from .rcmod import axes_style  # Avoid circular import\n    with axes_style(\"white\"):\n        f, ax = plt.subplots(figsize=(8.25, .75))\n    ax.set(xticks=[], yticks=[])\n    x = np.linspace(0, 1, 256)[np.newaxis, :]\n    ax.pcolormesh(x, cmap=cmap)",
        "import_statements": [
            "from matplotlib.colors import LinearSegmentedColormap"
        ],
        "reference_api": [
            "plt.subplots",
            "ax.set",
            "ax.pcolormesh",
            "np.linspace",
            "axes_style"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "axes_style",
            "plt.subplots",
            "ax.set",
            "np.linspace",
            "ax.pcolormesh"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "nschloe/tikzplotlib",
        "file_path": "src/tikzplotlib/_util.py",
        "function_declaration": "def transform_to_data_coordinates(obj, xdata, ydata)",
        "start_line": "26",
        "end_line": "41",
        "docstring": "The function transform_to_data_coordinates converts coordinates to data coordinates for a given object. It takes an object and its x and y coordinates as inputs. If the object's axes are defined and its current transform differs from the data transform, the function combines the object's transform with the inverse data transform to convert the coordinates. It then applies the composite transform to the points and returns the transformed coordinates. If no transformation is needed, it returns the original coordinates.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ad0781b5cc38",
        "ground_truth": "def transform_to_data_coordinates(obj, xdata, ydata):\n    \"\"\"The coordinates might not be in data coordinates, but could be sometimes in axes\n    coordinates. For example, the matplotlib command\n      axes.axvline(2)\n    will have the y coordinates set to 0 and 1, not to the limits. Therefore, a\n    two-stage transform has to be applied:\n      1. first transforming to display coordinates, then\n      2. from display to data.\n    \"\"\"\n    if obj.axes is not None and obj.get_transform() != obj.axes.transData:\n        points = np.array([xdata, ydata]).T\n        transform = matplotlib.transforms.composite_transform_factory(\n            obj.get_transform(), obj.axes.transData.inverted()\n        )\n        return transform.transform(points).T\n    return xdata, ydata",
        "import_statements": [
            "import matplotlib.transforms"
        ],
        "reference_api": [
            "transform.transform",
            "inverted",
            "obj.get_transform",
            "composite_transform_factory",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "obj.get_transform",
            "np.array",
            "composite_transform_factory",
            "obj.get_transform",
            "inverted",
            "transform.transform"
        ]
    },
    {
        "subclass": "seaborn",
        "owner/repo": "pandas-ml/pandas-ml",
        "file_path": "pandas_ml/snsaccessors/base.py",
        "function_declaration": "def kdeplot(self, data=None, data2=None, *args, **kwargs)",
        "start_line": "85",
        "end_line": "96",
        "docstring": "The function kdeplot takes data, an optional second dataset data2, and additional arguments and keyword arguments.\\nIt processes the first dataset using the method _maybe_target_series with a key of 'data'.\\nIf a second dataset data2 is provided and it is not list-like, it retrieves data2 from the dataframe attribute _df.\\nFinally, it calls the kdeplot method from the _module attribute, passing the processed data, data2, and any additional arguments and keyword arguments.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bf776cadef8d",
        "ground_truth": "def kdeplot(self, data=None, data2=None, *args, **kwargs):\n    \"\"\"\n    Call ``seaborn.kdeplot`` using automatic mapping.\n    - ``data``: ``ModelFrame.target``\n    \"\"\"\n    data = self._maybe_target_series(data, key='data')\n    if data2 is not None:\n        if not pd.api.types.is_list_like(data2):\n            data2 = self._df[data2]\n    return self._module.kdeplot(data, data2=data2, *args, **kwargs)",
        "import_statements": [
            "from pandas_ml.core.accessor import _AccessorMethods, _attach_methods"
        ],
        "reference_api": [
            "kdeplot",
            "self._maybe_target_series",
            "is_list_like"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._maybe_target_series",
                "code": "def _maybe_target_series(self, value, key):\n        if value is None:\n            if not self._df.has_target():\n                msg = (\"{key} can't be ommitted when ModelFrame doesn't have \"\n                       \"target column\")\n                raise ValueError(msg.format(key=key))\n            elif self._df.has_multi_targets():\n                msg = (\"{key} can't be ommitted when ModelFrame has multiple \"\n                       \"target columns\")\n                raise ValueError(msg.format(key=key))\n            value = self._df.target\n\n        elif not pd.api.types.is_list_like(value):\n            value = self._df[value]\n        return value"
            },
            {
                "name": "kdeplot",
                "code": "def kdeplot(self, data=None, data2=None, *args, **kwargs):\n        \"\"\"\n        Call ``seaborn.kdeplot`` using automatic mapping.\n\n        - ``data``: ``ModelFrame.target``\n        \"\"\"\n        data = self._maybe_target_series(data, key='data')\n\n        if data2 is not None:\n            if not pd.api.types.is_list_like(data2):\n                data2 = self._df[data2]\n        return self._module.kdeplot(data, data2=data2, *args, **kwargs)"
            }
        ],
        "third_party": [
            "is_list_like"
        ]
    },
    {
        "subclass": "seaborn",
        "owner/repo": "PhantomInsights/mexican-government-report",
        "file_path": "scripts/step3.py",
        "function_declaration": "def plot_most_used_words(df)",
        "start_line": "83",
        "end_line": "106",
        "docstring": "The function plot_most_used_words visualizes the most frequent words in a dataframe. It first corrects the word \"programa\" to \"programar\" in the dataframe. It then filters the dataframe to include only alphabetic tokens that are longer than one character and are not stop words. The function counts the occurrences of these words, selects the top 20, and creates a bar plot showing their frequency. The plot is labeled and saved as an image file.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fb44964174d9",
        "ground_truth": "def plot_most_used_words(df):\n    \"\"\"Generates a bar plot with the counts of the most used lemmas.\n     Parameters\n    ----------\n    df : pandas.DataFrame\n        The DataFrame to be plotted.\n     \"\"\"\n     # Small fix for programa and programar.\n    df.loc[df[\"lemma_lower\"] == \"programa\", \"lemma_lower\"] = \"programar\"\n     # Only take into account alphabet tokens that are longer than 1 character and are not stop words.\n    words = df[\n        (df[\"is_alphabet\"] == True) &\n        (df[\"is_stopword\"] == False) &\n        (df[\"lemma_lower\"].str.len() > 1)\n    ][\"lemma_lower\"].value_counts()[:20]\n     sns.barplot(x=words.values, y=words.index, palette=\"Blues_d\", linewidth=0)\n    plt.xlabel(\"Occurrences Count\")\n    plt.title(\"Most Frequent Words\")\n    plt.savefig(\"words_counts.png\", facecolor=\"#5C0E10\")",
        "import_statements": [
            "import geopandas"
        ],
        "reference_api": [
            "plt.savefig",
            "len",
            "plt.xlabel",
            "value_counts",
            "plt.title",
            "sns.barplot"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "value_counts",
            "sns.barplot",
            "plt.xlabel",
            "plt.title",
            "plt.savefig"
        ]
    },
    {
        "subclass": "dask",
        "owner/repo": "pydata/xarray",
        "file_path": "xarray/core/dask_array_ops.py",
        "function_declaration": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1)",
        "start_line": "6",
        "end_line": "26",
        "docstring": "The function dask_rolling_wrapper applies bottleneck moving window functions to Dask arrays.\\nIt imports Dask array as da and promotes the data type of the input array a to handle edge cases, setting the fill value accordingly.\\nThe function calculates the depth of overlap for the specified axis and sets the boundary fill values.\\nIt creates an overlapped array ag using da.overlap.overlap.\\nIt then applies the moving_func to the overlapped array using da.map_blocks with the specified window size and min_count.\\nFinally, the function trims the overlapped regions using da.overlap.trim_internal and returns the resulting array.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "13414a7eb4e3",
        "ground_truth": "def dask_rolling_wrapper(moving_func, a, window, min_count=None, axis=-1):\n    \"\"\"Wrapper to apply bottleneck moving window funcs on dask arrays\"\"\"\n    import dask.array as da\n     dtype, fill_value = dtypes.maybe_promote(a.dtype)\n    a = a.astype(dtype)\n    # inputs for overlap\n    if axis < 0:\n        axis = a.ndim + axis\n    depth = {d: 0 for d in range(a.ndim)}\n    depth[axis] = (window + 1) // 2\n    boundary = {d: fill_value for d in range(a.ndim)}\n    # Create overlap array.\n    ag = da.overlap.overlap(a, depth=depth, boundary=boundary)\n    # apply rolling func\n    out = da.map_blocks(\n        moving_func, ag, window, min_count=min_count, axis=axis, dtype=a.dtype\n    )\n    # trim array\n    result = da.overlap.trim_internal(out, depth)\n    return result",
        "import_statements": [
            "from xarray.core import dtypes, nputils"
        ],
        "reference_api": [
            "a.astype",
            "overlap",
            "trim_internal",
            "da.map_blocks",
            "dtypes.maybe_promote",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "dtypes.maybe_promote",
            "a.astype",
            "overlap",
            "da.map_blocks",
            "trim_internal"
        ]
    },
    {
        "subclass": "dask",
        "owner/repo": "pydata/xarray",
        "file_path": "xarray/core/dask_array_ops.py",
        "function_declaration": "def least_squares(lhs, rhs, rcond=None, skipna=False)",
        "start_line": "29",
        "end_line": "55",
        "docstring": "The function least_squares solves a linear least squares problem using Dask arrays. It converts the input arrays to Dask arrays and optionally handles missing values by applying a polynomial fit function along the specified axis. If skipna is True, it reshapes the right-hand side array if necessary and applies the polynomial fit, extracting coefficients and residuals from the result. If skipna is False, it directly computes the least squares solution using Dask's linear algebra functions. The function returns the coefficients and residuals of the least squares solution.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "68ce25d325d4",
        "ground_truth": "def least_squares(lhs, rhs, rcond=None, skipna=False):\n    import dask.array as da\n     lhs_da = da.from_array(lhs, chunks=(rhs.chunks[0], lhs.shape[1]))\n    if skipna:\n        added_dim = rhs.ndim == 1\n        if added_dim:\n            rhs = rhs.reshape(rhs.shape[0], 1)\n        results = da.apply_along_axis(\n            nputils._nanpolyfit_1d,\n            0,\n            rhs,\n            lhs_da,\n            dtype=float,\n            shape=(lhs.shape[1] + 1,),\n            rcond=rcond,\n        )\n        coeffs = results[:-1, ...]\n        residuals = results[-1, ...]\n        if added_dim:\n            coeffs = coeffs.reshape(coeffs.shape[0])\n            residuals = residuals.reshape(residuals.shape[0])\n    else:\n        # Residuals here are (1, 1) but should be (K,) as rhs is (N, K)\n        # See issue dask/dask#6516\n        coeffs, residuals, _, _ = da.linalg.lstsq(lhs_da, rhs)\n    return coeffs, residuals",
        "import_statements": [
            "from xarray.core import dtypes, nputils"
        ],
        "reference_api": [
            "coeffs.reshape",
            "da.apply_along_axis",
            "residuals.reshape",
            "lstsq",
            "rhs.reshape",
            "da.from_array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "da.from_array",
            "rhs.reshape",
            "da.apply_along_axis",
            "coeffs.reshape",
            "residuals.reshape",
            "lstsq"
        ]
    },
    {
        "subclass": "dask",
        "owner/repo": "pydata/xarray",
        "file_path": "xarray/namedarray/daskmanager.py",
        "function_declaration": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any",
        "start_line": "66",
        "end_line": "79",
        "docstring": "The function from_array is a method that takes data, chunks, and additional keyword arguments.\\nIt imports the dask.array module and checks if the data is an instance of ImplicitToExplicitIndexingAdapter.\\nIf so, it sets the meta keyword argument to np.ndarray, indicating that lazily loaded backend array classes should use NumPy array operations.\\nThe function then calls da.from_array with the provided data, chunks, and additional keyword arguments, returning a DaskArray or another appropriate type.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c0c2c53b76e0",
        "ground_truth": "def from_array(\n    self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n) -> DaskArray | Any:\n    import dask.array as da\n    if isinstance(data, ImplicitToExplicitIndexingAdapter):\n        # lazily loaded backend array classes should use NumPy array operations.\n        kwargs[\"meta\"] = np.ndarray\n    return da.from_array(\n        data,\n        chunks,\n        **kwargs,\n    )  # type: ignore[no-untyped-call]",
        "import_statements": [
            "from collections.abc import Iterable, Sequence",
            "from typing import TYPE_CHECKING, Any, Callable",
            "from packaging.version import Version",
            "from xarray.core.indexing import ImplicitToExplicitIndexingAdapter",
            "from xarray.namedarray.parallelcompat import ChunkManagerEntrypoint, T_ChunkedArray",
            "from xarray.namedarray.utils import is_duck_dask_array, module_available"
        ],
        "reference_api": [
            "da.from_array",
            "isinstance"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "da.from_array",
                "code": "def from_array(\n        self, data: Any, chunks: T_Chunks | _NormalizedChunks, **kwargs: Any\n    ) -> DaskArray | Any:\n        import dask.array as da\n\n        if isinstance(data, ImplicitToExplicitIndexingAdapter):\n            # lazily loaded backend array classes should use NumPy array operations.\n            kwargs[\"meta\"] = np.ndarray\n\n        return da.from_array(\n            data,\n            chunks,\n            **kwargs,\n        )  # type: ignore[no-untyped-call]"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "dask",
        "owner/repo": "pydata/xarray",
        "file_path": "xarray/namedarray/core.py",
        "function_declaration": "def __dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]",
        "start_line": "643",
        "end_line": "659",
        "docstring": "The function __dask_postpersist__ handles the post-persistence process for a dask array. It checks if the data is a dask array, then calls the data's __dask_postpersist__ method to get a function and its arguments. It returns a tuple containing a finalize function and the arguments, which include the function and its arguments from the dask array's post-persistence method. If the data is not a dask array, it raises an AttributeError.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6559019d09a0",
        "ground_truth": "def __dask_postpersist__(\n    self,\n) -> tuple[\n    Callable[\n        [Graph, PostPersistCallable[Any], Any, Any],\n        Self,\n    ],\n    tuple[Any, ...],\n]:\n    if is_duck_dask_array(self._data):\n        a: tuple[PostPersistCallable[Any], tuple[Any, ...]]\n        a = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n        array_func, array_args = a\n        return self._dask_finalize, (array_func,) + array_args\n    else:\n        raise AttributeError(\"Method requires self.data to be a dask array.\")",
        "import_statements": [
            "import copy",
            "import math",
            "import sys",
            "import warnings",
            "from collections.abc import Hashable, Iterable, Mapping, Sequence",
            "from typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Generic,\n    Literal,\n    TypeVar,\n    cast,\n    overload,\n)",
            "from xarray.core import dtypes, formatting, formatting_html",
            "from xarray.core.indexing import (\n    ExplicitlyIndexed,\n    ImplicitToExplicitIndexingAdapter,\n    OuterIndexer,\n)",
            "from xarray.namedarray._aggregations import NamedArrayAggregations",
            "from xarray.namedarray._typing import (\n    ErrorOptionsWithWarn,\n    _arrayapi,\n    _arrayfunction_or_api,\n    _chunkedarray,\n    _default,\n    _dtype,\n    _DType_co,\n    _ScalarType_co,\n    _ShapeType_co,\n    _sparsearrayfunction_or_api,\n    _SupportsImag,\n    _SupportsReal,\n)",
            "from xarray.namedarray.parallelcompat import guess_chunkmanager",
            "from xarray.namedarray.pycompat import to_numpy",
            "from xarray.namedarray.utils import (\n    either_dict_or_kwargs,\n    infix_dims,\n    is_dict_like,\n    is_duck_dask_array,\n    to_0d_object_array,\n)"
        ],
        "reference_api": [
            "is_duck_dask_array",
            "AttributeError",
            "__dask_postpersist__"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "is_duck_dask_array",
                "code": "def is_duck_dask_array(x: duckarray[Any, Any]) -> TypeGuard[DaskArray]:\n    return is_duck_array(x) and is_dask_collection(x)"
            },
            {
                "name": "__dask_postpersist__",
                "code": "__dask_postpersist__(\n        self,\n    ) -> tuple[\n        Callable[\n            [Graph, PostPersistCallable[Any], Any, Any],\n            Self,\n        ],\n        tuple[Any, ...],\n    ]:\n        if is_duck_dask_array(self._data):\n            a: tuple[PostPersistCallable[Any], tuple[Any, ...]]\n            a = self._data.__dask_postpersist__()  # type: ignore[no-untyped-call]\n            array_func, array_args = a\n\n            return self._dask_finalize, (array_func,) + array_args\n        else:\n            raise AttributeError(\"Method requires self.data to be a dask array.\")\n\n  "
            }
        ],
        "third_party": []
    },
    {
        "subclass": "dask",
        "owner/repo": "pydata/xarray",
        "file_path": "xarray/core/duck_array_ops.py",
        "function_declaration": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n)",
        "start_line": "91",
        "end_line": "110",
        "docstring": "The function _dask_or_eager_func creates a new function that dispatches to either a dask module or an eager execution module based on the input types.\\nIt takes the name of the function to be dispatched, an eager execution module (defaulting to NumPy), and a dask module (defaulting to \"dask.array\").\\nThe inner function f checks if any of the input arguments are dask arrays using is_duck_dask_array.\\nIf dask arrays are present, it dynamically imports the specified dask module (if provided as a string) and retrieves the corresponding function by name.\\nOtherwise, it retrieves the function from the eager execution module.\\nThe inner function f then calls the retrieved function with the provided arguments and keyword arguments.\\nFinally, the outer function returns the inner function f.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4a2801bb3851",
        "ground_truth": "def _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n):\n    \"\"\"Create a function that dispatches to dask for dask array inputs.\"\"\"\n     def f(*args, **kwargs):\n        if any(is_duck_dask_array(a) for a in args):\n            mod = (\n                import_module(dask_module)\n                if isinstance(dask_module, str)\n                else dask_module\n            )\n            wrapped = getattr(mod, name)\n        else:\n            wrapped = getattr(eager_module, name)\n        return wrapped(*args, **kwargs)\n     return f",
        "import_statements": [
            "import contextlib",
            "import datetime",
            "import inspect",
            "import warnings",
            "from functools import partial",
            "from importlib import import_module",
            "from numpy import all as array_all",
            "from numpy import any as array_any",
            "from numpy import (  # noqa\n    around,  # noqa\n    full_like,\n    gradient,\n    isclose,\n    isin,\n    isnat,\n    take,\n    tensordot,\n    transpose,\n    unravel_index,\n)",
            "from numpy import concatenate as _concatenate",
            "from numpy.lib.stride_tricks import sliding_window_view",
            "from packaging.version import Version",
            "from pandas.api.types import is_extension_array_dtype",
            "from xarray.core import dask_array_ops, dtypes, nputils",
            "from xarray.core.options import OPTIONS",
            "from xarray.core.utils import is_duck_array, is_duck_dask_array, module_available",
            "from xarray.namedarray import pycompat",
            "from xarray.namedarray.parallelcompat import get_chunked_array_type",
            "from xarray.namedarray.pycompat import array_type, is_chunked_array"
        ],
        "reference_api": [
            "getattr",
            "any",
            "isinstance",
            "wrapped",
            "is_duck_dask_array",
            "import_module"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "is_duck_dask_array",
            "import_module",
            "wrapped"
        ]
    },
    {
        "subclass": "dask",
        "owner/repo": "pydata/xarray",
        "file_path": "xarray/core/duck_array_ops.py",
        "function_declaration": "def lazy_array_equiv(arr1, arr2)",
        "start_line": "292",
        "end_line": "313",
        "docstring": "The function lazy_array_equiv takes two arrays, arr1 and arr2, as arguments.\\nIt first checks if the two arrays are the same object and returns True if they are.\\nIt then converts the arrays to NumPy arrays using asarray.\\nIf the shapes of the arrays are not the same, it returns False.\\nIf Dask is available and both arrays are Dask arrays, it uses the Dask tokenize function to compare the arrays.\\nIf the tokenized values are equal, it returns True; otherwise, it returns None.\\nIn cases where the arrays are not the same object, not the same shape, or not Dask arrays, it returns None.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "eef8d746d01e",
        "ground_truth": "def lazy_array_equiv(arr1, arr2):\n    \"\"\"Like array_equal, but doesn't actually compare values.\n    Returns True when arr1, arr2 identical or their dask tokens are equal.\n    Returns False when shapes are not equal.\n    Returns None when equality cannot determined: one or both of arr1, arr2 are numpy arrays;\n    or their dask tokens are not equal\n    \"\"\"\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if dask_available and is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        from dask.base import tokenize\n         # GH3068, GH4221\n        if tokenize(arr1) == tokenize(arr2):\n            return True\n        else:\n            return None\n    return None",
        "import_statements": [
            "import contextlib",
            "import datetime",
            "import inspect",
            "import warnings",
            "from functools import partial",
            "from importlib import import_module",
            "from numpy import all as array_all",
            "from numpy import any as array_any",
            "from numpy import (  # noqa\n    around,  # noqa\n    full_like,\n    gradient,\n    isclose,\n    isin,\n    isnat,\n    take,\n    tensordot,\n    transpose,\n    unravel_index,\n)",
            "from numpy import concatenate as _concatenate",
            "from numpy.lib.stride_tricks import sliding_window_view",
            "from packaging.version import Version",
            "from pandas.api.types import is_extension_array_dtype",
            "from xarray.core import dask_array_ops, dtypes, nputils",
            "from xarray.core.options import OPTIONS",
            "from xarray.core.utils import is_duck_array, is_duck_dask_array, module_available",
            "from xarray.namedarray import pycompat",
            "from xarray.namedarray.parallelcompat import get_chunked_array_type",
            "from xarray.namedarray.pycompat import array_type, is_chunked_array"
        ],
        "reference_api": [
            "is_duck_dask_array",
            "tokenize",
            "asarray"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "asarray",
                "code": "def asarray(data, xp=np, dtype=None):\n    converted = data if is_duck_array(data) else xp.asarray(data)\n\n    if dtype is None or converted.dtype == dtype:\n        return converted\n\n    if xp is np or not hasattr(xp, \"astype\"):\n        return converted.astype(dtype)\n    else:\n        return xp.astype(converted, dtype)"
            },
            {
                "name": "asarray",
                "code": "def asarray(data, xp=np, dtype=None):\n    converted = data if is_duck_array(data) else xp.asarray(data)\n\n    if dtype is None or converted.dtype == dtype:\n        return converted\n\n    if xp is np or not hasattr(xp, \"astype\"):\n        return converted.astype(dtype)\n    else:\n        return xp.astype(converted, dtype)"
            }
        ],
        "third_party": [
            "is_duck_dask_array",
            "is_duck_dask_array"
        ]
    },
    {
        "subclass": "dask",
        "owner/repo": "pydata/xarray",
        "file_path": "asv_bench/benchmarks/__init__.py",
        "function_declaration": "def randn(shape, frac_nan=None, chunks=None, seed=0)",
        "start_line": "32",
        "end_line": "46",
        "docstring": "The function randn generates an array of normally distributed random numbers with a specified shape, and optionally includes NaN values. It takes the shape of the array, a fraction of NaNs, chunk size, and a seed for random number generation as inputs. If chunking is specified, it uses Dask to create the array; otherwise, it uses NumPy. If a fraction of NaNs is provided, it randomly assigns NaNs to the specified proportion of the array's elements. The function returns the generated array.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "15e830ff5fcb",
        "ground_truth": "def randn(shape, frac_nan=None, chunks=None, seed=0):\n    rng = np.random.RandomState(seed)\n    if chunks is None:\n        x = rng.standard_normal(shape)\n    else:\n        import dask.array as da\n         rng = da.random.RandomState(seed)\n        x = rng.standard_normal(shape, chunks=chunks)\n     if frac_nan is not None:\n        inds = rng.choice(range(x.size), int(x.size * frac_nan))\n        x.flat[inds] = np.nan\n     return x",
        "import_statements": [
            "import itertools",
            "import os"
        ],
        "reference_api": [
            "rng.standard_normal",
            "rng.choice",
            "int",
            "RandomState",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "RandomState",
            "rng.standard_normal",
            "RandomState",
            "rng.standard_normal",
            "rng.choice"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "ranaroussi/yfinance",
        "file_path": "yfinance/scrapers/analysis.py",
        "function_declaration": "def analyst_price_target(self) -> pd.DataFrame",
        "start_line": "34",
        "end_line": "37",
        "docstring": "The function analyst_price_target returns a DataFrame containing analyst price target data.\\nIf the attribute _analyst_price_target is None, it raises a YFNotImplementedError with the message 'analyst_price_target'.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5b57abbbe38b",
        "ground_truth": "def analyst_price_target(self) -> pd.DataFrame:\n    if self._analyst_price_target is None:\n        raise YFNotImplementedError('analyst_price_target')\n    return self._analyst_price_target",
        "import_statements": [
            "from yfinance.data import YfData",
            "from yfinance.exceptions import YFNotImplementedError"
        ],
        "reference_api": [
            "YFNotImplementedError"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "YFNotImplementedError"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "ranaroussi/yfinance",
        "file_path": "yfinance/scrapers/fundamentals.py",
        "function_declaration": "def _fetch_time_series(self, name, timescale, proxy=None)",
        "start_line": "71",
        "end_line": "91",
        "docstring": "The function _fetch_time_series retrieves financial time series data based on the provided name and timescale. It validates the name against allowed financial statements and the timescale against allowed periods. If either is invalid, it raises a ValueError. The function attempts to create a financials table using a helper method, returning the table if successful. If an error occurs, it logs the error and returns an empty DataFrame.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "592e414671e9",
        "ground_truth": "def _fetch_time_series(self, name, timescale, proxy=None):\n    # Fetching time series preferred over scraping 'QuoteSummaryStore',\n    # because it matches what Yahoo shows. But for some tickers returns nothing,\n    # despite 'QuoteSummaryStore' containing valid data.\n    allowed_names = [\"income\", \"balance-sheet\", \"cash-flow\"]\n    allowed_timescales = [\"yearly\", \"quarterly\"]\n    if name not in allowed_names:\n        raise ValueError(f\"Illegal argument: name must be one of: {allowed_names}\")\n    if timescale not in allowed_timescales:\n        raise ValueError(f\"Illegal argument: timescale must be one of: {allowed_timescales}\")\n    try:\n        statement = self._create_financials_table(name, timescale, proxy)\n        if statement is not None:\n            return statement\n    except YFException as e:\n        utils.get_yf_logger().error(f\"{self._symbol}: Failed to create {name} financials table for reason: {e}\")\n    return pd.DataFrame()",
        "import_statements": [
            "import datetime",
            "import json",
            "from yfinance import utils, const",
            "from yfinance.data import YfData",
            "from yfinance.exceptions import YFException, YFNotImplementedError"
        ],
        "reference_api": [
            "error",
            "self._create_financials_table",
            "ValueError",
            "pd.DataFrame",
            "utils.get_yf_logger"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._create_financials_table",
                "code": "def _create_financials_table(self, name, timescale, proxy):\n        if name == \"income\":\n            # Yahoo stores the 'income' table internally under 'financials' key\n            name = \"financials\"\n\n        keys = const.fundamentals_keys[name]\n\n        try:\n            return self.get_financials_time_series(timescale, keys, proxy)\n        except Exception:\n            pass"
            }
        ],
        "third_party": [
            "error",
            "utils.get_yf_logger",
            "pd.DataFrame"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "ranaroussi/yfinance",
        "file_path": "yfinance/scrapers/holders.py",
        "function_declaration": "def _parse_fund_ownership(self, data)",
        "start_line": "127",
        "end_line": "137",
        "docstring": "The function _parse_fund_ownership processes fund ownership data from a given input.\\nIt extracts the list of ownership details and iterates through each owner's data, parsing raw values and removing the 'maxAge' field.\\nThe processed data is converted into a pandas DataFrame.\\nIf the DataFrame is not empty, it converts the 'reportDate' to a datetime format and renames specific columns for clarity.\\nThe resulting DataFrame is assigned to the instance variable _mutualfund.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "af01d9bae091",
        "ground_truth": "def _parse_fund_ownership(self, data):\n    holders = data[\"ownershipList\"]\n    for owner in holders:\n        for k, v in owner.items():\n            owner[k] = self._parse_raw_values(v)\n        del owner[\"maxAge\"]\n    df = pd.DataFrame(holders)\n    if not df.empty:\n        df[\"reportDate\"] = pd.to_datetime(df[\"reportDate\"], unit=\"s\")\n        df.rename(columns={\"reportDate\": \"Date Reported\", \"organization\": \"Holder\", \"position\": \"Shares\", \"value\": \"Value\"}, inplace=True)\n    self._mutualfund = df",
        "import_statements": [
            "import requests",
            "from yfinance import utils",
            "from yfinance.data import YfData",
            "from yfinance.const import _BASE_URL_",
            "from yfinance.exceptions import YFDataException"
        ],
        "reference_api": [
            "df.rename",
            "owner.items",
            "pd.to_datetime",
            "self._parse_raw_values",
            "pd.DataFrame"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "owner.items",
            "self._parse_raw_values",
            "pd.DataFrame",
            "pd.to_datetime",
            "df.rename"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "ranaroussi/yfinance",
        "file_path": "yfinance/scrapers/quote.py",
        "function_declaration": "def currency(self)",
        "start_line": "250",
        "end_line": "256",
        "docstring": "The function currency retrieves and returns the currency information for a given ticker. It first checks if the currency attribute is already set; if so, it returns this value. If not, it fetches the metadata for the ticker's history using a method that can utilize a proxy if specified, extracts the currency information from the metadata, assigns it to the currency attribute, and then returns the currency.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2b5c6f1253db",
        "ground_truth": "def currency(self):\n    if self._currency is not None:\n        return self._currency\n    md = self._tkr.get_history_metadata(proxy=self.proxy)\n    self._currency = md[\"currency\"]\n    return self._currency",
        "import_statements": [
            "import datetime",
            "import json",
            "import warnings",
            "from collections.abc import MutableMapping",
            "import requests",
            "from yfinance import utils",
            "from yfinance.data import YfData",
            "from yfinance.const import quote_summary_valid_modules, _BASE_URL_",
            "from yfinance.exceptions import YFNotImplementedError, YFDataException, YFException"
        ],
        "reference_api": [
            "get_history_metadata"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get_history_metadata"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "ranaroussi/yfinance",
        "file_path": "yfinance/scrapers/quote.py",
        "function_declaration": "def recommendations(self) -> pd.DataFrame",
        "start_line": "572",
        "end_line": "583",
        "docstring": "The method recommendations returns a DataFrame of stock recommendations.\\nIf the attribute _recommendations is None, it fetches data from a specified proxy with the module 'recommendationTrend'.\\nIf the fetch result is None, it sets _recommendations to an empty DataFrame.\\nOtherwise, it attempts to extract recommendation trend data from the fetched result and stores it in _recommendations as a DataFrame.\\nIf there is an error in parsing the data, it raises a YFDataException with an appropriate error message.\\nThe method then returns the _recommendations DataFrame.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "16b80be60e3b",
        "ground_truth": "def recommendations(self) -> pd.DataFrame:\n    if self._recommendations is None:\n        result = self._fetch(self.proxy, modules=['recommendationTrend'])\n        if result is None:\n            self._recommendations = pd.DataFrame()\n        else:\n            try:\n                data = result[\"quoteSummary\"][\"result\"][0][\"recommendationTrend\"][\"trend\"]\n            except (KeyError, IndexError):\n                raise YFDataException(f\"Failed to parse json response from Yahoo Finance: {result}\")\n            self._recommendations = pd.DataFrame(data)\n    return self._recommendations",
        "import_statements": [
            "import datetime",
            "import json",
            "import warnings",
            "from collections.abc import MutableMapping",
            "import requests",
            "from yfinance import utils",
            "from yfinance.data import YfData",
            "from yfinance.const import quote_summary_valid_modules, _BASE_URL_",
            "from yfinance.exceptions import YFNotImplementedError, YFDataException, YFException"
        ],
        "reference_api": [
            "pd.DataFrame",
            "YFDataException",
            "self._fetch"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._fetch",
                "code": "def _fetch(self, proxy, modules: list):\n        if not isinstance(modules, list):\n            raise YFException(\"Should provide a list of modules, see available modules using `valid_modules`\")\n\n        modules = ','.join([m for m in modules if m in quote_summary_valid_modules])\n        if len(modules) == 0:\n            raise YFException(\"No valid modules provided, see available modules using `valid_modules`\")\n        params_dict = {\"modules\": modules, \"corsDomain\": \"finance.yahoo.com\", \"formatted\": \"false\", \"symbol\": self._symbol}\n        try:\n            result = self._data.get_raw_json(_QUOTE_SUMMARY_URL_ + f\"/{self._symbol}\", user_agent_headers=self._data.user_agent_headers, params=params_dict, proxy=proxy)\n        except requests.exceptions.HTTPError as e:\n            utils.get_yf_logger().error(str(e))\n            return None\n        return result"
            }
        ],
        "third_party": [
            "pd.DataFrame",
            "YFDataException",
            "pd.DataFrame"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "reiinakano/scikit-plot",
        "file_path": "scikitplot/cluster.py",
        "function_declaration": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\")",
        "start_line": "19",
        "end_line": "107",
        "docstring": "The function plot_elbow_curve generates an elbow plot to help determine the optimal number of clusters for a given clustering algorithm and dataset. It takes the classifier, data, and various optional parameters including the title, cluster range, number of parallel jobs, whether to show clustering time, axes, figure size, and font sizes. The function checks if the classifier has the attribute 'n_clusters', raises an error if not, and then scores the classifier for each cluster number in the specified range. It plots the sum of squared errors against the number of clusters and optionally plots the clustering duration on a secondary y-axis. The function returns the axis object used for the plot.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "abaf8c2a3ecc",
        "ground_truth": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None, n_jobs=1,\n                     show_cluster_time=True, ax=None, figsize=None,\n                     title_fontsize=\"large\", text_fontsize=\"medium\"):\n    \"\"\"Plots elbow curve of different values of K for KMeans clustering.\n     Args:\n        clf: Clusterer instance that implements ``fit``,``fit_predict``, and\n            ``score`` methods, and an ``n_clusters`` hyperparameter.\n            e.g. :class:`sklearn.cluster.KMeans` instance\n         X (array-like, shape (n_samples, n_features)):\n            Data to cluster, where n_samples is the number of samples and\n            n_features is the number of features.\n         title (string, optional): Title of the generated plot. Defaults to\n            \"Elbow Plot\"\n         cluster_ranges (None or :obj:`list` of int, optional): List of\n            n_clusters for which to plot the explained variances. Defaults to\n            ``range(1, 12, 2)``.\n         n_jobs (int, optional): Number of jobs to run in parallel. Defaults to\n            1.\n         show_cluster_time (bool, optional): Include plot of time it took to\n            cluster for a particular K.\n         ax (:class:`matplotlib.axes.Axes`, optional): The axes upon which to\n            plot the curve. If None, the plot is drawn on a new set of axes.\n         figsize (2-tuple, optional): Tuple denoting figure size of the plot\n            e.g. (6, 6). Defaults to ``None``.\n         title_fontsize (string or int, optional): Matplotlib-style fontsizes.\n            Use e.g. \"small\", \"medium\", \"large\" or integer-values. Defaults to\n            \"large\".\n         text_fontsize (string or int, optional): Matplotlib-style fontsizes.\n            Use e.g. \"small\", \"medium\", \"large\" or integer-values. Defaults to\n            \"medium\".\n     Returns:\n        ax (:class:`matplotlib.axes.Axes`): The axes on which the plot was\n            drawn.\n     Example:\n        >>> import scikitplot as skplt\n        >>> kmeans = KMeans(random_state=1)\n        >>> skplt.cluster.plot_elbow_curve(kmeans, cluster_ranges=range(1, 30))\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fe967d64490>\n        >>> plt.show()\n         .. image:: _static/examples/plot_elbow_curve.png\n           :align: center\n           :alt: Elbow Curve\n    \"\"\"\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n    else:\n        cluster_ranges = sorted(cluster_ranges)\n     if not hasattr(clf, 'n_clusters'):\n        raise TypeError('\"n_clusters\" attribute not in classifier. '\n                        'Cannot plot elbow method.')\n     tuples = Parallel(n_jobs=n_jobs)(delayed(_clone_and_score_clusterer)\n                                     (clf, X, i) for i in cluster_ranges)\n    clfs, times = zip(*tuples)\n     if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n     ax.set_title(title, fontsize=title_fontsize)\n    ax.plot(cluster_ranges, np.absolute(clfs), 'b*-')\n    ax.grid(True)\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of Squared Errors', fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n     if show_cluster_time:\n        ax2_color = 'green'\n        ax2 = ax.twinx()\n        ax2.plot(cluster_ranges, times, ':', alpha=0.75, color=ax2_color)\n        ax2.set_ylabel('Clustering duration (seconds)',\n                       color=ax2_color, alpha=0.75,\n                       fontsize=text_fontsize)\n        ax2.tick_params(colors=ax2_color, labelsize=text_fontsize)\n     return ax",
        "import_statements": [
            "import time",
            "from sklearn.base import clone",
            "from joblib import Parallel, delayed"
        ],
        "reference_api": [
            "plt.subplots",
            "ax.plot",
            "ax2.set_ylabel",
            "np.absolute",
            "ax.set_ylabel",
            "Parallel",
            "hasattr",
            "TypeError",
            "ax2.plot",
            "zip",
            "ax.twinx",
            "ax2.tick_params",
            "ax.set_title",
            "sorted",
            "ax.set_xlabel",
            "ax.grid",
            "ax.tick_params",
            "delayed",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "Parallel",
            "delayed",
            "plt.subplots",
            "ax.set_title",
            "ax.plot",
            "np.absolute",
            "ax.grid",
            "ax.set_xlabel",
            "ax.set_ylabel",
            "ax.tick_params",
            "ax.twinx",
            "ax2.plot",
            "ax2.set_ylabel",
            "ax2.tick_params"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "reiinakano/scikit-plot",
        "file_path": "scikitplot/cluster.py",
        "function_declaration": "def _clone_and_score_clusterer(clf, X, n_clusters)",
        "start_line": "110",
        "end_line": "132",
        "docstring": "The function _clone_and_score_clusterer takes a clusterer clf, a dataset X, and the number of clusters n_clusters as arguments.\\nIt starts by recording the current time and then clones the given clusterer clf.\\nIt sets the number of clusters for the cloned clusterer to n_clusters.\\nThe function fits the cloned clusterer to the dataset X and returns the clustering score along with the time taken to perform the operation.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8a90815c4ee1",
        "ground_truth": "def _clone_and_score_clusterer(clf, X, n_clusters):\n    \"\"\"Clones and scores clusterer instance.\n     Args:\n        clf: Clusterer instance that implements ``fit``,``fit_predict``, and\n            ``score`` methods, and an ``n_clusters`` hyperparameter.\n            e.g. :class:`sklearn.cluster.KMeans` instance\n         X (array-like, shape (n_samples, n_features)):\n            Data to cluster, where n_samples is the number of samples and\n            n_features is the number of features.\n         n_clusters (int): Number of clusters\n     Returns:\n        score: Score of clusters\n         time: Number of seconds it took to fit cluster\n    \"\"\"\n    start = time.time()\n    clf = clone(clf)\n    setattr(clf, 'n_clusters', n_clusters)\n    return clf.fit(X).score(X), time.time() - start",
        "import_statements": [
            "import time",
            "from sklearn.base import clone",
            "from joblib import Parallel, delayed"
        ],
        "reference_api": [
            "score",
            "clf.fit",
            "setattr",
            "time.time",
            "clone"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "clone",
            "score",
            "clf.fit"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "reiinakano/scikit-plot",
        "file_path": "scikitplot/clustering.py",
        "function_declaration": "def clustering_factory(clf)",
        "start_line": "18",
        "end_line": "50",
        "docstring": "The function clustering_factory takes a clustering object clf as an argument.\\nIt first checks if the object has the required methods 'fit' and 'fit_predict'.\\nIf any of these methods are missing, it raises a TypeError indicating that the object is not a valid clusterer instance.\\nThe function then defines additional methods 'plot_silhouette' and 'plot_elbow_curve'.\\nIt iterates over these additional methods and attaches them to the clusterer object.\\nIf the object already has a method with the same name, it issues a warning and overrides the method.\\nFinally, it returns the modified clusterer object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "da40bd5dd81a",
        "ground_truth": "def clustering_factory(clf):\n    \"\"\"Embeds scikit-plot plotting methods in an sklearn clusterer instance.\n     Args:\n        clf: Scikit-learn clusterer instance\n     Returns:\n        The same scikit-learn clusterer instance passed in **clf** with\n        embedded scikit-plot instance methods.\n     Raises:\n        ValueError: If **clf** does not contain the instance methods necessary\n            for scikit-plot instance methods.\n    \"\"\"\n    required_methods = ['fit', 'fit_predict']\n     for method in required_methods:\n        if not hasattr(clf, method):\n            raise TypeError('\"{}\" is not in clf. Did you '\n                            'pass a clusterer instance?'.format(method))\n     additional_methods = {\n        'plot_silhouette': plot_silhouette,\n        'plot_elbow_curve': plot_elbow_curve\n    }\n     for key, fn in six.iteritems(additional_methods):\n        if hasattr(clf, key):\n            warnings.warn('\"{}\" method already in clf. '\n                          'Overriding anyway. This may '\n                          'result in unintended behavior.'.format(key))\n        setattr(clf, key, types.MethodType(fn, clf))\n    return clf",
        "import_statements": [
            "import six",
            "import warnings",
            "import types",
            "from sklearn.utils import deprecated",
            "from scikitplot.plotters import plot_silhouette, plot_elbow_curve"
        ],
        "reference_api": [
            "types.MethodType",
            "warnings.warn",
            "hasattr",
            "TypeError",
            "setattr",
            "six.iteritems",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "six.iteritems"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "reiinakano/scikit-plot",
        "file_path": "scikitplot/plotters.py",
        "function_declaration": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\")",
        "start_line": "666",
        "end_line": "770",
        "docstring": "The function plot_learning_curve generates a learning curve plot for a given classifier clf using the provided training data X and labels y.\\nIt optionally accepts parameters for the plot title, cross-validation strategy, training sizes, number of jobs, scoring method, axes object, figure size, and font sizes for the title and text.\\nIf no axes object is provided, it creates one.\\nThe function sets the plot title and labels for the x and y axes.\\nIt calculates the training and cross-validation scores using the learning_curve function and computes their means and standard deviations.\\nThe function then plots the learning curves, including shaded areas representing the standard deviation, and adds a legend.\\nFinally, it returns the axes object with the plot.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b8d8ba3c0b1b",
        "ground_truth": "def plot_learning_curve(clf, X, y, title='Learning Curve', cv=None,\n                        train_sizes=None, n_jobs=1, scoring=None,\n                        ax=None, figsize=None, title_fontsize=\"large\",\n                        text_fontsize=\"medium\"):\n    \"\"\"Generates a plot of the train and test learning curves for a classifier.\n     Args:\n        clf: Classifier instance that implements ``fit`` and ``predict``\n            methods.\n         X (array-like, shape (n_samples, n_features)):\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n         y (array-like, shape (n_samples) or (n_samples, n_features)):\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n         title (string, optional): Title of the generated plot. Defaults to\n            \"Learning Curve\"\n         cv (int, cross-validation generator, iterable, optional): Determines\n            the cross-validation strategy to be used for splitting.\n             Possible inputs for cv are:\n              - None, to use the default 3-fold cross-validation,\n              - integer, to specify the number of folds.\n              - An object to be used as a cross-validation generator.\n              - An iterable yielding train/test splits.\n             For integer/None inputs, if ``y`` is binary or multiclass,\n            :class:`StratifiedKFold` used. If the estimator is not a classifier\n            or if ``y`` is neither binary nor multiclass, :class:`KFold` is\n            used.\n         train_sizes (iterable, optional): Determines the training sizes used to\n            plot the learning curve. If None, ``np.linspace(.1, 1.0, 5)`` is\n            used.\n         n_jobs (int, optional): Number of jobs to run in parallel. Defaults to\n            1.\n                     scoring (string, callable or None, optional): default: None\n            A string (see scikit-learn model evaluation documentation) or a\n            scorerbcallable object / function with signature\n            scorer(estimator, X, y).\n         ax (:class:`matplotlib.axes.Axes`, optional): The axes upon which to\n            plot the curve. If None, the plot is drawn on a new set of axes.\n         figsize (2-tuple, optional): Tuple denoting figure size of the plot\n            e.g. (6, 6). Defaults to ``None``.\n         title_fontsize (string or int, optional): Matplotlib-style fontsizes.\n            Use e.g. \"small\", \"medium\", \"large\" or integer-values. Defaults to\n            \"large\".\n         text_fontsize (string or int, optional): Matplotlib-style fontsizes.\n            Use e.g. \"small\", \"medium\", \"large\" or integer-values. Defaults to\n            \"medium\".\n     Returns:\n        ax (:class:`matplotlib.axes.Axes`): The axes on which the plot was\n            drawn.\n     Example:\n        >>> import scikitplot.plotters as skplt\n        >>> rf = RandomForestClassifier()\n        >>> skplt.plot_learning_curve(rf, X, y)\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fe967d64490>\n        >>> plt.show()\n         .. image:: _static/examples/plot_learning_curve.png\n           :align: center\n           :alt: Learning Curve\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n     if train_sizes is None:\n        train_sizes = np.linspace(.1, 1.0, 5)\n     ax.set_title(title, fontsize=title_fontsize)\n    ax.set_xlabel(\"Training examples\", fontsize=text_fontsize)\n    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n    train_sizes, train_scores, test_scores = learning_curve(\n        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,\n        scoring=scoring)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax.grid()\n    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n    ax.tick_params(labelsize=text_fontsize)\n    ax.legend(loc=\"best\", fontsize=text_fontsize)\n     return ax",
        "import_statements": [
            "import warnings",
            "import itertools",
            "from sklearn.metrics import confusion_matrix",
            "from sklearn.preprocessing import label_binarize",
            "from sklearn.metrics import roc_curve",
            "from sklearn.metrics import auc",
            "from sklearn.metrics import precision_recall_curve",
            "from sklearn.metrics import average_precision_score",
            "from sklearn.utils.multiclass import unique_labels",
            "from sklearn.model_selection import learning_curve",
            "from sklearn.base import clone",
            "from sklearn.metrics import silhouette_score",
            "from sklearn.metrics import silhouette_samples",
            "from sklearn.utils import deprecated",
            "from scipy import interp",
            "from scikitplot.helpers import binary_ks_curve, validate_labels"
        ],
        "reference_api": [
            "plt.subplots",
            "ax.fill_between",
            "ax.legend",
            "ax.set_xlabel",
            "np.std",
            "ax.plot",
            "np.mean",
            "ax.set_title",
            "ax.grid",
            "ax.tick_params",
            "np.linspace",
            "learning_curve",
            "ax.set_ylabel"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "plt.subplots",
            "np.linspace",
            "ax.set_title",
            "ax.set_xlabel",
            "ax.set_ylabel",
            "learning_curve",
            "np.mean",
            "np.std",
            "np.mean",
            "np.std",
            "ax.grid",
            "ax.fill_between",
            "ax.fill_between",
            "ax.plot",
            "ax.plot",
            "ax.tick_params",
            "ax.legend"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "reiinakano/scikit-plot",
        "file_path": "scikitplot/plotters.py",
        "function_declaration": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\")",
        "start_line": "893",
        "end_line": "970",
        "docstring": "The function plot_elbow_curve generates an elbow plot to help determine the optimal number of clusters for a clustering algorithm.\\nIt accepts a classifier clf, data X, and optional parameters for the plot's appearance.\\nIf cluster_ranges is not provided, it defaults to a range from 1 to 11 with a step of 2.\\nThe function checks if the classifier has an n_clusters attribute and raises a TypeError if it does not.\\nIt then creates a list of classifiers with different cluster numbers, fits them to the data, and scores them.\\nIf no axes object ax is provided, it creates a new one.\\nThe function plots the number of clusters against the absolute value of the scores, adds grid lines, labels, and sets the title and font sizes.\\nIt returns the axes object with the plot.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e6e76eb35945",
        "ground_truth": "def plot_elbow_curve(clf, X, title='Elbow Plot', cluster_ranges=None,\n                     ax=None, figsize=None, title_fontsize=\"large\",\n                     text_fontsize=\"medium\"):\n    \"\"\"Plots elbow curve of different values of K for KMeans clustering.\n     Args:\n        clf: Clusterer instance that implements ``fit`` and ``fit_predict``\n            methods and a ``score`` parameter.\n         X (array-like, shape (n_samples, n_features)):\n            Data to cluster, where n_samples is the number of samples and\n            n_features is the number of features.\n         title (string, optional): Title of the generated plot. Defaults to\n            \"Elbow Plot\"\n         cluster_ranges (None or :obj:`list` of int, optional): List of\n            n_clusters for which to plot the explained variances. Defaults to\n            ``range(1, 12, 2)``.\n         copy (boolean, optional): Determines whether ``fit`` is used on\n            **clf** or on a copy of **clf**.\n         ax (:class:`matplotlib.axes.Axes`, optional): The axes upon which to\n            plot the curve. If None, the plot is drawn on a new set of axes.\n         figsize (2-tuple, optional): Tuple denoting figure size of the plot\n            e.g. (6, 6). Defaults to ``None``.\n         title_fontsize (string or int, optional): Matplotlib-style fontsizes.\n            Use e.g. \"small\", \"medium\", \"large\" or integer-values. Defaults to\n            \"large\".\n         text_fontsize (string or int, optional): Matplotlib-style fontsizes.\n            Use e.g. \"small\", \"medium\", \"large\" or integer-values. Defaults to\n            \"medium\".\n     Returns:\n        ax (:class:`matplotlib.axes.Axes`): The axes on which the plot was\n            drawn.\n     Example:\n        >>> import scikitplot.plotters as skplt\n        >>> kmeans = KMeans(random_state=1)\n        >>> skplt.plot_elbow_curve(kmeans, cluster_ranges=range(1, 11))\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fe967d64490>\n        >>> plt.show()\n         .. image:: _static/examples/plot_elbow_curve.png\n           :align: center\n           :alt: Elbow Curve\n    \"\"\"\n    if cluster_ranges is None:\n        cluster_ranges = range(1, 12, 2)\n    else:\n        cluster_ranges = sorted(cluster_ranges)\n     if not hasattr(clf, 'n_clusters'):\n        raise TypeError('\"n_clusters\" attribute not in classifier. '\n                        'Cannot plot elbow method.')\n     clfs = []\n    for i in cluster_ranges:\n        current_clf = clone(clf)\n        setattr(current_clf, \"n_clusters\", i)\n        clfs.append(current_clf.fit(X).score(X))\n     if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n     ax.set_title(title, fontsize=title_fontsize)\n    ax.plot(cluster_ranges, np.absolute(clfs), 'b*-')\n    ax.grid(True)\n    ax.set_xlabel('Number of clusters', fontsize=text_fontsize)\n    ax.set_ylabel('Sum of Squared Errors', fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n     return ax",
        "import_statements": [
            "import warnings",
            "import itertools",
            "from sklearn.metrics import confusion_matrix",
            "from sklearn.preprocessing import label_binarize",
            "from sklearn.metrics import roc_curve",
            "from sklearn.metrics import auc",
            "from sklearn.metrics import precision_recall_curve",
            "from sklearn.metrics import average_precision_score",
            "from sklearn.utils.multiclass import unique_labels",
            "from sklearn.model_selection import learning_curve",
            "from sklearn.base import clone",
            "from sklearn.metrics import silhouette_score",
            "from sklearn.metrics import silhouette_samples",
            "from sklearn.utils import deprecated",
            "from scipy import interp",
            "from scikitplot.helpers import binary_ks_curve, validate_labels"
        ],
        "reference_api": [
            "score",
            "current_clf.fit",
            "sorted",
            "plt.subplots",
            "ax.set_xlabel",
            "hasattr",
            "TypeError",
            "ax.plot",
            "clfs.append",
            "np.absolute",
            "setattr",
            "ax.set_title",
            "ax.grid",
            "ax.tick_params",
            "range",
            "ax.set_ylabel",
            "clone"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "clone",
            "clfs.append",
            "score",
            "current_clf.fit",
            "plt.subplots",
            "ax.set_title",
            "ax.plot",
            "np.absolute",
            "ax.grid",
            "ax.set_xlabel",
            "ax.set_ylabel",
            "ax.tick_params"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "reiinakano/scikit-plot",
        "file_path": "scikitplot/plotters.py",
        "function_declaration": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\")",
        "start_line": "1059",
        "end_line": "1131",
        "docstring": "The function plot_pca_2d_projection creates a 2D scatter plot of the PCA-transformed data for visualizing the first two principal components. It takes a classifier, data, labels, and several optional parameters including the title, axis, figure size, colormap, and font sizes. The function applies PCA transformation to the data, sets up the plot, and colors the points according to their labels. It labels the axes with the first and second principal components, adds a legend, and returns the axis object used for the plot.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6f4530c8a20f",
        "ground_truth": "def plot_pca_2d_projection(clf, X, y, title='PCA 2-D Projection', ax=None,\n                           figsize=None, cmap='Spectral',\n                           title_fontsize=\"large\", text_fontsize=\"medium\"):\n    \"\"\"Plots the 2-dimensional projection of PCA on a given dataset.\n     Args:\n        clf: Fitted PCA instance that can ``transform`` given data set into 2\n            dimensions.\n         X (array-like, shape (n_samples, n_features)):\n            Feature set to project, where n_samples is the number of samples\n            and n_features is the number of features.\n         y (array-like, shape (n_samples) or (n_samples, n_features)):\n            Target relative to X for labeling.\n         title (string, optional): Title of the generated plot. Defaults to\n            \"PCA 2-D Projection\"\n         ax (:class:`matplotlib.axes.Axes`, optional): The axes upon which to\n            plot the curve. If None, the plot is drawn on a new set of axes.\n         figsize (2-tuple, optional): Tuple denoting figure size of the plot\n            e.g. (6, 6). Defaults to ``None``.\n         cmap (string or :class:`matplotlib.colors.Colormap` instance, optional):\n            Colormap used for plotting the projection. View Matplotlib Colormap\n            documentation for available options.\n            https://matplotlib.org/users/colormaps.html\n         title_fontsize (string or int, optional): Matplotlib-style fontsizes.\n            Use e.g. \"small\", \"medium\", \"large\" or integer-values. Defaults to\n            \"large\".\n         text_fontsize (string or int, optional): Matplotlib-style fontsizes.\n            Use e.g. \"small\", \"medium\", \"large\" or integer-values. Defaults to\n            \"medium\".\n     Returns:\n        ax (:class:`matplotlib.axes.Axes`): The axes on which the plot was\n            drawn.\n     Example:\n        >>> import scikitplot.plotters as skplt\n        >>> pca = PCA(random_state=1)\n        >>> pca.fit(X)\n        >>> skplt.plot_pca_2d_projection(pca, X, y)\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fe967d64490>\n        >>> plt.show()\n         .. image:: _static/examples/plot_pca_2d_projection.png\n           :align: center\n           :alt: PCA 2D Projection\n    \"\"\"\n    transformed_X = clf.transform(X)\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n     ax.set_title(title, fontsize=title_fontsize)\n    classes = np.unique(np.array(y))\n     colors = plt.cm.get_cmap(cmap)(np.linspace(0, 1, len(classes)))\n     for label, color in zip(classes, colors):\n        ax.scatter(transformed_X[y == label, 0], transformed_X[y == label, 1],\n                   alpha=0.8, lw=2, label=label, color=color)\n    ax.legend(loc='best', shadow=False, scatterpoints=1,\n              fontsize=text_fontsize)\n    ax.set_xlabel('First Principal Component', fontsize=text_fontsize)\n    ax.set_ylabel('Second Principal Component', fontsize=text_fontsize)\n    ax.tick_params(labelsize=text_fontsize)\n     return ax",
        "import_statements": [
            "import warnings",
            "import itertools",
            "from sklearn.metrics import confusion_matrix",
            "from sklearn.preprocessing import label_binarize",
            "from sklearn.metrics import roc_curve",
            "from sklearn.metrics import auc",
            "from sklearn.metrics import precision_recall_curve",
            "from sklearn.metrics import average_precision_score",
            "from sklearn.utils.multiclass import unique_labels",
            "from sklearn.model_selection import learning_curve",
            "from sklearn.base import clone",
            "from sklearn.metrics import silhouette_score",
            "from sklearn.metrics import silhouette_samples",
            "from sklearn.utils import deprecated",
            "from scipy import interp",
            "from scikitplot.helpers import binary_ks_curve, validate_labels"
        ],
        "reference_api": [
            "plt.subplots",
            "ax.legend",
            "ax.set_xlabel",
            "ax.scatter",
            "get_cmap",
            "len",
            "clf.transform",
            "ax.set_title",
            "np.unique",
            "ax.tick_params",
            "np.linspace",
            "zip",
            "ax.set_ylabel",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "clf.transform",
            "plt.subplots",
            "ax.set_title",
            "np.unique",
            "np.array",
            "get_cmap",
            "np.linspace",
            "ax.scatter",
            "ax.legend",
            "ax.set_xlabel",
            "ax.set_ylabel",
            "ax.tick_params"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "rougier/matplotlib-cheatsheet",
        "file_path": "catalogue.py",
        "function_declaration": "def barplot(ax)",
        "start_line": "288",
        "end_line": "311",
        "docstring": "The function barplot takes a matplotlib axis object ax as an argument.\\nIt first generates two clipping paths using the clip_path function.\\nThe function then creates data for 16 bars using NumPy arrays, with values for Y1 and Y2 generated randomly within a specified range.\\nIt plots four sets of bars on the provided axis: two sets with different shades of gray, and two sets with shades of red, each clipped to one of the clipping paths.\\nThe x-axis limits are set from -1 to 16.\\nThe function also adds two text labels below the plot, one labeled 'Bar plot' in black and another labeled 'ax.bar()' in blue monospace font.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e7c2775d024c",
        "ground_truth": "def barplot(ax):\n    clip1, clip2 = clip_path(ax)\n     n = 16\n    X = np.arange(n)\n    Y1 = (1-0.25*X/n) * np.random.uniform(0.25, 0.75, n)\n    Y2 = (1-0.25*X/n) * np.random.uniform(0.25, 0.75, n)\n    ax.bar(X, +Y1, 1, facecolor='#cccccc', edgecolor='white', clip_path=clip1)\n    ax.bar(X, -Y2, 1, facecolor='#999999', edgecolor='white', clip_path=clip1)\n    ax.bar(X, +Y1, 1, facecolor='#ffaaaa', edgecolor='white', clip_path=clip2)\n    ax.bar(X, -Y2, 1, facecolor='#ff7777', edgecolor='white', clip_path=clip2)\n    ax.set_xlim(-1,n)\n     ax.text(0.0, -0.08, 'Bar plot',\n            color='k',\n            ha='left',\n            size=8,\n            transform=ax.transAxes)\n    ax.text(1.0, -0.08, 'ax.bar()',\n            color='blue',\n            ha='right',\n            size=8,\n            transform=ax.transAxes,\n            family=\"monospace\")",
        "import_statements": [
            "from matplotlib.text import Text",
            "from matplotlib.lines import Line2D",
            "from mpl_toolkits.mplot3d import Axes3D",
            "from matplotlib.patches import Rectangle, Polygon"
        ],
        "reference_api": [
            "clip_path",
            "ax.bar",
            "ax.set_xlim",
            "ax.text",
            "uniform",
            "np.arange"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "clip_path",
                "code": "def clip_path(ax):\n    T1 = [(0,0),(0,1),(1,1)]\n    T2 = [(0,0),(1,0),(1,1)]\n    clip1 = Polygon(T1, closed=True, transform=ax.transAxes,\n                   edgecolor='None', facecolor='None')\n    clip2 = Polygon(T2, closed=True, transform=ax.transAxes,\n                   edgecolor='None', facecolor='None')\n\n    line = Line2D([0, 1], [0, 1], transform=ax.transAxes, linewidth=0.5,\n                  color=\"black\", clip_on=False, zorder=50)\n    ax.add_artist(line)\n\n    return clip1, clip2"
            }
        ],
        "third_party": [
            "np.arange",
            "uniform",
            "uniform",
            "ax.bar",
            "ax.bar",
            "ax.bar",
            "ax.bar",
            "ax.set_xlim",
            "ax.text",
            "ax.text"
        ]
    },
    {
        "subclass": "matplotlib",
        "owner/repo": "rougier/matplotlib-cheatsheet",
        "file_path": "reference-line.py",
        "function_declaration": "def split(n_segment)",
        "start_line": "10",
        "end_line": "16",
        "docstring": "The function split calculates the starting and ending positions of segments within a fixed width. It takes the number of segments as input and computes the width of each segment and the padding between segments. The function then determines the starting (X0) and ending (X1) positions of each segment and returns them as two separate arrays.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0ec1d146a32b",
        "ground_truth": "def split(n_segment):\n    width = 9\n    segment_width = 0.75*(width/n_segment)\n    segment_pad = (width - n_segment*segment_width)/(n_segment-1)\n    X0 = 1+np.arange(n_segment)*(segment_width+segment_pad)\n    X1 = X0 + segment_width\n    return X0, X1",
        "import_statements": [],
        "reference_api": [
            "np.arange"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.arange"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "rougier/scientific-visualization-book",
        "file_path": "code/anatomy/raster-vector.py",
        "function_declaration": "def pixelated_text(dpi=100)",
        "start_line": "13",
        "end_line": "22",
        "docstring": "The function pixelated_text generates an image of a single character 'a' with a specified resolution given by dpi.\\nIt creates a figure and a canvas using Matplotlib, places the character 'a' at the center of the figure with a font size of 75, and removes the axis.\\nThe canvas is drawn and its content is converted to a NumPy array representing the image in ARGB format.\\nThe function reshapes this array to the dimensions specified by dpi and adjusts the color channels by rolling the array along the last axis.\\nFinally, it returns the resulting image array.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "604a3e93ac0d",
        "ground_truth": "def pixelated_text(dpi=100):\n    fig = Figure(figsize=(1, 1), dpi=dpi)\n    canvas, ax = FigureCanvasAgg(fig), fig.gca()\n    ax.text(0.5, 0.5, \"a\", fontsize=75, ha=\"center\", va=\"center\")\n    ax.axis(\"off\")\n    canvas.draw()\n    image = np.frombuffer(canvas.tostring_argb(), dtype=\"uint8\")\n    image = image.reshape(dpi, dpi, 4)\n    image = np.roll(image, 3, axis=2)\n    return image",
        "import_statements": [
            "from matplotlib.figure import Figure",
            "from matplotlib.backends.backend_agg import FigureCanvasAgg",
            "from matplotlib.patches import Rectangle"
        ],
        "reference_api": [
            "Figure",
            "np.frombuffer",
            "ax.axis",
            "image.reshape",
            "np.roll",
            "ax.text",
            "canvas.tostring_argb",
            "FigureCanvasAgg",
            "canvas.draw",
            "fig.gca"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "Figure",
            "FigureCanvasAgg",
            "fig.gca",
            "ax.text",
            "ax.axis",
            "canvas.draw",
            "np.frombuffer",
            "canvas.tostring_argb",
            "image.reshape",
            "np.roll"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "rougier/scientific-visualization-book",
        "file_path": "code/anatomy/zorder-plots.py",
        "function_declaration": "def curve()",
        "start_line": "11",
        "end_line": "22",
        "docstring": "The function curve generates a composite curve by summing multiple Gaussian-like components. It randomly determines the number of components, their centers, widths, and scales. The widths and scales are normalized to ensure they sum to specific values. The function initializes an array of zeros and iteratively adds scaled Gaussian components based on the calculated parameters. Finally, it returns the resulting composite curve.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2a7fc7d560ea",
        "ground_truth": "def curve():\n    n = np.random.randint(1, 5)\n    centers = np.random.normal(0.0, 1.0, n)\n    widths = np.random.uniform(5.0, 50.0, n)\n    widths = 10 * widths / widths.sum()\n    scales = np.random.uniform(0.1, 1.0, n)\n    scales /= scales.sum()\n    X = np.zeros(500)\n    x = np.linspace(-3, 3, len(X))\n    for center, width, scale in zip(centers, widths, scales):\n        X = X + scale * np.exp(-(x - center) * (x - center) * width)\n    return X",
        "import_statements": [],
        "reference_api": [
            "widths.sum",
            "np.linspace",
            "normal",
            "len",
            "np.exp",
            "uniform",
            "np.zeros",
            "randint",
            "scales.sum",
            "zip"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "randint",
            "normal",
            "uniform",
            "widths.sum",
            "uniform",
            "scales.sum",
            "np.zeros",
            "np.linspace",
            "np.exp"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "rougier/scientific-visualization-book",
        "file_path": "code/animation/fluid.py",
        "function_declaration": "def difference(derivative, accuracy=1)",
        "start_line": "13",
        "end_line": "20",
        "docstring": "The function difference computes finite difference coefficients for numerical differentiation.\\nIt takes a derivative order and an optional accuracy level as inputs.\\nThe derivative order is incremented by one, and the radius is calculated based on the accuracy and derivative order.\\nThe function defines a range of points centered around zero and calculates the inverse of the Vandermonde matrix for these points.\\nIt returns the coefficients for the specified derivative order, scaled by the factorial of one less than the derivative order, along with the points.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "be694335ebfc",
        "ground_truth": "def difference(derivative, accuracy=1):\n    # Central differences implemented based on the article here:\n    # http://web.media.mit.edu/~crtaylor/calculator.html\n    derivative += 1\n    radius = accuracy + derivative // 2 - 1\n    points = range(-radius, radius + 1)\n    coefficients = np.linalg.inv(np.vander(points))\n    return coefficients[-derivative] * factorial(derivative - 1), points",
        "import_statements": [
            "from math import factorial",
            "from itertools import cycle",
            "from functools import reduce",
            "from scipy.sparse.linalg import factorized",
            "from scipy.ndimage import map_coordinates, spline_filter"
        ],
        "reference_api": [
            "inv",
            "factorial",
            "np.vander",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "inv",
            "np.vander",
            "factorial"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "rougier/scientific-visualization-book",
        "file_path": "code/typography/typography-text-path.py",
        "function_declaration": "def interpolate(X, Y, T)",
        "start_line": "14",
        "end_line": "18",
        "docstring": "The function interpolate computes the linear interpolation of given X and Y coordinates based on a target array T. It first calculates the distances between consecutive points, then computes the cumulative distances. The function uses these cumulative distances to interpolate the X and Y coordinates at the positions specified by T. Finally, it returns the interpolated X and Y coordinates along with the total distance covered by the original coordinates.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fa3fb47f33fa",
        "ground_truth": "def interpolate(X, Y, T):\n    dR = (np.diff(X) ** 2 + np.diff(Y) ** 2) ** 0.5\n    R = np.zeros_like(X)\n    R[1:] = np.cumsum(dR)\n    return np.interp(T, R, X), np.interp(T, R, Y), R[-1]",
        "import_statements": [
            "import scipy",
            "from matplotlib.textpath import TextPath",
            "from matplotlib.patches import PathPatch",
            "from matplotlib.font_manager import FontProperties"
        ],
        "reference_api": [
            "np.interp",
            "np.zeros_like",
            "np.cumsum",
            "np.diff"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.diff",
            "np.diff",
            "np.zeros_like",
            "np.cumsum",
            "np.interp",
            "np.interp"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "rougier/scientific-visualization-book",
        "file_path": "code/showcases/mandelbrot.py",
        "function_declaration": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0)",
        "start_line": "11",
        "end_line": "22",
        "docstring": "The function mandelbrot_set generates the Mandelbrot set for a given range of complex numbers.\\nIt takes the minimum and maximum values for the real and imaginary parts (xmin, xmax, ymin, ymax), the number of points in the x and y directions (xn, yn), the maximum number of iterations (maxiter), and an optional escape radius (horizon).\\nThe function creates a grid of complex numbers C from the specified ranges, initializes iteration count array N and complex array Z.\\nIt iteratively computes the Mandelbrot sequence, updating N with the iteration count where the magnitude of Z exceeds the horizon.\\nPoints that remain bounded after maxiter iterations are set to 0 in N.\\nThe function returns the final complex values Z and the iteration counts N.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "870a2dad3c66",
        "ground_truth": "def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0):\n    X = np.linspace(xmin, xmax, xn, dtype=np.float32)\n    Y = np.linspace(ymin, ymax, yn, dtype=np.float32)\n    C = X + Y[:, None] * 1j\n    N = np.zeros(C.shape, dtype=int)\n    Z = np.zeros(C.shape, np.complex64)\n    for n in range(maxiter):\n        I = np.less(abs(Z), horizon)\n        N[I] = n\n        Z[I] = Z[I] ** 2 + C[I]\n    N[N == maxiter - 1] = 0\n    return Z, N",
        "import_statements": [
            "from matplotlib import colors"
        ],
        "reference_api": [
            "np.zeros",
            "abs",
            "np.linspace",
            "np.less",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.linspace",
            "np.linspace",
            "np.zeros",
            "np.zeros",
            "np.less"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "rougier/scientific-visualization-book",
        "file_path": "code/animation/less-is-more.py",
        "function_declaration": "def smooth2d(A, sigma=3)",
        "start_line": "31",
        "end_line": "37",
        "docstring": "The function smooth2d applies a smoothing operation to a 2D array using a specified sigma value. It calculates the window length based on sigma, ensuring it is an odd number. The function first smooths each row of the array using a 1D smoothing function, then transposes the result and smooths each row of the transposed array. Finally, it transposes the array again and returns the smoothed 2D array.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e1a908945acf",
        "ground_truth": "def smooth2d(A, sigma=3):\n    window_len = max(int(sigma), 3) * 2 + 1\n    A1 = np.array([smooth1d(x, window_len) for x in np.asarray(A)])\n    A2 = np.transpose(A1)\n    A3 = np.array([smooth1d(x, window_len) for x in A2])\n    A4 = np.transpose(A3)\n    return A4",
        "import_statements": [
            "import re",
            "import sys",
            "import matplotlib",
            "from matplotlib.artist import Artist"
        ],
        "reference_api": [
            "smooth1d",
            "np.asarray",
            "int",
            "np.transpose",
            "max",
            "np.array"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "smooth1d",
                "code": "def smooth1d(x, window_len):\n    s = np.r_[2 * x[0] - x[window_len:1:-1], x, 2 * x[-1] - x[-1:-window_len:-1]]\n    w = np.hanning(window_len)\n    y = np.convolve(w / w.sum(), s, mode=\"same\")\n    return y[window_len - 1 : -window_len + 1]"
            },
            {
                "name": "smooth1d",
                "code": "def smooth1d(x, window_len):\n    s = np.r_[2 * x[0] - x[window_len:1:-1], x, 2 * x[-1] - x[-1:-window_len:-1]]\n    w = np.hanning(window_len)\n    y = np.convolve(w / w.sum(), s, mode=\"same\")\n    return y[window_len - 1 : -window_len + 1]"
            }
        ],
        "third_party": [
            "np.array",
            "np.asarray",
            "np.transpose",
            "np.array",
            "np.transpose"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "rougier/scientific-visualization-book",
        "file_path": "code/animation/less-is-more.py",
        "function_declaration": "def process_image(self, padded_src, dpi)",
        "start_line": "67",
        "end_line": "71",
        "docstring": "The function process_image takes a padded source image padded_src and a DPI value as arguments.\\nIt retrieves the offsets ox and oy from the instance's attributes.\\nThe function then shifts the image horizontally by ox converted to pixels and vertically by oy converted to pixels using numpy's roll function.\\nFinally, it returns the processed image.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "94563c54b483",
        "ground_truth": "def process_image(self, padded_src, dpi):\n    ox, oy = self.offsets\n    a1 = np.roll(padded_src, int(ox / 72.0 * dpi), axis=1)\n    a2 = np.roll(a1, -int(oy / 72.0 * dpi), axis=0)\n    return a2",
        "import_statements": [
            "import re",
            "import sys",
            "import matplotlib",
            "from matplotlib.artist import Artist"
        ],
        "reference_api": [
            "int",
            "np.roll"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.roll",
            "np.roll"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "rougier/scientific-visualization-book",
        "file_path": "code/rules/helper.py",
        "function_declaration": "def gaussian(shape=(25, 25), width=0.5, center=0.0)",
        "start_line": "47",
        "end_line": "63",
        "docstring": "The function gaussian generates a multi-dimensional Gaussian distribution array with a specified shape, width, and center. It initializes the shape, width, and center parameters as tuples if they are provided as single values. It then creates a grid for the specified shape and computes the squared distance of each grid point from the center, normalized by the width. Finally, it returns the Gaussian values computed using the exponential function applied to the negative half of the squared distances.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d71d232872fe",
        "ground_truth": "def gaussian(shape=(25, 25), width=0.5, center=0.0):\n    \"\"\" Generate a gaussian of the form g(x) = height*exp(-(x-center)**2/width**2). \"\"\"\n    if type(shape) in [float, int]:\n        shape = (shape,)\n    if type(width) in [float, int]:\n        width = (width,) * len(shape)\n    if type(center) in [float, int]:\n        center = (center,) * len(shape)\n    grid = []\n    for size in shape:\n        grid.append(slice(0, size))\n    C = np.mgrid[tuple(grid)]\n    R = np.zeros(shape)\n    for i, size in enumerate(shape):\n        if shape[i] > 1:\n            R += (((C[i] / float(size - 1)) * 2 - 1 - center[i]) / width[i]) ** 2\n    return np.exp(-R / 2)",
        "import_statements": [
            "import os",
            "from parameters import *"
        ],
        "reference_api": [
            "tuple",
            "float",
            "len",
            "grid.append",
            "type",
            "np.exp",
            "np.zeros",
            "slice",
            "enumerate"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "grid.append",
            "np.zeros",
            "np.exp"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "rougier/scientific-visualization-book",
        "file_path": "code/rules/helper.py",
        "function_declaration": "def stimulus(position, size, intensity)",
        "start_line": "66",
        "end_line": "81",
        "docstring": "This function calculates a stimulus based on a position, size, and intensity.\\nIt converts polar coordinates to Cartesian coordinates and normalizes them.\\nIt creates a mesh grid representing the spatial domain, normalized and centered.\\nThe function computes the squared Euclidean distance (R) from each point in the grid to the stimulus center.\\nFinally, it returns an exponential decay function of these distances, scaled by the size parameter.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7bbd08c987bc",
        "ground_truth": "def stimulus(position, size, intensity):\n    \"\"\"\n    Parameters\n    ----------\n     position : (rho,theta) (degrees)\n    size :     float (degrees)\n    intensity: float\n    \"\"\"\n     x, y = cartesian(position[0] / 90.0, np.pi * position[1] / 180.0)\n    Y, X = np.mgrid[0 : shape[0], 0 : shape[1]]\n    X = X / float(shape[1])\n    Y = 2 * Y / float(shape[0]) - 1\n    R = (X - x) ** 2 + (Y - y) ** 2\n    return np.exp(-0.5 * R / (size / 90.0))",
        "import_statements": [
            "import os",
            "from parameters import *"
        ],
        "reference_api": [
            "np.exp",
            "cartesian",
            "float"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "cartesian",
            "np.exp"
        ]
    },
    {
        "subclass": "numpy",
        "owner/repo": "rougier/scientific-visualization-book",
        "file_path": "code/layout/layout-gridspec.py",
        "function_declaration": "def plot(ax, text)",
        "start_line": "31",
        "end_line": "41",
        "docstring": "The function plot configures the axes of a plot and adds text and a title. It sets the x and y limits from 0 to 1, defines the x and y ticks, and labels the axes. The function adds centered text with specified properties and sets the title with a specified font family and weight.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2b5806238d5d",
        "ground_truth": "def plot(ax, text):\n    ax.set_xlim(0, 1)\n    ax.set_xticks(np.linspace(0, 1, 5))\n    ax.set_xlabel(\"X Label\")\n    ax.set_ylim(0, 1)\n    ax.set_yticks(np.linspace(0, 1, 5))\n    ax.set_ylabel(\"Y Label\")\n    ax.text(\n        0.5, 0.5, text, alpha=0.75, ha=\"center\", va=\"center\", weight=\"bold\", size=12\n    )\n    ax.set_title(\"Title\", family=\"Roboto\", weight=500)",
        "import_statements": [],
        "reference_api": [
            "ax.set_yticks",
            "ax.set_xlabel",
            "ax.set_xlim",
            "ax.set_xticks",
            "ax.text",
            "ax.set_title",
            "np.linspace",
            "ax.set_ylim",
            "ax.set_ylabel"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ax.set_xlim",
            "ax.set_xticks",
            "np.linspace",
            "ax.set_xlabel",
            "ax.set_ylim",
            "ax.set_yticks",
            "np.linspace",
            "ax.set_ylabel",
            "ax.text",
            "ax.set_title"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/__init__.py",
        "function_declaration": "def predict_cumulative_hazard_function(self, X, **kwargs)",
        "start_line": "62",
        "end_line": "88",
        "docstring": "The function predict_cumulative_hazard_function predicts the cumulative hazard function for input data X. It iteratively applies a series of transformations to X, excluding the final transformation, by calling the transform method of each transformation in the pipeline. After all transformations, it uses the final estimator in the pipeline to predict the cumulative hazard function based on the transformed data and returns the prediction.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0b42aac61e21",
        "ground_truth": "def predict_cumulative_hazard_function(self, X, **kwargs):\n    \"\"\"Predict cumulative hazard function.\n     The cumulative hazard function for an individual\n    with feature vector :math:`x` is defined as\n     .. math::\n         H(t \\\\mid x) = \\\\exp(x^\\\\top \\\\beta) H_0(t) ,\n     where :math:`H_0(t)` is the baseline hazard function,\n    estimated by Breslow's estimator.\n     Parameters\n    ----------\n    X : array-like, shape = (n_samples, n_features)\n        Data matrix.\n     Returns\n    -------\n    cum_hazard : ndarray, shape = (n_samples,)\n        Predicted cumulative hazard functions.\n    \"\"\"\n    Xt = X\n    for _, _, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_cumulative_hazard_function(Xt, **kwargs)",
        "import_statements": [
            "from importlib.metadata import PackageNotFoundError, version",
            "import platform",
            "import sys",
            "from sklearn.pipeline import Pipeline, _final_estimator_has",
            "from sklearn.utils.metaestimators import available_if"
        ],
        "reference_api": [
            "predict_cumulative_hazard_function",
            "transform.transform",
            "self._iter"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self._iter",
            "transform.transform",
            "predict_cumulative_hazard_function"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/__init__.py",
        "function_declaration": "def predict_survival_function(self, X, **kwargs)",
        "start_line": "92",
        "end_line": "118",
        "docstring": "This function predicts the survival function for input data X.\\nIt iteratively applies a series of transformations to X using a pipeline of steps, excluding the final step.\\nAfter transforming the data, it uses the final step in the pipeline to predict the survival function with the transformed data and any additional keyword arguments.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c8439e4918ad",
        "ground_truth": "def predict_survival_function(self, X, **kwargs):\n    \"\"\"Predict survival function.\n     The survival function for an individual\n    with feature vector :math:`x` is defined as\n     .. math::\n         S(t \\\\mid x) = S_0(t)^{\\\\exp(x^\\\\top \\\\beta)} ,\n     where :math:`S_0(t)` is the baseline survival function,\n    estimated by Breslow's estimator.\n     Parameters\n    ----------\n    X : array-like, shape = (n_samples, n_features)\n        Data matrix.\n     Returns\n    -------\n    survival : ndarray, shape = (n_samples,)\n        Predicted survival functions.\n    \"\"\"\n    Xt = X\n    for _, _, transform in self._iter(with_final=False):\n        Xt = transform.transform(Xt)\n    return self.steps[-1][-1].predict_survival_function(Xt, **kwargs)",
        "import_statements": [
            "from importlib.metadata import PackageNotFoundError, version",
            "import platform",
            "import sys",
            "from sklearn.pipeline import Pipeline, _final_estimator_has",
            "from sklearn.utils.metaestimators import available_if"
        ],
        "reference_api": [
            "predict_survival_function",
            "transform.transform",
            "self._iter"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self._iter",
            "transform.transform",
            "predict_survival_function"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/svm/minlip.py",
        "function_declaration": "def _check_success(self, results)",
        "start_line": "145",
        "end_line": "159",
        "docstring": "The function _check_success evaluates the exit flag from solver results to determine the outcome of an optimization problem. If the exit flag indicates an optimal solution or an acceptable level of inaccuracy, the function completes without action. If the maximum iterations are reached, it issues a convergence warning. For exit flags indicating primal or dual infeasibility, it raises a RuntimeError. For any other unknown exit flags, it also raises a RuntimeError with the exit status.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "216662ca7db4",
        "ground_truth": "def _check_success(self, results):  # pylint: disable=no-self-use\n    exit_flag = results[\"info\"][\"exitFlag\"]\n    if exit_flag in (EcosSolver.EXIT_OPTIMAL, EcosSolver.EXIT_OPTIMAL + EcosSolver.EXIT_INACC_OFFSET):\n        return\n    if exit_flag == EcosSolver.EXIT_MAXIT:\n        warnings.warn(\n            \"ECOS solver did not converge: maximum iterations reached\", category=ConvergenceWarning, stacklevel=3\n        )\n    elif exit_flag == EcosSolver.EXIT_PINF:  # pragma: no cover\n        raise RuntimeError(\"Certificate of primal infeasibility found\")\n    elif exit_flag == EcosSolver.EXIT_DINF:  # pragma: no cover\n        raise RuntimeError(\"Certificate of dual infeasibility found\")\n    else:  # pragma: no cover\n        raise RuntimeError(f\"Unknown problem in ECOS solver, exit status: {exit_flag}\")",
        "import_statements": [
            "from abc import ABCMeta, abstractmethod",
            "import numbers",
            "import warnings",
            "from scipy import linalg, sparse",
            "from sklearn.base import BaseEstimator",
            "from sklearn.exceptions import ConvergenceWarning",
            "from sklearn.metrics.pairwise import PAIRWISE_KERNEL_FUNCTIONS, pairwise_kernels",
            "from sklearn.utils._param_validation import Interval, StrOptions"
        ],
        "reference_api": [
            "RuntimeError",
            "warnings.warn"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/svm/minlip.py",
        "function_declaration": "def _decompose(self, P)",
        "start_line": "161",
        "end_line": "180",
        "docstring": "This function performs matrix decomposition on matrix P using its eigenvalues and eigenvectors.\\nIt computes the eigenvalues and eigenvectors using the linalg.eigh function from scipy.linalg.\\nIt identifies the largest eigenvalue and calculates a condition number based on this eigenvalue, the shape of P, and machine epsilon.\\nThe function checks if all eigenvalues are above the negative cutoff and filters out those below the positive cutoff.\\nIt normalizes the eigenvalues so that the maximum eigenvalue is set to 1.\\nFinally, it returns the transposed decomposed matrix and the largest eigenvalue.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1fe2e379fca0",
        "ground_truth": "def _decompose(self, P):\n    # from scipy.linalg.pinvh\n    s, u = linalg.eigh(P)\n    largest_eigenvalue = np.max(np.abs(s))\n    cond = self.cond\n    if cond is None:\n        t = u.dtype\n        cond = largest_eigenvalue * max(P.shape) * np.finfo(t).eps\n    not_below_cutoff = abs(s) > -cond\n    assert not_below_cutoff.all(), f\"matrix has negative eigenvalues: {s.min()}\"\n    above_cutoff = abs(s) > cond\n    u = u[:, above_cutoff]\n    s = s[above_cutoff]\n    # set maximum eigenvalue to 1\n    decomposed = u * np.sqrt(s / largest_eigenvalue)\n    return decomposed.T, largest_eigenvalue",
        "import_statements": [
            "from abc import ABCMeta, abstractmethod",
            "import numbers",
            "import warnings",
            "from scipy import linalg, sparse",
            "from sklearn.base import BaseEstimator",
            "from sklearn.exceptions import ConvergenceWarning",
            "from sklearn.metrics.pairwise import PAIRWISE_KERNEL_FUNCTIONS, pairwise_kernels",
            "from sklearn.utils._param_validation import Interval, StrOptions"
        ],
        "reference_api": [
            "np.abs",
            "np.finfo",
            "abs",
            "not_below_cutoff.all",
            "max",
            "np.max",
            "s.min",
            "np.sqrt",
            "linalg.eigh"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "linalg.eigh",
            "np.max",
            "np.abs",
            "np.finfo",
            "not_below_cutoff.all",
            "s.min",
            "np.sqrt"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/svm/minlip.py",
        "function_declaration": "def _update_coef(self, coef, D)",
        "start_line": "604",
        "end_line": "606",
        "docstring": "The function _update_coef updates the model's coefficients by selecting elements of the input coefficient array that are greater than a small threshold and then scaling these selected coefficients by corresponding elements in another array. The resulting product is stored in the model's coefficient attribute.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "20c3f5d05d79",
        "ground_truth": "def _update_coef(self, coef, D):\n    sv = np.flatnonzero(coef > 1e-5)\n    self.coef_ = coef[:, sv] * D[sv, :]",
        "import_statements": [
            "from abc import ABCMeta, abstractmethod",
            "import numbers",
            "import warnings",
            "from scipy import linalg, sparse",
            "from sklearn.base import BaseEstimator",
            "from sklearn.exceptions import ConvergenceWarning",
            "from sklearn.metrics.pairwise import PAIRWISE_KERNEL_FUNCTIONS, pairwise_kernels",
            "from sklearn.utils._param_validation import Interval, StrOptions"
        ],
        "reference_api": [
            "np.flatnonzero"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.flatnonzero"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/ensemble/survival_loss.py",
        "function_declaration": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs)",
        "start_line": "41",
        "end_line": "54",
        "docstring": "This function computes the gradient for a Cox proportional hazards model.\\nIt takes true event data, raw predictions, and optional sample weights.\\nThe function calculates the negative gradient of the Cox PH model using event type and time.\\nIf sample weights are provided, it applies them to the computed gradient.\\nFinally, it returns the weighted or unweighted gradient.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "37c347d00d35",
        "ground_truth": "def gradient(self, y_true, raw_prediction, sample_weight=None, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Negative gradient of partial likelihood\n    Parameters\n    ---------\n    y : tuple, len = 2\n        First element is boolean event indicator and second element survival/censoring time.\n    y_pred : np.ndarray, shape=(n,):\n        The predictions.\n    \"\"\"\n    ret = coxph_negative_gradient(y_true[\"event\"].astype(np.uint8), y_true[\"time\"], raw_prediction.ravel())\n    if sample_weight is not None:\n        ret *= sample_weight\n    return ret",
        "import_statements": [
            "from abc import ABCMeta",
            "from sklearn._loss.link import IdentityLink",
            "from sklearn._loss.loss import BaseLoss",
            "from sklearn.utils.extmath import squared_norm"
        ],
        "reference_api": [
            "astype",
            "raw_prediction.ravel",
            "coxph_negative_gradient"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "coxph_negative_gradient",
            "astype",
            "raw_prediction.ravel"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/tree/tree.py",
        "function_declaration": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None)",
        "start_line": "212",
        "end_line": "250",
        "docstring": "The function _compute_missing_values_in_feature_mask checks for missing values in a feature matrix X. It takes an optional estimator name and sets default parameters. If the dataset does not support missing values, it verifies that all elements are finite and returns None if true. It calculates the overall sum of X while ignoring overflow errors. If the sum is not finite, it raises a ValueError for any infinite elements. If the sum is finite and not NaN, it returns None, indicating no missing values. If there are NaN values, it returns a mask indicating which features have missing values.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6be432d27e52",
        "ground_truth": "def _compute_missing_values_in_feature_mask(self, X, estimator_name=None):\n    \"\"\"Return boolean mask denoting if there are missing values for each feature.\n    This method also ensures that X is finite.\n    Parameter\n    ---------\n    X : array-like of shape (n_samples, n_features), dtype=DOUBLE\n        Input data.\n    estimator_name : str or None, default=None\n        Name to use when raising an error. Defaults to the class name.\n    Returns\n    -------\n    missing_values_in_feature_mask : ndarray of shape (n_features,), or None\n        Missing value mask. If missing values are not supported or there\n        are no missing values, return None.\n    \"\"\"\n    estimator_name = estimator_name or self.__class__.__name__\n    common_kwargs = dict(estimator_name=estimator_name, input_name=\"X\")\n    if not self._support_missing_values(X):\n        assert_all_finite(X, **common_kwargs)\n        return None\n    with np.errstate(over=\"ignore\"):\n        overall_sum = np.sum(X)\n    if not np.isfinite(overall_sum):\n        # Raise a ValueError in case of the presence of an infinite element.\n        _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n    # If the sum is not nan, then there are no missing values\n    if not np.isnan(overall_sum):\n        return None\n    missing_values_in_feature_mask = _any_isnan_axis0(X)\n    return missing_values_in_feature_mask",
        "import_statements": [
            "from math import ceil",
            "from numbers import Integral, Real",
            "from scipy.sparse import issparse",
            "from sklearn.base import BaseEstimator",
            "from sklearn.tree import _tree",
            "from sklearn.tree._classes import DENSE_SPLITTERS, SPARSE_SPLITTERS",
            "from sklearn.tree._splitter import Splitter",
            "from sklearn.tree._tree import BestFirstTreeBuilder, DepthFirstTreeBuilder, Tree",
            "from sklearn.tree._utils import _any_isnan_axis0",
            "from sklearn.utils._param_validation import Interval, StrOptions",
            "from sklearn.utils.validation import (\n    _assert_all_finite_element_wise,\n    assert_all_finite,\n    check_is_fitted,\n    check_random_state,\n)"
        ],
        "reference_api": [
            "np.isfinite",
            "self._support_missing_values",
            "assert_all_finite",
            "np.isnan",
            "np.sum",
            "dict",
            "_assert_all_finite_element_wise",
            "np.errstate",
            "_any_isnan_axis0"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._support_missing_values",
                "code": "upport_missing_values(self, X):\n        return not issparse(X) and self._get_tags()[\"allow_nan\"]\n\n    "
            }
        ],
        "third_party": [
            "assert_all_finite",
            "np.errstate",
            "np.sum",
            "np.isfinite",
            "_assert_all_finite_element_wise",
            "np.isnan",
            "_any_isnan_axis0"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/tree/tree.py",
        "function_declaration": "def predict_survival_function(self, X, check_input=True, return_array=False)",
        "start_line": "552",
        "end_line": "621",
        "docstring": "This function predicts a survival function based on the input data X.\\nIt first checks the model's memory and verifies that the model is fitted.\\nThe input data X is validated and possibly converted to a sparse matrix.\\nThe function then uses a decision tree to make predictions, extracting the second element from the predictions.\\nIf return_array is True, it returns this array of predictions directly.\\nOtherwise, it converts the array to a step function based on unique times and returns the step function.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "dc5bda895a07",
        "ground_truth": "def predict_survival_function(self, X, check_input=True, return_array=False):\n    \"\"\"Predict survival function.\n    The survival function for an individual\n    with feature vector :math:`x` is computed from\n    all samples of the training data that are in the\n    same terminal node as :math:`x`.\n    It is estimated by the Kaplan-Meier estimator.\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape = (n_samples, n_features)\n        Data matrix.\n        If ``splitter='best'``, `X` is allowed to contain missing\n        values and decisions are made as described in\n        :ref:`tree_missing_value_support`.\n    check_input : boolean, default: True\n        Allow to bypass several input checking.\n        Don't use this parameter unless you know what you do.\n    return_array : boolean, default: False\n        If set, return an array with the probability\n        of survival for each `self.unique_times_`,\n        otherwise an array of :class:`sksurv.functions.StepFunction`.\n    Returns\n    -------\n    survival : ndarray\n        If `return_array` is set, an array with the probability of\n        survival for each `self.unique_times_`, otherwise an array of\n        length `n_samples` of :class:`sksurv.functions.StepFunction`\n        instances will be returned.\n    Examples\n    --------\n    >>> import matplotlib.pyplot as plt\n    >>> from sksurv.datasets import load_whas500\n    >>> from sksurv.tree import SurvivalTree\n    Load and prepare the data.\n    >>> X, y = load_whas500()\n    >>> X = X.astype(float)\n    Fit the model.\n    >>> estimator = SurvivalTree().fit(X, y)\n    Estimate the survival function for the first 5 samples.\n    >>> surv_funcs = estimator.predict_survival_function(X.iloc[:5])\n    Plot the estimated survival functions.\n    >>> for fn in surv_funcs:\n    ...    plt.step(fn.x, fn(fn.x), where=\"post\")\n    ...\n    >>> plt.ylim(0, 1)\n    >>> plt.show()\n    \"\"\"\n    self._check_low_memory(\"predict_survival_function\")\n    check_is_fitted(self, \"tree_\")\n    X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n    pred = self.tree_.predict(X)\n    arr = pred[..., 1]\n    if return_array:\n        return arr\n    return _array_to_step_function(self.unique_times_, arr)",
        "import_statements": [
            "from math import ceil",
            "from numbers import Integral, Real",
            "from scipy.sparse import issparse",
            "from sklearn.base import BaseEstimator",
            "from sklearn.tree import _tree",
            "from sklearn.tree._classes import DENSE_SPLITTERS, SPARSE_SPLITTERS",
            "from sklearn.tree._splitter import Splitter",
            "from sklearn.tree._tree import BestFirstTreeBuilder, DepthFirstTreeBuilder, Tree",
            "from sklearn.tree._utils import _any_isnan_axis0",
            "from sklearn.utils._param_validation import Interval, StrOptions",
            "from sklearn.utils.validation import (\n    _assert_all_finite_element_wise,\n    assert_all_finite,\n    check_is_fitted,\n    check_random_state,\n)"
        ],
        "reference_api": [
            "predict",
            "self._validate_X_predict",
            "self._check_low_memory",
            "_array_to_step_function",
            "check_is_fitted"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._check_low_memory",
                "code": "heck_low_memory(self, function):\n        \"\"\"Check if `function` is supported in low memory mode and throw if it is not.\"\"\"\n        if self.low_memory:\n            raise NotImplementedError(\n                f\"{function} is not implemented in low memory mode.\"\n                + \" run fit with low_memory=False to disable low memory mode.\"\n            )\n\n    "
            },
            {
                "name": "self._validate_X_predict",
                "code": "alidate_X_predict(self, X, check_input, accept_sparse=\"csr\"):\n        \"\"\"Validate X whenever one tries to predict\"\"\"\n        if check_input:\n            if self._support_missing_values(X):\n                force_all_finite = \"allow-nan\"\n            else:\n                force_all_finite = True\n            X = self._validate_data(\n                X,\n                dtype=DTYPE,\n                accept_sparse=accept_sparse,\n                reset=False,\n                force_all_finite=force_all_finite,\n            )\n        else:\n            # The number of features is checked regardless of `check_input`\n            self._check_n_features(X, reset=False)\n\n        return X\n\n    "
            },
            {
                "name": "predict",
                "code": "edict(self, X, check_input=True):\n        \"\"\"Predict risk score.\n\n        The risk score is the total number of events, which can\n        be estimated by the sum of the estimated cumulative\n        hazard function :math:`\\\\hat{H}_h` in terminal node :math:`h`.\n\n        .. math::\n\n            \\\\sum_{j=1}^{n(h)} \\\\hat{H}_h(T_{j} \\\\mid x) ,\n\n        where :math:`n(h)` denotes the number of distinct event times\n        of samples belonging to the same terminal node as :math:`x`.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape = (n_samples, n_features)\n            Data matrix.\n            If ``splitter='best'``, `X` is allowed to contain missing\n            values and decisions are made as described in\n            :ref:`tree_missing_value_support`.\n\n        check_input : boolean, default: True\n            Allow to bypass several input checking.\n            Don't use this parameter unless you know what you do.\n\n        Returns\n        -------\n        risk_scores : ndarray, shape = (n_samples,)\n            Predicted risk scores.\n        \"\"\"\n\n        if self.low_memory:\n            check_is_fitted(self, \"tree_\")\n            X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n            pred = self.tree_.predict(X)\n            return pred[..., 0]\n\n        chf = self.predict_cumulative_hazard_function(X, check_input, return_array=True)\n        return chf[:, self.is_event_time_].sum(1)\n\n    "
            },
            {
                "name": "_array_to_step_function",
                "code": "def _array_to_step_function(x, array):\n    n_samples = array.shape[0]\n    funcs = np.empty(n_samples, dtype=np.object_)\n    for i in range(n_samples):\n        funcs[i] = StepFunction(x=x, y=array[i])\n    return funcs"
            }
        ],
        "third_party": [
            "check_is_fitted"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/linear_model/coxnet.py",
        "function_declaration": "def _pre_fit(self, X, y)",
        "start_line": "180",
        "end_line": "196",
        "docstring": "The function _pre_fit preprocesses input data for survival analysis. It validates the input data, ensuring a minimum number of samples and converting the data type to float64. It then separates survival events and times, centers the feature matrix by subtracting the mean, and optionally normalizes the data. The function sorts the data in descending order based on survival time and converts the sorted data to Fortran-contiguous arrays. It returns the processed feature matrix, event numbers, survival times, feature means, and scales.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "84ae915b5933",
        "ground_truth": "def _pre_fit(self, X, y):\n    X = self._validate_data(X, ensure_min_samples=2, dtype=np.float64, copy=self.copy_X)\n    event, time = check_array_survival(X, y)\n    # center feature matrix\n    X_offset = np.average(X, axis=0)\n    X -= X_offset\n    if self.normalize:\n        X, X_scale = f_normalize(X, copy=False, axis=0, return_norm=True)\n    else:\n        X_scale = np.ones(X.shape[1], dtype=X.dtype)\n    # sort descending\n    o = np.argsort(-time, kind=\"mergesort\")\n    X = np.asfortranarray(X[o, :])\n    event_num = event[o].astype(np.uint8)\n    time = time[o].astype(np.float64)\n    return X, event_num, time, X_offset, X_scale",
        "import_statements": [
            "import numbers",
            "import warnings",
            "from sklearn.base import BaseEstimator",
            "from sklearn.exceptions import ConvergenceWarning",
            "from sklearn.preprocessing import normalize as f_normalize",
            "from sklearn.utils._param_validation import Interval, StrOptions",
            "from sklearn.utils.validation import assert_all_finite, check_is_fitted, check_non_negative, column_or_1d"
        ],
        "reference_api": [
            "np.average",
            "np.ones",
            "astype",
            "check_array_survival",
            "np.asfortranarray",
            "np.argsort",
            "self._validate_data",
            "f_normalize"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self._validate_data",
            "check_array_survival",
            "np.average",
            "f_normalize",
            "np.ones",
            "np.argsort",
            "np.asfortranarray",
            "astype",
            "astype"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/linear_model/coxnet.py",
        "function_declaration": "def _check_alphas(self)",
        "start_line": "212",
        "end_line": "223",
        "docstring": "This function checks and prepares the alpha values used for a calculation.\\nIt determines if a new alpha path needs to be created based on whether the alphas attribute is None.\\nIf a new path is required, it validates that the number of alphas is a positive integer and initializes an array of the specified length.\\nIf an existing path is used, it ensures the alpha values are in a 1D array, are finite, and non-negative.\\nThe function returns the alpha values and a boolean indicating if a new path was created.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c70bb013f85f",
        "ground_truth": "def _check_alphas(self):\n    create_path = self.alphas is None\n    if create_path:\n        if self.n_alphas <= 0:\n            raise ValueError(\"n_alphas must be a positive integer\")\n        alphas = np.empty(int(self.n_alphas), dtype=np.float64)\n    else:\n        alphas = column_or_1d(self.alphas, warn=True)\n        assert_all_finite(alphas, input_name=\"alphas\")\n        check_non_negative(alphas, \"alphas\")\n    return alphas, create_path",
        "import_statements": [
            "import numbers",
            "import warnings",
            "from sklearn.base import BaseEstimator",
            "from sklearn.exceptions import ConvergenceWarning",
            "from sklearn.preprocessing import normalize as f_normalize",
            "from sklearn.utils._param_validation import Interval, StrOptions",
            "from sklearn.utils.validation import assert_all_finite, check_is_fitted, check_non_negative, column_or_1d"
        ],
        "reference_api": [
            "assert_all_finite",
            "ValueError",
            "int",
            "column_or_1d",
            "check_non_negative",
            "np.empty"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.empty",
            "column_or_1d",
            "assert_all_finite",
            "check_non_negative"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/nonparametric.py",
        "function_declaration": "def nelson_aalen_estimator(event, time)",
        "start_line": "340",
        "end_line": "375",
        "docstring": "The function nelson_aalen_estimator computes the Nelson-Aalen estimator for cumulative hazard rates. It takes event indicators and corresponding time values as inputs, validates them, and ensures they have consistent lengths. It calculates unique times, the number of events, and the number of subjects at risk at each time point. The function then computes the cumulative hazard by summing the ratio of events to subjects at risk over time. It returns the unique times and the cumulative hazard values.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7b54a0554ba4",
        "ground_truth": "def nelson_aalen_estimator(event, time):\n    \"\"\"Nelson-Aalen estimator of cumulative hazard function.\n     See [1]_, [2]_ for further description.\n     Parameters\n    ----------\n    event : array-like, shape = (n_samples,)\n        Contains binary event indicators.\n     time : array-like, shape = (n_samples,)\n        Contains event/censoring times.\n     Returns\n    -------\n    time : array, shape = (n_times,)\n        Unique times.\n     cum_hazard : array, shape = (n_times,)\n        Cumulative hazard at each unique time point.\n     References\n    ----------\n    .. [1] Nelson, W., \"Theory and applications of hazard plotting for censored failure data\",\n           Technometrics, vol. 14, pp. 945-965, 1972.\n     .. [2] Aalen, O. O., \"Nonparametric inference for a family of counting processes\",\n           Annals of Statistics, vol. 6, pp. 701\u2013726, 1978.\n    \"\"\"\n    event, time = check_y_survival(event, time)\n    check_consistent_length(event, time)\n    uniq_times, n_events, n_at_risk, _ = _compute_counts(event, time)\n     y = np.cumsum(n_events / n_at_risk)\n     return uniq_times, y",
        "import_statements": [
            "import numbers",
            "from scipy import stats",
            "from sklearn.base import BaseEstimator",
            "from sklearn.utils._param_validation import Interval, StrOptions",
            "from sklearn.utils.validation import check_array, check_consistent_length, check_is_fitted"
        ],
        "reference_api": [
            "check_y_survival",
            "check_consistent_length",
            "np.cumsum",
            "_compute_counts"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_compute_counts",
                "code": "def _compute_counts(event, time, order=None):\n    \"\"\"Count right censored and uncensored samples at each unique time point.\n\n    Parameters\n    ----------\n    event : array\n        Boolean event indicator.\n\n    time : array\n        Survival time or time of censoring.\n\n    order : array or None\n        Indices to order time in ascending order.\n        If None, order will be computed.\n\n    Returns\n    -------\n    times : array\n        Unique time points.\n\n    n_events : array\n        Number of events at each time point.\n\n    n_at_risk : array\n        Number of samples that have not been censored or have not had an event at each time point.\n\n    n_censored : array\n        Number of censored samples at each time point.\n    \"\"\"\n    n_samples = event.shape[0]\n\n    if order is None:\n        order = np.argsort(time, kind=\"mergesort\")\n\n    uniq_times = np.empty(n_samples, dtype=time.dtype)\n    uniq_events = np.empty(n_samples, dtype=int)\n    uniq_counts = np.empty(n_samples, dtype=int)\n\n    i = 0\n    prev_val = time[order[0]]\n    j = 0\n    while True:\n        count_event = 0\n        count = 0\n        while i < n_samples and prev_val == time[order[i]]:\n            if event[order[i]]:\n                count_event += 1\n\n            count += 1\n            i += 1\n\n        uniq_times[j] = prev_val\n        uniq_events[j] = count_event\n        uniq_counts[j] = count\n        j += 1\n\n        if i == n_samples:\n            break\n\n        prev_val = time[order[i]]\n\n    times = np.resize(uniq_times, j)\n    n_events = np.resize(uniq_events, j)\n    total_count = np.resize(uniq_counts, j)\n    n_censored = total_count - n_events\n\n    # offset cumulative sum by one\n    total_count = np.r_[0, total_count]\n    n_at_risk = n_samples - np.cumsum(total_count)\n\n    return times, n_events, n_at_risk[:-1], n_censored"
            }
        ],
        "third_party": [
            "check_y_survival",
            "check_consistent_length",
            "np.cumsum"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sebp/scikit-survival",
        "file_path": "sksurv/nonparametric.py",
        "function_declaration": "def ipc_weights(event, time)",
        "start_line": "378",
        "end_line": "413",
        "docstring": "This function computes inverse probability of censoring (IPC) weights for survival analysis.\\nIt first checks if all events have occurred, returning an array of ones if true.\\nOtherwise, it calculates the Kaplan-Meier estimator for the event and time data in reverse.\\nIt then finds the index positions of event times within the unique time values and retrieves the corresponding survival probabilities.\\nThe function ensures all retrieved probabilities are positive.\\nFinally, it creates a weights array, assigning inverse probabilities to events and returns the weights.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "35af806300cc",
        "ground_truth": "def ipc_weights(event, time):\n    \"\"\"Compute inverse probability of censoring weights\n     Parameters\n    ----------\n    event : array, shape = (n_samples,)\n        Boolean event indicator.\n     time : array, shape = (n_samples,)\n        Time when a subject experienced an event or was censored.\n     Returns\n    -------\n    weights : array, shape = (n_samples,)\n        inverse probability of censoring weights\n     See also\n    --------\n    CensoringDistributionEstimator\n        An estimator interface for estimating inverse probability\n        of censoring weights for unseen time points.\n    \"\"\"\n    if event.all():\n        return np.ones(time.shape[0])\n     unique_time, p = kaplan_meier_estimator(event, time, reverse=True)\n     idx = np.searchsorted(unique_time, time[event])\n    Ghat = p[idx]\n     assert (Ghat > 0).all()\n     weights = np.zeros(time.shape[0])\n    weights[event] = 1.0 / Ghat\n     return weights",
        "import_statements": [
            "import numbers",
            "from scipy import stats",
            "from sklearn.base import BaseEstimator",
            "from sklearn.utils._param_validation import Interval, StrOptions",
            "from sklearn.utils.validation import check_array, check_consistent_length, check_is_fitted"
        ],
        "reference_api": [
            "event.all",
            "np.ones",
            "kaplan_meier_estimator",
            "np.zeros",
            "all",
            "np.searchsorted"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "kaplan_meier_estimator",
                "code": "def kaplan_meier_estimator(\n    event,\n    time_exit,\n    time_enter=None,\n    time_min=None,\n    reverse=False,\n    conf_level=0.95,\n    conf_type=None,\n):\n    \"\"\"Kaplan-Meier estimator of survival function.\n\n    See [1]_ for further description.\n\n    Parameters\n    ----------\n    event : array-like, shape = (n_samples,)\n        Contains binary event indicators.\n\n    time_exit : array-like, shape = (n_samples,)\n        Contains event/censoring times.\n\n    time_enter : array-like, shape = (n_samples,), optional\n        Contains time when each individual entered the study for\n        left truncated survival data.\n\n    time_min : float, optional\n        Compute estimator conditional on survival at least up to\n        the specified time.\n\n    reverse : bool, optional, default: False\n        Whether to estimate the censoring distribution.\n        When there are ties between times at which events are observed,\n        then events come first and are subtracted from the denominator.\n        Only available for right-censored data, i.e. `time_enter` must\n        be None.\n\n    conf_level : float, optional, default: 0.95\n        The level for a two-sided confidence interval on the survival curves.\n\n    conf_type : None or {'log-log'}, optional, default: None.\n        The type of confidence intervals to estimate.\n        If `None`, no confidence intervals are estimated.\n        If \"log-log\", estimate confidence intervals using\n        the log hazard or :math:`log(-log(S(t)))` as described in [2]_.\n\n    Returns\n    -------\n    time : array, shape = (n_times,)\n        Unique times.\n\n    prob_survival : array, shape = (n_times,)\n        Survival probability at each unique time point.\n        If `time_enter` is provided, estimates are conditional probabilities.\n\n    conf_int : array, shape = (2, n_times)\n        Pointwise confidence interval of the Kaplan-Meier estimator\n        at each unique time point.\n        Only provided if `conf_type` is not None.\n\n    Examples\n    --------\n    Creating a Kaplan-Meier curve:\n\n    >>> x, y, conf_int = kaplan_meier_estimator(event, time, conf_type=\"log-log\")\n    >>> plt.step(x, y, where=\"post\")\n    >>> plt.fill_between(x, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n    >>> plt.ylim(0, 1)\n    >>> plt.show()\n\n    See also\n    --------\n    sksurv.nonparametric.SurvivalFunctionEstimator\n        Estimator API of the Kaplan-Meier estimator.\n\n    References\n    ----------\n    .. [1] Kaplan, E. L. and Meier, P., \"Nonparametric estimation from incomplete observations\",\n           Journal of The American Statistical Association, vol. 53, pp. 457-481, 1958.\n    .. [2] Borgan \u00d8. and Liest\u00f8l K., \"A Note on Confidence Intervals and Bands for the\n           Survival Function Based on Transformations\", Scandinavian Journal of\n           Statistics. 1990;17(1):35\u201341.\n    \"\"\"\n    event, time_enter, time_exit = check_y_survival(event, time_enter, time_exit, allow_all_censored=True)\n    check_consistent_length(event, time_enter, time_exit)\n\n    if conf_type is not None and reverse:\n        raise NotImplementedError(\"Confidence intervals of the censoring distribution is not implemented.\")\n\n    if time_enter is None:\n        uniq_times, n_events, n_at_risk, n_censored = _compute_counts(event, time_exit)\n\n        if reverse:\n            n_at_risk -= n_events\n            n_events = n_censored\n    else:\n        if reverse:\n            raise ValueError(\"The censoring distribution cannot be estimated from left truncated data\")\n\n        uniq_times, n_events, n_at_risk = _compute_counts_truncated(event, time_enter, time_exit)\n\n    # account for 0/0 = nan\n    ratio = np.divide(\n        n_events,\n        n_at_risk,\n        out=np.zeros(uniq_times.shape[0], dtype=float),\n        where=n_events != 0,\n    )\n    values = 1.0 - ratio\n\n    if conf_type is not None:\n        ratio_var = np.divide(\n            n_events,\n            n_at_risk * (n_at_risk - n_events),\n            out=np.zeros(uniq_times.shape[0], dtype=float),\n            where=(n_events != 0) & (n_at_risk != n_events),\n        )\n\n    if time_min is not None:\n        mask = uniq_times >= time_min\n        uniq_times = np.compress(mask, uniq_times)\n        values = np.compress(mask, values)\n\n    prob_survival = np.cumprod(values)\n\n    if conf_type is None:\n        return uniq_times, prob_survival\n\n    if time_min is not None:\n        ratio_var = np.compress(mask, ratio_var)\n\n    ci = _km_ci_estimator(prob_survival, ratio_var, conf_level, conf_type)\n\n    return uniq_times, prob_survival, ci\n\n\nd"
            }
        ],
        "third_party": [
            "event.all",
            "np.ones",
            "np.searchsorted",
            "np.zeros"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "Sinaptik-AI/pandas-ai",
        "file_path": "pandasai/agent/base.py",
        "function_declaration": "def get_dfs(\n        self,\n        dfs: Union[\n            pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n        ],\n    )",
        "start_line": "158",
        "end_line": "206",
        "docstring": "This function converts various types of data inputs into a list of connector objects.\\nIt accepts single or multiple dataframes, connectors, or other data structures.\\nIf a single dataframe is provided, it converts it into a list.\\nIt iterates over the list and checks the type of each item.\\nDepending on the type, it creates appropriate connector objects.\\nIt handles pandas, modin, and polars dataframes, as well as custom connectors.\\nIf the input type is invalid or cannot be converted, it raises a ValueError.\\nFinally, it returns a list of connector objects.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "72b19985ea11",
        "ground_truth": "def get_dfs(\n    self,\n    dfs: Union[\n        pd.DataFrame, BaseConnector, List[Union[pd.DataFrame, BaseConnector]]\n    ],\n):\n    \"\"\"\n    Load all the dataframes to be used in the agent.\n    Args:\n        dfs (List[Union[pd.DataFrame, Any]]): Pandas dataframe\n    \"\"\"\n    # Inline import to avoid circular import\n    from pandasai.smart_dataframe import SmartDataframe\n    # If only one dataframe is passed, convert it to a list\n    if not isinstance(dfs, list):\n        dfs = [dfs]\n    connectors = []\n    for df in dfs:\n        if isinstance(df, BaseConnector):\n            connectors.append(df)\n        elif isinstance(df, (pd.DataFrame, pd.Series, list, dict, str)):\n            connectors.append(PandasConnector({\"original_df\": df}))\n        elif df_type(df) == \"modin\":\n            connectors.append(PandasConnector({\"original_df\": df}))\n        elif isinstance(df, SmartDataframe) and isinstance(\n            df.dataframe, BaseConnector\n        ):\n            connectors.append(df.dataframe)\n        else:\n            try:\n                import polars as pl\n                if isinstance(df, pl.DataFrame):\n                    from ..connectors.polars import PolarsConnector\n                    connectors.append(PolarsConnector({\"original_df\": df}))\n                else:\n                    raise ValueError(\n                        \"Invalid input data. We cannot convert it to a dataframe.\"\n                    )\n            except ImportError as e:\n                raise ValueError(\n                    \"Invalid input data. We cannot convert it to a dataframe.\"\n                ) from e\n    return connectors",
        "import_statements": [
            "import json",
            "import os",
            "import uuid",
            "from typing import List, Optional, Union",
            "from pandasai.agent.base_security import BaseSecurity",
            "from pandasai.llm.bamboo_llm import BambooLLM",
            "from pandasai.pipelines.chat.chat_pipeline_input import ChatPipelineInput",
            "from pandasai.pipelines.chat.code_execution_pipeline_input import (\n    CodeExecutionPipelineInput,\n)",
            "from pandasai.vectorstores.vectorstore import VectorStore"
        ],
        "reference_api": [
            "PandasConnector",
            "df_type",
            "ValueError",
            "isinstance",
            "PolarsConnector",
            "connectors.append"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "connectors.append",
            "connectors.append",
            "PandasConnector",
            "df_type",
            "connectors.append",
            "PandasConnector",
            "connectors.append",
            "connectors.append",
            "PolarsConnector"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "Sinaptik-AI/pandas-ai",
        "file_path": "pandasai/connectors/airtable.py",
        "function_declaration": "def execute(self) -> pd.DataFrame",
        "start_line": "153",
        "end_line": "168",
        "docstring": "The function execute returns a pandas DataFrame. It first checks if there is a cached version of the data, either with or without additional filters, and if found, reads and returns it as a DataFrame. If the instance variable is already a DataFrame, it returns this instance. Otherwise, it fetches the data, assigns it to the instance variable, and returns the fetched data as a DataFrame.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cf22fb696e90",
        "ground_truth": "def execute(self) -> pd.DataFrame:\n    \"\"\"\n    Execute the connector and return the result.\n    Returns:\n        pd.DataFrame: The result of the connector.\n    \"\"\"\n    if cached := self._cached() or self._cached(include_additional_filters=True):\n        return pd.read_parquet(cached)\n    if isinstance(self._instance, pd.DataFrame):\n        return self._instance\n    else:\n        self._instance = self._fetch_data()\n    return self._instance",
        "import_statements": [
            "import hashlib",
            "import os",
            "import time",
            "from functools import cache, cached_property",
            "from typing import Optional, Union",
            "import requests"
        ],
        "reference_api": [
            "self._cached",
            "pd.read_parquet",
            "self._fetch_data",
            "isinstance"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._cached",
                "code": "def _cached(self, include_additional_filters: bool = False):\n        \"\"\"\n        Returns the cached Airtable data if it exists and\n        is not older than the cache interval.\n\n        Returns :\n            DataFrame | None : The cached data if\n                it exists and is not older than the cache\n                interval, None otherwise.\n        \"\"\"\n        cache_path = self._get_cache_path(include_additional_filters)\n        if not os.path.exists(cache_path):\n            return None\n\n        # If the file is older than 1 day , delete it.\n        if os.path.getmtime(cache_path) < time.time() - self._cache_interval:\n            if self.logger:\n                self.logger.log(f\"Deleting expired cached data from {cache_path}\")\n            os.remove(cache_path)\n            return None\n\n        if self.logger:\n            self.logger.log(f\"Loading cached data from {cache_path}\")\n\n        return cache_path"
            },
            {
                "name": "self._cached",
                "code": "def _cached(self, include_additional_filters: bool = False):\n        \"\"\"\n        Returns the cached Airtable data if it exists and\n        is not older than the cache interval.\n\n        Returns :\n            DataFrame | None : The cached data if\n                it exists and is not older than the cache\n                interval, None otherwise.\n        \"\"\"\n        cache_path = self._get_cache_path(include_additional_filters)\n        if not os.path.exists(cache_path):\n            return None\n\n        # If the file is older than 1 day , delete it.\n        if os.path.getmtime(cache_path) < time.time() - self._cache_interval:\n            if self.logger:\n                self.logger.log(f\"Deleting expired cached data from {cache_path}\")\n            os.remove(cache_path)\n            return None\n\n        if self.logger:\n            self.logger.log(f\"Loading cached data from {cache_path}\")\n\n        return cache_path"
            },
            {
                "name": "self._fetch_data",
                "code": "def _fetch_data(self):\n        \"\"\"\n        Fetches data from the Airtable server via API and converts it to a DataFrame.\n        \"\"\"\n\n        params = {\"pageSize\": 100, \"offset\": \"0\"}\n\n        if self.config.where is not None:\n            params[\"filterByFormula\"] = self._build_formula()\n\n        data = []\n        while True:\n            response = self._request_api(params=params)\n\n            if response.status_code != 200:\n                raise InvalidRequestError(\n                    f\"Failed to connect to Airtable. \"\n                    f\"Status code: {response.status_code}, \"\n                    f\"message: {response.text}\"\n                )\n\n            res = response.json()\n            records = res.get(\"records\", [])\n            data.extend({\"id\": record[\"id\"], **record[\"fields\"]} for record in records)\n\n            if len(records) < 100 or \"offset\" not in res:\n                break\n\n            params[\"offset\"] = res[\"offset\"]\n\n        return pd.DataFrame(data)"
            }
        ],
        "third_party": [
            "pd.read_parquet"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "Sinaptik-AI/pandas-ai",
        "file_path": "pandasai/connectors/airtable.py",
        "function_declaration": "def column_hash(self)",
        "start_line": "273",
        "end_line": "286",
        "docstring": "This function generates a SHA-256 hash based on the column names of a DataFrame and a formula.\\nIt first checks if the instance is a DataFrame, and if not, executes a method to obtain it.\\nThen, it creates a string of the column names separated by \"|\", appends a formula string to it,\\nand returns the SHA-256 hash of this combined string.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "21df5ecea95f",
        "ground_truth": "def column_hash(self):\n    \"\"\"\n    Return the hash code that is unique to the columns of the data source\n    that the connector is connected to.\n    Returns:\n        int: The hash code that is unique to the columns of the data source\n        that the connector is connected to.\n    \"\"\"\n    if not isinstance(self._instance, pd.DataFrame):\n        self._instance = self.execute()\n    columns_str = \"|\".join(self._instance.columns)\n    columns_str += f\"WHERE{self._build_formula()}\"\n    return hashlib.sha256(columns_str.encode(\"utf-8\")).hexdigest()",
        "import_statements": [
            "import hashlib",
            "import os",
            "import time",
            "from functools import cache, cached_property",
            "from typing import Optional, Union",
            "import requests"
        ],
        "reference_api": [
            "join",
            "self.execute",
            "hexdigest",
            "columns_str.encode",
            "isinstance",
            "hashlib.sha256",
            "self._build_formula"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.execute",
                "code": "def execute(self) -> pd.DataFrame:\n        \"\"\"\n        Execute the connector and return the result.\n\n        Returns:\n            pd.DataFrame: The result of the connector.\n        \"\"\"\n        if cached := self._cached() or self._cached(include_additional_filters=True):\n            return pd.read_parquet(cached)\n\n        if isinstance(self._instance, pd.DataFrame):\n            return self._instance\n        else:\n            self._instance = self._fetch_data()\n\n        return self._instance"
            },
            {
                "name": "self._build_formula",
                "code": "def _build_formula(self):\n        \"\"\"\n        Build Airtable query formula for filtering.\n        \"\"\"\n\n        condition_strings = []\n        if self.config.where is not None:\n            for i in self.config.where:\n                filter_query = f\"{i[0]}{i[1]}'{i[2]}'\"\n                condition_strings.append(filter_query)\n        return f'AND({\",\".join(condition_strings)})'"
            }
        ],
        "third_party": [
            "join",
            "hexdigest",
            "columns_str.encode"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "Sinaptik-AI/pandas-ai",
        "file_path": "pandasai/connectors/pandas.py",
        "function_declaration": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict])",
        "start_line": "58",
        "end_line": "79",
        "docstring": "The function _load_df loads data into a pandas DataFrame based on the input type. It converts a pandas Series to a DataFrame, directly assigns a DataFrame, converts a list or dictionary to a DataFrame, and imports data from a file if the input is a string representing a file path. If the input is of an unsupported type or cannot be converted, it raises a ValueError.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "27ee5f415000",
        "ground_truth": "def _load_df(self, df: Union[pd.DataFrame, pd.Series, str, list, dict]):\n    \"\"\"\n    Load the dataframe from a file or pandas dataframe.\n    Args:\n        df (Union[pd.DataFrame, pd.Series, str, list, dict]): The dataframe to load.\n    \"\"\"\n    if isinstance(df, pd.Series):\n        self.pandas_df = df.to_frame()\n    elif isinstance(df, pd.DataFrame):\n        self.pandas_df = df\n    elif isinstance(df, (list, dict)):\n        try:\n            self.pandas_df = pd.DataFrame(df)\n        except Exception as e:\n            raise ValueError(\n                \"Invalid input data. We cannot convert it to a dataframe.\"\n            ) from e\n    elif isinstance(df, str):\n        self.pandas_df = FileImporter.import_from_file(df)\n    else:\n        raise ValueError(\"Invalid input data. We cannot convert it to a dataframe.\")",
        "import_statements": [
            "import hashlib",
            "from functools import cache, cached_property",
            "from typing import Union",
            "import duckdb",
            "import sqlglot",
            "from pydantic import BaseModel",
            "from pandasai.exceptions import PandasConnectorTableNotFound"
        ],
        "reference_api": [
            "FileImporter.import_from_file",
            "ValueError",
            "isinstance",
            "pd.DataFrame",
            "df.to_frame"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "df.to_frame",
            "pd.DataFrame",
            "FileImporter.import_from_file"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "Sinaptik-AI/pandas-ai",
        "file_path": "pandasai/helpers/data_sampler.py",
        "function_declaration": "def sample(self, n: int = 3) -> pd.DataFrame",
        "start_line": "27",
        "end_line": "47",
        "docstring": "This function generates a sample from a DataFrame with a specified number of rows.\\nIf the DataFrame has fewer rows than the specified number, it returns a shuffled version of the entire DataFrame.\\nOtherwise, it samples each column individually using a custom sampling method and combines these samples into a new DataFrame.\\nThe function then anonymizes the first few rows of the sampled DataFrame before returning it.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "38f48a358606",
        "ground_truth": "def sample(self, n: int = 3) -> pd.DataFrame:\n    \"\"\"Sample the dataframe.\n    Args:\n        n (int, optional): Number of rows to sample. Defaults to 5.\n    Returns:\n        pd.DataFrame: Sampled dataframe.\n    \"\"\"\n    sampled_df = pd.DataFrame()\n    if len(self.df) <= n:\n        sampled_df = self.df.sample(frac=1)\n    else:\n        for col in self.df.columns:\n            col_sample = self._sample_column(col, n)\n            sampled_df[col] = col_sample\n    # anonymize the sampled dataframe head\n    sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n    return sampled_df",
        "import_statements": [
            "import random"
        ],
        "reference_api": [
            "Anonymizer.anonymize_dataframe_head",
            "len",
            "self._sample_column",
            "pd.DataFrame",
            "sample"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "sample",
                "code": "def sample(self, n: int = 3) -> pd.DataFrame:\n        \"\"\"Sample the dataframe.\n\n        Args:\n            n (int, optional): Number of rows to sample. Defaults to 5.\n\n        Returns:\n            pd.DataFrame: Sampled dataframe.\n        \"\"\"\n        sampled_df = pd.DataFrame()\n        if len(self.df) <= n:\n            sampled_df = self.df.sample(frac=1)\n        else:\n            for col in self.df.columns:\n                col_sample = self._sample_column(col, n)\n                sampled_df[col] = col_sample\n\n        # anonymize the sampled dataframe head\n        sampled_df = Anonymizer.anonymize_dataframe_head(sampled_df)\n\n        return sampled_df"
            },
            {
                "name": "self._sample_column",
                "code": "def _sample_column(self, col: str, n: int) -> list:\n        \"\"\"Sample a column.\n\n        Args:\n            col (str): Column name.\n            n (int): Number of rows to sample.\n\n        Returns:\n            list: Sampled column.\n        \"\"\"\n\n        col_sample = []\n        col_values = self.df[col].dropna().unique()\n\n        # if there is a null value in the column, it MUST be included in the sample\n        if self.df[col].isna().any():\n            col_sample.append(np.nan)\n            n -= 1\n\n        # if the column has less than n unique values, then a random sample of the\n        # unique values should be returned\n        if len(col_values) <= n:\n            col_sample.extend(col_values)\n            n -= len(col_values)\n        else:\n            col_sample.extend(random.sample(list(col_values), n))\n            n = 0\n\n        # if there are still rows to sample, sample them randomly\n        if n > 0 and len(col_values) > 0:\n            col_sample.extend(random.choices(list(col_values), k=n))\n        else:\n            col_sample.extend([np.nan] * n)\n\n        # shuffle the column sample before returning it\n        random.shuffle(col_sample)\n        return col_sample"
            }
        ],
        "third_party": [
            "pd.DataFrame",
            "Anonymizer.anonymize_dataframe_head"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "Sinaptik-AI/pandas-ai",
        "file_path": "pandasai/helpers/dataframe_serializer.py",
        "function_declaration": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str",
        "start_line": "35",
        "end_line": "63",
        "docstring": "The function convert_df_to_csv converts a pandas DataFrame into a string formatted with XML-like tags, including optional name and description attributes if available. It constructs an opening tag with the DataFrame's name and description, adds details about the DataFrame's dimensions and content in CSV format, and closes the tag. The function uses information from an extras dictionary to include additional details about the DataFrame.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8bddad2e31ca",
        "ground_truth": "def convert_df_to_csv(self, df: pd.DataFrame, extras: dict) -> str:\n    \"\"\"\n    Convert df to csv like format where csv is wrapped inside <dataframe></dataframe>\n    Args:\n        df (pd.DataFrame): PandasAI dataframe or dataframe\n        extras (dict, optional): expect index to exists\n    Returns:\n        str: dataframe stringify\n    \"\"\"\n    dataframe_info = \"<dataframe\"\n    # Add name attribute if available\n    if df.name is not None:\n        dataframe_info += f' name=\"{df.name}\"'\n    # Add description attribute if available\n    if df.description is not None:\n        dataframe_info += f' description=\"{df.description}\"'\n    dataframe_info += \">\"\n    # Add dataframe details\n    dataframe_info += f\"\\ndfs[{extras['index']}]:{df.rows_count}x{df.columns_count}\\n{df.to_csv()}\"\n    # Close the dataframe tag\n    dataframe_info += \"</dataframe>\\n\"\n    return dataframe_info",
        "import_statements": [
            "import json",
            "from enum import Enum",
            "import yaml"
        ],
        "reference_api": [
            "df.to_csv"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "df.to_csv"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "Sinaptik-AI/pandas-ai",
        "file_path": "pandasai/helpers/dataframe_serializer.py",
        "function_declaration": "def convert_df_sql_connector_to_str",
        "start_line": "65",
        "end_line": "81",
        "docstring": "This function converts a DataFrame into an HTML table string.\\nIt optionally includes a description if it exists in the DataFrame metadata.\\nThe function constructs the opening table tag with the table name and optional description.\\nIt then adds the CSV representation of the DataFrame's head.\\nFinally, it closes the table tag and returns the complete HTML string.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5380bb60fd39",
        "ground_truth": "def convert_df_sql_connector_to_str(\n    self, df: pd.DataFrame, extras: dict = None\n) -> str:\n    \"\"\"\n    Convert df to csv like format where csv is wrapped inside <table></table>\n    Args:\n        df (pd.DataFrame): PandasAI dataframe or dataframe\n        extras (dict, optional): expect index to exists\n    Returns:\n        str: dataframe stringify\n    \"\"\"\n    table_description_tag = (\n        f' description=\"{df.description}\"' if df.description is not None else \"\"\n    )\n    table_head_tag = f'<table name=\"{df.name}\"{table_description_tag}>'\n    return f\"{table_head_tag}\\n{df.get_head().to_csv()}\\n</table>\"",
        "import_statements": [
            "import json",
            "from enum import Enum",
            "import yaml"
        ],
        "reference_api": [
            "df.get_head",
            "to_csv"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "to_csv",
            "df.get_head"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "Sinaptik-AI/pandas-ai",
        "file_path": "pandasai/helpers/dataframe_serializer.py",
        "function_declaration": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:",
        "start_line": "161",
        "end_line": "166",
        "docstring": "The function convert_df_to_yml converts a DataFrame to a YAML formatted string. It first converts the DataFrame to JSON format using another method and then converts the JSON to a YAML string. If the extras dictionary contains the key \"is_direct_sql\" with a value of True, the function wraps the YAML string in HTML table tags. It returns the resulting YAML string or the wrapped string.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0afd995716c5",
        "ground_truth": "def convert_df_to_yml(self, df: pd.DataFrame, extras: dict) -> str:\n    json_df = self.convert_df_to_json(df, extras)\n    yml_str = yaml.dump(json_df, sort_keys=False)\n    if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]:\n        return f\"<table>\\n{yml_str}\\n</table>\\n\"\n    return yml_str",
        "import_statements": [
            "import json",
            "from enum import Enum",
            "import yaml"
        ],
        "reference_api": [
            "yaml.dump",
            "self.convert_df_to_json"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.convert_df_to_json",
                "code": "def convert_df_to_json(self, df: pd.DataFrame, extras: dict) -> dict:\n        \"\"\"\n        Convert df to json dictionary and return json\n        Args:\n            df (pd.DataFrame): PandasAI dataframe or dataframe\n            extras (dict, optional): expect index to exists\n\n        Returns:\n            str: dataframe json\n        \"\"\"\n        # Initialize the result dictionary\n        df_number_key = f\"dfs[{extras['index']}]\"\n\n        # Create a dictionary representing the data structure\n        df_info = {\n            \"name\": df.name,\n            \"description\": df.description,\n            \"type\": (\n                df.type\n                if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]\n                else extras[\"type\"]\n            ),\n        }\n        # Add DataFrame details to the result\n        data = {\n            \"rows\": df.rows_count,\n            \"columns\": df.columns_count,\n            \"schema\": {\"fields\": []},\n        }\n\n        # Iterate over DataFrame columns\n        df_head = df.get_head()\n        for col_name, col_dtype in df_head.dtypes.items():\n            col_info = {\n                \"name\": col_name,\n                \"type\": str(col_dtype),\n            }\n\n            if not extras.get(\"enforce_privacy\") or df.custom_head is not None:\n                col_info[\"samples\"] = df_head[col_name].head().tolist()\n\n            # Add column description if available\n            if df.field_descriptions and isinstance(df.field_descriptions, dict):\n                if col_description := df.field_descriptions.get(col_name, None):\n                    col_info[\"description\"] = col_description\n\n            if df.connector_relations:\n                for relation in df.connector_relations:\n                    from pandasai.ee.connectors.relations import ForeignKey, PrimaryKey\n\n                    if (\n                        isinstance(relation, PrimaryKey) and relation.name == col_name\n                    ) or (\n                        isinstance(relation, ForeignKey) and relation.field == col_name\n                    ):\n                        col_info[\"constraints\"] = relation.to_string()\n\n            data[\"schema\"][\"fields\"].append(col_info)\n\n        result = df_info | data\n\n        if \"is_direct_sql\" in extras and extras[\"is_direct_sql\"]:\n            return result\n\n        return {df_number_key: result}"
            }
        ],
        "third_party": [
            "yaml.dump"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "Sinaptik-AI/pandas-ai",
        "file_path": "pandasai/helpers/output_validator.py",
        "function_declaration": "def validate_value(self, expected_type: str) -> bool",
        "start_line": "51",
        "end_line": "68",
        "docstring": "This function validates if the value matches the expected type.\\nIf expected_type is empty, it returns True.\\nIf expected_type is \"number\", it checks if the value is an integer or float.\\nIf expected_type is \"string\", it checks if the value is a string.\\nIf expected_type is \"dataframe\", it checks if the value is a pandas DataFrame or Series.\\nIf expected_type is \"plot\", it checks if the value is a string matching a file path pattern or a dictionary.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e2b90ebd4e5d",
        "ground_truth": "def validate_value(self, expected_type: str) -> bool:\n    if not expected_type:\n        return True\n    elif expected_type == \"number\":\n        return isinstance(self, (int, float))\n    elif expected_type == \"string\":\n        return isinstance(self, str)\n    elif expected_type == \"dataframe\":\n        return isinstance(self, (pd.DataFrame, pd.Series))\n    elif expected_type == \"plot\":\n        if not isinstance(self, (str, dict)):\n            return False\n        if isinstance(self, dict):\n            return True\n        path_to_plot_pattern = r\"^(\\/[\\w.-]+)+(/[\\w.-]+)*$|^[^\\s/]+(/[\\w.-]+)*$\"\n        return bool(re.match(path_to_plot_pattern, self))",
        "import_statements": [
            "import re",
            "from typing import Any, Iterable",
            "from pandasai.exceptions import InvalidOutputValueMismatch"
        ],
        "reference_api": [
            "re.match",
            "bool",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/dists_kernels/base/adapters/_sklearn.py",
        "function_declaration": " def _distance(self, X, X2=None)",
        "start_line": "90",
        "end_line": "123",
        "docstring": "The function _distance calculates the distance between two datasets, X and X2, using a specified distance metric and its parameters. If no additional parameters are provided, it initializes an empty dictionary. If the distance metric is a string, it uses the pairwise_distance function from sktime.distances. Otherwise, it directly calls the distance function with the given datasets and parameters. If X2 is None, the function checks if the distance function can handle a single dataset by inspecting its signature and calls it accordingly. The function returns the computed distance.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d95828fdcbcb",
        "ground_truth": "def _distance(self, X, X2=None):\n    \"\"\"Compute distance - unified interface to str code and callable.\n    If self.distance is a string, it is assumed to be a numba distance,\n    and X, X2 are assumed in numpy3D format.\n    If self.distance is a callable, it is assumed to be a sktime distance,\n    and X, X2 are assumed in any of the sktime Panel formats,\n    e.g., pd-multiindex, numpy3D.\n    Consumers of this method should ensure that the input is in the correct format.\n    This method should not be used as a direct public interface,\n    only for internal use in estimators making use of this adapter.\n    \"\"\"\n    distance = self.distance\n    distance_params = self.distance_params\n    if distance_params is None:\n        distance_params = {}\n    if isinstance(distance, str):\n        from sktime.distances import pairwise_distance\n        return pairwise_distance(X, X2, distance, **distance_params)\n    else:\n        if X2 is not None:\n            return distance(X, X2, **distance_params)\n        # if X2 is None, check if distance allows None X2 to mean \"X2=X\"\n        else:\n            sig = signature(distance).parameters\n            X2_sig = sig[list(sig.keys())[1]]\n            if X2_sig.default is not None:\n                return distance(X, X2, **distance_params)\n            else:\n                return distance(X, **distance_params)",
        "import_statements": [
            "from inspect import signature",
            "from sktime.datatypes import convert"
        ],
        "reference_api": [
            "distance",
            "list",
            "sig.keys",
            "isinstance",
            "signature",
            "pairwise_distance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "pairwise_distance",
            "distance",
            "signature",
            "sig.keys",
            "distance",
            "distance"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/dists_kernels/base/adapters/_sklearn.py",
        "function_declaration": "def _convert_X_to_sklearn(self, X)",
        "start_line": "162",
        "end_line": "184",
        "docstring": "This function converts input data X to a 2D numpy array for use with sklearn.\\nFor unequal length series, it resets the index, pivots the data, fills missing values with zeros, converts it to a numpy array, calculates the lengths of individual series, and concatenates these lengths as the first column.\\nFor equal length series, it identifies the data type of X and converts it to a flattened numpy array.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "497d73b298ee",
        "ground_truth": "def _convert_X_to_sklearn(self, X):\n    \"\"\"Convert X to 2D numpy for sklearn.\"\"\"\n    # special treatment for unequal length series\n    if not self.is_equal_length:\n        # then we know we are dealing with pd-multiindex\n        # as a trick to deal with unequal length data,\n        # we flatten encode the length as the first column\n        X_w_ix = X.reset_index(-1)\n        X_pivot = X_w_ix.pivot(columns=[X_w_ix.columns[0]])\n        # fillna since this creates nan but sklearn does not accept these\n        # the fill value does not matter as the distance ignores it\n        X_pivot = X_pivot.fillna(0).to_numpy()\n        X_lens = X.groupby(X_w_ix.index).size().to_numpy()\n        # add the first column, encoding length of individual series\n        X_w_lens = np.concatenate([X_lens[:, None], X_pivot], axis=1)\n        return X_w_lens\n    # equal length series case\n    if isinstance(X, np.ndarray):\n        X_mtype = \"numpy3D\"\n    else:\n        X_mtype = \"pd-multiindex\"\n    return convert(X, from_type=X_mtype, to_type=\"numpyflat\")",
        "import_statements": [
            "from inspect import signature",
            "from sktime.datatypes import convert"
        ],
        "reference_api": [
            "X.groupby",
            "X_w_ix.pivot",
            "to_numpy",
            "size",
            "isinstance",
            "X.reset_index",
            "convert",
            "X_pivot.fillna",
            "np.concatenate"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "X.reset_index",
            "X_w_ix.pivot",
            "to_numpy",
            "X_pivot.fillna",
            "to_numpy",
            "size",
            "X.groupby",
            "np.concatenate",
            "convert"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/utils/sklearn/_scitype.py",
        "function_declaration": "def is_sklearn_estimator(obj)",
        "start_line": "17",
        "end_line": "35",
        "docstring": "The function is_sklearn_estimator checks if a given object is a scikit-learn estimator. It ensures the input is a class, then verifies if it is a subclass of SklearnBaseEstimator and not a subclass of BaseObject from sktime. The function returns True if both conditions are met, indicating the object is a scikit-learn estimator, otherwise it returns False.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8798260a6f12",
        "ground_truth": "def is_sklearn_estimator(obj):\n    \"\"\"Check whether obj is an sklearn estimator.\n     Parameters\n    ----------\n    obj : any class or object\n     Returns\n    -------\n    is_sklearn_est : bool, whether obj is an sklearn estimator (class or instance)\n    \"\"\"\n    if not isclass(obj):\n        obj = type(obj)\n     is_in_sklearn = issubclass(obj, SklearnBaseEstimator)\n    is_in_sktime = issubclass(obj, BaseObject)\n     is_sklearn_est = is_in_sklearn and not is_in_sktime\n    return is_sklearn_est",
        "import_statements": [
            "from inspect import isclass",
            "from sklearn.base import BaseEstimator as SklearnBaseEstimator",
            "from sklearn.base import ClassifierMixin, ClusterMixin, RegressorMixin, TransformerMixin",
            "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV",
            "from sklearn.pipeline import Pipeline",
            "from sktime.base import BaseObject"
        ],
        "reference_api": [
            "issubclass",
            "isclass",
            "type"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "isclass"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/regression/compose/_pipeline.py",
        "function_declaration": "def __rmul__(self, other)",
        "start_line": "437",
        "end_line": "461",
        "docstring": "This function defines the right multiplication behavior for an object.\\nIf the other object is an instance of BaseTransformer, it creates a TransformerPipeline by multiplying the other object with the current object's transformers.\\nIt then creates a new SklearnRegressorPipeline with the existing regressor and the expanded transformer pipeline.\\nIf the other object is not a BaseTransformer, it returns NotImplemented.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2ab2596d8520",
        "ground_truth": "def __rmul__(self, other):\n    \"\"\"Magic * method, return concatenated RegressorPipeline, transformers on left.\n    Implemented for `other` being a transformer, otherwise returns `NotImplemented`.\n    Parameters\n    ----------\n    other: `sktime` transformer, must inherit from BaseTransformer\n        otherwise, `NotImplemented` is returned\n    Returns\n    -------\n    RegressorPipeline object, concatenation of `other` (first) with `self` (last).\n    \"\"\"\n    if isinstance(other, BaseTransformer):\n        # use the transformers dunder to get a TransformerPipeline\n        trafo_pipeline = other * self.transformers_\n        # then stick the expanded pipeline in a SklearnRegressorPipeline\n        new_pipeline = SklearnRegressorPipeline(\n            regressor=self.regressor,\n            transformers=trafo_pipeline.steps,\n        )\n        return new_pipeline\n    else:\n        return NotImplemented",
        "import_statements": [
            "from sktime.base import _HeterogenousMetaEstimator",
            "from sktime.datatypes import convert_to",
            "from sktime.regression.base import BaseRegressor",
            "from sktime.transformations.base import BaseTransformer",
            "from sktime.transformations.compose import TransformerPipeline",
            "from sktime.utils.sklearn import is_sklearn_regressor"
        ],
        "reference_api": [
            "SklearnRegressorPipeline",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "SklearnRegressorPipeline"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/utils/sklearn/_adapt_df.py",
        "function_declaration": "def prep_skl_df(df, copy_df=False)",
        "start_line": "8",
        "end_line": "32",
        "docstring": "This function prepares a DataFrame for scikit-learn by ensuring all column names are strings.\\nIt takes a DataFrame and an optional boolean to determine whether to copy the DataFrame.\\nIt converts the column names to strings and checks if they are already strings.\\nIf they are not, and if the copy option is True, it creates a copy of the DataFrame.\\nIt then assigns the string-converted column names to the DataFrame and returns it.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5c1c6d14ca93",
        "ground_truth": "def prep_skl_df(df, copy_df=False):\n    \"\"\"Make df compatible with sklearn input expectations.\n     Changes:\n    turns column index into a list of strings\n     Parameters\n    ----------\n    df : pd.DataFrame\n        list of indices to sample from\n    copy_df : bool, default=False\n        whether to mutate df or return a copy\n        if False, index of df is mutated\n        if True, original df is not mutated. If index is not a list of strings,\n        a copy is made and the copy is mutated. Otherwise, the original df is returned.\n    \"\"\"\n    cols = df.columns\n    str_cols = cols.astype(str)\n     if not np.all(str_cols == cols):\n        if copy_df:\n            df = df.copy()\n        df.columns = str_cols\n     return df",
        "import_statements": [],
        "reference_api": [
            "np.all",
            "cols.astype",
            "df.copy"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "cols.astype",
            "np.all",
            "df.copy"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/classification/sklearn/_rotation_forest.py",
        "function_declaration": "def predict(self, X)",
        "start_line": "258",
        "end_line": "277",
        "docstring": "The function predict generates predictions for the given input data X. It initializes a random state using the object's random_state attribute. For each sample, it obtains the predicted probabilities, selects the indices of the maximum probability values, and randomly chooses one among them. It then maps the chosen index to the corresponding class label. The function returns an array of predicted class labels.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d5e567c20361",
        "ground_truth": "def predict(self, X):\n    \"\"\"Predict for all cases in X. Built on top of predict_proba.\n    Parameters\n    ----------\n    X : 2d ndarray or DataFrame of shape = [n_instances, n_attributes]\n        The data to make predictions for.\n    Returns\n    -------\n    y : array-like, shape = [n_instances]\n        Predicted class labels.\n    \"\"\"\n    rng = check_random_state(self.random_state)\n    return np.array(\n        [\n            self.classes_[int(rng.choice(np.flatnonzero(prob == prob.max())))]\n            for prob in self.predict_proba(X)\n        ]\n    )",
        "import_statements": [
            "import time",
            "from joblib import Parallel, delayed",
            "from sklearn.base import BaseEstimator",
            "from sklearn.decomposition import PCA",
            "from sklearn.tree import DecisionTreeClassifier",
            "from sklearn.utils import check_random_state",
            "from sktime.base._base import _clone_estimator",
            "from sktime.exceptions import NotFittedError",
            "from sktime.utils.validation import check_n_jobs"
        ],
        "reference_api": [
            "np.flatnonzero",
            "rng.choice",
            "self.predict_proba",
            "int",
            "check_random_state",
            "prob.max",
            "np.array"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.predict_proba",
                "code": "ef predict_proba(self, X):\n        \"\"\"Probability estimates for each class for all cases in X.\n\n        Parameters\n        ----------\n        X : 2d ndarray or DataFrame of shape = [n_instances, n_attributes]\n            The data to make predictions for.\n\n        Returns\n        -------\n        y : array-like, shape = [n_instances, n_classes_]\n            Predicted probabilities using the ordering in classes_.\n        \"\"\"\n        if not self._is_fitted:\n            raise NotFittedError(\n                f\"This instance of {self.__class__.__name__} has not \"\n                f\"been fitted yet; please call `fit` first.\"\n            )\n\n        # treat case of single class seen in fit\n        if self.n_classes_ == 1:\n            return np.repeat([[1]], X.shape[0], axis=0)\n\n        if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n            X = np.reshape(X, (X.shape[0], -1))\n        elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n            X = X.to_numpy()\n        elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n            raise ValueError(\n                \"RotationForest is not a time series classifier. \"\n                \"A valid sklearn input such as a 2d numpy array is required.\"\n                \"Sparse input formats are currently not supported.\"\n            )\n        X = self._validate_data(X=X, reset=False)\n\n        # replace missing values with 0 and remove useless attributes\n        X = X[:, self._useful_atts]\n\n        # normalise the data.\n        X = (X - self._min) / self._ptp\n\n        y_probas = Parallel(n_jobs=self._n_jobs)(\n            delayed(self._predict_proba_for_estimator)(\n                X,\n                self.estimators_[i],\n                self._pcas[i],\n                self._groups[i],\n            )\n            for i in range(self._n_estimators)\n        )\n\n        output = np.sum(y_probas, axis=0) / (\n            np.ones(self.n_classes_) * self._n_estimators\n        )\n        return output\n"
            }
        ],
        "third_party": [
            "check_random_state",
            "np.array",
            "rng.choice",
            "np.flatnonzero",
            "prob.max"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/classification/sklearn/_rotation_forest.py",
        "function_declaration": "def _predict_proba_for_estimator(self, X, clf, pcas, groups)",
        "start_line": "453",
        "end_line": "467",
        "docstring": "The function _predict_proba_for_estimator generates class probabilities for the given input data X using a specified classifier, PCA transformations, and groups of features. It first transforms the input data by applying the corresponding PCA to each group and concatenating the results. Missing values in the transformed data are replaced with zeros. The function then computes class probabilities using the classifier. If the number of predicted classes does not match the expected number of classes, it adjusts the probabilities to match the expected class structure. The function returns the adjusted class probabilities.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "210b60dffd55",
        "ground_truth": "def _predict_proba_for_estimator(self, X, clf, pcas, groups):\n    X_t = np.concatenate(\n        [pcas[i].transform(X[:, group]) for i, group in enumerate(groups)], axis=1\n    )\n    X_t = np.nan_to_num(X_t, False, 0, 0, 0)\n    probas = clf.predict_proba(X_t)\n    if probas.shape[1] != self.n_classes_:\n        new_probas = np.zeros((probas.shape[0], self.n_classes_))\n        for i, cls in enumerate(clf.classes_):\n            cls_idx = self._class_dictionary[cls]\n            new_probas[:, cls_idx] = probas[:, i]\n        probas = new_probas\n    return probas",
        "import_statements": [
            "import time",
            "from joblib import Parallel, delayed",
            "from sklearn.base import BaseEstimator",
            "from sklearn.decomposition import PCA",
            "from sklearn.tree import DecisionTreeClassifier",
            "from sklearn.utils import check_random_state",
            "from sktime.base._base import _clone_estimator",
            "from sktime.exceptions import NotFittedError",
            "from sktime.utils.validation import check_n_jobs"
        ],
        "reference_api": [
            "np.nan_to_num",
            "np.zeros",
            "transform",
            "clf.predict_proba",
            "np.concatenate",
            "enumerate"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "clf.predict_proba",
                "code": "ef predict_proba(self, X):\n        \"\"\"Probability estimates for each class for all cases in X.\n\n        Parameters\n        ----------\n        X : 2d ndarray or DataFrame of shape = [n_instances, n_attributes]\n            The data to make predictions for.\n\n        Returns\n        -------\n        y : array-like, shape = [n_instances, n_classes_]\n            Predicted probabilities using the ordering in classes_.\n        \"\"\"\n        if not self._is_fitted:\n            raise NotFittedError(\n                f\"This instance of {self.__class__.__name__} has not \"\n                f\"been fitted yet; please call `fit` first.\"\n            )\n\n        # treat case of single class seen in fit\n        if self.n_classes_ == 1:\n            return np.repeat([[1]], X.shape[0], axis=0)\n\n        if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n            X = np.reshape(X, (X.shape[0], -1))\n        elif isinstance(X, pd.DataFrame) and len(X.shape) == 2:\n            X = X.to_numpy()\n        elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n            raise ValueError(\n                \"RotationForest is not a time series classifier. \"\n                \"A valid sklearn input such as a 2d numpy array is required.\"\n                \"Sparse input formats are currently not supported.\"\n            )\n        X = self._validate_data(X=X, reset=False)\n\n        # replace missing values with 0 and remove useless attributes\n        X = X[:, self._useful_atts]\n\n        # normalise the data.\n        X = (X - self._min) / self._ptp\n\n        y_probas = Parallel(n_jobs=self._n_jobs)(\n            delayed(self._predict_proba_for_estimator)(\n                X,\n                self.estimators_[i],\n                self._pcas[i],\n                self._groups[i],\n            )\n            for i in range(self._n_estimators)\n        )\n\n        output = np.sum(y_probas, axis=0) / (\n            np.ones(self.n_classes_) * self._n_estimators\n        )\n        return output\n"
            }
        ],
        "third_party": [
            "np.concatenate",
            "transform",
            "np.nan_to_num",
            "np.zeros"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/classification/sklearn/_continuous_interval_tree.py",
        "function_declaration": "def predict_proba(self, X)",
        "start_line": "197",
        "end_line": "233",
        "docstring": "This function predicts class probabilities for the given input data.\\nIt first checks if the model is fitted and raises an error if not.\\nFor a single class, it returns a probability of 1 for all inputs.\\nIt reshapes the input if it is a 3D numpy array with a single feature, or raises an error for invalid input types or shapes.\\nThe input data is validated, allowing NaN values.\\nIt calculates the probability distributions by iterating over each input sample and using the root node's predict_proba method.\\nFinally, it returns the computed probability distributions.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9436ee0860bb",
        "ground_truth": "def predict_proba(self, X):\n    \"\"\"Probability estimates for each class for all cases in X.\n    Parameters\n    ----------\n    X : 2d ndarray or DataFrame of shape = [n_instances, n_attributes]\n        The data to make predictions for.\n    Returns\n    -------\n    y : array-like, shape = [n_instances, n_classes_]\n        Predicted probabilities using the ordering in classes_.\n    \"\"\"\n    if not self._is_fitted:\n        raise NotFittedError(\n            f\"This instance of {self.__class__.__name__} has not \"\n            f\"been fitted yet; please call `fit` first.\"\n        )\n    # treat case of single class seen in fit\n    if self.n_classes_ == 1:\n        return np.repeat([[1]], X.shape[0], axis=0)\n    if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n        X = np.reshape(X, (X.shape[0], -1))\n    elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n        raise ValueError(\n            \"ContinuousIntervalTree is not a time series classifier. \"\n            \"A valid sklearn input such as a 2d numpy array is required.\"\n            \"Sparse input formats are currently not supported.\"\n        )\n    X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n    dists = np.zeros((X.shape[0], self.n_classes_))\n    for i in range(X.shape[0]):\n        dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n    return dists",
        "import_statements": [
            "import sys",
            "from sklearn import preprocessing",
            "from sklearn.base import BaseEstimator",
            "from sklearn.utils import check_random_state",
            "from sktime.exceptions import NotFittedError"
        ],
        "reference_api": [
            "np.repeat",
            "NotFittedError",
            "ValueError",
            "len",
            "isinstance",
            "np.zeros",
            "predict_proba",
            "self._validate_data",
            "np.reshape",
            "range"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "predict_proba",
                "code": "def predict_proba(self, X):\n        \"\"\"Probability estimates for each class for all cases in X.\n\n        Parameters\n        ----------\n        X : 2d ndarray or DataFrame of shape = [n_instances, n_attributes]\n            The data to make predictions for.\n\n        Returns\n        -------\n        y : array-like, shape = [n_instances, n_classes_]\n            Predicted probabilities using the ordering in classes_.\n        \"\"\"\n        if not self._is_fitted:\n            raise NotFittedError(\n                f\"This instance of {self.__class__.__name__} has not \"\n                f\"been fitted yet; please call `fit` first.\"\n            )\n\n        # treat case of single class seen in fit\n        if self.n_classes_ == 1:\n            return np.repeat([[1]], X.shape[0], axis=0)\n\n        if isinstance(X, np.ndarray) and len(X.shape) == 3 and X.shape[1] == 1:\n            X = np.reshape(X, (X.shape[0], -1))\n        elif not isinstance(X, np.ndarray) or len(X.shape) > 2:\n            raise ValueError(\n                \"ContinuousIntervalTree is not a time series classifier. \"\n                \"A valid sklearn input such as a 2d numpy array is required.\"\n                \"Sparse input formats are currently not supported.\"\n            )\n        X = self._validate_data(X=X, reset=False, force_all_finite=\"allow-nan\")\n\n        dists = np.zeros((X.shape[0], self.n_classes_))\n        for i in range(X.shape[0]):\n            dists[i] = self._root.predict_proba(X[i], self.n_classes_)\n        return dists"
            }
        ],
        "third_party": [
            "NotFittedError",
            "np.repeat",
            "np.reshape",
            "self._validate_data",
            "np.zeros"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/classification/compose/_pipeline.py",
        "function_declaration": "def _convert_X_to_sklearn(self, X)",
        "start_line": "504",
        "end_line": "520",
        "docstring": "The function _convert_X_to_sklearn converts input data X into a 2D numpy array format required by scikit-learn. It determines the scitype of the transformed output from the transformers_ attribute. If the scitype is \"Primitives\", it converts X to a 2D numpy array with Table scitype. If the scitype is \"Series\", it converts X to a flat 2D numpy array with Panel scitype. If the scitype is neither, it raises a TypeError indicating an unexpected output type. The function returns the converted 2D numpy array.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d7cb1cb66840",
        "ground_truth": "def _convert_X_to_sklearn(self, X):\n    \"\"\"Convert a Table or Panel X to 2D numpy required by sklearn.\"\"\"\n    X_scitype = self.transformers_.get_tag(\"scitype:transform-output\")\n    # if X_scitype is Primitives, output is Table, convert to 2D numpy array\n    if X_scitype == \"Primitives\":\n        Xt = convert_to(X, to_type=\"numpy2D\", as_scitype=\"Table\")\n    # if X_scitype is Series, output is Panel, convert to 2D numpy array (numpyflat)\n    elif X_scitype == \"Series\":\n        Xt = convert_to(X, to_type=\"numpyflat\", as_scitype=\"Panel\")\n    else:\n        raise TypeError(\n            f\"unexpected X output type in {type(self.classifier).__name__}, \"\n            f'in tag \"scitype:transform-output\", found \"{X_scitype}\", '\n            'expected one of \"Primitives\" or \"Series\"'\n        )\n    return Xt",
        "import_statements": [
            "from sktime.base import _HeterogenousMetaEstimator",
            "from sktime.classification.base import BaseClassifier",
            "from sktime.datatypes import convert_to",
            "from sktime.transformations.base import BaseTransformer",
            "from sktime.transformations.compose import TransformerPipeline",
            "from sktime.utils.sklearn import is_sklearn_classifier"
        ],
        "reference_api": [
            "TypeError",
            "convert_to",
            "get_tag",
            "type"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get_tag",
            "convert_to",
            "convert_to"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/utils/random_state.py",
        "function_declaration": "def set_random_state(estimator, random_state=0)",
        "start_line": "9",
        "end_line": "37",
        "docstring": "This function sets the random state for an estimator.\\nIt first ensures the random_state parameter is a valid random state object.\\nThen, it initializes an empty dictionary to hold parameters that need updating.\\nIt iterates over the estimator's parameters, checking for those related to random state.\\nIf such parameters are found, they are assigned a random integer value.\\nFinally, the function updates the estimator with the new random state values if any were found.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "497cb75f2b4d",
        "ground_truth": "def set_random_state(estimator, random_state=0):\n    \"\"\"Set fixed random_state parameters for an estimator.\n     Finds all parameters ending ``random_state`` and sets them to integers\n    derived from ``random_state``.\n     Parameters\n    ----------\n    estimator : estimator supporting get_params, set_params\n        Estimator with potential randomness managed by random_state parameters.\n     random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the generation of the random\n        integers. Pass an int for reproducible output across multiple function calls.\n     Notes\n    -----\n    This does not necessarily set *all* ``random_state`` attributes that\n    control an estimator's randomness, only those accessible through\n    ``estimator.get_params()``.\n    \"\"\"\n    random_state = check_random_state(random_state)\n    to_set = {}\n    for key in sorted(estimator.get_params(deep=True)):\n        if key == \"random_state\" or key.endswith(\"__random_state\"):\n            to_set[key] = random_state.randint(np.iinfo(np.int32).max)\n     if to_set:\n        estimator.set_params(**to_set)",
        "import_statements": [
            "from sklearn.utils import check_random_state"
        ],
        "reference_api": [
            "sorted",
            "key.endswith",
            "check_random_state",
            "estimator.set_params",
            "np.iinfo",
            "random_state.randint",
            "estimator.get_params"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "check_random_state",
            "estimator.get_params",
            "key.endswith",
            "random_state.randint",
            "np.iinfo",
            "estimator.set_params"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "sktime/sktime",
        "file_path": "sktime/base/_panel/forest/_tsf.py",
        "function_declaration": "def _transform(X, intervals)",
        "start_line": "237",
        "end_line": "266",
        "docstring": "The function _transform processes the input matrix X using specified intervals. It calculates the number of instances and intervals, then initializes an empty array to store the transformed features. For each interval, it slices the input matrix, computes the mean, standard deviation, and slope of the slice, and stores these values in the transformed array. The function returns the transposed transformed array.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "59ffd1d596a1",
        "ground_truth": "def _transform(X, intervals):\n    \"\"\"Transform X for given intervals.\n     Compute the mean, standard deviation and slope for given intervals of input data X.\n     Parameters\n    ----------\n    Xt: np.ndarray or pd.DataFrame\n        Panel data to transform.\n    intervals : np.ndarray\n        Intervals containing start and end values.\n     Returns\n    -------\n    Xt: np.ndarray or pd.DataFrame\n     Transformed X, containing the mean, std and slope for each interval\n    \"\"\"\n    n_instances, _ = X.shape\n    n_intervals, _ = intervals.shape\n    transformed_x = np.empty(shape=(3 * n_intervals, n_instances), dtype=np.float32)\n    for j in range(n_intervals):\n        X_slice = X[:, intervals[j][0] : intervals[j][1]]\n        means = np.mean(X_slice, axis=1)\n        std_dev = np.std(X_slice, axis=1)\n        slope = _slope(X_slice, axis=1)\n        transformed_x[3 * j] = means\n        transformed_x[3 * j + 1] = std_dev\n        transformed_x[3 * j + 2] = slope\n     return transformed_x.T",
        "import_statements": [
            "import math",
            "from typing import Optional",
            "from joblib import Parallel, delayed",
            "from numpy.random import RandomState",
            "from sklearn.utils.multiclass import class_distribution",
            "from sklearn.utils.validation import check_random_state",
            "from sktime.utils.slope_and_trend import _slope",
            "from sktime.utils.validation import check_n_jobs"
        ],
        "reference_api": [
            "np.mean",
            "_slope",
            "np.std",
            "np.empty",
            "range"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_slope",
                "code": "def _slope(y, axis=0):\n    \"\"\"Find the slope for each series of y.\n\n    Parameters\n    ----------\n    y: np.ndarray\n        Time series\n    axis : int, optional (default=0)\n        Axis along which to compute slope\n\n    Returns\n    -------\n    slope : np.ndarray\n        Time series slope\n    \"\"\"\n    # Make sure y is always at least 2-dimensional\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n\n    # Generate time index with correct shape for broadcasting\n    shape = np.ones(y.ndim, dtype=int)\n    shape[axis] *= -1\n    x = np.arange(y.shape[axis]).reshape(shape) + 1\n\n    # Precompute mean\n    x_mean = x.mean()\n\n    # Compute slope along given axis\n    return (np.mean(y * x, axis=axis) - x_mean * np.mean(y, axis=axis)) / (\n        (x * x).mean() - x_mean**2\n    )"
            }
        ],
        "third_party": [
            "np.empty",
            "np.mean",
            "np.std"
        ]
    },
    {
        "subclass": "seaborn",
        "owner/repo": "trevismd/statannotations",
        "file_path": "statannotations/Annotator.py",
        "function_declaration": "def plot_and_annotate_facets(\n            self, plot: str, plot_params: dict, configuration: dict,\n            annotation_func: str, *args, annotation_params: dict = None,\n            ax_op_before: List[Union[str, Optional[list],\n                                     Optional[dict]]] = None,\n            ax_op_after: List[Union[str, Optional[list],\n                                    Optional[dict]]] = None,\n            annotate_params: dict = None, **kwargs)",
        "start_line": "817",
        "end_line": "860",
        "docstring": "This function generates and annotates a plot using Seaborn and various configuration parameters.\\nIt initializes annotation and operation parameters if they are not provided.\\nIt creates the plot with given parameters and applies pre-plot axis operations.\\nThe plot is further configured and annotated based on provided configuration and annotation functions.\\nPost-plot axis operations are then applied.\\nFinally, the function returns the output of the plotting process.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5a01594eebfb",
        "ground_truth": "def plot_and_annotate_facets(\n        self, plot: str, plot_params: dict, configuration: dict,\n        annotation_func: str, *args, annotation_params: dict = None,\n        ax_op_before: List[Union[str, Optional[list],\n                                 Optional[dict]]] = None,\n        ax_op_after: List[Union[str, Optional[list],\n                                Optional[dict]]] = None,\n        annotate_params: dict = None, **kwargs):\n    \"\"\"\n    Plots using seaborn and annotates in a single call, to be used within\n    a `FacetGrid`.\n    First, initialize the Annotator with `Annotator(None, pairs)` to define\n    the pairs, then map this function onto the `FacetGrid`.\n    :param plot: seaborn plotting function to call\n    :param plot_params: parameters for plotting function call\n    :param configuration: parameters for Annotator.configure\n    :param annotation_func: name of annotation function to be called, from:\n        * 'set_custom_annotations'\n        * 'set_pvalues'\n        * 'apply_test'\n    :param annotation_params: parameters for the annotation function\n    :param ax_op_before: list of [func_name, args, kwargs] to apply on `ax`\n        before annotating\n    :param ax_op_after: list of [func_name, args, kwargs] to apply on `ax`\n        after annotating\n    :param annotate_params: parameters for `Annotator.annotate`\n    :param args: additional parameters for the seaborn function\n    :param kwargs: additional parameters for the seaborn function\n    \"\"\"\n    annotate_params = empty_dict_if_none(annotate_params)\n    annotation_params = empty_dict_if_none(annotation_params)\n    ax = getattr(sns, plot)(*args, **plot_params, **kwargs)\n    _apply_ax_operations(ax, ax_op_before)\n    self.new_plot(ax, plot=plot, **plot_params, data=kwargs['data'])\n    self.configure(**configuration)\n    getattr(self, annotation_func)(**annotation_params)\n    self.annotate(**annotate_params)\n    _apply_ax_operations(ax, ax_op_after)\n    return self._get_output()",
        "import_statements": [
            "import warnings",
            "from typing import List, Optional, Union",
            "from matplotlib import lines",
            "from matplotlib.font_manager import FontProperties",
            "from statannotations.Annotation import Annotation",
            "from statannotations.PValueFormat import PValueFormat, \\\n    CONFIGURABLE_PARAMETERS as PVALUE_CONFIGURABLE_PARAMETERS",
            "from statannotations._Plotter import _Plotter, _SeabornPlotter",
            "from statannotations.stats.ComparisonsCorrection import \\\n    get_validated_comparisons_correction",
            "from statannotations.stats.StatResult import StatResult",
            "from statannotations.stats.StatTest import StatTest",
            "from statannotations.stats.test import apply_test, IMPLEMENTED_TESTS",
            "from statannotations.stats.utils import check_alpha, check_pvalues, \\\n    check_num_comparisons, get_num_comparisons",
            "from statannotations.utils import check_is_in, InvalidParametersError, \\\n    empty_dict_if_none"
        ],
        "reference_api": [
            "empty_dict_if_none",
            "getattr",
            "self.annotate",
            "self.new_plot",
            "_apply_ax_operations",
            "self._get_output",
            "self.configure"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "empty_dict_if_none",
                "code": "def empty_dict_if_none(data):\n    if data is None:\n        return {}\n    return data"
            },
            {
                "name": "empty_dict_if_none",
                "code": "def empty_dict_if_none(data):\n    if data is None:\n        return {}\n    return data"
            },
            {
                "name": "_apply_ax_operations",
                "code": "def _apply_ax_operations(ax, operations):\n    if operations is None:\n        return\n    for operation in operations:\n        _ensure_ax_operation_format(operation)\n        getattr(ax, operation[0])(*operation[1],\n                                  **empty_dict_if_none(operation[2]))"
            },
            {
                "name": "self.new_plot",
                "code": "def new_plot(self, ax, pairs=None, plot='boxplot', data=None, x=None,\n                 y=None, hue=None, order=None, hue_order=None,\n                 engine: str = \"seaborn\", **plot_params):\n        self.ax = ax\n\n        if pairs is None:\n            pairs = self.pairs\n\n        self._plotter: _Plotter = self._get_plotter(\n            engine, ax, pairs, plot, data, x, y, hue, order, hue_order,\n            **plot_params)\n\n        self.line_offset = None\n        self.line_offset_to_group = None\n        self.perform_stat_test = None\n\n        return self"
            },
            {
                "name": "self.configure",
                "code": "def configure(self, **parameters):\n        \"\"\"\n        * `alpha`: Acceptable type 1 error for statistical tests, default 0.05\n        * `color`\n        * `comparisons_correction`: Method for multiple comparisons correction.\n            One of `statsmodels` `multipletests` methods (w/ default FWER), or\n            a `ComparisonsCorrection` instance.\n        * `correction_format`: How to format the star notation on the plot when\n            the multiple comparisons correction method removes the significance\n            * `default`: a ' (ns)' suffix is added, such as in printed output,\n            corresponds to \"{star} ({suffix})\"\n            * `replace`: the original star value is replaced with 'ns'\n            corresponds to \"{suffix}\"\n            * a custom formatting string using \"{star}\" for the original\n            pvalue and '{suffix}' for 'ns'\n        * `hide_non_significant`: hide annotations for non-significant pair\n            comparisons\n        * `line_height`: in axes fraction coordinates\n        * `line_offset`\n        * `line_offset_to_group`\n        * `line_width`\n        * `loc`\n        * `pvalue_format`: list of lists, or tuples. Default values are:\n            * For \"star\" text_format:\n              `[[1e-4, \"****\"], [1e-3, \"***\"], [1e-2, \"**\"], [0.05, \"*\"], [1, \"ns\"]]`.\n            * For \"simple\" text_format:\n              `[[1e-5, \"1e-5\"], [1e-4, \"1e-4\"], [1e-3, \"0.001\"], [1e-2, \"0.01\"], [5e-2, \"0.05\"]]`.\n        * `show_test_name`: Set to False to not show the (short) name of\n            test\n        * `test`\n        * `text_offset`: in points\n        * `test_short_name`\n        * `use_fixed_offset`\n        * `verbose`\n        \"\"\"\n        self._check_has_plotter()\n\n        if parameters.get(\"pvalue_format\") is None:\n            parameters[\"pvalue_format\"] = {\n                item: value\n                for item, value in parameters.items()\n                if item in PVALUE_CONFIGURABLE_PARAMETERS\n            }\n\n        unmatched_parameters = parameters.keys() - set(CONFIGURABLE_PARAMETERS)\n        unmatched_parameters -= parameters[\"pvalue_format\"].keys()\n\n        if unmatched_parameters:\n            raise InvalidParametersError(unmatched_parameters)\n\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n\n        self._activate_configured_warning()\n\n        self._warn_alpha_thresholds_if_necessary(parameters)\n        return self"
            },
            {
                "name": "self.annotate",
                "code": "def annotate(self, line_offset=None, line_offset_to_group=None):\n        \"\"\"Add configured annotations to the plot.\"\"\"\n        self._check_has_plotter()\n\n        self._maybe_warn_about_configuration()\n\n        self._update_value_for_loc()\n\n        ann_list = []\n        orig_value_lim = self._plotter.get_value_lim()\n\n        offset_func = self.get_offset_func(self.loc)\n        self.value_offset, self.line_offset_to_group = offset_func(\n            line_offset, line_offset_to_group)\n\n        if self._verbose:\n            self.print_pvalue_legend()\n\n        ax_to_data = self._plotter.get_transform_func('ax_to_data')\n\n        self.validate_test_short_name()\n\n        for annotation in self.annotations:\n            if self.hide_non_significant and isinstance(annotation.data, StatResult) \\\n                    and not annotation.data.is_significant:\n                continue\n            self._annotate_pair(annotation,\n                                ax_to_data=ax_to_data,\n                                ann_list=ann_list,\n                                orig_value_lim=orig_value_lim)\n\n        # reset transformation\n        y_stack_max = max(self._value_stack_arr[1, :])\n        ax_to_data = self._plotter.get_transform_func('ax_to_data')\n        value_lims = (\n            ([(0, 0), (0, max(1.04 * y_stack_max, 1))]\n             if self.loc == 'inside'\n             else [(0, 0), (0, 1)])\n            if self.orient == 'v'\n            else\n            ([(0, 0), (max(1.04 * y_stack_max, 1), 0)]\n             if self.loc == 'inside'\n             else [(0, 0), (1, 0)])\n        )\n        set_lims = self.ax.set_ylim if self.orient == 'v' else self.ax.set_xlim\n        transformed = ax_to_data.transform(value_lims)\n        set_lims(transformed[:, 1 if self.orient == 'v' else 0])\n\n        return self._get_output()"
            },
            {
                "name": "_apply_ax_operations",
                "code": "def _apply_ax_operations(ax, operations):\n    if operations is None:\n        return\n    for operation in operations:\n        _ensure_ax_operation_format(operation)\n        getattr(ax, operation[0])(*operation[1],\n                                  **empty_dict_if_none(operation[2]))"
            },
            {
                "name": "self._get_output",
                "code": "def _get_output(self):\n        return self.ax, self.annotations"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "trevorstephens/gplearn",
        "file_path": "gplearn/genetic.py",
        "function_declaration": "def predict_proba(self, X)",
        "start_line": "1138",
        "end_line": "1168",
        "docstring": "The function predict_proba computes class probabilities for the input data X. It first checks if the model has been fitted by verifying the presence of the '_program' attribute, raising an error if not. The input data is validated and its feature count is checked against the model's expected feature count, raising an error if they do not match. The function then executes the program to obtain scores, transforms these scores into probabilities, and constructs a 2D array with the probabilities for both classes. It returns this array of class probabilities.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b5b17d312c74",
        "ground_truth": "def predict_proba(self, X):\n    \"\"\"Predict probabilities on test vectors X.\n    Parameters\n    ----------\n    X : array-like, shape = [n_samples, n_features]\n        Input vectors, where n_samples is the number of samples\n        and n_features is the number of features.\n    Returns\n    -------\n    proba : array, shape = [n_samples, n_classes]\n        The class probabilities of the input samples. The order of the\n        classes corresponds to that in the attribute `classes_`.\n    \"\"\"\n    if not hasattr(self, '_program'):\n        raise NotFittedError('SymbolicClassifier not fitted.')\n    X = check_array(X)\n    _, n_features = X.shape\n    if self.n_features_in_ != n_features:\n        raise ValueError('Number of features of the model must match the '\n                         'input. Model n_features is %s and input '\n                         'n_features is %s.'\n                         % (self.n_features_in_, n_features))\n    scores = self._program.execute(X)\n    proba = self._transformer(scores)\n    proba = np.vstack([1 - proba, proba]).T\n    return proba",
        "import_statements": [
            "import itertools",
            "from abc import ABCMeta, abstractmethod",
            "from time import time",
            "from warnings import warn",
            "from joblib import Parallel, delayed",
            "from scipy.stats import rankdata",
            "from sklearn.base import BaseEstimator",
            "from sklearn.base import RegressorMixin, TransformerMixin, ClassifierMixin",
            "from sklearn.exceptions import NotFittedError",
            "from sklearn.utils import compute_sample_weight",
            "from sklearn.utils.validation import check_array, _check_sample_weight",
            "from sklearn.utils.multiclass import check_classification_targets"
        ],
        "reference_api": [
            "NotFittedError",
            "self._transformer",
            "hasattr",
            "ValueError",
            "execute",
            "np.vstack",
            "check_array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "NotFittedError",
            "check_array",
            "execute",
            "self._transformer",
            "np.vstack"
        ]
    },
    {
        "subclass": "scikit-learn",
        "owner/repo": "trevorstephens/gplearn",
        "file_path": "gplearn/_program.py",
        "function_declaration": "def point_mutation(self, random_state)",
        "start_line": "619",
        "end_line": "666",
        "docstring": "This function performs a point mutation on a program using a given random state.\\nIt creates a copy of the program and identifies nodes to modify based on a uniform random distribution and a probability threshold.\\nFor each node to be mutated, if it is a function, it finds a valid replacement function with the same arity and updates the node.\\nIf the node is a terminal, it replaces it with either a constant or a variable, ensuring valid replacement according to the defined constant range and number of features.\\nFinally, it returns the mutated program and the list of modified node indices.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "83f71c959df7",
        "ground_truth": "def point_mutation(self, random_state):\n    \"\"\"Perform the point mutation operation on the program.\n    Point mutation selects random nodes from the embedded program to be\n    replaced. Terminals are replaced by other terminals and functions are\n    replaced by other functions that require the same number of arguments\n    as the original node. The resulting tree forms an offspring.\n    Parameters\n    ----------\n    random_state : RandomState instance\n        The random number generator.\n    Returns\n    -------\n    program : list\n        The flattened tree representation of the program.\n    \"\"\"\n    program = copy(self.program)\n    # Get the nodes to modify\n    mutate = np.where(random_state.uniform(size=len(program)) <\n                      self.p_point_replace)[0]\n    for node in mutate:\n        if isinstance(program[node], _Function):\n            arity = program[node].arity\n            # Find a valid replacement with same arity\n            replacement = len(self.arities[arity])\n            replacement = random_state.randint(replacement)\n            replacement = self.arities[arity][replacement]\n            program[node] = replacement\n        else:\n            # We've got a terminal, add a const or variable\n            if self.const_range is not None:\n                terminal = random_state.randint(self.n_features + 1)\n            else:\n                terminal = random_state.randint(self.n_features)\n            if terminal == self.n_features:\n                terminal = random_state.uniform(*self.const_range)\n                if self.const_range is None:\n                    # We should never get here\n                    raise ValueError('A constant was produced with '\n                                     'const_range=None.')\n            program[node] = terminal\n    return program, list(mutate)",
        "import_statements": [
            "from copy import copy",
            "from sklearn.utils.random import sample_without_replacement"
        ],
        "reference_api": [
            "np.where",
            "list",
            "ValueError",
            "len",
            "isinstance",
            "random_state.uniform",
            "random_state.randint",
            "copy"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.where",
            "random_state.uniform",
            "random_state.randint",
            "random_state.randint",
            "random_state.randint",
            "random_state.uniform"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "twopirllc/pandas-ta",
        "file_path": "pandas_ta/momentum/bias.py",
        "function_declaration": "def bias(close, length=None, mamode=None, offset=None, **kwargs)",
        "start_line": "6",
        "end_line": "34",
        "docstring": "The function bias calculates the Bias (BIAS) indicator for a given series of closing prices. It validates and sets default values for length, moving average mode, and offset. It verifies the input series and calculates the moving average based on the specified mode. The BIAS is then computed as the relative difference between the closing prices and the moving average. The result can be shifted by the offset and filled with specified methods if provided. The function names the resulting series and categorizes it as a momentum indicator before returning it.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f1853251e671",
        "ground_truth": "def bias(close, length=None, mamode=None, offset=None, **kwargs):\n    \"\"\"Indicator: Bias (BIAS)\"\"\"\n    # Validate Arguments\n    length = int(length) if length and length > 0 else 26\n    mamode = mamode if isinstance(mamode, str) else \"sma\"\n    close = verify_series(close, length)\n    offset = get_offset(offset)\n     if close is None: return\n     # Calculate Result\n    bma = ma(mamode, close, length=length, **kwargs)\n    bias = (close / bma) - 1\n     # Offset\n    if offset != 0:\n        bias = bias.shift(offset)\n     # Handle fills\n    if \"fillna\" in kwargs:\n        bias.fillna(kwargs[\"fillna\"], inplace=True)\n    if \"fill_method\" in kwargs:\n        bias.fillna(method=kwargs[\"fill_method\"], inplace=True)\n     # Name and Categorize it\n    bias.name = f\"BIAS_{bma.name}\"\n    bias.category = \"momentum\"\n     return bias",
        "import_statements": [
            "from pandas_ta.overlap import ma",
            "from pandas_ta.utils import get_offset, verify_series"
        ],
        "reference_api": [
            "bias.shift",
            "int",
            "isinstance",
            "bias.fillna",
            "get_offset",
            "verify_series",
            "ma"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "verify_series",
            "get_offset",
            "ma",
            "bias.shift",
            "bias.fillna",
            "bias.fillna"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "waditu/tushare",
        "file_path": "tushare/coins/market.py",
        "function_declaration": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000')",
        "start_line": "152",
        "end_line": "181",
        "docstring": "This function retrieves cryptocurrency data from a specified broker.\\nIt formats the URL based on the broker, cryptocurrency code, kline type, and size.\\nIt fetches the data and handles any broker-specific data structures.\\nThe data is then converted into a pandas DataFrame with specific columns.\\nDate formatting is adjusted based on the broker and kline type.\\nFinally, it converts the date column to datetime format and returns the DataFrame.\\nIf an exception occurs, it prints the traceback.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "77825be37c24",
        "ground_truth": "def coins_bar(broker='hb', code='btc', ktype='D', size='2000'):\n    \"\"\"\n            \u83b7\u53d6\u5404\u7c7bk\u7ebf\u6570\u636e\n    params:\n    broker:hb,ok,chbtc\n    code:btc,ltc,eth,etc,bcc\n    ktype:D,W,M,1min,5min,15min,30min,60min\n    size:<2000\n    return DataFrame: \u65e5\u671f\u65f6\u95f4\uff0c\u5f00\u76d8\u4ef7\uff0c\u6700\u9ad8\u4ef7\uff0c\u6700\u4f4e\u4ef7\uff0c\u6536\u76d8\u4ef7\uff0c\u6210\u4ea4\u91cf\n    \"\"\"\n    try:\n        js = _get_data(URL[broker]['kline'] % (code, KTYPES[ktype.strip().upper()][broker], size))\n        if js is None:\n            return js\n        if broker == 'chbtc':\n            js = js['data']\n        df = pd.DataFrame(js, columns=['DATE', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOL'])\n        if broker == 'hb':\n            if ktype.strip().upper() in ['D', 'W', 'M']:\n                df['DATE'] = df['DATE'].apply(lambda x: x[0:8])\n            else:\n                df['DATE'] = df['DATE'].apply(lambda x: x[0:12])\n        else:\n            df['DATE'] = df['DATE'].apply(lambda x: int2time(x / 1000))\n        if ktype.strip().upper() in ['D', 'W', 'M']:\n            df['DATE'] = df['DATE'].apply(lambda x: str(x)[0:10])\n        df['DATE'] = pd.to_datetime(df['DATE'])\n        return df\n    except Exception:\n        print(traceback.print_exc())",
        "import_statements": [
            "import traceback",
            "import time",
            "import json"
        ],
        "reference_api": [
            "print",
            "_get_data",
            "apply",
            "int2time",
            "pd.to_datetime",
            "ktype.strip",
            "pd.DataFrame",
            "str",
            "upper",
            "traceback.print_exc"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_get_data",
                "code": "(lines.decode('GBK'))\n        return js\n    except Exception:\n        print(traceback.print_exc())\n\n\ndef int2time(timestamp):\n    value = time.localtime(timestamp)\n    dt = time.strftime('%Y-%m-%d %H:%M:%S', value)\n    return dt\n\n"
            }
        ],
        "third_party": [
            "upper",
            "ktype.strip",
            "pd.DataFrame",
            "upper",
            "ktype.strip",
            "apply",
            "apply",
            "apply",
            "int2time",
            "upper",
            "ktype.strip",
            "apply",
            "pd.to_datetime"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "waditu/tushare",
        "file_path": "tushare/coins/market.py",
        "function_declaration": "def coins_snapshot(broker='hb', code='btc', size='5')",
        "start_line": "184",
        "end_line": "213",
        "docstring": "The function coins_snapshot retrieves and processes snapshot data for a specified cryptocurrency from a given broker. It attempts to fetch data using a helper function and processes the timestamp differently based on the broker. The function converts the ask and bid data into dataframes, adds the timestamp to each dataframe, and stores them in a dictionary. It then creates and returns a pandas Panel containing the ask and bid data. If an exception occurs, it prints the traceback.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c6552851da8a",
        "ground_truth": "def coins_snapshot(broker='hb', code='btc', size='5'):\n    \"\"\"\n            \u83b7\u53d6\u5b9e\u65f6\u5feb\u7167\u6570\u636e\n    params:\n    broker:hb,ok,chbtc\n    code:btc,ltc,eth,etc,bcc\n    size:<150\n    return Panel: asks,bids\n    \"\"\"\n    try:\n        js = _get_data(URL[broker]['snapshot'] % (code, size))\n        if js is None:\n            return js\n        if broker == 'hb':\n            timestr = js['ts']\n            timestr = int2time(timestr / 1000)\n        if broker == 'ok':\n            timestr = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n        if broker == 'chbtc':\n            timestr = js['timestamp']\n            timestr = int2time(timestr)\n        asks = pd.DataFrame(js['asks'], columns = ['price', 'vol'])\n        bids = pd.DataFrame(js['bids'], columns = ['price', 'vol'])\n        asks['time'] = timestr\n        bids['time'] = timestr\n        djs = {\"asks\": asks, \"bids\": bids}\n        pf = pd.Panel(djs)\n        return pf\n    except Exception:\n        print(traceback.print_exc())",
        "import_statements": [
            "import traceback",
            "import time",
            "import json"
        ],
        "reference_api": [
            "print",
            "time.strftime",
            "_get_data",
            "time.localtime",
            "int2time",
            "pd.Panel",
            "pd.DataFrame",
            "traceback.print_exc"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_get_data",
                "code": "(lines.decode('GBK'))\n        return js\n    except Exception:\n        print(traceback.print_exc())\n\n\ndef int2time(timestamp):\n    value = time.localtime(timestamp)\n    dt = time.strftime('%Y-%m-%d %H:%M:%S', value)\n    return dt\n\n"
            }
        ],
        "third_party": [
            "int2time",
            "int2time",
            "pd.DataFrame",
            "pd.DataFrame",
            "pd.Panel"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "waditu/tushare",
        "file_path": "tushare/coins/market.py",
        "function_declaration": "def coins_trade(broker='hb', code='btc')",
        "start_line": "216",
        "end_line": "249",
        "docstring": "This function retrieves and processes trade data for a specified cryptocurrency from a given broker.\\nIt fetches data from a URL based on the broker and cryptocurrency code.\\nIf the data is not None, it processes the data differently depending on the broker ('hb', 'ok', 'chbtc').\\nFor 'hb', it selects and renames specific columns and converts timestamps.\\nFor 'ok' and 'chbtc', it similarly selects, renames columns, and converts timestamps.\\nFinally, it standardizes the column names to ['tid', 'datetime', 'price', 'amount', 'type'] and returns the processed Data",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8b5fee723398",
        "ground_truth": "def coins_trade(broker='hb', code='btc'):\n    \"\"\"\n    \u83b7\u53d6\u5b9e\u65f6\u4ea4\u6613\u6570\u636e\n    params:\n    -------------\n    broker: hb,ok,chbtc\n    code:btc,ltc,eth,etc,bcc\n         return:\n    ---------------\n    DataFrame\n    'tid':order id\n    'datetime', date time \n    'price' : trade price\n    'amount' : trade amount\n    'type' : buy or sell\n    \"\"\"\n    js = _get_data(URL[broker]['tick'] % code)\n    if js is None:\n        return js\n    if broker == 'hb':\n        df = pd.DataFrame(js['trades'])\n        df = df[['id', 'ts', 'price', 'amount', 'direction']]\n        df['ts'] = df['ts'].apply(lambda x: int2time(x / 1000))\n    if broker == 'ok':\n        df = pd.DataFrame(js)\n        df = df[['tid', 'date_ms', 'price', 'amount', 'type']]\n        df['date_ms'] = df['date_ms'].apply(lambda x: int2time(x / 1000))\n    if broker == 'chbtc':\n        df = pd.DataFrame(js)\n        df = df[['tid', 'date', 'price', 'amount', 'type']]\n        df['date'] = df['date'].apply(lambda x: int2time(x))\n    df.columns = ['tid', 'datetime', 'price', 'amount', 'type']\n    return df",
        "import_statements": [
            "import traceback",
            "import time",
            "import json"
        ],
        "reference_api": [
            "_get_data",
            "pd.DataFrame",
            "int2time",
            "apply"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_get_data",
                "code": "(lines.decode('GBK'))\n        return js\n    except Exception:\n        print(traceback.print_exc())\n\n\ndef int2time(timestamp):\n    value = time.localtime(timestamp)\n    dt = time.strftime('%Y-%m-%d %H:%M:%S', value)\n    return dt\n\n"
            }
        ],
        "third_party": [
            "pd.DataFrame",
            "apply",
            "int2time",
            "pd.DataFrame",
            "apply",
            "int2time",
            "pd.DataFrame",
            "apply",
            "int2time"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "waditu/tushare",
        "file_path": "tushare/fund/nav.py",
        "function_declaration": "def _parse_fund_data(url, fund_type='open')",
        "start_line": "281",
        "end_line": "307",
        "docstring": "The function _parse_fund_data retrieves and processes fund data from a given URL. It writes to the console and attempts to request the URL. The function reads the response, handles text encoding, and extracts relevant data. It formats the text into valid JSON and loads it into a pandas DataFrame with specific columns based on the fund type. The DataFrame is then filled with zeros for any missing values and returned. If an error occurs, the function prints the error message.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "25e0a134c45b",
        "ground_truth": "def _parse_fund_data(url, fund_type='open'):\n     ct._write_console()\n     try:\n        request = Request(url)\n         text = urlopen(request, timeout=10).read()\n        if text == 'null':\n            return None\n        text = text.decode('gbk') if ct.PY3 else text\n        text = text.split('data:')[1].split(',exec_time')[0]\n        reg = re.compile(r'\\,(.*?)\\:')\n        text = reg.sub(r',\"\\1\":', text)\n        text = text.replace('\"{symbol', '{\"symbol')\n        text = text.replace('{symbol', '{\"symbol\"')\n        if ct.PY3:\n            jstr = json.dumps(text)\n        else:\n            jstr = json.dumps(text, encoding='gbk')\n        org_js = json.loads(jstr)\n        fund_df = pd.DataFrame(pd.read_json(org_js, dtype={'symbol': object}),\n                               columns=ct.NAV_COLUMNS[fund_type])\n        fund_df.fillna(0, inplace=True)\n        return fund_df\n    except Exception as er:\n        print(str(er))",
        "import_statements": [
            "import time",
            "import json",
            "import re",
            "from tushare.fund import cons as ct",
            "from tushare.util import dateu as du"
        ],
        "reference_api": [
            "print",
            "urlopen",
            "ct._write_console",
            "json.loads",
            "read",
            "fund_df.fillna",
            "text.decode",
            "reg.sub",
            "str",
            "split",
            "text.replace",
            "pd.read_json",
            "Request",
            "text.split",
            "json.dumps",
            "re.compile",
            "pd.DataFrame"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ct._write_console",
            "Request",
            "read",
            "urlopen",
            "text.decode",
            "split",
            "text.split",
            "reg.sub",
            "text.replace",
            "text.replace",
            "pd.DataFrame",
            "pd.read_json",
            "fund_df.fillna"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "waditu/tushare",
        "file_path": "tushare/futures/domestic.py",
        "function_declaration": "def get_future_daily(start = None, end = None, market = 'CFFEX')",
        "start_line": "401",
        "end_line": "450",
        "docstring": "This function retrieves daily future market data for a specified market and date range.\\nIt determines the appropriate function to call based on the market parameter, defaulting to 'CFFEX'.\\nThe start and end dates are converted to the correct format, defaulting to today's date if not provided.\\nIt initializes an empty list to hold data frames.\\nIt iterates over each date in the range, retrieving daily data using the appropriate function and appending it to the list if data is found.\\nFinally, it concatenates and returns the list of data frames if any data was collected.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "401d18e122d8",
        "ground_truth": "def get_future_daily(start = None, end = None, market = 'CFFEX'):\n    \"\"\"\n        \u83b7\u53d6\u4e2d\u91d1\u6240\u65e5\u4ea4\u6613\u6570\u636e\n    Parameters\n    ------\n        start: \u5f00\u59cb\u65e5\u671f format\uff1aYYYY-MM-DD \u6216 YYYYMMDD \u6216 datetime.date\u5bf9\u8c61 \u4e3a\u7a7a\u65f6\u4e3a\u5f53\u5929\n        end: \u7ed3\u675f\u6570\u636e format\uff1aYYYY-MM-DD \u6216 YYYYMMDD \u6216 datetime.date\u5bf9\u8c61 \u4e3a\u7a7a\u65f6\u4e3a\u5f53\u5929\n        market: 'CFFEX' \u4e2d\u91d1\u6240, 'CZCE' \u90d1\u5546\u6240,  'SHFE' \u4e0a\u671f\u6240, 'DCE' \u5927\u5546\u6240 \u4e4b\u4e00\u3002\u9ed8\u8ba4\u4e3a\u4e2d\u91d1\u6240 \n    Return\n    -------\n        DataFrame\n            \u4e2d\u91d1\u6240\u65e5\u4ea4\u6613\u6570\u636e(DataFrame):\n                symbol      \u5408\u7ea6\u4ee3\u7801\n                date       \u65e5\u671f\n                open       \u5f00\u76d8\u4ef7\n                high       \u6700\u9ad8\u4ef7\n                low       \u6700\u4f4e\u4ef7\n                close      \u6536\u76d8\u4ef7\n                volume      \u6210\u4ea4\u91cf\n                open_interest \u6301\u4ed3\u91cf\n                turnover    \u6210\u4ea4\u989d\n                settle     \u7ed3\u7b97\u4ef7\n                pre_settle   \u524d\u7ed3\u7b97\u4ef7\n                variety     \u5408\u7ea6\u7c7b\u522b\n        \u6216 None(\u7ed9\u5b9a\u65e5\u671f\u6ca1\u6709\u4ea4\u6613\u6570\u636e)\n    \"\"\"\n    if market.upper() == 'CFFEX':\n        f = get_cffex_daily\n    elif market.upper() == 'CZCE':\n        f = get_czce_daily\n    elif market.upper() == 'SHFE':\n        f = get_shfe_daily\n    elif market.upper() == 'DCE':\n        f = get_dce_daily\n    else:\n        print('Invalid market.')\n        return\n         start = ct.convert_date(start) if start is not None else datetime.date.today()\n    end = ct.convert_date(end) if end is not None else datetime.date.today()\n     df_list = list()\n    while start <= end:\n        df = f(start)\n        if df is not None:\n            df_list.append(df)\n        start += datetime.timedelta(days = 1)\n     if len(df_list) > 0:\n        return pd.concat(df_list)",
        "import_statements": [
            "import json",
            "import datetime",
            "from bs4 import BeautifulSoup",
            "from tushare.futures import domestic_cons as ct"
        ],
        "reference_api": [
            "print",
            "list",
            "pd.concat",
            "df_list.append",
            "len",
            "f",
            "market.upper",
            "datetime.timedelta",
            "today",
            "ct.convert_date"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "market.upper",
            "market.upper",
            "market.upper",
            "market.upper",
            "ct.convert_date",
            "today",
            "ct.convert_date",
            "today",
            "f",
            "df_list.append",
            "pd.concat"
        ]
    },
    {
        "subclass": "pandas",
        "owner/repo": "waditu/tushare",
        "file_path": "tushare/internet/boxoffice.py",
        "function_declaration": "def realtime_boxoffice(retry_count=3,pause=0.001)",
        "start_line": "19",
        "end_line": "55",
        "docstring": "The function realtime_boxoffice fetches real-time box office data with retries and pauses between attempts. It tries up to retry_count times, pausing for a specified duration each time. During each attempt, it sends a request to a specified URL and reads the response. If the response contains data, it parses the JSON, creates a DataFrame from the 'data2' field, drops the 'MovieImg' and 'mId' columns, adds a timestamp column, and returns the DataFrame. If an error occurs or no data is returned, it retries until the retry limit is reached.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "697779825988",
        "ground_truth": "def realtime_boxoffice(retry_count=3,pause=0.001):\n    \"\"\"\n    \u83b7\u53d6\u5b9e\u65f6\u7535\u5f71\u7968\u623f\u6570\u636e\n    \u6570\u636e\u6765\u6e90\uff1aEBOT\u827a\u6069\u7968\u623f\u667a\u5e93\n    Parameters\n    ------\n        retry_count : int, \u9ed8\u8ba4 3\n                  \u5982\u9047\u7f51\u7edc\u7b49\u95ee\u9898\u91cd\u590d\u6267\u884c\u7684\u6b21\u6570\n        pause : int, \u9ed8\u8ba4 0\n                 \u91cd\u590d\u8bf7\u6c42\u6570\u636e\u8fc7\u7a0b\u4e2d\u6682\u505c\u7684\u79d2\u6570\uff0c\u9632\u6b62\u8bf7\u6c42\u95f4\u9694\u65f6\u95f4\u592a\u77ed\u51fa\u73b0\u7684\u95ee\u9898\n     return\n     -------\n        DataFrame \n              BoxOffice     \u5b9e\u65f6\u7968\u623f\uff08\u4e07\uff09 \n              Irank         \u6392\u540d\n              MovieName     \u5f71\u7247\u540d \n              boxPer        \u7968\u623f\u5360\u6bd4 \uff08%\uff09\n              movieDay      \u4e0a\u6620\u5929\u6570\n              sumBoxOffice  \u7d2f\u8ba1\u7968\u623f\uff08\u4e07\uff09 \n              time          \u6570\u636e\u83b7\u53d6\u65f6\u95f4\n    \"\"\"\n    for _ in range(retry_count):\n        time.sleep(pause)\n        try:\n            request = Request(ct.MOVIE_BOX%(ct.P_TYPE['http'], ct.DOMAINS['mbox'],\n                              ct.BOX, _random()))\n            lines = urlopen(request, timeout = 10).read()\n            if len(lines) < 15: #no data\n                return None\n        except Exception as e:\n            print(e)\n        else:\n            js = json.loads(lines.decode('utf-8') if ct.PY3 else lines)\n            df = pd.DataFrame(js['data2'])\n            df = df.drop(['MovieImg','mId'], axis=1)\n            df['time'] = du.get_now()\n            return df",
        "import_statements": [
            "from tushare.stock import cons as ct",
            "from tushare.util import dateu as du",
            "import time",
            "import json"
        ],
        "reference_api": [
            "print",
            "urlopen",
            "df.drop",
            "json.loads",
            "read",
            "len",
            "pd.DataFrame",
            "du.get_now",
            "_random",
            "time.sleep",
            "lines.decode",
            "Request",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "Request",
            "read",
            "urlopen",
            "lines.decode",
            "pd.DataFrame",
            "df.drop",
            "du.get_now"
        ]
    }
]