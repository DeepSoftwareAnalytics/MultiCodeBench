[
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def load_model_from_config(self, half_attention)",
        "start_line": "22",
        "end_line": "52",
        "file_path": "extensions-builtin/LDSR/ldsr_model_arch.py",
        "docstring": "This function loads a model from a configuration file, with an option to load from a cache if available. It first checks if a cached model is available and valid, loading it if so. Otherwise, it loads the model from the specified path, handling different file formats (.safetensors or other PyTorch-compatible formats). The model is configured using settings from a YAML file and moved to the appropriate device. Optional settings include half-precision attention and memory format optimization. The function ensures the model is in evaluation mode and optionally caches the loaded model for future use. The function returns a dictionary containing the loaded model.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1f9f4d85e825",
        "ground_truth": "def load_model_from_config(self, half_attention):\n    global cached_ldsr_model\n    if shared.opts.ldsr_cached and cached_ldsr_model is not None:\n        print(\"Loading model from cache\")\n        model: torch.nn.Module = cached_ldsr_model\n    else:\n        print(f\"Loading model from {self.modelPath}\")\n        _, extension = os.path.splitext(self.modelPath)\n        if extension.lower() == \".safetensors\":\n            pl_sd = safetensors.torch.load_file(self.modelPath, device=\"cpu\")\n        else:\n            pl_sd = torch.load(self.modelPath, map_location=\"cpu\")\n        sd = pl_sd[\"state_dict\"] if \"state_dict\" in pl_sd else pl_sd\n        config = OmegaConf.load(self.yamlPath)\n        config.model.target = \"ldm.models.diffusion.ddpm.LatentDiffusionV1\"\n        model: torch.nn.Module = instantiate_from_config(config.model)\n        model.load_state_dict(sd, strict=False)\n        model = model.to(shared.device)\n        if half_attention:\n            model = model.half()\n        if shared.cmd_opts.opt_channelslast:\n            model = model.to(memory_format=torch.channels_last)\n        sd_hijack.model_hijack.hijack(model) # apply optimization\n        model.eval()\n        if shared.opts.ldsr_cached:\n            cached_ldsr_model = model\n    return {\"model\": model}",
        "import_statements": [
            "import os",
            "import gc",
            "import time",
            "import torch",
            "import torchvision",
            "from PIL import Image",
            "from einops import rearrange, repeat",
            "from omegaconf import OmegaConf",
            "import safetensors.torch",
            "from ldm.models.diffusion.ddim import DDIMSampler",
            "from ldm.util import instantiate_from_config, ismap",
            "from modules import shared, sd_hijack, devices"
        ],
        "reference_api": [
            "hijack",
            "print",
            "torch.load",
            "instantiate_from_config",
            "model.eval",
            "model.half",
            "model.to",
            "OmegaConf.load",
            "extension.lower",
            "model.load_state_dict",
            "load_file",
            "splitext"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "splitext",
            "extension.lower",
            "load_file",
            "torch.load",
            "OmegaConf.load",
            "instantiate_from_config",
            "model.load_state_dict",
            "model.to",
            "model.half",
            "model.to",
            "hijack",
            "model.eval"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    )",
        "start_line": "176",
        "end_line": "190",
        "file_path": "extensions-builtin/LDSR/ldsr_model_arch.py",
        "docstring": "# This function performs conditional sampling using the DDIM (Denoising Diffusion Implicit Models) sampler. It initializes the sampler with the provided model, extracts the batch size from the shape parameter, and prints the eta and steps values. The function then calls the sample method of the DDIMSampler instance, passing several parameters such as steps, batch size, shape, conditioning, eta, and other optional arguments like callback, normals_sequence, mask, x0, quantize_x0, temperature, score_corrector, corrector_kwargs, and x_t. Finally, it returns the generated samples and intermediate values.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8dc955007aa2",
        "ground_truth": "def convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    ):\n    ddim = DDIMSampler(model)\n    bs = shape[0]\n    shape = shape[1:]\n    print(f\"Sampling with eta = {eta}; steps: {steps}\")\n    samples, intermediates = ddim.sample(steps, batch_size=bs, shape=shape, conditioning=cond, callback=callback,\n                                         normals_sequence=normals_sequence, quantize_x0=quantize_x0, eta=eta,\n                                         mask=mask, x0=x0, temperature=temperature, verbose=False,\n                                         score_corrector=score_corrector,\n                                         corrector_kwargs=corrector_kwargs, x_t=x_t)\n     return samples, intermediates",
        "import_statements": [
            "import os",
            "import gc",
            "import time",
            "import torch",
            "import torchvision",
            "from PIL import Image",
            "from einops import rearrange, repeat",
            "from omegaconf import OmegaConf",
            "import safetensors.torch",
            "from ldm.models.diffusion.ddim import DDIMSampler",
            "from ldm.util import instantiate_from_config, ismap",
            "from modules import shared, sd_hijack, devices"
        ],
        "reference_api": [
            "DDIMSampler",
            "print",
            "ddim.sample"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "DDIMSampler",
            "ddim.sample"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def ema_scope(self, context=None)",
        "start_line": "70",
        "end_line": "82",
        "file_path": "extensions-builtin/LDSR/sd_hijack_autoencoder.py",
        "docstring": "This function manages the use of Exponential Moving Average (EMA) weights in a machine learning model.\\nWhen EMA is enabled, it temporarily replaces the model's weights with EMA weights and restores the original weights afterward.\\nIf a context is provided, it prints messages indicating when weights are switched and restored.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9bdcd9081480",
        "ground_truth": "def ema_scope(self, context=None):\n    if self.use_ema:\n        self.model_ema.store(self.parameters())\n        self.model_ema.copy_to(self)\n        if context is not None:\n            print(f\"{context}: Switched to EMA weights\")\n    try:\n        yield None\n    finally:\n        if self.use_ema:\n            self.model_ema.restore(self.parameters())\n            if context is not None:\n                print(f\"{context}: Restored training weights\")",
        "import_statements": [
            "import torch",
            "from contextlib import contextmanager",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from ldm.modules.ema import LitEma",
            "from vqvae_quantize import VectorQuantizer2 as VectorQuantizer",
            "from ldm.modules.diffusionmodules.model import Encoder, Decoder",
            "from ldm.util import instantiate_from_config",
            "import ldm.models.autoencoder",
            "from packaging import version"
        ],
        "reference_api": [
            "print",
            "restore",
            "store",
            "copy_to",
            "self.parameters"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "store",
            "self.parameters",
            "copy_to",
            "restore",
            "self.parameters"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def init_from_ckpt(self, path, ignore_keys=None)",
        "start_line": "84",
        "end_line": "97",
        "file_path": "extensions-builtin/LDSR/sd_hijack_autoencoder.py",
        "docstring": "This function loads a model's state dictionary from a checkpoint file at the specified path, optionally ignoring specified keys.\\nIt first loads the state dictionary from the checkpoint and then deletes any keys that match the ignore_keys list.\\nThe function then updates the model's state with the loaded state dictionary, printing out any missing or unexpected keys after the update.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "14ab009e1c5c",
        "ground_truth": "def init_from_ckpt(self, path, ignore_keys=None):\n    sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n    keys = list(sd.keys())\n    for k in keys:\n        for ik in ignore_keys or []:\n            if k.startswith(ik):\n                print(\"Deleting key {} from state_dict.\".format(k))\n                del sd[k]\n    missing, unexpected = self.load_state_dict(sd, strict=False)\n    print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n    if missing:\n        print(f\"Missing Keys: {missing}\")\n    if unexpected:\n        print(f\"Unexpected Keys: {unexpected}\")",
        "import_statements": [
            "import torch",
            "from contextlib import contextmanager",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from ldm.modules.ema import LitEma",
            "from vqvae_quantize import VectorQuantizer2 as VectorQuantizer",
            "from ldm.modules.diffusionmodules.model import Encoder, Decoder",
            "from ldm.util import instantiate_from_config",
            "import ldm.models.autoencoder",
            "from packaging import version"
        ],
        "reference_api": [
            "print",
            "torch.load",
            "list",
            "self.load_state_dict",
            "k.startswith",
            "len",
            "sd.keys",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "torch.load",
            "sd.keys",
            "k.startswith",
            "self.load_state_dict"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def forward(self, input, return_pred_indices=False)",
        "start_line": "124",
        "end_line": "129",
        "file_path": "extensions-builtin/LDSR/sd_hijack_autoencoder.py",
        "docstring": "This function processes an input through encoding and decoding steps, returning the decoded output and a difference metric.\\nOptionally, it can also return prediction indices if specified.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "754176e27b81",
        "ground_truth": "def forward(self, input, return_pred_indices=False):\n    quant, diff, (_,_,ind) = self.encode(input)\n    dec = self.decode(quant)\n    if return_pred_indices:\n        return dec, diff, ind\n    return dec, diff",
        "import_statements": [
            "import torch",
            "from contextlib import contextmanager",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from ldm.modules.ema import LitEma",
            "from vqvae_quantize import VectorQuantizer2 as VectorQuantizer",
            "from ldm.modules.diffusionmodules.model import Encoder, Decoder",
            "from ldm.util import instantiate_from_config",
            "import ldm.models.autoencoder",
            "from packaging import version"
        ],
        "reference_api": [
            "self.encode",
            "self.decode"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.encode",
                "code": "def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info"
            },
            {
                "name": "self.decode",
                "code": "def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def get_input(self, batch, k)",
        "start_line": "131",
        "end_line": "147",
        "file_path": "extensions-builtin/LDSR/sd_hijack_autoencoder.py",
        "docstring": "This function processes a batch of input data by selecting a specific key from the batch and adjusting its dimensions if necessary.\\nIt ensures the input has four dimensions, permutes the axes, and converts the data to a contiguous float tensor.\\nIf batch resizing is enabled, it adjusts the size of the input based on the current training step and a specified range, using bicubic interpolation.\\nFinally, it detaches the tensor from the computation graph and returns it.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "443df6fdaf0d",
        "ground_truth": "def get_input(self, batch, k):\n    x = batch[k]\n    if len(x.shape) == 3:\n        x = x[..., None]\n    x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n    if self.batch_resize_range is not None:\n        lower_size = self.batch_resize_range[0]\n        upper_size = self.batch_resize_range[1]\n        if self.global_step <= 4:\n            # do the first few batches with max size to avoid later oom\n            new_resize = upper_size\n        else:\n            new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n        if new_resize != x.shape[2]:\n            x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n        x = x.detach()\n    return x",
        "import_statements": [
            "import torch",
            "from contextlib import contextmanager",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from ldm.modules.ema import LitEma",
            "from vqvae_quantize import VectorQuantizer2 as VectorQuantizer",
            "from ldm.modules.diffusionmodules.model import Encoder, Decoder",
            "from ldm.util import instantiate_from_config",
            "import ldm.models.autoencoder",
            "from packaging import version"
        ],
        "reference_api": [
            "choice",
            "float",
            "x.permute",
            "len",
            "to",
            "F.interpolate",
            "np.arange",
            "x.detach"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "to",
            "x.permute",
            "choice",
            "np.arange",
            "F.interpolate",
            "x.detach"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def training_step(self, batch, batch_idx, optimizer_idx)",
        "start_line": "149",
        "end_line": "169",
        "file_path": "extensions-builtin/LDSR/sd_hijack_autoencoder.py",
        "docstring": "This function performs a training step for a model, processing a batch of data and computing losses for different optimizers.\\nIt first retrieves the input data and predictions, then computes either the autoencoding loss or the discriminator loss based on the optimizer index.\\nThe function logs relevant metrics and returns the computed loss for further optimization.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "953a2632fc15",
        "ground_truth": "def training_step(self, batch, batch_idx, optimizer_idx):\n    # https://github.com/pytorch/pytorch/issues/37142\n    # try not to fool the heuristics\n    x = self.get_input(batch, self.image_key)\n    xrec, qloss, ind = self(x, return_pred_indices=True)\n    if optimizer_idx == 0:\n        # autoencode\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                        last_layer=self.get_last_layer(), split=\"train\",\n                                        predicted_indices=ind)\n        self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        return aeloss\n    if optimizer_idx == 1:\n        # discriminator\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                        last_layer=self.get_last_layer(), split=\"train\")\n        self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n        return discloss",
        "import_statements": [
            "import torch",
            "from contextlib import contextmanager",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from ldm.modules.ema import LitEma",
            "from vqvae_quantize import VectorQuantizer2 as VectorQuantizer",
            "from ldm.modules.diffusionmodules.model import Encoder, Decoder",
            "from ldm.util import instantiate_from_config",
            "import ldm.models.autoencoder",
            "from packaging import version"
        ],
        "reference_api": [
            "self.get_input",
            "self.loss",
            "self",
            "self.log_dict",
            "self.get_last_layer"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.get_input",
                "code": "def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n            x = x.detach()\n        return x"
            },
            {
                "name": "self.get_last_layer",
                "code": "def get_last_layer(self):\n        return self.decoder.conv_out.weight"
            },
            {
                "name": "self.get_last_layer",
                "code": "def get_last_layer(self):\n        return self.decoder.conv_out.weight"
            }
        ],
        "third_party": [
            "self",
            "self.loss",
            "self.log_dict",
            "self.loss",
            "self.log_dict"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs)",
        "start_line": "240",
        "end_line": "261",
        "file_path": "extensions-builtin/LDSR/sd_hijack_autoencoder.py",
        "docstring": "This function logs images during model training, including inputs and reconstructions, and optionally uses Exponential Moving Average (EMA) weights.\\nIt processes input data, logs either just the inputs or both inputs and reconstructions, and applies colorization for multi-channel images if needed.\\nIf EMA logging is enabled, it includes reconstructions using EMA weights in the log.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b449b5e8b77f",
        "ground_truth": "def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n    log = {}\n    x = self.get_input(batch, self.image_key)\n    x = x.to(self.device)\n    if only_inputs:\n        log[\"inputs\"] = x\n        return log\n    xrec, _ = self(x)\n    if x.shape[1] > 3:\n        # colorize with random projection\n        assert xrec.shape[1] > 3\n        x = self.to_rgb(x)\n        xrec = self.to_rgb(xrec)\n    log[\"inputs\"] = x\n    log[\"reconstructions\"] = xrec\n    if plot_ema:\n        with self.ema_scope():\n            xrec_ema, _ = self(x)\n            if x.shape[1] > 3:\n                xrec_ema = self.to_rgb(xrec_ema)\n            log[\"reconstructions_ema\"] = xrec_ema\n    return log",
        "import_statements": [
            "import torch",
            "from contextlib import contextmanager",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from ldm.modules.ema import LitEma",
            "from vqvae_quantize import VectorQuantizer2 as VectorQuantizer",
            "from ldm.modules.diffusionmodules.model import Encoder, Decoder",
            "from ldm.util import instantiate_from_config",
            "import ldm.models.autoencoder",
            "from packaging import version"
        ],
        "reference_api": [
            "self.get_input",
            "x.to",
            "self",
            "self.ema_scope",
            "self.to_rgb"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.get_input",
                "code": "def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n            x = x.detach()\n        return x"
            },
            {
                "name": "self.to_rgb",
                "code": "def to_rgb(self, x):\n        assert self.image_key == \"segmentation\"\n        if not hasattr(self, \"colorize\"):\n            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n        return x"
            },
            {
                "name": "self.to_rgb",
                "code": "def to_rgb(self, x):\n        assert self.image_key == \"segmentation\"\n        if not hasattr(self, \"colorize\"):\n            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n        return x"
            },
            {
                "name": "self.to_rgb",
                "code": "def to_rgb(self, x):\n        assert self.image_key == \"segmentation\"\n        if not hasattr(self, \"colorize\"):\n            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n        return x"
            }
        ],
        "third_party": [
            "x.to",
            "self",
            "self.ema_scope",
            "self"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def get_loss(self, pred, target, mean=True)",
        "start_line": "278",
        "end_line": "291",
        "file_path": "extensions-builtin/LDSR/sd_hijack_ddpm_v1.py",
        "docstring": "This function calculates the loss between predicted and target values based on the specified loss type, either 'l1' or 'l2'.\\nFor 'l1' loss, it computes the mean absolute error if specified.\\nFor 'l2' loss, it uses mean squared error, either averaged or not based on the mean parameter.\\nIf an unknown loss type is provided, it raises an error.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "540d089e3481",
        "ground_truth": "def get_loss(self, pred, target, mean=True):\n    if self.loss_type == 'l1':\n        loss = (target - pred).abs()\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        if mean:\n            loss = torch.nn.functional.mse_loss(target, pred)\n        else:\n            loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n    else:\n        raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n    return loss",
        "import_statements": [
            "import torch",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from einops import rearrange, repeat",
            "from contextlib import contextmanager",
            "from functools import partial",
            "from tqdm import tqdm",
            "from torchvision.utils import make_grid",
            "from pytorch_lightning.utilities.distributed import rank_zero_only",
            "from ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config",
            "from ldm.modules.ema import LitEma",
            "from ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution",
            "from ldm.models.autoencoder import VQModelInterface, IdentityFirstStage, AutoencoderKL",
            "from ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like",
            "from ldm.models.diffusion.ddim import DDIMSampler",
            "import ldm.models.diffusion.ddpm"
        ],
        "reference_api": [
            "loss.mean",
            "abs",
            "NotImplementedError",
            "mse_loss"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "loss.mean",
            "mse_loss",
            "mse_loss"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def forward(self, x, c, *args, **kwargs)",
        "start_line": "869",
        "end_line": "878",
        "file_path": "extensions-builtin/LDSR/sd_hijack_ddpm_v1.py",
        "docstring": "This function performs a forward pass for a model, incorporating conditioning and time step information.\\nIt randomly selects time steps, applies learned conditioning if applicable, and processes the inputs according to the model's conditioning requirements.\\nFinally, it computes and returns the loss for the given inputs and conditioning.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "63a08283666f",
        "ground_truth": "def forward(self, x, c, *args, **kwargs):\n    t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n    if self.model.conditioning_key is not None:\n        assert c is not None\n        if self.cond_stage_trainable:\n            c = self.get_learned_conditioning(c)\n        if self.shorten_cond_schedule:  # TODO: drop this option\n            tc = self.cond_ids[t].to(self.device)\n            c = self.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.float()))\n    return self.p_losses(x, c, t, *args, **kwargs)",
        "import_statements": [
            "import torch",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from einops import rearrange, repeat",
            "from contextlib import contextmanager",
            "from functools import partial",
            "from tqdm import tqdm",
            "from torchvision.utils import make_grid",
            "from pytorch_lightning.utilities.distributed import rank_zero_only",
            "from ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config",
            "from ldm.modules.ema import LitEma",
            "from ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution",
            "from ldm.models.autoencoder import VQModelInterface, IdentityFirstStage, AutoencoderKL",
            "from ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like",
            "from ldm.models.diffusion.ddim import DDIMSampler",
            "import ldm.models.diffusion.ddpm"
        ],
        "reference_api": [
            "self.q_sample",
            "to",
            "torch.randn_like",
            "self.get_learned_conditioning",
            "long",
            "c.float",
            "self.p_losses",
            "torch.randint"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.get_learned_conditioning",
                "code": "def get_learned_conditioning(self, c):\n        if self.cond_stage_forward is None:\n            if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n                c = self.cond_stage_model.encode(c)\n                if isinstance(c, DiagonalGaussianDistribution):\n                    c = c.mode()\n            else:\n                c = self.cond_stage_model(c)\n        else:\n            assert hasattr(self.cond_stage_model, self.cond_stage_forward)\n            c = getattr(self.cond_stage_model, self.cond_stage_forward)(c)\n        return c"
            },
            {
                "name": "self.q_sample",
                "code": "def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)"
            },
            {
                "name": "self.p_losses",
                "code": "def p_losses(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        model_out = self.model(x_noisy, t)\n\n        loss_dict = {}\n        if self.parameterization == \"eps\":\n            target = noise\n        elif self.parameterization == \"x0\":\n            target = x_start\n        else:\n            raise NotImplementedError(f\"Parameterization {self.parameterization} not yet supported\")\n\n        loss = self.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3])\n\n        log_prefix = 'train' if self.training else 'val'\n\n        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n        loss_simple = loss.mean() * self.l_simple_weight\n\n        loss_vlb = (self.lvlb_weights[t] * loss).mean()\n        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n\n        loss = loss_simple + self.original_elbo_weight * loss_vlb\n\n        loss_dict.update({f'{log_prefix}/loss': loss})\n\n        return loss, loss_dict"
            }
        ],
        "third_party": [
            "long",
            "torch.randint",
            "to",
            "torch.randn_like",
            "c.float"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def get_codebook_entry(self, indices, shape)",
        "start_line": "132",
        "end_line": "147",
        "file_path": "extensions-builtin/LDSR/vqvae_quantize.py",
        "docstring": "This function retrieves the quantized latent vectors from the codebook based on given indices and reshapes them to match a specified shape.\\nIt handles optional remapping of indices and ensures the output latent vectors are correctly reshaped and permuted to match the original input dimensions.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4627e1e8ed62",
        "ground_truth": "def get_codebook_entry(self, indices, shape):\n    # shape specifying (batch, height, width, channel)\n    if self.remap is not None:\n        indices = indices.reshape(shape[0], -1)  # add batch axis\n        indices = self.unmap_to_all(indices)\n        indices = indices.reshape(-1)  # flatten again\n    # get quantized latent vectors\n    z_q = self.embedding(indices)\n    if shape is not None:\n        z_q = z_q.view(shape)\n        # reshape back to match original input shape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q",
        "import_statements": [
            "import torch",
            "from einops import rearrange"
        ],
        "reference_api": [
            "self.unmap_to_all",
            "indices.reshape",
            "z_q.view",
            "self.embedding",
            "z_q.permute",
            "contiguous"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.unmap_to_all",
                "code": "ef unmap_to_all(self, inds):\n        ishape = inds.shape\n        assert len(ishape) > 1\n        inds = inds.reshape(ishape[0], -1)\n        used = self.used.to(inds)\n        if self.re_embed > self.used.shape[0]:  # extra token\n            inds[inds >= self.used.shape[0]] = 0  # simply set to zero\n        back = torch.gather(used[None, :][inds.shape[0] * [0], :], 1, inds)\n        return back.reshape(ishape)\n"
            }
        ],
        "third_party": [
            "indices.reshape",
            "indices.reshape",
            "self.embedding",
            "z_q.view",
            "contiguous",
            "z_q.permute"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]",
        "start_line": "25",
        "end_line": "67",
        "file_path": "extensions-builtin/Lora/lyco_helpers.py",
        "docstring": "This function computes the factorization of a given dimension into two factors, m and n, with an optional specified factor.\\nIf the factor is positive and divides the dimension, it returns the factor and the quotient, ensuring m is less than or equal to n.\\nIf the factor is negative, it iteratively finds a factorization that minimizes the sum of the factors, while ensuring m is less than or equal to n and does not exceed the specified factor.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c55d014827ef",
        "ground_truth": "def factorization(dimension: int, factor:int=-1) -> tuple[int, int]:\n    '''\n    return a tuple of two value of input dimension decomposed by the number closest to factor\n    second value is higher or equal than first value.\n     In LoRA with Kroneckor Product, first value is a value for weight scale.\n    secon value is a value for weight.\n     Because of non-commutative property, A\u2297B \u2260 B\u2297A. Meaning of two matrices is slightly different.\n     examples)\n    factor\n        -1               2                4               8               16               ...\n    127 -> 1, 127   127 -> 1, 127    127 -> 1, 127   127 -> 1, 127   127 -> 1, 127\n    128 -> 8, 16    128 -> 2, 64     128 -> 4, 32    128 -> 8, 16    128 -> 8, 16\n    250 -> 10, 25   250 -> 2, 125    250 -> 2, 125   250 -> 5, 50    250 -> 10, 25\n    360 -> 8, 45    360 -> 2, 180    360 -> 4, 90    360 -> 8, 45    360 -> 12, 30\n    512 -> 16, 32   512 -> 2, 256    512 -> 4, 128   512 -> 8, 64    512 -> 16, 32\n    1024 -> 32, 32  1024 -> 2, 512   1024 -> 4, 256  1024 -> 8, 128  1024 -> 16, 64\n    '''\n     if factor > 0 and (dimension % factor) == 0:\n        m = factor\n        n = dimension // factor\n        if m > n:\n            n, m = m, n\n        return m, n\n    if factor < 0:\n        factor = dimension\n    m, n = 1, dimension\n    length = m + n\n    while m<n:\n        new_m = m + 1\n        while dimension%new_m != 0:\n            new_m += 1\n        new_n = dimension // new_m\n        if new_m + new_n > length or new_m>factor:\n            break\n        else:\n            m, n = new_m, new_n\n    if m > n:\n        n, m = m, n\n    return m, n",
        "import_statements": [
            "import torch"
        ],
        "reference_api": [],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def apply_weight_decompose(self, updown, orig_weight)",
        "start_line": "171",
        "end_line": "190",
        "file_path": "extensions-builtin/Lora/network.py",
        "docstring": "This function decomposes and adjusts the weight of a model by combining an input weight with an original weight, normalizing the result, and scaling it using a predefined scale factor.\\nThe final adjusted weight is then calculated by subtracting the original weight from the scaled and normalized combined weight.\\nThe function ensures that all tensors are on the same device and have the same data type before performing these operations.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5d613ec26036",
        "ground_truth": "def apply_weight_decompose(self, updown, orig_weight):\n    # Match the device/dtype\n    orig_weight = orig_weight.to(updown.dtype)\n    dora_scale = self.dora_scale.to(device=orig_weight.device, dtype=updown.dtype)\n    updown = updown.to(orig_weight.device)\n    merged_scale1 = updown + orig_weight\n    merged_scale1_norm = (\n        merged_scale1.transpose(0, 1)\n        .reshape(merged_scale1.shape[1], -1)\n        .norm(dim=1, keepdim=True)\n        .reshape(merged_scale1.shape[1], *[1] * self.dora_norm_dims)\n        .transpose(0, 1)\n    )\n    dora_merged = (\n        merged_scale1 * (dora_scale / merged_scale1_norm)\n    )\n    final_updown = dora_merged - orig_weight\n    return final_updown",
        "import_statements": [
            "import os",
            "from collections import namedtuple",
            "import enum",
            "from modules import sd_models, cache, errors, hashes, shared"
        ],
        "reference_api": [
            "updown.to",
            "orig_weight.to",
            "transpose",
            "to",
            "reshape",
            "merged_scale1.transpose",
            "norm"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "orig_weight.to",
            "to",
            "updown.to",
            "transpose",
            "reshape",
            "norm",
            "reshape",
            "merged_scale1.transpose"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None)",
        "start_line": "192",
        "end_line": "210",
        "file_path": "extensions-builtin/Lora/network.py",
        "docstring": "This function finalizes the upscaling or downscaling of weights by adjusting shapes and applying biases, scalars, and decompositions as needed.\\nIt first adjusts the updown tensor with the bias and reshapes it according to the output shape.\\nIf the original and updown weights have the same number of elements, it reshapes updown to match the original weight's shape.\\nIf an additional bias is provided, it applies a multiplier to it.\\nIf a scaling factor is defined, it applies a weight decomposition to updown.\\nFinally, it returns the scaled and adjusted updown tensor along with the modified additional bias.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c73d0d1cd3e5",
        "ground_truth": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None):\n    if self.bias is not None:\n        updown = updown.reshape(self.bias.shape)\n        updown += self.bias.to(orig_weight.device, dtype=updown.dtype)\n        updown = updown.reshape(output_shape)\n    if len(output_shape) == 4:\n        updown = updown.reshape(output_shape)\n    if orig_weight.size().numel() == updown.size().numel():\n        updown = updown.reshape(orig_weight.shape)\n    if ex_bias is not None:\n        ex_bias = ex_bias * self.multiplier()\n    if self.dora_scale is not None:\n        updown = self.apply_weight_decompose(updown, orig_weight)\n    return updown * self.calc_scale() * self.multiplier(), ex_bias",
        "import_statements": [
            "import os",
            "from collections import namedtuple",
            "import enum",
            "from modules import sd_models, cache, errors, hashes, shared"
        ],
        "reference_api": [
            "orig_weight.size",
            "self.calc_scale",
            "numel",
            "len",
            "updown.reshape",
            "to",
            "self.multiplier",
            "updown.size",
            "self.apply_weight_decompose"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.multiplier",
                "code": "def multiplier(self):\n        if 'transformer' in self.sd_key[:20]:\n            return self.network.te_multiplier\n        else:\n            return self.network.unet_multiplier"
            },
            {
                "name": "self.apply_weight_decompose",
                "code": "def apply_weight_decompose(self, updown, orig_weight):\n        # Match the device/dtype\n        orig_weight = orig_weight.to(updown.dtype)\n        dora_scale = self.dora_scale.to(device=orig_weight.device, dtype=updown.dtype)\n        updown = updown.to(orig_weight.device)\n\n        merged_scale1 = updown + orig_weight\n        merged_scale1_norm = (\n            merged_scale1.transpose(0, 1)\n            .reshape(merged_scale1.shape[1], -1)\n            .norm(dim=1, keepdim=True)\n            .reshape(merged_scale1.shape[1], *[1] * self.dora_norm_dims)\n            .transpose(0, 1)\n        )\n\n        dora_merged = (\n            merged_scale1 * (dora_scale / merged_scale1_norm)\n        )\n        final_updown = dora_merged - orig_weight\n        return final_updown"
            },
            {
                "name": "self.calc_scale",
                "code": "def calc_scale(self):\n        if self.scale is not None:\n            return self.scale\n        if self.dim is not None and self.alpha is not None:\n            return self.alpha / self.dim\n\n        return 1.0"
            },
            {
                "name": "self.multiplier",
                "code": "def multiplier(self):\n        if 'transformer' in self.sd_key[:20]:\n            return self.network.te_multiplier\n        else:\n            return self.network.unet_multiplier"
            }
        ],
        "third_party": [
            "updown.reshape",
            "to",
            "updown.reshape",
            "updown.reshape",
            "numel",
            "orig_weight.size",
            "numel",
            "updown.size",
            "updown.reshape"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def calc_updown(self, orig_weight)",
        "start_line": "19",
        "end_line": "27",
        "file_path": "extensions-builtin/Lora/network_full.py",
        "docstring": "This function calculates and returns an updated weight tensor based on the original weight tensor and the current weight tensor.\\nIt transfers the current weight tensor and optional bias to the device of the original weight tensor, then calls another function to finalize the updated weights.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c6a41665c62c",
        "ground_truth": "def calc_updown(self, orig_weight):\n    output_shape = self.weight.shape\n    updown = self.weight.to(orig_weight.device)\n    if self.ex_bias is not None:\n        ex_bias = self.ex_bias.to(orig_weight.device)\n    else:\n        ex_bias = None\n    return self.finalize_updown(updown, orig_weight, output_shape, ex_bias)",
        "import_statements": [
            "import network"
        ],
        "reference_api": [
            "self.finalize_updown",
            "to"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.finalize_updown",
                "code": "def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None):\n        if self.bias is not None:\n            updown = updown.reshape(self.bias.shape)\n            updown += self.bias.to(orig_weight.device, dtype=updown.dtype)\n            updown = updown.reshape(output_shape)\n\n        if len(output_shape) == 4:\n            updown = updown.reshape(output_shape)\n\n        if orig_weight.size().numel() == updown.size().numel():\n            updown = updown.reshape(orig_weight.shape)\n\n        if ex_bias is not None:\n            ex_bias = ex_bias * self.multiplier()\n\n        if self.dora_scale is not None:\n            updown = self.apply_weight_decompose(updown, orig_weight)\n\n        return updown * self.calc_scale() * self.multiplier(), ex_bias"
            }
        ],
        "third_party": [
            "to",
            "to"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def purge_networks_from_memory()",
        "start_line": "242",
        "end_line": "247",
        "file_path": "extensions-builtin/Lora/networks.py",
        "docstring": "This function purges networks from memory if the number of networks exceeds a specified limit.\\nIt removes networks from memory until the count is within the limit, then triggers garbage collection to free up memory.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1bb9e5d94af8",
        "ground_truth": "def purge_networks_from_memory():\n    while len(networks_in_memory) > shared.opts.lora_in_memory_limit and len(networks_in_memory) > 0:\n        name = next(iter(networks_in_memory))\n        networks_in_memory.pop(name, None)\n     devices.torch_gc()",
        "import_statements": [
            "import logging",
            "import os",
            "import re",
            "import lora_patches",
            "import network",
            "import network_lora",
            "import network_glora",
            "import network_hada",
            "import network_ia3",
            "import network_lokr",
            "import network_full",
            "import network_norm",
            "import network_oft",
            "import torch",
            "from typing import Union",
            "from modules import shared, devices, sd_models, errors, scripts, sd_hijack",
            "from lora_logger import logger"
        ],
        "reference_api": [
            "devices.torch_gc",
            "next",
            "len",
            "iter",
            "networks_in_memory.pop"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "networks_in_memory.pop",
            "devices.torch_gc"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):",
        "start_line": "328",
        "end_line": "351",
        "file_path": "extensions-builtin/Lora/networks.py",
        "docstring": "This function restores the weights and biases of a neural network layer from a backup.\\nIt checks if backup weights and biases exist and, if so, copies them back to the corresponding attributes of the layer.\\nSpecial handling is included for the MultiheadAttention layer to correctly restore its projection weights and biases.\\nIf no bias backup exists, the bias is set to None.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "42b48ebf4d07",
        "ground_truth": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    weights_backup = getattr(self, \"network_weights_backup\", None)\n    bias_backup = getattr(self, \"network_bias_backup\", None)\n     if weights_backup is None and bias_backup is None:\n        return\n     if weights_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.in_proj_weight.copy_(weights_backup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n     if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None",
        "import_statements": [
            "import logging",
            "import os",
            "import re",
            "import lora_patches",
            "import network",
            "import network_lora",
            "import network_glora",
            "import network_hada",
            "import network_ia3",
            "import network_lokr",
            "import network_full",
            "import network_norm",
            "import network_oft",
            "import torch",
            "from typing import Union",
            "from modules import shared, devices, sd_models, errors, scripts, sd_hijack",
            "from lora_logger import logger"
        ],
        "reference_api": [
            "getattr",
            "copy_",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "copy_",
            "copy_",
            "copy_",
            "copy_",
            "copy_"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def network_forward(org_module, input, original_forward)",
        "start_line": "464",
        "end_line": "488",
        "file_path": "extensions-builtin/Lora/networks.py",
        "docstring": "This function performs a forward pass through a neural network module, integrating additional processing from loaded networks if available.\\nIt first checks if any additional networks are loaded and uses the original forward method if none are present.\\nThe input is cast to the appropriate device, and the module's weights are restored from backup and reset.\\nThe original forward method is called, and then each loaded network is applied sequentially if it matches the current layer, modifying the output accordingly.\\nThe final output is returned.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c8fcd357131a",
        "ground_truth": "def network_forward(org_module, input, original_forward):\n    \"\"\"\n    Old way of applying Lora by executing operations during layer's forward.\n    Stacking many loras this way results in big performance degradation.\n    \"\"\"\n     if len(loaded_networks) == 0:\n        return original_forward(org_module, input)\n     input = devices.cond_cast_unet(input)\n     network_restore_weights_from_backup(org_module)\n    network_reset_cached_weight(org_module)\n     y = original_forward(org_module, input)\n     network_layer_name = getattr(org_module, 'network_layer_name', None)\n    for lora in loaded_networks:\n        module = lora.modules.get(network_layer_name, None)\n        if module is None:\n            continue\n         y = module.forward(input, y)\n     return y",
        "import_statements": [
            "import logging",
            "import os",
            "import re",
            "import lora_patches",
            "import network",
            "import network_lora",
            "import network_glora",
            "import network_hada",
            "import network_ia3",
            "import network_lokr",
            "import network_full",
            "import network_norm",
            "import network_oft",
            "import torch",
            "from typing import Union",
            "from modules import shared, devices, sd_models, errors, scripts, sd_hijack",
            "from lora_logger import logger"
        ],
        "reference_api": [
            "getattr",
            "network_reset_cached_weight",
            "module.forward",
            "network_restore_weights_from_backup",
            "len",
            "original_forward",
            "devices.cond_cast_unet",
            "get"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "network_restore_weights_from_backup",
                "code": "def network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    weights_backup = getattr(self, \"network_weights_backup\", None)\n    bias_backup = getattr(self, \"network_bias_backup\", None)\n\n    if weights_backup is None and bias_backup is None:\n        return\n\n    if weights_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.in_proj_weight.copy_(weights_backup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None"
            },
            {
                "name": "network_reset_cached_weight",
                "code": "def network_reset_cached_weight(self: Union[torch.nn.Conv2d, torch.nn.Linear]):\n    self.network_current_names = ()\n    self.network_weights_backup = None\n    self.network_bias_backup = None"
            },
            {
                "name": "module.forward",
                "code": "def forward(self, x, y):\n        \"\"\"A general forward implementation for all modules\"\"\"\n        if self.ops is None:\n            raise NotImplementedError()\n        else:\n            updown, ex_bias = self.calc_updown(self.sd_module.weight)\n            return y + self.ops(x, weight=updown, bias=ex_bias, **self.extra_kwargs)"
            }
        ],
        "third_party": [
            "original_forward",
            "devices.cond_cast_unet",
            "original_forward",
            "get"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def trainables(self)",
        "start_line": "120",
        "end_line": "125",
        "file_path": "modules/hypernetworks/hypernetwork.py",
        "docstring": "This function returns a list of trainable parameters (weights and biases) for linear and layer normalization layers in a model.\\nIt iterates through the layers in the model, collecting the weights and biases of layers that are instances of torch.nn.Linear or torch.nn.LayerNorm.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1070211f5c11",
        "ground_truth": "def trainables(self):\n    layer_structure = []\n    for layer in self.linear:\n        if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n            layer_structure += [layer.weight, layer.bias]\n    return layer_structure",
        "import_statements": [
            "import datetime",
            "import glob",
            "import html",
            "import os",
            "import inspect",
            "from contextlib import closing",
            "import modules.textual_inversion.dataset",
            "import torch",
            "import tqdm",
            "from einops import rearrange, repeat",
            "from ldm.util import default",
            "from modules import devices, sd_models, shared, sd_samplers, hashes, sd_hijack_checkpoint, errors",
            "from modules.textual_inversion import textual_inversion, saving_settings",
            "from modules.textual_inversion.learn_schedule import LearnRateScheduler",
            "from torch import einsum",
            "from torch.nn.init import normal_, xavier_normal_, xavier_uniform_, kaiming_normal_, kaiming_uniform_, zeros_",
            "from collections import deque",
            "from statistics import stdev, mean"
        ],
        "reference_api": [
            "type"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def load_hypernetworks(names, multipliers=None)",
        "start_line": "337",
        "end_line": "355",
        "file_path": "modules/hypernetworks/hypernetwork.py",
        "docstring": "This function loads specified hypernetworks, sets their multipliers, and stores them in a shared list of loaded hypernetworks.\\nIt first checks for already loaded hypernetworks, clears the current list, then loads or retrieves each hypernetwork by name, setting its multiplier if provided.\\nFinally, it appends each loaded hypernetwork to the shared list.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0c3ba62c96a7",
        "ground_truth": "def load_hypernetworks(names, multipliers=None):\n    already_loaded = {}\n     for hypernetwork in shared.loaded_hypernetworks:\n        if hypernetwork.name in names:\n            already_loaded[hypernetwork.name] = hypernetwork\n     shared.loaded_hypernetworks.clear()\n     for i, name in enumerate(names):\n        hypernetwork = already_loaded.get(name, None)\n        if hypernetwork is None:\n            hypernetwork = load_hypernetwork(name)\n         if hypernetwork is None:\n            continue\n         hypernetwork.set_multiplier(multipliers[i] if multipliers else 1.0)\n        shared.loaded_hypernetworks.append(hypernetwork)",
        "import_statements": [
            "import datetime",
            "import glob",
            "import html",
            "import os",
            "import inspect",
            "from contextlib import closing",
            "import modules.textual_inversion.dataset",
            "import torch",
            "import tqdm",
            "from einops import rearrange, repeat",
            "from ldm.util import default",
            "from modules import devices, sd_models, shared, sd_samplers, hashes, sd_hijack_checkpoint, errors",
            "from modules.textual_inversion import textual_inversion, saving_settings",
            "from modules.textual_inversion.learn_schedule import LearnRateScheduler",
            "from torch import einsum",
            "from torch.nn.init import normal_, xavier_normal_, xavier_uniform_, kaiming_normal_, kaiming_uniform_, zeros_",
            "from collections import deque",
            "from statistics import stdev, mean"
        ],
        "reference_api": [
            "append",
            "load_hypernetwork",
            "clear",
            "already_loaded.get",
            "hypernetwork.set_multiplier",
            "enumerate"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "load_hypernetwork",
                "code": "def load_hypernetwork(name):\n    path = shared.hypernetworks.get(name, None)\n\n    if path is None:\n        return None\n\n    try:\n        hypernetwork = Hypernetwork()\n        hypernetwork.load(path)\n        return hypernetwork\n    except Exception:\n        errors.report(f\"Error loading hypernetwork {path}\", exc_info=True)\n        return None"
            },
            {
                "name": "hypernetwork.set_multiplier",
                "code": "def set_multiplier(self, multiplier):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.multiplier = multiplier\n\n        return self"
            }
        ],
        "third_party": [
            "clear",
            "already_loaded.get",
            "append"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "\ndef apply_hypernetworks(hypernetworks, context, layer=None)",
        "start_line": "373",
        "end_line": "379",
        "file_path": "modules/hypernetworks/hypernetwork.py",
        "docstring": "This function applies multiple hypernetworks to a given context.\\nIt initializes two context variables, context_k and context_v, with the input context.\\nThen, it iteratively applies each hypernetwork to these context variables using a function that handles single hypernetwork application.\\nFinally, it returns the modified context_k and context_v.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bea3772ed532",
        "ground_truth": "def apply_hypernetworks(hypernetworks, context, layer=None):\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n     return context_k, context_v",
        "import_statements": [
            "import datetime",
            "import glob",
            "import html",
            "import os",
            "import inspect",
            "from contextlib import closing",
            "import modules.textual_inversion.dataset",
            "import torch",
            "import tqdm",
            "from einops import rearrange, repeat",
            "from ldm.util import default",
            "from modules import devices, sd_models, shared, sd_samplers, hashes, sd_hijack_checkpoint, errors",
            "from modules.textual_inversion import textual_inversion, saving_settings",
            "from modules.textual_inversion.learn_schedule import LearnRateScheduler",
            "from torch import einsum",
            "from torch.nn.init import normal_, xavier_normal_, xavier_uniform_, kaiming_normal_, kaiming_uniform_, zeros_",
            "from collections import deque",
            "from statistics import stdev, mean"
        ],
        "reference_api": [
            "apply_single_hypernetwork"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "apply_single_hypernetwork",
                "code": "def apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=None):\n    hypernetwork_layers = (hypernetwork.layers if hypernetwork is not None else {}).get(context_k.shape[2], None)\n\n    if hypernetwork_layers is None:\n        return context_k, context_v\n\n    if layer is not None:\n        layer.hyper_k = hypernetwork_layers[0]\n        layer.hyper_v = hypernetwork_layers[1]\n\n    context_k = devices.cond_cast_unet(hypernetwork_layers[0](devices.cond_cast_float(context_k)))\n    context_v = devices.cond_cast_unet(hypernetwork_layers[1](devices.cond_cast_float(context_v)))\n    return context_k, context_v"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs)",
        "start_line": "382",
        "end_line": "407",
        "file_path": "modules/hypernetworks/hypernetwork.py",
        "docstring": "This function performs the forward pass of a cross-attention mechanism.\\nIt computes query, key, and value projections from the input and context, applies hypernetworks to the context, and reshapes the tensors for multi-head attention.\\nThe function then calculates scaled dot-product attention, optionally applies a mask to the attention scores, and computes the final attention output.\\nThe result is reshaped back and passed through a final linear layer to produce the output.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "922efa3e092f",
        "ground_truth": "def attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs):\n    h = self.heads\n     q = self.to_q(x)\n    context = default(context, x)\n     context_k, context_v = apply_hypernetworks(shared.loaded_hypernetworks, context, self)\n    k = self.to_k(context_k)\n    v = self.to_v(context_v)\n     q, k, v = (rearrange(t, 'b n (h d) -> (b h) n d', h=h) for t in (q, k, v))\n     sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n     if mask is not None:\n        mask = rearrange(mask, 'b ... -> b (...)')\n        max_neg_value = -torch.finfo(sim.dtype).max\n        mask = repeat(mask, 'b j -> (b h) () j', h=h)\n        sim.masked_fill_(~mask, max_neg_value)\n     # attention, what we cannot get enough of\n    attn = sim.softmax(dim=-1)\n     out = einsum('b i j, b j d -> b i d', attn, v)\n    out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n    return self.to_out(out)",
        "import_statements": [
            "import datetime",
            "import glob",
            "import html",
            "import os",
            "import inspect",
            "from contextlib import closing",
            "import modules.textual_inversion.dataset",
            "import torch",
            "import tqdm",
            "from einops import rearrange, repeat",
            "from ldm.util import default",
            "from modules import devices, sd_models, shared, sd_samplers, hashes, sd_hijack_checkpoint, errors",
            "from modules.textual_inversion import textual_inversion, saving_settings",
            "from modules.textual_inversion.learn_schedule import LearnRateScheduler",
            "from torch import einsum",
            "from torch.nn.init import normal_, xavier_normal_, xavier_uniform_, kaiming_normal_, kaiming_uniform_, zeros_",
            "from collections import deque",
            "from statistics import stdev, mean"
        ],
        "reference_api": [
            "rearrange",
            "self.to_v",
            "self.to_out",
            "sim.softmax",
            "self.to_k",
            "self.to_q",
            "apply_hypernetworks",
            "repeat",
            "default",
            "sim.masked_fill_",
            "torch.finfo",
            "einsum"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "apply_hypernetworks",
                "code": "def apply_hypernetworks(hypernetworks, context, layer=None):\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v"
            }
        ],
        "third_party": [
            "self.to_q",
            "default",
            "self.to_k",
            "self.to_v",
            "rearrange",
            "einsum",
            "rearrange",
            "torch.finfo",
            "repeat",
            "sim.masked_fill_",
            "sim.softmax",
            "einsum",
            "rearrange",
            "self.to_out"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)",
        "start_line": "770",
        "end_line": "783",
        "file_path": "modules/hypernetworks/hypernetwork.py",
        "docstring": "This function saves the state of a hypernetwork to a file with a specified filename, temporarily updating its attributes with new checkpoint information.\\nIt backs up the original attributes, updates the hypernetwork with the new checkpoint data, and attempts to save it.\\nIf an error occurs, it restores the original attributes before re-raising the exception.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8b7455a03a0e",
        "ground_truth": "def save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename):\n    old_hypernetwork_name = hypernetwork.name\n    old_sd_checkpoint = hypernetwork.sd_checkpoint if hasattr(hypernetwork, \"sd_checkpoint\") else None\n    old_sd_checkpoint_name = hypernetwork.sd_checkpoint_name if hasattr(hypernetwork, \"sd_checkpoint_name\") else None\n    try:\n        hypernetwork.sd_checkpoint = checkpoint.shorthash\n        hypernetwork.sd_checkpoint_name = checkpoint.model_name\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.save(filename)\n    except:\n        hypernetwork.sd_checkpoint = old_sd_checkpoint\n        hypernetwork.sd_checkpoint_name = old_sd_checkpoint_name\n        hypernetwork.name = old_hypernetwork_name\n        raise",
        "import_statements": [
            "import datetime",
            "import glob",
            "import html",
            "import os",
            "import inspect",
            "from contextlib import closing",
            "import modules.textual_inversion.dataset",
            "import torch",
            "import tqdm",
            "from einops import rearrange, repeat",
            "from ldm.util import default",
            "from modules import devices, sd_models, shared, sd_samplers, hashes, sd_hijack_checkpoint, errors",
            "from modules.textual_inversion import textual_inversion, saving_settings",
            "from modules.textual_inversion.learn_schedule import LearnRateScheduler",
            "from torch import einsum",
            "from torch.nn.init import normal_, xavier_normal_, xavier_uniform_, kaiming_normal_, kaiming_uniform_, zeros_",
            "from collections import deque",
            "from statistics import stdev, mean"
        ],
        "reference_api": [
            "hasattr",
            "hypernetwork.save"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "hypernetwork.save",
                "code": "def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def ema_scope(self, context=None)",
        "start_line": "188",
        "end_line": "200",
        "file_path": "modules/models/diffusion/ddpm_edit.py",
        "docstring": "This function manages the use of Exponential Moving Average (EMA) weights for a model, temporarily switching to EMA weights and restoring the original weights after execution.\\nIf EMA is enabled, it stores the current model parameters, copies EMA weights to the model, and optionally logs this action.\\nAfter the code block execution, it restores the original weights and optionally logs the restoration.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3175f835140f",
        "ground_truth": "def ema_scope(self, context=None):\n    if self.use_ema:\n        self.model_ema.store(self.model.parameters())\n        self.model_ema.copy_to(self.model)\n        if context is not None:\n            print(f\"{context}: Switched to EMA weights\")\n    try:\n        yield None\n    finally:\n        if self.use_ema:\n            self.model_ema.restore(self.model.parameters())\n            if context is not None:\n                print(f\"{context}: Restored training weights\")",
        "import_statements": [
            "import torch",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from einops import rearrange, repeat",
            "from contextlib import contextmanager",
            "from functools import partial",
            "from tqdm import tqdm",
            "from torchvision.utils import make_grid",
            "from pytorch_lightning.utilities.distributed import rank_zero_only",
            "from ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config",
            "from ldm.modules.ema import LitEma",
            "from ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution",
            "from ldm.models.autoencoder import IdentityFirstStage, AutoencoderKL",
            "from ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like",
            "from ldm.models.diffusion.ddim import DDIMSampler"
        ],
        "reference_api": [
            "print",
            "restore",
            "parameters",
            "store",
            "copy_to"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "store",
            "parameters",
            "copy_to",
            "restore",
            "parameters"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def p_mean_variance(self, x, t, clip_denoised: bool)",
        "start_line": "270",
        "end_line": "280",
        "file_path": "modules/models/diffusion/ddpm_edit.py",
        "docstring": "This function computes the mean and variance for a denoising process based on the model's output and the given parameterization.\\nIt first gets the model output for the input and time step, then reconstructs the input either directly or from predicted noise.\\nIf specified, the reconstructed input is clipped to a range of -1 to 1.\\nFinally, it calculates and returns the model mean, posterior variance, and posterior log variance.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a7b5d5cd519a",
        "ground_truth": "def p_mean_variance(self, x, t, clip_denoised: bool):\n    model_out = self.model(x, t)\n    if self.parameterization == \"eps\":\n        x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n    elif self.parameterization == \"x0\":\n        x_recon = model_out\n    if clip_denoised:\n        x_recon.clamp_(-1., 1.)\n    model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n    return model_mean, posterior_variance, posterior_log_variance",
        "import_statements": [
            "import torch",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from einops import rearrange, repeat",
            "from contextlib import contextmanager",
            "from functools import partial",
            "from tqdm import tqdm",
            "from torchvision.utils import make_grid",
            "from pytorch_lightning.utilities.distributed import rank_zero_only",
            "from ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config",
            "from ldm.modules.ema import LitEma",
            "from ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution",
            "from ldm.models.autoencoder import IdentityFirstStage, AutoencoderKL",
            "from ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like",
            "from ldm.models.diffusion.ddim import DDIMSampler"
        ],
        "reference_api": [
            "x_recon.clamp_",
            "self.model",
            "self.q_posterior",
            "self.predict_start_from_noise"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.predict_start_from_noise",
                "code": "def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )"
            },
            {
                "name": "self.q_posterior",
                "code": "def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped"
            }
        ],
        "third_party": [
            "self.model",
            "x_recon.clamp_"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def get_loss(self, pred, target, mean=True)",
        "start_line": "318",
        "end_line": "331",
        "file_path": "modules/models/diffusion/ddpm_edit.py",
        "docstring": "This function computes the loss between predicted and target values based on the specified loss type, either 'l1' or 'l2'.\\nFor 'l1' loss, it calculates the absolute difference, optionally taking the mean.\\nFor 'l2' loss, it uses mean squared error, optionally without reduction.\\nIf an unknown loss type is specified, it raises an error.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a8652fa78473",
        "ground_truth": "def get_loss(self, pred, target, mean=True):\n    if self.loss_type == 'l1':\n        loss = (target - pred).abs()\n        if mean:\n            loss = loss.mean()\n    elif self.loss_type == 'l2':\n        if mean:\n            loss = torch.nn.functional.mse_loss(target, pred)\n        else:\n            loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n    else:\n        raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n    return loss",
        "import_statements": [
            "import torch",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from einops import rearrange, repeat",
            "from contextlib import contextmanager",
            "from functools import partial",
            "from tqdm import tqdm",
            "from torchvision.utils import make_grid",
            "from pytorch_lightning.utilities.distributed import rank_zero_only",
            "from ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config",
            "from ldm.modules.ema import LitEma",
            "from ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution",
            "from ldm.models.autoencoder import IdentityFirstStage, AutoencoderKL",
            "from ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like",
            "from ldm.models.diffusion.ddim import DDIMSampler"
        ],
        "reference_api": [
            "loss.mean",
            "abs",
            "NotImplementedError",
            "mse_loss"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "loss.mean",
            "mse_loss",
            "mse_loss"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def _prior_bpd(self, x_start)",
        "start_line": "1008",
        "end_line": "1020",
        "file_path": "modules/models/diffusion/ddpm_edit.py",
        "docstring": "This function calculates the prior bits-per-dimension (bpd) for the input tensor x_start.\\nIt determines the batch size and creates a tensor t representing the final timestep for each batch element.\\nThe function then computes the mean and log variance of the predicted distribution at this timestep and calculates the KL divergence between this distribution and a standard normal distribution.\\nFinally, it returns the mean KL divergence normalized by the logarithm of 2.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d68e4f061349",
        "ground_truth": "def _prior_bpd(self, x_start):\n    \"\"\"\n    Get the prior KL term for the variational lower-bound, measured in\n    bits-per-dim.\n    This term can't be optimized, as it only depends on the encoder.\n    :param x_start: the [N x C x ...] tensor of inputs.\n    :return: a batch of [N] KL values (in bits), one per batch element.\n    \"\"\"\n    batch_size = x_start.shape[0]\n    t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n    qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n    kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n    return mean_flat(kl_prior) / np.log(2.0)",
        "import_statements": [
            "import torch",
            "from torch.optim.lr_scheduler import LambdaLR",
            "from einops import rearrange, repeat",
            "from contextlib import contextmanager",
            "from functools import partial",
            "from tqdm import tqdm",
            "from torchvision.utils import make_grid",
            "from pytorch_lightning.utilities.distributed import rank_zero_only",
            "from ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config",
            "from ldm.modules.ema import LitEma",
            "from ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution",
            "from ldm.models.autoencoder import IdentityFirstStage, AutoencoderKL",
            "from ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like",
            "from ldm.models.diffusion.ddim import DDIMSampler"
        ],
        "reference_api": [
            "torch.tensor",
            "self.q_mean_variance",
            "np.log",
            "mean_flat",
            "normal_kl"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.q_mean_variance",
                "code": "def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance"
            }
        ],
        "third_party": [
            "torch.tensor",
            "normal_kl",
            "mean_flat",
            "np.log"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def make_cache(subsection: str) -> diskcache.Cache",
        "start_line": "23",
        "end_line": "28",
        "file_path": "modules/cache.py",
        "docstring": "# This function initializes a disk-based cache for a specific subsection. It creates a Cache object from the diskcache module with a path based on the provided subsection and a predefined cache directory. The cache has a size limit of 4 GB, and it prioritizes keeping up to 256KB in an SQLite database by setting a minimum file size for disk storage.\\n",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c695d93fdf2d",
        "ground_truth": "def make_cache(subsection: str) -> diskcache.Cache:\n    return diskcache.Cache(\n        os.path.join(cache_dir, subsection),\n        size_limit=2**32,  # 4 GB, culling oldest first\n        disk_min_file_size=2**18,  # keep up to 256KB in Sqlite\n    )",
        "import_statements": [
            "import json",
            "import os",
            "import os.path",
            "import threading",
            "import diskcache",
            "import tqdm",
            "from modules.paths import data_path, script_path"
        ],
        "reference_api": [
            "join",
            "diskcache.Cache"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "diskcache.Cache",
            "join"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def cache(subsection)",
        "start_line": "56",
        "end_line": "78",
        "file_path": "modules/cache.py",
        "docstring": "The function cache(subsection) aims to retrieve or create a cached object for a given subsection. It first attempts to get the cache object from a global caches dictionary. If the cache object does not exist, it acquires a lock (cache_lock) to ensure thread-safety and performs the following steps:\\n1. Checks if the cache directory exists and if the cache filename is a valid file. If these conditions are not met, it calls convert_old_cached_data() to convert legacy cached data.\\n2. Re-checks the caches dictionary for the subsection. If the cache object still does not exist, it creates a new cache object using make_cache(subsection) and stores it in the caches dictionary.\\nFinally, it returns the retrieved or newly created cache object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ed4c543f0d38",
        "ground_truth": "def cache(subsection):\n    \"\"\"\n    Retrieves or initializes a cache for a specific subsection.\n     Parameters:\n        subsection (str): The subsection identifier for the cache.\n     Returns:\n        diskcache.Cache: The cache data for the specified subsection.\n    \"\"\"\n     cache_obj = caches.get(subsection)\n    if not cache_obj:\n        with cache_lock:\n            if not os.path.exists(cache_dir) and os.path.isfile(cache_filename):\n                convert_old_cached_data()\n             cache_obj = caches.get(subsection)\n            if not cache_obj:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n     return cache_obj",
        "import_statements": [
            "import json",
            "import os",
            "import os.path",
            "import threading",
            "import diskcache",
            "import tqdm",
            "from modules.paths import data_path, script_path"
        ],
        "reference_api": [
            "exists",
            "make_cache",
            "caches.get",
            "convert_old_cached_data",
            "isfile"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "convert_old_cached_data",
                "code": "def convert_old_cached_data():\n    try:\n        with open(cache_filename, \"r\", encoding=\"utf8\") as file:\n            data = json.load(file)\n    except FileNotFoundError:\n        return\n    except Exception:\n        os.replace(cache_filename, os.path.join(script_path, \"tmp\", \"cache.json\"))\n        print('[ERROR] issue occurred while trying to read cache.json; old cache has been moved to tmp/cache.json')\n        return\n\n    total_count = sum(len(keyvalues) for keyvalues in data.values())\n\n    with tqdm.tqdm(total=total_count, desc=\"converting cache\") as progress:\n        for subsection, keyvalues in data.items():\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n            for key, value in keyvalues.items():\n                cache_obj[key] = value\n                progress.update(1)"
            },
            {
                "name": "make_cache",
                "code": "def make_cache(subsection: str) -> diskcache.Cache:\n    return diskcache.Cache(\n        os.path.join(cache_dir, subsection),\n        size_limit=2**32,  # 4 GB, culling oldest first\n        disk_min_file_size=2**18,  # keep up to 256KB in Sqlite\n    )"
            }
        ],
        "third_party": [
            "caches.get",
            "exists",
            "isfile",
            "caches.get"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def cached_data_for_file(subsection, title, filename, func)",
        "start_line": "81",
        "end_line": "123",
        "file_path": "modules/cache.py",
        "docstring": "The function cached_data_for_file(subsection, title, filename, func) manages cached data for a specific file. It starts by retrieving an existing cache for the given subsection using the cache(subsection) function. It then gets the last modification time (mtime) of the specified file. If a cache entry exists for the given title, it checks if the file's mtime is more recent than the cached mtime; if so, it invalidates the cache entry. If no valid cache entry exists, it calls the provided function func() to generate new data, stores this data along with the file's mtime in the cache, and updates the cache storage with dump_cache(). Finally, it returns the cached or newly generated data.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ccf5ea2d9fc1",
        "ground_truth": "def cached_data_for_file(subsection, title, filename, func):\n    \"\"\"\n    Retrieves or generates data for a specific file, using a caching mechanism.\n     Parameters:\n        subsection (str): The subsection of the cache to use.\n        title (str): The title of the data entry in the subsection of the cache.\n        filename (str): The path to the file to be checked for modifications.\n        func (callable): A function that generates the data if it is not available in the cache.\n     Returns:\n        dict or None: The cached or generated data, or None if data generation fails.\n     The `cached_data_for_file` function implements a caching mechanism for data stored in files.\n    It checks if the data associated with the given `title` is present in the cache and compares the\n    modification time of the file with the cached modification time. If the file has been modified,\n    the cache is considered invalid and the data is regenerated using the provided `func`.\n    Otherwise, the cached data is returned.\n     If the data generation fails, None is returned to indicate the failure. Otherwise, the generated\n    or cached data is returned as a dictionary.\n    \"\"\"\n     existing_cache = cache(subsection)\n    ondisk_mtime = os.path.getmtime(filename)\n     entry = existing_cache.get(title)\n    if entry:\n        cached_mtime = entry.get(\"mtime\", 0)\n        if ondisk_mtime > cached_mtime:\n            entry = None\n     if not entry or 'value' not in entry:\n        value = func()\n        if value is None:\n            return None\n         entry = {'mtime': ondisk_mtime, 'value': value}\n        existing_cache[title] = entry\n         dump_cache()\n     return entry['value']",
        "import_statements": [
            "import json",
            "import os",
            "import os.path",
            "import threading",
            "import diskcache",
            "import tqdm",
            "from modules.paths import data_path, script_path"
        ],
        "reference_api": [
            "existing_cache.get",
            "cache",
            "func",
            "dump_cache",
            "entry.get",
            "getmtime"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "cache",
                "code": "def cache(subsection):\n    \"\"\"\n    Retrieves or initializes a cache for a specific subsection.\n\n    Parameters:\n        subsection (str): The subsection identifier for the cache.\n\n    Returns:\n        diskcache.Cache: The cache data for the specified subsection.\n    \"\"\"\n\n    cache_obj = caches.get(subsection)\n    if not cache_obj:\n        with cache_lock:\n            if not os.path.exists(cache_dir) and os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if not cache_obj:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj"
            },
            {
                "name": "dump_cache",
                "code": "def dump_cache():\n    \"\"\"old function for dumping cache to disk; does nothing since diskcache.\"\"\"\n\n    pass"
            }
        ],
        "third_party": [
            "getmtime",
            "existing_cache.get",
            "entry.get",
            "func"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def report(message: str, *, exc_info: bool = False) -> None",
        "start_line": "38",
        "end_line": "49",
        "file_path": "modules/errors.py",
        "docstring": "The function report(message: str, *, exc_info: bool = False) -> None logs an error message to stderr. It first calls record_exception() to log the exception details. Then, it prints each line of the provided message to stderr, prefixed with \"***\". If the exc_info parameter is True, it additionally prints the formatted traceback, indented by four spaces, to stderr, followed by a line of \"---\".",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e01a2d4f0ac2",
        "ground_truth": "def report(message: str, *, exc_info: bool = False) -> None:\n    \"\"\"\n    Print an error message to stderr, with optional traceback.\n    \"\"\"\n     record_exception()\n     for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)",
        "import_statements": [
            "import sys",
            "import textwrap",
            "import traceback"
        ],
        "reference_api": [
            "record_exception",
            "print",
            "message.splitlines",
            "textwrap.indent",
            "traceback.format_exc"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "record_exception",
                "code": "def record_exception():\n    _, e, tb = sys.exc_info()\n    if e is None:\n        return\n\n    if exception_records and exception_records[-1] == e:\n        return\n\n    exception_records.append(format_exception(e, tb))\n\n    if len(exception_records) > 5:\n        exception_records.pop(0)"
            }
        ],
        "third_party": [
            "message.splitlines"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "AUTOMATIC1111/stable-diffusion-webui",
        "function_declaration": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt)",
        "start_line": "62",
        "end_line": "79",
        "file_path": "modules/styles.py",
        "docstring": "The function extract_original_prompts(style, prompt, negative_prompt) extracts specific style text from given prompts based on a provided style object. It checks if the style's prompt and negative prompt are empty; if so, it returns False along with the original prompts. It then attempts to extract style text from both the positive and negative prompts. If either extraction fails, it returns False with the original prompts. If both extractions are successful, it returns True along with the extracted positive and negative prompts.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "125fb9d2320b",
        "ground_truth": "def extract_original_prompts(style: PromptStyle, prompt, negative_prompt):\n    \"\"\"\n    Takes a style and compares it to the prompt and negative prompt. If the style\n    matches, returns True plus the prompt and negative prompt with the style text\n    removed. Otherwise, returns False with the original prompt and negative prompt.\n    \"\"\"\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n     match_positive, extracted_positive = extract_style_text_from_prompt(style.prompt, prompt)\n    if not match_positive:\n        return False, prompt, negative_prompt\n     match_negative, extracted_negative = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    if not match_negative:\n        return False, prompt, negative_prompt\n     return True, extracted_positive, extracted_negative",
        "import_statements": [
            "from pathlib import Path",
            "from modules import errors",
            "import csv",
            "import os",
            "import typing",
            "import shutil"
        ],
        "reference_api": [
            "extract_style_text_from_prompt"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "extract_style_text_from_prompt",
                "code": "def extract_style_text_from_prompt(style_text, prompt):\n    \"\"\"This function extracts the text from a given prompt based on a provided style text. It checks if the style text contains the placeholder {prompt} or if it appears at the end of the prompt. If a match is found, it returns True along with the extracted text. Otherwise, it returns False and the original prompt.\n\n    extract_style_text_from_prompt(\"masterpiece\", \"1girl, art by greg, masterpiece\") outputs (True, \"1girl, art by greg\")\n    extract_style_text_from_prompt(\"masterpiece, {prompt}\", \"masterpiece, 1girl, art by greg\") outputs (True, \"1girl, art by greg\")\n    extract_style_text_from_prompt(\"masterpiece, {prompt}\", \"exquisite, 1girl, art by greg\") outputs (False, \"exquisite, 1girl, art by greg\")\n    \"\"\"\n\n    stripped_prompt = prompt.strip()\n    stripped_style_text = style_text.strip()\n\n    if \"{prompt}\" in stripped_style_text:\n        left, _, right = stripped_style_text.partition(\"{prompt}\")\n        if stripped_prompt.startswith(left) and stripped_prompt.endswith(right):\n            prompt = stripped_prompt[len(left):len(stripped_prompt)-len(right)]\n            return True, prompt\n    else:\n        if stripped_prompt.endswith(stripped_style_text):\n            prompt = stripped_prompt[:len(stripped_prompt)-len(stripped_style_text)]\n\n            if prompt.endswith(', '):\n                prompt = prompt[:-2]\n\n            return True, prompt\n\n    return False, prompt"
            },
            {
                "name": "extract_style_text_from_prompt",
                "code": "def extract_style_text_from_prompt(style_text, prompt):\n    \"\"\"This function extracts the text from a given prompt based on a provided style text. It checks if the style text contains the placeholder {prompt} or if it appears at the end of the prompt. If a match is found, it returns True along with the extracted text. Otherwise, it returns False and the original prompt.\n\n    extract_style_text_from_prompt(\"masterpiece\", \"1girl, art by greg, masterpiece\") outputs (True, \"1girl, art by greg\")\n    extract_style_text_from_prompt(\"masterpiece, {prompt}\", \"masterpiece, 1girl, art by greg\") outputs (True, \"1girl, art by greg\")\n    extract_style_text_from_prompt(\"masterpiece, {prompt}\", \"exquisite, 1girl, art by greg\") outputs (False, \"exquisite, 1girl, art by greg\")\n    \"\"\"\n\n    stripped_prompt = prompt.strip()\n    stripped_style_text = style_text.strip()\n\n    if \"{prompt}\" in stripped_style_text:\n        left, _, right = stripped_style_text.partition(\"{prompt}\")\n        if stripped_prompt.startswith(left) and stripped_prompt.endswith(right):\n            prompt = stripped_prompt[len(left):len(stripped_prompt)-len(right)]\n            return True, prompt\n    else:\n        if stripped_prompt.endswith(stripped_style_text):\n            prompt = stripped_prompt[:len(stripped_prompt)-len(stripped_style_text)]\n\n            if prompt.endswith(', '):\n                prompt = prompt[:-2]\n\n            return True, prompt\n\n    return False, prompt"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def random_partial(self, count, n_frames)",
        "start_line": "20",
        "end_line": "40",
        "file_path": "encoder/data_objects/speaker.py",
        "docstring": "The function random_partial(self, count, n_frames) generates random partial segments from a specified number of utterances. If the utterances are not loaded, it calls self._load_utterances() to load them. It then samples the specified count of utterances using self.utterance_cycler.sample(count). For each sampled utterance, it creates a tuple containing the utterance and a random partial segment of length n_frames, obtained by calling u.random_partial(n_frames). Finally, it returns a list of these tuples.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0c5f729fbce2",
        "ground_truth": "def random_partial(self, count, n_frames):\n    \"\"\"\n    Samples a batch of <count> unique partial utterances from the disk in a way that all \n    utterances come up at least once every two cycles and in a random order every time.\n         :param count: The number of partial utterances to sample from the set of utterances from \n    that speaker. Utterances are guaranteed not to be repeated if <count> is not larger than \n    the number of utterances available.\n    :param n_frames: The number of frames in the partial utterance.\n    :return: A list of tuples (utterance, frames, range) where utterance is an Utterance, \n    frames are the frames of the partial utterances and range is the range of the partial \n    utterance with regard to the complete utterance.\n    \"\"\"\n    if self.utterances is None:\n        self._load_utterances()\n    utterances = self.utterance_cycler.sample(count)\n    a = [(u,) + u.random_partial(n_frames) for u in utterances]\n    return a",
        "import_statements": [
            "from encoder.data_objects.random_cycler import RandomCycler",
            "from encoder.data_objects.utterance import Utterance",
            "from pathlib import Path"
        ],
        "reference_api": [
            "sample",
            "self._load_utterances",
            "u.random_partial"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._load_utterances",
                "code": "def _load_utterances(self):\n        with self.root.joinpath(\"_sources.txt\").open(\"r\") as sources_file:\n            sources = [l.split(\",\") for l in sources_file]\n        sources = {frames_fname: wave_fpath for frames_fname, wave_fpath in sources}\n        self.utterances = [Utterance(self.root.joinpath(f), w) for f, w in sources.items()]\n        self.utterance_cycler = RandomCycler(self.utterances)"
            },
            {
                "name": "sample",
                "code": "def sample(self, count: int):\n        shuffle = lambda l: random.sample(l, len(l))\n        \n        out = []\n        while count > 0:\n            if count >= len(self.all_items):\n                out.extend(shuffle(list(self.all_items)))\n                count -= len(self.all_items)\n                continue\n            n = min(count, len(self.next_items))\n            out.extend(self.next_items[:n])\n            count -= n\n            self.next_items = self.next_items[n:]\n            if len(self.next_items) == 0:\n                self.next_items = shuffle(list(self.all_items))\n        return out"
            },
            {
                "name": "u.random_partial",
                "code": "def random_partial(self, count, n_frames):\n        \"\"\"\n        Samples a batch of <count> unique partial utterances from the disk in a way that all \n        utterances come up at least once every two cycles and in a random order every time.\n        \n        :param count: The number of partial utterances to sample from the set of utterances from \n        that speaker. Utterances are guaranteed not to be repeated if <count> is not larger than \n        the number of utterances available.\n        :param n_frames: The number of frames in the partial utterance.\n        :return: A list of tuples (utterance, frames, range) where utterance is an Utterance, \n        frames are the frames of the partial utterances and range is the range of the partial \n        utterance with regard to the complete utterance.\n        \"\"\"\n        if self.utterances is None:\n            self._load_utterances()\n\n        utterances = self.utterance_cycler.sample(count)\n\n        a = [(u,) + u.random_partial(n_frames) for u in utterances]\n\n        return a"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False)",
        "start_line": "111",
        "end_line": "117",
        "file_path": "encoder/audio.py",
        "docstring": "The function normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False) adjusts the volume of an audio waveform to match a target decibel Full Scale (dBFS). It first checks if both increase_only and decrease_only are set, raising a ValueError if so. Then, it calculates the change in dBFS needed to reach the target. If the change is negative and increase_only is True, or if the change is positive and decrease_only is True, it returns the original waveform without modification. Otherwise, it scales the waveform by the calculated change in dBFS.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "31e8b27b8f94",
        "ground_truth": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav ** 2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))",
        "import_statements": [
            "from scipy.ndimage.morphology import binary_dilation",
            "from encoder.params_data import *",
            "from pathlib import Path",
            "from typing import Optional, Union",
            "from warnings import warn",
            "import librosa",
            "import struct"
        ],
        "reference_api": [
            "ValueError",
            "np.mean",
            "np.log10"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.log10",
            "np.mean"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def load_model(weights_fpath: Path, device=None)",
        "start_line": "14",
        "end_line": "35",
        "file_path": "encoder/inference.py",
        "docstring": "The function load_model(weights_fpath: Path, device=None) is responsible for loading a pre-trained model for a speaker encoder. It sets the device to either a specified one or defaults to CUDA if available, otherwise CPU. It initializes a global speaker encoder model (_model) with the specified device and loads the model's state dictionary from a checkpoint file located at weights_fpath. The model is then set to evaluation mode, and a confirmation message is printed, indicating the loaded model and the training step it was trained to.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2353831e9033",
        "ground_truth": "def load_model(weights_fpath: Path, device=None):\n    \"\"\"\n    Loads the model in memory. If this function is not explicitely called, it will be run on the\n    first call to embed_frames() with the default weights file.\n     :param weights_fpath: the path to saved model weights.\n    :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\"). The\n    model will be loaded and will run on this device. Outputs will however always be on the cpu.\n    If None, will default to your GPU if it\"s available, otherwise your CPU.\n    \"\"\"\n    # TODO: I think the slow loading of the encoder might have something to do with the device it\n    #   was saved on. Worth investigating.\n    global _model, _device\n    if device is None:\n        _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    elif isinstance(device, str):\n        _device = torch.device(device)\n    _model = SpeakerEncoder(_device, torch.device(\"cpu\"))\n    checkpoint = torch.load(weights_fpath, _device)\n    _model.load_state_dict(checkpoint[\"model_state\"])\n    _model.eval()\n    print(\"Loaded encoder \\\"%s\\\" trained to step %d\" % (weights_fpath.name, checkpoint[\"step\"]))",
        "import_statements": [
            "from encoder.params_data import *",
            "from encoder.model import SpeakerEncoder",
            "from encoder.audio import preprocess_wav",
            "from matplotlib import cm",
            "from encoder import audio",
            "from pathlib import Path",
            "import torch"
        ],
        "reference_api": [
            "torch.load",
            "print",
            "_model.eval",
            "isinstance",
            "_model.load_state_dict",
            "torch.device",
            "SpeakerEncoder",
            "is_available"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "torch.device",
            "is_available",
            "torch.device",
            "SpeakerEncoder",
            "torch.device",
            "torch.load",
            "_model.load_state_dict",
            "_model.eval"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def embed_frames_batch(frames_batch)",
        "start_line": "42",
        "end_line": "55",
        "file_path": "encoder/inference.py",
        "docstring": "The function embed_frames_batch(frames_batch) performs the task of embedding a batch of frames using a pre-loaded deep learning model. It first checks if the model (_model) is loaded, raising an exception if it is not. The function then converts the input frames_batch from a NumPy array to a PyTorch tensor and transfers it to the appropriate device (_device). It performs a forward pass through the model to compute the embeddings, detaches the result from the computation graph, moves it to the CPU, converts it back to a NumPy array, and returns the embeddings.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3231ad480199",
        "ground_truth": "def embed_frames_batch(frames_batch):\n    \"\"\"\n    Computes embeddings for a batch of mel spectrogram.\n     :param frames_batch: a batch mel of spectrogram as a numpy array of float32 of shape\n    (batch_size, n_frames, n_channels)\n    :return: the embeddings as a numpy array of float32 of shape (batch_size, model_embedding_size)\n    \"\"\"\n    if _model is None:\n        raise Exception(\"Model was not loaded. Call load_model() before inference.\")\n     frames = torch.from_numpy(frames_batch).to(_device)\n    embed = _model.forward(frames).detach().cpu().numpy()\n    return embed",
        "import_statements": [
            "from encoder.params_data import *",
            "from encoder.model import SpeakerEncoder",
            "from encoder.audio import preprocess_wav",
            "from matplotlib import cm",
            "from encoder import audio",
            "from pathlib import Path",
            "import torch"
        ],
        "reference_api": [
            "torch.from_numpy",
            "cpu",
            "to",
            "_model.forward",
            "numpy",
            "Exception",
            "detach"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_model.forward",
                "code": "def forward(self, utterances, hidden_init=None):\n        \"\"\"\n        Computes the embeddings of a batch of utterance spectrograms.\n        \n        :param utterances: batch of mel-scale filterbanks of same duration as a tensor of shape \n        (batch_size, n_frames, n_channels) \n        :param hidden_init: initial hidden state of the LSTM as a tensor of shape (num_layers, \n        batch_size, hidden_size). Will default to a tensor of zeros if None.\n        :return: the embeddings as a tensor of shape (batch_size, embedding_size)\n        \"\"\"\n        # Pass the input through the LSTM layers and retrieve all outputs, the final hidden state\n        # and the final cell state.\n        out, (hidden, cell) = self.lstm(utterances, hidden_init)\n        \n        # We take only the hidden state of the last layer\n        embeds_raw = self.relu(self.linear(hidden[-1]))\n        \n        # L2-normalize it\n        embeds = embeds_raw / (torch.norm(embeds_raw, dim=1, keepdim=True) + 1e-5)        \n\n        return embeds"
            }
        ],
        "third_party": [
            "to",
            "torch.from_numpy",
            "numpy",
            "cpu",
            "detach"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5)",
        "start_line": "58",
        "end_line": "107",
        "file_path": "encoder/inference.py",
        "docstring": "The function compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\\n                           min_pad_coverage=0.75, overlap=0.5) computes partial slices for audio processing. It ensures the overlap and minimum padding coverage are within valid ranges using assertions.\\nIt calculates the number of frames and the frame step based on the sampling rate and overlap. It then computes wav and mel slices using a loop, considering the steps required based on the frames and frame step. After computing the slices, it evaluates the need for extra padding by checking the coverage of the last slice. If the coverage is below the minimum pad coverage and there is more than one slice, it removes the last slice from both wav and mel slices. Finally, it returns the computed wav and mel slices.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "117a41716c8d",
        "ground_truth": "def compute_partial_slices(n_samples, partial_utterance_n_frames=partials_n_frames,\n                           min_pad_coverage=0.75, overlap=0.5):\n    \"\"\"\n    Computes where to split an utterance waveform and its corresponding mel spectrogram to obtain\n    partial utterances of <partial_utterance_n_frames> each. Both the waveform and the mel\n    spectrogram slices are returned, so as to make each partial utterance waveform correspond to\n    its spectrogram. This function assumes that the mel spectrogram parameters used are those\n    defined in params_data.py.\n     The returned ranges may be indexing further than the length of the waveform. It is\n    recommended that you pad the waveform with zeros up to wave_slices[-1].stop.\n     :param n_samples: the number of samples in the waveform\n    :param partial_utterance_n_frames: the number of mel spectrogram frames in each partial\n    utterance\n    :param min_pad_coverage: when reaching the last partial utterance, it may or may not have\n    enough frames. If at least <min_pad_coverage> of <partial_utterance_n_frames> are present,\n    then the last partial utterance will be considered, as if we padded the audio. Otherwise,\n    it will be discarded, as if we trimmed the audio. If there aren't enough frames for 1 partial\n    utterance, this parameter is ignored so that the function always returns at least 1 slice.\n    :param overlap: by how much the partial utterance should overlap. If set to 0, the partial\n    utterances are entirely disjoint.\n    :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\n    respectively the waveform and the mel spectrogram with these slices to obtain the partial\n    utterances.\n    \"\"\"\n    assert 0 <= overlap < 1\n    assert 0 < min_pad_coverage <= 1\n     samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n    frame_step = max(int(np.round(partial_utterance_n_frames * (1 - overlap))), 1)\n     # Compute the slices\n    wav_slices, mel_slices = [], []\n    steps = max(1, n_frames - partial_utterance_n_frames + frame_step + 1)\n    for i in range(0, steps, frame_step):\n        mel_range = np.array([i, i + partial_utterance_n_frames])\n        wav_range = mel_range * samples_per_frame\n        mel_slices.append(slice(*mel_range))\n        wav_slices.append(slice(*wav_range))\n     # Evaluate whether extra padding is warranted or not\n    last_wav_range = wav_slices[-1]\n    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n    if coverage < min_pad_coverage and len(mel_slices) > 1:\n        mel_slices = mel_slices[:-1]\n        wav_slices = wav_slices[:-1]\n     return wav_slices, mel_slices",
        "import_statements": [
            "from encoder.params_data import *",
            "from encoder.model import SpeakerEncoder",
            "from encoder.audio import preprocess_wav",
            "from matplotlib import cm",
            "from encoder import audio",
            "from pathlib import Path",
            "import torch"
        ],
        "reference_api": [
            "int",
            "len",
            "wav_slices.append",
            "mel_slices.append",
            "np.ceil",
            "np.array",
            "slice",
            "max",
            "np.round",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.ceil",
            "np.round",
            "np.array",
            "mel_slices.append",
            "wav_slices.append"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30))",
        "start_line": "161",
        "end_line": "178",
        "file_path": "encoder/inference.py",
        "docstring": "The function plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30)) visualizes a given embedding as a heatmap using Matplotlib. It first checks if an Axes object (ax) is provided; if not, it uses the current Axes. If the shape of the embedding is not specified, it calculates the height by taking the square root of the embedding's length and reshapes the embedding accordingly. The function then plots the reshaped embedding as a heatmap using a colormap (cmap). A color bar is added to the plot for reference, with its limits set by the color_range parameter. The x and y ticks are removed for a cleaner visualization, and the specified title is set for the plot.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "34fe5c864f1c",
        "ground_truth": "def plot_embedding_as_heatmap(embed, ax=None, title=\"\", shape=None, color_range=(0, 0.30)):\n    import matplotlib.pyplot as plt\n    if ax is None:\n        ax = plt.gca()\n     if shape is None:\n        height = int(np.sqrt(len(embed)))\n        shape = (height, -1)\n    embed = embed.reshape(shape)\n     cmap = cm.get_cmap()\n    mappable = ax.imshow(embed, cmap=cmap)\n    cbar = plt.colorbar(mappable, ax=ax, fraction=0.046, pad=0.04)\n    sm = cm.ScalarMappable(cmap=cmap)\n    sm.set_clim(*color_range)\n     ax.set_xticks([]), ax.set_yticks([])\n    ax.set_title(title)",
        "import_statements": [
            "from encoder.params_data import *",
            "from encoder.model import SpeakerEncoder",
            "from encoder.audio import preprocess_wav",
            "from matplotlib import cm",
            "from encoder import audio",
            "from pathlib import Path",
            "import torch"
        ],
        "reference_api": [
            "plt.colorbar",
            "int",
            "len",
            "cm.ScalarMappable",
            "ax.set_xticks",
            "cm.get_cmap",
            "ax.set_title",
            "ax.imshow",
            "sm.set_clim",
            "ax.set_yticks",
            "np.sqrt",
            "embed.reshape",
            "plt.gca"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "plt.gca",
            "np.sqrt",
            "embed.reshape",
            "cm.get_cmap",
            "ax.imshow",
            "plt.colorbar",
            "cm.ScalarMappable",
            "sm.set_clim",
            "ax.set_xticks",
            "ax.set_yticks",
            "ax.set_title"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def similarity_matrix(self, embeds)",
        "start_line": "63",
        "end_line": "105",
        "file_path": "encoder/model.py",
        "docstring": "The function \\\"similarity_matrix\\\" calculates a similarity matrix for speaker embeddings. It starts by determining the number of speakers per batch and the number of utterances per speaker from the shape of the input embeddings. It then computes two types of centroids: \\n1. \\\"centroids_incl\\\" which includes the current embedding in the mean calculation and normalizes it. \\n2. \\\"centroids_excl\\\" which excludes the current embedding from the mean calculation, averages the remaining embeddings, and normalizes it. \\nThe function initializes a similarity matrix with zeros on the specified device and a mask matrix to exclude self-similarity. For each speaker, it computes the similarity scores using the respective centroids and updates the similarity matrix accordingly. Finally, it scales and biases the similarity matrix before returning it.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "92e93d50b62b",
        "ground_truth": "def similarity_matrix(self, embeds):\n    \"\"\"\n    Computes the similarity matrix according the section 2.1 of GE2E.\n    :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n    utterances_per_speaker, embedding_size)\n    :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n    utterances_per_speaker, speakers_per_batch)\n    \"\"\"\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n         # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n    # Exclusive centroids (1 per utterance)\n    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n    centroids_excl /= (utterances_per_speaker - 1)\n    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n    # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n    # We vectorize the computation for efficiency.\n    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n                             speakers_per_batch).to(self.loss_device)\n    mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n    for j in range(speakers_per_batch):\n        mask = np.where(mask_matrix[j])[0]\n        sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n        sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n         ## Even more vectorized version (slower maybe because of transpose)\n    # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances_per_speaker\n    #                           ).to(self.loss_device)\n    # eye = np.eye(speakers_per_batch, dtype=np.int)\n    # mask = np.where(1 - eye)\n    # sim_matrix2[mask] = (embeds[mask[0]] * centroids_incl[mask[1]]).sum(dim=2)\n    # mask = np.where(eye)\n    # sim_matrix2[mask] = (embeds * centroids_excl).sum(dim=2)\n    # sim_matrix2 = sim_matrix2.transpose(1, 2)\n         sim_matrix = sim_matrix * self.similarity_weight + self.similarity_bias\n    return sim_matrix",
        "import_statements": [
            "from encoder.params_model import *",
            "from encoder.params_data import *",
            "from scipy.interpolate import interp1d",
            "from sklearn.metrics import roc_curve",
            "from torch.nn.utils import clip_grad_norm_",
            "from scipy.optimize import brentq",
            "from torch import nn",
            "import torch"
        ],
        "reference_api": [
            "centroids_incl.clone",
            "np.where",
            "torch.mean",
            "np.eye",
            "torch.zeros",
            "torch.norm",
            "to",
            "sum",
            "torch.sum",
            "centroids_excl.clone",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "torch.mean",
            "centroids_incl.clone",
            "torch.norm",
            "torch.sum",
            "centroids_excl.clone",
            "torch.norm",
            "to",
            "torch.zeros",
            "np.eye",
            "np.where"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def loss(self, embeds)",
        "start_line": "107",
        "end_line": "135",
        "file_path": "encoder/model.py",
        "docstring": "```The function calculates the loss and the Equal Error Rate (EER) for a batch of speaker embeddings. It first determines the number of speakers and utterances per speaker from the shape of the input embeddings. It then computes a similarity matrix and reshapes it. Ground truth labels are generated, and the loss is calculated using these labels and the similarity matrix. The EER is computed without backpropagation by comparing the predicted and true labels using the ROC curve and finding the threshold where the false positive rate equals the false negative rate. The function returns both the loss and the EER.```",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a6eba54108a1",
        "ground_truth": "def loss(self, embeds):\n    \"\"\"\n    Computes the softmax loss according the section 2.1 of GE2E.\n         :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n    utterances_per_speaker, embedding_size)\n    :return: the loss and the EER for this batch of embeddings.\n    \"\"\"\n    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n         # Loss\n    sim_matrix = self.similarity_matrix(embeds)\n    sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker, \n                                     speakers_per_batch))\n    ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n    target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n    loss = self.loss_fn(sim_matrix, target)\n         # EER (not backpropagated)\n    with torch.no_grad():\n        inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=np.int)[0]\n        labels = np.array([inv_argmax(i) for i in ground_truth])\n        preds = sim_matrix.detach().cpu().numpy()\n        # Snippet from https://yangcha.github.io/EER-ROC/\n        fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())           \n        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n             return loss, eer",
        "import_statements": [
            "from encoder.params_model import *",
            "from encoder.params_data import *",
            "from scipy.interpolate import interp1d",
            "from sklearn.metrics import roc_curve",
            "from torch.nn.utils import clip_grad_norm_",
            "from scipy.optimize import brentq",
            "from torch import nn",
            "import torch"
        ],
        "reference_api": [
            "torch.from_numpy",
            "preds.flatten",
            "self.loss_fn",
            "numpy",
            "np.arange",
            "interp1d",
            "np.repeat",
            "brentq",
            "long",
            "cpu",
            "labels.flatten",
            "torch.no_grad",
            "roc_curve",
            "inv_argmax",
            "np.eye",
            "self.similarity_matrix",
            "to",
            "sim_matrix.reshape",
            "sim_matrix.detach",
            "np.array"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.similarity_matrix",
                "code": "def similarity_matrix(self, embeds):\n        \"\"\"\n        Computes the similarity matrix according the section 2.1 of GE2E.\n\n        :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n        utterances_per_speaker, embedding_size)\n        :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n        utterances_per_speaker, speakers_per_batch)\n        \"\"\"\n        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n        \n        # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n        centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n        centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n\n        # Exclusive centroids (1 per utterance)\n        centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n        centroids_excl /= (utterances_per_speaker - 1)\n        centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n\n        # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n        # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n        # We vectorize the computation for efficiency.\n        sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n                                 speakers_per_batch).to(self.loss_device)\n        mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n        for j in range(speakers_per_batch):\n            mask = np.where(mask_matrix[j])[0]\n            sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n            sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n        \n        ## Even more vectorized version (slower maybe because of transpose)\n        # sim_matrix2 = torch.zeros(speakers_per_batch, speakers_per_batch, utterances_per_speaker\n        #                           ).to(self.loss_device)\n        # eye = np.eye(speakers_per_batch, dtype=np.int)\n        # mask = np.where(1 - eye)\n        # sim_matrix2[mask] = (embeds[mask[0]] * centroids_incl[mask[1]]).sum(dim=2)\n        # mask = np.where(eye)\n        # sim_matrix2[mask] = (embeds * centroids_excl).sum(dim=2)\n        # sim_matrix2 = sim_matrix2.transpose(1, 2)\n        \n        sim_matrix = sim_matrix * self.similarity_weight + self.similarity_bias\n        return sim_matrix"
            }
        ],
        "third_party": [
            "sim_matrix.reshape",
            "np.repeat",
            "np.arange",
            "to",
            "long",
            "torch.from_numpy",
            "self.loss_fn",
            "torch.no_grad",
            "np.eye",
            "np.array",
            "inv_argmax",
            "numpy",
            "cpu",
            "sim_matrix.detach",
            "roc_curve",
            "labels.flatten",
            "preds.flatten",
            "brentq",
            "interp1d"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def log_params(self)",
        "start_line": "66",
        "end_line": "79",
        "file_path": "encoder/visualizations.py",
        "docstring": "The function logs model and data parameters if not disabled.\\nIt imports parameter configurations, then iterates through the model and data parameters, appending each name and value to a formatted string.\\nThis string is displayed using a visualization tool, with the title \"Parameters\".",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ac7b74b9d1b1",
        "ground_truth": "def log_params(self):\n    if self.disabled:\n        return\n    from encoder import params_data\n    from encoder import params_model\n    param_string = \"<b>Model parameters</b>:<br>\"\n    for param_name in (p for p in dir(params_model) if not p.startswith(\"__\")):\n        value = getattr(params_model, param_name)\n        param_string += \"\\t%s: %s<br>\" % (param_name, value)\n    param_string += \"<b>Data parameters</b>:<br>\"\n    for param_name in (p for p in dir(params_data) if not p.startswith(\"__\")):\n        value = getattr(params_data, param_name)\n        param_string += \"\\t%s: %s<br>\" % (param_name, value)\n    self.vis.text(param_string, opts={\"title\": \"Parameters\"})",
        "import_statements": [
            "from datetime import datetime",
            "from time import perf_counter as timer",
            "import umap",
            "import visdom",
            "from encoder.data_objects.speaker_verification_dataset import SpeakerVerificationDataset"
        ],
        "reference_api": [
            "getattr",
            "p.startswith",
            "dir",
            "text"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "p.startswith",
            "p.startswith",
            "text"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10)",
        "start_line": "156",
        "end_line": "175",
        "file_path": "encoder/visualizations.py",
        "docstring": "The function visualizes speaker embeddings using UMAP projections.\\nIt limits the number of speakers, flattens the embeddings, and assigns colors based on speaker identity.\\nUMAP reduces the dimensionality of the embeddings, which are then plotted and displayed or saved to a file if specified.\\nThe plot title includes the current step, and the visualization is updated unless disabled.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4726b7864386",
        "ground_truth": "def draw_projections(self, embeds, utterances_per_speaker, step, out_fpath=None, max_speakers=10):\n    import matplotlib.pyplot as plt\n    max_speakers = min(max_speakers, len(colormap))\n    embeds = embeds[:max_speakers * utterances_per_speaker]\n    n_speakers = len(embeds) // utterances_per_speaker\n    ground_truth = np.repeat(np.arange(n_speakers), utterances_per_speaker)\n    colors = [colormap[i] for i in ground_truth]\n    reducer = umap.UMAP()\n    projected = reducer.fit_transform(embeds)\n    plt.scatter(projected[:, 0], projected[:, 1], c=colors)\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.title(\"UMAP projection (step %d)\" % step)\n    if not self.disabled:\n        self.projection_win = self.vis.matplot(plt, win=self.projection_win)\n    if out_fpath is not None:\n        plt.savefig(out_fpath)\n    plt.clf()",
        "import_statements": [
            "from datetime import datetime",
            "from time import perf_counter as timer",
            "import umap",
            "import visdom",
            "from encoder.data_objects.speaker_verification_dataset import SpeakerVerificationDataset"
        ],
        "reference_api": [
            "np.repeat",
            "plt.scatter",
            "min",
            "plt.savefig",
            "plt.title",
            "matplot",
            "len",
            "umap.UMAP",
            "reducer.fit_transform",
            "plt.clf",
            "set_aspect",
            "np.arange",
            "plt.gca"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.repeat",
            "np.arange",
            "umap.UMAP",
            "reducer.fit_transform",
            "plt.scatter",
            "set_aspect",
            "plt.gca",
            "plt.title",
            "matplot",
            "plt.savefig",
            "plt.clf"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def add_speaker_embedding(self, x, speaker_embedding)",
        "start_line": "46",
        "end_line": "73",
        "file_path": "synthesizer/models/tacotron.py",
        "docstring": "The function adds a speaker embedding to an input tensor.\\nIt first saves the dimensions of the input tensor as human-readable names and determines the appropriate dimension index for the speaker embedding.\\nIt then repeats the speaker embedding to match the length of the input text, reshapes, and transposes it.\\nFinally, it concatenates the speaker embedding with the input tensor along the last dimension and returns the result.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "274fa50985c0",
        "ground_truth": "def add_speaker_embedding(self, x, speaker_embedding):\n    # SV2TTS\n    # The input x is the encoder output and is a 3D tensor with size (batch_size, num_chars, tts_embed_dims)\n    # When training, speaker_embedding is also a 2D tensor with size (batch_size, speaker_embedding_size)\n    #     (for inference, speaker_embedding is a 1D tensor with size (speaker_embedding_size))\n    # This concats the speaker embedding for each char in the encoder output\n    # Save the dimensions as human-readable names\n    batch_size = x.size()[0]\n    num_chars = x.size()[1]\n    if speaker_embedding.dim() == 1:\n        idx = 0\n    else:\n        idx = 1\n    # Start by making a copy of each speaker embedding to match the input text length\n    # The output of this has size (batch_size, num_chars * tts_embed_dims)\n    speaker_embedding_size = speaker_embedding.size()[idx]\n    e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n    # Reshape it and transpose\n    e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n    e = e.transpose(1, 2)\n    # Concatenate the tiled speaker embedding with the encoder output\n    x = torch.cat((x, e), 2)\n    return x",
        "import_statements": [
            "import os",
            "import torch",
            "from pathlib import Path",
            "from typing import Union"
        ],
        "reference_api": [
            "speaker_embedding.repeat_interleave",
            "x.size",
            "speaker_embedding.size",
            "torch.cat",
            "e.reshape",
            "e.transpose",
            "speaker_embedding.dim"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "x.size",
            "x.size",
            "speaker_embedding.dim",
            "speaker_embedding.size",
            "speaker_embedding.repeat_interleave",
            "e.reshape",
            "e.transpose",
            "torch.cat"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def init_attention(self, encoder_seq_proj)",
        "start_line": "215",
        "end_line": "219",
        "file_path": "synthesizer/models/tacotron.py",
        "docstring": "The function initializes attention-related tensors for the model.\\nIt first determines the device from the model's parameters and the shape of the input sequence projection.\\nThen, it creates and zeroes cumulative and attention tensors with the same batch size and sequence length as the input, ensuring they are on the correct device.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e6742ea0fe5e",
        "ground_truth": "def init_attention(self, encoder_seq_proj):\n    device = next(self.parameters()).device  # use same device as parameters\n    b, t, c = encoder_seq_proj.size()\n    self.cumulative = torch.zeros(b, t, device=device)\n    self.attention = torch.zeros(b, t, device=device)",
        "import_statements": [
            "import os",
            "import torch",
            "from pathlib import Path",
            "from typing import Union"
        ],
        "reference_api": [
            "self.parameters",
            "next",
            "torch.zeros",
            "encoder_seq_proj.size"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self.parameters",
            "encoder_seq_proj.size",
            "torch.zeros",
            "torch.zeros"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def load(self, path, optimizer=None)",
        "start_line": "493",
        "end_line": "500",
        "file_path": "synthesizer/models/tacotron.py",
        "docstring": "The function loads a model checkpoint from a given path.\\nIt sets the device to the model's parameter device and loads the checkpoint.\\nThe model's state dictionary is updated with the loaded state.\\nIf the checkpoint contains an optimizer state and an optimizer is provided, the optimizer's state dictionary is also updated.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4e1fc1ed0ed7",
        "ground_truth": "def load(self, path, optimizer=None):\n    # Use device of model params as location for loaded state\n    device = next(self.parameters()).device\n    checkpoint = torch.load(str(path), map_location=device)\n    self.load_state_dict(checkpoint[\"model_state\"])\n    if \"optimizer_state\" in checkpoint and optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])",
        "import_statements": [
            "import os",
            "import torch",
            "from pathlib import Path",
            "from typing import Union"
        ],
        "reference_api": [
            "torch.load",
            "self.load_state_dict",
            "next",
            "optimizer.load_state_dict",
            "self.parameters",
            "str"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "torch.load",
                "code": "def load(self, path, optimizer=None):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        checkpoint = torch.load(str(path), map_location=device)\n        self.load_state_dict(checkpoint[\"model_state\"])\n\n        if \"optimizer_state\" in checkpoint and optimizer is not None:\n            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])"
            }
        ],
        "third_party": [
            "self.parameters",
            "self.load_state_dict",
            "optimizer.load_state_dict"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def save(self, path, optimizer=None)",
        "start_line": "502",
        "end_line": "511",
        "file_path": "synthesizer/models/tacotron.py",
        "docstring": "The function saves the model state to a specified path.\\nIf an optimizer is provided, it saves both the model state and the optimizer state.\\nIf no optimizer is provided, it saves only the model state.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "178e6aa16c0f",
        "ground_truth": "def save(self, path, optimizer=None):\n    if optimizer is not None:\n        torch.save({\n            \"model_state\": self.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n        }, str(path))\n    else:\n        torch.save({\n            \"model_state\": self.state_dict(),\n        }, str(path))",
        "import_statements": [
            "import os",
            "import torch",
            "from pathlib import Path",
            "from typing import Union"
        ],
        "reference_api": [
            "str",
            "self.state_dict",
            "torch.save",
            "optimizer.state_dict"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "torch.save",
                "code": "def save(self, path, optimizer=None):\n        if optimizer is not None:\n            torch.save({\n                \"model_state\": self.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n            }, str(path))\n        else:\n            torch.save({\n                \"model_state\": self.state_dict(),\n            }, str(path))"
            },
            {
                "name": "torch.save",
                "code": "def save(self, path, optimizer=None):\n        if optimizer is not None:\n            torch.save({\n                \"model_state\": self.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n            }, str(path))\n        else:\n            torch.save({\n                \"model_state\": self.state_dict(),\n            }, str(path))"
            }
        ],
        "third_party": [
            "self.state_dict",
            "optimizer.state_dict",
            "self.state_dict"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def english_cleaners(text)",
        "start_line": "81",
        "end_line": "88",
        "file_path": "synthesizer/utils/cleaners.py",
        "docstring": "The function processes English text through a series of cleaning steps.\\nIt converts the text to ASCII, lowercases it, expands numbers and abbreviations, and collapses whitespace.\\nThe cleaned text is then returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6528fd80e87b",
        "ground_truth": "def english_cleaners(text):\n    \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text",
        "import_statements": [
            "import re",
            "from unidecode import unidecode",
            "from synthesizer.utils.numbers import normalize_numbers"
        ],
        "reference_api": [
            "collapse_whitespace",
            "lowercase",
            "expand_numbers",
            "convert_to_ascii",
            "expand_abbreviations"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "convert_to_ascii",
                "code": "def convert_to_ascii(text):\n    return unidecode(text)"
            },
            {
                "name": "lowercase",
                "code": "def lowercase(text):\n    \"\"\"lowercase input tokens.\"\"\"\n    return text.lower()"
            },
            {
                "name": "expand_numbers",
                "code": "def expand_numbers(text):\n    return normalize_numbers(text)"
            },
            {
                "name": "expand_abbreviations",
                "code": "def expand_abbreviations(text):\n    for regex, replacement in _abbreviations:\n        text = re.sub(regex, replacement, text)\n    return text"
            },
            {
                "name": "collapse_whitespace",
                "code": "def collapse_whitespace(text):\n    return re.sub(_whitespace_re, \" \", text)"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def text_to_sequence(text, cleaner_names)",
        "start_line": "14",
        "end_line": "41",
        "file_path": "synthesizer/utils/text.py",
        "docstring": "The function converts text to a sequence of symbol IDs.\\nIt processes the text, identifying segments within curly braces as ARPAbet phonetic notation, and converts these segments separately.\\nRegular text is cleaned and converted to symbol IDs using specified cleaners.\\nFinally, an end-of-sequence token is appended before returning the sequence.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8a4fe08e0985",
        "ground_truth": "def text_to_sequence(text, cleaner_names):\n    \"\"\"Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n       The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n      in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n       Args:\n        text: string to convert to a sequence\n        cleaner_names: names of the cleaner functions to run the text through\n       Returns:\n        List of integers corresponding to the symbols in the text\n    \"\"\"\n    sequence = []\n     # Check for curly braces and treat their contents as ARPAbet:\n    while len(text):\n        m = _curly_re.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n     # Append EOS token\n    sequence.append(_symbol_to_id[\"~\"])\n    return sequence",
        "import_statements": [
            "from synthesizer.utils.symbols import symbols",
            "from synthesizer.utils import cleaners",
            "import re"
        ],
        "reference_api": [
            "_curly_re.match",
            "_clean_text",
            "_arpabet_to_sequence",
            "len",
            "m.group",
            "sequence.append",
            "_symbols_to_sequence"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_symbols_to_sequence",
                "code": "def _symbols_to_sequence(symbols):\n    return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]"
            },
            {
                "name": "_clean_text",
                "code": "def _clean_text(text, cleaner_names):\n    for name in cleaner_names:\n        cleaner = getattr(cleaners, name)\n        if not cleaner:\n            raise Exception(\"Unknown cleaner: %s\" % name)\n        text = cleaner(text)\n    return text"
            },
            {
                "name": "_symbols_to_sequence",
                "code": "def _symbols_to_sequence(symbols):\n    return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]"
            },
            {
                "name": "_clean_text",
                "code": "def _clean_text(text, cleaner_names):\n    for name in cleaner_names:\n        cleaner = getattr(cleaners, name)\n        if not cleaner:\n            raise Exception(\"Unknown cleaner: %s\" % name)\n        text = cleaner(text)\n    return text"
            },
            {
                "name": "_arpabet_to_sequence",
                "code": "def _arpabet_to_sequence(text):\n    return _symbols_to_sequence([\"@\" + s for s in text.split()])"
            }
        ],
        "third_party": [
            "_curly_re.match",
            "m.group",
            "m.group",
            "m.group",
            "sequence.append"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def sequence_to_text(sequence)",
        "start_line": "44",
        "end_line": "54",
        "file_path": "synthesizer/utils/text.py",
        "docstring": "The function converts a sequence of IDs into a string.\\nIt iterates through the sequence, retrieves corresponding symbols from a dictionary, encloses ARPAbet symbols in curly braces, and concatenates them into a result string.\\nThe function ensures that adjacent curly braces are replaced with a space.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e1c25a37af1f",
        "ground_truth": "def sequence_to_text(sequence):\n    \"\"\"Converts a sequence of IDs back to a string\"\"\"\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            result += s\n    return result.replace(\"}{\", \" \")",
        "import_statements": [
            "from synthesizer.utils.symbols import symbols",
            "from synthesizer.utils import cleaners",
            "import re"
        ],
        "reference_api": [
            "len",
            "result.replace"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "result.replace"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def embed_utterance(fpaths, encoder_model_fpath)",
        "start_line": "229",
        "end_line": "238",
        "file_path": "synthesizer/preprocess.py",
        "docstring": "The function generates and saves the speaker embedding for a given utterance.\\nIt first loads the encoder model if it is not already loaded.\\nIt then loads and preprocesses the waveform from the specified file path, computes the speaker embedding, and saves the embedding to the specified file path.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7807cad4b05d",
        "ground_truth": "def embed_utterance(fpaths, encoder_model_fpath):\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n     # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)",
        "import_statements": [
            "from multiprocessing.pool import Pool",
            "from synthesizer import audio",
            "from functools import partial",
            "from itertools import chain",
            "from encoder import inference as encoder",
            "from pathlib import Path",
            "from utils import logmmse",
            "from tqdm import tqdm",
            "import librosa"
        ],
        "reference_api": [
            "encoder.embed_utterance",
            "np.load",
            "encoder.is_loaded",
            "encoder.preprocess_wav",
            "encoder.load_model",
            "np.save"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "encoder.embed_utterance",
                "code": "def embed_utterance(fpaths, encoder_model_fpath):\n    if not encoder.is_loaded():\n        encoder.load_model(encoder_model_fpath)\n\n    # Compute the speaker embedding of the utterance\n    wav_fpath, embed_fpath = fpaths\n    wav = np.load(wav_fpath)\n    wav = encoder.preprocess_wav(wav)\n    embed = encoder.embed_utterance(wav)\n    np.save(embed_fpath, embed, allow_pickle=False)"
            }
        ],
        "third_party": [
            "encoder.is_loaded",
            "encoder.load_model",
            "np.load",
            "encoder.preprocess_wav",
            "np.save"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int)",
        "start_line": "241",
        "end_line": "257",
        "file_path": "synthesizer/preprocess.py",
        "docstring": "The function creates embeddings for audio files.\\nIt verifies the existence of required directories and files, then reads metadata and prepares file paths for audio and embeddings.\\nA multiprocessing pool is used to process the audio files in parallel, generating embeddings using a specified encoder model.\\nProgress is tracked and displayed using a progress bar.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9f860273a325",
        "ground_truth": "def create_embeddings(synthesizer_root: Path, encoder_model_fpath: Path, n_processes: int):\n    wav_dir = synthesizer_root.joinpath(\"audio\")\n    metadata_fpath = synthesizer_root.joinpath(\"train.txt\")\n    assert wav_dir.exists() and metadata_fpath.exists()\n    embed_dir = synthesizer_root.joinpath(\"embeds\")\n    embed_dir.mkdir(exist_ok=True)\n     # Gather the input wave filepath and the target output embed filepath\n    with metadata_fpath.open(\"r\") as metadata_file:\n        metadata = [line.split(\"|\") for line in metadata_file]\n        fpaths = [(wav_dir.joinpath(m[0]), embed_dir.joinpath(m[2])) for m in metadata]\n     # TODO: improve on the multiprocessing, it's terrible. Disk I/O is the bottleneck here.\n    # Embed the utterances in separate threads\n    func = partial(embed_utterance, encoder_model_fpath=encoder_model_fpath)\n    job = Pool(n_processes).imap(func, fpaths)\n    list(tqdm(job, \"Embedding\", len(fpaths), unit=\"utterances\"))",
        "import_statements": [
            "from multiprocessing.pool import Pool",
            "from synthesizer import audio",
            "from functools import partial",
            "from itertools import chain",
            "from encoder import inference as encoder",
            "from pathlib import Path",
            "from utils import logmmse",
            "from tqdm import tqdm",
            "import librosa"
        ],
        "reference_api": [
            "synthesizer_root.joinpath",
            "list",
            "wav_dir.exists",
            "Pool",
            "metadata_fpath.open",
            "len",
            "wav_dir.joinpath",
            "line.split",
            "embed_dir.joinpath",
            "metadata_fpath.exists",
            "embed_dir.mkdir",
            "tqdm",
            "partial",
            "imap"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "synthesizer_root.joinpath",
            "synthesizer_root.joinpath",
            "wav_dir.exists",
            "metadata_fpath.exists",
            "synthesizer_root.joinpath",
            "embed_dir.mkdir",
            "metadata_fpath.open",
            "line.split",
            "wav_dir.joinpath",
            "embed_dir.joinpath",
            "partial",
            "imap",
            "Pool",
            "tqdm"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams)",
        "start_line": "237",
        "end_line": "258",
        "file_path": "synthesizer/train.py",
        "docstring": "The function evaluates the model by saving various outputs and visualizations for a given step and sample number.\\nIt saves the attention plot, predicted mel spectrogram, and reconstructed waveform to specified directories.\\nIt also generates and saves a plot comparing the predicted and target mel spectrograms with a title indicating the model type, current time, step number, and loss.\\nFinally, it prints the input sequence at the current step.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9eb0abda14a9",
        "ground_truth": "def eval_model(attention, mel_prediction, target_spectrogram, input_seq, step,\n               plot_dir, mel_output_dir, wav_dir, sample_num, loss, hparams):\n    # Save some results for evaluation\n    attention_path = str(plot_dir.joinpath(\"attention_step_{}_sample_{}\".format(step, sample_num)))\n    save_attention(attention, attention_path)\n     # save predicted mel spectrogram to disk (debug)\n    mel_output_fpath = mel_output_dir.joinpath(\"mel-prediction-step-{}_sample_{}.npy\".format(step, sample_num))\n    np.save(str(mel_output_fpath), mel_prediction, allow_pickle=False)\n     # save griffin lim inverted wav for debug (mel -> wav)\n    wav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n    wav_fpath = wav_dir.joinpath(\"step-{}-wave-from-mel_sample_{}.wav\".format(step, sample_num))\n    audio.save_wav(wav, str(wav_fpath), sr=hparams.sample_rate)\n     # save real and predicted mel-spectrogram plot to disk (control purposes)\n    spec_fpath = plot_dir.joinpath(\"step-{}-mel-spectrogram_sample_{}.png\".format(step, sample_num))\n    title_str = \"{}, {}, step={}, loss={:.5f}\".format(\"Tacotron\", time_string(), step, loss)\n    plot_spectrogram(mel_prediction, str(spec_fpath), title=title_str,\n                     target_spectrogram=target_spectrogram,\n                     max_len=target_spectrogram.size // hparams.num_mels)\n    print(\"Input at step {}: {}\".format(step, sequence_to_text(input_seq)))",
        "import_statements": [
            "from datetime import datetime",
            "from functools import partial",
            "from pathlib import Path",
            "import torch",
            "from torch import optim",
            "from torch.utils.data import DataLoader",
            "from synthesizer import audio",
            "from synthesizer.models.tacotron import Tacotron",
            "from synthesizer.synthesizer_dataset import SynthesizerDataset, collate_synthesizer",
            "from synthesizer.utils import ValueWindow, data_parallel_workaround",
            "from synthesizer.utils.plot import plot_spectrogram",
            "from synthesizer.utils.symbols import symbols",
            "from synthesizer.utils.text import sequence_to_text",
            "from vocoder.display import *"
        ],
        "reference_api": [
            "audio.save_wav",
            "print",
            "mel_output_dir.joinpath",
            "save_attention",
            "plot_spectrogram",
            "wav_dir.joinpath",
            "sequence_to_text",
            "plot_dir.joinpath",
            "time_string",
            "str",
            "audio.inv_mel_spectrogram",
            "format",
            "np.save"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "save_attention",
                "code": "save_attention(attn, path):\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n    fig.savefig(f'{path}.png', bbox_inches='tight')\n    plt.close(fig)\n\n\nd"
            },
            {
                "name": "np.save",
                "code": "def save(self, path, optimizer=None):\n        if optimizer is not None:\n            torch.save({\n                \"model_state\": self.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n            }, str(path))\n        else:\n            torch.save({\n                \"model_state\": self.state_dict(),\n            }, str(path))"
            },
            {
                "name": "time_string",
                "code": "def time_string():\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M\")"
            },
            {
                "name": "plot_spectrogram",
                "code": "def plot_spectrogram(pred_spectrogram, path, title=None, split_title=False, target_spectrogram=None, max_len=None, auto_aspect=False):\n\timport matplotlib\n\tmatplotlib.use(\"Agg\")\n\timport matplotlib.pyplot as plt\n\n\tif max_len is not None:\n\t\ttarget_spectrogram = target_spectrogram[:max_len]\n\t\tpred_spectrogram = pred_spectrogram[:max_len]\n\n\tif split_title:\n\t\ttitle = split_title_line(title)\n\n\tfig = plt.figure(figsize=(10, 8))\n\t# Set common labels\n\tfig.text(0.5, 0.18, title, horizontalalignment=\"center\", fontsize=16)\n\n\t#target spectrogram subplot\n\tif target_spectrogram is not None:\n\t\tax1 = fig.add_subplot(311)\n\t\tax2 = fig.add_subplot(312)\n\n\t\tif auto_aspect:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), aspect=\"auto\", interpolation=\"none\")\n\t\telse:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), interpolation=\"none\")\n\t\tax1.set_title(\"Target Mel-Spectrogram\")\n\t\tfig.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", ax=ax1)\n\t\tax2.set_title(\"Predicted Mel-Spectrogram\")\n\telse:\n\t\tax2 = fig.add_subplot(211)\n\n\tif auto_aspect:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), aspect=\"auto\", interpolation=\"none\")\n\telse:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), interpolation=\"none\")\n\tfig.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", ax=ax2)\n\n\tplt.tight_layout()\n\tplt.savefig(path, format=\"png\")\n\tplt.close()"
            },
            {
                "name": "sequence_to_text",
                "code": "def sequence_to_text(sequence):\n    \"\"\"Converts a sequence of IDs back to a string\"\"\"\n    result = \"\"\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \"@\":\n                s = \"{%s}\" % s[1:]\n            result += s\n    return result.replace(\"}{\", \" \")"
            }
        ],
        "third_party": [
            "plot_dir.joinpath",
            "mel_output_dir.joinpath",
            "audio.inv_mel_spectrogram",
            "wav_dir.joinpath",
            "audio.save_wav",
            "plot_dir.joinpath"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def num_params(self)",
        "start_line": "167",
        "end_line": "170",
        "file_path": "vocoder/models/deepmind_version.py",
        "docstring": "The function calculates and prints the number of trainable parameters in the model.\\nIt filters the parameters that require gradients, computes the product of their sizes, sums them up, and converts the total to millions.\\nFinally, it prints the result with a precision of three decimal places.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6bad4f500286",
        "ground_truth": "def num_params(self) :\n    parameters = filter(lambda p: p.requires_grad, self.parameters())\n    parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n    print('Trainable Parameters: %.3f million' % parameters)",
        "import_statements": [
            "import torch",
            "from utils.display import *",
            "from utils.dsp import *"
        ],
        "reference_api": [
            "filter",
            "np.prod",
            "print",
            "sum",
            "p.size",
            "self.parameters"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self.parameters",
            "np.prod",
            "p.size"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def pad_tensor(self, x, pad, side='both')",
        "start_line": "273",
        "end_line": "286",
        "file_path": "vocoder/models/fatchord_version.py",
        "docstring": "The function pads a given tensor along the time dimension based on the specified padding and side.\\nIt first determines the new total size of the tensor after padding.\\nThen, it creates a new tensor filled with zeros on either the CPU or GPU, depending on availability.\\nThe original tensor is placed into this new padded tensor according to the specified side ('before', 'after', or 'both').\\nFinally, the padded tensor is returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "78b876054bd4",
        "ground_truth": "def pad_tensor(self, x, pad, side='both'):\n    # NB - this is just a quick method i need right now\n    # i.e., it won't generalise to other shapes/dims\n    b, t, c = x.size()\n    total = t + 2 * pad if side == 'both' else t + pad\n    if torch.cuda.is_available():\n        padded = torch.zeros(b, total, c).cuda()\n    else:\n        padded = torch.zeros(b, total, c).cpu()\n    if side == 'before' or side == 'both':\n        padded[:, pad:pad + t, :] = x\n    elif side == 'after':\n        padded[:, :t, :] = x\n    return padded",
        "import_statements": [
            "import torch",
            "from vocoder.distribution import sample_from_discretized_mix_logistic",
            "from vocoder.display import *",
            "from vocoder.audio import *"
        ],
        "reference_api": [
            "cpu",
            "x.size",
            "torch.zeros",
            "cuda",
            "is_available"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "x.size",
            "is_available",
            "cuda",
            "torch.zeros",
            "cpu",
            "torch.zeros"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def save_attention(attn, path)",
        "start_line": "86",
        "end_line": "92",
        "file_path": "vocoder/display.py",
        "docstring": "The function saves an attention matrix as an image file.\\nIt imports the necessary plotting library, creates a figure, and visualizes the attention matrix with specific settings.\\nThe resulting image is saved to the specified path with a .png extension, and the figure is then closed.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a7ed1715a3df",
        "ground_truth": "def save_attention(attn, path):\n    import matplotlib.pyplot as plt\n     fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation='nearest', aspect='auto')\n    fig.savefig(f'{path}.png', bbox_inches='tight')\n    plt.close(fig)",
        "import_statements": [
            "import time",
            "import sys"
        ],
        "reference_api": [
            "plt.close",
            "fig.savefig",
            "plt.imshow",
            "plt.figure"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "plt.figure",
            "plt.imshow",
            "fig.savefig",
            "plt.close"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "CorentinJ/Real-Time-Voice-Cloning",
        "function_declaration": "def plot(array)",
        "start_line": "106",
        "end_line": "117",
        "file_path": "vocoder/display.py",
        "docstring": "The function plots a given array using matplotlib.\\nIt creates a figure with a specified size, adds a subplot, and customizes the axis labels and tick parameters with specific colors and font sizes.\\nFinally, it plots the array on the customized subplot.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "19375c9f44a2",
        "ground_truth": "def plot(array):\n    import matplotlib.pyplot as plt\n     fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)",
        "import_statements": [
            "import time",
            "import sys"
        ],
        "reference_api": [
            "plt.figure",
            "set_fontsize",
            "fig.add_subplot",
            "ax.tick_params",
            "plt.plot",
            "set_color"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "plt.plot",
                "code": "plot(array):\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\n\nd"
            }
        ],
        "third_party": [
            "plt.figure",
            "fig.add_subplot",
            "set_color",
            "set_color",
            "set_fontsize",
            "set_fontsize",
            "ax.tick_params",
            "ax.tick_params"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "ultralytics/yolov5",
        "function_declaration": "def write_to_csv(image_name, prediction, confidence)",
        "start_line": "218",
        "end_line": "225",
        "file_path": "detect.py",
        "docstring": "The function writes prediction data for an image to a CSV file, appending if the file exists.\\nIt creates a dictionary with the image name, prediction, and confidence.\\nIt opens the CSV file in append mode, writes the header if the file is new, and then writes the data as a new row.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ff94c425920c",
        "ground_truth": "def write_to_csv(image_name, prediction, confidence):\n    \"\"\"Writes prediction data for an image to a CSV file, appending if the file exists.\"\"\"\n    data = {\"Image Name\": image_name, \"Prediction\": prediction, \"Confidence\": confidence}\n    with open(csv_path, mode=\"a\", newline=\"\") as f:\n        writer = csv.DictWriter(f, fieldnames=data.keys())\n        if not csv_path.is_file():\n            writer.writeheader()\n        writer.writerow(data)",
        "import_statements": [
            "import argparse",
            "import csv",
            "import os",
            "import platform",
            "import sys",
            "from pathlib import Path",
            "import torch",
            "from ultralytics.utils.plotting import Annotator, colors, save_one_box",
            "from models.common import DetectMultiBackend",
            "from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams",
            "from utils.general import (\n    LOGGER,\n    Profile,\n    check_file,\n    check_img_size,\n    check_imshow,\n    check_requirements,\n    colorstr,\n    cv2,\n    increment_path,\n    non_max_suppression,\n    print_args,\n    scale_boxes,\n    strip_optimizer,\n    xyxy2xywh,\n)",
            "from utils.torch_utils import select_device, smart_inference_mode"
        ],
        "reference_api": [
            "writer.writerow",
            "csv.DictWriter",
            "writer.writeheader",
            "csv_path.is_file",
            "data.keys",
            "open"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "data.keys",
            "csv_path.is_file",
            "writer.writeheader",
            "writer.writerow"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "ultralytics/yolov5",
        "function_declaration": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\"))",
        "start_line": "227",
        "end_line": "279",
        "file_path": "export.py",
        "docstring": "The function exports a PyTorch model to TorchScript format.\\nIt logs the start of the export, traces the model with a given input, and includes extra files with model details.\\nIf optimization is specified, it saves the model for mobile use; otherwise, it saves the standard TorchScript model.\\nThe function returns the file path of the saved model and None.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "436161602009",
        "ground_truth": "def export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\")):\n    \"\"\"\n    Export a YOLOv5 model to the TorchScript format.\n     Args:\n        model (torch.nn.Module): The YOLOv5 model to be exported.\n        im (torch.Tensor): Example input tensor to be used for tracing the TorchScript model.\n        file (Path): File path where the exported TorchScript model will be saved.\n        optimize (bool): If True, applies optimizations for mobile deployment.\n        prefix (str): Optional prefix for log messages. Default is 'TorchScript:'.\n     Returns:\n        (str | None, torch.jit.ScriptModule | None): A tuple containing the file path of the exported model\n            (as a string) and the TorchScript model (as a torch.jit.ScriptModule). If the export fails, both elements\n            of the tuple will be None.\n     Notes:\n        - This function uses tracing to create the TorchScript model.\n        - Metadata, including the input image shape, model stride, and class names, is saved in an extra file (`config.txt`)\n          within the TorchScript model package.\n        - For mobile optimization, refer to the PyTorch tutorial: https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n     Example:\n        ```python\n        from pathlib import Path\n        import torch\n        from models.experimental import attempt_load\n        from utils.torch_utils import select_device\n         # Load model\n        weights = 'yolov5s.pt'\n        device = select_device('')\n        model = attempt_load(weights, map_location=device)\n         # Example input tensor\n        im = torch.zeros(1, 3, 640, 640).to(device)\n         # Export model\n        file = Path('yolov5s.torchscript')\n        export_torchscript(model, im, file, optimize=False)\n        ```\n    \"\"\"\n    LOGGER.info(f\"\\n{prefix} starting export with torch {torch.__version__}...\")\n    f = file.with_suffix(\".torchscript\")\n     ts = torch.jit.trace(model, im, strict=False)\n    d = {\"shape\": im.shape, \"stride\": int(max(model.stride)), \"names\": model.names}\n    extra_files = {\"config.txt\": json.dumps(d)}  # torch._C.ExtraFilesMap()\n    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\n    else:\n        ts.save(str(f), _extra_files=extra_files)\n    return f, None",
        "import_statements": [
            "import argparse",
            "import contextlib",
            "import json",
            "import os",
            "import platform",
            "import re",
            "import subprocess",
            "import sys",
            "import time",
            "import warnings",
            "from pathlib import Path",
            "import torch",
            "from torch.utils.mobile_optimizer import optimize_for_mobile",
            "from models.experimental import attempt_load",
            "from models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel",
            "from utils.dataloaders import LoadImages",
            "from utils.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,\n)",
            "from utils.torch_utils import select_device, smart_inference_mode"
        ],
        "reference_api": [
            "file.with_suffix",
            "json.dumps",
            "_save_for_lite_interpreter",
            "int",
            "LOGGER.info",
            "optimize_for_mobile",
            "max",
            "ts.save",
            "str",
            "colorstr",
            "trace"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "colorstr",
                "code": "SI escape codes, e.g., colorstr('blue', 'hello world').\n\n    See https://en.wikipedia.org/wiki/ANSI_escape_code.\n    \"\"\"\n    *args, string = input if len(input) > 1 else (\"blue\", \"bold\", input[0])  # color arguments, string\n    colors = {\n        \"black\": \"\\033[30m\",  # basic colors\n        \"red\": \"\\033[31m\",\n        \"green\": \"\\033[32m\",\n        \"yellow\": \"\\033[33m\",\n        \"blue\": \"\\033[34m\",\n        \"magenta\": \"\\033[35m\",\n        \"cyan\": \"\\033[36m\",\n        \"white\": \"\\033[37m\",\n        \"bright_black\": \"\\033[90m\",  # bright colors\n        \"bright_red\": \"\\033[91m\",\n        \"bright_green\": \"\\033[92m\",\n        \"bright_yellow\": \"\\033[93m\",\n        \"bright_blue\": \"\\033[94m\",\n        \"bright_magenta\": \"\\033[95m\",\n        \"bright_cyan\": \"\\033[96m\",\n        \"bright_white\": \"\\033[97m\",\n        \"end\": \"\\033[0m\",  # misc\n        \"bold\": \"\\033[1m\",\n        \"underline\": \"\\033[4m\",\n    }\n    return \"\".join(colors[x] for x in args) + f\"{string}\" + colors[\"end\"]\n\n\ndef labels_to_class_weights(labels, nc=80):\n    \"\"\"Calc"
            },
            {
                "name": "LOGGER.info",
                "code": " info(self, verbose=False, img_size=640):\n        \"\"\"Prints model information given verbosity and image size, e.g., `info(verbose=True, img_size=640)`.\"\"\"\n        model_info(self, verbose, img_size)\n\n "
            }
        ],
        "third_party": [
            "file.with_suffix",
            "_save_for_lite_interpreter",
            "optimize_for_mobile",
            "ts.save"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "ultralytics/yolov5",
        "function_declaration": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):",
        "start_line": "523",
        "end_line": "573",
        "file_path": "export.py",
        "docstring": "The function exports a PyTorch model to CoreML format.\\nIt checks for the coremltools requirement, logs the start of the export, and sets the file suffix to .mlmodel.\\nIf specified, it wraps the model with an iOSModel for NMS support.\\nThe function traces the model, converts it to CoreML, and applies quantization if needed and supported.\\nFinally, it saves the CoreML model and returns the file path and the CoreML model.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "78335487ac1c",
        "ground_truth": "def export_coreml(model, im, file, int8, half, nms, prefix=colorstr(\"CoreML:\")):\n    \"\"\"\n    Export a YOLOv5 model to CoreML format with optional NMS, INT8, and FP16 support.\n     Args:\n        model (torch.nn.Module): The YOLOv5 model to be exported.\n        im (torch.Tensor): Example input tensor to trace the model.\n        file (pathlib.Path): Path object where the CoreML model will be saved.\n        int8 (bool): Flag indicating whether to use INT8 quantization (default is False).\n        half (bool): Flag indicating whether to use FP16 quantization (default is False).\n        nms (bool): Flag indicating whether to include Non-Maximum Suppression (default is False).\n        prefix (str): Prefix string for logging purposes (default is 'CoreML:').\n     Returns:\n        tuple[pathlib.Path | None, None]: The path to the saved CoreML model file, or (None, None) if there is an error.\n     Notes:\n        The exported CoreML model will be saved with a .mlmodel extension.\n        Quantization is supported only on macOS.\n     Example:\n        ```python\n        from pathlib import Path\n        import torch\n        from models.yolo import Model\n        model = Model(cfg, ch=3, nc=80)\n        im = torch.randn(1, 3, 640, 640)\n        file = Path(\"yolov5s_coreml\")\n        export_coreml(model, im, file, int8=False, half=False, nms=True)\n        ```\n    \"\"\"\n    check_requirements(\"coremltools\")\n    import coremltools as ct\n     LOGGER.info(f\"\\n{prefix} starting export with coremltools {ct.__version__}...\")\n    f = file.with_suffix(\".mlmodel\")\n     if nms:\n        model = iOSModel(model, im)\n    ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n    ct_model = ct.convert(ts, inputs=[ct.ImageType(\"image\", shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])\n    bits, mode = (8, \"kmeans_lut\") if int8 else (16, \"linear\") if half else (32, None)\n    if bits < 32:\n        if MACOS:  # quantization only supported on macOS\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n                ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n        else:\n            print(f\"{prefix} quantization only supported on macOS, skipping...\")\n    ct_model.save(f)\n    return f, ct_model",
        "import_statements": [
            "import argparse",
            "import contextlib",
            "import json",
            "import os",
            "import platform",
            "import re",
            "import subprocess",
            "import sys",
            "import time",
            "import warnings",
            "from pathlib import Path",
            "import torch",
            "from torch.utils.mobile_optimizer import optimize_for_mobile",
            "from models.experimental import attempt_load",
            "from models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel",
            "from utils.dataloaders import LoadImages",
            "from utils.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,\n)",
            "from utils.torch_utils import select_device, smart_inference_mode"
        ],
        "reference_api": [
            "quantize_weights",
            "print",
            "ct.ImageType",
            "file.with_suffix",
            "ct.convert",
            "warnings.filterwarnings",
            "LOGGER.info",
            "warnings.catch_warnings",
            "trace",
            "iOSModel",
            "colorstr",
            "ct_model.save",
            "check_requirements"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "colorstr",
                "code": "SI escape codes, e.g., colorstr('blue', 'hello world').\n\n    See https://en.wikipedia.org/wiki/ANSI_escape_code.\n    \"\"\"\n    *args, string = input if len(input) > 1 else (\"blue\", \"bold\", input[0])  # color arguments, string\n    colors = {\n        \"black\": \"\\033[30m\",  # basic colors\n        \"red\": \"\\033[31m\",\n        \"green\": \"\\033[32m\",\n        \"yellow\": \"\\033[33m\",\n        \"blue\": \"\\033[34m\",\n        \"magenta\": \"\\033[35m\",\n        \"cyan\": \"\\033[36m\",\n        \"white\": \"\\033[37m\",\n        \"bright_black\": \"\\033[90m\",  # bright colors\n        \"bright_red\": \"\\033[91m\",\n        \"bright_green\": \"\\033[92m\",\n        \"bright_yellow\": \"\\033[93m\",\n        \"bright_blue\": \"\\033[94m\",\n        \"bright_magenta\": \"\\033[95m\",\n        \"bright_cyan\": \"\\033[96m\",\n        \"bright_white\": \"\\033[97m\",\n        \"end\": \"\\033[0m\",  # misc\n        \"bold\": \"\\033[1m\",\n        \"underline\": \"\\033[4m\",\n    }\n    return \"\".join(colors[x] for x in args) + f\"{string}\" + colors[\"end\"]\n\n\ndef labels_to_class_weights(labels, nc=80):\n    \"\"\"Calc"
            },
            {
                "name": "LOGGER.info",
                "code": " info(self, verbose=False, img_size=640):\n        \"\"\"Prints model information given verbosity and image size, e.g., `info(verbose=True, img_size=640)`.\"\"\"\n        model_info(self, verbose, img_size)\n\n "
            }
        ],
        "third_party": [
            "check_requirements",
            "file.with_suffix",
            "iOSModel",
            "ct.convert",
            "ct.ImageType",
            "quantize_weights",
            "ct_model.save"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "ultralytics/yolov5",
        "function_declaration": "def generate_individual(input_ranges, individual_length)",
        "start_line": "889",
        "end_line": "917",
        "file_path": "train.py",
        "docstring": "The function generates an individual with specified length using input ranges.\\nIt iterates over the given length, and for each position, it appends a random float within the specified bounds to the individual list.\\nFinally, it returns the generated individual.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "80233cf71580",
        "ground_truth": "def generate_individual(input_ranges, individual_length):\n    \"\"\"\n    Generate an individual with random hyperparameters within specified ranges.\n     Args:\n        input_ranges (list[tuple[float, float]]): List of tuples where each tuple contains the lower and upper bounds\n            for the corresponding gene (hyperparameter).\n        individual_length (int): The number of genes (hyperparameters) in the individual.\n     Returns:\n        list[float]: A list representing a generated individual with random gene values within the specified ranges.\n     Example:\n        ```python\n        input_ranges = [(0.01, 0.1), (0.1, 1.0), (0.9, 2.0)]\n        individual_length = 3\n        individual = generate_individual(input_ranges, individual_length)\n        print(individual)  # Output: [0.035, 0.678, 1.456] (example output)\n        ```\n     Note:\n        The individual returned will have a length equal to `individual_length`, with each gene value being a floating-point\n        number within its specified range in `input_ranges`.\n    \"\"\"\n    individual = []\n    for i in range(individual_length):\n        lower_bound, upper_bound = input_ranges[i]\n        individual.append(random.uniform(lower_bound, upper_bound))\n    return individual",
        "import_statements": [
            "import argparse",
            "import math",
            "import os",
            "import random",
            "import subprocess",
            "import sys",
            "import time",
            "from copy import deepcopy",
            "from datetime import datetime, timedelta",
            "from pathlib import Path",
            "import torch",
            "import yaml",
            "from torch.optim import lr_scheduler",
            "from tqdm import tqdm",
            "from models.experimental import attempt_load",
            "from models.yolo import Model",
            "from utils.autoanchor import check_anchors",
            "from utils.autobatch import check_train_batch_size",
            "from utils.callbacks import Callbacks",
            "from utils.dataloaders import create_dataloader",
            "from utils.downloads import attempt_download, is_url",
            "from utils.general import (\n    LOGGER,\n    TQDM_BAR_FORMAT,\n    check_amp,\n    check_dataset,\n    check_file,\n    check_git_info,\n    check_git_status,\n    check_img_size,\n    check_requirements,\n    check_suffix,\n    check_yaml,\n    colorstr,\n    get_latest_run,\n    increment_path,\n    init_seeds,\n    intersect_dicts,\n    labels_to_class_weights,\n    labels_to_image_weights,\n    methods,\n    one_cycle,\n    print_args,\n    print_mutation,\n    strip_optimizer,\n    yaml_save,\n)",
            "from utils.loggers import LOGGERS, Loggers",
            "from utils.loggers.comet.comet_utils import check_comet_resume",
            "from utils.loss import ComputeLoss",
            "from utils.metrics import fitness",
            "from utils.plots import plot_evolve",
            "from utils.torch_utils import (\n    EarlyStopping,\n    ModelEMA,\n    de_parallel,\n    select_device,\n    smart_DDP,\n    smart_optimizer,\n    smart_resume,\n    torch_distributed_zero_first,\n)"
        ],
        "reference_api": [
            "random.uniform",
            "individual.append",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "individual.append"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def marginal_log_mean_coeff(self, t)",
        "start_line": "129",
        "end_line": "140",
        "file_path": "comfy/extra_samplers/uni_pc.py",
        "docstring": "The function computes the log of the mean coefficient alpha_t for a given continuous-time label t.\\nDepending on the schedule type ('discrete', 'linear', or 'cosine'), it performs different calculations:\\n'interpolate for discrete schedule, use a quadratic expression for linear schedule, and apply a cosine-based formula for the cosine schedule.\\nIt returns the computed log(alpha_t).",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5ea979cb4a8e",
        "ground_truth": "def marginal_log_mean_coeff(self, t):\n    \"\"\"\n    Compute log(alpha_t) of a given continuous-time label t in [0, T].\n    \"\"\"\n    if self.schedule == 'discrete':\n        return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n    elif self.schedule == 'linear':\n        return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n    elif self.schedule == 'cosine':\n        log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n        log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n        return log_alpha_t",
        "import_statements": [
            "import torch",
            "import math",
            "from tqdm.auto import trange, tqdm"
        ],
        "reference_api": [
            "log_alpha_fn",
            "torch.log",
            "to",
            "torch.cos",
            "t.reshape",
            "reshape",
            "interpolate_fn"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "interpolate_fn",
                "code": "def interpolate_fn(x, xp, yp):\n    \"\"\"\n    A piecewise linear function y = f(x), using xp and yp as keypoints.\n    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n\n    Args:\n        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n        yp: PyTorch tensor with shape [C, K].\n    Returns:\n        The function values f(x), with shape [N, C].\n    \"\"\"\n    N, K = x.shape[0], xp.shape[1]\n    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)\n    sorted_all_x, x_indices = torch.sort(all_x, dim=2)\n    x_idx = torch.argmin(x_indices, dim=2)\n    cand_start_idx = x_idx - 1\n    start_idx = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(1, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)\n    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)\n    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)\n    start_idx2 = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(0, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)\n    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)\n    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)\n    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)\n    return cand"
            }
        ],
        "third_party": [
            "reshape",
            "t.reshape",
            "to",
            "to",
            "torch.log",
            "torch.cos",
            "log_alpha_fn"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def inverse_lambda(self, lamb)",
        "start_line": "162",
        "end_line": "178",
        "file_path": "comfy/extra_samplers/uni_pc.py",
        "docstring": "The function computes the continuous-time label t in [0, T] from a given half-logSNR lambda_t.\\nFor a linear schedule, it calculates t using beta values and a temporary variable.\\nFor a discrete schedule, it interpolates log_alpha to find t.\\nFor other schedules, it computes log_alpha and uses an arccos function to derive t.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7da4fae5a278",
        "ground_truth": "def inverse_lambda(self, lamb):\n    \"\"\"\n    Compute the continuous-time label t in [0, T] of a given half-logSNR lambda_t.\n    \"\"\"\n    if self.schedule == 'linear':\n        tmp = 2. * (self.beta_1 - self.beta_0) * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n        Delta = self.beta_0**2 + tmp\n        return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n    elif self.schedule == 'discrete':\n        log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n        t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n        return t.reshape((-1,))\n    else:\n        log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n        t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n        t = t_fn(log_alpha)\n        return t",
        "import_statements": [
            "import torch",
            "import math",
            "from tqdm.auto import trange, tqdm"
        ],
        "reference_api": [
            "torch.zeros",
            "t_fn",
            "to",
            "torch.logaddexp",
            "t.reshape",
            "log_alpha.reshape",
            "torch.arccos",
            "torch.exp",
            "interpolate_fn",
            "torch.sqrt",
            "torch.flip"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "interpolate_fn",
                "code": "def interpolate_fn(x, xp, yp):\n    \"\"\"\n    A piecewise linear function y = f(x), using xp and yp as keypoints.\n    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n\n    Args:\n        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n        yp: PyTorch tensor with shape [C, K].\n    Returns:\n        The function values f(x), with shape [N, C].\n    \"\"\"\n    N, K = x.shape[0], xp.shape[1]\n    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)\n    sorted_all_x, x_indices = torch.sort(all_x, dim=2)\n    x_idx = torch.argmin(x_indices, dim=2)\n    cand_start_idx = x_idx - 1\n    start_idx = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(1, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)\n    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)\n    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)\n    start_idx2 = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(0, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)\n    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)\n    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)\n    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)\n    return cand"
            }
        ],
        "third_party": [
            "torch.logaddexp",
            "to",
            "torch.zeros",
            "torch.sqrt",
            "torch.logaddexp",
            "to",
            "torch.zeros",
            "log_alpha.reshape",
            "torch.flip",
            "to",
            "torch.flip",
            "to",
            "t.reshape",
            "torch.logaddexp",
            "to",
            "torch.zeros",
            "torch.arccos",
            "torch.exp",
            "t_fn"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def noise_pred_fn(x, t_continuous, cond=None)",
        "start_line": "293",
        "end_line": "311",
        "file_path": "comfy/extra_samplers/uni_pc.py",
        "docstring": "The function predicts noise for a given input based on the model type.\\nIt ensures the time input is expanded to match the batch size, then processes the input through the model.\\nDepending on the model type, it applies different transformations to the model output using noise schedule parameters and returns the processed result.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a519c853a2b2",
        "ground_truth": "def noise_pred_fn(x, t_continuous, cond=None):\n    if t_continuous.reshape((-1,)).shape[0] == 1:\n        t_continuous = t_continuous.expand((x.shape[0]))\n    t_input = get_model_input_time(t_continuous)\n    output = model(x, t_input, **model_kwargs)\n    if model_type == \"noise\":\n        return output\n    elif model_type == \"x_start\":\n        alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n        dims = x.dim()\n        return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n    elif model_type == \"v\":\n        alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n        dims = x.dim()\n        return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n    elif model_type == \"score\":\n        sigma_t = noise_schedule.marginal_std(t_continuous)\n        dims = x.dim()\n        return -expand_dims(sigma_t, dims) * output",
        "import_statements": [
            "import torch",
            "import math",
            "from tqdm.auto import trange, tqdm"
        ],
        "reference_api": [
            "noise_schedule.marginal_alpha",
            "get_model_input_time",
            "expand_dims",
            "model",
            "t_continuous.expand",
            "noise_schedule.marginal_std",
            "t_continuous.reshape",
            "x.dim"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "noise_schedule.marginal_alpha",
                "code": "def marginal_alpha(self, t):\n        \"\"\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.exp(self.marginal_log_mean_coeff(t))"
            },
            {
                "name": "noise_schedule.marginal_std",
                "code": "def marginal_std(self, t):\n        \"\"\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))"
            },
            {
                "name": "expand_dims",
                "code": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]"
            },
            {
                "name": "expand_dims",
                "code": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]"
            },
            {
                "name": "noise_schedule.marginal_alpha",
                "code": "def marginal_alpha(self, t):\n        \"\"\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.exp(self.marginal_log_mean_coeff(t))"
            },
            {
                "name": "noise_schedule.marginal_std",
                "code": "def marginal_std(self, t):\n        \"\"\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))"
            },
            {
                "name": "expand_dims",
                "code": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]"
            },
            {
                "name": "expand_dims",
                "code": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]"
            },
            {
                "name": "noise_schedule.marginal_std",
                "code": "def marginal_std(self, t):\n        \"\"\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))"
            },
            {
                "name": "expand_dims",
                "code": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]"
            }
        ],
        "third_party": [
            "t_continuous.reshape",
            "t_continuous.expand",
            "get_model_input_time",
            "model",
            "x.dim",
            "x.dim",
            "x.dim"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def dynamic_thresholding_fn(self, x0, t=None)",
        "start_line": "373",
        "end_line": "382",
        "file_path": "comfy/extra_samplers/uni_pc.py",
        "docstring": "The function applies dynamic thresholding to the input tensor x0.\\nIt reshapes x0 to compute the quantile of absolute values, determines the scaling factor, and clamps x0 values based on this factor.\\nThe resulting tensor is then scaled and returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "96a9b7d54823",
        "ground_truth": "def dynamic_thresholding_fn(self, x0, t=None):\n    \"\"\"\n    The dynamic thresholding method. \n    \"\"\"\n    dims = x0.dim()\n    p = self.dynamic_thresholding_ratio\n    s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n    s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n    x0 = torch.clamp(x0, -s, s) / s\n    return x0",
        "import_statements": [
            "import torch",
            "import math",
            "from tqdm.auto import trange, tqdm"
        ],
        "reference_api": [
            "torch.quantile",
            "torch.abs",
            "expand_dims",
            "to",
            "torch.ones_like",
            "x0.dim",
            "reshape",
            "torch.maximum",
            "torch.clamp"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "expand_dims",
                "code": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]"
            }
        ],
        "third_party": [
            "x0.dim",
            "torch.quantile",
            "reshape",
            "torch.abs",
            "torch.maximum",
            "to",
            "torch.ones_like",
            "torch.clamp"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def data_prediction_fn(self, x, t)",
        "start_line": "390",
        "end_line": "403",
        "file_path": "comfy/extra_samplers/uni_pc.py",
        "docstring": "The function applies dynamic thresholding to the input tensor x0.\\nIt calculates the threshold value based on the dynamic thresholding ratio and maximum threshold value, then clamps the input tensor values within this threshold.\\nFinally, it normalizes the clamped values and returns the processed tensor.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "59c6ec6d7a6a",
        "ground_truth": "def data_prediction_fn(self, x, t):\n    \"\"\"\n    Return the data prediction model (with thresholding).\n    \"\"\"\n    noise = self.noise_prediction_fn(x, t)\n    dims = x.dim()\n    alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\n    x0 = (x - expand_dims(sigma_t, dims) * noise) / expand_dims(alpha_t, dims)\n    if self.thresholding:\n        p = 0.995   # A hyperparameter in the paper of \"Imagen\" [1].\n        s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n        s = expand_dims(torch.maximum(s, self.max_val * torch.ones_like(s).to(s.device)), dims)\n        x0 = torch.clamp(x0, -s, s) / s\n    return x0",
        "import_statements": [
            "import torch",
            "import math",
            "from tqdm.auto import trange, tqdm"
        ],
        "reference_api": [
            "torch.clamp",
            "torch.quantile",
            "torch.abs",
            "expand_dims",
            "to",
            "self.noise_prediction_fn",
            "torch.ones_like",
            "reshape",
            "torch.maximum",
            "marginal_std",
            "marginal_alpha",
            "x.dim"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.noise_prediction_fn",
                "code": "def noise_prediction_fn(self, x, t):\n        \"\"\"\n        Return the noise prediction model.\n        \"\"\"\n        return self.model(x, t)"
            },
            {
                "name": "marginal_alpha",
                "code": "def marginal_alpha(self, t):\n        \"\"\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.exp(self.marginal_log_mean_coeff(t))"
            },
            {
                "name": "marginal_std",
                "code": "def marginal_std(self, t):\n        \"\"\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))"
            },
            {
                "name": "expand_dims",
                "code": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]"
            },
            {
                "name": "expand_dims",
                "code": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]"
            },
            {
                "name": "expand_dims",
                "code": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]"
            }
        ],
        "third_party": [
            "x.dim",
            "torch.quantile",
            "reshape",
            "torch.abs",
            "torch.maximum",
            "to",
            "torch.ones_like",
            "torch.clamp"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def predict_eps_sigma(model, input, sigma_in, **kwargs)",
        "start_line": "842",
        "end_line": "845",
        "file_path": "comfy/extra_samplers/uni_pc.py",
        "docstring": "The function predicts epsilon given a model, input, and sigma.\\nIt reshapes sigma to match the dimensions of the input, scales the input, and then computes the prediction by adjusting the model output with the scaled input and sigma.\\nFinally, it returns the adjusted prediction.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "65e3618468a2",
        "ground_truth": "def predict_eps_sigma(model, input, sigma_in, **kwargs):\n    sigma = sigma_in.view(sigma_in.shape[:1] + (1,) * (input.ndim - 1))\n    input = input * ((sigma ** 2 + 1.0) ** 0.5)\n    return  (input - model(input, sigma_in, **kwargs)) / sigma",
        "import_statements": [
            "import torch",
            "import math",
            "from tqdm.auto import trange, tqdm"
        ],
        "reference_api": [
            "sigma_in.view",
            "model"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "sigma_in.view",
            "model"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def cal_intergrand(beta_0, beta_1, taus)",
        "start_line": "39",
        "end_line": "51",
        "file_path": "comfy/k_diffusion/deis.py",
        "docstring": "The function calculates the integrand for given parameters beta_0, beta_1, and taus.\\nIt operates within a gradient-enabled context, cloning and setting the required gradients for taus.\\nIt computes alpha and its logarithm, performs backpropagation to obtain the gradient of log_alpha with respect to taus, and finally calculates the integrand using this gradient and alpha values.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b35846dfbc66",
        "ground_truth": "def cal_intergrand(beta_0, beta_1, taus):\n    with torch.inference_mode(mode=False):\n        taus = taus.clone()\n        beta_0 = beta_0.clone()\n        beta_1 = beta_1.clone()\n        with torch.enable_grad():\n            taus.requires_grad_(True)\n            alpha = t2alpha_fn(beta_0, beta_1, taus)\n            log_alpha = alpha.log()\n            log_alpha.sum().backward()\n            d_log_alpha_dtau = taus.grad\n    integrand = -0.5 * d_log_alpha_dtau / torch.sqrt(alpha * (1 - alpha))\n    return integrand",
        "import_statements": [
            "import torch"
        ],
        "reference_api": [
            "beta_0.clone",
            "alpha.log",
            "backward",
            "t2alpha_fn",
            "taus.clone",
            "torch.sqrt",
            "log_alpha.sum",
            "torch.inference_mode",
            "taus.requires_grad_",
            "torch.enable_grad",
            "beta_1.clone"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "t2alpha_fn",
                "code": "def t2alpha_fn(beta_0, beta_1, t):\n    return torch.exp(-0.5 * t ** 2 * (beta_1 - beta_0) - t * beta_0)"
            }
        ],
        "third_party": [
            "torch.inference_mode",
            "taus.clone",
            "beta_0.clone",
            "beta_1.clone",
            "torch.enable_grad",
            "taus.requires_grad_",
            "alpha.log",
            "backward",
            "log_alpha.sum",
            "torch.sqrt"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None)",
        "start_line": "154",
        "end_line": "170",
        "file_path": "comfy/k_diffusion/sampling.py",
        "docstring": "The function performs ancestral sampling using the Euler method.\\nIt initializes extra arguments and a noise sampler if not provided, and iterates over the sigmas.\\nIn each step, it denoises the input, calculates the ancestral step, and updates the input using the Euler method.\\nOptionally, it calls a callback function with the current state.\\nFinally, it adds noise if required and returns the sampled output.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5f3667566a10",
        "ground_truth": "def sample_euler_ancestral(model, x, sigmas, extra_args=None, callback=None, disable=None, eta=1., s_noise=1., noise_sampler=None):\n    \"\"\"Ancestral sampling with Euler method steps.\"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1], eta=eta)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n        d = to_d(x, sigmas[i], denoised)\n        # Euler method\n        dt = sigma_down - sigmas[i]\n        x = x + d * dt\n        if sigmas[i + 1] > 0:\n            x = x + noise_sampler(sigmas[i], sigmas[i + 1]) * s_noise * sigma_up\n    return x",
        "import_statements": [
            "import math",
            "from scipy import integrate",
            "import torch",
            "from torch import nn",
            "import torchsde",
            "from tqdm.auto import trange, tqdm",
            "import comfy.model_patcher"
        ],
        "reference_api": [
            "noise_sampler",
            "get_ancestral_step",
            "len",
            "model",
            "callback",
            "default_noise_sampler",
            "trange",
            "x.new_ones",
            "to_d"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "default_noise_sampler",
                "code": "def default_noise_sampler(x):\n    return lambda sigma, sigma_next: torch.randn_like(x)"
            },
            {
                "name": "get_ancestral_step",
                "code": "def get_ancestral_step(sigma_from, sigma_to, eta=1.):\n    \"\"\"Calculates the noise level (sigma_down) to step down to and the amount\n    of noise to add (sigma_up) when doing an ancestral sampling step.\"\"\"\n    if not eta:\n        return sigma_to, 0.\n    sigma_up = min(sigma_to, eta * (sigma_to ** 2 * (sigma_from ** 2 - sigma_to ** 2) / sigma_from ** 2) ** 0.5)\n    sigma_down = (sigma_to ** 2 - sigma_up ** 2) ** 0.5\n    return sigma_down, sigma_up"
            },
            {
                "name": "to_d",
                "code": "def to_d(x, sigma, denoised):\n    \"\"\"Converts a denoiser output to a Karras ODE derivative.\"\"\"\n    return (x - denoised) / utils.append_dims(sigma, x.ndim)"
            }
        ],
        "third_party": [
            "x.new_ones",
            "trange",
            "model",
            "callback",
            "noise_sampler"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def linear_multistep_coeff(order, t, i, j)",
        "start_line": "273",
        "end_line": "283",
        "file_path": "comfy/k_diffusion/sampling.py",
        "docstring": "The function calculates the linear multistep coefficient for a given order, time steps, and indices i and j.\\nIt raises an error if the order is too high for the step index.\\nA nested function computes the product term for the coefficient, excluding the j-th term.\\nThe function then integrates this nested function over the interval between the i-th and (i+1)-th time steps and returns the result.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6261ded88df2",
        "ground_truth": "def linear_multistep_coeff(order, t, i, j):\n    if order - 1 > i:\n        raise ValueError(f'Order {order} too high for step {i}')\n    def fn(tau):\n        prod = 1.\n        for k in range(order):\n            if j == k:\n                continue\n            prod *= (tau - t[i - k]) / (t[i - j] - t[i - k])\n        return prod\n    return integrate.quad(fn, t[i], t[i + 1], epsrel=1e-4)[0]",
        "import_statements": [
            "import math",
            "from scipy import integrate",
            "import torch",
            "from torch import nn",
            "import torchsde",
            "from tqdm.auto import trange, tqdm",
            "import comfy.model_patcher"
        ],
        "reference_api": [
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None)",
        "start_line": "774",
        "end_line": "786",
        "file_path": "comfy/k_diffusion/sampling.py",
        "docstring": "The function performs latent code manipulation (LCM) sampling using a model and a set of sigmas.\\nIt initializes extra arguments and a noise sampler if not provided, then iterates over the sigmas, applying the model to the input and updating it with denoised values.\\nA callback function can be executed at each step to monitor progress.\\nIf the next sigma is greater than zero, noise scaling is applied to the updated input.\\nThe function returns the final sampled value.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4ee025e33ab1",
        "ground_truth": "def sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n         x = denoised\n        if sigmas[i + 1] > 0:\n            x = model.inner_model.inner_model.model_sampling.noise_scaling(sigmas[i + 1], noise_sampler(sigmas[i], sigmas[i + 1]), x)\n    return x",
        "import_statements": [
            "import math",
            "from scipy import integrate",
            "import torch",
            "from torch import nn",
            "import torchsde",
            "from tqdm.auto import trange, tqdm",
            "import comfy.model_patcher"
        ],
        "reference_api": [
            "noise_sampler",
            "len",
            "model",
            "callback",
            "default_noise_sampler",
            "trange",
            "noise_scaling",
            "x.new_ones"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "default_noise_sampler",
                "code": "def default_noise_sampler(x):\n    return lambda sigma, sigma_next: torch.randn_like(x)"
            }
        ],
        "third_party": [
            "x.new_ones",
            "trange",
            "model",
            "callback",
            "noise_scaling",
            "noise_sampler"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def append_dims(x, target_dims)",
        "start_line": "21",
        "end_line": "29",
        "file_path": "comfy/k_diffusion/utils.py",
        "docstring": "The function appends dimensions to a tensor until it reaches the specified target dimensions.\\nIt calculates the number of dimensions to append and raises an error if the target dimensions are fewer than the current dimensions.\\nIt expands the tensor by adding new dimensions at the end and returns a detached clone if the tensor is on an 'mps' device, otherwise it returns the expanded tensor.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cb51f91da606",
        "ground_truth": "def append_dims(x, target_dims):\n    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\"\"\"\n    dims_to_append = target_dims - x.ndim\n    if dims_to_append < 0:\n        raise ValueError(f'input has {x.ndim} dims but target_dims is {target_dims}, which is less')\n    expanded = x[(...,) + (None,) * dims_to_append]\n    # MPS will get inf values if it tries to index into the new axes, but detaching fixes this.\n    # https://github.com/pytorch/pytorch/issues/84364\n    return expanded.detach().clone() if expanded.device.type == 'mps' else expanded",
        "import_statements": [
            "from contextlib import contextmanager",
            "import hashlib",
            "import math",
            "from pathlib import Path",
            "import shutil",
            "import urllib",
            "import warnings",
            "from PIL import Image",
            "import torch",
            "from torch import nn, optim",
            "from torch.utils import data"
        ],
        "reference_api": [
            "ValueError",
            "expanded.detach",
            "clone"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "clone",
            "expanded.detach"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def download_file(path, url, digest=None)",
        "start_line": "37",
        "end_line": "48",
        "file_path": "comfy/k_diffusion/utils.py",
        "docstring": "The function downloads a file from a given URL to a specified path if it does not already exist, optionally verifying its SHA-256 hash.\\nIt ensures the target directory exists, downloads the file if necessary, and checks the file's hash against the provided digest, raising an error if they do not match.\\nThe function returns the path to the downloaded file.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9fac874a3bc7",
        "ground_truth": "def download_file(path, url, digest=None):\n    \"\"\"Downloads a file if it does not exist, optionally checking its SHA-256 hash.\"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if not path.exists():\n        with urllib.request.urlopen(url) as response, open(path, 'wb') as f:\n            shutil.copyfileobj(response, f)\n    if digest is not None:\n        file_digest = hashlib.sha256(open(path, 'rb').read()).hexdigest()\n        if digest != file_digest:\n            raise OSError(f'hash of {path} (url: {url}) failed to validate')\n    return path",
        "import_statements": [
            "from contextlib import contextmanager",
            "import hashlib",
            "import math",
            "from pathlib import Path",
            "import shutil",
            "import urllib",
            "import warnings",
            "from PIL import Image",
            "import torch",
            "from torch import nn, optim",
            "from torch.utils import data"
        ],
        "reference_api": [
            "mkdir",
            "hexdigest",
            "urlopen",
            "path.exists",
            "read",
            "shutil.copyfileobj",
            "Path",
            "hashlib.sha256",
            "OSError",
            "open"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "Path",
            "mkdir",
            "path.exists",
            "urlopen",
            "hexdigest",
            "read"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def train_mode(model, mode=True)",
        "start_line": "52",
        "end_line": "60",
        "file_path": "comfy/k_diffusion/utils.py",
        "docstring": "The function is a context manager that sets a model to training mode and restores its previous mode upon exit.\\nIt saves the current training modes of all modules, switches the model to the specified mode, and ensures the original modes are restored after the context is exited.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f23165e897ce",
        "ground_truth": "def train_mode(model, mode=True):\n    \"\"\"A context manager that places a model into training mode and restores\n    the previous mode on exit.\"\"\"\n    modes = [module.training for module in model.modules()]\n    try:\n        yield model.train(mode)\n    finally:\n        for i, module in enumerate(model.modules()):\n            module.training = modes[i]",
        "import_statements": [
            "from contextlib import contextmanager",
            "import hashlib",
            "import math",
            "from pathlib import Path",
            "import shutil",
            "import urllib",
            "import warnings",
            "from PIL import Image",
            "import torch",
            "from torch import nn, optim",
            "from torch.utils import data"
        ],
        "reference_api": [
            "model.modules",
            "model.train",
            "enumerate"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "model.modules",
            "model.train",
            "model.modules"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def ema_update(model, averaged_model, decay)",
        "start_line": "70",
        "end_line": "85",
        "file_path": "comfy/k_diffusion/utils.py",
        "docstring": "The function updates an exponentially moving averaged (EMA) model with the current model's parameters.\\nIt ensures both models have matching parameters and buffers.\\nFor each parameter, it updates the averaged model using a weighted average based on the decay factor.\\nIt also copies the current model's buffers to the averaged model.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c6d59c6cb8e1",
        "ground_truth": "def ema_update(model, averaged_model, decay):\n    \"\"\"Incorporates updated model parameters into an exponential moving averaged\n    version of a model. It should be called after each optimizer step.\"\"\"\n    model_params = dict(model.named_parameters())\n    averaged_params = dict(averaged_model.named_parameters())\n    assert model_params.keys() == averaged_params.keys()\n     for name, param in model_params.items():\n        averaged_params[name].mul_(decay).add_(param, alpha=1 - decay)\n     model_buffers = dict(model.named_buffers())\n    averaged_buffers = dict(averaged_model.named_buffers())\n    assert model_buffers.keys() == averaged_buffers.keys()\n     for name, buf in model_buffers.items():\n        averaged_buffers[name].copy_(buf)",
        "import_statements": [
            "from contextlib import contextmanager",
            "import hashlib",
            "import math",
            "from pathlib import Path",
            "import shutil",
            "import urllib",
            "import warnings",
            "from PIL import Image",
            "import torch",
            "from torch import nn, optim",
            "from torch.utils import data"
        ],
        "reference_api": [
            "model.named_buffers",
            "mul_",
            "averaged_model.named_parameters",
            "model.named_parameters",
            "model_buffers.items",
            "copy_",
            "model_params.items",
            "dict",
            "model_params.keys",
            "add_",
            "averaged_params.keys",
            "averaged_model.named_buffers",
            "model_buffers.keys",
            "averaged_buffers.keys"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "model.named_parameters",
            "averaged_model.named_parameters",
            "model_params.keys",
            "averaged_params.keys",
            "model_params.items",
            "add_",
            "mul_",
            "model.named_buffers",
            "averaged_model.named_buffers",
            "model_buffers.keys",
            "averaged_buffers.keys",
            "model_buffers.items",
            "copy_"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32)",
        "start_line": "223",
        "end_line": "230",
        "file_path": "comfy/k_diffusion/utils.py",
        "docstring": "The function generates samples from a log-logistic distribution with optional truncation.\\nIt converts the minimum and maximum values to tensors, computes their CDF values, and generates uniform random values within this range.\\nThese values are transformed using the logit function, scaled, and exponentiated to obtain the final samples in the specified dtype.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "600c39a30798",
        "ground_truth": "def rand_log_logistic(shape, loc=0., scale=1., min_value=0., max_value=float('inf'), device='cpu', dtype=torch.float32):\n    \"\"\"Draws samples from an optionally truncated log-logistic distribution.\"\"\"\n    min_value = torch.as_tensor(min_value, device=device, dtype=torch.float64)\n    max_value = torch.as_tensor(max_value, device=device, dtype=torch.float64)\n    min_cdf = min_value.log().sub(loc).div(scale).sigmoid()\n    max_cdf = max_value.log().sub(loc).div(scale).sigmoid()\n    u = torch.rand(shape, device=device, dtype=torch.float64) * (max_cdf - min_cdf) + min_cdf\n    return u.logit().mul(scale).add(loc).exp().to(dtype)",
        "import_statements": [
            "from contextlib import contextmanager",
            "import hashlib",
            "import math",
            "from pathlib import Path",
            "import shutil",
            "import urllib",
            "import warnings",
            "from PIL import Image",
            "import torch",
            "from torch import nn, optim",
            "from torch.utils import data"
        ],
        "reference_api": [
            "torch.rand",
            "div",
            "float",
            "exp",
            "sigmoid",
            "torch.as_tensor",
            "to",
            "mul",
            "sub",
            "add",
            "min_value.log",
            "u.logit",
            "max_value.log"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "torch.as_tensor",
            "torch.as_tensor",
            "sigmoid",
            "div",
            "sub",
            "min_value.log",
            "sigmoid",
            "div",
            "sub",
            "max_value.log",
            "torch.rand",
            "to",
            "exp",
            "add",
            "mul",
            "u.logit"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module",
        "start_line": "89",
        "end_line": "102",
        "file_path": "comfy/ldm/audio/autoencoder.py",
        "docstring": "The function returns a specified activation module based on the input string, which can be \"elu\", \"snake\", or \"none\".\\nIf the activation type is \"elu\", it returns an ELU activation.\\nIf the type is \"snake\", it returns a SnakeBeta activation with the specified channels.\\nIf the type is \"none\", it returns an Identity module.\\nIf the antialias flag is True, it wraps the activation in an Activation1d module before returning.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "75e2def2467f",
        "ground_truth": "def get_activation(activation: Literal[\"elu\", \"snake\", \"none\"], antialias=False, channels=None) -> nn.Module:\n    if activation == \"elu\":\n        act = torch.nn.ELU()\n    elif activation == \"snake\":\n        act = SnakeBeta(channels)\n    elif activation == \"none\":\n        act = torch.nn.Identity()\n    else:\n        raise ValueError(f\"Unknown activation {activation}\")\n     if antialias:\n        act = Activation1d(act)\n     return act",
        "import_statements": [
            "import torch",
            "from torch import nn",
            "from typing import Literal, Dict, Any",
            "import math",
            "import comfy.ops"
        ],
        "reference_api": [
            "Activation1d",
            "ValueError",
            "ELU",
            "SnakeBeta",
            "Identity"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ELU",
            "SnakeBeta",
            "Identity",
            "Activation1d"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def apply_rotary_pos_emb(t, freqs, scale = 1)",
        "start_line": "181",
        "end_line": "199",
        "file_path": "comfy/ldm/audio/dit.py",
        "docstring": "The function applies rotary position embeddings to a tensor using given frequency values.\\nIt ensures numerical stability by casting to float32 if necessary, adjusts the shapes of the input tensor and frequency tensor, and applies the rotary embeddings to a subset of the tensor.\\nThe processed and unrotated parts of the tensor are concatenated and returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ef1714c79e51",
        "ground_truth": "def apply_rotary_pos_emb(t, freqs, scale = 1):\n    out_dtype = t.dtype\n     # cast to float32 if necessary for numerical stability\n    dtype = t.dtype #reduce(torch.promote_types, (t.dtype, freqs.dtype, torch.float32))\n    rot_dim, seq_len = freqs.shape[-1], t.shape[-2]\n    freqs, t = freqs.to(dtype), t.to(dtype)\n    freqs = freqs[-seq_len:, :]\n     if t.ndim == 4 and freqs.ndim == 3:\n        freqs = rearrange(freqs, 'b n d -> b 1 n d')\n     # partial rotary embeddings, Wang et al. GPT-J\n    t, t_unrotated = t[..., :rot_dim], t[..., rot_dim:]\n    t = (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n     t, t_unrotated = t.to(out_dtype), t_unrotated.to(out_dtype)\n     return torch.cat((t, t_unrotated), dim = -1)",
        "import_statements": [
            "from comfy.ldm.modules.attention import optimized_attention",
            "import torch",
            "from einops import rearrange",
            "from torch import nn",
            "from torch.nn import functional as F",
            "import math"
        ],
        "reference_api": [
            "rearrange",
            "rotate_half",
            "t.to",
            "torch.cat",
            "t_unrotated.to",
            "freqs.to",
            "freqs.sin",
            "freqs.cos"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "rotate_half",
                "code": "def rotate_half(x):\n    x = rearrange(x, '... (j d) -> ... j d', j = 2)\n    x1, x2 = x.unbind(dim = -2)\n    return torch.cat((-x2, x1), dim = -1)"
            }
        ],
        "third_party": [
            "freqs.to",
            "t.to",
            "rearrange",
            "freqs.cos",
            "freqs.sin",
            "t.to",
            "t_unrotated.to",
            "torch.cat"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def idx2vq(self, idx, dim=-1)",
        "start_line": "91",
        "end_line": "95",
        "file_path": "comfy/ldm/cascade/stage_a.py",
        "docstring": "The function maps indices to vector quantized (VQ) embeddings using a codebook.\\nIt retrieves the embeddings corresponding to the given indices and, if specified, moves the last dimension to the given dimension.\\nThe function then returns the resulting embeddings.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a7a5f178c6f6",
        "ground_truth": "def idx2vq(self, idx, dim=-1):\n    q_idx = self.codebook(idx)\n    if dim != -1:\n        q_idx = q_idx.movedim(-1, dim)\n    return q_idx",
        "import_statements": [
            "import torch",
            "from torch import nn",
            "from torch.autograd import Function"
        ],
        "reference_api": [
            "self.codebook",
            "q_idx.movedim"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self.codebook",
            "q_idx.movedim"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def encode(self, x, quantize=False)",
        "start_line": "210",
        "end_line": "217",
        "file_path": "comfy/ldm/cascade/stage_a.py",
        "docstring": "The function encodes the input x using a series of blocks.\\nIf quantization is enabled, it applies a vector quantizer to the encoded output and returns the quantized encoding, original encoding, indices, and combined loss.\\nIf quantization is not enabled, it returns the encoded output.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4104c041fb20",
        "ground_truth": "def encode(self, x, quantize=False):\n    x = self.in_block(x)\n    x = self.down_blocks(x)\n    if quantize:\n        qe, (vq_loss, commit_loss), indices = self.vquantizer.forward(x, dim=1)\n        return qe, x, indices, vq_loss + commit_loss * 0.25\n    else:\n        return x",
        "import_statements": [
            "import torch",
            "from torch import nn",
            "from torch.autograd import Function"
        ],
        "reference_api": [
            "forward",
            "self.down_blocks",
            "self.in_block"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "forward",
                "code": "def forward(self, x, get_losses=True, dim=-1):\n        if dim != -1:\n            x = x.movedim(dim, -1)\n        z_e_x = x.contiguous().view(-1, x.size(-1)) if len(x.shape) > 2 else x\n        z_q_x, indices = self.vq(z_e_x, self.codebook.weight.detach())\n        vq_loss, commit_loss = None, None\n        if self.ema_loss and self.training:\n            self._updateEMA(z_e_x.detach(), indices.detach())\n        # pick the graded embeddings after updating the codebook in order to have a more accurate commitment loss\n        z_q_x_grd = torch.index_select(self.codebook.weight, dim=0, index=indices)\n        if get_losses:\n            vq_loss = (z_q_x_grd - z_e_x.detach()).pow(2).mean()\n            commit_loss = (z_e_x - z_q_x_grd.detach()).pow(2).mean()\n\n        z_q_x = z_q_x.view(x.shape)\n        if dim != -1:\n            z_q_x = z_q_x.movedim(-1, dim)\n        return z_q_x, (vq_loss, commit_loss), indices.view(x.shape[:-1])"
            }
        ],
        "third_party": [
            "self.in_block",
            "self.down_blocks"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def ema_scope(self, context=None)",
        "start_line": "66",
        "end_line": "78",
        "file_path": "comfy/ldm/models/autoencoder.py",
        "docstring": "The function manages the application of Exponential Moving Average (EMA) weights in a context manager.\\nIf EMA is used, it stores the current parameters and replaces them with EMA weights, optionally logging the switch.\\nUpon exiting the context, it restores the original parameters and logs the restoration if a context is provided.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "403c52ba2d3b",
        "ground_truth": "def ema_scope(self, context=None):\n    if self.use_ema:\n        self.model_ema.store(self.parameters())\n        self.model_ema.copy_to(self)\n        if context is not None:\n            logpy.info(f\"{context}: Switched to EMA weights\")\n    try:\n        yield None\n    finally:\n        if self.use_ema:\n            self.model_ema.restore(self.parameters())\n            if context is not None:\n                logpy.info(f\"{context}: Restored training weights\")",
        "import_statements": [
            "import torch",
            "from contextlib import contextmanager",
            "from typing import Any, Dict, List, Optional, Tuple, Union",
            "from comfy.ldm.modules.distributions.distributions import DiagonalGaussianDistribution",
            "from comfy.ldm.util import instantiate_from_config",
            "from comfy.ldm.modules.ema import LitEma",
            "import comfy.ops"
        ],
        "reference_api": [
            "restore",
            "logpy.info",
            "store",
            "copy_to",
            "self.parameters"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "store",
                "code": "def store(self, parameters):\n        \"\"\"\n        Save the current parameters for restoring later.\n        Args:\n          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n            temporarily stored.\n        \"\"\"\n        self.collected_params = [param.clone() for param in parameters]"
            },
            {
                "name": "copy_to",
                "code": "def copy_to(self, model):\n        m_param = dict(model.named_parameters())\n        shadow_params = dict(self.named_buffers())\n        for key in m_param:\n            if m_param[key].requires_grad:\n                m_param[key].data.copy_(shadow_params[self.m_name2s_name[key]].data)\n            else:\n                assert not key in self.m_name2s_name"
            },
            {
                "name": "restore",
                "code": "def restore(self, parameters):\n        \"\"\"\n        Restore the parameters stored with the `store` method.\n        Useful to validate the model with EMA parameters without affecting the\n        original optimization process. Store the parameters before the\n        `copy_to` method. After validation (or model saving), use this to\n        restore the former parameters.\n        Args:\n          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n            updated with the stored parameters.\n        \"\"\"\n        for c_param, param in zip(self.collected_params, parameters):\n            param.data.copy_(c_param.data)"
            }
        ],
        "third_party": [
            "self.parameters",
            "logpy.info",
            "self.parameters",
            "logpy.info"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp)",
        "start_line": "565",
        "end_line": "571",
        "file_path": "comfy/ldm/modules/diffusionmodules/mmdit.py",
        "docstring": "The function applies post-attention processing to the input tensor x.\\nIt first adds the attention output, scaled by gate_msa, to x.\\nThen, it normalizes x, modulates it using shift_mlp and scale_mlp, and applies an MLP scaled by gate_mlp.\\nThe final result is returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "94b7467f98e4",
        "ground_truth": "def post_attention(self, attn, x, gate_msa, shift_mlp, scale_mlp, gate_mlp):\n    assert not self.pre_only\n    x = x + gate_msa.unsqueeze(1) * self.attn.post_attention(attn)\n    x = x + gate_mlp.unsqueeze(1) * self.mlp(\n        modulate(self.norm2(x), shift_mlp, scale_mlp)\n    )\n    return x",
        "import_statements": [
            "import logging",
            "import math",
            "from typing import Dict, Optional",
            "import torch",
            "from einops import rearrange, repeat"
        ],
        "reference_api": [
            "self.norm2",
            "gate_mlp.unsqueeze",
            "modulate",
            "post_attention",
            "self.mlp",
            "gate_msa.unsqueeze"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "post_attention",
                "code": "def post_attention(self, x: torch.Tensor) -> torch.Tensor:\n        assert not self.pre_only\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x"
            },
            {
                "name": "modulate",
                "code": "def modulate(x, shift, scale):\n    if shift is None:\n        shift = torch.zeros_like(scale)\n    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)"
            }
        ],
        "third_party": [
            "gate_msa.unsqueeze",
            "gate_mlp.unsqueeze",
            "self.mlp",
            "self.norm2"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def _block_mixing(context, x, context_block, x_block, c)",
        "start_line": "592",
        "end_line": "617",
        "file_path": "comfy/ldm/modules/diffusionmodules/mmdit.py",
        "docstring": "The function performs block mixing for attention mechanisms between context and input x.\\nIt processes the context and x through their respective pre-attention blocks to obtain query, key, and value (qkv) tensors and intermediates.\\nIt concatenates the qkv tensors, applies optimized attention, and splits the attention results back into context and x parts.\\nFinally, it processes these parts through their respective post-attention blocks, depending on the configuration of the context block, and returns the updated context and x.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d9fdc05542a2",
        "ground_truth": "def _block_mixing(context, x, context_block, x_block, c):\n    context_qkv, context_intermediates = context_block.pre_attention(context, c)\n     x_qkv, x_intermediates = x_block.pre_attention(x, c)\n     o = []\n    for t in range(3):\n        o.append(torch.cat((context_qkv[t], x_qkv[t]), dim=1))\n    qkv = tuple(o)\n     attn = optimized_attention(\n        qkv,\n        num_heads=x_block.attn.num_heads,\n    )\n    context_attn, x_attn = (\n        attn[:, : context_qkv[0].shape[1]],\n        attn[:, context_qkv[0].shape[1] :],\n    )\n     if not context_block.pre_only:\n        context = context_block.post_attention(context_attn, *context_intermediates)\n     else:\n        context = None\n    x = x_block.post_attention(x_attn, *x_intermediates)\n    return context, x",
        "import_statements": [
            "import logging",
            "import math",
            "from typing import Dict, Optional",
            "import torch",
            "from einops import rearrange, repeat"
        ],
        "reference_api": [
            "x_block.pre_attention",
            "tuple",
            "o.append",
            "context_block.pre_attention",
            "torch.cat",
            "optimized_attention",
            "x_block.post_attention",
            "context_block.post_attention",
            "range"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "context_block.pre_attention",
                "code": "def pre_attention(self, x: torch.Tensor) -> torch.Tensor:\n        B, L, C = x.shape\n        qkv = self.qkv(x)\n        q, k, v = split_qkv(qkv, self.head_dim)\n        q = self.ln_q(q).reshape(q.shape[0], q.shape[1], -1)\n        k = self.ln_k(k).reshape(q.shape[0], q.shape[1], -1)\n        return (q, k, v)"
            },
            {
                "name": "x_block.pre_attention",
                "code": "def pre_attention(self, x: torch.Tensor) -> torch.Tensor:\n        B, L, C = x.shape\n        qkv = self.qkv(x)\n        q, k, v = split_qkv(qkv, self.head_dim)\n        q = self.ln_q(q).reshape(q.shape[0], q.shape[1], -1)\n        k = self.ln_k(k).reshape(q.shape[0], q.shape[1], -1)\n        return (q, k, v)"
            },
            {
                "name": "optimized_attention",
                "code": "def optimized_attention(qkv, num_heads):\n    return attention.optimized_attention(qkv[0], qkv[1], qkv[2], num_heads)"
            },
            {
                "name": "context_block.post_attention",
                "code": "def post_attention(self, x: torch.Tensor) -> torch.Tensor:\n        assert not self.pre_only\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x"
            },
            {
                "name": "x_block.post_attention",
                "code": "def post_attention(self, x: torch.Tensor) -> torch.Tensor:\n        assert not self.pre_only\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x"
            }
        ],
        "third_party": [
            "o.append",
            "torch.cat"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def get_timestep_embedding(timesteps, embedding_dim)",
        "start_line": "17",
        "end_line": "35",
        "file_path": "comfy/ldm/modules/diffusionmodules/model.py",
        "docstring": "The function generates timestep embeddings for a given sequence of timesteps and embedding dimension.\\nIt ensures the timesteps input is one-dimensional, then calculates a positional embedding using sine and cosine functions.\\nThe embedding is scaled by a logarithmic factor, and the result is concatenated to form the final embedding.\\nIf the embedding dimension is odd, it zero-pads the embedding to match the specified dimension before returning it.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b219bce30160",
        "ground_truth": "def get_timestep_embedding(timesteps, embedding_dim):\n    \"\"\"\n    This matches the implementation in Denoising Diffusion Probabilistic Models:\n    From Fairseq.\n    Build sinusoidal embeddings.\n    This matches the implementation in tensor2tensor, but differs slightly\n    from the description in Section 3.5 of \"Attention Is All You Need\".\n    \"\"\"\n    assert len(timesteps.shape) == 1\n     half_dim = embedding_dim // 2\n    emb = math.log(10000) / (half_dim - 1)\n    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n    emb = emb.to(device=timesteps.device)\n    emb = timesteps.float()[:, None] * emb[None, :]\n    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n    if embedding_dim % 2 == 1:  # zero pad\n        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n    return emb",
        "import_statements": [
            "import math",
            "import torch",
            "from typing import Optional, Any",
            "import logging",
            "from comfy import model_management",
            "import comfy.ops"
        ],
        "reference_api": [
            "math.log",
            "timesteps.float",
            "len",
            "torch.cat",
            "torch.cos",
            "emb.to",
            "torch.sin",
            "torch.exp",
            "torch.arange",
            "pad"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "torch.exp",
            "torch.arange",
            "emb.to",
            "timesteps.float",
            "torch.cat",
            "torch.sin",
            "torch.cos",
            "pad"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def pytorch_attention(q, k, v)",
        "start_line": "226",
        "end_line": "240",
        "file_path": "comfy/ldm/modules/diffusionmodules/model.py",
        "docstring": "The function computes attention for given query, key, and value tensors using PyTorch's scaled dot-product attention.\\nIt reshapes and transposes the tensors to prepare them for attention computation.\\nIf out-of-memory (OOM) exception occurs, it switches to slice attention as a fallback.\\nThe function returns the attention output reshaped back to the original dimensions.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6ae8453c86be",
        "ground_truth": "def pytorch_attention(q, k, v):\n    # compute attention\n    B, C, H, W = q.shape\n    q, k, v = map(\n        lambda t: t.view(B, 1, C, -1).transpose(2, 3).contiguous(),\n        (q, k, v),\n    )\n     try:\n        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=False)\n        out = out.transpose(2, 3).reshape(B, C, H, W)\n    except model_management.OOM_EXCEPTION as e:\n        logging.warning(\"scaled_dot_product_attention OOMed: switched to slice attention\")\n        out = slice_attention(q.view(B, -1, C), k.view(B, -1, C).transpose(1, 2), v.view(B, -1, C).transpose(1, 2)).reshape(B, C, H, W)\n    return out",
        "import_statements": [
            "import math",
            "import torch",
            "from typing import Optional, Any",
            "import logging",
            "from comfy import model_management",
            "import comfy.ops"
        ],
        "reference_api": [
            "q.view",
            "t.view",
            "out.transpose",
            "map",
            "scaled_dot_product_attention",
            "transpose",
            "logging.warning",
            "reshape",
            "v.view",
            "contiguous",
            "slice_attention",
            "k.view"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "slice_attention",
                "code": "def slice_attention(q, k, v):\n    r1 = torch.zeros_like(k, device=q.device)\n    scale = (int(q.shape[-1])**(-0.5))\n\n    mem_free_total = model_management.get_free_memory(q.device)\n\n    gb = 1024 ** 3\n    tensor_size = q.shape[0] * q.shape[1] * k.shape[2] * q.element_size()\n    modifier = 3 if q.element_size() == 2 else 2.5\n    mem_required = tensor_size * modifier\n    steps = 1\n\n    if mem_required > mem_free_total:\n        steps = 2**(math.ceil(math.log(mem_required / mem_free_total, 2)))\n\n    while True:\n        try:\n            slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n            for i in range(0, q.shape[1], slice_size):\n                end = i + slice_size\n                s1 = torch.bmm(q[:, i:end], k) * scale\n\n                s2 = torch.nn.functional.softmax(s1, dim=2).permute(0,2,1)\n                del s1\n\n                r1[:, :, i:end] = torch.bmm(v, s2)\n                del s2\n            break\n        except model_management.OOM_EXCEPTION as e:\n            model_management.soft_empty_cache(True)\n            steps *= 2\n            if steps > 128:\n                raise e\n            logging.warning(\"out of memory error, increasing steps and trying again {}\".format(steps))\n\n    return r1"
            }
        ],
        "third_party": [
            "contiguous",
            "transpose",
            "t.view",
            "scaled_dot_product_attention",
            "reshape",
            "out.transpose",
            "reshape",
            "q.view",
            "transpose",
            "k.view",
            "transpose",
            "v.view"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "comfyanonymous/ComfyUI",
        "function_declaration": "def apply_control(h, control, name)",
        "start_line": "356",
        "end_line": "364",
        "file_path": "comfy/ldm/modules/diffusionmodules/openaimodel.py",
        "docstring": "The function applies a control adjustment to the input h if the specified control and name exist and are non-empty.\\nIf a control is found, it is popped from the control list, and an attempt is made to add it to h.\\nIf the addition fails, a warning is logged.\\nThe function returns the potentially modified h.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fe4433c987ad",
        "ground_truth": "def apply_control(h, control, name):\n    if control is not None and name in control and len(control[name]) > 0:\n        ctrl = control[name].pop()\n        if ctrl is not None:\n            try:\n                h += ctrl\n            except:\n                logging.warning(\"warning control could not be applied {} {}\".format(h.shape, ctrl.shape))\n    return h",
        "import_statements": [
            "from abc import abstractmethod",
            "from einops import rearrange",
            "import logging",
            "from comfy.ldm.util import exists",
            "import comfy.ops"
        ],
        "reference_api": [
            "logging.warning",
            "pop",
            "len",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "pop"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "TencentARC/GFPGAN",
        "function_declaration": "def get_codebook_entry(self, indices, shape)",
        "start_line": "90",
        "end_line": "105",
        "file_path": "gfpgan/archs/restoreformer_arch.py",
        "docstring": "The function retrieves quantized latent vectors based on given indices and an optional shape.\\nIt creates a one-hot encoding of the indices and multiplies it with the embedding weights to get the quantized vectors.\\nIf a shape is provided, it reshapes and permutes the quantized vectors to match the original input shape before returning them.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "56da4547c101",
        "ground_truth": "def get_codebook_entry(self, indices, shape):\n    # shape specifying (batch, height, width, channel)\n    # TODO: check for more easy handling with nn.Embedding\n    min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n    min_encodings.scatter_(1, indices[:, None], 1)\n    # get quantized latent vectors\n    z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n    if shape is not None:\n        z_q = z_q.view(shape)\n        # reshape back to match original input shape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n    return z_q",
        "import_statements": [
            "import torch"
        ],
        "reference_api": [
            "torch.zeros",
            "z_q.view",
            "to",
            "min_encodings.float",
            "torch.matmul",
            "z_q.permute",
            "contiguous",
            "min_encodings.scatter_"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "to",
            "torch.zeros",
            "min_encodings.scatter_",
            "torch.matmul",
            "min_encodings.float",
            "z_q.view",
            "contiguous",
            "z_q.permute"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "TencentARC/GFPGAN",
        "function_declaration": "def make_noise(self)",
        "start_line": "279",
        "end_line": "288",
        "file_path": "gfpgan/archs/stylegan2_clean_arch.py",
        "docstring": "The function generates a list of noise tensors for noise injection.\\nIt initializes the list with a noise tensor of size 4x4 on the appropriate device.\\nFor each size from 2^3 to 2^log_size, it appends two noise tensors of the corresponding size to the list.\\nThe function returns the list of noise tensors.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "548f8a1fb37d",
        "ground_truth": "def make_noise(self):\n    \"\"\"Make noise for noise injection.\"\"\"\n    device = self.constant_input.weight.device\n    noises = [torch.randn(1, 1, 4, 4, device=device)]\n    for i in range(3, self.log_size + 1):\n        for _ in range(2):\n            noises.append(torch.randn(1, 1, 2**i, 2**i, device=device))\n    return noises",
        "import_statements": [
            "import math",
            "import random",
            "import torch",
            "from basicsr.archs.arch_util import default_init_weights",
            "from basicsr.utils.registry import ARCH_REGISTRY",
            "from torch import nn",
            "from torch.nn import functional as F"
        ],
        "reference_api": [
            "noises.append",
            "range",
            "torch.randn"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "torch.randn",
            "noises.append",
            "torch.randn"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "TencentARC/GFPGAN",
        "function_declaration": "def mean_latent(self, num_latent)",
        "start_line": "293",
        "end_line": "296",
        "file_path": "gfpgan/archs/stylegan2_clean_arch.py",
        "docstring": "The function generates a mean latent vector.\\nIt creates random latent inputs and processes them through a style MLP.\\nIt then computes the mean of these processed latents along the first dimension and returns the resulting mean latent vector.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "42fd805563c6",
        "ground_truth": "def mean_latent(self, num_latent):\n    latent_in = torch.randn(num_latent, self.num_style_feat, device=self.constant_input.weight.device)\n    latent = self.style_mlp(latent_in).mean(0, keepdim=True)\n    return latent",
        "import_statements": [
            "import math",
            "import random",
            "import torch",
            "from basicsr.archs.arch_util import default_init_weights",
            "from basicsr.utils.registry import ARCH_REGISTRY",
            "from torch import nn",
            "from torch.nn import functional as F"
        ],
        "reference_api": [
            "self.style_mlp",
            "mean",
            "torch.randn"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "torch.randn",
            "mean",
            "self.style_mlp"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "TencentARC/GFPGAN",
        "function_declaration": "def color_jitter(img, shift)",
        "start_line": "91",
        "end_line": "96",
        "file_path": "gfpgan/data/ffhq_degradation_dataset.py",
        "docstring": "The function applies color jitter to an image by randomly shifting its RGB values within a specified range.\\nIt generates a random jitter value for each RGB channel, adds these values to the image, and clips the result to ensure pixel values remain between 0 and 1.\\nThe function returns the jittered image.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2f485fd924af",
        "ground_truth": "def color_jitter(img, shift):\n    \"\"\"jitter color: randomly jitter the RGB values, in numpy formats\"\"\"\n    jitter_val = np.random.uniform(-shift, shift, 3).astype(np.float32)\n    img = img + jitter_val\n    img = np.clip(img, 0, 1)\n    return img",
        "import_statements": [
            "import cv2",
            "import math",
            "import torch",
            "from basicsr.data import degradations as degradations",
            "from basicsr.data.data_util import paths_from_folder",
            "from basicsr.data.transforms import augment",
            "from basicsr.utils import FileClient, get_root_logger, imfrombytes, img2tensor",
            "from basicsr.utils.registry import DATASET_REGISTRY",
            "from torchvision.transforms.functional import (adjust_brightness, adjust_contrast, adjust_hue, adjust_saturation,\n                                               normalize)"
        ],
        "reference_api": [
            "uniform",
            "astype",
            "np.clip"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "astype",
            "uniform",
            "np.clip"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "TencentARC/GFPGAN",
        "function_declaration": "def feed_data(self, data)",
        "start_line": "201",
        "end_line": "210",
        "file_path": "gfpgan/models/gfpgan_model.py",
        "docstring": "The function transfers input data to the specified device.\\nIt assigns the 'lq' data to self.lq and, if present, assigns the 'gt' data to self.gt.\\nIf facial component locations are included in the data, it assigns the left eye, right eye, and mouth locations to the respective instance variables.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c87ccf1fbf48",
        "ground_truth": "def feed_data(self, data):\n    self.lq = data['lq'].to(self.device)\n    if 'gt' in data:\n        self.gt = data['gt'].to(self.device)\n    if 'loc_left_eye' in data:\n        # get facial component locations, shape (batch, 4)\n        self.loc_left_eyes = data['loc_left_eye']\n        self.loc_right_eyes = data['loc_right_eye']\n        self.loc_mouths = data['loc_mouth']",
        "import_statements": [
            "import math",
            "import torch",
            "from basicsr.archs import build_network",
            "from basicsr.losses import build_loss",
            "from basicsr.losses.gan_loss import r1_penalty",
            "from basicsr.metrics import calculate_metric",
            "from basicsr.models.base_model import BaseModel",
            "from basicsr.utils import get_root_logger, imwrite, tensor2img",
            "from basicsr.utils.registry import MODEL_REGISTRY",
            "from collections import OrderedDict",
            "from torch.nn import functional as F",
            "from torchvision.ops import roi_align",
            "from tqdm import tqdm"
        ],
        "reference_api": [
            "to"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "to",
            "to"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "TencentARC/GFPGAN",
        "function_declaration": "def construct_img_pyramid(self)",
        "start_line": "225",
        "end_line": "232",
        "file_path": "gfpgan/models/gfpgan_model.py",
        "docstring": "The function constructs an image pyramid for intermediate restoration loss.\\nIt starts with the ground truth image and iteratively downscales it using bilinear interpolation.\\nThe downscaled images are inserted at the beginning of the pyramid list.\\nThe function returns the constructed image pyramid.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8f8c7cb13037",
        "ground_truth": "def construct_img_pyramid(self):\n    \"\"\"Construct image pyramid for intermediate restoration loss\"\"\"\n    pyramid_gt = [self.gt]\n    down_img = self.gt\n    for _ in range(0, self.log_size - 3):\n        down_img = F.interpolate(down_img, scale_factor=0.5, mode='bilinear', align_corners=False)\n        pyramid_gt.insert(0, down_img)\n    return pyramid_gt",
        "import_statements": [
            "import math",
            "import torch",
            "from basicsr.archs import build_network",
            "from basicsr.losses import build_loss",
            "from basicsr.losses.gan_loss import r1_penalty",
            "from basicsr.metrics import calculate_metric",
            "from basicsr.models.base_model import BaseModel",
            "from basicsr.utils import get_root_logger, imwrite, tensor2img",
            "from basicsr.utils.registry import MODEL_REGISTRY",
            "from collections import OrderedDict",
            "from torch.nn import functional as F",
            "from torchvision.ops import roi_align",
            "from tqdm import tqdm"
        ],
        "reference_api": [
            "pyramid_gt.insert",
            "F.interpolate",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "F.interpolate",
            "pyramid_gt.insert"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "TencentARC/GFPGAN",
        "function_declaration": "def save(self, epoch, current_iter)",
        "start_line": "569",
        "end_line": "579",
        "file_path": "gfpgan/models/gfpgan_model.py",
        "docstring": "The function saves the state of the model at a given epoch and iteration.\\nIt saves the generator network, its EMA version, and the discriminator network.\\nIf facial discriminators are used, it also saves the discriminators for the left eye, right eye, and mouth.\\nFinally, it saves the overall training state.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "51247274d40c",
        "ground_truth": "def save(self, epoch, current_iter):\n    # save net_g and net_d\n    self.save_network([self.net_g, self.net_g_ema], 'net_g', current_iter, param_key=['params', 'params_ema'])\n    self.save_network(self.net_d, 'net_d', current_iter)\n    # save component discriminators\n    if self.use_facial_disc:\n        self.save_network(self.net_d_left_eye, 'net_d_left_eye', current_iter)\n        self.save_network(self.net_d_right_eye, 'net_d_right_eye', current_iter)\n        self.save_network(self.net_d_mouth, 'net_d_mouth', current_iter)\n    # save training state\n    self.save_training_state(epoch, current_iter)",
        "import_statements": [
            "import math",
            "import torch",
            "from basicsr.archs import build_network",
            "from basicsr.losses import build_loss",
            "from basicsr.losses.gan_loss import r1_penalty",
            "from basicsr.metrics import calculate_metric",
            "from basicsr.models.base_model import BaseModel",
            "from basicsr.utils import get_root_logger, imwrite, tensor2img",
            "from basicsr.utils.registry import MODEL_REGISTRY",
            "from collections import OrderedDict",
            "from torch.nn import functional as F",
            "from torchvision.ops import roi_align",
            "from tqdm import tqdm"
        ],
        "reference_api": [
            "self.save_network",
            "self.save_training_state"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self.save_network",
            "self.save_network",
            "self.save_network",
            "self.save_network",
            "self.save_network",
            "self.save_training_state"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "TencentARC/GFPGAN",
        "function_declaration": "def clean_folder(folder)",
        "start_line": "152",
        "end_line": "161",
        "file_path": "cog_predict.py",
        "docstring": "The function deletes all files and directories within a specified folder.\\nIt iterates through the items in the folder, removing files and links using os.unlink, and directories using shutil.rmtree.\\nIf an error occurs during deletion, it prints an error message with the file path and reason.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "afa0dc8128c1",
        "ground_truth": "def clean_folder(folder):\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f'Failed to delete {file_path}. Reason: {e}')",
        "import_statements": [
            "import os",
            "import cv2",
            "import shutil",
            "import tempfile",
            "import torch",
            "from basicsr.archs.srvgg_arch import SRVGGNetCompact",
            "from gfpgan import GFPGANer"
        ],
        "reference_api": [
            "join",
            "print",
            "os.listdir",
            "os.unlink",
            "isfile",
            "shutil.rmtree",
            "isdir",
            "islink"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "join",
            "isfile",
            "islink",
            "isdir"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "microsoft/DeepSpeed",
        "function_declaration": "def next_batch(self, sample_size=1)",
        "start_line": "17",
        "end_line": "24",
        "file_path": "deepspeed/autotuning/tuner/index_based_tuner.py",
        "docstring": "The function retrieves the next batch of samples from a list of experiences.\\nIt adjusts the sample size if it exceeds the available experiences.\\nIt randomly selects the specified number of samples, removes them from the original list, and returns the sampled batch.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "53588b481bdd",
        "ground_truth": "def next_batch(self, sample_size=1):\n    if sample_size > len(self.all_exps):\n        sample_size = len(self.all_exps)\n    sampled_batch = random.sample(self.all_exps, sample_size)\n    self.all_exps = [x for x in self.all_exps if x not in sampled_batch]\n    return sampled_batch",
        "import_statements": [
            "import random"
        ],
        "reference_api": [
            "random.sample",
            "len"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "microsoft/DeepSpeed",
        "function_declaration": "def find_estimated_top_configs(self)",
        "start_line": "58",
        "end_line": "81",
        "file_path": "deepspeed/autotuning/tuner/model_based_tuner.py",
        "docstring": "The function finds the estimated top configurations based on a cost model.\\nIt flattens each configuration in all_configs, collects numerical values, and converts them into a numpy array.\\nThe cost model predicts estimates for these configurations, which are then sorted to identify the top configurations.\\nThe sorting order depends on the specified metric, and the function returns the indices of the top configurations.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6bfa6a4e621c",
        "ground_truth": "def find_estimated_top_configs(self):\n    \"\"\"Use the cost model to predict the estimated performance of configurations and find the top ones for the next round of evaluation\"\"\"\n    configs = []\n    for c in self.all_configs:\n        flattened_ds_config = flatten(c)\n        feature_val = []\n        for k, v in flattened_ds_config.items():\n            if isinstance(v, numbers.Number):\n                feature_val.append(v)\n        configs.append(feature_val)\n    # print(configs)\n    # TODO the current implementation requires that all configs have the same shape.\n    configs = np.array(configs, dtype=np.float32)\n    estimates = self.cost_model.predict(configs)\n    n = len(estimates)\n    top_idx = np.argsort(estimates)\n    top_idx_ret = top_idx if self.metric == AUTOTUNING_METRIC_LATENCY else top_idx[::-1][:n]\n    # top_configs = [self.all_configs[i] for i in top_idx]\n    return top_idx_ret",
        "import_statements": [
            "import hjson",
            "import numbers"
        ],
        "reference_api": [
            "predict",
            "configs.append",
            "feature_val.append",
            "len",
            "isinstance",
            "np.argsort",
            "flatten",
            "flattened_ds_config.items",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "flatten",
            "flattened_ds_config.items",
            "feature_val.append",
            "configs.append",
            "np.array",
            "predict",
            "np.argsort"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "microsoft/DeepSpeed",
        "function_declaration": "def flatten(d, parent_key='', sep='_')",
        "start_line": "55",
        "end_line": "63",
        "file_path": "deepspeed/autotuning/tuner/utils.py",
        "docstring": "The function recursively flattens a nested dictionary.\\nIt concatenates parent and child keys using a specified separator to create new keys.\\nIf a value is a dictionary, it recursively flattens it and extends the items list.\\nOtherwise, it adds the key-value pair to the items list and returns the flattened dictionary.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d4cf48413a57",
        "ground_truth": "def flatten(d, parent_key='', sep='_'):\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)",
        "import_statements": [
            "import itertools",
            "import collections.abc"
        ],
        "reference_api": [
            "d.items",
            "items.append",
            "items.extend",
            "dict",
            "isinstance",
            "items",
            "flatten"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "flatten",
                "code": "def flatten(d, parent_key='', sep='_'):\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)"
            }
        ],
        "third_party": [
            "d.items",
            "items.extend",
            "items",
            "items.append"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "microsoft/DeepSpeed",
        "function_declaration": "def run_after_tuning(self)",
        "start_line": "1103",
        "end_line": "1113",
        "file_path": "deepspeed/autotuning/autotuner.py",
        "docstring": "The function executes a command after tuning if an optimal command is found.\\nIf optimal_cmd is available, it runs the command using subprocess.Popen and waits for it to complete, then logs a success message.\\nIf no optimal command is found, it logs a message indicating that autotuning did not find an optimal configuration.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c2a514249c25",
        "ground_truth": "def run_after_tuning(self):\n    \"\"\" Launches the training with the optimal DeepSpeed configuration found through the autotuning process.\n        \"ds_config_optimal.json\" describing the optimal DeepSpeed configuration as well the command used to launch training \"cmd_optimal.txt\" are saved to self.results_dir.\n    \"\"\"\n    if self.optimal_cmd:\n        result = subprocess.Popen(self.optimal_cmd)\n        result.wait()\n        logger.info(f\"Done running with the optimal DeepSpeed configuration using {self.optimal_cmd}\")\n    else:\n        logger.info(f\"No optimal DeepSpeed configuration found by autotuning.\")",
        "import_statements": [
            "import shutil",
            "import subprocess",
            "import time",
            "import datetime",
            "import math",
            "import hjson",
            "from deepspeed.accelerator import get_accelerator"
        ],
        "reference_api": [
            "subprocess.Popen",
            "logger.info",
            "result.wait"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "result.wait"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "microsoft/DeepSpeed",
        "function_declaration": "def run_job(self, exp: dict, reservations)",
        "start_line": "89",
        "end_line": "109",
        "file_path": "deepspeed/autotuning/scheduler.py",
        "docstring": "The function runs an experiment job by configuring its parameters and starting it in a new thread.\\nIt sets the experiment's master port and result directory, and updates user arguments based on argument mappings.\\nA new thread is created to run the experiment with the specified script and arguments, and the thread is started.\\nThe running experiment details are stored with a timestamp in running_experiments.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "61802de52df0",
        "ground_truth": "def run_job(self, exp: dict, reservations):\n    exp_id = exp[\"exp_id\"]\n    exp[\"master_port\"] = self.args.master_port + exp_id\n    exp[\"result_dir\"] = os.path.join(self.results_dir, exp['name'])\n    user_script = self.args.user_script\n    user_args = self.args.user_args\n    # overwrite the user arg in the arg_mappings\n    for key, val in self.arg_mappings.items():\n        nval = get_val_by_key(exp, key)\n        if nval and str(nval) != \"auto\":\n            if val in user_args:\n                idx = user_args.index(val)\n                user_args[idx + 1] = str(nval)\n            else:\n                user_args.append(val)\n                user_args.append(str(nval))\n    t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n    t.start()\n    self.running_experiments[exp_id] = (t, exp, reservations, time.time())",
        "import_statements": [
            "import copy",
            "import json",
            "import subprocess",
            "import sys",
            "import threading",
            "import time",
            "import base64",
            "import os",
            "import hjson",
            "from tqdm import tqdm",
            "from deepspeed import comm as dist"
        ],
        "reference_api": [
            "join",
            "threading.Thread",
            "t.start",
            "user_args.append",
            "user_args.index",
            "items",
            "str",
            "get_val_by_key",
            "time.time"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "join",
            "items",
            "get_val_by_key",
            "user_args.index",
            "user_args.append",
            "user_args.append",
            "t.start"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "microsoft/DeepSpeed",
        "function_declaration": "def save_exp_results_to_database(self, message, ranks=None, path=None):",
        "start_line": "186",
        "end_line": "209",
        "file_path": "deepspeed/autotuning/scheduler.py",
        "docstring": "The function saves experimental results to a database if certain logging conditions are met.\\nIt checks if distributed training is initialized and sets logging permissions accordingly.\\nIf logging is permitted, it adds the rank to the message and appends the message to a specified file in JSON format.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4884e2e9665c",
        "ground_truth": "def save_exp_results_to_database(self, message, ranks=None, path=None):\n    \"\"\"Print message when one of following condition meets\n    + not dist.is_initialized()\n    + dist.get_rank() in ranks if ranks is not None or ranks = [-1]\nArgs:\n        message (str)\n        ranks (list)\n        path (str)\n    \"\"\"\n    should_log = not dist.is_initialized()\n    ranks = ranks or []\n    my_rank = dist.get_rank() if dist.is_initialized() else -1\n    if ranks and not should_log:\n        should_log = ranks[0] == -1\n        should_log = should_log or (my_rank in set(ranks))\n    logger.debug(f\"*** Should log: {should_log}\")\n    if should_log:\n        message['rank'] = my_rank\n        with open(path, 'a') as outfile:\n            json.dump(message, outfile)\n            outfile.write('\\n')",
        "import_statements": [
            "import copy",
            "import json",
            "import subprocess",
            "import sys",
            "import threading",
            "import time",
            "import base64",
            "import os",
            "import hjson",
            "from tqdm import tqdm",
            "from deepspeed import comm as dist"
        ],
        "reference_api": [
            "json.dump",
            "dist.get_rank",
            "set",
            "dist.is_initialized",
            "logger.debug",
            "outfile.write",
            "open"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "dist.is_initialized",
            "dist.get_rank",
            "dist.is_initialized",
            "outfile.write"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "microsoft/DeepSpeed",
        "function_declaration": "def _build_tp_other_layer_map(self, layer_index: int)",
        "start_line": "206",
        "end_line": "214",
        "file_path": "deepspeed/checkpoint/deepspeed_checkpoint.py",
        "docstring": "The function builds a data map for a specific layer index.\\nIt first checks if there are any layer files and verifies the layer index.\\nIt retrieves files with a specific prefix for the given layer and partitions these files based on tp_degree.\\nThe function then creates and returns a dictionary mapping partition indices to their corresponding file lists.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0429344c3c4d",
        "ground_truth": "def _build_tp_other_layer_map(self, layer_index: int):\n    data_map = {}\n    if len(self.layer_files) < 1:\n        return data_map\n    assert layer_index <= len(self.layer_files)\n    layer_files = get_files_with_prefix(self.layer_files, self.layer_keys[layer_index])\n    layer_file_partitions = partition_data(layer_files, self.tp_degree)\n    data_map = {i: flist for i, flist in enumerate(layer_file_partitions)}\n    return data_map",
        "import_statements": [
            "import os",
            "import re",
            "from typing import Dict",
            "import torch"
        ],
        "reference_api": [
            "get_files_with_prefix",
            "partition_data",
            "len",
            "enumerate"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get_files_with_prefix",
            "partition_data"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def system_info()",
        "start_line": "15",
        "end_line": "22",
        "file_path": "TTS/bin/collect_env_info.py",
        "docstring": "The function retrieves system information and returns it as a dictionary.\\nIt includes the operating system, architecture, version, processor, and Python version.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1b65ecb9be93",
        "ground_truth": "def system_info():\n    return {\n        \"OS\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python\": platform.python_version(),\n    }",
        "import_statements": [
            "import os",
            "import platform",
            "import sys",
            "import numpy",
            "import torch",
            "import json",
            "import TTS"
        ],
        "reference_api": [
            "platform.python_version",
            "platform.version",
            "platform.architecture",
            "platform.system",
            "platform.processor"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def cuda_info()",
        "start_line": "25",
        "end_line": "30",
        "file_path": "TTS/bin/collect_env_info.py",
        "docstring": "The function returns information about the CUDA setup.\\nIt provides a list of GPU device names, the availability status of CUDA, and the CUDA version being used.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "969bcc5cf3b5",
        "ground_truth": "def cuda_info():\n    return {\n        \"GPU\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n        \"available\": torch.cuda.is_available(),\n        \"version\": torch.version.cuda,\n    }",
        "import_statements": [
            "import os",
            "import platform",
            "import sys",
            "import numpy",
            "import torch",
            "import json",
            "import TTS"
        ],
        "reference_api": [
            "get_device_name",
            "device_count",
            "is_available",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get_device_name",
            "device_count",
            "is_available"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def compute_encoder_accuracy(dataset_items, encoder_manager)",
        "start_line": "12",
        "end_line": "50",
        "file_path": "TTS/bin/eval_encoder.py",
        "docstring": "The function computes the encoder accuracy for a given dataset.\\nIt retrieves the class name key and optional class ID to class name mapping from the encoder manager's configuration.\\nFor each item in the dataset, it extracts the audio file, computes the embedding, and predicts the class label if criteria and mappings are available.\\nIt records the accuracy for each class by comparing the true class name with the predicted label.\\nFinally, it calculates and prints the accuracy for each class and the average accuracy across all classes.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a2ba908a4c70",
        "ground_truth": "def compute_encoder_accuracy(dataset_items, encoder_manager):\n    class_name_key = encoder_manager.encoder_config.class_name_key\n    map_classid_to_classname = getattr(encoder_manager.encoder_config, \"map_classid_to_classname\", None)\n     class_acc_dict = {}\n     # compute embeddings for all wav_files\n    for item in tqdm(dataset_items):\n        class_name = item[class_name_key]\n        wav_file = item[\"audio_file\"]\n         # extract the embedding\n        embedd = encoder_manager.compute_embedding_from_clip(wav_file)\n        if encoder_manager.encoder_criterion is not None and map_classid_to_classname is not None:\n            embedding = torch.FloatTensor(embedd).unsqueeze(0)\n            if encoder_manager.use_cuda:\n                embedding = embedding.cuda()\n             class_id = encoder_manager.encoder_criterion.softmax.inference(embedding).item()\n            predicted_label = map_classid_to_classname[str(class_id)]\n        else:\n            predicted_label = None\n         if class_name is not None and predicted_label is not None:\n            is_equal = int(class_name == predicted_label)\n            if class_name not in class_acc_dict:\n                class_acc_dict[class_name] = [is_equal]\n            else:\n                class_acc_dict[class_name].append(is_equal)\n        else:\n            raise RuntimeError(\"Error: class_name or/and predicted_label are None\")\n     acc_avg = 0\n    for key, values in class_acc_dict.items():\n        acc = sum(values) / len(values)\n        print(\"Class\", key, \"Accuracy:\", acc)\n        acc_avg += acc\n     print(\"Average Accuracy:\", acc_avg / len(class_acc_dict))",
        "import_statements": [
            "import argparse",
            "from argparse import RawTextHelpFormatter",
            "import torch",
            "from tqdm import tqdm",
            "from TTS.config import load_config",
            "from TTS.tts.datasets import load_tts_samples",
            "from TTS.tts.utils.speakers import SpeakerManager"
        ],
        "reference_api": [
            "getattr",
            "RuntimeError",
            "print",
            "embedding.cuda",
            "class_acc_dict.items",
            "item",
            "int",
            "len",
            "append",
            "sum",
            "encoder_manager.compute_embedding_from_clip",
            "tqdm",
            "inference",
            "str",
            "unsqueeze",
            "torch.FloatTensor"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tqdm",
            "encoder_manager.compute_embedding_from_clip",
            "unsqueeze",
            "torch.FloatTensor",
            "embedding.cuda",
            "item",
            "inference",
            "append",
            "class_acc_dict.items"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def adjust_path_and_remove_silence(audio_path)",
        "start_line": "15",
        "end_line": "31",
        "file_path": "TTS/bin/remove_silence_using_vad.py",
        "docstring": "The function adjusts the output path and removes silence from an audio file.\\nIt replaces the input directory path with the output directory path to determine the new file location.\\nIf the file already exists and force is not specified, it returns the output path and a False flag.\\nOtherwise, it creates the necessary directory structure, removes silence from the audio using specified parameters, and saves the processed audio.\\nFinally, it returns the output path and a flag indicating if speech was detected.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e01ec69ad58a",
        "ground_truth": "def adjust_path_and_remove_silence(audio_path):\n    output_path = audio_path.replace(os.path.join(args.input_dir, \"\"), os.path.join(args.output_dir, \"\"))\n    # ignore if the file exists\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n     # create all directory structure\n    pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    # remove the silence and save the audio\n    output_path, is_speech = remove_silence(\n        model_and_utils,\n        audio_path,\n        output_path,\n        trim_just_beginning_and_end=args.trim_just_beginning_and_end,\n        use_cuda=args.use_cuda,\n    )\n    return output_path, is_speech",
        "import_statements": [
            "import argparse",
            "import glob",
            "import multiprocessing",
            "import os",
            "import pathlib",
            "import torch",
            "from tqdm import tqdm",
            "from TTS.utils.vad import get_vad_model_and_utils, remove_silence"
        ],
        "reference_api": [
            "join",
            "exists",
            "mkdir",
            "pathlib.Path",
            "remove_silence",
            "audio_path.replace"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "remove_silence",
                "code": "def remove_silence(\n    model_and_utils, audio_path, out_path, vad_sample_rate=8000, trim_just_beginning_and_end=True, use_cuda=False\n):\n    # get the VAD model and utils functions\n    model, get_speech_timestamps, _, collect_chunks = model_and_utils\n\n    # read ground truth wav and resample the audio for the VAD\n    try:\n        wav, gt_sample_rate = read_audio(audio_path)\n    except:\n        print(f\"> \u2757 Failed to read {audio_path}\")\n        return None, False\n\n    # if needed, resample the audio for the VAD model\n    if gt_sample_rate != vad_sample_rate:\n        wav_vad = resample_wav(wav, gt_sample_rate, vad_sample_rate)\n    else:\n        wav_vad = wav\n\n    if use_cuda:\n        wav_vad = wav_vad.cuda()\n\n    # get speech timestamps from full audio file\n    speech_timestamps = get_speech_timestamps(wav_vad, model, sampling_rate=vad_sample_rate, window_size_samples=768)\n\n    # map the current speech_timestamps to the sample rate of the ground truth audio\n    new_speech_timestamps = map_timestamps_to_new_sr(\n        vad_sample_rate, gt_sample_rate, speech_timestamps, trim_just_beginning_and_end\n    )\n\n    # if have speech timestamps else save the wav\n    if new_speech_timestamps:\n        wav = collect_chunks(new_speech_timestamps, wav)\n        is_speech = True\n    else:\n        print(f\"> The file {audio_path} probably does not have speech please check it !!\")\n        is_speech = False\n\n    # save\n    torchaudio.save(out_path, wav[None, :], gt_sample_rate)\n    return out_path, is_speech\n"
            }
        ],
        "third_party": [
            "audio_path.replace",
            "join",
            "join",
            "exists",
            "mkdir"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10)",
        "start_line": "19",
        "end_line": "34",
        "file_path": "TTS/bin/resample.py",
        "docstring": "The function resamples audio files in a specified directory to a given sample rate.\\nIf an output directory is provided, it copies the input directory to the output directory.\\nIt then recursively finds all audio files with the specified extension and resamples them in parallel using a specified number of jobs.\\nProgress is displayed with a progress bar, and a completion message is printed at the end.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7e0bdb0feff8",
        "ground_truth": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10):\n    if output_dir:\n        print(\"Recursively copying the input folder...\")\n        copytree(input_dir, output_dir)\n        input_dir = output_dir\n     print(\"Resampling the audio files...\")\n    audio_files = glob.glob(os.path.join(input_dir, f\"**/*.{file_ext}\"), recursive=True)\n    print(f\"Found {len(audio_files)} files...\")\n    audio_files = list(zip(audio_files, len(audio_files) * [output_sr]))\n    with Pool(processes=n_jobs) as p:\n        with tqdm(total=len(audio_files)) as pbar:\n            for _, _ in enumerate(p.imap_unordered(resample_file, audio_files)):\n                pbar.update()\n     print(\"Done !\")",
        "import_statements": [
            "import argparse",
            "import glob",
            "import os",
            "from argparse import RawTextHelpFormatter",
            "from multiprocessing import Pool",
            "from shutil import copytree",
            "import librosa",
            "from tqdm import tqdm"
        ],
        "reference_api": [
            "join",
            "copytree",
            "print",
            "list",
            "p.imap_unordered",
            "pbar.update",
            "Pool",
            "len",
            "tqdm",
            "zip",
            "glob.glob",
            "enumerate"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "copytree",
            "join",
            "Pool",
            "tqdm",
            "p.imap_unordered",
            "pbar.update"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def evaluation(model, criterion, data_loader, global_step)",
        "start_line": "85",
        "end_line": "123",
        "file_path": "TTS/bin/train_encoder.py",
        "docstring": "The function evaluates a model using a given criterion and data loader at a specific global step.\\nIt computes the average evaluation loss over all batches without updating model parameters.\\nInputs and labels are reshaped to group samples of each class, and data is dispatched to the GPU if available.\\nAfter computing the loss for each batch, it calculates the average loss and logs evaluation statistics.\\nIt also generates and logs a UMAP plot of the embeddings from the last batch.\\nThe function returns the average evaluation loss.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a1351834ca57",
        "ground_truth": "def evaluation(model, criterion, data_loader, global_step):\n    eval_loss = 0\n    for _, data in enumerate(data_loader):\n        with torch.no_grad():\n            # setup input data\n            inputs, labels = data\n             # agroup samples of each class in the batch. perfect sampler produces [3,2,1,3,2,1] we need [3,3,2,2,1,1]\n            labels = torch.transpose(\n                labels.view(c.eval_num_utter_per_class, c.eval_num_classes_in_batch), 0, 1\n            ).reshape(labels.shape)\n            inputs = torch.transpose(\n                inputs.view(c.eval_num_utter_per_class, c.eval_num_classes_in_batch, -1), 0, 1\n            ).reshape(inputs.shape)\n             # dispatch data to GPU\n            if use_cuda:\n                inputs = inputs.cuda(non_blocking=True)\n                labels = labels.cuda(non_blocking=True)\n             # forward pass model\n            outputs = model(inputs)\n             # loss computation\n            loss = criterion(\n                outputs.view(c.eval_num_classes_in_batch, outputs.shape[0] // c.eval_num_classes_in_batch, -1), labels\n            )\n             eval_loss += loss.item()\n     eval_avg_loss = eval_loss / len(data_loader)\n    # save stats\n    dashboard_logger.eval_stats(global_step, {\"loss\": eval_avg_loss})\n    # plot the last batch in the evaluation\n    figures = {\n        \"UMAP Plot\": plot_embeddings(outputs.detach().cpu().numpy(), c.num_classes_in_batch),\n    }\n    dashboard_logger.eval_figures(global_step, figures)\n    return eval_avg_loss",
        "import_statements": [
            "import os",
            "import sys",
            "import time",
            "import traceback",
            "import torch",
            "from torch.utils.data import DataLoader",
            "from trainer.io import copy_model_files, save_best_model, save_checkpoint",
            "from trainer.torch import NoamLR",
            "from trainer.trainer_utils import get_optimizer",
            "from TTS.encoder.dataset import EncoderDataset",
            "from TTS.encoder.utils.generic_utils import setup_encoder_model",
            "from TTS.encoder.utils.training import init_training",
            "from TTS.encoder.utils.visual import plot_embeddings",
            "from TTS.tts.datasets import load_tts_samples",
            "from TTS.utils.audio import AudioProcessor",
            "from TTS.utils.generic_utils import count_parameters, remove_experiment_folder",
            "from TTS.utils.samplers import PerfectBatchSampler",
            "from TTS.utils.training import check_update"
        ],
        "reference_api": [
            "numpy",
            "plot_embeddings",
            "labels.cuda",
            "torch.transpose",
            "reshape",
            "labels.view",
            "loss.item",
            "dashboard_logger.eval_stats",
            "inputs.view",
            "cpu",
            "len",
            "model",
            "torch.no_grad",
            "enumerate",
            "inputs.cuda",
            "outputs.detach",
            "dashboard_logger.eval_figures",
            "criterion",
            "outputs.view"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "plot_embeddings",
                "code": "def plot_embeddings(embeddings, num_classes_in_batch):\n    num_utter_per_class = embeddings.shape[0] // num_classes_in_batch\n\n    # if necessary get just the first 10 classes\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n        embeddings = embeddings[: num_classes_in_batch * num_utter_per_class]\n\n    model = umap.UMAP()\n    projection = model.fit_transform(embeddings)\n    ground_truth = np.repeat(np.arange(num_classes_in_batch), num_utter_per_class)\n    colors = [colormap[i] for i in ground_truth]\n    fig, ax = plt.subplots(figsize=(16, 10))\n    _ = ax.scatter(projection[:, 0], projection[:, 1], c=colors)\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.title(\"UMAP projection\")\n    plt.tight_layout()\n    plt.savefig(\"umap\")\n    return fig"
            }
        ],
        "third_party": [
            "torch.no_grad",
            "reshape",
            "torch.transpose",
            "labels.view",
            "reshape",
            "torch.transpose",
            "inputs.view",
            "inputs.cuda",
            "labels.cuda",
            "model",
            "criterion",
            "outputs.view",
            "loss.item",
            "dashboard_logger.eval_stats",
            "numpy",
            "cpu",
            "outputs.detach",
            "dashboard_logger.eval_figures"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def register_config(model_name: str) -> Coqpit",
        "start_line": "23",
        "end_line": "51",
        "file_path": "TTS/config/__init__.py",
        "docstring": "The function registers a configuration class for a given model name.\\nIt constructs the configuration class name and checks if the model is \"xtts\" to import its specific configuration.\\nIt searches through predefined paths to find and assign the configuration class, handling ModuleNotFoundError exceptions.\\nIf the configuration class is not found, it raises a ModuleNotFoundError.\\nThe function returns the found configuration class.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "565f8ed09916",
        "ground_truth": "def register_config(model_name: str) -> Coqpit:\n    \"\"\"Find the right config for the given model name.\n     Args:\n        model_name (str): Model name.\n     Raises:\n        ModuleNotFoundError: No matching config for the model name.\n     Returns:\n        Coqpit: config class.\n    \"\"\"\n    config_class = None\n    config_name = model_name + \"_config\"\n     # TODO: fix this\n    if model_name == \"xtts\":\n        from TTS.tts.configs.xtts_config import XttsConfig\n         config_class = XttsConfig\n    paths = [\"TTS.tts.configs\", \"TTS.vocoder.configs\", \"TTS.encoder.configs\", \"TTS.vc.configs\"]\n    for path in paths:\n        try:\n            config_class = find_module(path, config_name)\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        raise ModuleNotFoundError(f\" [!] Config for {model_name} cannot be found.\")\n    return config_class",
        "import_statements": [
            "import json",
            "import os",
            "import re",
            "from typing import Dict",
            "import fsspec",
            "import yaml",
            "from coqpit import Coqpit",
            "from TTS.config.shared_configs import *",
            "from TTS.utils.generic_utils import find_module"
        ],
        "reference_api": [
            "find_module",
            "ModuleNotFoundError"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "find_module",
                "code": "def find_module(module_path: str, module_name: str) -> object:\n    module_name = module_name.lower()\n    module = importlib.import_module(module_path + \".\" + module_name)\n    class_name = to_camel(module_name)\n    return getattr(module, class_name)"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def load_config(config_path: str) -> Coqpit",
        "start_line": "68",
        "end_line": "100",
        "file_path": "TTS/config/__init__.py",
        "docstring": "The function loads a configuration from a given file path.\\nIt determines the file extension and reads the content accordingly as YAML or JSON, raising an error for unknown types.\\nThe function processes the configuration dictionary to obtain the model name and registers the appropriate configuration class.\\nIt then creates an instance of the configuration class, populates it with the loaded data, and returns the configuration object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0ea7862506a7",
        "ground_truth": "def load_config(config_path: str) -> Coqpit:\n    \"\"\"Import `json` or `yaml` files as TTS configs. First, load the input file as a `dict` and check the model name\n    to find the corresponding Config class. Then initialize the Config.\n     Args:\n        config_path (str): path to the config file.\n     Raises:\n        TypeError: given config file has an unknown type.\n     Returns:\n        Coqpit: TTS config object.\n    \"\"\"\n    config_dict = {}\n    ext = os.path.splitext(config_path)[1]\n    if ext in (\".yml\", \".yaml\"):\n        with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n            data = yaml.safe_load(f)\n    elif ext == \".json\":\n        try:\n            with fsspec.open(config_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n        except json.decoder.JSONDecodeError:\n            # backwards compat.\n            data = read_json_with_comments(config_path)\n    else:\n        raise TypeError(f\" [!] Unknown config file type {ext}\")\n    config_dict.update(data)\n    model_name = _process_model_name(config_dict)\n    config_class = register_config(model_name.lower())\n    config = config_class()\n    config.from_dict(config_dict)\n    return config",
        "import_statements": [
            "import json",
            "import os",
            "import re",
            "from typing import Dict",
            "import fsspec",
            "import yaml",
            "from coqpit import Coqpit",
            "from TTS.config.shared_configs import *",
            "from TTS.utils.generic_utils import find_module"
        ],
        "reference_api": [
            "fsspec.open",
            "yaml.safe_load",
            "config_dict.update",
            "_process_model_name",
            "TypeError",
            "model_name.lower",
            "register_config",
            "config_class",
            "read_json_with_comments",
            "splitext",
            "config.from_dict",
            "json.load"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "read_json_with_comments",
                "code": "def read_json_with_comments(json_path):\n    \"\"\"for backward compat.\"\"\"\n    # fallback to json\n    with fsspec.open(json_path, \"r\", encoding=\"utf-8\") as f:\n        input_str = f.read()\n    # handle comments but not urls with //\n    input_str = re.sub(r\"(\\\"(?:[^\\\"\\\\]|\\\\.)*\\\")|(/\\*(?:.|[\\\\n\\\\r])*?\\*/)|(//.*)\", lambda m: m.group(1) or m.group(2) or \"\", input_str)\n    return json.loads(input_str)"
            },
            {
                "name": "_process_model_name",
                "code": "def _process_model_name(config_dict: Dict) -> str:\n    \"\"\"Format the model name as expected. It is a band-aid for the old `vocoder` model names.\n\n    Args:\n        config_dict (Dict): A dictionary including the config fields.\n\n    Returns:\n        str: Formatted modelname.\n    \"\"\"\n    model_name = config_dict[\"model\"] if \"model\" in config_dict else config_dict[\"generator_model\"]\n    model_name = model_name.replace(\"_generator\", \"\").replace(\"_discriminator\", \"\")\n    return model_name"
            },
            {
                "name": "register_config",
                "code": "def register_config(model_name: str) -> Coqpit:\n    \"\"\"Find the right config for the given model name.\n\n    Args:\n        model_name (str): Model name.\n\n    Raises:\n        ModuleNotFoundError: No matching config for the model name.\n\n    Returns:\n        Coqpit: config class.\n    \"\"\"\n    config_class = None\n    config_name = model_name + \"_config\"\n\n    # TODO: fix this\n    if model_name == \"xtts\":\n        from TTS.tts.configs.xtts_config import XttsConfig\n\n        config_class = XttsConfig\n    paths = [\"TTS.tts.configs\", \"TTS.vocoder.configs\", \"TTS.encoder.configs\", \"TTS.vc.configs\"]\n    for path in paths:\n        try:\n            config_class = find_module(path, config_name)\n        except ModuleNotFoundError:\n            pass\n    if config_class is None:\n        raise ModuleNotFoundError(f\" [!] Config for {model_name} cannot be found.\")\n    return config_class"
            }
        ],
        "third_party": [
            "splitext",
            "fsspec.open",
            "yaml.safe_load",
            "fsspec.open",
            "config_dict.update",
            "model_name.lower",
            "config_class",
            "config.from_dict"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def list_files(basePath, validExts=None, contains=None)",
        "start_line": "28",
        "end_line": "45",
        "file_path": "TTS/demos/xtts_ft_demo/utils/formatter.py",
        "docstring": "The function lists files in a directory structure based on specified criteria.\\nIt recursively walks through the base directory and processes each file.\\nIf a 'contains' string is provided, it skips files that do not contain this string in their names.\\nIt checks the file extension against valid extensions if provided.\\nFor valid files, it constructs and yields the full file path.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e0db68b63143",
        "ground_truth": "def list_files(basePath, validExts=None, contains=None):\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n             # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n             # check to see if the file is an audio and should be processed\n            if validExts is None or ext.endswith(validExts):\n                # construct the path to the audio and yield it\n                audioPath = os.path.join(rootDir, filename)\n                yield audioPath",
        "import_statements": [
            "import os",
            "import gc",
            "import torchaudio",
            "import pandas",
            "from faster_whisper import WhisperModel",
            "from glob import glob",
            "from tqdm import tqdm",
            "import torch",
            "import torchaudio",
            "from TTS.tts.layers.xtts.tokenizer import multilingual_cleaners",
            "import os"
        ],
        "reference_api": [
            "join",
            "filename.find",
            "os.walk",
            "ext.endswith",
            "lower",
            "filename.rfind"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "filename.find",
            "lower",
            "filename.rfind",
            "ext.endswith",
            "join"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab)",
        "start_line": "27",
        "end_line": "41",
        "file_path": "TTS/demos/xtts_ft_demo/xtts_demo.py",
        "docstring": "The function loads an XTTS model using specified checkpoint, config, and vocab paths.\\nIt first clears the GPU cache and checks if the required paths are provided, returning an error message if not.\\nIt then loads the configuration from the provided path, initializes the XTTS model, and loads the model checkpoint and vocabulary.\\nIf a GPU is available, it moves the model to the GPU.\\nThe function prints messages indicating the loading progress and returns a success message once the model is loaded.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b836fcaf0a27",
        "ground_truth": "def load_model(xtts_checkpoint, xtts_config, xtts_vocab):\n    global XTTS_MODEL\n    clear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"You need to run the previous steps or manually set the `XTTS checkpoint path`, `XTTS config path`, and `XTTS vocab path` fields !!\"\n    config = XttsConfig()\n    config.load_json(xtts_config)\n    XTTS_MODEL = Xtts.init_from_config(config)\n    print(\"Loading XTTS model! \")\n    XTTS_MODEL.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab, use_deepspeed=False)\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n     print(\"Model Loaded!\")\n    return \"Model Loaded!\"",
        "import_statements": [
            "import argparse",
            "import os",
            "import sys",
            "import tempfile",
            "import librosa.display",
            "import os",
            "import torch",
            "import torchaudio",
            "import traceback",
            "from TTS.demos.xtts_ft_demo.utils.formatter import format_audio_list",
            "from TTS.demos.xtts_ft_demo.utils.gpt_train import train_gpt",
            "from TTS.tts.configs.xtts_config import XttsConfig",
            "from TTS.tts.models.xtts import Xtts",
            "import logging"
        ],
        "reference_api": [
            "XTTS_MODEL.load_checkpoint",
            "print",
            "XttsConfig",
            "clear_gpu_cache",
            "Xtts.init_from_config",
            "config.load_json",
            "XTTS_MODEL.cuda",
            "is_available"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "clear_gpu_cache",
                "code": "def clear_gpu_cache():\n    # clear the GPU cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()"
            },
            {
                "name": "XTTS_MODEL.load_checkpoint",
                "code": "heckpoint(\n        self,\n        config,\n        checkpoint_dir=None,\n        checkpoint_path=None,\n        vocab_path=None,\n        eval=True,\n        strict=True,\n        use_deepspeed=False,\n        speaker_file_path=None,\n    ):\n        \"\"\"\n        Loads a checkpoint from disk and initializes the model's state and tokenizer.\n\n        Args:\n            config (dict): The configuration dictionary for the model.\n            checkpoint_dir (str, optional): The directory where the checkpoint is stored. Defaults to None.\n            checkpoint_path (str, optional): The path to the checkpoint file. Defaults to None.\n            vocab_path (str, optional): The path to the vocabulary file. Defaults to None.\n            eval (bool, optional): Whether to set the model to evaluation mode. Defaults to True.\n            strict (bool, optional): Whether to strictly enforce that the keys in the checkpoint match the keys in the model. Defaults to True.\n\n        Returns:\n            None\n        \"\"\"\n\n        model_path = checkpoint_path or os.path.join(checkpoint_dir, \"model.pth\")\n        vocab_path = vocab_path or os.path.join(checkpoint_dir, \"vocab.json\")\n\n        if speaker_file_path is None and checkpoint_dir is not None:\n            speaker_file_path = os.path.join(checkpoint_dir, \"speakers_xtts.pth\")\n\n        self.language_manager = LanguageManager(config)\n        self.speaker_manager = None\n        if speaker_file_path is not None and os.path.exists(speaker_file_path):\n            self.speaker_manager = SpeakerManager(speaker_file_path)\n\n        if os.path.exists(vocab_path):\n            self.tokenizer = VoiceBpeTokenizer(vocab_file=vocab_path)\n\n        self.init_models()\n\n        checkpoint = self.get_compatible_checkpoint_state_dict(model_path)\n\n        # deal with v1 and v1.1. V1 has the init_gpt_for_inference keys, v1.1 do not\n        try:\n            self.load_state_dict(checkpoint, strict=strict)\n        except:\n            if eval:\n                self.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cache)\n            self.load_state_dict(checkpoint, strict=strict)\n\n        if eval:\n            self.hifigan_decoder.eval()\n            self.gpt.init_gpt_for_inference(kv_cache=self.args.kv_cache, use_deepspeed=use_deepspeed)\n            self.gpt.eval()\n\n    def "
            }
        ],
        "third_party": [
            "XttsConfig",
            "config.load_json",
            "Xtts.init_from_config",
            "is_available",
            "XTTS_MODEL.cuda"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True)",
        "start_line": "68",
        "end_line": "96",
        "file_path": "TTS/encoder/models/base_encoder.py",
        "docstring": "The function computes embeddings for a given input x.\\nIt adjusts the number of frames based on the hop length if using torch specifications.\\nIt ensures the number of frames does not exceed the input length and calculates offset positions for evaluation.\\nIt extracts frames at these offsets, concatenates them into a batch, and performs inference to obtain embeddings.\\nIf specified, it computes the mean of the embeddings and returns the result, optionally normalizing them with L2 norm.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c5d63aa0df26",
        "ground_truth": "def compute_embedding(self, x, num_frames=250, num_eval=10, return_mean=True, l2_norm=True):\n    \"\"\"\n    Generate embeddings for a batch of utterances\n    x: 1xTxD\n    \"\"\"\n    # map to the waveform size\n    if self.use_torch_spec:\n        num_frames = num_frames * self.audio_config[\"hop_length\"]\n    max_len = x.shape[1]\n    if max_len < num_frames:\n        num_frames = max_len\n    offsets = np.linspace(0, max_len - num_frames, num=num_eval)\n    frames_batch = []\n    for offset in offsets:\n        offset = int(offset)\n        end_offset = int(offset + num_frames)\n        frames = x[:, offset:end_offset]\n        frames_batch.append(frames)\n    frames_batch = torch.cat(frames_batch, dim=0)\n    embeddings = self.inference(frames_batch, l2_norm=l2_norm)\n    if return_mean:\n        embeddings = torch.mean(embeddings, dim=0, keepdim=True)\n    return embeddings",
        "import_statements": [
            "import torch",
            "import torchaudio",
            "from coqpit import Coqpit",
            "from torch import nn",
            "from TTS.encoder.losses import AngleProtoLoss, GE2ELoss, SoftmaxAngleProtoLoss",
            "from TTS.utils.generic_utils import set_init_dict",
            "from TTS.utils.io import load_fsspec"
        ],
        "reference_api": [
            "torch.mean",
            "self.inference",
            "int",
            "torch.cat",
            "frames_batch.append",
            "np.linspace"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.inference",
                "code": "def inference(self, embedding):\n        x = self.fc(embedding)\n        activations = torch.nn.functional.softmax(x, dim=1).squeeze(0)\n        class_id = torch.argmax(activations)\n        return class_id"
            }
        ],
        "third_party": [
            "np.linspace",
            "frames_batch.append",
            "torch.cat",
            "torch.mean"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def create_layer(self, block, planes, blocks, stride=1)",
        "start_line": "131",
        "end_line": "145",
        "file_path": "TTS/encoder/models/resnet.py",
        "docstring": "The function creates a neural network layer consisting of a specified block repeated a given number of times.\\nIt initializes a downsample operation if needed based on the stride and input planes.\\nThe function appends the first block with the downsample to the layer list, updates the input planes, and then appends the remaining blocks.\\nFinally, it returns the constructed layer as an nn.Sequential object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "06f5b0335b26",
        "ground_truth": "def create_layer(self, block, planes, blocks, stride=1):\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n    layers = []\n    layers.append(block(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n        layers.append(block(self.inplanes, planes))\n    return nn.Sequential(*layers)",
        "import_statements": [
            "import torch",
            "from torch import nn",
            "from TTS.encoder.models.base_encoder import BaseEncoder"
        ],
        "reference_api": [
            "nn.Conv2d",
            "nn.Sequential",
            "layers.append",
            "nn.BatchNorm2d",
            "block",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "nn.Sequential",
            "nn.Conv2d",
            "nn.BatchNorm2d",
            "layers.append",
            "block",
            "layers.append",
            "block",
            "nn.Sequential"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def plot_embeddings(embeddings, num_classes_in_batch)",
        "start_line": "32",
        "end_line": "50",
        "file_path": "TTS/encoder/utils/visual.py",
        "docstring": "The function plots embeddings using UMAP for dimensionality reduction.\\nIt calculates the number of utterances per class and limits the number of classes to 10 if necessary.\\nUMAP transforms the embeddings, and a scatter plot is created with colors representing different classes.\\nThe plot is titled \"UMAP projection\", adjusted for equal aspect ratio, saved as \"umap\", and returned as a figure.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9ac60597b633",
        "ground_truth": "def plot_embeddings(embeddings, num_classes_in_batch):\n    num_utter_per_class = embeddings.shape[0] // num_classes_in_batch\n     # if necessary get just the first 10 classes\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n        embeddings = embeddings[: num_classes_in_batch * num_utter_per_class]\n     model = umap.UMAP()\n    projection = model.fit_transform(embeddings)\n    ground_truth = np.repeat(np.arange(num_classes_in_batch), num_utter_per_class)\n    colors = [colormap[i] for i in ground_truth]\n    fig, ax = plt.subplots(figsize=(16, 10))\n    _ = ax.scatter(projection[:, 0], projection[:, 1], c=colors)\n    plt.gca().set_aspect(\"equal\", \"datalim\")\n    plt.title(\"UMAP projection\")\n    plt.tight_layout()\n    plt.savefig(\"umap\")\n    return fig",
        "import_statements": [
            "import matplotlib",
            "import umap"
        ],
        "reference_api": [
            "plt.subplots",
            "model.fit_transform",
            "np.repeat",
            "plt.savefig",
            "plt.title",
            "umap.UMAP",
            "plt.tight_layout",
            "set_aspect",
            "np.arange",
            "ax.scatter",
            "plt.gca"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "umap.UMAP",
            "model.fit_transform",
            "np.repeat",
            "np.arange",
            "plt.subplots",
            "ax.scatter",
            "set_aspect",
            "plt.gca",
            "plt.title",
            "plt.tight_layout",
            "plt.savefig"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def calc_cosine_sim(self, dvecs, centroids)",
        "start_line": "50",
        "end_line": "72",
        "file_path": "TTS/encoder/losses.py",
        "docstring": "The function calculates a cosine similarity matrix for given d-vectors and centroids.\\nIt iterates over each speaker and their corresponding utterances, computing new centroids excluding the current utterance.\\nFor each utterance, it calculates the cosine similarity with the new centroids using vector-based operations for speed.\\nThe similarity values are clamped to avoid small values, concatenated, and stacked to form the final cosine similarity matrix.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "19c2b4af256d",
        "ground_truth": "def calc_cosine_sim(self, dvecs, centroids):\n    \"\"\"\n    Make the cosine similarity matrix with dims (N,M,N)\n    \"\"\"\n    cos_sim_matrix = []\n    for spkr_idx, speaker in enumerate(dvecs):\n        cs_row = []\n        for utt_idx, utterance in enumerate(speaker):\n            new_centroids = self.calc_new_centroids(dvecs, centroids, spkr_idx, utt_idx)\n            # vector based cosine similarity for speed\n            cs_row.append(\n                torch.clamp(\n                    torch.mm(\n                        utterance.unsqueeze(1).transpose(0, 1),\n                        new_centroids.transpose(0, 1),\n                    )\n                    / (torch.norm(utterance) * torch.norm(new_centroids, dim=1)),\n                    1e-6,\n                )\n            )\n        cs_row = torch.cat(cs_row, dim=0)\n        cos_sim_matrix.append(cs_row)\n    return torch.stack(cos_sim_matrix)",
        "import_statements": [
            "import torch",
            "from torch import nn"
        ],
        "reference_api": [
            "utterance.unsqueeze",
            "torch.mm",
            "cos_sim_matrix.append",
            "enumerate",
            "transpose",
            "torch.norm",
            "torch.cat",
            "self.calc_new_centroids",
            "new_centroids.transpose",
            "torch.clamp",
            "torch.stack",
            "cs_row.append"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.calc_new_centroids",
                "code": "def calc_new_centroids(self, dvecs, centroids, spkr, utt):\n        \"\"\"\n        Calculates the new centroids excluding the reference utterance\n        \"\"\"\n        excl = torch.cat((dvecs[spkr, :utt], dvecs[spkr, utt + 1 :]))\n        excl = torch.mean(excl, 0)\n        new_centroids = []\n        for i, centroid in enumerate(centroids):\n            if i == spkr:\n                new_centroids.append(excl)\n            else:\n                new_centroids.append(centroid)\n        return torch.stack(new_centroids)"
            }
        ],
        "third_party": [
            "cs_row.append",
            "torch.clamp",
            "torch.mm",
            "transpose",
            "utterance.unsqueeze",
            "new_centroids.transpose",
            "torch.norm",
            "torch.norm",
            "torch.cat",
            "cos_sim_matrix.append",
            "torch.stack"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": " def embed_loss_softmax(self, dvecs, cos_sim_matrix)",
        "start_line": "75",
        "end_line": "87",
        "file_path": "TTS/encoder/losses.py",
        "docstring": "The function calculates the embedding loss using softmax.\\nIt iterates over the embeddings, computing the negative log softmax of the cosine similarity matrix for each embedding.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c525b411a1e7",
        "ground_truth": "def embed_loss_softmax(self, dvecs, cos_sim_matrix):\n    \"\"\"\n    Calculates the loss on each embedding $L(e_{ji})$ by taking softmax\n    \"\"\"\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            L_row.append(-F.log_softmax(cos_sim_matrix[j, i], 0)[j])\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)",
        "import_statements": [
            "import torch",
            "from torch import nn"
        ],
        "reference_api": [
            "L.append",
            "L_row.append",
            "range",
            "torch.stack",
            "F.log_softmax"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "L_row.append",
            "F.log_softmax",
            "torch.stack",
            "L.append",
            "torch.stack"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def embed_loss_contrast(self, dvecs, cos_sim_matrix)",
        "start_line": "90",
        "end_line": "104",
        "file_path": "TTS/encoder/losses.py",
        "docstring": "The function calculates the embedding loss using contrast loss with the closest centroid.\\nIt iterates over the embeddings, computing the sigmoid of the cosine similarity matrix for each embedding.\\nFor each embedding, it excludes the current centroid, computes the contrast loss, and appends the result to the loss row.\\nThe losses for each row are stacked into tensors, and the function returns the final stacked tensor of losses.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "25ac3945da2e",
        "ground_truth": "def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n    \"\"\"\n    Calculates the loss on each embedding $L(e_{ji})$ by contrast loss with closest centroid\n    \"\"\"\n    N, M, _ = dvecs.shape\n    L = []\n    for j in range(N):\n        L_row = []\n        for i in range(M):\n            centroids_sigmoids = torch.sigmoid(cos_sim_matrix[j, i])\n            excl_centroids_sigmoids = torch.cat((centroids_sigmoids[:j], centroids_sigmoids[j + 1 :]))\n            L_row.append(1.0 - torch.sigmoid(cos_sim_matrix[j, i, j]) + torch.max(excl_centroids_sigmoids))\n        L_row = torch.stack(L_row)\n        L.append(L_row)\n    return torch.stack(L)",
        "import_statements": [
            "import torch",
            "from torch import nn"
        ],
        "reference_api": [
            "L.append",
            "torch.stack",
            "torch.cat",
            "L_row.append",
            "torch.sigmoid",
            "range",
            "torch.max"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "torch.sigmoid",
            "torch.cat",
            "L_row.append",
            "torch.sigmoid",
            "torch.max",
            "torch.stack",
            "L.append",
            "torch.stack"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def forward(self, x, _label=None)",
        "start_line": "106",
        "end_line": "118",
        "file_path": "TTS/encoder/losses.py",
        "docstring": "The function calculates the GE2E loss for input data with dimensions (num_speakers, num_utts_per_speaker, dvec_feats).\\nIt ensures the input has at least two utterances per speaker, computes centroids by averaging the embeddings, and calculates a cosine similarity matrix between the embeddings and centroids.\\nThe similarity matrix is scaled and shifted, then the embedding loss is computed and averaged.\\nThe function returns the mean loss.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "878d14ffabd2",
        "ground_truth": "def forward(self, x, _label=None):\n    \"\"\"\n    Calculates the GE2E loss for an input of dimensions (num_speakers, num_utts_per_speaker, dvec_feats)\n    \"\"\"\n    assert x.size()[1] >= 2\n    centroids = torch.mean(x, 1)\n    cos_sim_matrix = self.calc_cosine_sim(x, centroids)\n    torch.clamp(self.w, 1e-6)\n    cos_sim_matrix = self.w * cos_sim_matrix + self.b\n    L = self.embed_loss(x, cos_sim_matrix)\n    return L.mean()",
        "import_statements": [
            "import torch",
            "from torch import nn"
        ],
        "reference_api": [
            "torch.mean",
            "self.embed_loss",
            "x.size",
            "self.calc_cosine_sim",
            "torch.clamp",
            "L.mean"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.calc_cosine_sim",
                "code": "def calc_cosine_sim(self, dvecs, centroids):\n        \"\"\"\n        Make the cosine similarity matrix with dims (N,M,N)\n        \"\"\"\n        cos_sim_matrix = []\n        for spkr_idx, speaker in enumerate(dvecs):\n            cs_row = []\n            for utt_idx, utterance in enumerate(speaker):\n                new_centroids = self.calc_new_centroids(dvecs, centroids, spkr_idx, utt_idx)\n                # vector based cosine similarity for speed\n                cs_row.append(\n                    torch.clamp(\n                        torch.mm(\n                            utterance.unsqueeze(1).transpose(0, 1),\n                            new_centroids.transpose(0, 1),\n                        )\n                        / (torch.norm(utterance) * torch.norm(new_centroids, dim=1)),\n                        1e-6,\n                    )\n                )\n            cs_row = torch.cat(cs_row, dim=0)\n            cos_sim_matrix.append(cs_row)\n        return torch.stack(cos_sim_matrix)"
            }
        ],
        "third_party": [
            "x.size",
            "torch.mean",
            "torch.clamp",
            "self.embed_loss",
            "L.mean"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def get_voices(extra_voice_dirs: List[str] = [])",
        "start_line": "36",
        "end_line": "48",
        "file_path": "TTS/tts/layers/bark/inference_funcs.py",
        "docstring": "The function retrieves voice files from specified directories.\\nIt iterates through each directory and subdirectory, collecting .npz, .wav, and .mp3 files for each subdirectory.\\nIf no .npz files are found, it looks for .wav and .mp3 files.\\nThe function returns a dictionary where keys are subdirectory names and values are lists of file paths.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "86a364c72caa",
        "ground_truth": "def get_voices(extra_voice_dirs: List[str] = []):  # pylint: disable=dangerous-default-value\n    dirs = extra_voice_dirs\n    voices: Dict[str, List[str]] = {}\n    for d in dirs:\n        subs = os.listdir(d)\n        for sub in subs:\n            subj = os.path.join(d, sub)\n            if os.path.isdir(subj):\n                voices[sub] = list(glob(f\"{subj}/*.npz\"))\n                # fetch audio files if no npz files are found\n                if len(voices[sub]) == 0:\n                    voices[sub] = list(glob(f\"{subj}/*.wav\")) + list(glob(f\"{subj}/*.mp3\"))\n    return voices",
        "import_statements": [
            "import logging",
            "import os",
            "import re",
            "from glob import glob",
            "from typing import Dict, List",
            "import librosa",
            "import torch",
            "import torchaudio",
            "import tqdm",
            "from encodec.utils import convert_audio",
            "from scipy.special import softmax",
            "from torch.nn import functional as F",
            "from TTS.tts.layers.bark.hubert.hubert_manager import HubertManager",
            "from TTS.tts.layers.bark.hubert.kmeans_hubert import CustomHubert",
            "from TTS.tts.layers.bark.hubert.tokenizer import HubertTokenizer",
            "from TTS.tts.layers.bark.load_model import clear_cuda_cache, inference_mode"
        ],
        "reference_api": [
            "join",
            "list",
            "os.listdir",
            "glob",
            "len",
            "isdir"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "join",
            "isdir"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = [])",
        "start_line": "59",
        "end_line": "82",
        "file_path": "TTS/tts/layers/bark/inference_funcs.py",
        "docstring": "The function loads a voice for a given model from specified directories.\\nIf the voice is \"random\", it returns None for all values.\\nIt retrieves voice paths using get_voices and checks if there are multiple paths, raising an error if so.\\nIt verifies the existence of the voice and handles .npz files directly by loading them.\\nFor other audio files, it generates a .npz file and recursively loads the voice.\\nThe function returns the loaded voice data.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "02f20c8917e6",
        "ground_truth": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = []):  # pylint: disable=dangerous-default-value\n    if voice == \"random\":\n        return None, None, None\n     voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n     # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n     try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n     if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n     audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)",
        "import_statements": [
            "import logging",
            "import os",
            "import re",
            "from glob import glob",
            "from typing import Dict, List",
            "import librosa",
            "import torch",
            "import torchaudio",
            "import tqdm",
            "from encodec.utils import convert_audio",
            "from scipy.special import softmax",
            "from torch.nn import functional as F",
            "from TTS.tts.layers.bark.hubert.hubert_manager import HubertManager",
            "from TTS.tts.layers.bark.hubert.kmeans_hubert import CustomHubert",
            "from TTS.tts.layers.bark.hubert.tokenizer import HubertTokenizer",
            "from TTS.tts.layers.bark.load_model import clear_cuda_cache, inference_mode"
        ],
        "reference_api": [
            "get_voices",
            "ValueError",
            "endswith",
            "len",
            "KeyError",
            "load_voice",
            "generate_voice",
            "splitext",
            "load_npz"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "get_voices",
                "code": "def get_voices(extra_voice_dirs: List[str] = []):  # pylint: disable=dangerous-default-value\n    dirs = extra_voice_dirs\n    voices: Dict[str, List[str]] = {}\n    for d in dirs:\n        subs = os.listdir(d)\n        for sub in subs:\n            subj = os.path.join(d, sub)\n            if os.path.isdir(subj):\n                voices[sub] = list(glob(f\"{subj}/*.npz\"))\n                # fetch audio files if no npz files are found\n                if len(voices[sub]) == 0:\n                    voices[sub] = list(glob(f\"{subj}/*.wav\")) + list(glob(f\"{subj}/*.mp3\"))\n    return voices"
            },
            {
                "name": "load_npz",
                "code": "def load_npz(npz_file):\n    x_history = np.load(npz_file)\n    semantic = x_history[\"semantic_prompt\"]\n    coarse = x_history[\"coarse_prompt\"]\n    fine = x_history[\"fine_prompt\"]\n    return semantic, coarse, fine"
            },
            {
                "name": "generate_voice",
                "code": "def generate_voice(\n    audio,\n    model,\n    output_path,\n):\n    \"\"\"Generate a new voice from a given audio and text prompt.\n\n    Args:\n        audio (np.ndarray): The audio to use as a base for the new voice.\n        text (str): Transcription of the audio you are clonning.\n        model (BarkModel): The BarkModel to use for generating the new voice.\n        output_path (str): The path to save the generated voice to.\n    \"\"\"\n    if isinstance(audio, str):\n        audio, sr = torchaudio.load(audio)\n        audio = convert_audio(audio, sr, model.config.sample_rate, model.encodec.channels)\n        audio = audio.unsqueeze(0).to(model.device)\n\n    with torch.no_grad():\n        encoded_frames = model.encodec.encode(audio)\n    codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]\n\n    # move codes to cpu\n    codes = codes.cpu().numpy()\n\n    # generate semantic tokens\n    # Load the HuBERT model\n    hubert_manager = HubertManager()\n    # hubert_manager.make_sure_hubert_installed(model_path=model.config.LOCAL_MODEL_PATHS[\"hubert\"])\n    hubert_manager.make_sure_tokenizer_installed(model_path=model.config.LOCAL_MODEL_PATHS[\"hubert_tokenizer\"])\n\n    hubert_model = CustomHubert(checkpoint_path=model.config.LOCAL_MODEL_PATHS[\"hubert\"]).to(model.device)\n\n    # Load the CustomTokenizer model\n    tokenizer = HubertTokenizer.load_from_checkpoint(\n        model.config.LOCAL_MODEL_PATHS[\"hubert_tokenizer\"], map_location=model.device\n    )\n    # semantic_tokens = model.text_to_semantic(\n    #     text, max_gen_duration_s=seconds, top_k=50, top_p=0.95, temp=0.7\n    # )  # not 100%\n    semantic_vectors = hubert_model.forward(audio[0], input_sample_hz=model.config.sample_rate)\n    semantic_tokens = tokenizer.get_token(semantic_vectors)\n    semantic_tokens = semantic_tokens.cpu().numpy()\n\n    np.savez(output_path, fine_prompt=codes, coarse_prompt=codes[:2, :], semantic_prompt=semantic_tokens)"
            },
            {
                "name": "load_voice",
                "code": "def load_voice(model, voice: str, extra_voice_dirs: List[str] = []):  # pylint: disable=dangerous-default-value\n    if voice == \"random\":\n        return None, None, None\n\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n\n    # bark only uses a single sample for cloning\n    if len(paths) > 1:\n        raise ValueError(f\"Voice {voice} has multiple paths: {paths}\")\n\n    try:\n        path = voices[voice]\n    except KeyError as e:\n        raise KeyError(f\"Voice {voice} not found in {extra_voice_dirs}\") from e\n\n    if len(paths) == 1 and paths[0].endswith(\".npz\"):\n        return load_npz(path[0])\n\n    audio_path = paths[0]\n    # replace the file extension with .npz\n    output_path = os.path.splitext(audio_path)[0] + \".npz\"\n    generate_voice(audio=audio_path, model=model, output_path=output_path)\n    return load_voice(model, voice, extra_voice_dirs)"
            }
        ],
        "third_party": [
            "endswith",
            "splitext"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor",
        "start_line": "407",
        "end_line": "413",
        "file_path": "TTS/tts/layers/delightful_tts/conformer.py",
        "docstring": "The function performs a relative shift on a position score tensor.\\nIt first extracts the tensor's dimensions and creates a zero tensor with an additional dimension.\\nThis zero tensor is concatenated with the original position score tensor along the last dimension, then reshaped and sliced to achieve the relative shift.\\nThe function returns the shifted position score tensor.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9458e981389f",
        "ground_truth": "def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor:  # pylint: disable=no-self-use\n    batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n    zeros = torch.zeros((batch_size, num_heads, seq_length1, 1), device=pos_score.device)\n    padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n    padded_pos_score = padded_pos_score.view(batch_size, num_heads, seq_length2 + 1, seq_length1)\n    pos_score = padded_pos_score[:, :, 1:].view_as(pos_score)\n    return pos_score",
        "import_statements": [
            "import math",
            "from typing import Tuple",
            "import torch",
            "from TTS.tts.layers.delightful_tts.conv_layers import Conv1dGLU, DepthWiseConv1d, PointwiseConv1d",
            "from TTS.tts.layers.delightful_tts.networks import GLUActivation"
        ],
        "reference_api": [
            "torch.zeros",
            "torch.cat",
            "padded_pos_score.view",
            "view_as",
            "pos_score.size"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "pos_score.size",
            "torch.zeros",
            "torch.cat",
            "padded_pos_score.view",
            "view_as"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def run_padded_sequence(self, context, lens)",
        "start_line": "141",
        "end_line": "149",
        "file_path": "TTS/tts/layers/delightful_tts/conv_layers.py",
        "docstring": "The function processes a padded sequence through a series of convolutional layers with ReLU activation and dropout.\\nFor each batch element, it extracts the context up to the specified length, applies the convolutional layers, and stores the results.\\nThe processed contexts are then padded to create a consistent batch size, and the function returns the padded sequence.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c046d4f00e80",
        "ground_truth": "def run_padded_sequence(self, context, lens):\n    context_embedded = []\n    for b_ind in range(context.size()[0]):  # TODO: speed up\n        curr_context = context[b_ind : b_ind + 1, :, : lens[b_ind]].clone()\n        for conv in self.convolutions:\n            curr_context = self.dropout(F.relu(conv(curr_context)))\n        context_embedded.append(curr_context[0].transpose(0, 1))\n    context = nn.utils.rnn.pad_sequence(context_embedded, batch_first=True)\n    return context",
        "import_statements": [
            "from typing import Tuple",
            "import torch",
            "from torch.nn.utils import parametrize",
            "from TTS.tts.layers.delightful_tts.kernel_predictor import KernelPredictor"
        ],
        "reference_api": [
            "F.relu",
            "context.size",
            "context_embedded.append",
            "transpose",
            "pad_sequence",
            "self.dropout",
            "conv",
            "range",
            "clone"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "context.size",
            "clone",
            "self.dropout",
            "F.relu",
            "conv",
            "context_embedded.append",
            "transpose",
            "pad_sequence"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def run_unsorted_inputs(self, fn, context, lens)",
        "start_line": "151",
        "end_line": "165",
        "file_path": "TTS/tts/layers/delightful_tts/conv_layers.py",
        "docstring": "The function processes unsorted inputs by sorting them based on their lengths, running a given function on the sorted inputs, and then unsorting the results to match the original order.\\nIt sorts the lengths and indices, packs the context sequences, applies the function, unpacks the sequences, and finally maps the results back to the original indices.\\nThe function returns the context in its original order.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "db2644f3c681",
        "ground_truth": "def run_unsorted_inputs(self, fn, context, lens):  # pylint: disable=no-self-use\n    lens_sorted, ids_sorted = torch.sort(lens, descending=True)\n    unsort_ids = [0] * lens.size(0)\n    for i in range(len(ids_sorted)):  # pylint: disable=consider-using-enumerate\n        unsort_ids[ids_sorted[i]] = i\n    lens_sorted = lens_sorted.long().cpu()\n    context = context[ids_sorted]\n    context = nn.utils.rnn.pack_padded_sequence(context, lens_sorted, batch_first=True)\n    context = fn(context)[0]\n    context = nn.utils.rnn.pad_packed_sequence(context, batch_first=True)[0]\n    # map back to original indices\n    context = context[unsort_ids]\n    return context",
        "import_statements": [
            "from typing import Tuple",
            "import torch",
            "from torch.nn.utils import parametrize",
            "from TTS.tts.layers.delightful_tts.kernel_predictor import KernelPredictor"
        ],
        "reference_api": [
            "fn",
            "torch.sort",
            "cpu",
            "pad_packed_sequence",
            "pack_padded_sequence",
            "len",
            "lens.size",
            "lens_sorted.long",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "torch.sort",
            "lens.size",
            "cpu",
            "lens_sorted.long",
            "pack_padded_sequence",
            "fn",
            "pad_packed_sequence"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "coqui-ai/TTS",
        "function_declaration": "def remove_weight_norm(self)",
        "start_line": "667",
        "end_line": "671",
        "file_path": "TTS/tts/layers/delightful_tts/conv_layers.py",
        "docstring": "The function removes weight normalization from specific layers in a neural network.\\nIt calls the remove_weight_norm method on the kernel_predictor, removes parameterizations from the convt_pre layer, and iterates through the conv_blocks to remove parameterizations from each block's weight.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ced082bffc63",
        "ground_truth": "def remove_weight_norm(self):\n    self.kernel_predictor.remove_weight_norm()\n    parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n    for block in self.conv_blocks:\n        parametrize.remove_parametrizations(block[1], \"weight\")",
        "import_statements": [
            "from typing import Tuple",
            "import torch",
            "from torch.nn.utils import parametrize",
            "from TTS.tts.layers.delightful_tts.kernel_predictor import KernelPredictor"
        ],
        "reference_api": [
            "parametrize.remove_parametrizations",
            "remove_weight_norm"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "remove_weight_norm",
                "code": " remove_weight_norm(self):\n        self.kernel_predictor.remove_weight_norm()\n        parametrize.remove_parametrizations(self.convt_pre[1], \"weight\")\n        for block in self.conv_blocks:\n            parametrize.remove_parametrizations(block[1], \"weight\")\n"
            }
        ],
        "third_party": [
            "parametrize.remove_parametrizations",
            "parametrize.remove_parametrizations"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def map_mx_to_torch_model(mx_name)",
        "start_line": "74",
        "end_line": "83",
        "file_path": "convert/convert_from_mxnet.py",
        "docstring": "The function maps a model name from MXNet to its corresponding PyTorch model name.\\nIt converts the input name to lowercase and applies specific replacements for 'se_', 'senet_', and 'inceptionv3' prefixes.\\nFinally, it prefixes the name with 'gluon_' and returns the modified name.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b5a39fd97061",
        "ground_truth": "def map_mx_to_torch_model(mx_name):\n    torch_name = mx_name.lower()\n    if torch_name.startswith('se_'):\n        torch_name = torch_name.replace('se_', 'se')\n    elif torch_name.startswith('senet_'):\n        torch_name = torch_name.replace('senet_', 'senet')\n    elif torch_name.startswith('inceptionv3'):\n        torch_name = torch_name.replace('inceptionv3', 'inception_v3')\n    torch_name = 'gluon_' + torch_name\n    return torch_name",
        "import_statements": [
            "import argparse",
            "import hashlib",
            "import os",
            "import gluoncv",
            "import torch",
            "from timm import create_model"
        ],
        "reference_api": [
            "torch_name.replace",
            "mx_name.lower",
            "torch_name.startswith"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "mx_name.lower",
            "torch_name.startswith",
            "torch_name.replace",
            "torch_name.startswith",
            "torch_name.replace",
            "torch_name.startswith",
            "torch_name.replace"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def _num_samples_per_worker(self)",
        "start_line": "154",
        "end_line": "161",
        "file_path": "timm/data/readers/reader_hfids.py",
        "docstring": "The function calculates the number of samples to be processed by each worker.\\nIt computes the initial number of samples per worker based on the repeats, total samples, and the number of workers or replicas.\\nIf training or multiple replicas are used, it rounds up the number of samples.\\nIf a batch size is specified during training, it adjusts the number of samples to be a multiple of the batch size.\\nFinally, it returns the calculated number of samples as an integer.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7eee5efdbf67",
        "ground_truth": "def _num_samples_per_worker(self):\n    num_worker_samples = \\\n        max(1, self.repeats) * self.num_samples / max(self.global_num_workers, self.dist_num_replicas)\n    if self.is_training or self.dist_num_replicas > 1:\n        num_worker_samples = math.ceil(num_worker_samples)\n    if self.is_training and self.batch_size is not None:\n        num_worker_samples = math.ceil(num_worker_samples / self.batch_size) * self.batch_size\n    return int(num_worker_samples)",
        "import_statements": [
            "import math",
            "import os",
            "from itertools import repeat, chain",
            "from typing import Optional",
            "import torch",
            "from PIL import Image"
        ],
        "reference_api": [
            "int",
            "math.ceil",
            "max"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n)",
        "start_line": "18",
        "end_line": "56",
        "file_path": "timm/data/readers/reader_image_folder.py",
        "docstring": "The function searches for images and their corresponding class labels in a given folder.\\nIt optionally filters by file types and builds a list of filenames and labels based on the folder structure.\\nIf no class-to-index mapping is provided, it creates one from the found labels.\\nIt then pairs each image file with its class index, optionally sorting the results, and returns the list of image-path and class-index pairs along with the class-to-index mapping.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a698d65ae190",
        "ground_truth": "def find_images_and_targets(\n        folder: str,\n        types: Optional[Union[List, Tuple, Set]] = None,\n        class_to_idx: Optional[Dict] = None,\n        leaf_name_only: bool = True,\n        sort: bool = True\n):\n    \"\"\" Walk folder recursively to discover images and map them to classes by folder names.\n     Args:\n        folder: root of folder to recrusively search\n        types: types (file extensions) to search for in path\n        class_to_idx: specify mapping for class (folder name) to class index if set\n        leaf_name_only: use only leaf-name of folder walk for class names\n        sort: re-sort found images by name (for consistent ordering)\n     Returns:\n        A list of image and target tuples, class_to_idx mapping\n    \"\"\"\n    types = get_img_extensions(as_set=True) if not types else set(types)\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder, topdown=False, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    if class_to_idx is None:\n        # building class index\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    images_and_targets = [(f, class_to_idx[l]) for f, l in zip(filenames, labels) if l in class_to_idx]\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    return images_and_targets, class_to_idx",
        "import_statements": [
            "import os",
            "from typing import Dict, List, Optional, Set, Tuple, Union",
            "from timm.utils.misc import natural_key"
        ],
        "reference_api": [
            "join",
            "filenames.append",
            "sorted",
            "list",
            "get_img_extensions",
            "ext.lower",
            "os.walk",
            "splitext",
            "set",
            "natural_key",
            "rel_path.replace",
            "relpath",
            "basename",
            "zip",
            "enumerate",
            "labels.append"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "natural_key",
                "code": "def natural_key(string_):\n    \"\"\"See http://www.codinghorror.com/blog/archives/001018.html\"\"\"\n    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_.lower())]"
            }
        ],
        "third_party": [
            "get_img_extensions",
            "relpath",
            "basename",
            "rel_path.replace",
            "splitext",
            "ext.lower",
            "filenames.append",
            "join",
            "labels.append"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def pytorch_worker_seed()",
        "start_line": "171",
        "end_line": "178",
        "file_path": "timm/data/readers/reader_wds.py",
        "docstring": "The function retrieves the seed for a PyTorch dataloader worker.\\nIt first attempts to get the seed from the PyTorch worker info.\\nIf the worker info is not available, it falls back to a seed based on the wds rank.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e7fe43bdecdc",
        "ground_truth": "def pytorch_worker_seed():\n    \"\"\"get dataloader worker seed from pytorch\"\"\"\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()",
        "import_statements": [
            "import io",
            "import json",
            "import logging",
            "import math",
            "import os",
            "import random",
            "import sys",
            "from dataclasses import dataclass",
            "from functools import partial",
            "from itertools import islice",
            "from typing import Any, Callable, Dict, List, Optional, Tuple",
            "import torch",
            "import yaml",
            "from PIL import Image",
            "from torch.utils.data import Dataset, IterableDataset, get_worker_info"
        ],
        "reference_api": [
            "get_worker_info",
            "pytorch_worker_seed"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "pytorch_worker_seed",
                "code": "def pytorch_worker_seed():\n    \"\"\"get dataloader worker seed from pytorch\"\"\"\n    worker_info = get_worker_info()\n    if worker_info is not None:\n        # favour the seed already created for pytorch dataloader workers if it exists\n        return worker_info.seed\n    # fallback to wds rank based seed\n    return wds.utils.pytorch_worker_seed()"
            }
        ],
        "third_party": [
            "get_worker_info"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def solarize_add(img, add, thresh=128, **__)",
        "start_line": "145",
        "end_line": "158",
        "file_path": "timm/data/auto_augment.py",
        "docstring": "The function applies a solarize add effect to an image.\\nIt creates a lookup table (LUT) that adds a specified value to pixel values below a threshold and keeps other pixels unchanged.\\nIf the image mode is \"L\" or \"RGB\", it applies the LUT to the image.\\nIt returns the modified image or the original image if the mode is not supported.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0ffc7e7ded25",
        "ground_truth": "def solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n     if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n     return img",
        "import_statements": [
            "import random",
            "import math",
            "import re",
            "from functools import partial",
            "from typing import Dict, List, Optional, Union",
            "from PIL import Image, ImageOps, ImageEnhance, ImageChops, ImageFilter",
            "import PIL"
        ],
        "reference_api": [
            "lut.append",
            "min",
            "img.point",
            "len",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "lut.append",
            "lut.append",
            "img.point"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def _get_weighted_transforms(transforms: Dict)",
        "start_line": "707",
        "end_line": "711",
        "file_path": "timm/data/auto_augment.py",
        "docstring": "The function processes a dictionary of transforms and their probabilities.\\nIt extracts the transform names and their corresponding probabilities, normalizes the probabilities so they sum to one, and returns the transforms and normalized probabilities as separate tuples.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5a0d29577a21",
        "ground_truth": "def _get_weighted_transforms(transforms: Dict):\n    transforms, probs = list(zip(*transforms.items()))\n    probs = np.array(probs)\n    probs = probs / np.sum(probs)\n    return transforms, probs",
        "import_statements": [
            "import random",
            "import math",
            "import re",
            "from functools import partial",
            "from typing import Dict, List, Optional, Union",
            "from PIL import Image, ImageOps, ImageEnhance, ImageChops, ImageFilter",
            "import PIL"
        ],
        "reference_api": [
            "list",
            "np.sum",
            "zip",
            "transforms.items",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "transforms.items",
            "np.array",
            "np.sum"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def _apply_basic(self, img, mixing_weights, m)",
        "start_line": "914",
        "end_line": "929",
        "file_path": "timm/data/auto_augment.py",
        "docstring": "The function applies a series of augmentations to an image and blends the results.\\nIt initializes an empty array for the mixed image and iterates through given mixing weights.\\nFor each weight, it applies a random sequence of operations to the image, accumulating the results.\\nThe mixed image is then clipped to valid pixel values, converted back to an image, and blended with the original image using a specified blending factor.\\nThe final blended image is returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "84bc5658c135",
        "ground_truth": "def _apply_basic(self, img, mixing_weights, m):\n    # This is a literal adaptation of the paper/official implementation without normalizations and\n    # PIL <-> Numpy conversions between every op. It is still quite CPU compute heavy compared to the\n    # typical augmentation transforms, could use a GPU / Kornia implementation.\n    img_shape = img.size[0], img.size[1], len(img.getbands())\n    mixed = np.zeros(img_shape, dtype=np.float32)\n    for mw in mixing_weights:\n        depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n        ops = np.random.choice(self.ops, depth, replace=True)\n        img_aug = img  # no ops are in-place, deep copy not necessary\n        for op in ops:\n            img_aug = op(img_aug)\n        mixed += mw * np.asarray(img_aug, dtype=np.float32)\n    np.clip(mixed, 0, 255., out=mixed)\n    mixed = Image.fromarray(mixed.astype(np.uint8))\n    return Image.blend(img, mixed, m)",
        "import_statements": [
            "import random",
            "import math",
            "import re",
            "from functools import partial",
            "from typing import Dict, List, Optional, Union",
            "from PIL import Image, ImageOps, ImageEnhance, ImageChops, ImageFilter",
            "import PIL"
        ],
        "reference_api": [
            "img.getbands",
            "choice",
            "np.asarray",
            "len",
            "op",
            "np.clip",
            "np.zeros",
            "mixed.astype",
            "Image.blend",
            "randint",
            "Image.fromarray"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "img.getbands",
            "np.zeros",
            "randint",
            "choice",
            "op",
            "np.asarray",
            "np.clip",
            "Image.fromarray",
            "mixed.astype",
            "Image.blend"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def adapt_to_chs(x, n)",
        "start_line": "64",
        "end_line": "73",
        "file_path": "timm/data/loader.py",
        "docstring": "The function adapts input x to match the required number of channels n.\\nIf x is not a tuple or list, it repeats x n times to form a tuple.\\nIf x is a tuple or list but its length is not equal to n, it calculates the mean of x and repeats it n times, logging a warning.\\nIf x is already a tuple or list of length n, it asserts the length matches n.\\nThe function returns the adapted tuple x.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "84b5b752ff04",
        "ground_truth": "def adapt_to_chs(x, n):\n    if not isinstance(x, (tuple, list)):\n        x = tuple(repeat(x, n))\n    elif len(x) != n:\n        x_mean = np.mean(x).item()\n        x = (x_mean,) * n\n        _logger.warning(f'Pretrained mean/std different shape than model, using avg value {x}.')\n    else:\n        assert len(x) == n, 'normalization stats must match image channels'\n    return x",
        "import_statements": [
            "import logging",
            "import random",
            "from contextlib import suppress",
            "from functools import partial",
            "from itertools import repeat",
            "from typing import Callable, Optional, Tuple, Union",
            "import torch",
            "import torch.utils.data"
        ],
        "reference_api": [
            "tuple",
            "item",
            "len",
            "isinstance",
            "_logger.warning",
            "np.mean",
            "repeat"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "repeat",
            "item",
            "np.mean",
            "_logger.warning"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def _worker_init(worker_id, worker_seeding='all')",
        "start_line": "173",
        "end_line": "186",
        "file_path": "timm/data/loader.py",
        "docstring": "The function initializes a worker with a specified seed for reproducibility.\\nIt retrieves the worker information and verifies the worker ID.\\nIf a callable worker_seeding is provided, it generates and sets the seed for random, torch, and numpy.\\nIf worker_seeding is a string, it sets the numpy seed based on the worker_info seed for 'all' seeding.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "54754b6ba394",
        "ground_truth": "def _worker_init(worker_id, worker_seeding='all'):\n    worker_info = torch.utils.data.get_worker_info()\n    assert worker_info.id == worker_id\n    if isinstance(worker_seeding, Callable):\n        seed = worker_seeding(worker_info)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed % (2 ** 32 - 1))\n    else:\n        assert worker_seeding in ('all', 'part')\n        # random / torch seed already called in dataloader iter class w/ worker_info.seed\n        # to reproduce some old results (same seed + hparam combo), partial seeding is required (skip numpy re-seed)\n        if worker_seeding == 'all':\n            np.random.seed(worker_info.seed % (2 ** 32 - 1))",
        "import_statements": [
            "import logging",
            "import random",
            "from contextlib import suppress",
            "from functools import partial",
            "from itertools import repeat",
            "from typing import Callable, Optional, Tuple, Union",
            "import torch",
            "import torch.utils.data"
        ],
        "reference_api": [
            "get_worker_info",
            "random.seed",
            "worker_seeding",
            "isinstance",
            "seed",
            "torch.manual_seed"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get_worker_info",
            "worker_seeding",
            "torch.manual_seed",
            "seed",
            "seed"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def _decode_and_center_crop(image_bytes, image_size, resize_method)",
        "start_line": "114",
        "end_line": "132",
        "file_path": "timm/data/tf_preprocessing.py",
        "docstring": "The function decodes a JPEG image, crops it to the center with padding, and resizes it to a specified size.\\nIt first extracts the image shape and calculates the padded center crop size based on the given image size and a padding constant.\\nIt determines the offset for the crop window, decodes and crops the image, and then resizes the cropped image using the specified resize method.\\nThe processed image is returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "81dfdce940db",
        "ground_truth": "def _decode_and_center_crop(image_bytes, image_size, resize_method):\n    \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n     padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n     offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n     return image",
        "import_statements": [],
        "reference_api": [
            "decode_and_crop_jpeg",
            "tf.stack",
            "resize",
            "tf.minimum",
            "tf.cast",
            "extract_jpeg_shape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "extract_jpeg_shape",
            "tf.cast",
            "tf.cast",
            "tf.minimum",
            "tf.stack",
            "decode_and_crop_jpeg",
            "resize"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')",
        "start_line": "141",
        "end_line": "159",
        "file_path": "timm/data/tf_preprocessing.py",
        "docstring": "The function preprocesses image bytes for training.\\nIt decodes and randomly crops the image using a specified interpolation method, flips the image, and reshapes it to the target size.\\nThe image is then converted to the specified dtype, either bfloat16 or float32, and returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cf16a6d37111",
        "ground_truth": "def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic'):\n    \"\"\"Preprocesses the given image for evaluation.\n     Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n     Returns:\n      A preprocessed image `Tensor`.\n    \"\"\"\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image",
        "import_statements": [],
        "reference_api": [
            "convert_image_dtype",
            "tf.reshape",
            "_decode_and_random_crop",
            "_flip"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_decode_and_random_crop",
                "code": "def _decode_and_random_crop(image_bytes, image_size, resize_method):\n    \"\"\"Make a random crop of image_size.\"\"\"\n    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n    image = distorted_bounding_box_crop(\n        image_bytes,\n        bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=(3. / 4, 4. / 3.),\n        area_range=(0.08, 1.0),\n        max_attempts=10,\n        scope=None)\n    original_shape = tf.image.extract_jpeg_shape(image_bytes)\n    bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n    image = tf.cond(\n        bad,\n        lambda: _decode_and_center_crop(image_bytes, image_size),\n        lambda: tf.image.resize([image], [image_size, image_size], resize_method)[0])\n\n    return image"
            },
            {
                "name": "_flip",
                "code": "def _flip(image):\n    \"\"\"Random horizontal image flip.\"\"\"\n    image = tf.image.random_flip_left_right(image)\n    return image"
            }
        ],
        "third_party": [
            "tf.reshape",
            "convert_image_dtype"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "huggingface/pytorch-image-models",
        "function_declaration": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic')",
        "start_line": "162",
        "end_line": "179",
        "file_path": "timm/data/tf_preprocessing.py",
        "docstring": "The function preprocesses an image for evaluation by decoding and centering the crop, resizing it, and converting its data type.\\nIt selects the resize method based on the interpolation parameter, decodes and centers the image crop, reshapes it to the specified size, and converts the image data type to either bfloat16 or float32 based on the use_bfloat16 flag.\\nThe preprocessed image is then returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "78b2a3bd2142",
        "ground_truth": "def preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic'):\n    \"\"\"Preprocesses the given image for evaluation.\n     Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n     Returns:\n      A preprocessed image `Tensor`.\n    \"\"\"\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image",
        "import_statements": [],
        "reference_api": [
            "tf.reshape",
            "_decode_and_center_crop",
            "convert_image_dtype"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "_decode_and_center_crop",
                "code": "def _decode_and_center_crop(image_bytes, image_size, resize_method):\n    \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n\n    return image"
            }
        ],
        "third_party": [
            "tf.reshape",
            "convert_image_dtype"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "facebookresearch/fairseq",
        "function_declaration": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\")",
        "start_line": "48",
        "end_line": "54",
        "file_path": "examples/MMPT/mmpt/evaluators/evaluator.py",
        "docstring": "The function evaluates a model using a specified or default evaluation dataloader.\\nIt runs a prediction loop with the model and dataloader, saving the outputs to a file.\\nIt then computes evaluation metrics based on the predictions and returns the results.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8bd4364a9a0e",
        "ground_truth": "def evaluate(self, model, eval_dataloader=None, output_file=\"merged\"):\n    if eval_dataloader is None:\n        eval_dataloader = self.eval_dataloader\n    outputs = self.predictor.predict_loop(\n        model, eval_dataloader, output_file)\n    results = self.metric.compute_metrics(**outputs)\n    return results",
        "import_statements": [
            "import os",
            "import glob"
        ],
        "reference_api": [
            "compute_metrics",
            "predict_loop"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "predict_loop",
            "compute_metrics"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "facebookresearch/fairseq",
        "function_declaration": " def print_computed_metrics(self, metrics)",
        "start_line": "95",
        "end_line": "107",
        "file_path": "examples/MMPT/mmpt/evaluators/metric.py",
        "docstring": "The function prints the computed metrics for rank1, rank5, and miou.\\nIt retrieves these values from the provided metrics dictionary and prints them formatted to four decimal places.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "22d682c958c0",
        "ground_truth": "def print_computed_metrics(self, metrics):\n    rank1 = metrics[\"rank1\"]\n    rank5 = metrics[\"rank5\"]\n    miou = metrics[\"miou\"]\n    # print(\"Average rank@1: %f\" % rank1)\n    # print(\"Average rank@5: %f\" % rank5)\n    # print(\"Average iou: %f\" % miou)\n    print(\n        \"Average rank@1: {:.4f} Average rank@5: {:.4f} Average iou: {:.4f}\".format(\n            rank1, rank5, miou\n        )\n    )",
        "import_statements": [
            "import json"
        ],
        "reference_api": [
            "print",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "facebookresearch/fairseq",
        "function_declaration": "def _eval_predictions(self, segments, data)",
        "start_line": "117",
        "end_line": "142",
        "file_path": "examples/MMPT/mmpt/evaluators/metric.py",
        "docstring": "The function evaluates prediction segments against ground truth data.\\nIt computes average Intersection over Union (IoU) and rank for the top predictions.\\nFor each segment-data pair, it calculates IoUs and ranks, averaging the top values for each.\\nIt then calculates the percentage of segments with rank 1 and rank 5, and the mean IoU across all segments.\\nThe function returns rank1, rank5, and mean IoU.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "39c1a50b7e7d",
        "ground_truth": "def _eval_predictions(self, segments, data):\n    '''\n    Inputs:\n    segments: For each item in the ground truth data, rank possible video segments given the description and video.\n        In DiDeMo, there are 21 posible moments extracted for each video so the list of video segments will be of length 21.\n        The first video segment should be the video segment that best corresponds to the text query.\n        There are 4180 sentence in the validation data, so when evaluating a model on the val dataset,\n        segments should be a list of lenght 4180, and each item in segments should be a list of length 21.\n    data: ground truth data\n    '''\n    average_ranks = []\n    average_iou = []\n    for s, d in zip(segments, data):\n        pred = s[0]\n        ious = [self._iou(pred, t) for t in d['times']]\n        average_iou.append(np.mean(np.sort(ious)[-3:]))\n        ranks = [self._rank(s, t) for t in d['times'] if tuple(t) in s]  # if t in s] is added for s, e not in prediction.\n        average_ranks.append(np.mean(np.sort(ranks)[:3]))\n    rank1 = np.sum(np.array(average_ranks) <= 1)/float(len(average_ranks))\n    rank5 = np.sum(np.array(average_ranks) <= 5)/float(len(average_ranks))\n    miou = np.mean(average_iou)\n    # print(\"Average rank@1: %f\" % rank1)\n    # print(\"Average rank@5: %f\" % rank5)\n    # print(\"Average iou: %f\" % miou)\n    return rank1, rank5, miou",
        "import_statements": [
            "import json"
        ],
        "reference_api": [
            "average_ranks.append",
            "tuple",
            "float",
            "self._rank",
            "np.sum",
            "len",
            "np.sort",
            "average_iou.append",
            "self._iou",
            "np.mean",
            "zip",
            "np.array"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._iou",
                "code": "def _iou(self, pred, gt):\n        intersection = max(0, min(pred[1], gt[1]) + 1 - max(pred[0], gt[0]))\n        union = max(pred[1], gt[1]) + 1 - min(pred[0], gt[0])\n        return float(intersection)/union"
            },
            {
                "name": "self._rank",
                "code": "def _rank(self, pred, gt):\n        return pred.index(tuple(gt)) + 1"
            }
        ],
        "third_party": [
            "average_iou.append",
            "np.mean",
            "np.sort",
            "average_ranks.append",
            "np.mean",
            "np.sort",
            "np.sum",
            "np.array",
            "np.sum",
            "np.array",
            "np.mean"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "facebookresearch/fairseq",
        "function_declaration": "def predict_loop(self, model, eval_dataloader, output_file=None)",
        "start_line": "31",
        "end_line": "42",
        "file_path": "examples/MMPT/mmpt/evaluators/predictor.py",
        "docstring": "The function performs on-the-fly predictions using a given model and evaluation dataloader on a single GPU.\\nIt initializes an empty list for scores, sets the model to evaluation mode, and transfers it to GPU 0.\\nIt iterates over the dataloader, transferring data to the appropriate context, and makes predictions without computing gradients.\\nThe outputs are updated with the input data and processed further.\\nFinally, it finalizes and returns the results, optionally saving them to an output file.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8ccf84f8d034",
        "ground_truth": "def predict_loop(self, model, eval_dataloader, output_file=None):\n    \"\"\"on-the-fly prediction on a single gpu.\"\"\"\n    self.full_scores = []\n    model.eval()\n    model = model.to(0)\n    with torch.no_grad():\n        for data in eval_dataloader:\n            data = self.to_ctx(data)\n            outputs = model(**data)\n            outputs.update(data)\n            self(outputs)\n    return self.finalize(output_file)",
        "import_statements": [
            "import os",
            "import random",
            "import json",
            "import torch",
            "import pickle",
            "import math",
            "from tqdm import tqdm"
        ],
        "reference_api": [
            "model.eval",
            "self.finalize",
            "self.to_ctx",
            "model",
            "model.to",
            "self",
            "torch.no_grad",
            "outputs.update"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.to_ctx",
                "code": "def to_ctx(self, data, ctx=0, dtype=None):\n        if isinstance(data, dict):\n            for key in data:\n                if torch.is_tensor(data[key]):\n                    if dtype is not None and data[key].dtype == torch.float32:\n                        data[key] = data[key].to(dtype)\n                    data[key] = data[key].to(ctx)\n            return data\n        else:\n            raise ValueError(\"non-dict type of batch is not supported yet.\")"
            },
            {
                "name": "self.finalize",
                "code": "def finalize(self, output_file):\n        pass"
            }
        ],
        "third_party": [
            "model.eval",
            "model.to",
            "torch.no_grad",
            "model",
            "outputs.update",
            "self"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "facebookresearch/fairseq",
        "function_declaration": "def _aggregate_scores(self, scores)",
        "start_line": "177",
        "end_line": "183",
        "file_path": "examples/MMPT/mmpt/evaluators/predictor.py",
        "docstring": "The function aggregates scores by concatenating video and text hidden representations from the input scores.\\nIt asserts that there are exactly two sets of scores, concatenates each set along the first axis, clears the full_scores attribute, and returns the dot product of the text and video hidden representations.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e68529ad727d",
        "ground_truth": "def _aggregate_scores(self, scores):\n    assert len(scores) == 2\n    video_hidden = np.concatenate(scores[0], axis=0)\n    text_hidden = np.concatenate(scores[1], axis=0)\n    # clear up.\n    self.full_scores = []\n    return np.matmul(text_hidden, video_hidden.T)",
        "import_statements": [
            "import os",
            "import random",
            "import json",
            "import torch",
            "import pickle",
            "import math",
            "from tqdm import tqdm"
        ],
        "reference_api": [
            "np.concatenate",
            "len",
            "np.matmul"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.concatenate",
            "np.concatenate",
            "np.matmul"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "facebookresearch/fairseq",
        "function_declaration": "def finalize(self, Y_pred, Y_true, output_file=None)",
        "start_line": "410",
        "end_line": "426",
        "file_path": "examples/MMPT/mmpt/evaluators/predictor.py",
        "docstring": "The function finalizes predictions by concatenating and converting predicted and true labels to numpy arrays.\\nIt checks for prediction errors and prints samples of these errors.\\nIf an output file is specified, it saves the predictions and true labels as a pickle file.\\nThe function returns a dictionary containing the predicted and true labels.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e4e1861fc357",
        "ground_truth": "def finalize(self, Y_pred, Y_true, output_file=None):\n    Y_pred = torch.cat(Y_pred, dim=0).numpy()\n    Y_true = torch.cat(Y_true, dim=0).numpy()\n    assert len(Y_pred) == len(Y_true)\n    error_mask = Y_pred != Y_true\n    print(\"sample error\", Y_pred[error_mask][:10], Y_true[error_mask][:10])\n    print(\"sample error\", Y_pred[error_mask][10:20], Y_true[error_mask][10:20])\n    if output_file is not None:\n        with open(\n                os.path.join(self.pred_dir, output_file + \".pkl\"),\n                \"wb\") as fw:\n            pickle.dump(\n                {\"Y_pred\": Y_pred, \"Y_true\": Y_true}, fw,\n                protocol=pickle.HIGHEST_PROTOCOL)\n    return {\"outputs\": Y_pred, \"targets\": Y_true}",
        "import_statements": [
            "import os",
            "import random",
            "import json",
            "import torch",
            "import pickle",
            "import math",
            "from tqdm import tqdm"
        ],
        "reference_api": [
            "join",
            "print",
            "len",
            "torch.cat",
            "pickle.dump",
            "numpy",
            "open"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "numpy",
            "torch.cat",
            "numpy",
            "torch.cat",
            "join"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "facebookresearch/fairseq",
        "function_declaration": "def reduce_metrics(logging_outputs) -> None",
        "start_line": "48",
        "end_line": "54",
        "file_path": "examples/MMPT/mmpt/losses/fairseqmmloss.py",
        "docstring": "The function reduces metrics by calculating the average loss from logging outputs.\\nIt sums the loss values and sample sizes from the logs, then logs the average loss as a scalar with three decimal places.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7df9ab9a4920",
        "ground_truth": "def reduce_metrics(logging_outputs) -> None:\n    \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n    \"\"\"since we use NCE, our actual batch_size is 1 per GPU.\n    Then we take the mean of each worker.\"\"\"\n    loss_sum = sum(log.get(\"loss\", 0.0) for log in logging_outputs)\n    sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n    metrics.log_scalar(\"loss\", loss_sum / sample_size, round=3)",
        "import_statements": [
            "from fairseq.criterions import FairseqCriterion, register_criterion",
            "from fairseq.logging import metrics"
        ],
        "reference_api": [
            "sum",
            "metrics.log_scalar",
            "log.get"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "log.get",
            "log.get",
            "metrics.log_scalar"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "facebookresearch/fairseq",
        "function_declaration": "def _mm_on_the_fly(\n        self,\n        cmasks,\n        vmasks,\n        attention_mask\n    )",
        "start_line": "149",
        "end_line": "178",
        "file_path": "examples/MMPT/mmpt/models/mmfusion.py",
        "docstring": "The function generates an attention mask and token type IDs for multimodal inputs.\\nIf an attention mask is not provided, it creates one using cmasks and vmasks.\\nIt then constructs token type IDs by concatenating zeros for visual tokens and ones for text tokens.\\nThe function returns the attention mask and token type IDs.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7f9c326a96f7",
        "ground_truth": "def _mm_on_the_fly(\n    self,\n    cmasks,\n    vmasks,\n    attention_mask\n):\n    \"\"\"helper function for mask, seg_ids and token_type_ids.\"\"\"\n    if attention_mask is None:\n        attention_mask = self._mm_attention_mask(cmasks, vmasks)\n    \"\"\"\n    0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n    | first sequence    | second sequence |\n    \"\"\"\n    token_type_ids = torch.cat(\n        [\n            torch.zeros(\n                (vmasks.size(0), vmasks.size(1) + 2),\n                dtype=torch.long,\n                device=vmasks.device,\n            ),\n            torch.ones(\n                (cmasks.size(0), cmasks.size(1) - 2),\n                dtype=torch.long,\n                device=cmasks.device,\n            ),\n        ],\n        dim=1,\n    )\n    return attention_mask, token_type_ids",
        "import_statements": [
            "import torch",
            "from torch import nn"
        ],
        "reference_api": [
            "torch.zeros",
            "self._mm_attention_mask",
            "torch.cat",
            "cmasks.size",
            "vmasks.size",
            "torch.ones"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._mm_attention_mask",
                "code": "def _mm_attention_mask(self, cmasks, vmasks):\n        assert cmasks.size(0) == vmasks.size(0), \"{}, {}, {}, {}\".format(\n            str(cmasks.size()),\n            str(vmasks.size()),\n            str(cmasks.size(0)),\n            str(vmasks.size(0)),\n        )\n\n        mm_mask = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:]], dim=1)\n        if self.last_iso_layer == 0:\n            # hard attention mask.\n            return mm_mask\n        else:\n            # a gpu iso mask; 0 : num_iso_layer is isolated;\n            # num_iso_layer: are MM-fused.\n            # make an iso layer\n            batch_size = cmasks.size(0)\n            iso_mask = self._make_iso_mask(batch_size, cmasks, vmasks)\n            mm_mask = mm_mask[:, None, :].repeat(1, mm_mask.size(-1), 1)\n            iso_mm_masks = []\n            # hard attention mask.\n            iso_mask = iso_mask[:, None, :, :].repeat(\n                1, self.last_iso_layer, 1, 1)\n            iso_mm_masks.append(iso_mask)\n            if self.last_iso_layer < self.num_hidden_layers:\n                mm_mask = mm_mask[:, None, :, :].repeat(\n                    1, self.num_hidden_layers - self.last_iso_layer, 1, 1\n                )\n                iso_mm_masks.append(mm_mask)\n            iso_mm_masks = torch.cat(iso_mm_masks, dim=1)\n            return iso_mm_masks"
            }
        ],
        "third_party": [
            "torch.cat",
            "torch.zeros",
            "vmasks.size",
            "vmasks.size",
            "torch.ones",
            "cmasks.size",
            "cmasks.size"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "facebookresearch/fairseq",
        "function_declaration": "def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        input_video_embeds,\n        attention_mask=None,\n        token_type_ids=None,\n        **model_kwargs\n    )",
        "start_line": "192",
        "end_line": "217",
        "file_path": "examples/MMPT/mmpt/models/mmfusionnlg.py",
        "docstring": "The function prepares inputs for text and video generation models.\\nIt adjusts the sequence length to match the combined length of input_ids and input_video_embeds.\\nIt modifies the attention_mask and token_type_ids to fit the new sequence length.\\nThe function returns a dictionary containing the updated input_ids, input_video_embeds, attention_mask, and token_type_ids.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fa99d2a148a5",
        "ground_truth": "def prepare_inputs_for_generation(\n    self,\n    input_ids,\n    input_video_embeds,\n    attention_mask=None,\n    token_type_ids=None,\n    **model_kwargs\n):\n    # must return a dictionary.\n    seq_len = input_ids.size(1) + input_video_embeds.size(1)\n    if attention_mask is not None:\n        if len(attention_mask.size()) == 4:\n            attention_mask = attention_mask[:, :, :seq_len, :seq_len]\n        elif len(attention_mask.size()) == 3:\n            attention_mask = attention_mask[:, :seq_len, :seq_len]\n        else:\n            attention_mask = attention_mask[:, :seq_len]\n    if token_type_ids is not None:\n        token_type_ids = token_type_ids[:, :seq_len]\n    return {\n        \"input_ids\": input_ids,\n        \"input_video_embeds\": input_video_embeds,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n    }",
        "import_statements": [
            "import torch",
            "from torch.nn import functional as F",
            "from typing import Optional, Iterable"
        ],
        "reference_api": [
            "attention_mask.size",
            "input_video_embeds.size",
            "len",
            "input_ids.size"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "input_ids.size",
            "input_video_embeds.size",
            "attention_mask.size",
            "attention_mask.size"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "open-mmlab/mmdetection",
        "function_declaration": "def _init_pipeline(self, cfg: ConfigType) -> Compose",
        "start_line": "159",
        "end_line": "175",
        "file_path": "mmdet/apis/det_inferencer.py",
        "docstring": "The function initializes the test pipeline based on the provided configuration.\\nIt adjusts the pipeline configuration by removing the 'img_id' key from meta_keys if present.\\nIt finds the index of the 'LoadImageFromFile' transform, raising an error if not found.\\nIt updates the transform type to 'mmdet.InferencerLoader' and returns the modified pipeline configuration wrapped in a Compose object.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b1b0d25fe620",
        "ground_truth": "def _init_pipeline(self, cfg: ConfigType) -> Compose:\n    \"\"\"Initialize the test pipeline.\"\"\"\n    pipeline_cfg = cfg.test_dataloader.dataset.pipeline\n    # For inference, the key of ``img_id`` is not used.\n    if 'meta_keys' in pipeline_cfg[-1]:\n        pipeline_cfg[-1]['meta_keys'] = tuple(\n            meta_key for meta_key in pipeline_cfg[-1]['meta_keys']\n            if meta_key != 'img_id')\n    load_img_idx = self._get_transform_idx(\n        pipeline_cfg, ('LoadImageFromFile', LoadImageFromFile))\n    if load_img_idx == -1:\n        raise ValueError(\n            'LoadImageFromFile is not found in the test pipeline')\n    pipeline_cfg[load_img_idx]['type'] = 'mmdet.InferencerLoader'\n    return Compose(pipeline_cfg)",
        "import_statements": [
            "import copy",
            "import warnings",
            "from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Union",
            "import mmcv",
            "import mmengine",
            "from mmcv.transforms import LoadImageFromFile",
            "from mmengine.dataset import Compose",
            "from mmengine.fileio import (get_file_backend, isdir, join_path,\n                             list_dir_or_file)",
            "from mmengine.infer.infer import BaseInferencer, ModelType",
            "from mmengine.model.utils import revert_sync_batchnorm",
            "from mmengine.registry import init_default_scope",
            "from mmengine.runner.checkpoint import _load_checkpoint_to_model",
            "from mmengine.visualization import Visualizer",
            "from rich.progress import track",
            "from mmdet.evaluation import INSTANCE_OFFSET",
            "from mmdet.registry import DATASETS",
            "from mmdet.structures import DetDataSample",
            "from mmdet.structures.mask import encode_mask_results, mask2bbox",
            "from mmdet.utils import ConfigType"
        ],
        "reference_api": [
            "ValueError",
            "self._get_transform_idx",
            "Compose",
            "tuple"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_transform_idx",
                "code": "def _get_transform_idx(self, pipeline_cfg: ConfigType,\n                           name: Union[str, Tuple[str, type]]) -> int:\n        \"\"\"Returns the index of the transform in a pipeline.\n\n        If the transform is not found, returns -1.\n        \"\"\"\n        for i, transform in enumerate(pipeline_cfg):\n            if transform['type'] in name:\n                return i\n        return -1"
            }
        ],
        "third_party": [
            "Compose"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "open-mmlab/mmdetection",
        "function_declaration": "def _inputs_to_list(self, inputs: InputsType) -> list",
        "start_line": "201",
        "end_line": "234",
        "file_path": "mmdet/apis/det_inferencer.py",
        "docstring": "The function converts inputs to a list format.\\nIf the input is a string representing a directory, it lists image files in the directory and updates the input to be a list of file paths.\\nIf the input is not already a list or tuple, it wraps the input in a list.\\nFinally, it returns the input as a list.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7967962856eb",
        "ground_truth": "def _inputs_to_list(self, inputs: InputsType) -> list:\n    \"\"\"Preprocess the inputs to a list.\n    Preprocess inputs to a list according to its type:\n    - list or tuple: return inputs\n    - str:\n        - Directory path: return all files in the directory\n        - other cases: return a list containing the string. The string\n          could be a path to file, a url or other types of string according\n          to the task.\n    Args:\n        inputs (InputsType): Inputs for the inferencer.\n    Returns:\n        list: List of input for the :meth:`preprocess`.\n    \"\"\"\n    if isinstance(inputs, str):\n        backend = get_file_backend(inputs)\n        if hasattr(backend, 'isdir') and isdir(inputs):\n            # Backends like HttpsBackend do not implement `isdir`, so only\n            # those backends that implement `isdir` could accept the inputs\n            # as a directory\n            filename_list = list_dir_or_file(\n                inputs, list_dir=False, suffix=IMG_EXTENSIONS)\n            inputs = [\n                join_path(inputs, filename) for filename in filename_list\n            ]\n    if not isinstance(inputs, (list, tuple)):\n        inputs = [inputs]\n    return list(inputs)",
        "import_statements": [
            "import copy",
            "import warnings",
            "from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Union",
            "import mmcv",
            "import mmengine",
            "from mmcv.transforms import LoadImageFromFile",
            "from mmengine.dataset import Compose",
            "from mmengine.fileio import (get_file_backend, isdir, join_path,\n                             list_dir_or_file)",
            "from mmengine.infer.infer import BaseInferencer, ModelType",
            "from mmengine.model.utils import revert_sync_batchnorm",
            "from mmengine.registry import init_default_scope",
            "from mmengine.runner.checkpoint import _load_checkpoint_to_model",
            "from mmengine.visualization import Visualizer",
            "from rich.progress import track",
            "from mmdet.evaluation import INSTANCE_OFFSET",
            "from mmdet.registry import DATASETS",
            "from mmdet.structures import DetDataSample",
            "from mmdet.structures.mask import encode_mask_results, mask2bbox",
            "from mmdet.utils import ConfigType"
        ],
        "reference_api": [
            "list",
            "list_dir_or_file",
            "hasattr",
            "isinstance",
            "get_file_backend",
            "isdir",
            "join_path"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "get_file_backend",
            "isdir",
            "list_dir_or_file",
            "join_path"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "open-mmlab/mmdetection",
        "function_declaration": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int)",
        "start_line": "263",
        "end_line": "293",
        "file_path": "mmdet/apis/det_inferencer.py",
        "docstring": "The function processes input data in chunks.\\nIt iterates through the inputs, collecting a specified chunk size of data.\\nFor each input, it checks if the input is a dictionary containing an 'img' or 'img_path' key and processes it with the pipeline function.\\nIt yields each chunk of processed data until all inputs are processed.\\nIf there are remaining inputs after the last chunk, it yields the final chunk.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "74f727f06fa0",
        "ground_truth": "def _get_chunk_data(self, inputs: Iterable, chunk_size: int):\n    \"\"\"Get batch data from inputs.\n    Args:\n        inputs (Iterable): An iterable dataset.\n        chunk_size (int): Equivalent to batch size.\n    Yields:\n        list: batch data.\n    \"\"\"\n    inputs_iter = iter(inputs)\n    while True:\n        try:\n            chunk_data = []\n            for _ in range(chunk_size):\n                inputs_ = next(inputs_iter)\n                if isinstance(inputs_, dict):\n                    if 'img' in inputs_:\n                        ori_inputs_ = inputs_['img']\n                    else:\n                        ori_inputs_ = inputs_['img_path']\n                    chunk_data.append(\n                        (ori_inputs_,\n                         self.pipeline(copy.deepcopy(inputs_))))\n                else:\n                    chunk_data.append((inputs_, self.pipeline(inputs_)))\n            yield chunk_data\n        except StopIteration:\n            if chunk_data:\n                yield chunk_data\n            break",
        "import_statements": [
            "import copy",
            "import warnings",
            "from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Union",
            "import mmcv",
            "import mmengine",
            "from mmcv.transforms import LoadImageFromFile",
            "from mmengine.dataset import Compose",
            "from mmengine.fileio import (get_file_backend, isdir, join_path,\n                             list_dir_or_file)",
            "from mmengine.infer.infer import BaseInferencer, ModelType",
            "from mmengine.model.utils import revert_sync_batchnorm",
            "from mmengine.registry import init_default_scope",
            "from mmengine.runner.checkpoint import _load_checkpoint_to_model",
            "from mmengine.visualization import Visualizer",
            "from rich.progress import track",
            "from mmdet.evaluation import INSTANCE_OFFSET",
            "from mmdet.registry import DATASETS",
            "from mmdet.structures import DetDataSample",
            "from mmdet.structures.mask import encode_mask_results, mask2bbox",
            "from mmdet.utils import ConfigType"
        ],
        "reference_api": [
            "chunk_data.append",
            "next",
            "self.pipeline",
            "isinstance",
            "iter",
            "copy.deepcopy",
            "range"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "chunk_data.append",
            "self.pipeline",
            "chunk_data.append",
            "self.pipeline"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "open-mmlab/mmdetection",
        "function_declaration": "def postprocess(\n        self,\n        preds: PredType,\n        visualization: Optional[List[np.ndarray]] = None,\n        return_datasamples: bool = False,\n        print_result: bool = False,\n        no_save_pred: bool = False,\n        pred_out_dir: str = '',\n        **kwargs,\n    ) -> Dict",
        "start_line": "507",
        "end_line": "569",
        "file_path": "mmdet/apis/det_inferencer.py",
        "docstring": "The function post-processes prediction results, optionally saving and printing them.\\nIt initializes the output directory if predictions are not to be saved and prepares a result dictionary.\\nIf return_datasamples is False, it converts predictions to dictionaries and appends them to results.\\nIf pred_out_dir is not empty and return_datasamples is True, it issues a warning that saving datasamples is not supported.\\nThe function adds predictions and visualization to the result dictionary, prints the results if required, and returns the result dictionary.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a43ab51afa59",
        "ground_truth": "def postprocess(\n    self,\n    preds: PredType,\n    visualization: Optional[List[np.ndarray]] = None,\n    return_datasamples: bool = False,\n    print_result: bool = False,\n    no_save_pred: bool = False,\n    pred_out_dir: str = '',\n    **kwargs,\n) -> Dict:\n    \"\"\"Process the predictions and visualization results from ``forward``\n    and ``visualize``.\n    This method should be responsible for the following tasks:\n    1. Convert datasamples into a json-serializable dict if needed.\n    2. Pack the predictions and visualization results and return them.\n    3. Dump or log the predictions.\n    Args:\n        preds (List[:obj:`DetDataSample`]): Predictions of the model.\n        visualization (Optional[np.ndarray]): Visualized predictions.\n        return_datasamples (bool): Whether to use Datasample to store\n            inference results. If False, dict will be used.\n        print_result (bool): Whether to print the inference result w/o\n            visualization to the console. Defaults to False.\n        no_save_pred (bool): Whether to force not to save prediction\n            results. Defaults to False.\n        pred_out_dir: Dir to save the inference results w/o\n            visualization. If left as empty, no file will be saved.\n            Defaults to ''.\n    Returns:\n        dict: Inference and visualization results with key ``predictions``\n        and ``visualization``.\n        - ``visualization`` (Any): Returned by :meth:`visualize`.\n        - ``predictions`` (dict or DataSample): Returned by\n            :meth:`forward` and processed in :meth:`postprocess`.\n            If ``return_datasamples=False``, it usually should be a\n            json-serializable dict containing only basic data elements such\n            as strings and numbers.\n    \"\"\"\n    if no_save_pred is True:\n        pred_out_dir = ''\n    result_dict = {}\n    results = preds\n    if not return_datasamples:\n        results = []\n        for pred in preds:\n            result = self.pred2dict(pred, pred_out_dir)\n            results.append(result)\n    elif pred_out_dir != '':\n        warnings.warn('Currently does not support saving datasample '\n                      'when return_datasamples is set to True. '\n                      'Prediction results are not saved!')\n    # Add img to the results after printing and dumping\n    result_dict['predictions'] = results\n    if print_result:\n        print(result_dict)\n    result_dict['visualization'] = visualization\n    return result_dict",
        "import_statements": [
            "import copy",
            "import warnings",
            "from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Union",
            "import mmcv",
            "import mmengine",
            "from mmcv.transforms import LoadImageFromFile",
            "from mmengine.dataset import Compose",
            "from mmengine.fileio import (get_file_backend, isdir, join_path,\n                             list_dir_or_file)",
            "from mmengine.infer.infer import BaseInferencer, ModelType",
            "from mmengine.model.utils import revert_sync_batchnorm",
            "from mmengine.registry import init_default_scope",
            "from mmengine.runner.checkpoint import _load_checkpoint_to_model",
            "from mmengine.visualization import Visualizer",
            "from rich.progress import track",
            "from mmdet.evaluation import INSTANCE_OFFSET",
            "from mmdet.registry import DATASETS",
            "from mmdet.structures import DetDataSample",
            "from mmdet.structures.mask import encode_mask_results, mask2bbox",
            "from mmdet.utils import ConfigType"
        ],
        "reference_api": [
            "print",
            "results.append",
            "warnings.warn",
            "self.pred2dict"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.pred2dict",
                "code": "def pred2dict(self,\n                  data_sample: DetDataSample,\n                  pred_out_dir: str = '') -> Dict:\n        \"\"\"Extract elements necessary to represent a prediction into a\n        dictionary.\n\n        It's better to contain only basic data elements such as strings and\n        numbers in order to guarantee it's json-serializable.\n\n        Args:\n            data_sample (:obj:`DetDataSample`): Predictions of the model.\n            pred_out_dir: Dir to save the inference results w/o\n                visualization. If left as empty, no file will be saved.\n                Defaults to ''.\n\n        Returns:\n            dict: Prediction results.\n        \"\"\"\n        is_save_pred = True\n        if pred_out_dir == '':\n            is_save_pred = False\n\n        if is_save_pred and 'img_path' in data_sample:\n            img_path = osp.basename(data_sample.img_path)\n            img_path = osp.splitext(img_path)[0]\n            out_img_path = osp.join(pred_out_dir, 'preds',\n                                    img_path + '_panoptic_seg.png')\n            out_json_path = osp.join(pred_out_dir, 'preds', img_path + '.json')\n        elif is_save_pred:\n            out_img_path = osp.join(\n                pred_out_dir, 'preds',\n                f'{self.num_predicted_imgs}_panoptic_seg.png')\n            out_json_path = osp.join(pred_out_dir, 'preds',\n                                     f'{self.num_predicted_imgs}.json')\n            self.num_predicted_imgs += 1\n\n        result = {}\n        if 'pred_instances' in data_sample:\n            masks = data_sample.pred_instances.get('masks')\n            pred_instances = data_sample.pred_instances.numpy()\n            result = {\n                'labels': pred_instances.labels.tolist(),\n                'scores': pred_instances.scores.tolist()\n            }\n            if 'bboxes' in pred_instances:\n                result['bboxes'] = pred_instances.bboxes.tolist()\n            if masks is not None:\n                if 'bboxes' not in pred_instances or pred_instances.bboxes.sum(\n                ) == 0:\n                    # Fake bbox, such as the SOLO.\n                    bboxes = mask2bbox(masks.cpu()).numpy().tolist()\n                    result['bboxes'] = bboxes\n                encode_masks = encode_mask_results(pred_instances.masks)\n                for encode_mask in encode_masks:\n                    if isinstance(encode_mask['counts'], bytes):\n                        encode_mask['counts'] = encode_mask['counts'].decode()\n                result['masks'] = encode_masks\n\n        if 'pred_panoptic_seg' in data_sample:\n            if VOID is None:\n                raise RuntimeError(\n                    'panopticapi is not installed, please install it by: '\n                    'pip install git+https://github.com/cocodataset/'\n                    'panopticapi.git.')\n\n            pan = data_sample.pred_panoptic_seg.sem_seg.cpu().numpy()[0]\n            pan[pan % INSTANCE_OFFSET == len(\n                self.model.dataset_meta['classes'])] = VOID\n            pan = id2rgb(pan).astype(np.uint8)\n\n            if is_save_pred:\n                mmcv.imwrite(pan[:, :, ::-1], out_img_path)\n                result['panoptic_seg_path'] = out_img_path\n            else:\n                result['panoptic_seg'] = pan\n\n        if is_save_pred:\n            mmengine.dump(result, out_json_path)\n\n        return result"
            }
        ],
        "third_party": [
            "results.append"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "open-mmlab/mmdetection",
        "function_declaration": "def build_test_pipeline(cfg: ConfigType) -> ConfigType",
        "start_line": "248",
        "end_line": "266",
        "file_path": "mmdet/apis/inference.py",
        "docstring": "The function builds a test pipeline from a given configuration.\\nIt copies the first transform in the dataset pipeline and modifies it to include only the 'Resize' transform.\\nIt then copies the last transform in the pipeline and combines the modified first and copied last transforms into a new test pipeline using Compose.\\nThe function returns the constructed test pipeline.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a3c2561f199c",
        "ground_truth": "def build_test_pipeline(cfg: ConfigType) -> ConfigType:\n    \"\"\"Build test_pipeline for mot/vis demo. In mot/vis infer, original\n    test_pipeline should remove the \"LoadImageFromFile\" and\n    \"LoadTrackAnnotations\".\n     Args:\n         cfg (ConfigDict): The loaded config.\n    Returns:\n         ConfigType: new test_pipeline\n    \"\"\"\n    # remove the \"LoadImageFromFile\" and \"LoadTrackAnnotations\" in pipeline\n    transform_broadcaster = cfg.test_dataloader.dataset.pipeline[0].copy()\n    for transform in transform_broadcaster['transforms']:\n        if transform['type'] == 'Resize':\n            transform_broadcaster['transforms'] = transform\n    pack_track_inputs = cfg.test_dataloader.dataset.pipeline[-1].copy()\n    test_pipeline = Compose([transform_broadcaster, pack_track_inputs])\n     return test_pipeline",
        "import_statements": [
            "import copy",
            "import warnings",
            "from pathlib import Path",
            "from typing import Optional, Sequence, Union",
            "import torch",
            "from mmcv.ops import RoIPool",
            "from mmcv.transforms import Compose",
            "from mmengine.config import Config",
            "from mmengine.dataset import default_collate",
            "from mmengine.model.utils import revert_sync_batchnorm",
            "from mmengine.registry import init_default_scope",
            "from mmengine.runner import load_checkpoint",
            "from mmdet.registry import DATASETS",
            "from mmdet.utils import ConfigType"
        ],
        "reference_api": [
            "Compose",
            "copy"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "Compose"
        ]
    },
    {
        "subclass": "Pytorch",
        "owner/repo": "open-mmlab/mmdetection",
        "function_declaration": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList",
        "start_line": "269",
        "end_line": "302",
        "file_path": "mmdet/apis/inference.py",
        "docstring": "The function performs inference on a multi-object tracking (MOT) model.\\nIt prepares the input data by constructing a dictionary with image, frame ID, original shape, image ID, and video length.\\nThe data is processed through a test pipeline built from the model's configuration.\\nIf the model is on the CPU, it ensures no unsupported modules like RoIPool are used.\\nThe function performs a forward pass of the model without gradient computation and returns the inference result.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "48a358ab5a38",
        "ground_truth": "def inference_mot(model: nn.Module, img: np.ndarray, frame_id: int,\n                  video_len: int) -> SampleList:\n    \"\"\"Inference image(s) with the mot model.\n     Args:\n        model (nn.Module): The loaded mot model.\n        img (np.ndarray): Loaded image.\n        frame_id (int): frame id.\n        video_len (int): demo video length\n    Returns:\n        SampleList: The tracking data samples.\n    \"\"\"\n    cfg = model.cfg\n    data = dict(\n        img=[img.astype(np.float32)],\n        frame_id=[frame_id],\n        ori_shape=[img.shape[:2]],\n        img_id=[frame_id + 1],\n        ori_video_length=[video_len])\n     test_pipeline = build_test_pipeline(cfg)\n    data = test_pipeline(data)\n     if not next(model.parameters()).is_cuda:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n     # forward the model\n    with torch.no_grad():\n        data = default_collate([data])\n        result = model.test_step(data)[0]\n    return result",
        "import_statements": [
            "import copy",
            "import warnings",
            "from pathlib import Path",
            "from typing import Optional, Sequence, Union",
            "import torch",
            "from mmcv.ops import RoIPool",
            "from mmcv.transforms import Compose",
            "from mmengine.config import Config",
            "from mmengine.dataset import default_collate",
            "from mmengine.model.utils import revert_sync_batchnorm",
            "from mmengine.registry import init_default_scope",
            "from mmengine.runner import load_checkpoint",
            "from mmdet.registry import DATASETS",
            "from mmdet.utils import ConfigType"
        ],
        "reference_api": [
            "test_pipeline",
            "build_test_pipeline",
            "next",
            "model.parameters",
            "dict",
            "img.astype",
            "isinstance",
            "default_collate",
            "model.test_step",
            "torch.no_grad",
            "model.modules"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "build_test_pipeline",
                "code": "def build_test_pipeline(cfg: ConfigType) -> ConfigType:\n    \"\"\"Build test_pipeline for mot/vis demo. In mot/vis infer, original\n    test_pipeline should remove the \"LoadImageFromFile\" and\n    \"LoadTrackAnnotations\".\n\n    Args:\n         cfg (ConfigDict): The loaded config.\n    Returns:\n         ConfigType: new test_pipeline\n    \"\"\"\n    # remove the \"LoadImageFromFile\" and \"LoadTrackAnnotations\" in pipeline\n    transform_broadcaster = cfg.test_dataloader.dataset.pipeline[0].copy()\n    for transform in transform_broadcaster['transforms']:\n        if transform['type'] == 'Resize':\n            transform_broadcaster['transforms'] = transform\n    pack_track_inputs = cfg.test_dataloader.dataset.pipeline[-1].copy()\n    test_pipeline = Compose([transform_broadcaster, pack_track_inputs])\n\n    return test_pipeline"
            }
        ],
        "third_party": [
            "img.astype",
            "test_pipeline",
            "model.parameters",
            "model.modules",
            "torch.no_grad",
            "default_collate",
            "model.test_step"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)",
        "start_line": "418",
        "end_line": "433",
        "file_path": "create_pretraining_data.py",
        "docstring": "The function truncates two sequences of tokens to ensure their combined length does not exceed a specified maximum.\\nIt repeatedly removes tokens from the longer sequence until the total length is within the limit.\\nTokens are randomly removed from either the start or the end of the longer sequence.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6c4ea2ce1787",
        "ground_truth": "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n  \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_num_tokens:\n      break\n     trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n    assert len(trunc_tokens) >= 1\n     # We want to sometimes truncate from the front and sometimes from the\n    # back to add more randomness and avoid biases.\n    if rng.random() < 0.5:\n      del trunc_tokens[0]\n    else:\n      trunc_tokens.pop()",
        "import_statements": [
            "import collections",
            "import random",
            "import tokenization"
        ],
        "reference_api": [
            "rng.random",
            "len",
            "trunc_tokens.pop"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "rng.random",
            "trunc_tokens.pop"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def get_activation(activation_string)",
        "start_line": "280",
        "end_line": "314",
        "file_path": "modeling.py",
        "docstring": "The function retrieves the activation function based on a given string.\\nIt first checks if the input is a string and returns it directly if not.\\nIf the string is empty, it returns None.\\nIt then matches the string to known activation functions, returning the corresponding function or raising an error if unsupported.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "007181b6729d",
        "ground_truth": "def get_activation(activation_string):\n  \"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n   Args:\n    activation_string: String name of the activation function.\n   Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or \"linear\", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n   Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  \"\"\"\n   # We assume that anything that\"s not a string is already an activation\n  # function, so we just return it.\n  if not isinstance(activation_string, six.string_types):\n    return activation_string\n   if not activation_string:\n    return None\n   act = activation_string.lower()\n  if act == \"linear\":\n    return None\n  elif act == \"relu\":\n    return tf.nn.relu\n  elif act == \"gelu\":\n    return gelu\n  elif act == \"tanh\":\n    return tf.tanh\n  else:\n    raise ValueError(\"Unsupported activation: %s\" % act)",
        "import_statements": [
            "import collections",
            "import copy",
            "import json",
            "import math",
            "import re",
            "import six"
        ],
        "reference_api": [
            "ValueError",
            "activation_string.lower",
            "isinstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "activation_string.lower"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def dropout(input_tensor, dropout_prob)",
        "start_line": "344",
        "end_line": "359",
        "file_path": "modeling.py",
        "docstring": "The function applies dropout to the input tensor based on the specified dropout probability.\\nIf the dropout probability is None or 0.0, it returns the input tensor unchanged.\\nIf a dropout probability is provided, it applies TensorFlow's dropout operation to the input tensor.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d5d6e0315899",
        "ground_truth": "def dropout(input_tensor, dropout_prob):\n  \"\"\"Perform dropout.\n   Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n   Returns:\n    A version of `input_tensor` with dropout applied.\n  \"\"\"\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n   output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n  return output",
        "import_statements": [
            "import collections",
            "import copy",
            "import json",
            "import math",
            "import re",
            "import six"
        ],
        "reference_api": [
            "dropout"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "dropout",
                "code": "def dropout(input_tensor, dropout_prob):\n  \"\"\"Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  \"\"\"\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n\n  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n  return output"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def create_attention_mask_from_input_mask(from_tensor, to_mask)",
        "start_line": "524",
        "end_line": "555",
        "file_path": "modeling.py",
        "docstring": "The function creates an attention mask for a tensor based on an input mask.\\nIt retrieves the shape of the input tensor and mask, reshapes the mask, and casts it to a float tensor.\\nIt then creates a tensor of ones with a shape matching the from_tensor sequence length and multiplies it by the reshaped mask.\\nThe function returns the resulting attention mask.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "828917b9ac53",
        "ground_truth": "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n  \"\"\"Create 3D attention mask from a 2D tensor mask.\n   Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n   Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  \"\"\"\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  batch_size = from_shape[0]\n  from_seq_length = from_shape[1]\n   to_shape = get_shape_list(to_mask, expected_rank=2)\n  to_seq_length = to_shape[1]\n   to_mask = tf.cast(\n      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n   # We don't assume that `from_tensor` is a mask (although it could be). We\n  # don't actually care if we attend *from* padding tokens (only *to* padding)\n  # tokens so we create a tensor of all ones.\n  #\n  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n  broadcast_ones = tf.ones(\n      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n   # Here we broadcast along two dimensions to create the mask.\n  mask = broadcast_ones * to_mask\n   return mask",
        "import_statements": [
            "import collections",
            "import copy",
            "import json",
            "import math",
            "import re",
            "import six"
        ],
        "reference_api": [
            "tf.reshape",
            "tf.ones",
            "tf.cast",
            "get_shape_list"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "get_shape_list",
                "code": "def get_shape_list(tensor, expected_rank=None, name=None):\n  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  \"\"\"\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape"
            },
            {
                "name": "get_shape_list",
                "code": "def get_shape_list(tensor, expected_rank=None, name=None):\n  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  \"\"\"\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape"
            }
        ],
        "third_party": [
            "tf.cast",
            "tf.reshape",
            "tf.ones"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width)",
        "start_line": "629",
        "end_line": "635",
        "file_path": "modeling.py",
        "docstring": "The function reshapes and transposes an input tensor for attention scores.\\nIt first reshapes the input tensor to a 4D tensor with dimensions [batch_size, seq_length, num_attention_heads, width].\\nThen, it transposes the tensor to reorder the dimensions to [batch_size, num_attention_heads, seq_length, width].\\nThe function returns the transposed tensor.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d7798e41a723",
        "ground_truth": "def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                         seq_length, width):\n  output_tensor = tf.reshape(\n      input_tensor, [batch_size, seq_length, num_attention_heads, width])\n  output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n  return output_tensor",
        "import_statements": [
            "import collections",
            "import copy",
            "import json",
            "import math",
            "import re",
            "import six"
        ],
        "reference_api": [
            "tf.reshape",
            "tf.transpose"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.reshape",
            "tf.transpose"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def reshape_to_matrix(input_tensor)",
        "start_line": "932",
        "end_line": "943",
        "file_path": "modeling.py",
        "docstring": "The function reshapes a tensor of rank 2 or higher into a matrix (rank 2 tensor).\\nIt first checks the number of dimensions of the input tensor, raising an error if it is less than 2.\\nIf the tensor is already rank 2, it returns the tensor as is.\\nOtherwise, it reshapes the tensor to have a shape of [-1, width], where width is the size of the last dimension, and returns the reshaped tensor.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "711d466ace09",
        "ground_truth": "def reshape_to_matrix(input_tensor):\n  \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n  ndims = input_tensor.shape.ndims\n  if ndims < 2:\n    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n                     (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n   width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor",
        "import_statements": [
            "import collections",
            "import copy",
            "import json",
            "import math",
            "import re",
            "import six"
        ],
        "reference_api": [
            "ValueError",
            "tf.reshape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.reshape"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def get_train_examples(self, data_dir)",
        "start_line": "213",
        "end_line": "230",
        "file_path": "run_classifier.py",
        "docstring": "The function retrieves training examples from a specified directory.\\nIt reads a TSV file containing training data, skipping the header row.\\nFor each line, it extracts the text and label, converting them to unicode.\\nIf the label is \"contradictory\", it changes it to \"contradiction\".\\nIt creates an InputExample object for each line, storing the examples in a list.\\nThe function returns the list of training examples.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7adcd08f531e",
        "ground_truth": "def get_train_examples(self, data_dir):\n  \"\"\"See base class.\"\"\"\n  lines = self._read_tsv(\n      os.path.join(data_dir, \"multinli\",\n                   \"multinli.train.%s.tsv\" % self.language))\n  examples = []\n  for (i, line) in enumerate(lines):\n    if i == 0:\n      continue\n    guid = \"train-%d\" % (i)\n    text_a = tokenization.convert_to_unicode(line[0])\n    text_b = tokenization.convert_to_unicode(line[1])\n    label = tokenization.convert_to_unicode(line[2])\n    if label == tokenization.convert_to_unicode(\"contradictory\"):\n      label = tokenization.convert_to_unicode(\"contradiction\")\n    examples.append(\n        InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n  return examples",
        "import_statements": [
            "import collections",
            "import csv",
            "import os",
            "import modeling",
            "import optimization",
            "import tokenization"
        ],
        "reference_api": [
            "join",
            "examples.append",
            "self._read_tsv",
            "tokenization.convert_to_unicode",
            "InputExample",
            "enumerate"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "tokenization.convert_to_unicode",
                "code": "def convert_to_unicode(text):\n  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(\"utf-8\", \"ignore\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(\"utf-8\", \"ignore\")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  else:\n    raise ValueError(\"Not running on Python2 or Python 3?\")"
            },
            {
                "name": "tokenization.convert_to_unicode",
                "code": "def convert_to_unicode(text):\n  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(\"utf-8\", \"ignore\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(\"utf-8\", \"ignore\")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  else:\n    raise ValueError(\"Not running on Python2 or Python 3?\")"
            },
            {
                "name": "tokenization.convert_to_unicode",
                "code": "def convert_to_unicode(text):\n  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(\"utf-8\", \"ignore\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(\"utf-8\", \"ignore\")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  else:\n    raise ValueError(\"Not running on Python2 or Python 3?\")"
            },
            {
                "name": "tokenization.convert_to_unicode",
                "code": "def convert_to_unicode(text):\n  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(\"utf-8\", \"ignore\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(\"utf-8\", \"ignore\")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  else:\n    raise ValueError(\"Not running on Python2 or Python 3?\")"
            },
            {
                "name": "tokenization.convert_to_unicode",
                "code": "def convert_to_unicode(text):\n  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(\"utf-8\", \"ignore\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(\"utf-8\", \"ignore\")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  else:\n    raise ValueError(\"Not running on Python2 or Python 3?\")"
            }
        ],
        "third_party": [
            "self._read_tsv",
            "join",
            "examples.append",
            "InputExample"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def metric_fn(per_example_loss, label_ids, logits, is_real_example)",
        "start_line": "684",
        "end_line": "692",
        "file_path": "run_classifier.py",
        "docstring": "The function calculates evaluation metrics for a model.\\nIt computes predictions by taking the argmax of the logits and calculates accuracy using the true labels and a mask for real examples.\\nIt also calculates the mean loss for the examples, again weighted by the real example mask.\\nThe function returns a dictionary containing the evaluation accuracy and loss.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "26a0bb96dab9",
        "ground_truth": "def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n  predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n  accuracy = tf.metrics.accuracy(\n      labels=label_ids, predictions=predictions, weights=is_real_example)\n  loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n  return {\n      \"eval_accuracy\": accuracy,\n      \"eval_loss\": loss,\n  }",
        "import_statements": [
            "import collections",
            "import csv",
            "import os",
            "import modeling",
            "import optimization",
            "import tokenization"
        ],
        "reference_api": [
            "tf.argmax",
            "mean",
            "accuracy"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.argmax",
            "accuracy",
            "mean"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer)",
        "start_line": "767",
        "end_line": "780",
        "file_path": "run_classifier.py",
        "docstring": "The function converts a set of InputExamples to a list of InputFeatures.\\nIt initializes an empty list to store the features and iterates over the examples.\\nFor every 10,000 examples, it logs the progress.\\nFor each example, it converts the example to features using the convert_single_example function and appends the result to the features list.\\nFinally, it returns the list of features.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6154e4d28509",
        "ground_truth": "def convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer):\n  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n   features = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n     feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n     features.append(feature)\n  return features",
        "import_statements": [
            "import collections",
            "import csv",
            "import os",
            "import modeling",
            "import optimization",
            "import tokenization"
        ],
        "reference_api": [
            "convert_single_example",
            "features.append",
            "len",
            "info",
            "enumerate"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "convert_single_example",
                "code": "def convert_single_example(ex_index, example, label_list, max_seq_length,\n                           tokenizer):\n  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n\n  if isinstance(example, PaddingInputExample):\n    return InputFeatures(\n        input_ids=[0] * max_seq_length,\n        input_mask=[0] * max_seq_length,\n        segment_ids=[0] * max_seq_length,\n        label_id=0,\n        is_real_example=False)\n\n  label_map = {}\n  for (i, label) in enumerate(label_list):\n    label_map[label] = i\n\n  tokens_a = tokenizer.tokenize(example.text_a)\n  tokens_b = None\n  if example.text_b:\n    tokens_b = tokenizer.tokenize(example.text_b)\n\n  if tokens_b:\n    # Modifies `tokens_a` and `tokens_b` in place so that the total\n    # length is less than the specified length.\n    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n  else:\n    # Account for [CLS] and [SEP] with \"- 2\"\n    if len(tokens_a) > max_seq_length - 2:\n      tokens_a = tokens_a[0:(max_seq_length - 2)]\n\n  # The convention in BERT is:\n  # (a) For sequence pairs:\n  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n  # (b) For single sequences:\n  #  tokens:   [CLS] the dog is hairy . [SEP]\n  #  type_ids: 0     0   0   0  0     0 0\n  #\n  # Where \"type_ids\" are used to indicate whether this is the first\n  # sequence or the second sequence. The embedding vectors for `type=0` and\n  # `type=1` were learned during pre-training and are added to the wordpiece\n  # embedding vector (and position vector). This is not *strictly* necessary\n  # since the [SEP] token unambiguously separates the sequences, but it makes\n  # it easier for the model to learn the concept of sequences.\n  #\n  # For classification tasks, the first vector (corresponding to [CLS]) is\n  # used as the \"sentence vector\". Note that this only makes sense because\n  # the entire model is fine-tuned.\n  tokens = []\n  segment_ids = []\n  tokens.append(\"[CLS]\")\n  segment_ids.append(0)\n  for token in tokens_a:\n    tokens.append(token)\n    segment_ids.append(0)\n  tokens.append(\"[SEP]\")\n  segment_ids.append(0)\n\n  if tokens_b:\n    for token in tokens_b:\n      tokens.append(token)\n      segment_ids.append(1)\n    tokens.append(\"[SEP]\")\n    segment_ids.append(1)\n\n  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n  # tokens are attended to.\n  input_mask = [1] * len(input_ids)\n\n  # Zero-pad up to the sequence length.\n  while len(input_ids) < max_seq_length:\n    input_ids.append(0)\n    input_mask.append(0)\n    segment_ids.append(0)\n\n  assert len(input_ids) == max_seq_length\n  assert len(input_mask) == max_seq_length\n  assert len(segment_ids) == max_seq_length\n\n  label_id = label_map[example.label]\n  if ex_index < 5:\n    tf.logging.info(\"*** Example ***\")\n    tf.logging.info(\"guid: %s\" % (example.guid))\n    tf.logging.info(\"tokens: %s\" % \" \".join(\n        [tokenization.printable_text(x) for x in tokens]))\n    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n\n  feature = InputFeatures(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids,\n      label_id=label_id,\n      is_real_example=True)\n  return feature"
            }
        ],
        "third_party": [
            "info",
            "features.append"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def get_next_sentence_output(bert_config, input_tensor, labels)",
        "start_line": "285",
        "end_line": "305",
        "file_path": "run_pretraining.py",
        "docstring": "The function computes the next sentence prediction loss and related outputs for a BERT model.\\nIt defines the output weights and bias, computes logits through matrix multiplication and bias addition, and applies a log softmax to get log probabilities.\\nLabels are reshaped and converted to one-hot encoding.\\nThe per-example loss is calculated as the negative sum of the product of one-hot labels and log probabilities, and the mean loss is computed.\\nThe function returns the mean loss, per-example loss, and log probabilities.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e09c0db71ebb",
        "ground_truth": "def get_next_sentence_output(bert_config, input_tensor, labels):\n  \"\"\"Get loss and log probs for the next sentence prediction.\"\"\"\n   # Simple binary classification. Note that 0 is \"next sentence\" and 1 is\n  # \"random sentence\". This weight matrix is not used after pre-training.\n  with tf.variable_scope(\"cls/seq_relationship\"):\n    output_weights = tf.get_variable(\n        \"output_weights\",\n        shape=[2, bert_config.hidden_size],\n        initializer=modeling.create_initializer(bert_config.initializer_range))\n    output_bias = tf.get_variable(\n        \"output_bias\", shape=[2], initializer=tf.zeros_initializer())\n     logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n    labels = tf.reshape(labels, [-1])\n    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, per_example_loss, log_probs)",
        "import_statements": [
            "import os",
            "import modeling",
            "import optimization"
        ],
        "reference_api": [
            "bias_add",
            "tf.get_variable",
            "tf.reduce_mean",
            "tf.reshape",
            "tf.variable_scope",
            "tf.matmul",
            "tf.zeros_initializer",
            "modeling.create_initializer",
            "log_softmax",
            "tf.one_hot",
            "tf.reduce_sum"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "modeling.create_initializer",
                "code": "def create_initializer(initializer_range=0.02):\n  \"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"\n  return tf.truncated_normal_initializer(stddev=initializer_range)"
            }
        ],
        "third_party": [
            "tf.variable_scope",
            "tf.get_variable",
            "tf.get_variable",
            "tf.zeros_initializer",
            "tf.matmul",
            "bias_add",
            "log_softmax",
            "tf.reshape",
            "tf.one_hot",
            "tf.reduce_sum",
            "tf.reduce_mean"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def gather_indexes(sequence_tensor, positions)",
        "start_line": "308",
        "end_line": "321",
        "file_path": "run_pretraining.py",
        "docstring": "The function gathers vectors at specific positions from a sequence tensor over a minibatch.\\nIt retrieves the shape of the sequence tensor and computes flat offsets based on the batch size and sequence length.\\nThe positions are adjusted by these offsets and flattened.\\nThe sequence tensor is also flattened, and the function gathers the vectors at the specified positions from this flattened tensor.\\nThe function returns the gathered vectors as the output tensor.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1f66b020f485",
        "ground_truth": "def gather_indexes(sequence_tensor, positions):\n  \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n  batch_size = sequence_shape[0]\n  seq_length = sequence_shape[1]\n  width = sequence_shape[2]\n   flat_offsets = tf.reshape(\n      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n  flat_positions = tf.reshape(positions + flat_offsets, [-1])\n  flat_sequence_tensor = tf.reshape(sequence_tensor,\n                                    [batch_size * seq_length, width])\n  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n  return output_tensor",
        "import_statements": [
            "import os",
            "import modeling",
            "import optimization"
        ],
        "reference_api": [
            "tf.reshape",
            "modeling.get_shape_list",
            "tf.gather",
            "tf.range"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "modeling.get_shape_list",
                "code": "def get_shape_list(tensor, expected_rank=None, name=None):\n  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  \"\"\"\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape"
            }
        ],
        "third_party": [
            "tf.reshape",
            "tf.range",
            "tf.reshape",
            "tf.reshape",
            "tf.gather"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def _decode_record(record, name_to_features)",
        "start_line": "391",
        "end_line": "403",
        "file_path": "run_pretraining.py",
        "docstring": "The function decodes a TensorFlow record into a TensorFlow example based on the provided feature specifications.\\nIt parses the record using the name_to_features dictionary.\\nSince tf.Example only supports tf.int64 and TPUs only support tf.int32, it casts all int64 types to int32.\\nThe function returns the decoded example with the necessary type conversions.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "956bffee9a33",
        "ground_truth": "def _decode_record(record, name_to_features):\n  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n  example = tf.parse_single_example(record, name_to_features)\n   # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n  # So cast all int64 to int32.\n  for name in list(example.keys()):\n    t = example[name]\n    if t.dtype == tf.int64:\n      t = tf.to_int32(t)\n    example[name] = t\n   return example",
        "import_statements": [
            "import os",
            "import modeling",
            "import optimization"
        ],
        "reference_api": [
            "tf.to_int32",
            "list",
            "example.keys",
            "tf.parse_single_example"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.parse_single_example",
            "example.keys",
            "tf.to_int32"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text)",
        "start_line": "476",
        "end_line": "510",
        "file_path": "run_squad.py",
        "docstring": "The function improves the answer span to better match the annotated answer text.\\nIt tokenizes the original answer text and then iterates through possible new start and end positions within the input span.\\nFor each possible span, it checks if the text matches the tokenized answer text.\\nIf a match is found, it returns the new start and end positions.\\nIf no match is found, it returns the original input start and end positions.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6b97f354e641",
        "ground_truth": "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text):\n  \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n   # The SQuAD annotations are character based. We first project them to\n  # whitespace-tokenized words. But then after WordPiece tokenization, we can\n  # often find a \"better match\". For example:\n  #\n  #   Question: What year was John Smith born?\n  #   Context: The leader was John Smith (1895-1943).\n  #   Answer: 1895\n  #\n  # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n  # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n  # the exact answer, 1895.\n  #\n  # However, this is not always possible. Consider the following:\n  #\n  #   Question: What country is the top exporter of electornics?\n  #   Context: The Japanese electronics industry is the lagest in the world.\n  #   Answer: Japan\n  #\n  # In this case, the annotator chose \"Japan\" as a character sub-span of\n  # the word \"Japanese\". Since our WordPiece tokenizer does not split\n  # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n  # in SQuAD, but does happen.\n  tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n   for new_start in range(input_start, input_end + 1):\n    for new_end in range(input_end, new_start - 1, -1):\n      text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n      if text_span == tok_answer_text:\n        return (new_start, new_end)\n   return (input_start, input_end)",
        "import_statements": [
            "import collections",
            "import json",
            "import math",
            "import os",
            "import random",
            "import modeling",
            "import optimization",
            "import tokenization",
            "import six"
        ],
        "reference_api": [
            "join",
            "tokenizer.tokenize",
            "range"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "tokenizer.tokenize",
                "code": "def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens"
            }
        ],
        "third_party": [
            "join",
            "join"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def _check_is_max_context(doc_spans, cur_span_index, position)",
        "start_line": "513",
        "end_line": "547",
        "file_path": "run_squad.py",
        "docstring": "The function checks if a given span has the maximum context for a token position within document spans.\\nIt iterates over the document spans to find spans that include the token position.\\nFor each span, it calculates a score based on the token's left and right context and the span's length.\\nIt tracks the span with the highest score and compares its index to the current span index.\\nThe function returns True if the current span index is the one with the highest score, indicating it has the maximum context.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "aba334b92b34",
        "ground_truth": "def _check_is_max_context(doc_spans, cur_span_index, position):\n  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n   # Because of the sliding window approach taken to scoring documents, a single\n  # token can appear in multiple documents. E.g.\n  #  Doc: the man went to the store and bought a gallon of milk\n  #  Span A: the man went to the\n  #  Span B: to the store and bought\n  #  Span C: and bought a gallon of\n  #  ...\n  #\n  # Now the word 'bought' will have two scores from spans B and C. We only\n  # want to consider the score with \"maximum context\", which we define as\n  # the *minimum* of its left and right context (the *sum* of left and\n  # right context will always be the same, of course).\n  #\n  # In the example the maximum context for 'bought' would be span C since\n  # it has 1 left context and 3 right context, while span B has 4 left context\n  # and 0 right context.\n  best_score = None\n  best_span_index = None\n  for (span_index, doc_span) in enumerate(doc_spans):\n    end = doc_span.start + doc_span.length - 1\n    if position < doc_span.start:\n      continue\n    if position > end:\n      continue\n    num_left_context = position - doc_span.start\n    num_right_context = end - position\n    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n    if best_score is None or score > best_score:\n      best_score = score\n      best_span_index = span_index\n   return cur_span_index == best_span_index",
        "import_statements": [
            "import collections",
            "import json",
            "import math",
            "import os",
            "import random",
            "import modeling",
            "import optimization",
            "import tokenization",
            "import six"
        ],
        "reference_api": [
            "min",
            "enumerate"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def load_vocab(vocab_file)",
        "start_line": "121",
        "end_line": "133",
        "file_path": "tokenization.py",
        "docstring": "The function loads a vocabulary file into an ordered dictionary.\\nIt initializes an ordered dictionary and an index counter.\\nIt reads the vocabulary file line by line, converts each line to Unicode, strips any whitespace, and adds the token to the dictionary with its corresponding index.\\nThe function increments the index for each token and returns the populated dictionary.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cf92ad509f6a",
        "ground_truth": "def load_vocab(vocab_file):\n  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n  vocab = collections.OrderedDict()\n  index = 0\n  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n    while True:\n      token = convert_to_unicode(reader.readline())\n      if not token:\n        break\n      token = token.strip()\n      vocab[token] = index\n      index += 1\n  return vocab",
        "import_statements": [
            "import collections",
            "import re",
            "import unicodedata",
            "import six"
        ],
        "reference_api": [
            "convert_to_unicode",
            "GFile",
            "reader.readline",
            "token.strip",
            "collections.OrderedDict"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "convert_to_unicode",
                "code": "def convert_to_unicode(text):\n  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(\"utf-8\", \"ignore\")\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(\"utf-8\", \"ignore\")\n    elif isinstance(text, unicode):\n      return text\n    else:\n      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n  else:\n    raise ValueError(\"Not running on Python2 or Python 3?\")"
            }
        ],
        "third_party": [
            "GFile",
            "reader.readline"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def convert_by_vocab(vocab, items)",
        "start_line": "136",
        "end_line": "141",
        "file_path": "tokenization.py",
        "docstring": "The function converts a sequence of tokens or IDs using a given vocabulary.\\nIt iterates over each item in the input sequence and appends the corresponding vocabulary value to the output list.\\nThe function returns the converted sequence as a list.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6c865803ac12",
        "ground_truth": "def convert_by_vocab(vocab, items):\n  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output",
        "import_statements": [
            "import collections",
            "import re",
            "import unicodedata",
            "import six"
        ],
        "reference_api": [
            "output.append"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "output.append"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "google-research/bert",
        "function_declaration": "def tokenize(self, text)",
        "start_line": "170",
        "end_line": "176",
        "file_path": "tokenization.py",
        "docstring": "The function tokenizes a given text by first using a basic tokenizer to split the text into tokens.\\nIt then further tokenizes each token using a wordpiece tokenizer, appending the resulting sub-tokens to a list.\\nFinally, it returns the list of all sub-tokens.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e93b9200dc4a",
        "ground_truth": "def tokenize(self, text):\n  split_tokens = []\n  for token in self.basic_tokenizer.tokenize(text):\n    for sub_token in self.wordpiece_tokenizer.tokenize(token):\n      split_tokens.append(sub_token)\n  return split_tokens",
        "import_statements": [
            "import collections",
            "import re",
            "import unicodedata",
            "import six"
        ],
        "reference_api": [
            "tokenize",
            "split_tokens.append"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "split_tokens.append"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def safe_load(path, offset, duration, sample_rate, dtype)",
        "start_line": "107",
        "end_line": "121",
        "file_path": "spleeter/audio/adapter.py",
        "docstring": "The function safely loads audio data from a specified file path within a given time range.\\nIt logs the loading process and attempts to load the audio data using provided parameters.\\nIf successful, it returns the audio data and a False flag indicating no error.\\nIf an exception occurs, it logs the error and returns a default value of -1.0 with a True flag indicating an error.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2ff6f6db53c9",
        "ground_truth": "def safe_load(path, offset, duration, sample_rate, dtype):\n    logger.info(f\"Loading audio {path} from {offset} to {offset + duration}\")\n    try:\n        (data, _) = self.load(\n            path.numpy(),\n            offset.numpy(),\n            duration.numpy(),\n            sample_rate.numpy(),\n            dtype=dtype.numpy(),\n        )\n        logger.info(\"Audio data loaded successfully\")\n        return (data, False)\n    except Exception as e:\n        logger.exception(\"An error occurs while loading audio\", exc_info=e)\n    return (np.float32(-1.0), True)",
        "import_statements": [
            "from abc import ABC, abstractmethod",
            "from importlib import import_module",
            "from pathlib import Path",
            "from typing import Any, Dict, List, Optional, Union",
            "from spleeter.audio import Codec"
        ],
        "reference_api": [
            "sample_rate.numpy",
            "logger.exception",
            "np.float32",
            "duration.numpy",
            "offset.numpy",
            "self.load",
            "dtype.numpy",
            "path.numpy",
            "logger.info"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self.load",
            "path.numpy",
            "offset.numpy",
            "duration.numpy",
            "sample_rate.numpy",
            "dtype.numpy",
            "np.float32"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor",
        "start_line": "20",
        "end_line": "39",
        "file_path": "spleeter/audio/convertor.py",
        "docstring": "The function adjusts the number of channels in a waveform tensor to the specified number of channels.\\nIf the waveform has more or equal channels, it trims to the desired number.\\nIf it has fewer channels, it tiles the waveform to increase the channels and then trims it.\\nThe function returns the adjusted waveform tensor.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "12fbe1f03d8f",
        "ground_truth": "def to_n_channels(waveform: tf.Tensor, n_channels: int) -> tf.Tensor:\n    \"\"\"\n    Convert a waveform to n_channels by removing or duplicating channels if\n    needed (in tensorflow).\n     Parameters:\n        waveform (tf.Tensor):\n            Waveform to transform.\n        n_channels (int):\n            Number of channel to reshape waveform in.\n     Returns:\n        tf.Tensor:\n            Reshaped waveform.\n    \"\"\"\n    return tf.cond(\n        tf.shape(waveform)[1] >= n_channels,\n        true_fn=lambda: waveform[:, :n_channels],\n        false_fn=lambda: tf.tile(waveform, [1, n_channels])[:, :n_channels],\n    )",
        "import_statements": [],
        "reference_api": [
            "tf.cond",
            "tf.tile",
            "tf.shape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.cond",
            "tf.shape",
            "tf.tile"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor",
        "start_line": "94",
        "end_line": "115",
        "file_path": "spleeter/audio/convertor.py",
        "docstring": "The function converts a spectrogram to a dB-scaled uint8 format.\\nIt first converts the spectrogram to dB using gain_to_db.\\nThen, it finds the maximum dB value and clips the dB spectrogram to a specified range below this maximum.\\nFinally, it converts the clipped dB spectrogram from float32 to uint8 format and returns it.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5e215f029b2e",
        "ground_truth": "def spectrogram_to_db_uint(\n    spectrogram: tf.Tensor, db_range: float = 100.0, **kwargs\n) -> tf.Tensor:\n    \"\"\"\n    Encodes given spectrogram into uint8 using decibel scale.\n     Parameters:\n        spectrogram (tf.Tensor):\n            Spectrogram to be encoded as TF float tensor.\n        db_range (float):\n            Range in decibel for encoding.\n     Returns:\n        tf.Tensor:\n            Encoded decibel spectrogram as `uint8` tensor.\n    \"\"\"\n    db_spectrogram: tf.Tensor = gain_to_db(spectrogram)\n    max_db_spectrogram: tf.Tensor = tf.reduce_max(db_spectrogram)\n    int_db_spectrogram: tf.Tensor = tf.maximum(\n        db_spectrogram, max_db_spectrogram - db_range\n    )\n    return from_float32_to_uint8(int_db_spectrogram, **kwargs)",
        "import_statements": [],
        "reference_api": [
            "tf.maximum",
            "from_float32_to_uint8",
            "tf.reduce_max",
            "gain_to_db"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "gain_to_db",
                "code": "def gain_to_db(tensor: tf.Tensor, espilon: float = 10e-10) -> tf.Tensor:\n    \"\"\"\n    Convert from gain to decibel in tensorflow.\n\n    Parameters:\n        tensor (tf.Tensor):\n            Tensor to convert\n        epsilon (float):\n            Operation constant.\n\n    Returns:\n        tf.Tensor:\n            Converted tensor.\n    \"\"\"\n    return 20.0 / np.log(10) * tf.math.log(tf.maximum(tensor, espilon))"
            }
        ],
        "third_party": [
            "tf.reduce_max",
            "tf.maximum",
            "from_float32_to_uint8"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor",
        "start_line": "18",
        "end_line": "60",
        "file_path": "spleeter/audio/spectrogram.py",
        "docstring": "The function computes the spectrogram of a given waveform tensor using Short-Time Fourier Transform (STFT).\\nIt applies a Hann window function raised to a specified exponent and performs the STFT with defined frame length and step.\\nThe resulting tensor is transposed and the absolute value is raised to a specified exponent.\\nThe function returns the computed spectrogram tensor.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b65c0968dae7",
        "ground_truth": "def compute_spectrogram_tf(\n    waveform: tf.Tensor,\n    frame_length: int = 2048,\n    frame_step: int = 512,\n    spec_exponent: float = 1.0,\n    window_exponent: float = 1.0,\n) -> tf.Tensor:\n    \"\"\"\n    Compute magnitude / power spectrogram from waveform as a\n    `n_samples x n_channels` tensor.\n     Parameters:\n        waveform (tf.Tensor):\n            Input waveform as `(times x number of channels)` tensor.\n        frame_length (int):\n            (Optional) Length of a STFT frame to use.\n        frame_step (int):\n            (Optional) HOP between successive frames.\n        spec_exponent (float):\n            (Optional) Exponent of the spectrogram (usually 1 for\n            magnitude spectrogram, or 2 for power spectrogram).\n        window_exponent (float):\n            (Optional) Exponent applied to the Hann windowing function\n            (may be useful for making perfect STFT/iSTFT reconstruction).\n     Returns:\n        tf.Tensor:\n            Computed magnitude / power spectrogram as a\n            `(T x F x n_channels)` tensor.\n    \"\"\"\n    stft_tensor: tf.Tensor = tf.transpose(\n        stft(\n            tf.transpose(waveform),\n            frame_length,\n            frame_step,\n            window_fn=lambda f, dtype: hann_window(\n                f, periodic=True, dtype=waveform.dtype\n            )\n            ** window_exponent,\n        ),\n        perm=[1, 2, 0],\n    )\n    return tf.abs(stft_tensor) ** spec_exponent",
        "import_statements": [
            "from tensorflow.signal import hann_window, stft"
        ],
        "reference_api": [
            "tf.abs",
            "hann_window",
            "stft",
            "tf.transpose"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.transpose",
            "stft",
            "tf.transpose",
            "hann_window",
            "tf.abs"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor",
        "start_line": "63",
        "end_line": "90",
        "file_path": "spleeter/audio/spectrogram.py",
        "docstring": "The function time-stretches a spectrogram by a given factor.\\nIt calculates the new time dimension by scaling the original time dimension by the factor.\\nThe spectrogram is resized using the specified interpolation method.\\nFinally, the resized spectrogram is adjusted to the original dimensions by cropping or padding, and the resulting tensor is returned.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a4686140a634",
        "ground_truth": "def time_stretch(\n    spectrogram: tf.Tensor,\n    factor: float = 1.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor:\n    \"\"\"\n    Time stretch a spectrogram preserving shape in tensorflow. Note that\n    this is an approximation in the frequency domain.\n     Parameters:\n        spectrogram (tf.Tensor):\n            Input spectrogram to be time stretched as tensor.\n        factor (float):\n            (Optional) Time stretch factor, must be > 0, default to `1`.\n        method (tf.image.ResizeMethod):\n            (Optional) Interpolation method, default to `BILINEAR`.\n     Returns:\n        tf.Tensor:\n            Time stretched spectrogram as tensor with same shape.\n    \"\"\"\n    T = tf.shape(spectrogram)[0]\n    T_ts = tf.cast(tf.cast(T, tf.float32) * factor, tf.int32)[0]\n    F = tf.shape(spectrogram)[1]\n    ts_spec = tf.image.resize_images(\n        spectrogram, [T_ts, F], method=method, align_corners=True\n    )\n    return tf.image.resize_image_with_crop_or_pad(ts_spec, T, F)",
        "import_statements": [
            "from tensorflow.signal import hann_window, stft"
        ],
        "reference_api": [
            "tf.cast",
            "resize_images",
            "resize_image_with_crop_or_pad",
            "tf.shape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.shape",
            "tf.cast",
            "tf.cast",
            "tf.shape",
            "resize_images",
            "resize_image_with_crop_or_pad"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor",
        "start_line": "121",
        "end_line": "150",
        "file_path": "spleeter/audio/spectrogram.py",
        "docstring": "The function performs pitch shifting on a given spectrogram by resizing it based on a semitone shift factor.\\nIt calculates the resizing factor and resizes the spectrogram along the frequency axis using the specified method.\\nIt then pads the resized spectrogram to maintain the original dimensions and returns the pitch-shifted spectrogram.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6dd01101d4ae",
        "ground_truth": "def pitch_shift(\n    spectrogram: tf.Tensor,\n    semitone_shift: float = 0.0,\n    method: tf.image.ResizeMethod = tf.image.ResizeMethod.BILINEAR,\n) -> tf.Tensor:\n    \"\"\"\n    Pitch shift a spectrogram preserving shape in tensorflow. Note that\n    this is an approximation in the frequency domain.\n     Parameters:\n        spectrogram (tf.Tensor):\n            Input spectrogram to be pitch shifted as tensor.\n        semitone_shift (float):\n            (Optional) Pitch shift in semitone, default to `0.0`.\n        method (tf.image.ResizeMethod):\n            (Optional) Interpolation method, default to `BILINEAR`.\n     Returns:\n        tf.Tensor:\n            Pitch shifted spectrogram (same shape as spectrogram).\n    \"\"\"\n    factor = 2 ** (semitone_shift / 12.0)\n    T = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(tf.cast(F, tf.float32) * factor, tf.int32)[0]\n    ps_spec = tf.image.resize_images(\n        spectrogram, [T, F_ps], method=method, align_corners=True\n    )\n    paddings = [[0, 0], [0, tf.maximum(0, F - F_ps)], [0, 0]]\n    return tf.pad(ps_spec[:, :F, :], paddings, \"CONSTANT\")",
        "import_statements": [
            "from tensorflow.signal import hann_window, stft"
        ],
        "reference_api": [
            "tf.pad",
            "resize_images",
            "tf.maximum",
            "tf.cast",
            "tf.shape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.shape",
            "tf.shape",
            "tf.cast",
            "tf.cast",
            "resize_images",
            "tf.maximum",
            "tf.pad"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def _get_conv_activation_layer(params: Dict) -> Any",
        "start_line": "46",
        "end_line": "61",
        "file_path": "spleeter/model/functions/unet.py",
        "docstring": "The function returns a convolutional activation layer based on provided parameters.\\nIt checks the \"conv_activation\" parameter and returns the corresponding activation layer: ReLU, ELU, or LeakyReLU with a negative slope of 0.2 as the default.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "612f0826e56a",
        "ground_truth": "def _get_conv_activation_layer(params: Dict) -> Any:\n    \"\"\"\n    Parameters:\n        params (Dict):\n            Model parameters.\n     Returns:\n        Any:\n            Required Activation function.\n    \"\"\"\n    conv_activation: str = str(params.get(\"conv_activation\"))\n    if conv_activation == \"ReLU\":\n        return ReLU()\n    elif conv_activation == \"ELU\":\n        return ELU()\n    return LeakyReLU(0.2)",
        "import_statements": [
            "from functools import partial",
            "from typing import Any, Dict, Iterable, Optional",
            "from tensorflow.compat.v1 import logging",
            "from tensorflow.compat.v1.keras.initializers import he_uniform",
            "from tensorflow.keras.layers import (  # type: ignore\n    ELU,\n    BatchNormalization,\n    Concatenate,\n    Conv2D,\n    Conv2DTranspose,\n    Dropout,\n    LeakyReLU,\n    Multiply,\n    ReLU,\n    Softmax,\n)"
        ],
        "reference_api": [
            "params.get",
            "LeakyReLU",
            "ELU",
            "str",
            "ReLU"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "params.get",
            "ReLU",
            "ELU",
            "LeakyReLU"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict",
        "start_line": "206",
        "end_line": "240",
        "file_path": "spleeter/model/functions/unet.py",
        "docstring": "The function applies a U-Net model to an input tensor for each specified instrument to generate spectrograms.\\nIt iterates over the instruments, applies the U-Net model to the input tensor for each instrument, and collects the logit masks.\\nThe logit masks are combined using a softmax function along a specified axis.\\nThe function then creates an output dictionary where each entry is a spectrogram for an instrument, obtained by multiplying the corresponding mask with the input tensor.\\nFinally, the function returns the dictionary of instrument spectrograms.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2f8b5a2fc0f2",
        "ground_truth": "def softmax_unet(\n    input_tensor: tf.Tensor, instruments: Iterable[str], params: Dict = {}\n) -> Dict:\n    \"\"\"\n    Apply softmax to multitrack unet in order to have mask suming to one.\n     Parameters:\n        input_tensor (tf.Tensor):\n            Tensor to apply blstm to.\n        instruments (Iterable[str]):\n            Iterable that provides a collection of instruments.\n        params (Dict):\n            (Optional) dict of BLSTM parameters.\n     Returns:\n        Dict:\n            Created output tensor dict.\n    \"\"\"\n    logit_mask_list = []\n    for instrument in instruments:\n        out_name = f\"{instrument}_spectrogram\"\n        logit_mask_list.append(\n            apply_unet(\n                input_tensor,\n                output_name=out_name,\n                params=params,\n                output_mask_logit=True,\n            )\n        )\n    masks = Softmax(axis=4)(tf.stack(logit_mask_list, axis=4))\n    output_dict = {}\n    for i, instrument in enumerate(instruments):\n        out_name = f\"{instrument}_spectrogram\"\n        output_dict[out_name] = Multiply(name=out_name)([masks[..., i], input_tensor])\n    return output_dict",
        "import_statements": [
            "from functools import partial",
            "from typing import Any, Dict, Iterable, Optional",
            "from tensorflow.compat.v1 import logging",
            "from tensorflow.compat.v1.keras.initializers import he_uniform",
            "from tensorflow.keras.layers import (  # type: ignore\n    ELU,\n    BatchNormalization,\n    Concatenate,\n    Conv2D,\n    Conv2DTranspose,\n    Dropout,\n    LeakyReLU,\n    Multiply,\n    ReLU,\n    Softmax,\n)"
        ],
        "reference_api": [
            "Multiply",
            "Softmax",
            "tf.stack",
            "logit_mask_list.append",
            "apply_unet",
            "enumerate"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "apply_unet",
                "code": "def apply_unet(\n    input_tensor: tf.Tensor,\n    output_name: str = \"output\",\n    params: Dict = {},\n    output_mask_logit: bool = False,\n) -> tf.Tensor:\n    \"\"\"\n    Apply a convolutionnal U-net to model a single instrument (one U-net\n    is used for each instrument).\n\n    Parameters:\n        input_tensor (tf.Tensor):\n            Input of the model.\n        output_name (str):\n            (Optional) name of the output, default to 'output'.\n        params (Dict):\n            (Optional) dict of BLSTM parameters.\n        output_mask_logit (bool):\n            (Optional) Sigmoid or logit?\n\n    Returns:\n        tf.Tensor:\n            Output tensor.\n    \"\"\"\n    logging.info(f\"Apply unet for {output_name}\")\n    conv_n_filters = params.get(\"conv_n_filters\", [16, 32, 64, 128, 256, 512])\n    conv_activation_layer = _get_conv_activation_layer(params)\n    deconv_activation_layer = _get_deconv_activation_layer(params)\n    kernel_initializer = he_uniform(seed=50)\n    conv2d_factory = partial(\n        Conv2D, strides=(2, 2), padding=\"same\", kernel_initializer=kernel_initializer\n    )\n    # First layer.\n    conv1 = conv2d_factory(conv_n_filters[0], (5, 5))(input_tensor)\n    batch1 = BatchNormalization(axis=-1)(conv1)\n    rel1 = conv_activation_layer(batch1)\n    # Second layer.\n    conv2 = conv2d_factory(conv_n_filters[1], (5, 5))(rel1)\n    batch2 = BatchNormalization(axis=-1)(conv2)\n    rel2 = conv_activation_layer(batch2)\n    # Third layer.\n    conv3 = conv2d_factory(conv_n_filters[2], (5, 5))(rel2)\n    batch3 = BatchNormalization(axis=-1)(conv3)\n    rel3 = conv_activation_layer(batch3)\n    # Fourth layer.\n    conv4 = conv2d_factory(conv_n_filters[3], (5, 5))(rel3)\n    batch4 = BatchNormalization(axis=-1)(conv4)\n    rel4 = conv_activation_layer(batch4)\n    # Fifth layer.\n    conv5 = conv2d_factory(conv_n_filters[4], (5, 5))(rel4)\n    batch5 = BatchNormalization(axis=-1)(conv5)\n    rel5 = conv_activation_layer(batch5)\n    # Sixth layer\n    conv6 = conv2d_factory(conv_n_filters[5], (5, 5))(rel5)\n    batch6 = BatchNormalization(axis=-1)(conv6)\n    _ = conv_activation_layer(batch6)\n    #\n    #\n    conv2d_transpose_factory = partial(\n        Conv2DTranspose,\n        strides=(2, 2),\n        padding=\"same\",\n        kernel_initializer=kernel_initializer,\n    )\n    #\n    up1 = conv2d_transpose_factory(conv_n_filters[4], (5, 5))((conv6))\n    up1 = deconv_activation_layer(up1)\n    batch7 = BatchNormalization(axis=-1)(up1)\n    drop1 = Dropout(0.5)(batch7)\n    merge1 = Concatenate(axis=-1)([conv5, drop1])\n    #\n    up2 = conv2d_transpose_factory(conv_n_filters[3], (5, 5))((merge1))\n    up2 = deconv_activation_layer(up2)\n    batch8 = BatchNormalization(axis=-1)(up2)\n    drop2 = Dropout(0.5)(batch8)\n    merge2 = Concatenate(axis=-1)([conv4, drop2])\n    #\n    up3 = conv2d_transpose_factory(conv_n_filters[2], (5, 5))((merge2))\n    up3 = deconv_activation_layer(up3)\n    batch9 = BatchNormalization(axis=-1)(up3)\n    drop3 = Dropout(0.5)(batch9)\n    merge3 = Concatenate(axis=-1)([conv3, drop3])\n    #\n    up4 = conv2d_transpose_factory(conv_n_filters[1], (5, 5))((merge3))\n    up4 = deconv_activation_layer(up4)\n    batch10 = BatchNormalization(axis=-1)(up4)\n    merge4 = Concatenate(axis=-1)([conv2, batch10])\n    #\n    up5 = conv2d_transpose_factory(conv_n_filters[0], (5, 5))((merge4))\n    up5 = deconv_activation_layer(up5)\n    batch11 = BatchNormalization(axis=-1)(up5)\n    merge5 = Concatenate(axis=-1)([conv1, batch11])\n    #\n    up6 = conv2d_transpose_factory(1, (5, 5), strides=(2, 2))((merge5))\n    up6 = deconv_activation_layer(up6)\n    batch12 = BatchNormalization(axis=-1)(up6)\n    # Last layer to ensure initial shape reconstruction.\n    if not output_mask_logit:\n        up7 = Conv2D(\n            2,\n            (4, 4),\n            dilation_rate=(2, 2),\n            activation=\"sigmoid\",\n            padding=\"same\",\n            kernel_initializer=kernel_initializer,\n        )((batch12))\n        output = Multiply(name=output_name)([up7, input_tensor])\n        return output\n    return Conv2D(\n        2,\n        (4, 4),\n        dilation_rate=(2, 2),\n        padding=\"same\",\n        kernel_initializer=kernel_initializer,\n    )((batch12))"
            }
        ],
        "third_party": [
            "logit_mask_list.append",
            "Softmax",
            "tf.stack",
            "Multiply"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def compute_file_checksum(path)",
        "start_line": "40",
        "end_line": "56",
        "file_path": "spleeter/model/provider/github.py",
        "docstring": "The function computes the SHA-256 checksum of a file.\\nIt initializes a SHA-256 hash object and reads the file in chunks, updating the hash with each chunk.\\nFinally, it returns the hexadecimal representation of the computed hash.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "400b19ce8520",
        "ground_truth": "def compute_file_checksum(path):\n    \"\"\"\n    Computes given path file sha256.\n     Parameters:\n        path (str):\n            Path of the file to compute checksum for.\n     Returns:\n        str:\n            File checksum.\n    \"\"\"\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as stream:\n        for chunk in iter(lambda: stream.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()",
        "import_statements": [
            "import hashlib",
            "import os",
            "import tarfile",
            "from os import environ",
            "from tempfile import NamedTemporaryFile",
            "from typing import Dict",
            "import httpx"
        ],
        "reference_api": [
            "stream.read",
            "sha256.hexdigest",
            "sha256.update",
            "hashlib.sha256",
            "iter",
            "open"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "stream.read",
            "sha256.update",
            "sha256.hexdigest"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def checksum(self, name: str) -> str",
        "start_line": "99",
        "end_line": "129",
        "file_path": "spleeter/model/provider/github.py",
        "docstring": "The function retrieves the checksum for a specified model name.\\nIt constructs a URL using host, repository, release path, and checksum index.\\nIt sends an HTTP GET request to this URL and checks for a successful response.\\nIt parses the response as JSON and checks if the model name is in the index.\\nIf the model name is not found, it raises a ValueError.\\nThe function returns the checksum for the specified model name.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5f2e9c6e15f1",
        "ground_truth": "def checksum(self, name: str) -> str:\n    \"\"\"\n    Downloads and returns reference checksum for the given model name.\n    Parameters:\n        name (str):\n            Name of the model to get checksum for.\n    Returns:\n        str:\n            Checksum of the required model.\n    Raises:\n        ValueError:\n            If the given model name is not indexed.\n    \"\"\"\n    url: str = \"/\".join(\n        (\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.CHECKSUM_INDEX,\n        )\n    )\n    response: httpx.Response = httpx.get(url)\n    response.raise_for_status()\n    index: Dict = response.json()\n    if name not in index:\n        raise ValueError(f\"No checksum for model {name}\")\n    return index[name]",
        "import_statements": [
            "import hashlib",
            "import os",
            "import tarfile",
            "from os import environ",
            "from tempfile import NamedTemporaryFile",
            "from typing import Dict",
            "import httpx"
        ],
        "reference_api": [
            "join",
            "response.json",
            "ValueError",
            "httpx.get",
            "response.raise_for_status"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "join",
            "httpx.get",
            "response.raise_for_status",
            "response.json"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def download(self, name: str, path: str) -> None:",
        "start_line": "131",
        "end_line": "163",
        "file_path": "spleeter/model/provider/github.py",
        "docstring": "The function downloads a model archive from a specified URL, validates its checksum, and extracts its contents to a given path.\\nIt constructs the download URL, logs the download initiation, and streams the file using HTTP/2.\\nAfter downloading, it checks the file's integrity by comparing its checksum with the expected value.\\nIf the checksum is valid, it extracts the archive to the specified path and deletes the temporary file.\\nFinally, it logs the extraction completion.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9312c8b6032e",
        "ground_truth": "def download(self, name: str, path: str) -> None:\n    \"\"\"\n    Download model denoted by the given name to disk.\n    Parameters:\n        name (str):\n            Name of the model to download.\n        path (str):\n            Path of the directory to save model into.\n    \"\"\"\n    url: str = \"/\".join(\n        (self._host, self._repository, self.RELEASE_PATH, self._release, name)\n    )\n    url = f\"{url}.tar.gz\"\n    logger.info(f\"Downloading model archive {url}\")\n    with httpx.Client(http2=True) as client:\n        with client.stream(\"GET\", url) as response:\n            response.raise_for_status()\n            archive = NamedTemporaryFile(delete=False)\n            try:\n                with archive as stream:\n                    for chunk in response.iter_raw():\n                        stream.write(chunk)\n                logger.info(\"Validating archive checksum\")\n                checksum: str = compute_file_checksum(archive.name)\n                if checksum != self.checksum(name):\n                    raise IOError(\"Downloaded file is corrupted, please retry\")\n                logger.info(f\"Extracting downloaded {name} archive\")\n                with tarfile.open(name=archive.name) as tar:\n                    tar.extractall(path=path)\n            finally:\n                os.unlink(archive.name)\n    logger.info(f\"{name} model file(s) extracted\")",
        "import_statements": [
            "import hashlib",
            "import os",
            "import tarfile",
            "from os import environ",
            "from tempfile import NamedTemporaryFile",
            "from typing import Dict",
            "import httpx"
        ],
        "reference_api": [
            "join",
            "tar.extractall",
            "compute_file_checksum",
            "os.unlink",
            "client.stream",
            "self.checksum",
            "httpx.Client",
            "tarfile.open",
            "logger.info",
            "stream.write",
            "response.iter_raw",
            "response.raise_for_status",
            "IOError",
            "NamedTemporaryFile"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "compute_file_checksum",
                "code": "def compute_file_checksum(path):\n    \"\"\"\n    Computes given path file sha256.\n\n    Parameters:\n        path (str):\n            Path of the file to compute checksum for.\n\n    Returns:\n        str:\n            File checksum.\n    \"\"\"\n    sha256 = hashlib.sha256()\n    with open(path, \"rb\") as stream:\n        for chunk in iter(lambda: stream.read(4096), b\"\"):\n            sha256.update(chunk)\n    return sha256.hexdigest()"
            },
            {
                "name": "self.checksum",
                "code": "def checksum(self, name: str) -> str:\n        \"\"\"\n        Downloads and returns reference checksum for the given model name.\n\n        Parameters:\n            name (str):\n                Name of the model to get checksum for.\n\n        Returns:\n            str:\n                Checksum of the required model.\n\n        Raises:\n            ValueError:\n                If the given model name is not indexed.\n        \"\"\"\n        url: str = \"/\".join(\n            (\n                self._host,\n                self._repository,\n                self.RELEASE_PATH,\n                self._release,\n                self.CHECKSUM_INDEX,\n            )\n        )\n        response: httpx.Response = httpx.get(url)\n        response.raise_for_status()\n        index: Dict = response.json()\n        if name not in index:\n            raise ValueError(f\"No checksum for model {name}\")\n        return index[name]"
            }
        ],
        "third_party": [
            "join",
            "httpx.Client",
            "client.stream",
            "response.raise_for_status",
            "NamedTemporaryFile",
            "response.iter_raw",
            "stream.write",
            "tar.extractall"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def load_configuration(descriptor: str) -> Dict",
        "start_line": "20",
        "end_line": "51",
        "file_path": "spleeter/utils/configuration.py",
        "docstring": "The function loads a configuration based on the provided descriptor string.\\nIf the descriptor indicates an embedded configuration, it extracts the name and reads the corresponding JSON resource.\\nIf the descriptor is a standard file path, it checks for file existence and reads the JSON content from the file.\\nThe function raises an error if the embedded configuration or file is not found, and returns the loaded configuration as a dictionary.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ee9493f37d77",
        "ground_truth": "def load_configuration(descriptor: str) -> Dict:\n    \"\"\"\n    Load configuration from the given descriptor.\n    Could be either a `spleeter:` prefixed embedded configuration name\n    or a file system path to read configuration from.\n     Parameters:\n        descriptor (str):\n            Configuration descriptor to use for lookup.\n     Returns:\n        Dict:\n            Loaded description as dict.\n     Raises:\n        ValueError:\n            If required embedded configuration does not exists.\n        SpleeterError:\n            If required configuration file does not exists.\n    \"\"\"\n    # Embedded configuration reading.\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX) :]\n        if not loader.is_resource(resources, f\"{name}.json\"):\n            raise SpleeterError(f\"No embedded configuration {name} found\")\n        with loader.open_text(resources, f\"{name}.json\") as stream:\n            return json.load(stream)\n    # Standard file reading.\n    if not exists(descriptor):\n        raise SpleeterError(f\"Configuration file {descriptor} not found\")\n    with open(descriptor, \"r\") as stream:\n        return json.load(stream)",
        "import_statements": [
            "import json",
            "from os.path import exists",
            "from typing import Dict"
        ],
        "reference_api": [
            "exists",
            "loader.open_text",
            "len",
            "SpleeterError",
            "descriptor.startswith",
            "loader.is_resource",
            "open",
            "json.load"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "descriptor.startswith",
            "loader.is_resource",
            "SpleeterError",
            "loader.open_text",
            "exists",
            "SpleeterError"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]",
        "start_line": "21",
        "end_line": "67",
        "file_path": "spleeter/utils/tensor.py",
        "docstring": "The function applies a given function to concatenated tensors from a dictionary.\\nIt checks if the concatenation axis is either 0 or 1, raising an error if not.\\nIt concatenates the tensors along the specified axis, applies the function to the concatenated tensor, and then splits the processed tensor back into the original dictionary format based on the concatenation axis.\\nThe function returns the dictionary of processed tensors.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "546134b67fbc",
        "ground_truth": "def sync_apply(\n    tensor_dict: Dict[str, tf.Tensor], func: Callable, concat_axis: int = 1\n) -> Dict[str, tf.Tensor]:\n    \"\"\"\n    Return a function that applies synchronously the provided func on the\n    provided dictionnary of tensor. This means that func is applied to the\n    concatenation of the tensors in tensor_dict. This is useful for\n    performing random operation that needs the same drawn value on multiple\n    tensor, such as a random time-crop on both input data and label (the\n    same crop should be applied to both input data and label, so random\n    crop cannot be applied separately on each of them).\n     Note:\n        All tensor are assumed to be the same shape.\n     Parameters:\n        tensor_dict (Dict[str, tf.Tensor]):\n            A dictionary of tensor.\n        func (Callable):\n            Function to be applied to the concatenation of the tensors in\n            `tensor_dict`.\n        concat_axis (int):\n            (Optional) The axis on which to perform the concatenation.\n     Returns:\n        Dict[str, tf.Tensor]:\n            Processed tensors dictionary with the same name (keys) as input\n            tensor_dict.\n    \"\"\"\n    if concat_axis not in {0, 1}:\n        raise NotImplementedError(\n            \"Function only implemented for concat_axis equal to 0 or 1\"\n        )\n    tensor_list = list(tensor_dict.values())\n    concat_tensor = tf.concat(tensor_list, concat_axis)\n    processed_concat_tensor = func(concat_tensor)\n    tensor_shape = tf.shape(list(tensor_dict.values())[0])\n    D = tensor_shape[concat_axis]\n    if concat_axis == 0:\n        return {\n            name: processed_concat_tensor[index * D : (index + 1) * D, :, :]\n            for index, name in enumerate(tensor_dict)\n        }\n    return {\n        name: processed_concat_tensor[:, index * D : (index + 1) * D, :]\n        for index, name in enumerate(tensor_dict)\n    }",
        "import_statements": [
            "from typing import Any, Callable, Dict"
        ],
        "reference_api": [
            "list",
            "NotImplementedError",
            "tf.concat",
            "tensor_dict.values",
            "func",
            "enumerate",
            "tf.shape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tensor_dict.values",
            "tf.concat",
            "func",
            "tf.shape",
            "tensor_dict.values"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor",
        "start_line": "70",
        "end_line": "85",
        "file_path": "spleeter/utils/tensor.py",
        "docstring": "The function normalizes a float32 tensor to the uint8 range (0-255).\\nIt computes the minimum and maximum values of the tensor and scales the tensor values to this range.\\nIt returns a dictionary containing the scaled uint8 tensor, the original minimum value, and the original maximum value.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "df48e78d46d2",
        "ground_truth": "def from_float32_to_uint8(\n    tensor: tf.Tensor,\n    tensor_key: str = \"tensor\",\n    min_key: str = \"min\",\n    max_key: str = \"max\",\n) -> tf.Tensor:\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    return {\n        tensor_key: tf.cast(\n            (tensor - tensor_min) / (tensor_max - tensor_min + 1e-16) * 255.9999,\n            dtype=tf.uint8,\n        ),\n        min_key: tensor_min,\n        max_key: tensor_max,\n    }",
        "import_statements": [
            "from typing import Any, Callable, Dict"
        ],
        "reference_api": [
            "tf.cast",
            "tf.reduce_min",
            "tf.reduce_max"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.reduce_min",
            "tf.reduce_max",
            "tf.cast"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor",
        "start_line": "96",
        "end_line": "126",
        "file_path": "spleeter/utils/tensor.py",
        "docstring": "The function pads and partitions a tensor into segments of a specified length.\\nIt calculates the padding needed to make the tensor length a multiple of the segment length, pads the tensor, and then reshapes it into a new tensor with the specified segment length.\\nThe function returns the reshaped tensor, where the first dimension represents the number of segments.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7f479e5de492",
        "ground_truth": "def pad_and_partition(tensor: tf.Tensor, segment_len: int) -> tf.Tensor:\n    \"\"\"\n    Pad and partition a tensor into segment of len `segment_len`\n    along the first dimension. The tensor is padded with 0 in order\n    to ensure that the first dimension is a multiple of `segment_len`.\n     Examples:\n    ```python\n    >>> tensor = [[1, 2, 3], [4, 5, 6]]\n    >>> segment_len = 2\n    >>> pad_and_partition(tensor, segment_len)\n    [[[1, 2], [4, 5]], [[3, 0], [6, 0]]]\n    ````\n     Parameters:\n        tensor (tf.Tensor):\n            Tensor of known fixed rank\n        segment_len (int):\n            Segment length.\n     Returns:\n        tf.Tensor:\n            Padded and partitioned tensor.\n    \"\"\"\n    tensor_size = tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    pad_size = tf.math.floormod(segment_len - tensor_size, segment_len)\n    padded = tf.pad(tensor, [[0, pad_size]] + [[0, 0]] * (len(tensor.shape) - 1))\n    split = (tf.shape(padded)[0] + segment_len - 1) // segment_len\n    return tf.reshape(\n        padded, tf.concat([[split, segment_len], tf.shape(padded)[1:]], axis=0)\n    )",
        "import_statements": [
            "from typing import Any, Callable, Dict"
        ],
        "reference_api": [
            "floormod",
            "tf.reshape",
            "len",
            "tf.pad",
            "tf.concat",
            "tf.shape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "floormod",
            "tf.shape",
            "floormod",
            "tf.pad",
            "tf.shape",
            "tf.reshape",
            "tf.concat",
            "tf.shape"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def pad_and_reshape(instr_spec, frame_length, F) -> Any",
        "start_line": "129",
        "end_line": "138",
        "file_path": "spleeter/utils/tensor.py",
        "docstring": "The function pads and reshapes an input spectrogram tensor.\\nIt calculates the number of extra rows needed for padding and creates an extension of zeros.\\nThe function concatenates this extension to the original spectrogram along the frequency axis.\\nIt then reshapes the extended spectrogram by merging the first two dimensions and returns the processed spectrogram.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1789fae7253f",
        "ground_truth": "def pad_and_reshape(instr_spec, frame_length, F) -> Any:\n    spec_shape = tf.shape(instr_spec)\n    extension_row = tf.zeros((spec_shape[0], spec_shape[1], 1, spec_shape[-1]))\n    n_extra_row = (frame_length) // 2 + 1 - F\n    extension = tf.tile(extension_row, [1, 1, n_extra_row, 1])\n    extended_spec = tf.concat([instr_spec, extension], axis=2)\n    old_shape = tf.shape(extended_spec)\n    new_shape = tf.concat([[old_shape[0] * old_shape[1]], old_shape[2:]], axis=0)\n    processed_instr_spec = tf.reshape(extended_spec, new_shape)\n    return processed_instr_spec",
        "import_statements": [
            "from typing import Any, Callable, Dict"
        ],
        "reference_api": [
            "tf.zeros",
            "tf.tile",
            "tf.reshape",
            "tf.concat",
            "tf.shape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.shape",
            "tf.zeros",
            "tf.tile",
            "tf.concat",
            "tf.shape",
            "tf.concat",
            "tf.reshape"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def dataset_from_csv(csv_path: str, **kwargs) -> Any",
        "start_line": "141",
        "end_line": "156",
        "file_path": "spleeter/utils/tensor.py",
        "docstring": "The function creates a TensorFlow dataset from a CSV file.\\nIt reads the CSV file into a pandas DataFrame, then converts the DataFrame columns into a TensorFlow dataset by slicing the tensor values for each column.\\nThe function returns the created TensorFlow dataset.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8a4f4658d97c",
        "ground_truth": "def dataset_from_csv(csv_path: str, **kwargs) -> Any:\n    \"\"\"\n    Load dataset from a CSV file using Pandas.\n    kwargs if any are forwarded to the `pandas.read_csv` function.\n     Parameters:\n        csv_path (str):\n            Path of the CSV file to load dataset from.\n     Returns:\n        Any:\n            Loaded dataset.\n    \"\"\"\n    df = pd.read_csv(csv_path, **kwargs)\n    dataset = tf.data.Dataset.from_tensor_slices({key: df[key].values for key in df})\n    return dataset",
        "import_statements": [
            "from typing import Any, Callable, Dict"
        ],
        "reference_api": [
            "pd.read_csv",
            "from_tensor_slices"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "pd.read_csv",
            "from_tensor_slices"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool",
        "start_line": "159",
        "end_line": "181",
        "file_path": "spleeter/utils/tensor.py",
        "docstring": "The function checks if a TensorFlow tensor matches a specified target shape.\\nIt initializes a boolean result as True and iterates over the target shape dimensions.\\nFor each dimension with a specified length, it updates the result by logically AND-ing it with the comparison between the tensor's corresponding dimension and the target length.\\nThe function returns the final boolean result indicating if the tensor matches the target shape.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9a16e81b182f",
        "ground_truth": "def check_tensor_shape(tensor_tf: tf.Tensor, target_shape: Any) -> bool:\n    \"\"\"\n    Return a Tensorflow boolean graph that indicates whether\n    sample[features_key] has the specified target shape.\n    Only check not None entries of target_shape.\n     Parameters:\n        tensor_tf (tensorflow.Tensor):\n            Tensor to check shape for.\n        target_shape (Any):\n            Target shape to compare tensor to.\n     Returns:\n        bool:\n            `True` if shape is valid, `False` otherwise (as TF boolean).\n    \"\"\"\n    result = tf.constant(True)\n    for i, target_length in enumerate(target_shape):\n        if target_length:\n            result = tf.logical_and(\n                result, tf.equal(tf.constant(target_length), tf.shape(tensor_tf)[i])\n            )\n    return result",
        "import_statements": [
            "from typing import Any, Callable, Dict"
        ],
        "reference_api": [
            "tf.constant",
            "tf.logical_and",
            "enumerate",
            "tf.equal",
            "tf.shape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.constant",
            "tf.logical_and",
            "tf.equal",
            "tf.constant",
            "tf.shape"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any",
        "start_line": "60",
        "end_line": "93",
        "file_path": "spleeter/dataset.py",
        "docstring": "The function creates a training dataset using provided audio parameters, an audio adapter, and an audio path.\\nIt initializes a DatasetBuilder with these parameters, specifying chunk duration and random seed.\\nThe function then calls the build method of the builder with additional parameters like train_csv path, cache directory, batch size, number of chunks per song, and options for data augmentation and caching.\\nFinally, it returns the built dataset.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "49d5d3084b3a",
        "ground_truth": "def get_training_dataset(\n    audio_params: Dict, audio_adapter: AudioAdapter, audio_path: str\n) -> Any:\n    \"\"\"\n    Builds training dataset.\n     Parameters:\n        audio_params (Dict):\n            Audio parameters.\n        audio_adapter (AudioAdapter):\n            Adapter to load audio from.\n        audio_path (str):\n            Path of directory containing audio.\n     Returns:\n        Any:\n            Built dataset.\n    \"\"\"\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=audio_params.get(\"chunk_duration\", 20.0),\n        random_seed=audio_params.get(\"random_seed\", 0),\n    )\n    return builder.build(\n        str(audio_params.get(\"train_csv\")),\n        cache_directory=audio_params.get(\"training_cache\"),\n        batch_size=audio_params.get(\"batch_size\", 8),\n        n_chunks_per_song=audio_params.get(\"n_chunks_per_song\", 2),\n        random_data_augmentation=False,\n        convert_to_uint=True,\n        wait_for_cache=False,\n    )",
        "import_statements": [
            "import os",
            "import time",
            "from os.path import exists",
            "from os.path import sep as SEPARATOR",
            "from typing import Any, Dict, List, Optional, Tuple"
        ],
        "reference_api": [
            "DatasetBuilder",
            "builder.build",
            "str",
            "audio_params.get"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "builder.build",
                "code": "def build(\n        self,\n        csv_path: str,\n        batch_size: int = 8,\n        shuffle: bool = True,\n        convert_to_uint: bool = True,\n        random_data_augmentation: bool = False,\n        random_time_crop: bool = True,\n        infinite_generator: bool = True,\n        cache_directory: Optional[str] = None,\n        wait_for_cache: bool = False,\n        num_parallel_calls: int = 4,\n        n_chunks_per_song: int = 2,\n    ) -> Any:\n        dataset = dataset_from_csv(csv_path)\n        dataset = self.compute_segments(dataset, n_chunks_per_song)\n        # Shuffle data\n        if shuffle:\n            dataset = dataset.shuffle(\n                buffer_size=200000,\n                seed=self._random_seed,\n                # useless since it is cached :\n                reshuffle_each_iteration=True,\n            )\n        # Expand audio path.\n        dataset = dataset.map(self.expand_path)\n        # Load waveform, compute spectrogram, and filtering error,\n        # K bins frequencies, and waveform.\n        N = num_parallel_calls\n        for instrument in self.instruments:\n            dataset = (\n                dataset.map(instrument.load_waveform, num_parallel_calls=N)\n                .filter(self.filter_error)\n                .map(instrument.compute_spectrogram, num_parallel_calls=N)\n                .map(instrument.filter_frequencies)\n            )\n        dataset = dataset.map(self.filter_waveform)\n        # Convert to uint before caching in order to save space.\n        if convert_to_uint:\n            for instrument in self.instruments:\n                dataset = dataset.map(instrument.convert_to_uint)\n        dataset = self.cache(dataset, cache_directory, wait_for_cache)\n        # Check for INFINITY (should not happen)\n        for instrument in self.instruments:\n            dataset = dataset.filter(instrument.filter_infinity)\n        # Repeat indefinitly\n        if infinite_generator:\n            dataset = dataset.repeat(count=-1)\n        # Ensure same size for vocals and mix spectrograms.\n        # NOTE: could be done before caching ?\n        dataset = dataset.map(self.harmonize_spectrogram)\n        # Filter out too short segment.\n        # NOTE: could be done before caching ?\n        dataset = dataset.filter(self.filter_short_segments)\n        # Random time crop of 11.88s\n        if random_time_crop:\n            dataset = dataset.map(self.random_time_crop, num_parallel_calls=N)\n        else:\n            # frame_duration = 11.88/T\n            # take central segment (for validation)\n            for instrument in self.instruments:\n                dataset = dataset.map(instrument.time_crop)\n        # Post cache shuffling. Done where the data are the lightest:\n        # after croping but before converting back to float.\n        if shuffle:\n            dataset = dataset.shuffle(\n                buffer_size=256, seed=self._random_seed, reshuffle_each_iteration=True\n            )\n        # Convert back to float32\n        if convert_to_uint:\n            for instrument in self.instruments:\n                dataset = dataset.map(\n                    instrument.convert_to_float32, num_parallel_calls=N\n                )\n        M = 8  # Parallel call post caching.\n        # Must be applied with the same factor on mix and vocals.\n        if random_data_augmentation:\n            dataset = dataset.map(self.random_time_stretch, num_parallel_calls=M).map(\n                self.random_pitch_shift, num_parallel_calls=M\n            )\n        # Filter by shape (remove badly shaped tensors).\n        for instrument in self.instruments:\n            dataset = dataset.filter(instrument.filter_shape).map(\n                instrument.reshape_spectrogram\n            )\n        # Select features and annotation.\n        dataset = dataset.map(self.map_features)\n        # Make batch (done after selection to avoid\n        # error due to unprocessed instrument spectrogram batching).\n        dataset = dataset.batch(batch_size)\n        return dataset"
            }
        ],
        "third_party": [
            "DatasetBuilder",
            "audio_params.get",
            "audio_params.get",
            "audio_params.get",
            "audio_params.get",
            "audio_params.get",
            "audio_params.get"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def instruments(self) -> Any",
        "start_line": "472",
        "end_line": "487",
        "file_path": "spleeter/dataset.py",
        "docstring": "The function initializes instrument dataset builders if they are not already created.\\nIt iterates over a list of instruments, creating and storing an InstrumentDatasetBuilder for each.\\nThe function then yields each builder from the list of instrument builders.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fe8bbbba06e3",
        "ground_truth": "def instruments(self) -> Any:\n    \"\"\"\n    Instrument dataset builder generator.\n    Yields:\n        Any:\n            InstrumentBuilder instance.\n    \"\"\"\n    if self._instrument_builders is None:\n        self._instrument_builders = []\n        for instrument in self._instruments:\n            self._instrument_builders.append(\n                InstrumentDatasetBuilder(self, instrument)\n            )\n    for builder in self._instrument_builders:\n        yield builder",
        "import_statements": [
            "import os",
            "import time",
            "from os.path import exists",
            "from os.path import sep as SEPARATOR",
            "from typing import Any, Dict, List, Optional, Tuple"
        ],
        "reference_api": [
            "append",
            "InstrumentDatasetBuilder"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "append",
            "InstrumentDatasetBuilder"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any",
        "start_line": "489",
        "end_line": "515",
        "file_path": "spleeter/dataset.py",
        "docstring": "The function caches a dataset if a cache path is provided.\\nIf wait is True, it repeatedly checks for the existence of the cache index file, logging a message and waiting for a specified period if not found.\\nIt ensures the cache directory exists, then caches the dataset to the specified path.\\nIf no cache path is provided, it returns the original dataset.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "438b57fd3413",
        "ground_truth": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any:\n    \"\"\"\n    Cache the given dataset if cache is enabled. Eventually waits for\n    cache to be available (useful if another process is already\n    computing cache) if provided wait flag is `True`.\n    Parameters:\n        dataset (Any):\n            Dataset to be cached if cache is required.\n        cache (str):\n            Path of cache directory to be used, None if no cache.\n        wait (bool):\n            If caching is enabled, True is cache should be waited.\n    Returns:\n        Any:\n            Cached dataset if needed, original dataset otherwise.\n    \"\"\"\n    if cache is not None:\n        if wait:\n            while not exists(f\"{cache}.index\"):\n                logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                time.sleep(self.WAIT_PERIOD)\n        cache_path = os.path.split(cache)[0]\n        os.makedirs(cache_path, exist_ok=True)\n        return dataset.cache(cache)\n    return dataset",
        "import_statements": [
            "import os",
            "import time",
            "from os.path import exists",
            "from os.path import sep as SEPARATOR",
            "from typing import Any, Dict, List, Optional, Tuple"
        ],
        "reference_api": [
            "exists",
            "dataset.cache",
            "split",
            "logger.info",
            "time.sleep",
            "os.makedirs"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "dataset.cache",
                "code": "def cache(self, dataset: Any, cache: Optional[str], wait: bool) -> Any:\n        \"\"\"\n        Cache the given dataset if cache is enabled. Eventually waits for\n        cache to be available (useful if another process is already\n        computing cache) if provided wait flag is `True`.\n\n        Parameters:\n            dataset (Any):\n                Dataset to be cached if cache is required.\n            cache (str):\n                Path of cache directory to be used, None if no cache.\n            wait (bool):\n                If caching is enabled, True is cache should be waited.\n\n        Returns:\n            Any:\n                Cached dataset if needed, original dataset otherwise.\n        \"\"\"\n        if cache is not None:\n            if wait:\n                while not exists(f\"{cache}.index\"):\n                    logger.info(f\"Cache not available, wait {self.WAIT_PERIOD}\")\n                    time.sleep(self.WAIT_PERIOD)\n            cache_path = os.path.split(cache)[0]\n            os.makedirs(cache_path, exist_ok=True)\n            return dataset.cache(cache)\n        return dataset"
            }
        ],
        "third_party": [
            "exists",
            "split"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor",
        "start_line": "67",
        "end_line": "93",
        "file_path": "spleeter/separator.py",
        "docstring": "The function creates a TensorFlow estimator using provided parameters and a Model Weight File (MWF) flag.\\nIt loads the model directory using a ModelProvider and updates the parameters.\\nIt sets up the session configuration to limit GPU memory usage and creates a RunConfig with this session configuration.\\nFinally, it initializes and returns a TensorFlow estimator using the specified model function, model directory, parameters, and configuration.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8838a3a09a66",
        "ground_truth": "def create_estimator(params: Dict, MWF: bool) -> tf.Tensor:\n    \"\"\"\n    Initialize tensorflow estimator that will perform separation\n     Parameters:\n        params (Dict):\n            A dictionary of parameters for building the model\n        MWF (bool):\n            Wiener filter enabled?\n     Returns:\n        tf.Tensor:\n            A tensorflow estimator\n    \"\"\"\n    # Load model.\n    provider: ModelProvider = ModelProvider.default()\n    params[\"model_dir\"] = provider.get(params[\"model_dir\"])\n    params[\"MWF\"] = MWF\n    # Setup config\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.7\n    config = tf.estimator.RunConfig(session_config=session_config)\n    # Setup estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn, model_dir=params[\"model_dir\"], params=params, config=config\n    )\n    return estimator",
        "import_statements": [
            "import atexit",
            "import os",
            "from multiprocessing import Pool",
            "from os.path import basename, dirname, join, splitext",
            "from typing import Any, Dict, Generator, List, Optional"
        ],
        "reference_api": [
            "Estimator",
            "provider.get",
            "ConfigProto",
            "ModelProvider.default",
            "RunConfig"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ModelProvider.default",
            "provider.get",
            "ConfigProto",
            "RunConfig",
            "Estimator"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "deezer/spleeter",
        "function_declaration": "def _separate_tensorflow(\n        self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n    ) -> Dict",
        "start_line": "196",
        "end_line": "223",
        "file_path": "spleeter/separator.py",
        "docstring": "The function performs audio source separation using TensorFlow.\\nIt ensures the input waveform is stereo, then obtains a prediction generator.\\nThe data generator is updated with the waveform and audio descriptor.\\nA prediction is generated, and the \"audio_id\" key is removed from the prediction before returning it.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "779c812bf2cb",
        "ground_truth": "def _separate_tensorflow(\n    self, waveform: np.ndarray, audio_descriptor: AudioDescriptor\n) -> Dict:\n    \"\"\"\n    Performs source separation over the given waveform with tensorflow\n    backend.\n    Parameters:\n        waveform (np.ndarray):\n            Waveform to be separated (as a numpy array)\n        audio_descriptor (AudioDescriptor):\n            Audio descriptor to be used.\n    Returns:\n        Dict:\n            Separated waveforms.\n    \"\"\"\n    if not waveform.shape[-1] == 2:\n        waveform = to_stereo(waveform)\n    prediction_generator = self._get_prediction_generator()\n    # NOTE: update data in generator before performing separation.\n    self._data_generator.update_data(\n        {\"waveform\": waveform, \"audio_id\": np.array(audio_descriptor)}\n    )\n    # NOTE: perform separation.\n    prediction = next(prediction_generator)\n    prediction.pop(\"audio_id\")\n    return prediction",
        "import_statements": [
            "import atexit",
            "import os",
            "from multiprocessing import Pool",
            "from os.path import basename, dirname, join, splitext",
            "from typing import Any, Dict, Generator, List, Optional"
        ],
        "reference_api": [
            "to_stereo",
            "update_data",
            "next",
            "prediction.pop",
            "self._get_prediction_generator",
            "np.array"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._get_prediction_generator",
                "code": "def _get_prediction_generator(self) -> Generator:\n        \"\"\"\n        Lazy loading access method for internal prediction generator\n        returned by the predict method of a tensorflow estimator.\n\n        Returns:\n            Generator:\n                Generator of prediction.\n        \"\"\"\n        if self._prediction_generator is None:\n            estimator = create_estimator(self._params, self._MWF)\n\n            def get_dataset():\n                return tf.data.Dataset.from_generator(\n                    self._data_generator,\n                    output_types={\"waveform\": tf.float32, \"audio_id\": tf.string},\n                    output_shapes={\"waveform\": (None, 2), \"audio_id\": ()},\n                )\n\n            self._prediction_generator = estimator.predict(\n                get_dataset, yield_single_examples=False\n            )\n        return self._prediction_generator"
            },
            {
                "name": "update_data",
                "code": "def update_data(self, data) -> None:\n        \"\"\"Replace internal data.\"\"\"\n        self._current_data = data"
            }
        ],
        "third_party": [
            "to_stereo",
            "np.array",
            "prediction.pop"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "matterport/Mask_RCNN",
        "function_declaration": "def compute_backbone_shapes(config, image_shape)",
        "start_line": "71",
        "end_line": "85",
        "file_path": "mrcnn/model.py",
        "docstring": "The function computes the shapes of feature maps at different stages of a backbone network given an image shape and configuration.\\nIf the backbone is a callable function, it uses the configuration's COMPUTE_BACKBONE_SHAPE method to get the shapes.\\nOtherwise, it asserts that the backbone is either \"resnet50\" or \"resnet101\" and calculates the shapes based on the BACKBONE_STRIDES configuration using the image dimensions divided by each stride.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cf580cd2ed93",
        "ground_truth": "def compute_backbone_shapes(config, image_shape):\n    \"\"\"Computes the width and height of each stage of the backbone network.\n     Returns:\n        [N, (height, width)]. Where N is the number of stages\n    \"\"\"\n    if callable(config.BACKBONE):\n        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n     # Currently supports ResNet only\n    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n    return np.array(\n        [[int(math.ceil(image_shape[0] / stride)),\n            int(math.ceil(image_shape[1] / stride))]\n            for stride in config.BACKBONE_STRIDES])",
        "import_statements": [
            "import os",
            "import random",
            "import datetime",
            "import re",
            "import math",
            "import logging",
            "from collections import OrderedDict",
            "import multiprocessing",
            "import keras",
            "from mrcnn import utils",
            "from distutils.version import LooseVersion"
        ],
        "reference_api": [
            "int",
            "math.ceil",
            "callable",
            "config.COMPUTE_BACKBONE_SHAPE",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "config.COMPUTE_BACKBONE_SHAPE",
            "np.array"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "matterport/Mask_RCNN",
        "function_declaration": "def clip_boxes_graph(boxes, window)",
        "start_line": "237",
        "end_line": "252",
        "file_path": "mrcnn/model.py",
        "docstring": "The function clips bounding boxes to fit within a given window.\\nIt splits the window and box coordinates into individual components.\\nEach coordinate is clipped to lie within the window boundaries using TensorFlow operations.\\nThe clipped coordinates are concatenated back into a tensor and returned with a fixed shape.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ebd61a2e59ca",
        "ground_truth": "def clip_boxes_graph(boxes, window):\n    \"\"\"\n    boxes: [N, (y1, x1, y2, x2)]\n    window: [4] in the form y1, x1, y2, x2\n    \"\"\"\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped",
        "import_statements": [
            "import os",
            "import random",
            "import datetime",
            "import re",
            "import math",
            "import logging",
            "from collections import OrderedDict",
            "import multiprocessing",
            "import keras",
            "from mrcnn import utils",
            "from distutils.version import LooseVersion"
        ],
        "reference_api": [
            "tf.concat",
            "tf.split",
            "clipped.set_shape",
            "tf.maximum",
            "tf.minimum"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.split",
            "tf.split",
            "tf.maximum",
            "tf.minimum",
            "tf.maximum",
            "tf.minimum",
            "tf.maximum",
            "tf.minimum",
            "tf.maximum",
            "tf.minimum",
            "tf.concat",
            "clipped.set_shape"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "matterport/Mask_RCNN",
        "function_declaration": "def rpn_graph(feature_map, anchors_per_location, anchor_stride)",
        "start_line": "830",
        "end_line": "871",
        "file_path": "mrcnn/model.py",
        "docstring": "The function defines the Region Proposal Network (RPN) graph for object detection.\\nIt applies a shared convolutional layer to the feature map, followed by two separate convolutional layers for class scores and bounding box predictions.\\nThe class scores are reshaped and passed through a softmax activation to obtain class probabilities.\\nThe bounding box predictions are also reshaped.\\nThe function returns the class logits, class probabilities, and bounding box predictions.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8e0411587a70",
        "ground_truth": "def rpn_graph(feature_map, anchors_per_location, anchor_stride):\n    \"\"\"Builds the computation graph of Region Proposal Network.\n     feature_map: backbone features [batch, height, width, depth]\n    anchors_per_location: number of anchors per pixel in the feature map\n    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n                   every pixel in the feature map), or 2 (every other pixel).\n     Returns:\n        rpn_class_logits: [batch, H * W * anchors_per_location, 2] Anchor classifier logits (before softmax)\n        rpn_probs: [batch, H * W * anchors_per_location, 2] Anchor classifier probabilities.\n        rpn_bbox: [batch, H * W * anchors_per_location, (dy, dx, log(dh), log(dw))] Deltas to be\n                  applied to anchors.\n    \"\"\"\n    # TODO: check if stride of 2 causes alignment issues if the feature map\n    # is not even.\n    # Shared convolutional base of the RPN\n    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n                       strides=anchor_stride,\n                       name='rpn_conv_shared')(feature_map)\n     # Anchor Score. [batch, height, width, anchors per location * 2].\n    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n                  activation='linear', name='rpn_class_raw')(shared)\n     # Reshape to [batch, anchors, 2]\n    rpn_class_logits = KL.Lambda(\n        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n     # Softmax on last dimension of BG/FG.\n    rpn_probs = KL.Activation(\n        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n     # Bounding box refinement. [batch, H, W, anchors per location * depth]\n    # where depth is [x, y, log(w), log(h)]\n    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n                  activation='linear', name='rpn_bbox_pred')(shared)\n     # Reshape to [batch, anchors, 4]\n    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n     return [rpn_class_logits, rpn_probs, rpn_bbox]",
        "import_statements": [
            "import os",
            "import random",
            "import datetime",
            "import re",
            "import math",
            "import logging",
            "from collections import OrderedDict",
            "import multiprocessing",
            "import keras",
            "from mrcnn import utils",
            "from distutils.version import LooseVersion"
        ],
        "reference_api": [
            "KL.Conv2D",
            "KL.Activation",
            "KL.Lambda",
            "tf.reshape",
            "tf.shape"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "KL.Conv2D",
            "KL.Conv2D",
            "KL.Lambda",
            "tf.reshape",
            "tf.shape",
            "KL.Activation",
            "KL.Conv2D",
            "KL.Lambda",
            "tf.reshape",
            "tf.shape"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "matterport/Mask_RCNN",
        "function_declaration": "def smooth_l1_loss(y_true, y_pred)",
        "start_line": "1012",
        "end_line": "1019",
        "file_path": "mrcnn/model.py",
        "docstring": "The function computes the Smooth L1 loss between true and predicted values.\\nIt calculates the absolute difference and creates a mask for values less than one.\\nThe loss is computed using a combination of squared differences for small values and linear differences for large values.\\nThe function returns the calculated loss.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "70777c9261d8",
        "ground_truth": "def smooth_l1_loss(y_true, y_pred):\n    \"\"\"Implements Smooth-L1 loss.\n    y_true and y_pred are typically: [N, 4], but could be any shape.\n    \"\"\"\n    diff = K.abs(y_true - y_pred)\n    less_than_one = K.cast(K.less(diff, 1.0), \"float32\")\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss",
        "import_statements": [
            "import os",
            "import random",
            "import datetime",
            "import re",
            "import math",
            "import logging",
            "from collections import OrderedDict",
            "import multiprocessing",
            "import keras",
            "from mrcnn import utils",
            "from distutils.version import LooseVersion"
        ],
        "reference_api": [
            "K.cast",
            "K.less",
            "K.abs"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "K.abs",
            "K.cast",
            "K.less"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "matterport/Mask_RCNN",
        "function_declaration": "def build_model(x_train, num_classes)",
        "start_line": "128",
        "end_line": "144",
        "file_path": "mrcnn/parallel_model.py",
        "docstring": "The function builds a convolutional neural network model for image classification.\\nIt starts by resetting the TensorFlow graph and then defines the input layer with the shape of the training data.\\nThe model consists of two convolutional layers with ReLU activation, followed by a max-pooling layer.\\nThe output is flattened and passed through a dense layer with ReLU activation, followed by a dense layer with softmax activation for classification.\\nThe function returns the compiled model.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0e747b604087",
        "ground_truth": "def build_model(x_train, num_classes):\n    # Reset default graph. Keras leaves old ops in the graph,\n    # which are ignored for execution but clutter graph\n    # visualization in TensorBoard.\n    tf.reset_default_graph()\n    inputs = KL.Input(shape=x_train.shape[1:], name=\"input_image\")\n    x = KL.Conv2D(32, (3, 3), activation='relu', padding=\"same\",\n                  name=\"conv1\")(inputs)\n    x = KL.Conv2D(64, (3, 3), activation='relu', padding=\"same\",\n                  name=\"conv2\")(x)\n    x = KL.MaxPooling2D(pool_size=(2, 2), name=\"pool1\")(x)\n    x = KL.Flatten(name=\"flat1\")(x)\n    x = KL.Dense(128, activation='relu', name=\"dense1\")(x)\n    x = KL.Dense(num_classes, activation='softmax', name=\"dense2\")(x)\n    return KM.Model(inputs, x, \"digit_classifier_model\")",
        "import_statements": [],
        "reference_api": [
            "KL.Conv2D",
            "KL.Input",
            "KM.Model",
            "tf.reset_default_graph",
            "KL.Flatten",
            "KL.Dense",
            "KL.MaxPooling2D"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "tf.reset_default_graph",
            "KL.Input",
            "KL.Conv2D",
            "KL.Conv2D",
            "KL.MaxPooling2D",
            "KL.Flatten",
            "KL.Dense",
            "KL.Dense",
            "KM.Model"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "matterport/Mask_RCNN",
        "function_declaration": "def extract_bboxes(mask)",
        "start_line": "34",
        "end_line": "57",
        "file_path": "mrcnn/utils.py",
        "docstring": "The function extracts bounding boxes from a mask tensor.\\nIt initializes an array to store bounding boxes for each mask layer.\\nFor each mask layer, it calculates the horizontal and vertical indices where the mask is present.\\nIt then determines the coordinates of the bounding box based on these indices and stores them in the array.\\nIf no mask is present, it sets the bounding box to zeros.\\nThe function returns the array of bounding boxes as integers.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "813e3e2844f4",
        "ground_truth": "def extract_bboxes(mask):\n    \"\"\"Compute bounding boxes from masks.\n    mask: [height, width, num_instances]. Mask pixels are either 1 or 0.\n     Returns: bbox array [num_instances, (y1, x1, y2, x2)].\n    \"\"\"\n    boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)\n    for i in range(mask.shape[-1]):\n        m = mask[:, :, i]\n        # Bounding box.\n        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n        vertical_indicies = np.where(np.any(m, axis=1))[0]\n        if horizontal_indicies.shape[0]:\n            x1, x2 = horizontal_indicies[[0, -1]]\n            y1, y2 = vertical_indicies[[0, -1]]\n            # x2 and y2 should not be part of the box. Increment by 1.\n            x2 += 1\n            y2 += 1\n        else:\n            # No mask for this instance. Might happen due to\n            # resizing or cropping. Set bbox to zeros\n            x1, x2, y1, y2 = 0, 0, 0, 0\n        boxes[i] = np.array([y1, x1, y2, x2])\n    return boxes.astype(np.int32)",
        "import_statements": [
            "import sys",
            "import os",
            "import logging",
            "import math",
            "import random",
            "import scipy",
            "import skimage.color",
            "import skimage.io",
            "import skimage.transform",
            "import urllib.request",
            "import shutil",
            "import warnings",
            "from distutils.version import LooseVersion"
        ],
        "reference_api": [
            "np.where",
            "boxes.astype",
            "np.zeros",
            "np.any",
            "range",
            "np.array"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.zeros",
            "np.where",
            "np.any",
            "np.where",
            "np.any",
            "np.array",
            "boxes.astype"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "matterport/Mask_RCNN",
        "function_declaration": "def compute_iou(box, boxes, box_area, boxes_area)",
        "start_line": "60",
        "end_line": "78",
        "file_path": "mrcnn/utils.py",
        "docstring": "The function computes the Intersection over Union (IoU) between a given box and multiple boxes.\\nIt calculates the intersection areas between the box and each box in the array, then computes the union areas using the provided box areas and the intersection areas.\\nThe IoU is obtained by dividing the intersection areas by the union areas, and the function returns the IoU values.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7ad5d430d6ed",
        "ground_truth": "def compute_iou(box, boxes, box_area, boxes_area):\n    \"\"\"Calculates IoU of the given box with the array of the given boxes.\n    box: 1D vector [y1, x1, y2, x2]\n    boxes: [boxes_count, (y1, x1, y2, x2)]\n    box_area: float. the area of 'box'\n    boxes_area: array of length boxes_count.\n     Note: the areas are passed in rather than calculated here for\n    efficiency. Calculate once in the caller to avoid duplicate work.\n    \"\"\"\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou",
        "import_statements": [
            "import sys",
            "import os",
            "import logging",
            "import math",
            "import random",
            "import scipy",
            "import skimage.color",
            "import skimage.io",
            "import skimage.transform",
            "import urllib.request",
            "import shutil",
            "import warnings",
            "from distutils.version import LooseVersion"
        ],
        "reference_api": [
            "np.minimum",
            "np.maximum"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "np.maximum",
            "np.minimum",
            "np.maximum",
            "np.minimum",
            "np.maximum",
            "np.maximum"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "matterport/Mask_RCNN",
        "function_declaration": "def compute_overlaps(boxes1, boxes2)",
        "start_line": "81",
        "end_line": "97",
        "file_path": "mrcnn/utils.py",
        "docstring": "The function computes overlap areas between two sets of bounding boxes.\\nIt first calculates the area of each box in both sets.\\nIt then initializes an overlap matrix and iterates through each box in the second set, computing the Intersection over Union (IoU) with all boxes in the first set.\\nThe function returns the matrix of overlaps.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "15941b852859",
        "ground_truth": "def compute_overlaps(boxes1, boxes2):\n    \"\"\"Computes IoU overlaps between two sets of boxes.\n    boxes1, boxes2: [N, (y1, x1, y2, x2)].\n     For better performance, pass the largest set first and the smaller second.\n    \"\"\"\n    # Areas of anchors and GT boxes\n    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n     # Compute overlaps to generate matrix [boxes1 count, boxes2 count]\n    # Each cell contains the IoU value.\n    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n    for i in range(overlaps.shape[1]):\n        box2 = boxes2[i]\n        overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)\n    return overlaps",
        "import_statements": [
            "import sys",
            "import os",
            "import logging",
            "import math",
            "import random",
            "import scipy",
            "import skimage.color",
            "import skimage.io",
            "import skimage.transform",
            "import urllib.request",
            "import shutil",
            "import warnings",
            "from distutils.version import LooseVersion"
        ],
        "reference_api": [
            "np.zeros",
            "compute_iou",
            "range"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "compute_iou",
                "code": "def compute_iou(box, boxes, box_area, boxes_area):\n    \"\"\"Calculates IoU of the given box with the array of the given boxes.\n    box: 1D vector [y1, x1, y2, x2]\n    boxes: [boxes_count, (y1, x1, y2, x2)]\n    box_area: float. the area of 'box'\n    boxes_area: array of length boxes_count.\n\n    Note: the areas are passed in rather than calculated here for\n    efficiency. Calculate once in the caller to avoid duplicate work.\n    \"\"\"\n    # Calculate intersection areas\n    y1 = np.maximum(box[0], boxes[:, 0])\n    y2 = np.minimum(box[2], boxes[:, 2])\n    x1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[3], boxes[:, 3])\n    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n    union = box_area + boxes_area[:] - intersection[:]\n    iou = intersection / union\n    return iou"
            }
        ],
        "third_party": [
            "np.zeros"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "matterport/Mask_RCNN",
        "function_declaration": "def load_image(self, image_id):",
        "start_line": "355",
        "end_line": "366",
        "file_path": "mrcnn/utils.py",
        "docstring": "The function loads an image using its ID from the image information dictionary.\\nIt reads the image from the specified path and converts it to RGB if it is not already in RGB format.\\nIf the image has four channels, it removes the alpha channel, returning only the first three channels.\\nThe function then returns the processed image.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "597533722ce3",
        "ground_truth": "def load_image(self, image_id):\n    \"\"\"Load the specified image and return a [H,W,3] Numpy array.\n    \"\"\"\n    # Load image\n    image = skimage.io.imread(self.image_info[image_id]['path'])\n    # If grayscale. Convert to RGB for consistency.\n    if image.ndim != 3:\n        image = skimage.color.gray2rgb(image)\n    # If has an alpha channel, remove it for consistency\n    if image.shape[-1] == 4:\n        image = image[..., :3]\n    return image",
        "import_statements": [
            "import sys",
            "import os",
            "import logging",
            "import math",
            "import random",
            "import scipy",
            "import skimage.color",
            "import skimage.io",
            "import skimage.transform",
            "import urllib.request",
            "import shutil",
            "import warnings",
            "from distutils.version import LooseVersion"
        ],
        "reference_api": [
            "imread",
            "gray2rgb"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "imread",
            "gray2rgb"
        ]
    },
    {
        "subclass": "tensorflow",
        "owner/repo": "matterport/Mask_RCNN",
        "function_declaration": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None)",
        "start_line": "35",
        "end_line": "56",
        "file_path": "mrcnn/visualize.py",
        "docstring": "The function displays a list of images with optional titles in a grid format.\\nIt sets default titles if none are provided and calculates the number of rows needed based on the number of images and columns.\\nIt creates a figure and iterates over the images and titles, displaying each image in a subplot with its corresponding title.\\nThe function sets display parameters like colormap, normalization, and interpolation before showing the figure.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9b852997ca05",
        "ground_truth": "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n                   interpolation=None):\n    \"\"\"Display the given set of images, optionally with titles.\n    images: list or array of image tensors in HWC format.\n    titles: optional. A list of titles to display with each image.\n    cols: number of images per row\n    cmap: Optional. Color map to use. For example, \"Blues\".\n    norm: Optional. A Normalize instance to map values to colors.\n    interpolation: Optional. Image interpolation to use for display.\n    \"\"\"\n    titles = titles if titles is not None else [\"\"] * len(images)\n    rows = len(images) // cols + 1\n    plt.figure(figsize=(14, 14 * rows // cols))\n    i = 1\n    for image, title in zip(images, titles):\n        plt.subplot(rows, cols, i)\n        plt.title(title, fontsize=9)\n        plt.axis('off')\n        plt.imshow(image.astype(np.uint8), cmap=cmap,\n                   norm=norm, interpolation=interpolation)\n        i += 1\n    plt.show()",
        "import_statements": [
            "import os",
            "import sys",
            "import random",
            "import itertools",
            "import colorsys",
            "from skimage.measure import find_contours",
            "from matplotlib import patches,  lines",
            "from matplotlib.patches import Polygon",
            "import IPython.display",
            "from mrcnn import utils"
        ],
        "reference_api": [
            "image.astype",
            "plt.figure",
            "plt.show",
            "len",
            "plt.imshow",
            "plt.axis",
            "plt.title",
            "zip",
            "plt.subplot"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "plt.figure",
            "plt.subplot",
            "plt.title",
            "plt.axis",
            "plt.imshow",
            "image.astype",
            "plt.show"
        ]
    }
]