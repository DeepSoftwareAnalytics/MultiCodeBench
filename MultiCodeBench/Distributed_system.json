[
    {
        "subclass": "ZooKeeper",
        "owner/repo": "patroni/patroni",
        "function_declaration": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]",
        "start_line": "128",
        "end_line": "141",
        "file_path": "patroni/dcs/zookeeper.py",
        "docstring": "# This function, _kazoo_connect, calls an original connection method with any given arguments, captures its return value, and then returns a tuple. The tuple consists of a calculated wait time in milliseconds and the second element of the original method's return value. The wait time is determined by subtracting 2 from the loop_wait attribute, ensuring it does not go below 2, and then multiplying by 1000.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ba8707e206f9",
        "ground_truth": "def _kazoo_connect(self, *args: Any) -> Tuple[Union[int, float], Union[int, float]]:\n    \"\"\"Kazoo is using Ping's to determine health of connection to zookeeper. If there is no\n    response on Ping after Ping interval (1/2 from read_timeout) it will consider current\n    connection dead and try to connect to another node. Without this \"magic\" it was taking\n    up to 2/3 from session timeout (ttl) to figure out that connection was dead and we had\n    only small time for reconnect and retry.\n    This method is needed to return different value of read_timeout, which is not calculated\n    from negotiated session timeout but from value of `loop_wait`. And it is 2 sec smaller\n    than loop_wait, because we can spend up to 2 seconds when calling `touch_member()` and\n    `write_leader_optime()` methods, which also may hang...\"\"\"\n    ret = self._orig_kazoo_connect(*args)\n    return max(self.loop_wait - 2, 2) * 1000, ret[1]",
        "import_statements": [
            "import json",
            "import logging",
            "import select",
            "import socket",
            "import time",
            "from kazoo.client import KazooClient, KazooState, KazooRetry",
            "from kazoo.exceptions import ConnectionClosedError, NoNodeError, NodeExistsError, SessionExpiredError",
            "from kazoo.handlers.threading import AsyncResult, SequentialThreadingHandler",
            "from kazoo.protocol.states import KeeperState, WatchedEvent, ZnodeStat",
            "from kazoo.retry import RetryFailedError",
            "from kazoo.security import ACL, make_acl",
            "from typing import Any, Callable, Dict, List, Optional, Union, Tuple, TYPE_CHECKING"
        ],
        "reference_api": [
            "max",
            "self._orig_kazoo_connect"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "self._orig_kazoo_connect"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "patroni/patroni",
        "function_declaration": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None",
        "start_line": "147",
        "end_line": "164",
        "file_path": "patroni/dcs/zookeeper.py",
        "docstring": "The function reload_config updates the configuration settings of an instance based on the provided config parameter, which can be a Config object or a dictionary. It updates the retry timeout, loop wait time, and time-to-live (TTL) values. If the loop wait time changes and the client handler is of type PatroniSequentialThreadingHandler, it also updates the connect timeout. If the TTL update fails and the loop wait time has changed, it closes the client's socket connection.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "88e24a7c26a4",
        "ground_truth": "def reload_config(self, config: Union['Config', Dict[str, Any]]) -> None:\n    self.set_retry_timeout(config['retry_timeout'])\n    loop_wait = config['loop_wait']\n    loop_wait_changed = self._loop_wait != loop_wait\n    self._loop_wait = loop_wait\n    if isinstance(self._client.handler, PatroniSequentialThreadingHandler):\n        self._client.handler.set_connect_timeout(loop_wait)\n    # We need to reestablish connection to zookeeper if we want to change\n    # read_timeout (and Ping interval respectively), because read_timeout\n    # is calculated in `_kazoo_connect` method. If we are changing ttl at\n    # the same time, set_ttl method will reestablish connection and return\n    # `!True`, otherwise we will close existing connection and let kazoo\n    # open the new one.\n    if not self.set_ttl(config['ttl']) and loop_wait_changed:\n        self._client._connection._socket.close()",
        "import_statements": [
            "import json",
            "import logging",
            "import select",
            "import socket",
            "import time",
            "from kazoo.client import KazooClient, KazooState, KazooRetry",
            "from kazoo.exceptions import ConnectionClosedError, NoNodeError, NodeExistsError, SessionExpiredError",
            "from kazoo.handlers.threading import AsyncResult, SequentialThreadingHandler",
            "from kazoo.protocol.states import KeeperState, WatchedEvent, ZnodeStat",
            "from kazoo.retry import RetryFailedError",
            "from kazoo.security import ACL, make_acl",
            "from typing import Any, Callable, Dict, List, Optional, Union, Tuple, TYPE_CHECKING"
        ],
        "reference_api": [
            "self.set_retry_timeout",
            "set_connect_timeout",
            "isinstance",
            "close",
            "self.set_ttl"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.set_retry_timeout",
                "code": "def set_retry_timeout(self, retry_timeout: int) -> None:\n        retry = self._client.retry if isinstance(self._client.retry, KazooRetry) else self._client._retry\n        retry.deadline = retry_timeout"
            },
            {
                "name": "set_connect_timeout",
                "code": "def set_connect_timeout(self, connect_timeout: Union[int, float]) -> None:\n        self._connect_timeout = max(1.0, connect_timeout / 2.0)  # try to connect to zookeeper node during loop_wait/2"
            },
            {
                "name": "self.set_ttl",
                "code": "def set_ttl(self, ttl: int) -> Optional[bool]:\n        \"\"\"It is not possible to change ttl (session_timeout) in zookeeper without\n        destroying old session and creating the new one. This method returns `!True`\n        if session_timeout has been changed (`restart()` has been called).\"\"\"\n        ttl = int(ttl * 1000)\n        if self._client._session_timeout != ttl:\n            self._client._session_timeout = ttl\n            self._client.restart()\n            return True"
            }
        ],
        "third_party": [
            "close"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "patroni/patroni",
        "function_declaration": "def _postgresql_cluster_loader(self, path: str) -> Cluster",
        "start_line": "217",
        "end_line": "265",
        "file_path": "patroni/dcs/zookeeper.py",
        "docstring": "This function loads a PostgreSQL cluster configuration from a given path. It retrieves various cluster components such as initialization state, configuration, timeline history, synchronization state, members, leader information, cluster status, failover state, and failsafe configuration. The function checks the existence of these components in the specified path, processes them, and returns a Cluster object containing all the gathered information.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2af7d7b3c887",
        "ground_truth": "def _postgresql_cluster_loader(self, path: str) -> Cluster:\n    \"\"\"Load and build the :class:`Cluster` object from DCS, which represents a single PostgreSQL cluster.\n    :param path: the path in DCS where to load :class:`Cluster` from.\n    :returns: :class:`Cluster` instance.\n    \"\"\"\n    nodes = set(self.get_children(path))\n    # get initialize flag\n    initialize = (self.get_node(path + self._INITIALIZE) or [None])[0] if self._INITIALIZE in nodes else None\n    # get global dynamic configuration\n    config = self.get_node(path + self._CONFIG, watch=self._watcher) if self._CONFIG in nodes else None\n    config = config and ClusterConfig.from_node(config[1].version, config[0], config[1].mzxid)\n    # get timeline history\n    history = self.get_node(path + self._HISTORY) if self._HISTORY in nodes else None\n    history = history and TimelineHistory.from_node(history[1].mzxid, history[0])\n    # get synchronization state\n    sync = self.get_node(path + self._SYNC) if self._SYNC in nodes else None\n    sync = SyncState.from_node(sync and sync[1].version, sync and sync[0])\n    # get list of members\n    members = self.load_members(path) if self._MEMBERS[:-1] in nodes else []\n    # get leader\n    leader = self.get_node(path + self._LEADER, watch=self._watcher) if self._LEADER in nodes else None\n    if leader:\n        member = Member(-1, leader[0], None, {})\n        member = ([m for m in members if m.name == leader[0]] or [member])[0]\n        leader = Leader(leader[1].version, leader[1].ephemeralOwner, member)\n    # get last known leader lsn and slots\n    status = self.get_status(path, leader)\n    # failover key\n    failover = self.get_node(path + self._FAILOVER) if self._FAILOVER in nodes else None\n    failover = failover and Failover.from_node(failover[1].version, failover[0])\n    # get failsafe topology\n    failsafe = self.get_node(path + self._FAILSAFE) if self._FAILSAFE in nodes else None\n    try:\n        failsafe = json.loads(failsafe[0]) if failsafe else None\n    except Exception:\n        failsafe = None\n    return Cluster(initialize, config, leader, status, members, failover, sync, history, failsafe)",
        "import_statements": [
            "import json",
            "import logging",
            "import select",
            "import socket",
            "import time",
            "from kazoo.client import KazooClient, KazooState, KazooRetry",
            "from kazoo.exceptions import ConnectionClosedError, NoNodeError, NodeExistsError, SessionExpiredError",
            "from kazoo.handlers.threading import AsyncResult, SequentialThreadingHandler",
            "from kazoo.protocol.states import KeeperState, WatchedEvent, ZnodeStat",
            "from kazoo.retry import RetryFailedError",
            "from kazoo.security import ACL, make_acl",
            "from typing import Any, Callable, Dict, List, Optional, Union, Tuple, TYPE_CHECKING"
        ],
        "reference_api": [
            "self.get_children",
            "TimelineHistory.from_node",
            "Cluster",
            "self.get_node",
            "json.loads",
            "SyncState.from_node",
            "Leader",
            "set",
            "ClusterConfig.from_node",
            "Failover.from_node",
            "self.get_status",
            "Member",
            "self.load_members"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.get_children",
                "code": "def get_children(self, key: str) -> List[str]:\n        try:\n            return self._client.get_children(key)\n        except NoNodeError:\n            return []"
            },
            {
                "name": "self.get_node",
                "code": "def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None"
            },
            {
                "name": "self.get_node",
                "code": "def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None"
            },
            {
                "name": "self.get_node",
                "code": "def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None"
            },
            {
                "name": "self.get_node",
                "code": "def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None"
            },
            {
                "name": "self.load_members",
                "code": "def load_members(self, path: str) -> List[Member]:\n        members: List[Member] = []\n        for member in self.get_children(path + self._MEMBERS):\n            data = self.get_node(path + self._MEMBERS + member)\n            if data is not None:\n                members.append(self.member(member, *data))\n        return members"
            },
            {
                "name": "self.get_node",
                "code": "def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None"
            },
            {
                "name": "self.get_status",
                "code": "def get_status(self, path: str, leader: Optional[Leader]) -> Status:\n        status = self.get_node(path + self._STATUS)\n        if not status:\n            status = self.get_node(path + self._LEADER_OPTIME)\n        return Status.from_node(status and status[0])"
            },
            {
                "name": "self.get_node",
                "code": "def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None"
            },
            {
                "name": "self.get_node",
                "code": "def get_node(\n            self, key: str, watch: Optional[Callable[[WatchedEvent], None]] = None\n    ) -> Optional[Tuple[str, ZnodeStat]]:\n        try:\n            ret = self._client.get(key, watch)\n            return (ret[0].decode('utf-8'), ret[1])\n        except NoNodeError:\n            return None"
            }
        ],
        "third_party": [
            "ClusterConfig.from_node",
            "TimelineHistory.from_node",
            "SyncState.from_node",
            "Member",
            "Leader",
            "Failover.from_node",
            "Cluster"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "patroni/patroni",
        "function_declaration": "def attempt_to_acquire_leader(self) -> bool",
        "start_line": "300",
        "end_line": "311",
        "file_path": "patroni/dcs/zookeeper.py",
        "docstring": "This function attempts to acquire a leader lock by creating an ephemeral node in ZooKeeper at a specified path. It uses a retry mechanism to handle transient issues. If successful, it returns True. If the connection to ZooKeeper is closed or retries fail, it raises a ZooKeeperError. For other exceptions, it logs an error unless the exception is a NodeExistsError, indicating another instance has the lock. If the lock cannot be acquired, it logs an info message and returns False.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "43e1bc044d8e",
        "ground_truth": "def attempt_to_acquire_leader(self) -> bool:\n    try:\n        self._client.retry(self._client.create, self.leader_path, self._name.encode('utf-8'),\n                           makepath=True, ephemeral=True)\n        return True\n    except (ConnectionClosedError, RetryFailedError) as e:\n        raise ZooKeeperError(e)\n    except Exception as e:\n        if not isinstance(e, NodeExistsError):\n            logger.error('Failed to create %s: %r', self.leader_path, e)\n    logger.info('Could not take out TTL lock')\n    return False",
        "import_statements": [
            "import json",
            "import logging",
            "import select",
            "import socket",
            "import time",
            "from kazoo.client import KazooClient, KazooState, KazooRetry",
            "from kazoo.exceptions import ConnectionClosedError, NoNodeError, NodeExistsError, SessionExpiredError",
            "from kazoo.handlers.threading import AsyncResult, SequentialThreadingHandler",
            "from kazoo.protocol.states import KeeperState, WatchedEvent, ZnodeStat",
            "from kazoo.retry import RetryFailedError",
            "from kazoo.security import ACL, make_acl",
            "from typing import Any, Callable, Dict, List, Optional, Union, Tuple, TYPE_CHECKING"
        ],
        "reference_api": [
            "encode",
            "retry",
            "isinstance",
            "logger.info",
            "logger.error",
            "ZooKeeperError"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "retry",
            "encode",
            "ZooKeeperError"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "patroni/patroni",
        "function_declaration": "def _update_leader(self, leader: Leader) -> bool",
        "start_line": "395",
        "end_line": "416",
        "file_path": "patroni/dcs/zookeeper.py",
        "docstring": "This function updates the leader node in a distributed system. It first checks if the current client ID matches the leader's session. If there is a mismatch, it logs a warning and attempts to delete the existing leader ZNode. In case of errors such as `NoNodeError`, `ConnectionClosedError`, or `RetryFailedError`, it handles them appropriately, either by ignoring or raising a `ZooKeeperError`. If deletion is successful, it tries to create a new leader ZNode with the current client's name encoded in UTF-8. Any errors during creation are logged, and the function returns `False`. If everything succeeds, it returns `True`.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "93d126ab8a22",
        "ground_truth": "def _update_leader(self, leader: Leader) -> bool:\n    if self._client.client_id and self._client.client_id[0] != leader.session:\n        logger.warning('Recreating the leader ZNode due to ownership mismatch')\n        try:\n            self._client.retry(self._client.delete, self.leader_path)\n        except NoNodeError:\n            pass\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            logger.error('Failed to remove %s: %r', self.leader_path, e)\n            return False\n        try:\n            self._client.retry(self._client.create, self.leader_path,\n                               self._name.encode('utf-8'), makepath=True, ephemeral=True)\n        except (ConnectionClosedError, RetryFailedError) as e:\n            raise ZooKeeperError(e)\n        except Exception as e:\n            logger.error('Failed to create %s: %r', self.leader_path, e)\n            return False\n    return True",
        "import_statements": [
            "import json",
            "import logging",
            "import select",
            "import socket",
            "import time",
            "from kazoo.client import KazooClient, KazooState, KazooRetry",
            "from kazoo.exceptions import ConnectionClosedError, NoNodeError, NodeExistsError, SessionExpiredError",
            "from kazoo.handlers.threading import AsyncResult, SequentialThreadingHandler",
            "from kazoo.protocol.states import KeeperState, WatchedEvent, ZnodeStat",
            "from kazoo.retry import RetryFailedError",
            "from kazoo.security import ACL, make_acl",
            "from typing import Any, Callable, Dict, List, Optional, Union, Tuple, TYPE_CHECKING"
        ],
        "reference_api": [
            "retry",
            "encode",
            "logger.warning",
            "logger.error",
            "ZooKeeperError"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "retry",
            "ZooKeeperError",
            "retry",
            "encode",
            "ZooKeeperError"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "patroni/patroni",
        "function_declaration": "def poll(self) -> bool",
        "start_line": "34",
        "end_line": "51",
        "file_path": "patroni/dcs/exhibitor.py",
        "docstring": "The function checks if it's time to poll again, based on a preset interval. It queries the status of the exhibitors, and if it gets valid data containing server and port information, it updates the ZooKeeper connection string if it has changed, logging the change and updating internal state. It returns True if a change occurred, and False otherwise.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f68719982812",
        "ground_truth": "def poll(self) -> bool:\n    if self._next_poll and self._next_poll > time.time():\n        return False\n    json = self._query_exhibitors(self._exhibitors)\n    if not json:\n        json = self._query_exhibitors(self._boot_exhibitors)\n    if isinstance(json, dict) and 'servers' in json and 'port' in json:\n        self._next_poll = time.time() + self._poll_interval\n        servers: List[str] = json['servers']\n        zookeeper_hosts = ','.join([h + ':' + str(json['port']) for h in sorted(servers)])\n        if self._zookeeper_hosts != zookeeper_hosts:\n            logger.info('ZooKeeper connection string has changed: %s => %s', self._zookeeper_hosts, zookeeper_hosts)\n            self._zookeeper_hosts = zookeeper_hosts\n            self._exhibitors = json['servers']\n            return True\n    return False",
        "import_statements": [
            "import json",
            "import logging",
            "import random",
            "import time",
            "from typing import Any, Callable, Dict, List, Union"
        ],
        "reference_api": [
            "join",
            "sorted",
            "self._query_exhibitors",
            "isinstance",
            "logger.info",
            "str",
            "time.time"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._query_exhibitors",
                "code": "def _query_exhibitors(self, exhibitors: List[str]) -> Union[Dict[str, Any], Any]:\n        random.shuffle(exhibitors)\n        for host in exhibitors:\n            try:\n                response = requests_get(uri('http', (host, self._exhibitor_port), self._uri_path), timeout=self.TIMEOUT)\n                return json.loads(response.data.decode('utf-8'))\n            except Exception:\n                logging.debug('Request to %s failed', host)\n        return None"
            },
            {
                "name": "self._query_exhibitors",
                "code": "def _query_exhibitors(self, exhibitors: List[str]) -> Union[Dict[str, Any], Any]:\n        random.shuffle(exhibitors)\n        for host in exhibitors:\n            try:\n                response = requests_get(uri('http', (host, self._exhibitor_port), self._uri_path), timeout=self.TIMEOUT)\n                return json.loads(response.data.decode('utf-8'))\n            except Exception:\n                logging.debug('Request to %s failed', host)\n        return None"
            }
        ],
        "third_party": [
            "join"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "patroni/patroni",
        "function_declaration": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]",
        "start_line": "75",
        "end_line": "80",
        "file_path": "patroni/dcs/exhibitor.py",
        "docstring": "This function is designed to load a cluster configuration from a specified path using a provided loader function. It checks if the ensemble provider is available and updates the client's hosts with the Zookeeper hosts if necessary. Finally, it invokes the parent class's _load_cluster method to complete the loading process.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "022ed3721913",
        "ground_truth": "def _load_cluster(\n        self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n) -> Union[Cluster, Dict[int, Cluster]]:\n    if self._ensemble_provider.poll():\n        self._client.set_hosts(self._ensemble_provider.zookeeper_hosts)\n    return super(Exhibitor, self)._load_cluster(path, loader)",
        "import_statements": [
            "import json",
            "import logging",
            "import random",
            "import time",
            "from typing import Any, Callable, Dict, List, Union"
        ],
        "reference_api": [
            "super",
            "_load_cluster",
            "set_hosts",
            "poll"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "poll",
                "code": "def poll(self) -> bool:\n        if self._next_poll and self._next_poll > time.time():\n            return False\n\n        json = self._query_exhibitors(self._exhibitors)\n        if not json:\n            json = self._query_exhibitors(self._boot_exhibitors)\n\n        if isinstance(json, dict) and 'servers' in json and 'port' in json:\n            self._next_poll = time.time() + self._poll_interval\n            servers: List[str] = json['servers']\n            zookeeper_hosts = ','.join([h + ':' + str(json['port']) for h in sorted(servers)])\n            if self._zookeeper_hosts != zookeeper_hosts:\n                logger.info('ZooKeeper connection string has changed: %s => %s', self._zookeeper_hosts, zookeeper_hosts)\n                self._zookeeper_hosts = zookeeper_hosts\n                self._exhibitors = json['servers']\n                return True\n        return False"
            },
            {
                "name": "_load_cluster",
                "code": "def _load_cluster(\n            self, path: str, loader: Callable[[str], Union[Cluster, Dict[int, Cluster]]]\n    ) -> Union[Cluster, Dict[int, Cluster]]:\n        if self._ensemble_provider.poll():\n            self._client.set_hosts(self._ensemble_provider.zookeeper_hosts)\n        return super(Exhibitor, self)._load_cluster(path, loader)"
            }
        ],
        "third_party": [
            "set_hosts"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener)",
        "start_line": "21",
        "end_line": "24",
        "file_path": "discovery-commons/discovery-common-zookeeper/src/main/java/com/nepxion/discovery/common/zookeeper/operation/ZookeeperListener.java",
        "docstring": "The constructor initializes a ZookeeperListener with the provided NodeCache and NodeCacheListener.\\nIt assigns the nodeCache and nodeCacheListener to the instance variables.\\nThis setup allows the listener to monitor node changes and handle events accordingly.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c7d2e1db0cda",
        "ground_truth": "public ZookeeperListener(NodeCache nodeCache, NodeCacheListener nodeCacheListener) {\n    this.nodeCache = nodeCache;\n    this.nodeCacheListener = nodeCacheListener;\n}",
        "import_statements": [
            "import java.io.IOException;",
            "import org.apache.curator.framework.recipes.cache.NodeCache;",
            "import org.apache.curator.framework.recipes.cache.NodeCacheListener;"
        ],
        "reference_api": [],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "public void destroy()",
        "start_line": "69",
        "end_line": "84",
        "file_path": "discovery-commons/discovery-common-zookeeper/src/main/java/com/nepxion/discovery/common/zookeeper/proccessor/ZookeeperProcessor.java",
        "docstring": "The function checks if the zookeeperListener is null and returns if it is.\\nIf not, it retrieves the group and dataId, logs the start of the unsubscribe process, and attempts to unsubscribe from the Zookeeper configuration using the zookeeperListener.\\nIf an exception occurs during this process, it logs the failure.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c779f567d3e2",
        "ground_truth": "public void destroy() {\n    if (zookeeperListener == null) {\n        return;\n    }\n    String group = getGroup();\n    String dataId = getDataId();\n    logUnsubscribeStarted();\n    try {\n        zookeeperOperation.unsubscribeConfig(group, dataId, zookeeperListener);\n    } catch (Exception e) {\n        logUnsubscribeFailed(e);\n    }\n}",
        "import_statements": [
            "import javax.annotation.PostConstruct;",
            "import org.springframework.beans.factory.annotation.Autowired;",
            "import com.nepxion.discovery.common.entity.ConfigType;",
            "import com.nepxion.discovery.common.processor.DiscoveryConfigProcessor;",
            "import com.nepxion.discovery.common.zookeeper.operation.ZookeeperListener;",
            "import com.nepxion.discovery.common.zookeeper.operation.ZookeeperOperation;",
            "import com.nepxion.discovery.common.zookeeper.operation.ZookeeperSubscribeCallback;"
        ],
        "reference_api": [
            "unsubscribeConfig",
            "getDataId",
            "logUnsubscribeStarted",
            "logUnsubscribeFailed",
            "getGroup"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "unsubscribeConfig",
            "getDataId",
            "logUnsubscribeStarted",
            "logUnsubscribeFailed",
            "getGroup"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "public Map<String, String> getServerMetadata(Server server)",
        "start_line": "39",
        "end_line": "49",
        "file_path": "discovery-plugin-register-center/discovery-plugin-register-center-starter-zookeeper/src/main/java/com/nepxion/discovery/plugin/registercenter/zookeeper/adapter/ZookeeperAdapter.java",
        "docstring": "The function getServerMetadata retrieves metadata from a Server instance.\\nIf the server is an instance of ZookeeperServer, it casts the server to ZookeeperServer and obtains metadata from its payload.\\nIf the server is not a ZookeeperServer, it returns empty metadata.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7f0ce4db487b",
        "ground_truth": "public Map<String, String> getServerMetadata(Server server) {\n    if (server instanceof ZookeeperServer) {\n        ZookeeperServer zookeeperServer = (ZookeeperServer) server;\n        return zookeeperServer.getInstance().getPayload().getMetadata();\n    }\n    return emptyMetadata;\n    // throw new DiscoveryException(\"Server instance isn't the type of ZookeeperServer\");\n}",
        "import_statements": [
            "import java.util.Map;",
            "import javax.annotation.PostConstruct;",
            "import org.springframework.cloud.zookeeper.discovery.ZookeeperServer;",
            "import org.springframework.cloud.zookeeper.serviceregistry.ZookeeperRegistration;",
            "import com.nepxion.discovery.common.exception.DiscoveryException;",
            "import com.nepxion.discovery.plugin.framework.adapter.AbstractPluginAdapter;",
            "import com.netflix.loadbalancer.Server;"
        ],
        "reference_api": [
            "getInstance",
            "getMetadata",
            "getPayload"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getInstance",
            "getMetadata",
            "getPayload"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception",
        "start_line": "71",
        "end_line": "90",
        "file_path": "discovery-commons/discovery-common-zookeeper/src/main/java/com/nepxion/discovery/common/zookeeper/operation/ZookeeperOperation.java",
        "docstring": "The subscribeConfig function sets up a ZookeeperListener for a specific group and serviceId.\\nIt creates a NodeCache for the given path and starts it.\\nA NodeCacheListener is defined to handle node changes by converting the config and invoking a callback.\\nThe function then initializes a ZookeeperListener with the NodeCache and NodeCacheListener, adds the listener, and returns the ZookeeperListener instance.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7365d194b7ee",
        "ground_truth": "public ZookeeperListener subscribeConfig(String group, String serviceId, ZookeeperSubscribeCallback zookeeperSubscribeCallback) throws Exception {\n    String path = getPath(group, serviceId);\n    NodeCache nodeCache = new NodeCache(curatorFramework, path);\n    nodeCache.start(true);\n    NodeCacheListener nodeCacheListener = new NodeCacheListener() {\n        @Override\n        public void nodeChanged() throws Exception {\n            String config = convertConfig(nodeCache);\n            zookeeperSubscribeCallback.callback(config);\n        }\n    };\n    ZookeeperListener zookeeperListener = new ZookeeperListener(nodeCache, nodeCacheListener);\n    zookeeperListener.addListener();\n    return zookeeperListener;\n}",
        "import_statements": [
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.recipes.cache.ChildData;",
            "import org.apache.curator.framework.recipes.cache.NodeCache;",
            "import org.apache.curator.framework.recipes.cache.NodeCacheListener;",
            "import org.apache.zookeeper.CreateMode;",
            "import org.apache.zookeeper.data.Stat;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import org.springframework.beans.factory.DisposableBean;",
            "import org.springframework.beans.factory.annotation.Autowired;"
        ],
        "reference_api": [
            "convertConfig",
            "callback"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "convertConfig",
                "code": "public String convertConfig(String path) throws Exception {\n        return convertConfig(curatorFramework, path);\n    }"
            }
        ],
        "third_party": [
            "callback"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery)",
        "start_line": "47",
        "end_line": "54",
        "file_path": "discovery-plugin-register-center/discovery-plugin-register-center-starter-zookeeper/src/main/java/com/nepxion/discovery/plugin/registercenter/zookeeper/configuration/ZookeeperLoadBalanceConfiguration.java",
        "docstring": "The function initializes and returns a ZookeeperServerListDecorator configured with the provided IClientConfig and ServiceDiscovery.\\nIt sets up the server list with the client configuration, assigns a load balance listener executor, and sets the service ID based on the client name from the configuration.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6f492670162b",
        "ground_truth": "public ServerList<?> ribbonServerList(IClientConfig config, ServiceDiscovery<ZookeeperInstance> serviceDiscovery) {\n    ZookeeperServerListDecorator serverList = new ZookeeperServerListDecorator(serviceDiscovery);\n    serverList.initWithNiwsConfig(config);\n    serverList.setLoadBalanceListenerExecutor(loadBalanceListenerExecutor);\n    serverList.setServiceId(config.getClientName());\n    return serverList;\n}",
        "import_statements": [
            "import org.apache.curator.x.discovery.ServiceDiscovery;",
            "import org.springframework.beans.factory.annotation.Autowired;",
            "import org.springframework.boot.autoconfigure.AutoConfigureAfter;",
            "import org.springframework.cloud.zookeeper.discovery.ZookeeperInstance;",
            "import org.springframework.cloud.zookeeper.discovery.ZookeeperRibbonClientConfiguration;",
            "import org.springframework.cloud.zookeeper.discovery.dependency.ConditionalOnDependenciesNotPassed;",
            "import org.springframework.cloud.zookeeper.discovery.dependency.ConditionalOnDependenciesPassed;",
            "import org.springframework.cloud.zookeeper.discovery.dependency.ZookeeperDependencies;",
            "import org.springframework.context.annotation.Bean;",
            "import org.springframework.context.annotation.Configuration;",
            "import com.nepxion.discovery.plugin.framework.listener.loadbalance.LoadBalanceListenerExecutor;",
            "import com.nepxion.discovery.plugin.registercenter.zookeeper.decorator.ZookeeperServerListDecorator;",
            "import com.netflix.client.config.IClientConfig;",
            "import com.netflix.loadbalancer.ServerList;"
        ],
        "reference_api": [
            "getClientName",
            "initWithNiwsConfig",
            "setServiceId",
            "setLoadBalanceListenerExecutor"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getClientName",
            "initWithNiwsConfig",
            "setServiceId",
            "setLoadBalanceListenerExecutor"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "public void register(ZookeeperRegistration registration)",
        "start_line": "33",
        "end_line": "42",
        "file_path": "discovery-plugin-register-center/discovery-plugin-register-center-starter-zookeeper/src/main/java/com/nepxion/discovery/plugin/registercenter/zookeeper/decorator/ZookeeperServiceRegistryDecorator.java",
        "docstring": "The register method handles the registration of a ZookeeperRegistration object.\\nIt attempts to retrieve a RegisterListenerExecutor bean from the application context and invoke its onRegister method with the registration object.\\nIf a BeansException occurs, it is caught and ignored.\\nFinally, the registration is passed to the serviceRegistry for completion.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "17c01a5f8ef8",
        "ground_truth": "public void register(ZookeeperRegistration registration) {\n    try {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onRegister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n    serviceRegistry.register(registration);\n}",
        "import_statements": [
            "import org.springframework.beans.BeansException;",
            "import org.springframework.cloud.zookeeper.serviceregistry.ZookeeperRegistration;",
            "import org.springframework.cloud.zookeeper.serviceregistry.ZookeeperServiceRegistry;",
            "import org.springframework.context.ConfigurableApplicationContext;",
            "import com.nepxion.discovery.plugin.framework.listener.register.RegisterListenerExecutor;"
        ],
        "reference_api": [
            "register",
            "getBean",
            "onRegister"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "register",
                "code": "@Override\n    public void register(ZookeeperRegistration registration) {\n        try {\n            RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n            registerListenerExecutor.onRegister(registration);\n        } catch (BeansException e) {\n            // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n        }\n\n        serviceRegistry.register(registration);\n    }"
            }
        ],
        "third_party": [
            "getBean",
            "onRegister"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "public void deregister(ZookeeperRegistration registration)",
        "start_line": "45",
        "end_line": "54",
        "file_path": "discovery-plugin-register-center/discovery-plugin-register-center-starter-zookeeper/src/main/java/com/nepxion/discovery/plugin/registercenter/zookeeper/decorator/ZookeeperServiceRegistryDecorator.java",
        "docstring": "The function deregister removes a ZookeeperRegistration from the service registry.\\nIt attempts to get a RegisterListenerExecutor bean from the application context and calls its onDeregister method with the registration.\\nIf a BeansException occurs, it is caught and ignored.\\nFinally, the registration is removed from the serviceRegistry.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "99392dfa2335",
        "ground_truth": "public void deregister(ZookeeperRegistration registration) {\n    try {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onDeregister(registration);\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n    serviceRegistry.deregister(registration);\n}",
        "import_statements": [
            "import org.springframework.beans.BeansException;",
            "import org.springframework.cloud.zookeeper.serviceregistry.ZookeeperRegistration;",
            "import org.springframework.cloud.zookeeper.serviceregistry.ZookeeperServiceRegistry;",
            "import org.springframework.context.ConfigurableApplicationContext;",
            "import com.nepxion.discovery.plugin.framework.listener.register.RegisterListenerExecutor;"
        ],
        "reference_api": [
            "deregister",
            "getBean",
            "onDeregister"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "deregister",
                "code": "@Override\n    public void deregister(ZookeeperRegistration registration) {\n        try {\n            RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n            registerListenerExecutor.onDeregister(registration);\n        } catch (BeansException e) {\n            // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n        }\n\n        serviceRegistry.deregister(registration);\n    }"
            }
        ],
        "third_party": [
            "getBean",
            "onDeregister"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "public void close()",
        "start_line": "74",
        "end_line": "83",
        "file_path": "discovery-plugin-register-center/discovery-plugin-register-center-starter-zookeeper/src/main/java/com/nepxion/discovery/plugin/registercenter/zookeeper/decorator/ZookeeperServiceRegistryDecorator.java",
        "docstring": "The close() method attempts to retrieve a RegisterListenerExecutor bean from the application context and calls its onClose() method.\\nIf a BeansException occurs, it is caught and ignored.\\nFinally, it calls the close() method on the serviceRegistry to perform cleanup.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "02a835946e90",
        "ground_truth": "public void close() {\n    try {\n        RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n        registerListenerExecutor.onClose();\n    } catch (BeansException e) {\n        // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n    }\n    serviceRegistry.close();\n}",
        "import_statements": [
            "import org.springframework.beans.BeansException;",
            "import org.springframework.cloud.zookeeper.serviceregistry.ZookeeperRegistration;",
            "import org.springframework.cloud.zookeeper.serviceregistry.ZookeeperServiceRegistry;",
            "import org.springframework.context.ConfigurableApplicationContext;",
            "import com.nepxion.discovery.plugin.framework.listener.register.RegisterListenerExecutor;"
        ],
        "reference_api": [
            "onClose",
            "getBean",
            "close"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "close",
                "code": "@Override\n    public void close() {\n        try {\n            RegisterListenerExecutor registerListenerExecutor = applicationContext.getBean(RegisterListenerExecutor.class);\n            registerListenerExecutor.onClose();\n        } catch (BeansException e) {\n            // LOG.warn(\"Get bean for RegisterListenerExecutor failed, ignore to executor listener\");\n        }\n\n        serviceRegistry.close();\n    }"
            }
        ],
        "third_party": [
            "onClose",
            "getBean"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "private ZookeeperListener subscribeConfig(boolean globalConfig)",
        "start_line": "42",
        "end_line": "60",
        "file_path": "discovery-plugin-config-center/discovery-plugin-config-center-starter-zookeeper/src/main/java/com/nepxion/discovery/plugin/configcenter/zookeeper/adapter/ZookeeperConfigAdapter.java",
        "docstring": "The subscribeConfig method subscribes to a configuration based on the globalConfig parameter.\\nIt retrieves the group and dataId values, logs the subscription start, and attempts to subscribe using zookeeperOperation.\\nA callback is defined to handle configuration updates.\\nIf an exception occurs, it logs the subscription failure.\\nThe method returns a ZookeeperListener or null if the subscription fails.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "64cf817f5945",
        "ground_truth": "private ZookeeperListener subscribeConfig(boolean globalConfig) {\n    String group = getGroup();\n    String dataId = getDataId(globalConfig);\n    logSubscribeStarted(globalConfig);\n    try {\n        return zookeeperOperation.subscribeConfig(group, dataId, new ZookeeperSubscribeCallback() {\n            @Override\n            public void callback(String config) {\n                callbackConfig(config, globalConfig);\n            }\n        });\n    } catch (Exception e) {\n        logSubscribeFailed(e, globalConfig);\n    }\n    return null;\n}",
        "import_statements": [
            "import javax.annotation.PostConstruct;",
            "import org.springframework.beans.factory.annotation.Autowired;",
            "import com.nepxion.discovery.common.entity.ConfigType;",
            "import com.nepxion.discovery.common.zookeeper.operation.ZookeeperListener;",
            "import com.nepxion.discovery.common.zookeeper.operation.ZookeeperOperation;",
            "import com.nepxion.discovery.common.zookeeper.operation.ZookeeperSubscribeCallback;",
            "import com.nepxion.discovery.plugin.configcenter.adapter.ConfigAdapter;"
        ],
        "reference_api": [
            "callbackConfig"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "callbackConfig"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "public CuratorFramework curatorFramework()",
        "start_line": "38",
        "end_line": "51",
        "file_path": "discovery-commons/discovery-common-zookeeper/src/main/java/com/nepxion/discovery/common/zookeeper/configuration/ZookeeperAutoConfiguration.java",
        "docstring": "The function curatorFramework initializes and returns a CuratorFramework instance.\\nIt retrieves the Zookeeper connection string and other properties from the environment.\\nIf the connection string is empty, it throws a DiscoveryException.\\nIt sets up the CuratorFramework with an ExponentialBackoffRetry policy using the retrieved retry count and sleep time.\\nThe CuratorFramework instance is then started and returned.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "be408acc71f6",
        "ground_truth": "public CuratorFramework curatorFramework() {\n    String connectString = environment.getProperty(ZookeeperConstant.ZOOKEEPER_CONNECT_STRING);\n    if (StringUtils.isEmpty(connectString)) {\n        throw new DiscoveryException(ZookeeperConstant.ZOOKEEPER_CONNECT_STRING + \" can't be null or empty\");\n    }\n    int retryCount = environment.getProperty(ZookeeperConstant.ZOOKEEPER_RETRY_COUNT, Integer.class, ZookeeperConstant.ZOOKEEPER_DEFAULT_RETRY_COUNT_VALUE);\n    int sleepTime = environment.getProperty(ZookeeperConstant.ZOOKEEPER_SLEEP_TIME, Integer.class, ZookeeperConstant.ZOOKEEPER_DEFAULT_SLEEP_TIME_VALUE);\n    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder().connectString(connectString).retryPolicy(new ExponentialBackoffRetry(sleepTime, retryCount)).build();\n    curatorFramework.start();\n    return curatorFramework;\n}",
        "import_statements": [
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.CuratorFrameworkFactory;",
            "import org.apache.curator.retry.ExponentialBackoffRetry;",
            "import org.springframework.beans.factory.annotation.Autowired;",
            "import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;",
            "import org.springframework.context.annotation.Bean;",
            "import org.springframework.context.annotation.Configuration;",
            "import org.springframework.core.env.Environment;",
            "import com.nepxion.discovery.common.exception.DiscoveryException;",
            "import com.nepxion.discovery.common.zookeeper.constant.ZookeeperConstant;",
            "import com.nepxion.discovery.common.zookeeper.operation.ZookeeperOperation;"
        ],
        "reference_api": [
            "getProperty",
            "retryPolicy",
            "connectString",
            "start",
            "build",
            "builder",
            "isEmpty"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getProperty",
            "retryPolicy",
            "connectString",
            "start",
            "build",
            "builder",
            "isEmpty"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Nepxion/Discovery",
        "function_declaration": "protected static class GatewayRouteZookeeperConfiguration",
        "start_line": "123",
        "end_line": "129",
        "file_path": "discovery-plugin-strategy/discovery-plugin-strategy-starter-gateway/src/main/java/com/nepxion/discovery/plugin/strategy/gateway/configuration/GatewayStrategyAutoConfiguration.java",
        "docstring": "The GatewayRouteZookeeperConfiguration class defines a configuration for gateway routes.\\nIt includes a bean method, which returns a new instance of GatewayStrategyRouteZookeeperProcessor.\\nThis method is conditional, only executing if the property \"spring.cloud.gateway.discovery.locator.enabled\" is set to \"false\" or is missing.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3628d7b6f0a5",
        "ground_truth": "protected static class GatewayRouteZookeeperConfiguration {\n    @Bean\n    @ConditionalOnProperty(value = \"spring.cloud.gateway.discovery.locator.enabled\", havingValue = \"false\", matchIfMissing = true)\n    public ZookeeperProcessor gatewayStrategyRouteZookeeperProcessor() {\n        return new GatewayStrategyRouteZookeeperProcessor();\n    }\n}",
        "import_statements": [
            "import org.apache.skywalking.apm.agent.core.context.TracingContext;",
            "import org.springframework.beans.factory.annotation.Autowired;",
            "import org.springframework.boot.autoconfigure.AutoConfigureBefore;",
            "import org.springframework.boot.autoconfigure.condition.ConditionalOnClass;",
            "import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;",
            "import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;",
            "import org.springframework.cloud.netflix.ribbon.RibbonClientConfiguration;",
            "import org.springframework.context.annotation.Bean;",
            "import org.springframework.context.annotation.Configuration;",
            "import org.springframework.core.env.ConfigurableEnvironment;",
            "import com.nepxion.discovery.common.apollo.proccessor.ApolloProcessor;",
            "import com.nepxion.discovery.common.consul.proccessor.ConsulProcessor;",
            "import com.nepxion.discovery.common.etcd.proccessor.EtcdProcessor;",
            "import com.nepxion.discovery.common.nacos.proccessor.NacosProcessor;",
            "import com.nepxion.discovery.common.redis.proccessor.RedisProcessor;",
            "import com.nepxion.discovery.common.zookeeper.proccessor.ZookeeperProcessor;",
            "import com.nepxion.discovery.plugin.strategy.constant.StrategyConstant;",
            "import com.nepxion.discovery.plugin.strategy.gateway.constant.GatewayStrategyConstant;",
            "import com.nepxion.discovery.plugin.strategy.gateway.context.GatewayStrategyContextListener;",
            "import com.nepxion.discovery.plugin.strategy.gateway.filter.DefaultGatewayStrategyClearFilter;",
            "import com.nepxion.discovery.plugin.strategy.gateway.filter.DefaultGatewayStrategyRouteFilter;",
            "import com.nepxion.discovery.plugin.strategy.gateway.filter.GatewayStrategyClearFilter;",
            "import com.nepxion.discovery.plugin.strategy.gateway.filter.GatewayStrategyRouteFilter;",
            "import com.nepxion.discovery.plugin.strategy.gateway.filter.SkyWalkingGatewayStrategyFilter;",
            "import com.nepxion.discovery.plugin.strategy.gateway.monitor.DefaultGatewayStrategyMonitor;",
            "import com.nepxion.discovery.plugin.strategy.gateway.monitor.GatewayStrategyMonitor;",
            "import com.nepxion.discovery.plugin.strategy.gateway.processor.GatewayStrategyRouteApolloProcessor;",
            "import com.nepxion.discovery.plugin.strategy.gateway.processor.GatewayStrategyRouteConsulProcessor;",
            "import com.nepxion.discovery.plugin.strategy.gateway.processor.GatewayStrategyRouteEtcdProcessor;",
            "import com.nepxion.discovery.plugin.strategy.gateway.processor.GatewayStrategyRouteNacosProcessor;",
            "import com.nepxion.discovery.plugin.strategy.gateway.processor.GatewayStrategyRouteRedisProcessor;",
            "import com.nepxion.discovery.plugin.strategy.gateway.processor.GatewayStrategyRouteZookeeperProcessor;",
            "import com.nepxion.discovery.plugin.strategy.gateway.route.DefaultGatewayStrategyRoute;",
            "import com.nepxion.discovery.plugin.strategy.gateway.route.GatewayStrategyRoute;",
            "import com.nepxion.discovery.plugin.strategy.gateway.wrapper.DefaultGatewayStrategyCallableWrapper;",
            "import com.nepxion.discovery.plugin.strategy.gateway.wrapper.GatewayStrategyCallableWrapper;"
        ],
        "reference_api": [],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "2227324689/gpmall",
        "function_declaration": "public  CuratorFramework createCuratorFramework()",
        "start_line": "37",
        "end_line": "46",
        "file_path": "gpmall-commons/commons-tool/src/main/java/com/gpmall/commons/tool/zookeeperConfig/CuratorFrameworkClient.java",
        "docstring": "The createCuratorFramework function initializes a CuratorFramework instance with specific properties and a retry policy.\\nIt configures the connection settings using ZooKeeper client properties, including hosts, session timeout, and namespace.\\nAfter setting the retry policy, it starts the CuratorFramework and returns the initialized instance.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "68a13b54ad7c",
        "ground_truth": "public  CuratorFramework createCuratorFramework(){\n    RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000,100,2000);\n    curatorFramework  = (CuratorFramework) CuratorFrameworkFactory.builder().\n            connectString(zooKeeperClientProperties.getZkHosts()).\n            sessionTimeoutMs(zooKeeperClientProperties.getSessionTimeout()).\n            namespace(zooKeeperClientProperties.getNamespace()).\n            retryPolicy(retryPolicy);\n    curatorFramework.start();\n    return curatorFramework;\n}",
        "import_statements": [
            "import org.apache.curator.RetryPolicy;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.CuratorFrameworkFactory;",
            "import org.apache.curator.retry.ExponentialBackoffRetry;",
            "import org.springframework.beans.factory.annotation.Autowired;"
        ],
        "reference_api": [
            "sessionTimeoutMs",
            "namespace",
            "getSessionTimeout",
            "retryPolicy",
            "connectString",
            "getZkHosts",
            "start",
            "builder",
            "getNamespace"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "sessionTimeoutMs",
            "namespace",
            "getSessionTimeout",
            "retryPolicy",
            "connectString",
            "getZkHosts",
            "start",
            "builder",
            "getNamespace"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "2227324689/gpmall",
        "function_declaration": "public void lock(String path) throws DistributedLockException",
        "start_line": "43",
        "end_line": "65",
        "file_path": "gpmall-commons/commons-lock/src/main/java/com/gpmall/commons/lock/impl/zk_v1/DistributedZooKeeperReentrantLock.java",
        "docstring": "The lock function attempts to acquire a distributed lock for the given path using an InterProcessMutex.\\nIf the current thread does not already hold a lock, it creates a new InterProcessMutex, tries to acquire the lock, and stores it in locksMap.\\nIf the thread already holds a lock, it re-acquires the existing lock.\\nIf an error occurs during lock acquisition, a DistributedLockException is thrown.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "018a3d7b5f59",
        "ground_truth": "public void lock(String path) throws DistributedLockException {\n    if(locksMap.get(Thread.currentThread())==null){\n        //\u4e92\u65a5\u53ef\u91cd\u5165\u9501\uff0c\u4e2a\u4eba\u7406\u89e3interProcessMutex \u7ecf\u6d4e\u662f\u4e00\u4e2a\u9501\u7684\u8282\u70b9\uff0cpath \u5bf9\u5e94\u7684\u8282\u70b9\u624d\u662f\u4e00\u4e2a\u552f\u4e00\u7684\u9501\u5bf9\u8c61\n        InterProcessMutex interProcessMutex = new InterProcessMutex(curatorFrameworkClient.getZkCleint(),path);\n        //\u4e00\u81f4\u7b49\u5f85\u83b7\u5f97\u9501,\u4f1a\u5728path \u4e0b\u521b\u5efa\u4e00\u4e2a\u4e34\u65f6\u6709\u5e8f\u63a5\u70b9\n       try {\n           interProcessMutex.acquire();\n       }catch (Exception e) {\n           throw new DistributedLockException(\"zk-acquire\u52a0\u9501\u5f02\u5e38: \", e);\n       }\n        locksMap.put(Thread.currentThread(),interProcessMutex);\n    }else{\n        InterProcessMutex interProcessMutex = locksMap.get(Thread.currentThread());\n        //\u4e00\u81f4\u7b49\u5f85\u83b7\u5f97\u9501\n        try {\n            interProcessMutex.acquire();\n        }catch (Exception e) {\n            throw new DistributedLockException(\"zk-acquire\u52a0\u9501\u5f02\u5e38: \", e);\n        }\n    }\n}",
        "import_statements": [
            "import com.gpmall.commons.lock.ApplicationContextUtils;",
            "import com.gpmall.commons.lock.DistributedLock;",
            "import com.gpmall.commons.lock.DistributedLockException;",
            "import com.gpmall.commons.tool.zookeeperConfig.CuratorFrameworkClient;",
            "import org.apache.curator.CuratorZookeeperClient;",
            "import org.apache.curator.framework.recipes.locks.InterProcessLock;",
            "import org.apache.curator.framework.recipes.locks.InterProcessMutex;",
            "import org.redisson.api.RLock;",
            "import org.redisson.api.RedissonClient;",
            "import java.util.concurrent.ConcurrentHashMap;",
            "import java.util.concurrent.TimeUnit;"
        ],
        "reference_api": [
            "currentThread",
            "acquire",
            "get",
            "put",
            "getZkCleint"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "currentThread",
            "acquire",
            "get",
            "put",
            "getZkCleint"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "dromara/dynamic-tp",
        "function_declaration": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties)",
        "start_line": "88",
        "end_line": "103",
        "file_path": "starter/starter-configcenter/starter-zookeeper/src/main/java/org/dromara/dynamictp/starter/zookeeper/util/CuratorUtil.java",
        "docstring": "The genPropertiesMap function generates a properties map based on the configuration type specified in dtpProperties.\\nIt first initializes a CuratorFramework instance and determines the node path.\\nIf the configuration type is PROPERTIES, it generates the map using genPropertiesTypeMap.\\nIf the configuration type is JSON, it constructs the node path with a config key, retrieves the value, and parses it into a map using ConfigHandler.\\nThe resulting map is then returned.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2068af09c973",
        "ground_truth": "public static Map<Object, Object> genPropertiesMap(DtpProperties dtpProperties) {\n    val curatorFramework = getCuratorFramework(dtpProperties);\n    String nodePath = nodePath(dtpProperties);\n    Map<Object, Object> result = Maps.newHashMap();\n    if (PROPERTIES.getValue().equalsIgnoreCase(dtpProperties.getConfigType().trim())) {\n        result = genPropertiesTypeMap(nodePath, curatorFramework);\n    } else if (JSON.getValue().equalsIgnoreCase(dtpProperties.getConfigType().trim())) {\n        nodePath = ZKPaths.makePath(nodePath, dtpProperties.getZookeeper().getConfigKey());\n        String value = getVal(nodePath, curatorFramework);\n        result = ConfigHandler.getInstance().parseConfig(value, JSON);\n    }\n    return result;\n}",
        "import_statements": [
            "import org.dromara.dynamictp.common.properties.DtpProperties;",
            "import org.dromara.dynamictp.core.handler.ConfigHandler;",
            "import com.google.common.collect.Maps;",
            "import lombok.SneakyThrows;",
            "import lombok.extern.slf4j.Slf4j;",
            "import lombok.val;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.CuratorFrameworkFactory;",
            "import org.apache.curator.framework.api.GetChildrenBuilder;",
            "import org.apache.curator.framework.api.GetDataBuilder;",
            "import org.apache.curator.framework.state.ConnectionState;",
            "import org.apache.curator.framework.state.ConnectionStateListener;",
            "import org.apache.curator.retry.ExponentialBackoffRetry;",
            "import org.apache.curator.utils.ZKPaths;",
            "import java.nio.charset.StandardCharsets;",
            "import java.util.Collections;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.concurrent.CountDownLatch;",
            "import static org.dromara.dynamictp.common.em.ConfigFileTypeEnum.JSON;",
            "import static org.dromara.dynamictp.common.em.ConfigFileTypeEnum.PROPERTIES;"
        ],
        "reference_api": [
            "nodePath",
            "getCuratorFramework",
            "getZookeeper",
            "getValue",
            "newHashMap",
            "trim",
            "equalsIgnoreCase",
            "genPropertiesTypeMap",
            "parseConfig",
            "getInstance",
            "makePath",
            "getConfigKey",
            "getVal",
            "getConfigType"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "nodePath",
                "code": "public static String nodePath(DtpProperties dtpProperties) {\n        DtpProperties.Zookeeper zookeeper = dtpProperties.getZookeeper();\n        return ZKPaths.makePath(ZKPaths.makePath(zookeeper.getRootNode(),\n                zookeeper.getConfigVersion()), zookeeper.getNode());\n    }"
            },
            {
                "name": "getCuratorFramework",
                "code": "public static CuratorFramework getCuratorFramework(DtpProperties dtpProperties) {\n        if (curatorFramework == null) {\n            DtpProperties.Zookeeper zookeeper = dtpProperties.getZookeeper();\n            curatorFramework = CuratorFrameworkFactory.newClient(zookeeper.getZkConnectStr(),\n                    new ExponentialBackoffRetry(1000, 3));\n            final ConnectionStateListener connectionStateListener = (client, newState) -> {\n                if (newState == ConnectionState.CONNECTED) {\n                    COUNT_DOWN_LATCH.countDown();\n                }\n            };\n            curatorFramework.getConnectionStateListenable().addListener(connectionStateListener);\n            curatorFramework.start();\n            try {\n                COUNT_DOWN_LATCH.await();\n            } catch (InterruptedException e) {\n                log.error(\"get zk client error\", e);\n                Thread.currentThread().interrupt();\n            }\n        }\n        return curatorFramework;\n    }"
            },
            {
                "name": "genPropertiesTypeMap",
                "code": "private static Map<Object, Object> genPropertiesTypeMap(String nodePath, CuratorFramework curatorFramework) {\n        try {\n            final GetChildrenBuilder childrenBuilder = curatorFramework.getChildren();\n            final List<String> children = childrenBuilder.watched().forPath(nodePath);\n            Map<Object, Object> properties = Maps.newHashMap();\n            children.forEach(c -> {\n                String path = ZKPaths.makePath(nodePath, c);\n                final String nodeName = ZKPaths.getNodeFromPath(path);\n                String value = getVal(path, curatorFramework);\n                properties.put(nodeName, value);\n            });\n            return properties;\n        } catch (Exception e) {\n            log.error(\"get zk configs error, nodePath is {}\", nodePath, e);\n            return Collections.emptyMap();\n        }\n    }"
            },
            {
                "name": "getVal",
                "code": "private static String getVal(String path, CuratorFramework curatorFramework) {\n        final GetDataBuilder data = curatorFramework.getData();\n        String value = \"\";\n        try {\n            value = new String(data.watched().forPath(path), StandardCharsets.UTF_8);\n        } catch (Exception e) {\n            log.error(\"get zk config value failed, path: {}\", path, e);\n        }\n        return value;\n    }"
            }
        ],
        "third_party": [
            "getZookeeper",
            "getValue",
            "newHashMap",
            "equalsIgnoreCase",
            "parseConfig",
            "getInstance",
            "makePath",
            "getConfigKey",
            "getConfigType"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "dromara/dynamic-tp",
        "function_declaration": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application)",
        "start_line": "45",
        "end_line": "52",
        "file_path": "starter/starter-configcenter/starter-zookeeper/src/main/java/org/dromara/dynamictp/starter/zookeeper/autoconfigure/ZkConfigEnvironmentProcessor.java",
        "docstring": "The postProcessEnvironment function configures the environment for a Spring application using DtpProperties.\\nIt retrieves an instance of DtpProperties and binds it with the current environment.\\nThen, it generates a properties map from DtpProperties and checks if the required property exists in the environment.\\nIf the property does not exist, it creates a Zookeeper property source with the generated properties.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3d3c9020d299",
        "ground_truth": "public void postProcessEnvironment(ConfigurableEnvironment environment, SpringApplication application) {\n    DtpProperties dtpProperties = DtpProperties.getInstance();\n    BinderHelper.bindDtpProperties(environment, dtpProperties);\n    Map<Object, Object> properties = CuratorUtil.genPropertiesMap(dtpProperties);\n    if (!checkPropertyExist(environment)) {\n        createZkPropertySource(environment, properties);\n    }\n}",
        "import_statements": [
            "import lombok.extern.slf4j.Slf4j;",
            "import org.dromara.dynamictp.common.properties.DtpProperties;",
            "import org.dromara.dynamictp.core.support.BinderHelper;",
            "import org.dromara.dynamictp.starter.zookeeper.util.CuratorUtil;",
            "import org.springframework.boot.SpringApplication;",
            "import org.springframework.boot.env.EnvironmentPostProcessor;",
            "import org.springframework.boot.env.OriginTrackedMapPropertySource;",
            "import org.springframework.core.Ordered;",
            "import org.springframework.core.env.ConfigurableEnvironment;",
            "import org.springframework.core.env.MutablePropertySources;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "bindDtpProperties",
            "checkPropertyExist",
            "genPropertiesMap",
            "createZkPropertySource",
            "getInstance"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "checkPropertyExist",
                "code": "private boolean checkPropertyExist(ConfigurableEnvironment environment) {\n        MutablePropertySources propertySources = environment.getPropertySources();\n        return propertySources.stream().anyMatch(p -> ZK_PROPERTY_SOURCE_NAME.equals(p.getName()));\n    }"
            },
            {
                "name": "createZkPropertySource",
                "code": "private void createZkPropertySource(ConfigurableEnvironment environment, Map<Object, Object> properties) {\n        MutablePropertySources propertySources = environment.getPropertySources();\n        OriginTrackedMapPropertySource zkSource = new OriginTrackedMapPropertySource(ZK_PROPERTY_SOURCE_NAME, properties);\n        propertySources.addLast(zkSource);\n    }"
            }
        ],
        "third_party": [
            "bindDtpProperties",
            "genPropertiesMap",
            "getInstance"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo)",
        "start_line": "91",
        "end_line": "102",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZookeeperModule.cs",
        "docstring": "The UseZooKeeperRouteManager function configures the builder to use a ZooKeeper-based service route manager.\\nIt sets up the necessary dependencies including serializers, route factory, logger, and Zookeeper client provider.\\nThe function returns the current instance of ZookeeperModule.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0a934e7885be",
        "ground_truth": "public ZookeeperModule UseZooKeeperRouteManager(ContainerBuilderWrapper builder, ConfigInfo configInfo)\n{\n    UseRouteManager(builder, provider =>\n  new ZooKeeperServiceRouteManager(\n     GetConfigInfo(configInfo),\n   provider.GetRequiredService<ISerializer<byte[]>>(),\n     provider.GetRequiredService<ISerializer<string>>(),\n     provider.GetRequiredService<IServiceRouteFactory>(),\n     provider.GetRequiredService<ILogger<ZooKeeperServiceRouteManager>>(),\n          provider.GetRequiredService<IZookeeperClientProvider>()));\n    return this;\n}",
        "import_statements": [
            "using Microsoft.Extensions.Configuration",
            "using Microsoft.Extensions.DependencyInjection",
            "using Microsoft.Extensions.Logging",
            "using Surging.Core.CPlatform",
            "using Surging.Core.CPlatform.Cache",
            "using Surging.Core.CPlatform.Module",
            "using Surging.Core.CPlatform.Mqtt",
            "using Surging.Core.CPlatform.Routing",
            "using Surging.Core.CPlatform.Runtime.Client",
            "using Surging.Core.CPlatform.Runtime.Server",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.CPlatform.Support",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.Internal.Cluster.HealthChecks",
            "using Surging.Core.Zookeeper.Internal.Cluster.HealthChecks.Implementation",
            "using Surging.Core.Zookeeper.Internal.Cluster.Implementation.Selectors",
            "using Surging.Core.Zookeeper.Internal.Cluster.Implementation.Selectors.Implementation",
            "using Surging.Core.Zookeeper.Internal.Implementation",
            "using System"
        ],
        "reference_api": [
            "UseRouteManager",
            "GetConfigInfo",
            "provider.GetRequiredService<ILogger<ZooKeeperServiceRouteManager>>",
            "provider.GetRequiredService<ISerializer<byte[]>>",
            "provider.GetRequiredService<IServiceRouteFactory>",
            "provider.GetRequiredService<ISerializer<string>>",
            "provider.GetRequiredService<IZookeeperClientProvider>"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "UseRouteManager",
                "code": "public ContainerBuilderWrapper UseRouteManager(ContainerBuilderWrapper builder, Func<IServiceProvider, IServiceRouteManager> factory)\n        {\n            builder.RegisterAdapter(factory).InstancePerLifetimeScope();\n            return builder;\n        }"
            },
            {
                "name": "GetConfigInfo",
                "code": "private static ConfigInfo GetConfigInfo(ConfigInfo config)\n        {\n            ZookeeperOption option = null;\n            var section = CPlatform.AppConfig.GetSection(\"Zookeeper\");\n            if (section.Exists())\n                option = section.Get<ZookeeperOption>();\n            else if (AppConfig.Configuration != null)\n                option = AppConfig.Configuration.Get<ZookeeperOption>();\n            if (option != null)\n            {\n                var sessionTimeout = config.SessionTimeout.TotalSeconds;\n                Double.TryParse(option.SessionTimeout, out sessionTimeout);\n                config = new ConfigInfo(\n                    option.ConnectionString,\n                    TimeSpan.FromSeconds(sessionTimeout),\n                    option.RoutePath ?? config.RoutePath,\n                    option.SubscriberPath ?? config.SubscriberPath,\n                    option.CommandPath ?? config.CommandPath,\n                    option.CachePath ?? config.CachePath,\n                    option.MqttRoutePath ?? config.MqttRoutePath,\n                    option.ChRoot ?? config.ChRoot,\n                    option.ReloadOnChange != null ? bool.Parse(option.ReloadOnChange) :\n                    config.ReloadOnChange,\n                   option.EnableChildrenMonitor != null ? bool.Parse(option.EnableChildrenMonitor) :\n                    config.EnableChildrenMonitor\n                   );\n            }\n            return config;\n        }"
            }
        ],
        "third_party": [
            "provider.GetRequiredService<ILogger<ZooKeeperServiceRouteManager>>",
            "provider.GetRequiredService<ISerializer<byte[]>>",
            "provider.GetRequiredService<IServiceRouteFactory>",
            "provider.GetRequiredService<ISerializer<string>>",
            "provider.GetRequiredService<IZookeeperClientProvider>"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": " public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo)",
        "start_line": "127",
        "end_line": "142",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZookeeperModule.cs",
        "docstring": "The UseZooKeeperCommandManager function configures the ZookeeperModule to use a ZookeeperServiceCommandManager.\\nIt takes a ContainerBuilderWrapper and ConfigInfo as parameters, sets up the command manager with required services, and returns the ZookeeperModule instance.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c30a17587331",
        "ground_truth": "public ZookeeperModule UseZooKeeperCommandManager(ContainerBuilderWrapper builder, ConfigInfo configInfo)\n{\n    UseCommandManager(builder, provider =>\n   {\n       var result = new ZookeeperServiceCommandManager(\n           GetConfigInfo(configInfo),\n         provider.GetRequiredService<ISerializer<byte[]>>(),\n           provider.GetRequiredService<ISerializer<string>>(),\n         provider.GetRequiredService<IServiceRouteManager>(),\n           provider.GetRequiredService<IServiceEntryManager>(),\n           provider.GetRequiredService<ILogger<ZookeeperServiceCommandManager>>(),\n          provider.GetRequiredService<IZookeeperClientProvider>());\n       return result;\n   });\n    return this;\n}",
        "import_statements": [
            "using Microsoft.Extensions.Configuration",
            "using Microsoft.Extensions.DependencyInjection",
            "using Microsoft.Extensions.Logging",
            "using Surging.Core.CPlatform",
            "using Surging.Core.CPlatform.Cache",
            "using Surging.Core.CPlatform.Module",
            "using Surging.Core.CPlatform.Mqtt",
            "using Surging.Core.CPlatform.Routing",
            "using Surging.Core.CPlatform.Runtime.Client",
            "using Surging.Core.CPlatform.Runtime.Server",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.CPlatform.Support",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.Internal.Cluster.HealthChecks",
            "using Surging.Core.Zookeeper.Internal.Cluster.HealthChecks.Implementation",
            "using Surging.Core.Zookeeper.Internal.Cluster.Implementation.Selectors",
            "using Surging.Core.Zookeeper.Internal.Cluster.Implementation.Selectors.Implementation",
            "using Surging.Core.Zookeeper.Internal.Implementation",
            "using System"
        ],
        "reference_api": [
            "GetConfigInfo",
            "provider.GetRequiredService<IServiceEntryManager>",
            "UseCommandManager",
            "provider.GetRequiredService<ISerializer<byte[]>>",
            "provider.GetRequiredService<ISerializer<string>>",
            "provider.GetRequiredService<ILogger<ZookeeperServiceCommandManager>>",
            "provider.GetRequiredService<IServiceRouteManager>",
            "provider.GetRequiredService<IZookeeperClientProvider>"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "GetConfigInfo",
                "code": "private static ConfigInfo GetConfigInfo(ConfigInfo config)\n        {\n            ZookeeperOption option = null;\n            var section = CPlatform.AppConfig.GetSection(\"Zookeeper\");\n            if (section.Exists())\n                option = section.Get<ZookeeperOption>();\n            else if (AppConfig.Configuration != null)\n                option = AppConfig.Configuration.Get<ZookeeperOption>();\n            if (option != null)\n            {\n                var sessionTimeout = config.SessionTimeout.TotalSeconds;\n                Double.TryParse(option.SessionTimeout, out sessionTimeout);\n                config = new ConfigInfo(\n                    option.ConnectionString,\n                    TimeSpan.FromSeconds(sessionTimeout),\n                    option.RoutePath ?? config.RoutePath,\n                    option.SubscriberPath ?? config.SubscriberPath,\n                    option.CommandPath ?? config.CommandPath,\n                    option.CachePath ?? config.CachePath,\n                    option.MqttRoutePath ?? config.MqttRoutePath,\n                    option.ChRoot ?? config.ChRoot,\n                    option.ReloadOnChange != null ? bool.Parse(option.ReloadOnChange) :\n                    config.ReloadOnChange,\n                   option.EnableChildrenMonitor != null ? bool.Parse(option.EnableChildrenMonitor) :\n                    config.EnableChildrenMonitor\n                   );\n            }\n            return config;\n        }"
            },
            {
                "name": "UseCommandManager",
                "code": "public ContainerBuilderWrapper UseCommandManager(ContainerBuilderWrapper builder, Func<IServiceProvider, IServiceCommandManager> factory)\n        {\n            builder.RegisterAdapter(factory).InstancePerLifetimeScope();\n            return builder;\n        }"
            }
        ],
        "third_party": [
            "provider.GetRequiredService<IServiceEntryManager>",
            "provider.GetRequiredService<ISerializer<byte[]>>",
            "provider.GetRequiredService<ISerializer<string>>",
            "provider.GetRequiredService<ILogger<ZookeeperServiceCommandManager>>",
            "provider.GetRequiredService<IServiceRouteManager>",
            "provider.GetRequiredService<IZookeeperClientProvider>"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange)",
        "start_line": "28",
        "end_line": "47",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/Configurations/ZookeeperConfigurationExtensions.cs",
        "docstring": "The AddZookeeperFile function extends IConfigurationBuilder to include a Zookeeper configuration file.\\nIt validates the builder and path parameters, assigns a PhysicalFileProvider if the path is rooted and provider is null, and creates a ZookeeperConfigurationSource with the specified properties.\\nThe source is added to the builder, the configuration is built, and the builder is returned.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "213cc4aa9b19",
        "ground_truth": "public static IConfigurationBuilder AddZookeeperFile(this IConfigurationBuilder builder, IFileProvider provider, string path, bool optional, bool reloadOnChange)\n{\n    Check.NotNull(builder, \"builder\");\n    Check.CheckCondition(() => string.IsNullOrEmpty(path), \"path\");\n    if (provider == null && Path.IsPathRooted(path))\n    {\n        provider = new PhysicalFileProvider(Path.GetDirectoryName(path));\n        path = Path.GetFileName(path);\n    }\n    var source = new ZookeeperConfigurationSource\n    {\n        FileProvider = provider,\n        Path = path,\n        Optional = optional,\n        ReloadOnChange = reloadOnChange\n    };\n    builder.Add(source);\n    AppConfig.Configuration = builder.Build();\n    return builder;\n}",
        "import_statements": [
            "using Microsoft.Extensions.Configuration",
            "using Microsoft.Extensions.FileProviders",
            "using Surging.Core.CPlatform.Utilities",
            "using System",
            "using System.Collections.Generic",
            "using System.IO",
            "using System.Text"
        ],
        "reference_api": [
            "string.IsNullOrEmpty",
            "builder.Add",
            "Path.GetDirectoryName",
            "builder.Build",
            "Path.IsPathRooted",
            "Path.GetFileName",
            "Check.NotNull",
            "Check.CheckCondition"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "string.IsNullOrEmpty",
            "builder.Add",
            "Path.GetDirectoryName",
            "builder.Build",
            "Path.IsPathRooted",
            "Path.GetFileName",
            "Check.NotNull",
            "Check.CheckCondition"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)",
        "start_line": "144",
        "end_line": "152",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZooKeeperServiceRouteManager.cs",
        "docstring": "The RemveAddressAsync function asynchronously removes specified addresses from a list of routes.\\nIt retrieves the current routes, removes the provided addresses from each route, and updates the routes using SetRoutesAsync.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "15bbffa82a13",
        "ground_truth": "public override async Task RemveAddressAsync(IEnumerable<AddressModel> Address)\n{\n    var routes = await GetRoutesAsync();\n    foreach (var route in routes)\n    {\n        route.Address = route.Address.Except(Address);\n    }\n    await base.SetRoutesAsync(routes);\n}",
        "import_statements": [
            "using Microsoft.Extensions.Logging",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Address",
            "using Surging.Core.CPlatform.Routing",
            "using Surging.Core.CPlatform.Routing.Implementation",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.CPlatform.Transport.Implementation",
            "using Surging.Core.CPlatform.Utilities",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using System",
            "using System.Collections.Concurrent",
            "using System.Collections.Generic",
            "using System.Linq",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks"
        ],
        "reference_api": [
            "route.Address.Except",
            "GetRoutesAsync",
            "base.SetRoutesAsync"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "GetRoutesAsync",
                "code": "public override async Task<IEnumerable<ServiceRoute>> GetRoutesAsync()\n        {\n            await EnterRoutes();\n            return _routes;\n        }"
            }
        ],
        "third_party": [
            "route.Address.Except",
            "base.SetRoutesAsync"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)",
        "start_line": "181",
        "end_line": "206",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZooKeeperServiceRouteManager.cs",
        "docstring": "The RemoveExceptRoutesAsync function removes specific routes from Zookeeper except those provided in the routes parameter.\\nIt constructs the route path and retrieves the current Zookeeper clients.\\nFor each Zookeeper client, it compares the existing routes with the new routes and identifies routes to be deleted.\\nIf the addresses of the routes to be deleted contain the specified hostAddr, it deletes the corresponding node from Zookeeper.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "125d23de2a60",
        "ground_truth": "private async Task RemoveExceptRoutesAsync(IEnumerable<ServiceRoute> routes, AddressModel hostAddr)\n{\n    var path = _configInfo.RoutePath;\n    if (!path.EndsWith(\"/\"))\n        path += \"/\";\n    routes = routes.ToArray();\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers)\n    {\n        if (_routes != null)\n        {\n            var oldRouteIds = _routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var newRouteIds = routes.Select(i => i.ServiceDescriptor.Id).ToArray();\n            var deletedRouteIds = oldRouteIds.Except(newRouteIds).ToArray();\n            foreach (var deletedRouteId in deletedRouteIds)\n            {\n                var addresses = _routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault();\n                if (addresses.Contains(hostAddr))\n                {\n                    var nodePath = $\"{path}{deletedRouteId}\";\n                    await zooKeeper.Item2.deleteAsync(nodePath);\n                }\n            }\n        }\n    }\n}",
        "import_statements": [
            "using Microsoft.Extensions.Logging",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Address",
            "using Surging.Core.CPlatform.Routing",
            "using Surging.Core.CPlatform.Routing.Implementation",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.CPlatform.Transport.Implementation",
            "using Surging.Core.CPlatform.Utilities",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using System",
            "using System.Collections.Concurrent",
            "using System.Collections.Generic",
            "using System.Linq",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks"
        ],
        "reference_api": [
            "_routes.Select(i => i.ServiceDescriptor.Id).ToArray",
            "_routes.Select",
            "_routes.Where",
            "_zookeeperClientProvider.GetZooKeepers",
            "oldRouteIds.Except",
            "oldRouteIds.Except(newRouteIds).ToArray",
            "routes.Select",
            "_routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select",
            "zooKeeper.Item2.deleteAsync",
            "routes.Select(i => i.ServiceDescriptor.Id).ToArray",
            "path.EndsWith",
            "addresses.Contains",
            "_routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault",
            "routes.ToArray"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "_routes.Select(i => i.ServiceDescriptor.Id).ToArray",
            "_routes.Select",
            "_routes.Where",
            "_zookeeperClientProvider.GetZooKeepers",
            "oldRouteIds.Except",
            "oldRouteIds.Except(newRouteIds).ToArray",
            "routes.Select",
            "_routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select",
            "zooKeeper.Item2.deleteAsync",
            "routes.Select(i => i.ServiceDescriptor.Id).ToArray",
            "path.EndsWith",
            "addresses.Contains",
            "_routes.Where(p => p.ServiceDescriptor.Id == deletedRouteId).Select(p => p.Address).FirstOrDefault",
            "routes.ToArray"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "private async Task<ServiceRoute> GetRoute(byte[] data)",
        "start_line": "231",
        "end_line": "241",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZooKeeperServiceRouteManager.cs",
        "docstring": "The GetRoute function asynchronously retrieves a ServiceRoute from the provided byte array data.\\nIf logging at the Debug level is enabled, it logs the route data.\\nIf the data is null, it returns null.\\nIt deserializes the byte array into a ServiceRouteDescriptor and uses a service route factory to create and return the first ServiceRoute.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b020a235d57f",
        "ground_truth": "private async Task<ServiceRoute> GetRoute(byte[] data)\n{\n    if (_logger.IsEnabled(LogLevel.Debug))\n        _logger.LogDebug($\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\");\n    if (data == null)\n        return null;\n    var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n    return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n}",
        "import_statements": [
            "using Microsoft.Extensions.Logging",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Address",
            "using Surging.Core.CPlatform.Routing",
            "using Surging.Core.CPlatform.Routing.Implementation",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.CPlatform.Transport.Implementation",
            "using Surging.Core.CPlatform.Utilities",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using System",
            "using System.Collections.Concurrent",
            "using System.Collections.Generic",
            "using System.Linq",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks"
        ],
        "reference_api": [
            "_logger.IsEnabled",
            ", ServiceRouteDescriptor>(data);\n            return (aw",
            "viceRoutesAsync(new[] { descriptor })).First(",
            "eateServiceRoutesAsync(new[] { descriptor })).First();\n        }",
            "_logger.LogDebug",
            ")}\u3002\");\n\n            if "
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "_logger.IsEnabled",
            ", ServiceRouteDescriptor>(data);\n            return (aw",
            "viceRoutesAsync(new[] { descriptor })).First(",
            "eateServiceRoutesAsync(new[] { descriptor })).First();\n        }",
            "_logger.LogDebug",
            ")}\u3002\");\n\n            if "
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "public async Task NodeChange(byte[] oldData, byte[] newData)",
        "start_line": "318",
        "end_line": "338",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZooKeeperServiceRouteManager.cs",
        "docstring": "The NodeChange function asynchronously handles changes in node data.\\nIf the old and new data are identical, it returns immediately.\\nIt retrieves the new route from the newData and finds the corresponding old route.\\nThe function updates the _routes list by replacing the old route with the new one within a thread-safe lock.\\nFinally, it triggers the OnChanged event with the new and old route information.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "169ed4cc2ad4",
        "ground_truth": "public async Task NodeChange(byte[] oldData, byte[] newData)\n{\n    if (DataEquals(oldData, newData))\n        return;\n    var newRoute = await GetRoute(newData);\n    //\u5f97\u5230\u65e7\u7684\u8def\u7531\u3002\n    var oldRoute = _routes.FirstOrDefault(i => i.ServiceDescriptor.Id == newRoute.ServiceDescriptor.Id);\n    lock (_routes)\n    {\n        //\u5220\u9664\u65e7\u8def\u7531\uff0c\u5e76\u6dfb\u52a0\u4e0a\u65b0\u7684\u8def\u7531\u3002\n        _routes =\n            _routes\n                .Where(i => i.ServiceDescriptor.Id != newRoute.ServiceDescriptor.Id)\n                .Concat(new[] { newRoute }).ToArray();\n    }\n    //\u89e6\u53d1\u8def\u7531\u53d8\u66f4\u4e8b\u4ef6\u3002\n    OnChanged(new ServiceRouteChangedEventArgs(newRoute, oldRoute));\n}",
        "import_statements": [
            "using Microsoft.Extensions.Logging",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Address",
            "using Surging.Core.CPlatform.Routing",
            "using Surging.Core.CPlatform.Routing.Implementation",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.CPlatform.Transport.Implementation",
            "using Surging.Core.CPlatform.Utilities",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using System",
            "using System.Collections.Concurrent",
            "using System.Collections.Generic",
            "using System.Linq",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks"
        ],
        "reference_api": [
            "GetRoute",
            "i.ServiceDescriptor.Id != newRoute.ServiceDescriptor.Id)\n                        .Concat(new[] { newRoute }).ToArray();\n            ",
            "DataEquals",
            ");\n      ",
            "i.ServiceDescriptor.Id != newRoute.ServiceDescriptor.Id)\n                        .Concat(new[] { newRoute }).ToArray();\n            }\n\n            //\u89e6\u53d1\u8def\u7531\u53d8\u66f4\u4e8b\u4ef6\u3002\n ",
            "rDefault(i => i.Servic",
            "i.ServiceDescriptor.Id != newRoute.Ser"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "GetRoute",
                "code": "private async Task<ServiceRoute> GetRoute(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u8def\u7531\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceRouteDescriptor>(data);\n            return (await _serviceRouteFactory.CreateServiceRoutesAsync(new[] { descriptor })).First();\n        }"
            },
            {
                "name": "DataEquals",
                "code": "private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }"
            }
        ],
        "third_party": [
            "i.ServiceDescriptor.Id != newRoute.ServiceDescriptor.Id)\n                        .Concat(new[] { newRoute }).ToArray();\n            ",
            ");\n      ",
            "i.ServiceDescriptor.Id != newRoute.ServiceDescriptor.Id)\n                        .Concat(new[] { newRoute }).ToArray();\n            }\n\n            //\u89e6\u53d1\u8def\u7531\u53d8\u66f4\u4e8b\u4ef6\u3002\n ",
            "rDefault(i => i.Servic",
            "i.ServiceDescriptor.Id != newRoute.Ser"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers)",
        "start_line": "151",
        "end_line": "164",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZooKeeperServiceSubscribeManager.cs",
        "docstring": "The SetSubscribersAsync function updates service subscribers asynchronously.\\nIt retrieves existing subscribers matching the provided ones and updates the address list of each subscriber by merging new addresses with existing ones.\\nFinally, it calls the base method to set the updated subscribers.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "af01261d3199",
        "ground_truth": "public override async Task SetSubscribersAsync(IEnumerable<ServiceSubscriber> subscribers)\n{\n    var serviceSubscribers = await GetSubscribers(subscribers.Select(p => p.ServiceDescriptor.Id));\n    foreach (var subscriber in subscribers)\n    {\n        var serviceSubscriber = serviceSubscribers.Where(p => p.ServiceDescriptor.Id == subscriber.ServiceDescriptor.Id).FirstOrDefault();\n        if (serviceSubscriber != null)\n        {\n            subscriber.Address = subscriber.Address.Concat(\n                subscriber.Address.Except(serviceSubscriber.Address));\n        }\n    }\n    await base.SetSubscribersAsync(subscribers);\n}",
        "import_statements": [
            "using Surging.Core.CPlatform.Runtime.Client.Implementation",
            "using System",
            "using System.Collections.Generic",
            "using System.Text",
            "using Surging.Core.CPlatform.Runtime.Client",
            "using System.Threading.Tasks",
            "using Surging.Core.Zookeeper.Configurations",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Serialization",
            "using System.Threading",
            "using Microsoft.Extensions.Logging",
            "using System.Linq",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using Surging.Core.Zookeeper.Internal"
        ],
        "reference_api": [
            "subscribers.Select",
            "GetSubscribers",
            "base.SetSubscribersAsync",
            "serviceSubscribers.Where(p => p.ServiceDescriptor.Id == subscriber.ServiceDescriptor.Id).FirstOrDefault",
            "serviceSubscribers.Where",
            "subscriber.Address.Concat",
            "subscriber.Address.Except"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "GetSubscribers",
                "code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.SubscriberPath;\n            if (!rootPath.EndsWith(\"/\"))\n                rootPath += \"/\";\n\n            childrens = childrens.ToArray();\n            var subscribers = new List<ServiceSubscriber>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8ba2\u9605\u8005\u4fe1\u606f\u3002\");\n\n                var nodePath = $\"{rootPath}{children}\";\n                var subscriber = await GetSubscriber(nodePath);\n                if (subscriber != null)\n                    subscribers.Add(subscriber);\n            }\n            return subscribers.ToArray();\n        }"
            }
        ],
        "third_party": [
            "subscribers.Select",
            "base.SetSubscribersAsync",
            "serviceSubscribers.Where(p => p.ServiceDescriptor.Id == subscriber.ServiceDescriptor.Id).FirstOrDefault",
            "serviceSubscribers.Where",
            "subscriber.Address.Concat",
            "subscriber.Address.Except"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens)",
        "start_line": "215",
        "end_line": "235",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZooKeeperServiceSubscribeManager.cs",
        "docstring": "The GetSubscribers function retrieves an array of ServiceSubscriber objects for the given children node identifiers asynchronously.\\nIt constructs the root path for subscribers and iterates through each child identifier, logging debug information if enabled.\\nFor each child, it constructs the full node path, retrieves the subscriber information, and adds it to the list of subscribers.\\nFinally, it returns the list of subscribers as an array.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "fda02cec3296",
        "ground_truth": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens)\n{\n    var rootPath = _configInfo.SubscriberPath;\n    if (!rootPath.EndsWith(\"/\"))\n        rootPath += \"/\";\n    childrens = childrens.ToArray();\n    var subscribers = new List<ServiceSubscriber>(childrens.Count());\n    foreach (var children in childrens)\n    {\n        if (_logger.IsEnabled(LogLevel.Debug))\n            _logger.LogDebug($\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8ba2\u9605\u8005\u4fe1\u606f\u3002\");\n        var nodePath = $\"{rootPath}{children}\";\n        var subscriber = await GetSubscriber(nodePath);\n        if (subscriber != null)\n            subscribers.Add(subscriber);\n    }\n    return subscribers.ToArray();\n}",
        "import_statements": [
            "using Surging.Core.CPlatform.Runtime.Client.Implementation",
            "using System",
            "using System.Collections.Generic",
            "using System.Text",
            "using Surging.Core.CPlatform.Runtime.Client",
            "using System.Threading.Tasks",
            "using Surging.Core.Zookeeper.Configurations",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Serialization",
            "using System.Threading",
            "using Microsoft.Extensions.Logging",
            "using System.Linq",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using Surging.Core.Zookeeper.Internal"
        ],
        "reference_api": [
            "           }\n  ",
            "_logger.IsEnabled",
            " }",
            "childrens.Count",
            "           if",
            "_logger.LogDebug",
            "childrens.ToArray",
            "rootPath.EndsWith"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "           }\n  ",
            "_logger.IsEnabled",
            " }",
            "childrens.Count",
            "           if",
            "_logger.LogDebug",
            "childrens.ToArray",
            "rootPath.EndsWith"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "private async Task EnterSubscribers()",
        "start_line": "237",
        "end_line": "256",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZooKeeperServiceSubscribeManager.cs",
        "docstring": "The EnterSubscribers function initializes the _subscribers field asynchronously.\\nIf _subscribers is already set, it returns immediately.\\nIt retrieves a ZooKeeper client and waits for the connection.\\nIf the subscriber path exists, it fetches the children nodes and populates _subscribers with their data.\\nIf the path does not exist, it logs a warning and sets _subscribers to an empty array.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6c3018ff0c0d",
        "ground_truth": "private async Task EnterSubscribers()\n{\n    if (_subscribers != null)\n        return;\n    var zooKeeper = await GetZooKeeper();\n    zooKeeper.Item1.WaitOne(); \n    if (await zooKeeper.Item2.existsAsync(_configInfo.SubscriberPath) != null)\n    {\n        var result = await zooKeeper.Item2.getChildrenAsync(_configInfo.SubscriberPath);\n        var childrens = result.Children.ToArray();\n        _subscribers = await GetSubscribers(childrens);\n    }\n    else\n    {\n        if (_logger.IsEnabled(LogLevel.Warning))\n            _logger.LogWarning($\"\u65e0\u6cd5\u83b7\u53d6\u8ba2\u9605\u8005\u4fe1\u606f\uff0c\u56e0\u4e3a\u8282\u70b9\uff1a{_configInfo.SubscriberPath}\uff0c\u4e0d\u5b58\u5728\u3002\");\n        _subscribers = new ServiceSubscriber[0];\n    }\n}",
        "import_statements": [
            "using Surging.Core.CPlatform.Runtime.Client.Implementation",
            "using System",
            "using System.Collections.Generic",
            "using System.Text",
            "using Surging.Core.CPlatform.Runtime.Client",
            "using System.Threading.Tasks",
            "using Surging.Core.Zookeeper.Configurations",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Serialization",
            "using System.Threading",
            "using Microsoft.Extensions.Logging",
            "using System.Linq",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using Surging.Core.Zookeeper.Internal"
        ],
        "reference_api": [
            "_logger.IsEnabled",
            "GetSubscribers",
            "zooKeeper.Item2.getChildrenAsync",
            "_logger.LogWarning",
            "GetZooKeeper",
            "result.Children.ToArray",
            "zooKeeper.Item2.existsAsync",
            "zooKeeper.Item1.WaitOne"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "GetSubscribers",
                "code": "private async Task<ServiceSubscriber[]> GetSubscribers(IEnumerable<string> childrens)\n        {\n            var rootPath = _configInfo.SubscriberPath;\n            if (!rootPath.EndsWith(\"/\"))\n                rootPath += \"/\";\n\n            childrens = childrens.ToArray();\n            var subscribers = new List<ServiceSubscriber>(childrens.Count());\n\n            foreach (var children in childrens)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u51c6\u5907\u4ece\u8282\u70b9\uff1a{children}\u4e2d\u83b7\u53d6\u8ba2\u9605\u8005\u4fe1\u606f\u3002\");\n\n                var nodePath = $\"{rootPath}{children}\";\n                var subscriber = await GetSubscriber(nodePath);\n                if (subscriber != null)\n                    subscribers.Add(subscriber);\n            }\n            return subscribers.ToArray();\n        }"
            },
            {
                "name": "GetZooKeeper",
                "code": "private async ValueTask<(ManualResetEvent, ZooKeeper)> GetZooKeeper()\n        {\n            var zooKeeper = await _zookeeperClientProvider.GetZooKeeper();\n            return zooKeeper;\n        }"
            }
        ],
        "third_party": [
            "_logger.IsEnabled",
            "zooKeeper.Item2.getChildrenAsync",
            "_logger.LogWarning",
            "result.Children.ToArray",
            "zooKeeper.Item2.existsAsync",
            "zooKeeper.Item1.WaitOne"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "public void NodeChange(byte[] oldData, byte[] newData)",
        "start_line": "299",
        "end_line": "318",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZookeeperServiceCommandManager.cs",
        "docstring": "The NodeChange function handles changes in node data.\\nIt compares old and new data, and if they differ, retrieves the new service command.\\nThe function updates the service commands list by replacing the old command with the new one for the same service ID.\\nFinally, it triggers the OnChanged event with the new and old commands.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "60ff973ba60a",
        "ground_truth": "public void NodeChange(byte[] oldData, byte[] newData)\n{\n    if (DataEquals(oldData, newData))\n        return;\n    var newCommand = GetServiceCommand(newData);\n    //\u5f97\u5230\u65e7\u7684\u670d\u52a1\u547d\u4ee4\u3002\n    var oldCommand = _serviceCommands.FirstOrDefault(i => i.ServiceId == newCommand.ServiceId);\n    lock (_serviceCommands)\n    {\n        //\u5220\u9664\u65e7\u670d\u52a1\u547d\u4ee4\uff0c\u5e76\u6dfb\u52a0\u4e0a\u65b0\u7684\u670d\u52a1\u547d\u4ee4\u3002\n        _serviceCommands =\n            _serviceCommands\n                .Where(i => i.ServiceId != newCommand.ServiceId)\n                .Concat(new[] { newCommand }).ToArray();\n    }\n    //\u89e6\u53d1\u670d\u52a1\u547d\u4ee4\u53d8\u66f4\u4e8b\u4ef6\u3002\n    OnChanged(new ServiceCommandChangedEventArgs(newCommand, oldCommand));\n}",
        "import_statements": [
            "using Microsoft.Extensions.Logging",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Routing",
            "using Surging.Core.CPlatform.Routing.Implementation",
            "using Surging.Core.CPlatform.Runtime.Server",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.CPlatform.Support",
            "using Surging.Core.CPlatform.Support.Implementation",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using System",
            "using System.Collections.Generic",
            "using System.Linq",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks"
        ],
        "reference_api": [
            "irstOrDefault(i => i.ServiceId ",
            "GetServiceCommand",
            "erviceId != newCommand.ServiceId)\n                        .Concat(new[] { newCommand }).ToArray();\n            }\n            //\u89e6\u53d1\u670d\u52a1\u547d\u4ee4\u53d8\u66f4\u4e8b\u4ef6\u3002\n            ",
            " }",
            "DataEquals",
            "erviceId != newCommand.ServiceId)\n             ",
            "erviceId != newCommand.ServiceId)\n                        .Concat(new[] { newCommand }).ToArray();\n            }\n        "
        ],
        "repo_defined_api_with_code": [
            {
                "name": "GetServiceCommand",
                "code": "private ServiceCommandDescriptor GetServiceCommand(byte[] data)\n        {\n            if (_logger.IsEnabled(LogLevel.Debug))\n                _logger.LogDebug($\"\u51c6\u5907\u8f6c\u6362\u670d\u52a1\u547d\u4ee4\uff0c\u914d\u7f6e\u5185\u5bb9\uff1a{Encoding.UTF8.GetString(data)}\u3002\");\n\n            if (data == null)\n                return null;\n\n            var descriptor = _serializer.Deserialize<byte[], ServiceCommandDescriptor>(data);\n            return descriptor;\n        }"
            },
            {
                "name": "DataEquals",
                "code": "private static bool DataEquals(IReadOnlyList<byte> data1, IReadOnlyList<byte> data2)\n        {\n            if (data1.Count != data2.Count)\n                return false;\n            for (var i = 0; i < data1.Count; i++)\n            {\n                var b1 = data1[i];\n                var b2 = data2[i];\n                if (b1 != b2)\n                    return false;\n            }\n            return true;\n        }"
            }
        ],
        "third_party": [
            "irstOrDefault(i => i.ServiceId ",
            "erviceId != newCommand.ServiceId)\n                        .Concat(new[] { newCommand }).ToArray();\n            }\n            //\u89e6\u53d1\u670d\u52a1\u547d\u4ee4\u53d8\u66f4\u4e8b\u4ef6\u3002\n            ",
            " }",
            "erviceId != newCommand.ServiceId)\n             ",
            "erviceId != newCommand.ServiceId)\n                        .Concat(new[] { newCommand }).ToArray();\n            }\n        "
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "protected override async Task ProcessImpl(WatchedEvent watchedEvent)",
        "start_line": "31",
        "end_line": "45",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/WatcherProvider/NodeMonitorWatcher.cs",
        "docstring": "The ProcessImpl function handles watched events in a ZooKeeper node.\\nWhen a NodeDataChanged event occurs, it retrieves the updated data from the node asynchronously using a ZooKeeper client.\\nIt then executes a specified action with the current and new data, and updates the watcher with the new data.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4597b75f2b3d",
        "ground_truth": "protected override async Task ProcessImpl(WatchedEvent watchedEvent)\n{\n    var path = Path;\n    switch (watchedEvent.get_Type())\n    {\n        case Event.EventType.NodeDataChanged:\n            var zooKeeper = await _zooKeeperCall();\n            var watcher = new NodeMonitorWatcher(_zooKeeperCall, path, _action);\n            var data = await zooKeeper.Item2.getDataAsync(path, watcher);\n            var newData = data.Data;\n            _action(_currentData, newData);\n            watcher.SetCurrentData(newData);\n            break;\n    }\n}",
        "import_statements": [
            "using org.apache.zookeeper",
            "using System",
            "using System.Collections.Generic",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks"
        ],
        "reference_api": [
            "watcher.SetCurrentData",
            "_zooKeeperCall",
            "watchedEvent.get_Type",
            "_action",
            "zooKeeper.Item2.getDataAsync"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "watcher.SetCurrentData",
            "_zooKeeperCall",
            "watchedEvent.get_Type",
            "_action",
            "zooKeeper.Item2.getDataAsync"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo)",
        "start_line": "146",
        "end_line": "155",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ContainerBuilderExtensions.cs",
        "docstring": "The UseZookeeperClientProvider function extends IServiceBuilder to use a Zookeeper client provider.\\nIt registers a DefaultZookeeperClientProvider with the service collection, resolving dependencies like health check service, address selector, and logger.\\nThe function configures the provider as a single instance and returns the modified builder.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "cb4f0e22a0d5",
        "ground_truth": "public static IServiceBuilder UseZookeeperClientProvider(this IServiceBuilder builder, ConfigInfo configInfo)\n{\n    builder.Services.Register(provider =>\nnew DefaultZookeeperClientProvider(\n   GetConfigInfo(configInfo),\nprovider.Resolve<IHealthCheckService>(),\n  provider.Resolve<IZookeeperAddressSelector>(),\n  provider.Resolve<ILogger<DefaultZookeeperClientProvider>>())).As<IZookeeperClientProvider>().SingleInstance();\n    return builder;\n}",
        "import_statements": [
            "using Autofac",
            "using Microsoft.Extensions.Configuration",
            "using Microsoft.Extensions.DependencyInjection",
            "using Microsoft.Extensions.Logging",
            "using Surging.Core.CPlatform",
            "using Surging.Core.CPlatform.Cache",
            "using Surging.Core.CPlatform.Mqtt",
            "using Surging.Core.CPlatform.Routing",
            "using Surging.Core.CPlatform.Runtime.Client",
            "using Surging.Core.CPlatform.Runtime.Server",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.Internal.Cluster.HealthChecks",
            "using Surging.Core.Zookeeper.Internal.Cluster.HealthChecks.Implementation",
            "using Surging.Core.Zookeeper.Internal.Cluster.Implementation.Selectors",
            "using Surging.Core.Zookeeper.Internal.Cluster.Implementation.Selectors.Implementation",
            "using Surging.Core.Zookeeper.Internal.Implementation",
            "using System"
        ],
        "reference_api": [
            "provider.Resolve<ILogger<DefaultZookeeperClientProvider>>",
            "GetConfigInfo",
            "provider.Resolve<IHealthCheckService>",
            "builder.Services.Register",
            "provider.Resolve<IZookeeperAddressSelector>",
            "builder.Services.Register(provider =>\n       new DefaultZookeeperClientProvider(\n           GetConfigInfo(configInfo),\n        provider.Resolve<IHealthCheckService>(),\n          provider.Resolve<IZookeeperAddressSelector>(),\n          provider.Resolve<ILogger<DefaultZookeeperClientProvider>>())).As<IZookeeperClientProvider>",
            "builder.Services.Register(provider =>\n       new DefaultZookeeperClientProvider(\n           GetConfigInfo(configInfo),\n        provider.Resolve<IHealthCheckService>(),\n          provider.Resolve<IZookeeperAddressSelector>(),\n          provider.Resolve<ILogger<DefaultZookeeperClientProvider>>())).As<IZookeeperClientProvider>().SingleInstance"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "GetConfigInfo",
                "code": "private static ConfigInfo GetConfigInfo(ConfigInfo config)\n        {\n            ZookeeperOption option = null;\n            var section = CPlatform.AppConfig.GetSection(\"Zookeeper\");\n            if (section.Exists())\n                option = section.Get<ZookeeperOption>();\n            else if (AppConfig.Configuration != null)\n                option = AppConfig.Configuration.Get<ZookeeperOption>();\n            if (option != null)\n            {\n                var sessionTimeout = config.SessionTimeout.TotalSeconds;\n                Double.TryParse(option.SessionTimeout, out sessionTimeout);\n                config = new ConfigInfo(\n                    option.ConnectionString,\n                    TimeSpan.FromSeconds(sessionTimeout),\n                    option.RoutePath ?? config.RoutePath,\n                    option.SubscriberPath ?? config.SubscriberPath,\n                    option.CommandPath ?? config.CommandPath,\n                    option.CachePath ?? config.CachePath,\n                    option.MqttRoutePath ?? config.MqttRoutePath,\n                    option.ChRoot ?? config.ChRoot,\n                    option.ReloadOnChange != null ? bool.Parse(option.ReloadOnChange) :\n                    config.ReloadOnChange,\n                   option.EnableChildrenMonitor != null ? bool.Parse(option.EnableChildrenMonitor) :\n                    config.EnableChildrenMonitor\n                   );\n            }\n            return config;\n        }"
            }
        ],
        "third_party": [
            "provider.Resolve<ILogger<DefaultZookeeperClientProvider>>",
            "provider.Resolve<IHealthCheckService>",
            "builder.Services.Register",
            "provider.Resolve<IZookeeperAddressSelector>",
            "builder.Services.Register(provider =>\n       new DefaultZookeeperClientProvider(\n           GetConfigInfo(configInfo),\n        provider.Resolve<IHealthCheckService>(),\n          provider.Resolve<IZookeeperAddressSelector>(),\n          provider.Resolve<ILogger<DefaultZookeeperClientProvider>>())).As<IZookeeperClientProvider>",
            "builder.Services.Register(provider =>\n       new DefaultZookeeperClientProvider(\n           GetConfigInfo(configInfo),\n        provider.Resolve<IHealthCheckService>(),\n          provider.Resolve<IZookeeperAddressSelector>(),\n          provider.Resolve<ILogger<DefaultZookeeperClientProvider>>())).As<IZookeeperClientProvider>().SingleInstance"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)",
        "start_line": "102",
        "end_line": "139",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZooKeeperMqttServiceRouteManager.cs",
        "docstring": "The SetRoutesAsync function sets up MQTT service routes in ZooKeeper.\\nIt logs the process of adding routes and retrieves ZooKeeper clients.\\nFor each client, it ensures the MQTT route path subdirectory exists, iterates over the given routes, and creates or updates nodes with serialized route data.\\nIt logs actions taken, such as node creation or data updates, and confirms successful addition of routes.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "80709a4b2b69",
        "ground_truth": "protected override async Task SetRoutesAsync(IEnumerable<MqttServiceDescriptor> routes)\n{\n    if (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0mqtt\u670d\u52a1\u8def\u7531\u3002\");\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers)\n    {\n        await CreateSubdirectory(zooKeeper, _configInfo.MqttRoutePath);\n        var path = _configInfo.MqttRoutePath;\n        routes = routes.ToArray();\n        foreach (var serviceRoute in routes)\n        {\n            var nodePath = $\"{path}{serviceRoute.MqttDescriptor.Topic}\";\n            var nodeData = _serializer.Serialize(serviceRoute);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\");\n                var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                if (!DataEquals(nodeData, onlineData))\n                    await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n            }\n        }\n        if (_logger.IsEnabled(LogLevel.Information))\n            _logger.LogInformation(\"mqtt\u670d\u52a1\u8def\u7531\u6dfb\u52a0\u6210\u529f\u3002\");\n    }\n}",
        "import_statements": [
            "using Microsoft.Extensions.Logging",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Address",
            "using Surging.Core.CPlatform.Mqtt",
            "using Surging.Core.CPlatform.Mqtt.Implementation",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.CPlatform.Transport.Implementation",
            "using Surging.Core.CPlatform.Utilities",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using System",
            "using System.Collections.Generic",
            "using System.Linq",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks"
        ],
        "reference_api": [
            "istsAsync(nodePath) == null",
            "\n\n              ",
            "_logger.LogInformation",
            "_logger.IsEnabled",
            "\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58",
            "Data, ZooDefs.Ids.OPEN_ACL_",
            "        }\n                }\n",
            "ize(serviceRoute);\n  ",
            " }",
            "ovider.GetZooKeepers();\n            fo",
            "logger.LogInforma",
            "LogLevel.Debug))\n",
            "await zooK",
            "\n             ",
            "                 ",
            "        if (!DataEquals(node",
            "(zooKeeper, _confi"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "istsAsync(nodePath) == null",
            "\n\n              ",
            "_logger.LogInformation",
            "_logger.IsEnabled",
            "\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58",
            "Data, ZooDefs.Ids.OPEN_ACL_",
            "        }\n                }\n",
            "ize(serviceRoute);\n  ",
            " }",
            "ovider.GetZooKeepers();\n            fo",
            "logger.LogInforma",
            "LogLevel.Debug))\n",
            "await zooK",
            "\n             ",
            "                 ",
            "        if (!DataEquals(node",
            "(zooKeeper, _confi"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr)",
        "start_line": "189",
        "end_line": "212",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZooKeeperMqttServiceRouteManager.cs",
        "docstring": "The RemoveExceptRoutesAsync function removes MQTT service routes from Zookeeper except those provided in the routes parameter.\\nIt constructs the route path and retrieves the current Zookeeper clients.\\nFor each Zookeeper client, it compares the existing routes with the new routes and identifies routes to be deleted.\\nIf the addresses of the routes to be deleted contain the specified hostAddr, it deletes the corresponding node from Zookeeper.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "69ff4d3427cd",
        "ground_truth": "private async Task RemoveExceptRoutesAsync(IEnumerable<MqttServiceRoute> routes, AddressModel hostAddr)\n{\n    var path = _configInfo.MqttRoutePath;\n    routes = routes.ToArray();\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers)\n    {\n        if (_routes != null)\n        {\n            var oldRouteTopics = _routes.Select(i => i.MqttDescriptor.Topic).ToArray();\n            var newRouteTopics = routes.Select(i => i.MqttDescriptor.Topic).ToArray();\n            var deletedRouteTopics = oldRouteTopics.Except(newRouteTopics).ToArray();\n            foreach (var deletedRouteTopic in deletedRouteTopics)\n            {\n                var addresses = _routes.Where(p => p.MqttDescriptor.Topic == deletedRouteTopic).Select(p => p.MqttEndpoint).FirstOrDefault();\n                if (addresses.Contains(hostAddr))\n                {\n                    var nodePath = $\"{path}{deletedRouteTopic}\";\n                    await zooKeeper.Item2.deleteAsync(nodePath);\n                }\n            }\n        }\n    }\n}",
        "import_statements": [
            "using Microsoft.Extensions.Logging",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Address",
            "using Surging.Core.CPlatform.Mqtt",
            "using Surging.Core.CPlatform.Mqtt.Implementation",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.CPlatform.Transport.Implementation",
            "using Surging.Core.CPlatform.Utilities",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using System",
            "using System.Collections.Generic",
            "using System.Linq",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks"
        ],
        "reference_api": [
            "_routes.Select",
            "_routes.Where",
            "_zookeeperClientProvider.GetZooKeepers",
            "oldRouteTopics.Except(newRouteTopics).ToArray",
            "_routes.Select(i => i.MqttDescriptor.Topic).ToArray",
            "routes.Select(i => i.MqttDescriptor.Topic).ToArray",
            "routes.Select",
            "zooKeeper.Item2.deleteAsync",
            "addresses.Contains",
            "_routes.Where(p => p.MqttDescriptor.Topic == deletedRouteTopic).Select(p => p.MqttEndpoint).FirstOrDefault",
            "oldRouteTopics.Except",
            "routes.ToArray",
            "_routes.Where(p => p.MqttDescriptor.Topic == deletedRouteTopic).Select"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "_routes.Select",
            "_routes.Where",
            "_zookeeperClientProvider.GetZooKeepers",
            "oldRouteTopics.Except(newRouteTopics).ToArray",
            "_routes.Select(i => i.MqttDescriptor.Topic).ToArray",
            "routes.Select(i => i.MqttDescriptor.Topic).ToArray",
            "routes.Select",
            "zooKeeper.Item2.deleteAsync",
            "addresses.Contains",
            "_routes.Where(p => p.MqttDescriptor.Topic == deletedRouteTopic).Select(p => p.MqttEndpoint).FirstOrDefault",
            "oldRouteTopics.Except",
            "routes.ToArray",
            "_routes.Where(p => p.MqttDescriptor.Topic == deletedRouteTopic).Select"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors)",
        "start_line": "115",
        "end_line": "153",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZookeeperServiceCacheManager.cs",
        "docstring": "The SetCachesAsync function updates service cache descriptors in Zookeeper.\\nIt logs the operation, constructs the cache path, and retrieves Zookeeper clients.\\nFor each Zookeeper client, it ensures the cache path exists and iterates through the cache descriptors.\\nIt serializes each descriptor and checks if the corresponding node exists in Zookeeper.\\nIf the node does not exist, it creates it; otherwise, it updates the node's data if necessary.\\nFinally, it logs the successful addition of the service caches.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8856055e4872",
        "ground_truth": "public override async Task SetCachesAsync(IEnumerable<ServiceCacheDescriptor> cacheDescriptors)\n{\n    if (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation(\"\u51c6\u5907\u6dfb\u52a0\u670d\u52a1\u547d\u4ee4\u3002\");\n    var path = _configInfo.CachePath;\n    var zooKeepers = await _zookeeperClientProvider.GetZooKeepers();\n    foreach (var zooKeeper in zooKeepers)\n    {\n        await CreateSubdirectory(zooKeeper, _configInfo.CachePath);\n        if (!path.EndsWith(\"/\"))\n            path += \"/\";\n        cacheDescriptors = cacheDescriptors.ToArray();\n        foreach (var cacheDescriptor in cacheDescriptors)\n        {\n            var nodePath = $\"{path}{cacheDescriptor.CacheDescriptor.Id}\";\n            var nodeData = _serializer.Serialize(cacheDescriptor);\n            if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58\u5728\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n                await zooKeeper.Item2.createAsync(nodePath, nodeData, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n            }\n            else\n            {\n                if (_logger.IsEnabled(LogLevel.Debug))\n                    _logger.LogDebug($\"\u5c06\u66f4\u65b0\u8282\u70b9\uff1a{nodePath}\u7684\u6570\u636e\u3002\");\n                var onlineData = (await zooKeeper.Item2.getDataAsync(nodePath)).Data;\n                if (!DataEquals(nodeData, onlineData))\n                    await zooKeeper.Item2.setDataAsync(nodePath, nodeData);\n            }\n        }\n        if (_logger.IsEnabled(LogLevel.Information))\n            _logger.LogInformation(\"\u670d\u52a1\u7f13\u5b58\u6dfb\u52a0\u6210\u529f\u3002\");\n    }\n}",
        "import_statements": [
            "using Microsoft.Extensions.Logging",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Cache",
            "using Surging.Core.CPlatform.Cache.Implementation",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using System",
            "using System.Collections.Generic",
            "using System.Linq",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks"
        ],
        "reference_api": [
            "istsAsync(nodePath) == null",
            "\n\n              ",
            "_logger.LogInformation",
            "_logger.IsEnabled",
            "\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58",
            "Data, ZooDefs.Ids.OPEN_ACL_",
            "        }\n                }\n",
            "",
            "ovider.GetZooKeepers();\n            fo",
            "logger.LogInforma",
            "oArray();\n\n             ",
            "ize(cacheDescriptor);",
            "LogLevel.Debug))\n",
            "                 ",
            "        if (!DataEquals(node",
            "await zooK",
            ")\n           ",
            "(zooKeeper, _confi"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "istsAsync(nodePath) == null",
            "\n\n              ",
            "_logger.LogInformation",
            "_logger.IsEnabled",
            "\"\u8282\u70b9\uff1a{nodePath}\u4e0d\u5b58",
            "Data, ZooDefs.Ids.OPEN_ACL_",
            "        }\n                }\n",
            "",
            "ovider.GetZooKeepers();\n            fo",
            "logger.LogInforma",
            "oArray();\n\n             ",
            "ize(cacheDescriptor);",
            "LogLevel.Debug))\n",
            "                 ",
            "        if (!DataEquals(node",
            "await zooK",
            ")\n           ",
            "(zooKeeper, _confi"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path)",
        "start_line": "177",
        "end_line": "198",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/ZookeeperServiceCacheManager.cs",
        "docstring": "The CreateSubdirectory function ensures the creation of a specified Zookeeper node path if it does not exist.\\nIt waits for a manual reset event before proceeding, then checks if the path already exists.\\nIf not, it logs the creation process and iteratively creates each subdirectory in the path if it does not exist.\\nThe function uses the provided Zookeeper client to perform the operations.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "57d92c17de83",
        "ground_truth": "private async Task CreateSubdirectory((ManualResetEvent, ZooKeeper) zooKeeper, string path)\n{\n    zooKeeper.Item1.WaitOne();\n    if (await zooKeeper.Item2.existsAsync(path) != null)\n        return;\n    if (_logger.IsEnabled(LogLevel.Information))\n        _logger.LogInformation($\"\u8282\u70b9{path}\u4e0d\u5b58\u5728\uff0c\u5c06\u8fdb\u884c\u521b\u5efa\u3002\");\n    var childrens = path.Split(new[] { '/' }, StringSplitOptions.RemoveEmptyEntries);\n    var nodePath = \"/\";\n    foreach (var children in childrens)\n    {\n        nodePath += children;\n        if (await zooKeeper.Item2.existsAsync(nodePath) == null)\n        {\n            await zooKeeper.Item2.createAsync(nodePath, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n        }\n        nodePath += \"/\";\n    }\n}",
        "import_statements": [
            "using Microsoft.Extensions.Logging",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform.Cache",
            "using Surging.Core.CPlatform.Cache.Implementation",
            "using Surging.Core.CPlatform.Serialization",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using System",
            "using System.Collections.Generic",
            "using System.Linq",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks"
        ],
        "reference_api": [
            "ync(nodePath, null, ZooDefs",
            "_logger.LogInformation",
            "_logger.IsEnabled",
            "zooKeeper.Item2.existsAsync",
            "zooKeeper.Item1.WaitOne",
            "ync(nodePath) == null)\n    ",
            ", StringSp"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ync(nodePath, null, ZooDefs",
            "_logger.LogInformation",
            "_logger.IsEnabled",
            "zooKeeper.Item2.existsAsync",
            "zooKeeper.Item1.WaitOne",
            "ync(nodePath) == null)\n    ",
            ", StringSp"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "fanliang11/surging",
        "function_declaration": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers()",
        "start_line": "116",
        "end_line": "129",
        "file_path": "src/Surging.Core/Surging.Core.Zookeeper/Internal/Implementation/DefaultZookeeperClientProvider.cs",
        "docstring": "The GetZooKeepers function asynchronously retrieves a list of healthy ZooKeeper instances along with their associated ManualResetEvent.\\nIt iterates through configured addresses and checks their health status using a health check service.\\nFor each healthy address, it creates a ZooKeeper instance and adds it to the result list, which is then returned.",
        "language": "CSharp",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b276a07da453",
        "ground_truth": "public async ValueTask<IEnumerable<(ManualResetEvent, ZooKeeper)>> GetZooKeepers()\n{\n    var result = new List<(ManualResetEvent, ZooKeeper)>();\n    foreach (var address in _config.Addresses)\n    {\n        var ipAddress = address as IpAddressModel;\n        if (await _healthCheckService.IsHealth(address))\n        {\n            result.Add(CreateZooKeeper(ipAddress));\n        }\n    }\n    return result;\n}",
        "import_statements": [
            "using System",
            "using System.Collections.Concurrent",
            "using System.Collections.Generic",
            "using System.Text",
            "using System.Threading",
            "using System.Threading.Tasks",
            "using Microsoft.Extensions.Logging",
            "using org.apache.zookeeper",
            "using Surging.Core.CPlatform",
            "using Surging.Core.CPlatform.Address",
            "using Surging.Core.CPlatform.Exceptions",
            "using Surging.Core.CPlatform.Runtime.Client.Address.Resolvers.Implementation.Selectors",
            "using Surging.Core.Zookeeper.Configurations",
            "using Surging.Core.Zookeeper.Internal.Cluster.HealthChecks",
            "using Surging.Core.Zookeeper.Internal.Cluster.Implementation.Selectors",
            "using Surging.Core.Zookeeper.WatcherProvider",
            "using Level = Microsoft.Extensions.Logging.LogLevel"
        ],
        "reference_api": [
            "CreateZooKeeper",
            "_healthCheckService.IsHealth",
            "result.Add"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "CreateZooKeeper",
                "code": "protected (ManualResetEvent, ZooKeeper) CreateZooKeeper(IpAddressModel ipAddress)\n        {\n            if (!_zookeeperClients.TryGetValue(ipAddress, out (ManualResetEvent, ZooKeeper) result))\n            {\n                var connectionWait = new ManualResetEvent(false);\n                result = new ValueTuple<ManualResetEvent, ZooKeeper>(connectionWait, new ZooKeeper($\"{ipAddress.Ip}:{ipAddress.Port}\", (int)_config.SessionTimeout.TotalMilliseconds\n                 , new ReconnectionWatcher(\n                      () =>\n                      {\n                          connectionWait.Set();\n                      },\n                      () =>\n                      {\n                          connectionWait.Close();\n                      },\n                      async () =>\n                      {\n                          connectionWait.Reset();\n                          if (_zookeeperClients.TryRemove(ipAddress, out (ManualResetEvent, ZooKeeper) value))\n                          { \n                              await value.Item2.closeAsync();\n                              value.Item1.Close();\n                          }\n                          CreateZooKeeper(ipAddress);\n                      })));\n                _zookeeperClients.AddOrUpdate(ipAddress, result,(k,v)=> result);\n            }\n            return result;\n        }"
            }
        ],
        "third_party": [
            "_healthCheckService.IsHealth",
            "result.Add"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "ZooKeeper getZooKeeper() throws Exception",
        "start_line": "90",
        "end_line": "102",
        "file_path": "curator-client/src/main/java/org/apache/curator/HandleHolder.java",
        "docstring": "The getZooKeeper function retrieves a ZooKeeper instance, ensuring thread safety with synchronization.\\nIf the ZooKeeper handle is null, it initializes a new ZooKeeper connection using the connection string from the ensemble provider and other configuration parameters.\\nA Helper instance is created with the current data, and the ZooKeeper instance is returned.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "dbb6463e7ee8",
        "ground_truth": "ZooKeeper getZooKeeper() throws Exception {\n    synchronized (this) {\n        if (data.zooKeeperHandle == null) {\n            resetConnectionString(ensembleProvider.getConnectionString());\n            data.zooKeeperHandle = zookeeperFactory.newZooKeeper(\n                    data.connectionString, sessionTimeout, watcher, canBeReadOnly);\n        }\n        helper = new Helper(data);\n        return super.getZooKeeper();\n    }\n}",
        "import_statements": [
            "import org.apache.curator.ensemble.EnsembleProvider;",
            "import org.apache.curator.utils.ZookeeperFactory;",
            "import org.apache.zookeeper.WatchedEvent;",
            "import org.apache.zookeeper.Watcher;",
            "import org.apache.zookeeper.ZooKeeper;"
        ],
        "reference_api": [
            "getConnectionString",
            "resetConnectionString",
            "newZooKeeper",
            "getZooKeeper"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getConnectionString",
                "code": "String getConnectionString() {\n        return (helper != null) ? helper.getConnectionString() : null;\n    }"
            },
            {
                "name": "resetConnectionString",
                "code": "void resetConnectionString(String connectionString) {\n        if (helper != null) {\n            helper.resetConnectionString(connectionString);\n        }\n    }"
            },
            {
                "name": "getZooKeeper",
                "code": "ZooKeeper getZooKeeper() throws Exception {\n        return (helper != null) ? helper.getZooKeeper() : null;\n    }"
            }
        ],
        "third_party": [
            "newZooKeeper"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception",
        "start_line": "106",
        "end_line": "126",
        "file_path": "curator-client/src/main/java/org/apache/curator/HandleHolder.java",
        "docstring": "The internalClose function safely closes a ZooKeeper instance.\\nIt retrieves the ZooKeeper instance from the helper if available and registers a dummy watcher to clear the default watcher, preventing new events from being processed.\\nDepending on the provided timeout, it either closes the ZooKeeper immediately or waits for the specified timeout before closing.\\nIf an InterruptedException occurs, the thread's interrupt status is restored.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0d9f311aa482",
        "ground_truth": "private void internalClose(int waitForShutdownTimeoutMs) throws Exception {\n    try {\n        ZooKeeper zooKeeper = (helper != null) ? helper.getZooKeeper() : null;\n        if (zooKeeper != null) {\n            Watcher dummyWatcher = new Watcher() {\n                @Override\n                public void process(WatchedEvent event) {}\n            };\n            zooKeeper.register(\n                    dummyWatcher); // clear the default watcher so that no new events get processed by mistake\n            if (waitForShutdownTimeoutMs == 0) {\n                zooKeeper.close(); // coming from closeAndReset() which is executed in ZK's event thread. Cannot use\n                // zooKeeper.close(n) otherwise we'd get a dead lock\n            } else {\n                zooKeeper.close(waitForShutdownTimeoutMs);\n            }\n        }\n    } catch (InterruptedException dummy) {\n        Thread.currentThread().interrupt();\n    }\n}",
        "import_statements": [
            "import org.apache.curator.ensemble.EnsembleProvider;",
            "import org.apache.curator.utils.ZookeeperFactory;",
            "import org.apache.zookeeper.WatchedEvent;",
            "import org.apache.zookeeper.Watcher;",
            "import org.apache.zookeeper.ZooKeeper;"
        ],
        "reference_api": [],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException",
        "start_line": "221",
        "end_line": "235",
        "file_path": "curator-client/src/main/java/org/apache/curator/CuratorZookeeperClient.java",
        "docstring": "The blockUntilConnectedOrTimedOut function checks if the client is started and blocks execution until the client is connected or a timeout occurs.\\nIt logs the start and end of the operation, traces the connection process, and returns the connection status.\\nIf the client is connected, it returns true; otherwise, it returns false.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d57ef4592303",
        "ground_truth": "public boolean blockUntilConnectedOrTimedOut() throws InterruptedException {\n    Preconditions.checkState(started.get(), \"Client is not started\");\n    log.debug(\"blockUntilConnectedOrTimedOut() start\");\n    OperationTrace trace = startAdvancedTracer(\"blockUntilConnectedOrTimedOut\");\n    internalBlockUntilConnectedOrTimedOut();\n    trace.commit();\n    boolean localIsConnected = state.isConnected();\n    log.debug(\"blockUntilConnectedOrTimedOut() end. isConnected: \" + localIsConnected);\n    return localIsConnected;\n}",
        "import_statements": [
            "import com.google.common.base.Preconditions;",
            "import java.io.Closeable;",
            "import java.io.IOException;",
            "import java.util.concurrent.CountDownLatch;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.concurrent.atomic.AtomicBoolean;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "import org.apache.curator.drivers.OperationTrace;",
            "import org.apache.curator.drivers.TracerDriver;",
            "import org.apache.curator.ensemble.EnsembleProvider;",
            "import org.apache.curator.ensemble.fixed.FixedEnsembleProvider;",
            "import org.apache.curator.utils.DefaultTracerDriver;",
            "import org.apache.curator.utils.DefaultZookeeperFactory;",
            "import org.apache.curator.utils.ThreadUtils;",
            "import org.apache.curator.utils.ZookeeperFactory;",
            "import org.apache.zookeeper.WatchedEvent;",
            "import org.apache.zookeeper.Watcher;",
            "import org.apache.zookeeper.ZooKeeper;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;"
        ],
        "reference_api": [
            "commit",
            "checkState",
            "debug",
            "get",
            "startAdvancedTracer",
            "internalBlockUntilConnectedOrTimedOut",
            "isConnected"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "startAdvancedTracer",
                "code": "public OperationTrace startAdvancedTracer(String name) {\n        return new OperationTrace(name, tracer.get(), state.getSessionId());\n    }"
            },
            {
                "name": "internalBlockUntilConnectedOrTimedOut",
                "code": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n        long waitTimeMs = connectionTimeoutMs;\n        while (!state.isConnected() && (waitTimeMs > 0)) {\n            final CountDownLatch latch = new CountDownLatch(1);\n            Watcher tempWatcher = new Watcher() {\n                @Override\n                public void process(WatchedEvent event) {\n                    latch.countDown();\n                }\n            };\n\n            state.addParentWatcher(tempWatcher);\n            long startTimeMs = System.currentTimeMillis();\n            long timeoutMs = Math.min(waitTimeMs, 1000);\n            try {\n                latch.await(timeoutMs, TimeUnit.MILLISECONDS);\n            } finally {\n                state.removeParentWatcher(tempWatcher);\n            }\n            long elapsed = Math.max(1, System.currentTimeMillis() - startTimeMs);\n            waitTimeMs -= elapsed;\n        }\n    }"
            },
            {
                "name": "isConnected",
                "code": "public boolean isConnected() {\n        return state.isConnected();\n    }"
            }
        ],
        "third_party": [
            "commit",
            "checkState",
            "debug",
            "get"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "public void close(int waitForShutdownTimeoutMs)",
        "start_line": "271",
        "end_line": "281",
        "file_path": "curator-client/src/main/java/org/apache/curator/CuratorZookeeperClient.java",
        "docstring": "The close function shuts down the service with a specified timeout.\\nIt logs the shutdown attempt, sets the started state to false, and attempts to close the state within the given timeout.\\nIf an IOException occurs, it checks for thread interruption and logs the error.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d722f6d0beb6",
        "ground_truth": "public void close(int waitForShutdownTimeoutMs) {\n    log.debug(\"Closing, waitForShutdownTimeoutMs {}\", waitForShutdownTimeoutMs);\n    started.set(false);\n    try {\n        state.close(waitForShutdownTimeoutMs);\n    } catch (IOException e) {\n        ThreadUtils.checkInterrupted(e);\n        log.error(\"\", e);\n    }\n}",
        "import_statements": [
            "import com.google.common.base.Preconditions;",
            "import java.io.Closeable;",
            "import java.io.IOException;",
            "import java.util.concurrent.CountDownLatch;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.concurrent.atomic.AtomicBoolean;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "import org.apache.curator.drivers.OperationTrace;",
            "import org.apache.curator.drivers.TracerDriver;",
            "import org.apache.curator.ensemble.EnsembleProvider;",
            "import org.apache.curator.ensemble.fixed.FixedEnsembleProvider;",
            "import org.apache.curator.utils.DefaultTracerDriver;",
            "import org.apache.curator.utils.DefaultZookeeperFactory;",
            "import org.apache.curator.utils.ThreadUtils;",
            "import org.apache.curator.utils.ZookeeperFactory;",
            "import org.apache.zookeeper.WatchedEvent;",
            "import org.apache.zookeeper.Watcher;",
            "import org.apache.zookeeper.ZooKeeper;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;"
        ],
        "reference_api": [
            "error",
            "set",
            "debug",
            "close",
            "checkInterrupted"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "close",
                "code": "@Override\n    public void close() {\n        close(waitForShutdownTimeoutMs);\n    }"
            }
        ],
        "third_party": [
            "error",
            "set",
            "debug",
            "checkInterrupted"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException ",
        "start_line": "399",
        "end_line": "421",
        "file_path": "curator-client/src/main/java/org/apache/curator/CuratorZookeeperClient.java",
        "docstring": "The internalBlockUntilConnectedOrTimedOut function waits until the state is connected or the connection timeout is reached.\\nIt repeatedly checks the connection state within the specified timeout period.\\nA temporary watcher is added to monitor connection events, and a countdown latch is used to wait for either the connection or a timeout.\\nThe watcher is removed after each wait cycle, and the remaining wait time is adjusted accordingly.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8492e37ce46e",
        "ground_truth": "public void internalBlockUntilConnectedOrTimedOut() throws InterruptedException {\n    long waitTimeMs = connectionTimeoutMs;\n    while (!state.isConnected() && (waitTimeMs > 0)) {\n        final CountDownLatch latch = new CountDownLatch(1);\n        Watcher tempWatcher = new Watcher() {\n            @Override\n            public void process(WatchedEvent event) {\n                latch.countDown();\n            }\n        };\n        state.addParentWatcher(tempWatcher);\n        long startTimeMs = System.currentTimeMillis();\n        long timeoutMs = Math.min(waitTimeMs, 1000);\n        try {\n            latch.await(timeoutMs, TimeUnit.MILLISECONDS);\n        } finally {\n            state.removeParentWatcher(tempWatcher);\n        }\n        long elapsed = Math.max(1, System.currentTimeMillis() - startTimeMs);\n        waitTimeMs -= elapsed;\n    }\n}",
        "import_statements": [
            "import com.google.common.base.Preconditions;",
            "import java.io.Closeable;",
            "import java.io.IOException;",
            "import java.util.concurrent.CountDownLatch;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.concurrent.atomic.AtomicBoolean;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "import org.apache.curator.drivers.OperationTrace;",
            "import org.apache.curator.drivers.TracerDriver;",
            "import org.apache.curator.ensemble.EnsembleProvider;",
            "import org.apache.curator.ensemble.fixed.FixedEnsembleProvider;",
            "import org.apache.curator.utils.DefaultTracerDriver;",
            "import org.apache.curator.utils.DefaultZookeeperFactory;",
            "import org.apache.curator.utils.ThreadUtils;",
            "import org.apache.curator.utils.ZookeeperFactory;",
            "import org.apache.zookeeper.WatchedEvent;",
            "import org.apache.zookeeper.Watcher;",
            "import org.apache.zookeeper.ZooKeeper;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;"
        ],
        "reference_api": [
            "countDown"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "countDown"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception",
        "start_line": "85",
        "end_line": "102",
        "file_path": "curator-framework/src/main/java/org/apache/curator/framework/imps/ProtectedMode.java",
        "docstring": "The validateFoundNode function validates a found ZooKeeper node during a protected mode operation with an ephemeral CreateMode.\\nIt checks if the client's session ID has changed since the initial session.\\nIf the session has changed, it logs the change, deletes the old node associated with the previous session, and updates the session ID.\\nFinally, it returns the found node.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4a01f5365731",
        "ground_truth": "String validateFoundNode(CuratorFrameworkImpl client, CreateMode createMode, String foundNode) throws Exception {\n    if (doProtected() && createMode.isEphemeral()) {\n        long clientSessionId = client.getZooKeeper().getSessionId();\n        if (this.sessionId != clientSessionId) {\n            log.info(\n                    \"Session has changed during protected mode with ephemeral. old: {} new: {}\",\n                    this.sessionId,\n                    clientSessionId);\n            if (foundNode != null) {\n                log.info(\"Deleted old session's found node: {}\", foundNode);\n                client.getFailedDeleteManager().executeGuaranteedOperationInBackground(foundNode);\n                foundNode = null;\n            }\n            this.sessionId = clientSessionId;\n        }\n    }\n    return foundNode;\n}",
        "import_statements": [
            "import java.util.UUID;",
            "import org.apache.zookeeper.CreateMode;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;"
        ],
        "reference_api": [
            "executeGuaranteedOperationInBackground",
            "getZooKeeper",
            "getFailedDeleteManager",
            "isEphemeral",
            "info",
            "getSessionId",
            "doProtected"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "doProtected",
                "code": "boolean doProtected() {\n        return (protectedId != null);\n    }"
            }
        ],
        "third_party": [
            "executeGuaranteedOperationInBackground",
            "getZooKeeper",
            "getFailedDeleteManager",
            "isEphemeral",
            "info",
            "getSessionId"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception",
        "start_line": "176",
        "end_line": "186",
        "file_path": "curator-framework/src/main/java/org/apache/curator/framework/imps/SetACLBuilderImpl.java",
        "docstring": "The pathInForeground function sets the ACL for a given ZooKeeper path in a synchronous manner.\\nIt starts an advanced trace for the operation and uses a retry loop to ensure the ACL is set successfully.\\nThe result, including the path and the status, is recorded in the trace, which is then committed.\\nFinally, it returns the status of the operation.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "126d0922a699",
        "ground_truth": "private Stat pathInForeground(final String path, final List<ACL> aclList) throws Exception {\n    OperationTrace trace = client.getZookeeperClient().startAdvancedTracer(\"SetACLBuilderImpl-Foreground\");\n    Stat resultStat = RetryLoop.callWithRetry(client.getZookeeperClient(), new Callable<Stat>() {\n        @Override\n        public Stat call() throws Exception {\n            return client.getZooKeeper().setACL(path, aclList, version);\n        }\n    });\n    trace.setPath(path).setStat(resultStat).commit();\n    return resultStat;\n}",
        "import_statements": [
            "import java.util.List;",
            "import java.util.concurrent.Callable;",
            "import java.util.concurrent.Executor;",
            "import org.apache.curator.RetryLoop;",
            "import org.apache.curator.drivers.OperationTrace;",
            "import org.apache.curator.framework.api.*;",
            "import org.apache.curator.framework.api.BackgroundCallback;",
            "import org.apache.curator.framework.api.CuratorEvent;",
            "import org.apache.curator.framework.api.CuratorEventType;",
            "import org.apache.curator.framework.api.SetACLBuilder;",
            "import org.apache.zookeeper.AsyncCallback;",
            "import org.apache.zookeeper.data.ACL;",
            "import org.apache.zookeeper.data.Stat;"
        ],
        "reference_api": [
            "getZooKeeper",
            "setACL"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getZooKeeper",
            "setACL"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload)",
        "start_line": "64",
        "end_line": "71",
        "file_path": "curator-recipes/src/main/java/org/apache/curator/framework/recipes/nodes/GroupMember.java",
        "docstring": "The GroupMember constructor initializes a group member instance with a given CuratorFramework client, membership path, member ID, and payload.\\nIt checks the validity of the member ID and sets up a CuratorCache for the specified membership path.\\nAdditionally, it creates a PersistentNode with ephemeral mode for the member using the provided payload.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e07b079ff61f",
        "ground_truth": "public GroupMember(CuratorFramework client, String membershipPath, String thisId, byte[] payload) {\n    this.membershipPath = membershipPath;\n    this.thisId = Preconditions.checkNotNull(thisId, \"thisId cannot be null\");\n    cache = CuratorCache.bridgeBuilder(client, membershipPath).build();\n    pen = new PersistentNode(\n            client, CreateMode.EPHEMERAL, false, ZKPaths.makePath(membershipPath, thisId), payload);\n}",
        "import_statements": [
            "import static org.apache.curator.framework.recipes.cache.CuratorCacheAccessor.parentPathFilter;",
            "import com.google.common.base.Preconditions;",
            "import com.google.common.base.Throwables;",
            "import com.google.common.collect.ImmutableMap;",
            "import java.io.Closeable;",
            "import java.util.Iterator;",
            "import java.util.Map;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.CuratorFrameworkFactory;",
            "import org.apache.curator.framework.recipes.cache.ChildData;",
            "import org.apache.curator.framework.recipes.cache.CuratorCache;",
            "import org.apache.curator.framework.recipes.cache.CuratorCacheBridge;",
            "import org.apache.curator.utils.CloseableUtils;",
            "import org.apache.curator.utils.ThreadUtils;",
            "import org.apache.curator.utils.ZKPaths;",
            "import org.apache.zookeeper.CreateMode;"
        ],
        "reference_api": [
            "checkNotNull",
            "makePath",
            "build",
            "bridgeBuilder"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "checkNotNull",
            "makePath",
            "build",
            "bridgeBuilder"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "public static void createThenWatch(CuratorFramework client, String path) ",
        "start_line": "54",
        "end_line": "72",
        "file_path": "curator-examples/src/main/java/async/AsyncExamples.java",
        "docstring": "The createThenWatch function creates a Zookeeper node at the specified path using an asynchronous CuratorFramework instance.\\nIf the node creation is successful, it sets up a watch on the node to handle future events.\\nIf an exception occurs during creation, it prints the stack trace.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d11c76b2b912",
        "ground_truth": "public static void createThenWatch(CuratorFramework client, String path) {\n    AsyncCuratorFramework async =\n            AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n    // this example shows to asynchronously use watchers for both event\n    // triggering and connection problems. If you don't need to be notified\n    // of connection problems, use the simpler approach shown in createThenWatchSimple()\n    // create a node at the given path with the given payload asynchronously\n    // then watch the created node\n    async.create().forPath(path).whenComplete((name, exception) -> {\n        if (exception != null) {\n            // there was a problem creating the node\n            exception.printStackTrace();\n        } else {\n            handleWatchedStage(async.watched().checkExists().forPath(path).event());\n        }\n    });\n}",
        "import_statements": [
            "import java.util.concurrent.CompletionStage;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.x.async.AsyncCuratorFramework;",
            "import org.apache.curator.x.async.AsyncEventException;",
            "import org.apache.curator.x.async.WatchMode;",
            "import org.apache.zookeeper.WatchedEvent;"
        ],
        "reference_api": [
            "watched",
            "checkExists",
            "forPath",
            "event",
            "handleWatchedStage",
            "create",
            "wrap",
            "printStackTrace",
            "whenComplete"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "handleWatchedStage",
                "code": "private static void handleWatchedStage(CompletionStage<WatchedEvent> watchedStage) {\n        // async handling of Watchers is complicated because watchers can trigger multiple times\n        // and CompletionStage don't support this behavior\n\n        // thenAccept() handles normal watcher triggering.\n        watchedStage.thenAccept(event -> {\n            System.out.println(event.getType());\n            System.out.println(event);\n            // etc.\n        });\n\n        // exceptionally is called if there is a connection problem in which case\n        // watchers trigger to signal the connection problem. \"reset()\" must be called\n        // to reset the watched stage\n        watchedStage.exceptionally(exception -> {\n            AsyncEventException asyncEx = (AsyncEventException) exception;\n            asyncEx.printStackTrace(); // handle the error as needed\n            handleWatchedStage(asyncEx.reset());\n            return null;\n        });\n    }"
            },
            {
                "name": "create",
                "code": "public static void create(CuratorFramework client, String path, byte[] payload) {\n        AsyncCuratorFramework async =\n                AsyncCuratorFramework.wrap(client); // normally you'd wrap early in your app and reuse the instance\n\n        // create a node at the given path with the given payload asynchronously\n        async.create().forPath(path, payload).whenComplete((name, exception) -> {\n            if (exception != null) {\n                // there was a problem\n                exception.printStackTrace();\n            } else {\n                System.out.println(\"Created node name is: \" + name);\n            }\n        });\n    }"
            },
            {
                "name": "wrap",
                "code": "public static AsyncCuratorFramework wrap(CuratorFramework client) {\n        // wrap a CuratorFramework instance so that it can be used async.\n        // do this once and re-use the returned AsyncCuratorFramework instance\n        return AsyncCuratorFramework.wrap(client);\n    }"
            }
        ],
        "third_party": [
            "watched",
            "checkExists",
            "forPath",
            "event",
            "printStackTrace",
            "whenComplete"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "private List<OpResult> doOperation() throws Exception",
        "start_line": "173",
        "end_line": "187",
        "file_path": "curator-framework/src/main/java/org/apache/curator/framework/imps/CuratorTransactionImpl.java",
        "docstring": "The doOperation function executes a multi-operation transaction on the ZooKeeper client.\\nIt processes the list of operation results and checks if the first result indicates an error.\\nIf an error is detected, it retrieves the corresponding error code and throws a KeeperException with the appropriate code.\\nFinally, it returns the list of operation results.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4571de52ad5d",
        "ground_truth": "private List<OpResult> doOperation() throws Exception {\n    List<OpResult> opResults = client.getZooKeeper().multi(transaction);\n    if (opResults.size() > 0) {\n        OpResult firstResult = opResults.get(0);\n        if (firstResult.getType() == ZooDefs.OpCode.error) {\n            OpResult.ErrorResult error = (OpResult.ErrorResult) firstResult;\n            KeeperException.Code code = KeeperException.Code.get(error.getErr());\n            if (code == null) {\n                code = KeeperException.Code.UNIMPLEMENTED;\n            }\n            throw KeeperException.create(code);\n        }\n    }\n    return opResults;\n}",
        "import_statements": [
            "import com.google.common.base.Preconditions;",
            "import com.google.common.collect.ImmutableList;",
            "import java.util.Collection;",
            "import java.util.List;",
            "import java.util.concurrent.Callable;",
            "import org.apache.curator.RetryLoop;",
            "import org.apache.curator.framework.api.Pathable;",
            "import org.apache.curator.framework.api.transaction.*;",
            "import org.apache.zookeeper.KeeperException;",
            "import org.apache.zookeeper.Op;",
            "import org.apache.zookeeper.OpResult;",
            "import org.apache.zookeeper.ZooDefs;",
            "import org.apache.zookeeper.data.Stat;"
        ],
        "reference_api": [
            "getErr",
            "getZooKeeper",
            "size",
            "get",
            "create",
            "multi",
            "getType"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "create",
                "code": "@Override\n    public TransactionCreateBuilder<CuratorTransactionBridge> create() {\n        Preconditions.checkState(!isCommitted, \"transaction already committed\");\n\n        CuratorTransactionBridge asBridge = this;\n        return new CreateBuilderImpl(client).asTransactionCreateBuilder(asBridge, transaction);\n    }"
            }
        ],
        "third_party": [
            "getErr",
            "getZooKeeper",
            "size",
            "get",
            "multi",
            "getType"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception",
        "start_line": "167",
        "end_line": "187",
        "file_path": "curator-examples/src/main/java/discovery/DiscoveryExample.java",
        "docstring": "The listInstances function retrieves and prints the service names and their instances from the given ServiceDiscovery instance.\\nIt queries for all service names and their corresponding instances, printing each service name and its instances.\\nIf no instances are registered, it catches the NoNodeException and prints an error message.\\nFinally, it closes the ServiceDiscovery instance quietly.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "99263700ce6d",
        "ground_truth": "private static void listInstances(ServiceDiscovery<InstanceDetails> serviceDiscovery) throws Exception {\n    // This shows how to query all the instances in service discovery\n    try {\n        Collection<String> serviceNames = serviceDiscovery.queryForNames();\n        System.out.println(serviceNames.size() + \" type(s)\");\n        for (String serviceName : serviceNames) {\n            Collection<ServiceInstance<InstanceDetails>> instances =\n                    serviceDiscovery.queryForInstances(serviceName);\n            System.out.println(serviceName);\n            for (ServiceInstance<InstanceDetails> instance : instances) {\n                outputInstance(instance);\n            }\n        }\n    } catch (KeeperException.NoNodeException e) {\n        System.err.println(\"There are no registered instances.\");\n    } finally {\n        CloseableUtils.closeQuietly(serviceDiscovery);\n    }\n}",
        "import_statements": [
            "import com.google.common.base.Predicate;",
            "import com.google.common.collect.Iterables;",
            "import com.google.common.collect.Lists;",
            "import com.google.common.collect.Maps;",
            "import java.io.BufferedReader;",
            "import java.io.InputStreamReader;",
            "import java.util.Arrays;",
            "import java.util.Collection;",
            "import java.util.List;",
            "import java.util.Map;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.CuratorFrameworkFactory;",
            "import org.apache.curator.retry.ExponentialBackoffRetry;",
            "import org.apache.curator.test.TestingServer;",
            "import org.apache.curator.utils.CloseableUtils;",
            "import org.apache.curator.x.discovery.ServiceDiscovery;",
            "import org.apache.curator.x.discovery.ServiceDiscoveryBuilder;",
            "import org.apache.curator.x.discovery.ServiceInstance;",
            "import org.apache.curator.x.discovery.ServiceProvider;",
            "import org.apache.curator.x.discovery.details.JsonInstanceSerializer;",
            "import org.apache.curator.x.discovery.strategies.RandomStrategy;",
            "import org.apache.zookeeper.KeeperException;"
        ],
        "reference_api": [
            "queryForNames",
            "queryForInstances",
            "outputInstance",
            "size",
            "println",
            "closeQuietly"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "outputInstance",
                "code": "private static void outputInstance(ServiceInstance<InstanceDetails> instance) {\n        System.out.println(\"\\t\" + instance.getPayload().getDescription() + \": \" + instance.buildUriSpec());\n    }"
            }
        ],
        "third_party": [
            "queryForNames",
            "queryForInstances",
            "size",
            "println",
            "closeQuietly"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "apache/curator",
        "function_declaration": "private List<String> filterAndSortChildren(List<String> children)",
        "start_line": "165",
        "end_line": "176",
        "file_path": "curator-recipes/src/main/java/org/apache/curator/framework/recipes/barriers/DistributedDoubleBarrier.java",
        "docstring": "The filterAndSortChildren function filters and sorts a list of child node names.\\nIt removes any names matching the READY_NODE constant, converts the filtered results to a list, sorts the list in ascending order, and returns it.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "891a390fa04d",
        "ground_truth": "private List<String> filterAndSortChildren(List<String> children) {\n    Iterable<String> filtered = Iterables.filter(children, new Predicate<String>() {\n        @Override\n        public boolean apply(String name) {\n            return !name.equals(READY_NODE);\n        }\n    });\n    ArrayList<String> filteredList = Lists.newArrayList(filtered);\n    Collections.sort(filteredList);\n    return filteredList;\n}",
        "import_statements": [
            "import com.google.common.annotations.VisibleForTesting;",
            "import com.google.common.base.Preconditions;",
            "import com.google.common.base.Predicate;",
            "import com.google.common.collect.Iterables;",
            "import com.google.common.collect.Lists;",
            "import java.util.ArrayList;",
            "import java.util.Collections;",
            "import java.util.List;",
            "import java.util.UUID;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.concurrent.atomic.AtomicBoolean;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.utils.PathUtils;",
            "import org.apache.curator.utils.ZKPaths;",
            "import org.apache.zookeeper.CreateMode;",
            "import org.apache.zookeeper.KeeperException;",
            "import org.apache.zookeeper.WatchedEvent;",
            "import org.apache.zookeeper.Watcher;",
            "import org.apache.zookeeper.data.Stat;"
        ],
        "reference_api": [
            "equals"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "protected void connectToZK(String newHost) throws InterruptedException, IOException",
        "start_line": "393",
        "end_line": "402",
        "file_path": "specification-impl/src/main/java/cc/cc1234/zookeeper/ZooKeeperMain.java",
        "docstring": "The connectToZK function connects to a new ZooKeeper host.\\nIf there is an existing active ZooKeeper connection, it closes it.\\nIt then initializes a new ZooKeeper instance with the specified host, timeout, and watcher, optionally setting the connection to read-only mode based on a command-line option.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f25af95a13b6",
        "ground_truth": "protected void connectToZK(String newHost) throws InterruptedException, IOException {\n    if (zk != null && zk.getState().isAlive()) {\n        zk.close();\n    }\n    host = newHost;\n    boolean readOnly = cl.getOption(\"readonly\") != null;\n    zk = new ZooKeeper(host,\n            Integer.parseInt(cl.getOption(\"timeout\")),\n            new MyWatcher(), readOnly);\n}",
        "import_statements": [
            "import cc.cc1234.specification.util.StringWriter;",
            "import org.apache.zookeeper.*;",
            "import org.apache.zookeeper.AsyncCallback.DataCallback;",
            "import org.apache.zookeeper.ZooDefs.Ids;",
            "import org.apache.zookeeper.data.ACL;",
            "import org.apache.zookeeper.data.Id;",
            "import org.apache.zookeeper.data.Stat;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.nio.charset.StandardCharsets;",
            "import java.util.*;"
        ],
        "reference_api": [
            "getState",
            "parseInt",
            "close",
            "isAlive",
            "getOption"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getOption",
                "code": "public String getOption(String opt) {\n            return options.get(opt);\n        }"
            }
        ],
        "third_party": [
            "getState",
            "parseInt",
            "close",
            "isAlive"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "private List<ACL> parseACLs(String aclString) throws IOException",
        "start_line": "663",
        "end_line": "681",
        "file_path": "specification-impl/src/main/java/cc/cc1234/zookeeper/ZooKeeperMain.java",
        "docstring": "The parseACLs function parses a string representation of ACLs into a list of ACL objects.\\nIt splits the input string by commas and processes each segment to extract the scheme, id, and permissions.\\nIf a segment does not conform to the expected format, it logs an error message.\\nFor valid segments, it creates an ACL object, sets its id and permissions, and adds it to the list, which is then returned.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "269eaac5cec6",
        "ground_truth": "private List<ACL> parseACLs(String aclString) throws IOException {\n    List<ACL> acl;\n    String acls[] = aclString.split(\",\");\n    acl = new ArrayList<ACL>();\n    for (String a : acls) {\n        int firstColon = a.indexOf(':');\n        int lastColon = a.lastIndexOf(':');\n        if (firstColon == -1 || lastColon == -1 || firstColon == lastColon) {\n            outputStream.write((a + \" does not have the form scheme:id:perm\").getBytes());\n            continue;\n        }\n        ACL newAcl = new ACL();\n        newAcl.setId(new Id(a.substring(0, firstColon), a.substring(\n                firstColon + 1, lastColon)));\n        newAcl.setPerms(getPermFromString(a.substring(lastColon + 1)));\n        acl.add(newAcl);\n    }\n    return acl;\n}",
        "import_statements": [
            "import cc.cc1234.specification.util.StringWriter;",
            "import org.apache.zookeeper.*;",
            "import org.apache.zookeeper.AsyncCallback.DataCallback;",
            "import org.apache.zookeeper.ZooDefs.Ids;",
            "import org.apache.zookeeper.data.ACL;",
            "import org.apache.zookeeper.data.Id;",
            "import org.apache.zookeeper.data.Stat;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.nio.charset.StandardCharsets;",
            "import java.util.*;"
        ],
        "reference_api": [
            "getBytes",
            "write",
            "split",
            "setId",
            "indexOf",
            "lastIndexOf",
            "setPerms",
            "getPermFromString",
            "add",
            "substring"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getPermFromString",
                "code": "private int getPermFromString(String permString) throws IOException {\n        int perm = 0;\n        for (int i = 0; i < permString.length(); i++) {\n            switch (permString.charAt(i)) {\n                case 'r':\n                    perm |= ZooDefs.Perms.READ;\n                    break;\n                case 'w':\n                    perm |= ZooDefs.Perms.WRITE;\n                    break;\n                case 'c':\n                    perm |= ZooDefs.Perms.CREATE;\n                    break;\n                case 'd':\n                    perm |= ZooDefs.Perms.DELETE;\n                    break;\n                case 'a':\n                    perm |= ZooDefs.Perms.ADMIN;\n                    break;\n                default:\n                    outputStream.write((\"Unknown perm type: \" + permString.charAt(i)).getBytes());\n            }\n        }\n        return perm;\n    }"
            }
        ],
        "third_party": [
            "getBytes",
            "write",
            "setId",
            "setPerms",
            "add"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public void delete(String serverId, List<String> pathList) throws Exception",
        "start_line": "63",
        "end_line": "73",
        "file_path": "core/src/main/java/cc/cc1234/core/zookeeper/service/ZookeeperDomainService.java",
        "docstring": "The delete function removes nodes from a Zookeeper instance based on a server ID and a list of paths.\\nIt ensures the path list is not null and verifies the Zookeeper connection for the given server ID.\\nIf the path list contains fewer than 20 items, it deletes each path synchronously.\\nFor larger lists, it performs the deletions asynchronously.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c25db637a176",
        "ground_truth": "public void delete(String serverId, List<String> pathList) throws Exception {\n    Objects.requireNonNull(pathList);\n    assertZookeeperExists(serverId);\n    if (pathList.size() < 20) {\n        for (String path : pathList) {\n            zookeeperMap.get(serverId).delete(path);\n        }\n    } else {\n        zookeeperMap.get(serverId).deleteAsync(pathList);\n    }\n}",
        "import_statements": [
            "import cc.cc1234.core.configuration.entity.ServerConfiguration;",
            "import cc.cc1234.core.zookeeper.entity.FourLetterCommand;",
            "import cc.cc1234.core.zookeeper.entity.Terminal;",
            "import cc.cc1234.core.zookeeper.entity.Zookeeper;",
            "import cc.cc1234.core.zookeeper.factory.ZookeeperFactory;",
            "import cc.cc1234.specification.listener.ServerListener;",
            "import cc.cc1234.specification.listener.ZookeeperNodeListener;",
            "import cc.cc1234.specification.node.NodeMode;",
            "import cc.cc1234.specification.util.StringWriter;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.zookeeper.data.Stat;",
            "import java.util.*;",
            "import java.util.concurrent.ConcurrentHashMap;"
        ],
        "reference_api": [
            "requireNonNull",
            "deleteAsync",
            "size",
            "assertZookeeperExists",
            "get",
            "delete"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "assertZookeeperExists",
                "code": "private void assertZookeeperExists(String serverId) {\n        if (!zookeeperMap.containsKey(serverId)) {\n            throw new IllegalStateException(\"connect zookeeper first \" + serverId);\n        }\n    }"
            },
            {
                "name": "delete",
                "code": "public void delete(String serverId, List<String> pathList) throws Exception {\n        Objects.requireNonNull(pathList);\n        assertZookeeperExists(serverId);\n        if (pathList.size() < 20) {\n            for (String path : pathList) {\n                zookeeperMap.get(serverId).delete(path);\n            }\n        } else {\n            zookeeperMap.get(serverId).deleteAsync(pathList);\n        }\n    }"
            }
        ],
        "third_party": [
            "requireNonNull",
            "deleteAsync",
            "size",
            "get"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception",
        "start_line": "55",
        "end_line": "68",
        "file_path": "core/src/main/java/cc/cc1234/core/zookeeper/factory/ZookeeperFactory.java",
        "docstring": "The createTerminal function establishes a connection to a Zookeeper instance and initializes a terminal interface.\\nIt writes a connecting message to the provided StringWriter, creates a connection using CuratorZookeeperConnectionFactory with the specified parameters, and retrieves the Zookeeper client.\\nUpon successful connection, it writes a success message, creates a ZooKeeperMain instance, and returns a new Terminal instance with the provided id, url, connection, and ZooKeeperMain.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3ecb1ce78f90",
        "ground_truth": "public Terminal createTerminal(String id, String url, StringWriter writer) throws Exception {\n    writer.write(\"connecting to \" + url + \"...\\n\");\n    var factory = new CuratorZookeeperConnectionFactory();\n    var params = ZookeeperParams.builder()\n            .id(id)\n            .url(url)\n            .aclList(List.of())\n            .build();\n    var connection = factory.create(params);\n    var zk = connection.getClient().getZookeeperClient().getZooKeeper();\n    writer.write(\"connect success \\n\");\n    var zkMain = new ZooKeeperMain(zk, writer);\n    return new Terminal(id, url, connection, zkMain);\n}",
        "import_statements": [
            "import cc.cc1234.client.curator.CuratorZookeeperConnectionFactory;",
            "import cc.cc1234.core.configuration.entity.ServerConfiguration;",
            "import cc.cc1234.core.configuration.value.SSHTunnelConfiguration;",
            "import cc.cc1234.core.zookeeper.entity.SSHTunnel;",
            "import cc.cc1234.core.zookeeper.entity.Terminal;",
            "import cc.cc1234.core.zookeeper.entity.Zookeeper;",
            "import cc.cc1234.specification.connection.ZookeeperParams;",
            "import cc.cc1234.specification.listener.ServerListener;",
            "import cc.cc1234.specification.listener.ZookeeperNodeListener;",
            "import cc.cc1234.specification.util.StringWriter;",
            "import cc.cc1234.zookeeper.ZooKeeperMain;",
            "import java.util.List;"
        ],
        "reference_api": [
            "write",
            "of",
            "getClient",
            "aclList",
            "url",
            "getZooKeeper",
            "getZookeeperClient",
            "create",
            "id",
            "build",
            "builder"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "create",
                "code": "public Zookeeper create(ServerConfiguration serverConfig,\n                            List<ZookeeperNodeListener> nodeListeners,\n                            List<ServerListener> serverListeners) {\n        SSHTunnel tunnel = null;\n        if (serverConfig.getSshTunnelEnabled() && serverConfig.getSshTunnel() != null) {\n            final SSHTunnelConfiguration tunnelConfig = serverConfig.getSshTunnel();\n            tunnel = SSHTunnel.builder()\n                    .localhost(tunnelConfig.getLocalhost())\n                    .localPort(tunnelConfig.getLocalPort())\n                    .sshUsername(tunnelConfig.getSshUsername())\n                    .sshPassword(tunnelConfig.getSshPassword())\n                    .sshKeyFilePath(tunnelConfig.getSshKeyFilePath())\n                    .sshHost(tunnelConfig.getSshHost())\n                    .sshPort(tunnelConfig.getSshPort())\n                    .remoteHost(tunnelConfig.getRemoteHost())\n                    .remotePort(tunnelConfig.getRemotePort())\n                    .build();\n        }\n        var factory = new CuratorZookeeperConnectionFactory();\n        var params = ZookeeperParams.builder()\n                .id(serverConfig.getId())\n                .url(serverConfig.getConnectionTo())\n                .aclList(serverConfig.getAclList())\n                .maxRetries(serverConfig.getConnectionConfiguration().getMaxRetries())\n                .connectionTimeout(serverConfig.getConnectionConfiguration().getConnectionTimeout())\n                .retryIntervalTime(serverConfig.getConnectionConfiguration().getRetryIntervalTime())\n                .sessionTimeout(serverConfig.getConnectionConfiguration().getSessionTimeout())\n                .build();\n        return new Zookeeper(serverConfig.getId(),\n                serverConfig.getConnectionTo(),\n                () -> factory.createAsync(params, serverListeners),\n                tunnel,\n                nodeListeners,\n                serverListeners);\n    }"
            }
        ],
        "third_party": [
            "write",
            "of",
            "getClient",
            "aclList",
            "url",
            "getZooKeeper",
            "getZookeeperClient",
            "id",
            "build",
            "builder"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public void blockUntilConnected()",
        "start_line": "81",
        "end_line": "99",
        "file_path": "core/src/main/java/cc/cc1234/core/zookeeper/entity/SSHTunnel.java",
        "docstring": "The blockUntilConnected function attempts to establish an SSH tunnel connection.\\nIt retries the connection up to six times, logging each attempt and waiting one second between attempts.\\nIf an exception occurs, it closes the connection and throws an IllegalStateException.\\nIf the connection is not established after the retries, it closes the connection and throws an IllegalStateException indicating the failure.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a3606e777eb5",
        "ground_truth": "public void blockUntilConnected() {\n    // block until connected\n    try {\n        int times = 1;\n        while (!isConnected() && times < 7) {\n            log.info(\"Try to connect SSH-Tunnel \" + times + \" times, tunnel = \" + this);\n            Thread.sleep(1000);\n            times++;\n        }\n    } catch (Exception e) {\n        this.close();\n        throw new IllegalStateException(e);\n    }\n    if (!isConnected()) {\n        this.close();\n        throw new IllegalStateException(\"connect SSH Tunnel failed\");\n    }\n}",
        "import_statements": [
            "import lombok.Builder;",
            "import lombok.Getter;",
            "import lombok.extern.slf4j.Slf4j;",
            "import net.schmizz.sshj.SSHClient;",
            "import net.schmizz.sshj.connection.channel.direct.Parameters;",
            "import net.schmizz.sshj.transport.verification.PromiscuousVerifier;",
            "import java.io.IOException;",
            "import java.net.InetSocketAddress;",
            "import java.net.ServerSocket;"
        ],
        "reference_api": [
            "sleep",
            "close",
            "info",
            "isConnected"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "close",
                "code": "public void close() {\n        if (proxySocket != null) {\n            try {\n                proxySocket.close();\n            } catch (IOException e) {\n                throw new IllegalStateException(e);\n            }\n        }\n\n        if (sshClient != null) {\n            try {\n                sshClient.close();\n            } catch (IOException e) {\n                throw new IllegalStateException(e);\n            }\n        }\n    }"
            },
            {
                "name": "isConnected",
                "code": "public boolean isConnected() {\n        return sshClient.isConnected();\n    }"
            }
        ],
        "third_party": [
            "sleep",
            "info"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public void deleteNode(String serverId, List<String> pathList)",
        "start_line": "66",
        "end_line": "73",
        "file_path": "app/src/main/java/cc/cc1234/app/facade/PrettyZooFacade.java",
        "docstring": "The deleteNode function attempts to delete nodes from a Zookeeper instance based on a server ID and a list of paths.\\nIt calls the zookeeperDomainService to perform the deletion and catches any exceptions that occur.\\nIf an exception is caught, it logs an error message and throws an IllegalStateException.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a4e5b886b72b",
        "ground_truth": "public void deleteNode(String serverId, List<String> pathList) {\n    try {\n        zookeeperDomainService.delete(serverId, pathList);\n    } catch (Exception e) {\n        log.error(\"delete node failed\", e);\n        throw new IllegalStateException(e);\n    }\n}",
        "import_statements": [
            "import cc.cc1234.app.cache.TreeItemCache;",
            "import cc.cc1234.app.context.ActiveServerContext;",
            "import cc.cc1234.app.context.LocaleContext;",
            "import cc.cc1234.app.context.LogTailerThreadContext;",
            "import cc.cc1234.app.context.PrimaryStageContext;",
            "import cc.cc1234.app.fp.Try;",
            "import cc.cc1234.app.util.Asserts;",
            "import cc.cc1234.app.util.Fills;",
            "import cc.cc1234.app.util.ResourceBundleUtils;",
            "import cc.cc1234.app.view.toast.VToast;",
            "import cc.cc1234.app.vo.ConfigurationVOTransfer;",
            "import cc.cc1234.app.vo.ConnectionConfigurationVO;",
            "import cc.cc1234.app.vo.ServerConfigurationVO;",
            "import cc.cc1234.app.vo.ZkNodeSearchResult;",
            "import cc.cc1234.core.configuration.entity.Configuration;",
            "import cc.cc1234.core.configuration.entity.ConnectionConfiguration;",
            "import cc.cc1234.core.configuration.entity.ServerConfiguration;",
            "import cc.cc1234.core.configuration.service.ConfigurationDomainService;",
            "import cc.cc1234.core.configuration.value.SSHTunnelConfiguration;",
            "import cc.cc1234.core.zookeeper.service.ZookeeperDomainService;",
            "import cc.cc1234.specification.config.PrettyZooConfigRepository;",
            "import cc.cc1234.specification.config.model.ConfigData;",
            "import cc.cc1234.specification.listener.ConfigurationChangeListener;",
            "import cc.cc1234.specification.listener.ServerListener;",
            "import cc.cc1234.specification.listener.ZookeeperNodeListener;",
            "import cc.cc1234.specification.node.NodeMode;",
            "import cc.cc1234.specification.util.StringWriter;",
            "import com.google.common.base.Strings;",
            "import javafx.application.Platform;",
            "import javafx.collections.ObservableList;",
            "import javafx.scene.Scene;",
            "import javafx.scene.text.Text;",
            "import javafx.scene.text.TextFlow;",
            "import org.apache.commons.io.input.ReversedLinesFileReader;",
            "import org.apache.commons.io.input.Tailer;",
            "import org.apache.commons.io.input.TailerListener;",
            "import org.apache.zookeeper.data.Stat;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.*;",
            "import java.nio.charset.Charset;",
            "import java.nio.file.Files;",
            "import java.nio.file.Paths;",
            "import java.util.*;",
            "import java.util.concurrent.CompletableFuture;",
            "import java.util.function.Consumer;",
            "import java.util.stream.Collectors;"
        ],
        "reference_api": [
            "delete",
            "error"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "delete",
            "error"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public void disconnect(String id)",
        "start_line": "85",
        "end_line": "91",
        "file_path": "app/src/main/java/cc/cc1234/app/facade/PrettyZooFacade.java",
        "docstring": "The disconnect function disconnects a server from Zookeeper based on its ID.\\nIt runs a task on the JavaFX application thread that retrieves the server configuration by ID, disconnects from Zookeeper using the server's ID, and removes the server from the tree item cache.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4fdbc9ec2e75",
        "ground_truth": "public void disconnect(String id) {\n    Platform.runLater(() -> {\n        ServerConfiguration serverConfiguration = configurationDomainService.getById(id).orElseThrow();\n        zookeeperDomainService.disconnect(serverConfiguration.getId());\n        treeItemCache.remove(id);\n    });\n}",
        "import_statements": [
            "import cc.cc1234.app.cache.TreeItemCache;",
            "import cc.cc1234.app.context.ActiveServerContext;",
            "import cc.cc1234.app.context.LocaleContext;",
            "import cc.cc1234.app.context.LogTailerThreadContext;",
            "import cc.cc1234.app.context.PrimaryStageContext;",
            "import cc.cc1234.app.fp.Try;",
            "import cc.cc1234.app.util.Asserts;",
            "import cc.cc1234.app.util.Fills;",
            "import cc.cc1234.app.util.ResourceBundleUtils;",
            "import cc.cc1234.app.view.toast.VToast;",
            "import cc.cc1234.app.vo.ConfigurationVOTransfer;",
            "import cc.cc1234.app.vo.ConnectionConfigurationVO;",
            "import cc.cc1234.app.vo.ServerConfigurationVO;",
            "import cc.cc1234.app.vo.ZkNodeSearchResult;",
            "import cc.cc1234.core.configuration.entity.Configuration;",
            "import cc.cc1234.core.configuration.entity.ConnectionConfiguration;",
            "import cc.cc1234.core.configuration.entity.ServerConfiguration;",
            "import cc.cc1234.core.configuration.service.ConfigurationDomainService;",
            "import cc.cc1234.core.configuration.value.SSHTunnelConfiguration;",
            "import cc.cc1234.core.zookeeper.service.ZookeeperDomainService;",
            "import cc.cc1234.specification.config.PrettyZooConfigRepository;",
            "import cc.cc1234.specification.config.model.ConfigData;",
            "import cc.cc1234.specification.listener.ConfigurationChangeListener;",
            "import cc.cc1234.specification.listener.ServerListener;",
            "import cc.cc1234.specification.listener.ZookeeperNodeListener;",
            "import cc.cc1234.specification.node.NodeMode;",
            "import cc.cc1234.specification.util.StringWriter;",
            "import com.google.common.base.Strings;",
            "import javafx.application.Platform;",
            "import javafx.collections.ObservableList;",
            "import javafx.scene.Scene;",
            "import javafx.scene.text.Text;",
            "import javafx.scene.text.TextFlow;",
            "import org.apache.commons.io.input.ReversedLinesFileReader;",
            "import org.apache.commons.io.input.Tailer;",
            "import org.apache.commons.io.input.TailerListener;",
            "import org.apache.zookeeper.data.Stat;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.*;",
            "import java.nio.charset.Charset;",
            "import java.nio.file.Files;",
            "import java.nio.file.Paths;",
            "import java.util.*;",
            "import java.util.concurrent.CompletableFuture;",
            "import java.util.function.Consumer;",
            "import java.util.stream.Collectors;"
        ],
        "reference_api": [
            "getById",
            "disconnect",
            "orElseThrow",
            "runLater",
            "getId",
            "remove"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "disconnect",
                "code": "public void disconnect(String id) {\n        Platform.runLater(() -> {\n            ServerConfiguration serverConfiguration = configurationDomainService.getById(id).orElseThrow();\n            zookeeperDomainService.disconnect(serverConfiguration.getId());\n            treeItemCache.remove(id);\n        });\n    }"
            }
        ],
        "third_party": [
            "getById",
            "orElseThrow",
            "runLater",
            "getId",
            "remove"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public void startTerminal(String serverId, StringWriter stream)",
        "start_line": "305",
        "end_line": "315",
        "file_path": "app/src/main/java/cc/cc1234/app/facade/PrettyZooFacade.java",
        "docstring": "The startTerminal function initializes a terminal connection to a server using its ID and a StringWriter stream.\\nIt retrieves the server configuration and determines the connection URL based on whether SSH tunneling is enabled.\\nIt then calls the zookeeperDomainService to start the terminal with the specified server ID and connection URL.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6bf0d7ac9390",
        "ground_truth": "public void startTerminal(String serverId, StringWriter stream) {\n    ServerConfiguration server = configurationDomainService.getById(serverId)\n            .orElseThrow();\n    String urlToConnect;\n    if (server.getSshTunnelEnabled()) {\n        urlToConnect = \"localhost:\" + server.getPort();\n    } else {\n        urlToConnect = server.getHost() + \":\" + server.getPort();\n    }\n    zookeeperDomainService.initTerminal(serverId, urlToConnect, stream);\n}",
        "import_statements": [
            "import cc.cc1234.app.cache.TreeItemCache;",
            "import cc.cc1234.app.context.ActiveServerContext;",
            "import cc.cc1234.app.context.LocaleContext;",
            "import cc.cc1234.app.context.LogTailerThreadContext;",
            "import cc.cc1234.app.context.PrimaryStageContext;",
            "import cc.cc1234.app.fp.Try;",
            "import cc.cc1234.app.util.Asserts;",
            "import cc.cc1234.app.util.Fills;",
            "import cc.cc1234.app.util.ResourceBundleUtils;",
            "import cc.cc1234.app.view.toast.VToast;",
            "import cc.cc1234.app.vo.ConfigurationVOTransfer;",
            "import cc.cc1234.app.vo.ConnectionConfigurationVO;",
            "import cc.cc1234.app.vo.ServerConfigurationVO;",
            "import cc.cc1234.app.vo.ZkNodeSearchResult;",
            "import cc.cc1234.core.configuration.entity.Configuration;",
            "import cc.cc1234.core.configuration.entity.ConnectionConfiguration;",
            "import cc.cc1234.core.configuration.entity.ServerConfiguration;",
            "import cc.cc1234.core.configuration.service.ConfigurationDomainService;",
            "import cc.cc1234.core.configuration.value.SSHTunnelConfiguration;",
            "import cc.cc1234.core.zookeeper.service.ZookeeperDomainService;",
            "import cc.cc1234.specification.config.PrettyZooConfigRepository;",
            "import cc.cc1234.specification.config.model.ConfigData;",
            "import cc.cc1234.specification.listener.ConfigurationChangeListener;",
            "import cc.cc1234.specification.listener.ServerListener;",
            "import cc.cc1234.specification.listener.ZookeeperNodeListener;",
            "import cc.cc1234.specification.node.NodeMode;",
            "import cc.cc1234.specification.util.StringWriter;",
            "import com.google.common.base.Strings;",
            "import javafx.application.Platform;",
            "import javafx.collections.ObservableList;",
            "import javafx.scene.Scene;",
            "import javafx.scene.text.Text;",
            "import javafx.scene.text.TextFlow;",
            "import org.apache.commons.io.input.ReversedLinesFileReader;",
            "import org.apache.commons.io.input.Tailer;",
            "import org.apache.commons.io.input.TailerListener;",
            "import org.apache.zookeeper.data.Stat;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.*;",
            "import java.nio.charset.Charset;",
            "import java.nio.file.Files;",
            "import java.nio.file.Paths;",
            "import java.util.*;",
            "import java.util.concurrent.CompletableFuture;",
            "import java.util.function.Consumer;",
            "import java.util.stream.Collectors;"
        ],
        "reference_api": [
            "getById",
            "getHost",
            "getPort",
            "getSshTunnelEnabled",
            "orElseThrow",
            "initTerminal"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getById",
            "getHost",
            "getPort",
            "getSshTunnelEnabled",
            "orElseThrow",
            "initTerminal"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public void executeCommand(String id, String command)",
        "start_line": "317",
        "end_line": "324",
        "file_path": "app/src/main/java/cc/cc1234/app/facade/PrettyZooFacade.java",
        "docstring": "The executeCommand function attempts to execute a given command on a Zookeeper server identified by its ID.\\nIf the execution fails, it logs the error with the server ID and command, and displays an error message indicating that the command execution failed and should be retried.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "635dea120e55",
        "ground_truth": "public void executeCommand(String id, String command) {\n    try {\n        zookeeperDomainService.execute(id, command);\n    } catch (Exception e) {\n        log.error(\"execute command failed at  \" + id + \":\" + command, e);\n        VToast.error(\"\u547d\u4ee4\u6267\u884c\u5931\u8d25\uff0c\u8bf7\u91cd\u8bd5\");\n    }\n}",
        "import_statements": [
            "import cc.cc1234.app.cache.TreeItemCache;",
            "import cc.cc1234.app.context.ActiveServerContext;",
            "import cc.cc1234.app.context.LocaleContext;",
            "import cc.cc1234.app.context.LogTailerThreadContext;",
            "import cc.cc1234.app.context.PrimaryStageContext;",
            "import cc.cc1234.app.fp.Try;",
            "import cc.cc1234.app.util.Asserts;",
            "import cc.cc1234.app.util.Fills;",
            "import cc.cc1234.app.util.ResourceBundleUtils;",
            "import cc.cc1234.app.view.toast.VToast;",
            "import cc.cc1234.app.vo.ConfigurationVOTransfer;",
            "import cc.cc1234.app.vo.ConnectionConfigurationVO;",
            "import cc.cc1234.app.vo.ServerConfigurationVO;",
            "import cc.cc1234.app.vo.ZkNodeSearchResult;",
            "import cc.cc1234.core.configuration.entity.Configuration;",
            "import cc.cc1234.core.configuration.entity.ConnectionConfiguration;",
            "import cc.cc1234.core.configuration.entity.ServerConfiguration;",
            "import cc.cc1234.core.configuration.service.ConfigurationDomainService;",
            "import cc.cc1234.core.configuration.value.SSHTunnelConfiguration;",
            "import cc.cc1234.core.zookeeper.service.ZookeeperDomainService;",
            "import cc.cc1234.specification.config.PrettyZooConfigRepository;",
            "import cc.cc1234.specification.config.model.ConfigData;",
            "import cc.cc1234.specification.listener.ConfigurationChangeListener;",
            "import cc.cc1234.specification.listener.ServerListener;",
            "import cc.cc1234.specification.listener.ZookeeperNodeListener;",
            "import cc.cc1234.specification.node.NodeMode;",
            "import cc.cc1234.specification.util.StringWriter;",
            "import com.google.common.base.Strings;",
            "import javafx.application.Platform;",
            "import javafx.collections.ObservableList;",
            "import javafx.scene.Scene;",
            "import javafx.scene.text.Text;",
            "import javafx.scene.text.TextFlow;",
            "import org.apache.commons.io.input.ReversedLinesFileReader;",
            "import org.apache.commons.io.input.Tailer;",
            "import org.apache.commons.io.input.TailerListener;",
            "import org.apache.zookeeper.data.Stat;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.*;",
            "import java.nio.charset.Charset;",
            "import java.nio.file.Files;",
            "import java.nio.file.Paths;",
            "import java.util.*;",
            "import java.util.concurrent.CompletableFuture;",
            "import java.util.function.Consumer;",
            "import java.util.stream.Collectors;"
        ],
        "reference_api": [
            "execute",
            "error"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "execute",
            "error"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public Properties loadZookeeperSystemProperties()",
        "start_line": "390",
        "end_line": "407",
        "file_path": "app/src/main/java/cc/cc1234/app/facade/PrettyZooFacade.java",
        "docstring": "The loadZookeeperSystemProperties function loads system properties from a specified file path.\\nIt checks if the file exists and attempts to load the properties from it.\\nIf successful, it logs the loaded properties.\\nIf an error occurs or the file does not exist, it logs an appropriate message and returns an empty Properties object.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8627d161f5fc",
        "ground_truth": "public Properties loadZookeeperSystemProperties() {\n    String sysPropPath = PrettyZooConfigRepository.SYS_PROP_PATH;\n    if (Files.exists(Paths.get(sysPropPath))) {\n        try {\n            var properties = new Properties();\n            properties.load(new FileInputStream(sysPropPath));\n            log.info(\"load system properties success ->\\n {}\", properties);\n            return properties;\n        } catch (IOException e) {\n            // ignore error and log it\n            log.error(\"load system properties failed\", e);\n            return new Properties();\n        }\n    } else {\n        log.info(\"ignore load system properties, file not exists -> {}\", sysPropPath);\n        return new Properties();\n    }\n}",
        "import_statements": [
            "import cc.cc1234.app.cache.TreeItemCache;",
            "import cc.cc1234.app.context.ActiveServerContext;",
            "import cc.cc1234.app.context.LocaleContext;",
            "import cc.cc1234.app.context.LogTailerThreadContext;",
            "import cc.cc1234.app.context.PrimaryStageContext;",
            "import cc.cc1234.app.fp.Try;",
            "import cc.cc1234.app.util.Asserts;",
            "import cc.cc1234.app.util.Fills;",
            "import cc.cc1234.app.util.ResourceBundleUtils;",
            "import cc.cc1234.app.view.toast.VToast;",
            "import cc.cc1234.app.vo.ConfigurationVOTransfer;",
            "import cc.cc1234.app.vo.ConnectionConfigurationVO;",
            "import cc.cc1234.app.vo.ServerConfigurationVO;",
            "import cc.cc1234.app.vo.ZkNodeSearchResult;",
            "import cc.cc1234.core.configuration.entity.Configuration;",
            "import cc.cc1234.core.configuration.entity.ConnectionConfiguration;",
            "import cc.cc1234.core.configuration.entity.ServerConfiguration;",
            "import cc.cc1234.core.configuration.service.ConfigurationDomainService;",
            "import cc.cc1234.core.configuration.value.SSHTunnelConfiguration;",
            "import cc.cc1234.core.zookeeper.service.ZookeeperDomainService;",
            "import cc.cc1234.specification.config.PrettyZooConfigRepository;",
            "import cc.cc1234.specification.config.model.ConfigData;",
            "import cc.cc1234.specification.listener.ConfigurationChangeListener;",
            "import cc.cc1234.specification.listener.ServerListener;",
            "import cc.cc1234.specification.listener.ZookeeperNodeListener;",
            "import cc.cc1234.specification.node.NodeMode;",
            "import cc.cc1234.specification.util.StringWriter;",
            "import com.google.common.base.Strings;",
            "import javafx.application.Platform;",
            "import javafx.collections.ObservableList;",
            "import javafx.scene.Scene;",
            "import javafx.scene.text.Text;",
            "import javafx.scene.text.TextFlow;",
            "import org.apache.commons.io.input.ReversedLinesFileReader;",
            "import org.apache.commons.io.input.Tailer;",
            "import org.apache.commons.io.input.TailerListener;",
            "import org.apache.zookeeper.data.Stat;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.*;",
            "import java.nio.charset.Charset;",
            "import java.nio.file.Files;",
            "import java.nio.file.Paths;",
            "import java.util.*;",
            "import java.util.concurrent.CompletableFuture;",
            "import java.util.function.Consumer;",
            "import java.util.stream.Collectors;"
        ],
        "reference_api": [
            "exists",
            "error",
            "load",
            "get",
            "info"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "exists",
            "error",
            "load",
            "get",
            "info"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public void saveZookeeperSystemProperties(String content)",
        "start_line": "409",
        "end_line": "425",
        "file_path": "app/src/main/java/cc/cc1234/app/facade/PrettyZooFacade.java",
        "docstring": "The saveZookeeperSystemProperties function saves Zookeeper system properties from a given content string.\\nIt loads the properties from the content into a Properties object using a StringReader.\\nIf loading fails, it logs an error and shows a notification.\\nIt then attempts to save the properties to a file specified by SYS_PROP_PATH.\\nIf saving fails, it logs an error and shows a notification.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "dce20e143e04",
        "ground_truth": "public void saveZookeeperSystemProperties(String content) {\n    String sysPropPath = PrettyZooConfigRepository.SYS_PROP_PATH;\n    Properties properties = new Properties();\n    try (StringReader reader = new StringReader(content)) {\n        properties.load(reader);\n    } catch (IOException e) {\n        log.error(\"save properties failed: \" + content, e);\n        Platform.runLater(() -> VToast.info(ResourceBundleUtils.getContent(\"notification.save.failed\")));\n    }\n    try (OutputStream out = Files.newOutputStream(Paths.get(sysPropPath))) {\n        properties.store(out, null);\n    } catch (IOException e) {\n        log.error(\"save properties file failed\", e);\n        Platform.runLater(() -> VToast.info(ResourceBundleUtils.getContent(\"notification.save.failed\")));\n    }\n}",
        "import_statements": [
            "import cc.cc1234.app.cache.TreeItemCache;",
            "import cc.cc1234.app.context.ActiveServerContext;",
            "import cc.cc1234.app.context.LocaleContext;",
            "import cc.cc1234.app.context.LogTailerThreadContext;",
            "import cc.cc1234.app.context.PrimaryStageContext;",
            "import cc.cc1234.app.fp.Try;",
            "import cc.cc1234.app.util.Asserts;",
            "import cc.cc1234.app.util.Fills;",
            "import cc.cc1234.app.util.ResourceBundleUtils;",
            "import cc.cc1234.app.view.toast.VToast;",
            "import cc.cc1234.app.vo.ConfigurationVOTransfer;",
            "import cc.cc1234.app.vo.ConnectionConfigurationVO;",
            "import cc.cc1234.app.vo.ServerConfigurationVO;",
            "import cc.cc1234.app.vo.ZkNodeSearchResult;",
            "import cc.cc1234.core.configuration.entity.Configuration;",
            "import cc.cc1234.core.configuration.entity.ConnectionConfiguration;",
            "import cc.cc1234.core.configuration.entity.ServerConfiguration;",
            "import cc.cc1234.core.configuration.service.ConfigurationDomainService;",
            "import cc.cc1234.core.configuration.value.SSHTunnelConfiguration;",
            "import cc.cc1234.core.zookeeper.service.ZookeeperDomainService;",
            "import cc.cc1234.specification.config.PrettyZooConfigRepository;",
            "import cc.cc1234.specification.config.model.ConfigData;",
            "import cc.cc1234.specification.listener.ConfigurationChangeListener;",
            "import cc.cc1234.specification.listener.ServerListener;",
            "import cc.cc1234.specification.listener.ZookeeperNodeListener;",
            "import cc.cc1234.specification.node.NodeMode;",
            "import cc.cc1234.specification.util.StringWriter;",
            "import com.google.common.base.Strings;",
            "import javafx.application.Platform;",
            "import javafx.collections.ObservableList;",
            "import javafx.scene.Scene;",
            "import javafx.scene.text.Text;",
            "import javafx.scene.text.TextFlow;",
            "import org.apache.commons.io.input.ReversedLinesFileReader;",
            "import org.apache.commons.io.input.Tailer;",
            "import org.apache.commons.io.input.TailerListener;",
            "import org.apache.zookeeper.data.Stat;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.*;",
            "import java.nio.charset.Charset;",
            "import java.nio.file.Files;",
            "import java.nio.file.Paths;",
            "import java.util.*;",
            "import java.util.concurrent.CompletableFuture;",
            "import java.util.function.Consumer;",
            "import java.util.stream.Collectors;"
        ],
        "reference_api": [
            "error",
            "getContent",
            "store",
            "load",
            "newOutputStream",
            "get",
            "info",
            "runLater"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "error",
            "getContent",
            "store",
            "load",
            "newOutputStream",
            "get",
            "info",
            "runLater"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "private String response(Socket client)",
        "start_line": "31",
        "end_line": "44",
        "file_path": "core/src/main/java/cc/cc1234/core/zookeeper/entity/FourLetterCommand.java",
        "docstring": "The response function reads data from a socket client and returns it as a string.\\nIt uses a BufferedReader to read the input stream line by line, appending each line to a StringBuilder.\\nAfter reading all data, it cleans up the socket and returns the accumulated string.\\nIf an exception occurs, it throws an IllegalStateException.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d89f8e6bfb31",
        "ground_truth": "private String response(Socket client) {\n    try {\n        var reader = new BufferedReader(new InputStreamReader(client.getInputStream()));\n        var builder = new StringBuilder(\"\");\n        String str;\n        while ((str = reader.readLine()) != null) {\n            builder.append(str).append(\"\\n\");\n        }\n        cleanup(client);\n        return builder.toString();\n    } catch (Exception e) {\n        throw new IllegalStateException(e);\n    }\n}",
        "import_statements": [
            "import java.io.*;",
            "import java.net.InetSocketAddress;",
            "import java.net.Socket;"
        ],
        "reference_api": [
            "getInputStream",
            "readLine",
            "toString",
            "append",
            "cleanup"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "cleanup",
                "code": "private void cleanup(Closeable c) {\n        if (c != null) {\n            try {\n                c.close();\n            } catch (IOException e) {\n                // ignore\n            }\n        }\n    }"
            }
        ],
        "third_party": [
            "getInputStream",
            "readLine",
            "append"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params)",
        "start_line": "29",
        "end_line": "42",
        "file_path": "specification-impl/src/main/java/cc/cc1234/client/curator/CuratorZookeeperConnectionFactory.java",
        "docstring": "The create function establishes a Zookeeper connection using the given parameters.\\nIt initializes and starts a CuratorFramework client, then waits up to 5 seconds for the connection to be established.\\nIf the connection fails or is interrupted, it closes the client and throws an exception.\\nOn successful connection, it returns a new CuratorZookeeperConnection with the provided parameters.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b0a277c5e3c0",
        "ground_truth": "public ZookeeperConnection<CuratorFramework> create(ZookeeperParams params) {\n    final CuratorFramework client = curatorFramework(params);\n    client.start();\n    try {\n        if (!client.blockUntilConnected(5, TimeUnit.SECONDS)) {\n            client.close();\n            throw new IllegalStateException(\"connect timeout\");\n        }\n    } catch (InterruptedException e) {\n        throw new IllegalStateException(\"connect timeout\", e);\n    }\n    return new CuratorZookeeperConnection(params.getId(), client);\n}",
        "import_statements": [
            "import cc.cc1234.specification.connection.ZookeeperConnection;",
            "import cc.cc1234.specification.connection.ZookeeperConnectionFactory;",
            "import cc.cc1234.specification.connection.ZookeeperParams;",
            "import cc.cc1234.specification.listener.ServerListener;",
            "import org.apache.curator.RetryPolicy;",
            "import org.apache.curator.framework.AuthInfo;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.CuratorFrameworkFactory;",
            "import org.apache.curator.framework.api.ACLProvider;",
            "import org.apache.curator.framework.api.CuratorEventType;",
            "import org.apache.curator.framework.state.ConnectionState;",
            "import org.apache.curator.framework.state.ConnectionStateListener;",
            "import org.apache.curator.retry.ExponentialBackoffRetry;",
            "import org.apache.zookeeper.ZooDefs;",
            "import org.apache.zookeeper.data.ACL;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.util.List;",
            "import java.util.concurrent.TimeUnit;"
        ],
        "reference_api": [
            "blockUntilConnected",
            "close",
            "getId",
            "start",
            "curatorFramework"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "curatorFramework",
                "code": "private CuratorFramework curatorFramework(ZookeeperParams params) {\n        final RetryPolicy retryPolicy =\n            new ExponentialBackoffRetry(params.getRetryIntervalTime(), params.getMaxRetries());\n        final CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder()\n            .connectString(params.getUrl())\n            .connectionTimeoutMs(params.getConnectionTimeout())\n            .sessionTimeoutMs(params.getSessionTimeout())\n            .retryPolicy(retryPolicy);\n\n        List<AuthInfo> acls = params.getAclList()\n            .stream()\n            .filter(s -> !s.isBlank())\n            .map(ACLs::parseDigest)\n            .toList();\n        if (!acls.isEmpty()) {\n            builder.authorization(acls)\n                .aclProvider(new ACLProvider() {\n                    @Override\n                    public List<ACL> getDefaultAcl() {\n                        return ZooDefs.Ids.CREATOR_ALL_ACL;\n                    }\n\n                    @Override\n                    public List<ACL> getAclForPath(String path) {\n                        return ZooDefs.Ids.CREATOR_ALL_ACL;\n                    }\n                });\n        }\n\n        return builder.build();\n    }"
            }
        ],
        "third_party": [
            "blockUntilConnected",
            "close",
            "getId",
            "start"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "vran-dev/PrettyZoo",
        "function_declaration": "public void sync(List<ZookeeperNodeListener> listeners)",
        "start_line": "89",
        "end_line": "104",
        "file_path": "specification-impl/src/main/java/cc/cc1234/client/curator/CuratorZookeeperConnection.java",
        "docstring": "The sync function synchronizes Zookeeper nodes using a list of ZookeeperNodeListeners.\\nIt retrieves the current Zookeeper connection string and checks if synchronization is already in progress.\\nIf not, it logs the start of the synchronization process, adds a CuratorTreeCacheListener to the TreeCache, and attempts to start the TreeCache.\\nIf an error occurs, it logs the error and closes the TreeCache.\\nIf synchronization is already in progress, it logs an informational message.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b144bcd67462",
        "ground_truth": "public void sync(List<ZookeeperNodeListener> listeners) {\n    final String server = curatorFramework.getZookeeperClient().getCurrentConnectionString();\n    if (!isSync.get()) {\n        log.debug(\"begin to sync tree node from {}\", server);\n        treeCache.getListenable().addListener(new CuratorTreeCacheListener(id, listeners));\n        try {\n            treeCache.start();\n            isSync.set(true);\n        } catch (Exception e) {\n            log.error(\"sync zookeeper node error\", e);\n            treeCache.close();\n        }\n    } else {\n        log.info(\"ignore sync operation, because of {} [{}] has been sync\", server, getId());\n    }\n}",
        "import_statements": [
            "import cc.cc1234.specification.connection.ZookeeperConnection;",
            "import cc.cc1234.specification.listener.ZookeeperNodeListener;",
            "import cc.cc1234.specification.node.NodeMode;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.api.CreateBuilder;",
            "import org.apache.curator.framework.api.DeleteBuilder;",
            "import org.apache.curator.framework.recipes.cache.TreeCache;",
            "import org.apache.zookeeper.CreateMode;",
            "import org.apache.zookeeper.data.Stat;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.util.List;",
            "import java.util.concurrent.atomic.AtomicBoolean;"
        ],
        "reference_api": [
            "error",
            "set",
            "addListener",
            "getZookeeperClient",
            "getCurrentConnectionString",
            "getListenable",
            "debug",
            "get",
            "close",
            "info",
            "getId",
            "start"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "close",
                "code": "@Override\n    public void close() {\n        treeCache.close();\n        curatorFramework.close();\n        isSync.set(false);\n    }"
            },
            {
                "name": "getId",
                "code": "@Override\n    public String getId() {\n        return this.id;\n    }"
            }
        ],
        "third_party": [
            "error",
            "set",
            "addListener",
            "getZookeeperClient",
            "getCurrentConnectionString",
            "getListenable",
            "debug",
            "get",
            "info",
            "start"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "luxiaoxun/NettyRpc",
        "function_declaration": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception",
        "start_line": "75",
        "end_line": "80",
        "file_path": "netty-rpc-common/src/main/java/com/netty/rpc/zookeeper/CuratorClient.java",
        "docstring": "The watchPathChildrenNode function sets up a watch on the children of a specified Zookeeper node path.\\nIt creates a PathChildrenCache for the given path, starts it in BUILD_INITIAL_CACHE mode, and adds a listener to handle child node events.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6452f272f2d7",
        "ground_truth": "public void watchPathChildrenNode(String path, PathChildrenCacheListener listener) throws Exception {\n    PathChildrenCache pathChildrenCache = new PathChildrenCache(client, path, true);\n    //BUILD_INITIAL_CACHE \u4ee3\u8868\u4f7f\u7528\u540c\u6b65\u7684\u65b9\u5f0f\u8fdb\u884c\u7f13\u5b58\u521d\u59cb\u5316\u3002\n    pathChildrenCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);\n    pathChildrenCache.getListenable().addListener(listener);\n}",
        "import_statements": [
            "import com.netty.rpc.config.Constant;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.CuratorFrameworkFactory;",
            "import org.apache.curator.framework.recipes.cache.PathChildrenCache;",
            "import org.apache.curator.framework.recipes.cache.PathChildrenCacheListener;",
            "import org.apache.curator.framework.recipes.cache.TreeCache;",
            "import org.apache.curator.framework.recipes.cache.TreeCacheListener;",
            "import org.apache.curator.framework.state.ConnectionStateListener;",
            "import org.apache.curator.retry.ExponentialBackoffRetry;",
            "import org.apache.zookeeper.CreateMode;",
            "import org.apache.zookeeper.Watcher;",
            "import java.util.List;"
        ],
        "reference_api": [
            "getListenable",
            "start",
            "addListener"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getListenable",
            "start",
            "addListener"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "luxiaoxun/NettyRpc",
        "function_declaration": "public void unregisterService()",
        "start_line": "78",
        "end_line": "88",
        "file_path": "netty-rpc-server/src/main/java/com/netty/rpc/server/registry/ServiceRegistry.java",
        "docstring": "The unregisterService function unregisters all services by deleting their paths from Zookeeper.\\nIt logs the start of the unregistration process and iterates through a list of paths, attempting to delete each one using the Curator client.\\nIf an error occurs during deletion, it logs the error message.\\nFinally, it closes the Curator client.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "17f786e147c3",
        "ground_truth": "public void unregisterService() {\n    logger.info(\"Unregister all service\");\n    for (String path : pathList) {\n        try {\n            this.curatorClient.deletePath(path);\n        } catch (Exception ex) {\n            logger.error(\"Delete service path error: \" + ex.getMessage());\n        }\n    }\n    this.curatorClient.close();\n}",
        "import_statements": [
            "import com.netty.rpc.config.Constant;",
            "import com.netty.rpc.protocol.RpcProtocol;",
            "import com.netty.rpc.protocol.RpcServiceInfo;",
            "import com.netty.rpc.util.ServiceUtil;",
            "import com.netty.rpc.zookeeper.CuratorClient;",
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.state.ConnectionState;",
            "import org.apache.curator.framework.state.ConnectionStateListener;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.util.ArrayList;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "error",
            "getMessage",
            "deletePath",
            "close",
            "info"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "error",
            "getMessage",
            "deletePath",
            "close",
            "info"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "javahongxi/whatsmars",
        "function_declaration": "public void addListener(IZkStateListener listener)",
        "start_line": "48",
        "end_line": "55",
        "file_path": "whatsmars-zk/src/main/java/org/hongxi/whatsmars/zk/remoting/zkclient/ZkClientWrapper.java",
        "docstring": "The addListener function adds a state listener to the Zookeeper client.\\nIt uses a CompletableFuture to execute the makeClientReady method and, if no exception occurs, subscribes the provided listener to state changes in the client.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "32fec6cfa353",
        "ground_truth": "public void addListener(IZkStateListener listener) {\n    completableFuture.whenComplete((value, exception) -> {\n        this.makeClientReady(value, exception);\n        if (exception == null) {\n            client.subscribeStateChanges(listener);\n        }\n    });\n}",
        "import_statements": [
            "import org.I0Itec.zkclient.IZkChildListener;",
            "import org.I0Itec.zkclient.IZkStateListener;",
            "import org.I0Itec.zkclient.ZkClient;",
            "import org.apache.zookeeper.Watcher.Event.KeeperState;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.util.List;",
            "import java.util.concurrent.CompletableFuture;",
            "import java.util.concurrent.TimeUnit;"
        ],
        "reference_api": [
            "whenComplete",
            "subscribeStateChanges",
            "makeClientReady"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "makeClientReady",
                "code": "private void makeClientReady(ZkClient client, Throwable e) {\n        if (e != null) {\n            logger.error(\"Got an exception when trying to create zkclient instance, can not connect to zookeeper server, please check!\", e);\n        } else {\n            this.client = client;\n//            this.client.subscribeStateChanges(stateListener);\n        }\n    }"
            }
        ],
        "third_party": [
            "whenComplete",
            "subscribeStateChanges"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "javahongxi/whatsmars",
        "function_declaration": "public CuratorZookeeperClient(String serverAddr, String authority)",
        "start_line": "24",
        "end_line": "50",
        "file_path": "whatsmars-zk/src/main/java/org/hongxi/whatsmars/zk/remoting/curator/CuratorZookeeperClient.java",
        "docstring": "The CuratorZookeeperClient constructor initializes a CuratorFramework client to connect to a Zookeeper server using the provided server address and optional authority for authentication.\\nIt sets up a retry policy and connection timeout, and configures the client with authorization if provided.\\nA connection state listener is added to handle state changes such as LOST, CONNECTED, and RECONNECTED, triggering appropriate actions.\\nThe client is then started, and any exceptions during initialization result in an IllegalStateException being thrown.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "619d57562a76",
        "ground_truth": "public CuratorZookeeperClient(String serverAddr, String authority) {\n    try {\n        CuratorFrameworkFactory.Builder builder = CuratorFrameworkFactory.builder()\n                .connectString(serverAddr)\n                .retryPolicy(new RetryNTimes(1, 1000))\n                .connectionTimeoutMs(5000);\n        if (authority != null && authority.length() > 0) {\n            builder = builder.authorization(\"digest\", authority.getBytes());\n        }\n        client = builder.build();\n        client.getConnectionStateListenable().addListener(new ConnectionStateListener() {\n            @Override\n            public void stateChanged(CuratorFramework client, ConnectionState state) {\n                if (state == ConnectionState.LOST) {\n                    CuratorZookeeperClient.this.stateChanged(StateListener.DISCONNECTED);\n                } else if (state == ConnectionState.CONNECTED) {\n                    CuratorZookeeperClient.this.stateChanged(StateListener.CONNECTED);\n                } else if (state == ConnectionState.RECONNECTED) {\n                    CuratorZookeeperClient.this.stateChanged(StateListener.RECONNECTED);\n                }\n            }\n        });\n        client.start();\n    } catch (Exception e) {\n        throw new IllegalStateException(e.getMessage(), e);\n    }\n}",
        "import_statements": [
            "import org.apache.curator.framework.CuratorFramework;",
            "import org.apache.curator.framework.CuratorFrameworkFactory;",
            "import org.apache.curator.framework.api.CuratorWatcher;",
            "import org.apache.curator.framework.state.ConnectionState;",
            "import org.apache.curator.framework.state.ConnectionStateListener;",
            "import org.apache.curator.retry.RetryNTimes;",
            "import org.apache.zookeeper.CreateMode;",
            "import org.apache.zookeeper.KeeperException.NoNodeException;",
            "import org.apache.zookeeper.KeeperException.NodeExistsException;",
            "import org.apache.zookeeper.WatchedEvent;",
            "import org.hongxi.whatsmars.zk.remoting.ChildListener;",
            "import org.hongxi.whatsmars.zk.remoting.StateListener;",
            "import org.hongxi.whatsmars.zk.remoting.support.AbstractZookeeperClient;",
            "import java.util.Collections;",
            "import java.util.List;"
        ],
        "reference_api": [
            "stateChanged"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "stateChanged",
                "code": "@Override\n                public void stateChanged(CuratorFramework client, ConnectionState state) {\n                    if (state == ConnectionState.LOST) {\n                        CuratorZookeeperClient.this.stateChanged(StateListener.DISCONNECTED);\n                    } else if (state == ConnectionState.CONNECTED) {\n                        CuratorZookeeperClient.this.stateChanged(StateListener.CONNECTED);\n                    } else if (state == ConnectionState.RECONNECTED) {\n                        CuratorZookeeperClient.this.stateChanged(StateListener.RECONNECTED);\n                    }\n                }"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "javahongxi/whatsmars",
        "function_declaration": "public void removeChildListener(String path, ChildListener listener)",
        "start_line": "73",
        "end_line": "81",
        "file_path": "whatsmars-zk/src/main/java/org/hongxi/whatsmars/zk/remoting/support/AbstractZookeeperClient.java",
        "docstring": "The removeChildListener function removes a specified ChildListener from the listeners associated with a given path.\\nIt retrieves the map of listeners for the path and removes the specified listener if it exists.\\nIf the listener is successfully removed, it also removes the corresponding TargetChildListener for the path.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9e7fb18946e4",
        "ground_truth": "public void removeChildListener(String path, ChildListener listener) {\n    ConcurrentMap<ChildListener, TargetChildListener> listeners = childListeners.get(path);\n    if (listeners != null) {\n        TargetChildListener targetListener = listeners.remove(listener);\n        if (targetListener != null) {\n            removeTargetChildListener(path, targetListener);\n        }\n    }\n}",
        "import_statements": [
            "import org.hongxi.whatsmars.zk.remoting.ChildListener;",
            "import org.hongxi.whatsmars.zk.remoting.StateListener;",
            "import org.hongxi.whatsmars.zk.remoting.ZookeeperClient;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.util.List;",
            "import java.util.Set;",
            "import java.util.concurrent.ConcurrentHashMap;",
            "import java.util.concurrent.ConcurrentMap;",
            "import java.util.concurrent.CopyOnWriteArraySet;"
        ],
        "reference_api": [
            "removeTargetChildListener",
            "get",
            "remove"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "removeTargetChildListener",
                "code": "protected abstract void removeTargetChildListener(String path, TargetChildListener listener);"
            }
        ],
        "third_party": [
            "get",
            "remove"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Qihoo360/QConf",
        "function_declaration": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh)",
        "start_line": "778",
        "end_line": "815",
        "file_path": "agent/qconf_watcher.cc",
        "docstring": "The watcher_reconnect_to_zookeeper function attempts to reconnect a ZooKeeper handle.\\nIt first checks if the handle is null and returns an error if so.\\nIt retrieves and deletes the IDC host information associated with the handle, then closes the old handle.\\nIt deserializes the IDC and host information and initializes a new ZooKeeper handle with this information.\\nIf successful, it initializes the environment for the new handle and resets the table watcher, returning success.\\nIf any step fails, it logs an error and returns an error code.",
        "language": "CPP",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "2c3fae642654",
        "ground_truth": "static int watcher_reconnect_to_zookeeper(zhandle_t *zh)\n{\n    if (NULL == zh) return QCONF_ERR_OTHER;\n     zhandle_t *hthandle = NULL;\n    string idc_host, idc, host;\n    int ret = QCONF_ERR_OTHER;\n    unsigned long htkey = reinterpret_cast<unsigned long>(zh);\n    if (QCONF_OK == lock_ht_find(_ht_handle_idchost, _ht_hi_mutex, htkey, idc_host))\n    {\n        lock_ht_delete(_ht_handle_idchost, _ht_hi_mutex, htkey);\n        lock_ht_delete(_ht_idchost_handle, _ht_ih_mutex, idc_host);\n         // close old handle\n        zookeeper_close(zh);\n         deserialize_from_idc_host(idc_host, idc, host);\n        hthandle = zookeeper_init(host.c_str(), global_watcher, _recv_timeout, NULL, NULL, 0);\n        if (NULL != hthandle)\n        {\n            // init env zk related\n            init_env_for_zk(hthandle, idc_host, idc);\n             // reset the table watcher\n            _finish_process_tbl_sleep_setting = true;\n            ret = QCONF_OK;\n        }\n        else\n        {\n            LOG_ERR(\"Failed to initial zookeeper. host:%s\", host.c_str());\n        }\n    }\n    else\n    {\n        LOG_ERR(\"Failed to find in [handle to host] table.\");\n    }\n    return ret;\n}",
        "import_statements": [
            "#include <string.h>\n",
            "#include <unistd.h>\n",
            "#include <pthread.h>\n",
            "#include <map>\n",
            "#include <set>\n",
            "#include <deque>\n",
            "#include <vector>\n",
            "#include \"qconf_zoo.h\"\n",
            "#include \"qconf_log.h\"\n",
            "#include \"qconf_msg.h\"\n",
            "#include \"qconf_shm.h\"\n",
            "#include \"qconf_dump.h\"\n",
            "#include \"qconf_const.h\"\n",
            "#include \"qconf_script.h\"\n",
            "#include \"qconf_format.h\"\n",
            "#include \"qconf_config.h\"\n",
            "#include \"qconf_watcher.h\"\n",
            "#include \"qconf_feedback.h\"\n",
            "#include \"qconf_feedback.h\"\n",
            "#include \"qconf_gray.h\"\n",
            "#include \"qconf_lock.h\"\n"
        ],
        "reference_api": [
            "host.c_str",
            "deserialize_from_idc_host",
            "lock_ht_find",
            "lock_ht_delete",
            "init_env_for_zk",
            "zookeeper_init",
            "zookeeper_close",
            "reinterpret_cast<unsigned long>",
            "LOG_ERR"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "lock_ht_find",
                "code": "static int lock_ht_find(const map<K, V> &ht, Mutex &mu, const K &key, V &val)\n{\n    int ret = QCONF_ERR_OTHER;\n    typename map<K, V>::const_iterator it; \n    mu.Lock();\n    it = ht.find(key);\n    if (it != ht.end())\n    {\n        val = it->second;\n        ret = QCONF_OK;\n    }\n    mu.Unlock();\n    return ret;\n}"
            },
            {
                "name": "lock_ht_delete",
                "code": "static void lock_ht_delete(map<K, V> &ht, Mutex &mu, const K &key)\n{\n    mu.Lock();\n    ht.erase(key);\n    mu.Unlock();\n}"
            }
        ],
        "third_party": [
            "host.c_str",
            "deserialize_from_idc_host",
            "init_env_for_zk",
            "zookeeper_init",
            "zookeeper_close",
            "reinterpret_cast<unsigned long>",
            "LOG_ERR"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "Qihoo360/QConf",
        "function_declaration": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc)",
        "start_line": "817",
        "end_line": "847",
        "file_path": "agent/qconf_watcher.cc",
        "docstring": "The init_env_for_zk function initializes the environment for a Zookeeper handle with a specified IDC host and IDC.\\nIt updates internal tables with the handle and host information, then checks the connection state of the Zookeeper handle.\\nIf the handle is not connected, the function exits early.\\nIf connected, it registers the current host on the Zookeeper server and sets a watcher on the notify node.\\nDepending on the result of setting the watcher, it may add the IDC to a gray list or log an error if the operation fails.",
        "language": "CPP",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "0b5ab34e1dd9",
        "ground_truth": "static void init_env_for_zk(zhandle_t *zh, const string &idc_host, const string &idc)\n{\n    // Update table\n    unsigned long htkey_new = reinterpret_cast<unsigned long>(zh);\n    lock_ht_update(_ht_handle_idchost, _ht_hi_mutex, htkey_new, idc_host);\n    lock_ht_update(_ht_idchost_handle, _ht_ih_mutex, idc_host, zh);\n     /*\n    ** \u5982\u679c\u6ca1\u6709\u771f\u6b63\u5efa\u7acb\u8fde\u63a5\uff0c\u76f4\u63a5\u9000\u51fa\uff0c\u7b49\u5f85global_watcher\u56de\u8c03\u65f6\u91cd\u65b0init\n    */\n    int state = zoo_state(zh);\n    LOG_INFO(\"the state is %d\",state);\n    if(state != CONNECTED_STATE_DEF){\n        return;\n    }\n     // Reregister Current Host on Zookeeper host\n    zk_register_ephemeral(zh, _register_node_path, QCONF_AGENT_VERSION);\n     // Watch notify node for current machine\n    switch (watch_notify_node(zh))\n    {\n        case QCONF_OK:\n            add_gray_idc(idc);\n            break;\n        case QCONF_NODE_NOT_EXIST:\n            break;\n        default:\n            LOG_FATAL_ERR(\"Failed to set watcher for notify node on idc: %s!\", idc.c_str());\n    }\n}",
        "import_statements": [
            "#include <string.h>\n",
            "#include <unistd.h>\n",
            "#include <pthread.h>\n",
            "#include <map>\n",
            "#include <set>\n",
            "#include <deque>\n",
            "#include <vector>\n",
            "#include \"qconf_zoo.h\"\n",
            "#include \"qconf_log.h\"\n",
            "#include \"qconf_msg.h\"\n",
            "#include \"qconf_shm.h\"\n",
            "#include \"qconf_dump.h\"\n",
            "#include \"qconf_const.h\"\n",
            "#include \"qconf_script.h\"\n",
            "#include \"qconf_format.h\"\n",
            "#include \"qconf_config.h\"\n",
            "#include \"qconf_watcher.h\"\n",
            "#include \"qconf_feedback.h\"\n",
            "#include \"qconf_feedback.h\"\n",
            "#include \"qconf_gray.h\"\n",
            "#include \"qconf_lock.h\"\n"
        ],
        "reference_api": [
            "zoo_state",
            "watch_notify_node",
            "zk_register_ephemeral",
            "add_gray_idc",
            "LOG_FATAL_ERR",
            "LOG_INFO",
            "idc.c_str",
            "lock_ht_update",
            "reinterpret_cast<unsigned long>"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "lock_ht_update",
                "code": "static void lock_ht_update(map<K, V> &ht, Mutex &mu, const K &key, const V &val)\n{\n    mu.Lock();\n    ht[key] = val;\n    mu.Unlock();\n}"
            }
        ],
        "third_party": [
            "zoo_state",
            "watch_notify_node",
            "zk_register_ephemeral",
            "add_gray_idc",
            "LOG_FATAL_ERR",
            "LOG_INFO",
            "idc.c_str",
            "reinterpret_cast<unsigned long>"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "python-zk/kazoo",
        "function_declaration": "def make_digest_acl_credential(username, password)",
        "start_line": "65",
        "end_line": "79",
        "file_path": "kazoo/security.py",
        "docstring": "The make_digest_acl_credential function creates a digest ACL credential for Zookeeper.\\nIt combines the username and password, hashes them using SHA-1, encodes the hash in base64, and returns the credential in the format \"username:base64_hash\".",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "19fc5847a7c5",
        "ground_truth": "def make_digest_acl_credential(username, password):\n    \"\"\"Create a SHA1 digest credential.\n     .. note::\n         This function uses UTF-8 to encode non-ASCII codepoints,\n        whereas ZooKeeper uses the \"default locale\" for decoding.  It\n        may be a good idea to start the JVM with `-Dfile.encoding=UTF-8`\n        in non-UTF-8 locales.\n        See: https://github.com/python-zk/kazoo/pull/584\n     \"\"\"\n    credential = username.encode(\"utf-8\") + b\":\" + password.encode(\"utf-8\")\n    cred_hash = b64encode(hashlib.sha1(credential).digest()).strip()\n    return username + \":\" + cred_hash.decode(\"utf-8\")",
        "import_statements": [
            "from base64 import b64encode",
            "from collections import namedtuple",
            "import hashlib"
        ],
        "reference_api": [
            "digest",
            "hashlib.sha1",
            "username.encode",
            "b64encode",
            "password.encode",
            "strip",
            "cred_hash.decode"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "username.encode",
            "password.encode",
            "strip",
            "b64encode",
            "digest",
            "cred_hash.decode"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "python-zk/kazoo",
        "function_declaration": "def holds_lock(self)",
        "start_line": "240",
        "end_line": "252",
        "file_path": "kazoo/recipe/queue.py",
        "docstring": "The holds_lock function checks if the current instance holds a lock.\\nIf there is no processing element, it returns False.\\nOtherwise, it constructs the lock path and synchronizes with the Zookeeper client.\\nIt retrieves the lock value and compares it with the instance ID, returning True if they match, otherwise False.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4a062391deb0",
        "ground_truth": "def holds_lock(self):\n    \"\"\"Checks if a node still holds the lock.\n    :returns: True if a node still holds the lock, False otherwise.\n    :rtype: bool\n    \"\"\"\n    if self.processing_element is None:\n        return False\n    lock_id, _ = self.processing_element\n    lock_path = \"{path}/{id}\".format(path=self._lock_path, id=lock_id)\n    self.client.sync(lock_path)\n    value, stat = self.client.retry(self.client.get, lock_path)\n    return value == self.id",
        "import_statements": [
            "import uuid",
            "from kazoo.exceptions import NoNodeError, NodeExistsError",
            "from kazoo.protocol.states import EventType",
            "from kazoo.retry import ForceRetryError"
        ],
        "reference_api": [
            "retry",
            "sync",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "sync",
            "retry"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "python-zk/kazoo",
        "function_declaration": "def consume(self)",
        "start_line": "254",
        "end_line": "272",
        "file_path": "kazoo/recipe/queue.py",
        "docstring": "The consume function processes and deletes a currently held lock and its associated entry if the processing element is not None and the lock is held.\\nIt performs the deletions within a transaction and resets the processing element to None.\\nIf successful, it returns True; otherwise, it returns False.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "aaac7523d2b5",
        "ground_truth": "def consume(self):\n    \"\"\"Removes a currently processing entry from the queue.\n    :returns: True if element was removed successfully, False otherwise.\n    :rtype: bool\n    \"\"\"\n    if self.processing_element is not None and self.holds_lock():\n        id_, value = self.processing_element\n        with self.client.transaction() as transaction:\n            transaction.delete(\n                \"{path}/{id}\".format(path=self._entries_path, id=id_)\n            )\n            transaction.delete(\n                \"{path}/{id}\".format(path=self._lock_path, id=id_)\n            )\n        self.processing_element = None\n        return True\n    else:\n        return False",
        "import_statements": [
            "import uuid",
            "from kazoo.exceptions import NoNodeError, NodeExistsError",
            "from kazoo.protocol.states import EventType",
            "from kazoo.retry import ForceRetryError"
        ],
        "reference_api": [
            "self.holds_lock",
            "transaction.delete",
            "format",
            "transaction"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self.holds_lock",
                "code": "def holds_lock(self):\n        \"\"\"Checks if a node still holds the lock.\n\n        :returns: True if a node still holds the lock, False otherwise.\n        :rtype: bool\n        \"\"\"\n        if self.processing_element is None:\n            return False\n        lock_id, _ = self.processing_element\n        lock_path = \"{path}/{id}\".format(path=self._lock_path, id=lock_id)\n        self.client.sync(lock_path)\n        value, stat = self.client.retry(self.client.get, lock_path)\n        return value == self.id"
            }
        ],
        "third_party": [
            "transaction",
            "transaction.delete",
            "transaction.delete"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "python-zk/kazoo",
        "function_declaration": "def _read_socket(self, read_timeout)",
        "start_line": "463",
        "end_line": "485",
        "file_path": "kazoo/protocol/connection.py",
        "docstring": "The _read_socket function processes incoming data on a socket with a specified read timeout.\\nIt reads the header and buffer from the socket and handles different types of messages based on the header's xid.\\nFor PING_XID, it logs a received ping and clears the outstanding ping flag.\\nFor AUTH_XID, it handles authentication responses, setting an exception if authentication fails or confirming success.\\nFor WATCH_XID, it processes watch events.\\nFor other xids, it logs the header and reads the corresponding response.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bed0c998ac4c",
        "ground_truth": "def _read_socket(self, read_timeout):\n    \"\"\"Called when there's something to read on the socket\"\"\"\n    client = self.client\n    header, buffer, offset = self._read_header(read_timeout)\n    if header.xid == PING_XID:\n        self.logger.log(BLATHER, \"Received Ping\")\n        self.ping_outstanding.clear()\n    elif header.xid == AUTH_XID:\n        self.logger.log(BLATHER, \"Received AUTH\")\n        request, async_object, xid = client._pending.popleft()\n        if header.err:\n            async_object.set_exception(AuthFailedError())\n            client._session_callback(KeeperState.AUTH_FAILED)\n        else:\n            async_object.set(True)\n    elif header.xid == WATCH_XID:\n        self._read_watch_event(buffer, offset)\n    else:\n        self.logger.log(BLATHER, \"Reading for header %r\", header)\n        return self._read_response(header, buffer, offset)",
        "import_statements": [
            "from binascii import hexlify",
            "from contextlib import contextmanager",
            "import copy",
            "import logging",
            "import random",
            "import select",
            "import socket",
            "import ssl",
            "import sys",
            "import time",
            "from kazoo.exceptions import (\n    AuthFailedError,\n    ConnectionDropped,\n    EXCEPTIONS,\n    SessionExpiredError,\n    NoNodeError,\n    SASLException,\n)",
            "from kazoo.loggingsupport import BLATHER",
            "from kazoo.protocol.serialization import (\n    Auth,\n    Close,\n    Connect,\n    Exists,\n    GetChildren,\n    GetChildren2,\n    Ping,\n    PingInstance,\n    ReplyHeader,\n    SASL,\n    Transaction,\n    Watch,\n    int_struct,\n)",
            "from kazoo.protocol.states import (\n    Callback,\n    KeeperState,\n    WatchedEvent,\n    EVENT_TYPE_MAP,\n)",
            "from kazoo.retry import (\n    ForceRetryError,\n    RetryFailedError,\n)"
        ],
        "reference_api": [
            "self._read_header",
            "async_object.set_exception",
            "self._read_watch_event",
            "log",
            "self._read_response",
            "popleft",
            "clear",
            "AuthFailedError",
            "client._session_callback",
            "async_object.set"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._read_header",
                "code": "def _read_header(self, timeout):\n        b = self._read(4, timeout)\n        length = int_struct.unpack(b)[0]\n        b = self._read(length, timeout)\n        header, offset = ReplyHeader.deserialize(b, 0)\n        return header, b, offset"
            },
            {
                "name": "self._read_watch_event",
                "code": "def _read_watch_event(self, buffer, offset):\n        client = self.client\n        watch, offset = Watch.deserialize(buffer, offset)\n        path = watch.path\n\n        self.logger.debug(\"Received EVENT: %s\", watch)\n\n        watchers = []\n\n        if watch.type in (CREATED_EVENT, CHANGED_EVENT):\n            watchers.extend(client._data_watchers.pop(path, []))\n        elif watch.type == DELETED_EVENT:\n            watchers.extend(client._data_watchers.pop(path, []))\n            watchers.extend(client._child_watchers.pop(path, []))\n        elif watch.type == CHILD_EVENT:\n            watchers.extend(client._child_watchers.pop(path, []))\n        else:\n            self.logger.warn(\"Received unknown event %r\", watch.type)\n            return\n\n        # Strip the chroot if needed\n        path = client.unchroot(path)\n        ev = WatchedEvent(EVENT_TYPE_MAP[watch.type], client._state, path)\n\n        # Last check to ignore watches if we've been stopped\n        if client._stopped.is_set():\n            return\n\n        # Dump the watchers to the watch thread\n        for watch in watchers:\n            client.handler.dispatch_callback(Callback(\"watch\", watch, (ev,)))"
            },
            {
                "name": "self._read_response",
                "code": "def _read_response(self, header, buffer, offset):\n        client = self.client\n        request, async_object, xid = client._pending.popleft()\n        if header.zxid and header.zxid > 0:\n            client.last_zxid = header.zxid\n        if header.xid != xid:\n            exc = RuntimeError(\n                \"xids do not match, expected %r \" \"received %r\",\n                xid,\n                header.xid,\n            )\n            async_object.set_exception(exc)\n            raise exc\n\n        # Determine if its an exists request and a no node error\n        exists_error = (\n            header.err == NoNodeError.code and request.type == Exists.type\n        )\n\n        # Set the exception if its not an exists error\n        if header.err and not exists_error:\n            callback_exception = EXCEPTIONS[header.err]()\n            self.logger.debug(\n                \"Received error(xid=%s) %r\", xid, callback_exception\n            )\n            if async_object:\n                async_object.set_exception(callback_exception)\n        elif request and async_object:\n            if exists_error:\n                # It's a NoNodeError, which is fine for an exists\n                # request\n                async_object.set(None)\n            else:\n                try:\n                    response = request.deserialize(buffer, offset)\n                except Exception as exc:\n                    self.logger.exception(\n                        \"Exception raised during deserialization \"\n                        \"of request: %s\",\n                        request,\n                    )\n                    async_object.set_exception(exc)\n                    return\n                self.logger.debug(\n                    \"Received response(xid=%s): %r\", xid, response\n                )\n\n                # We special case a Transaction as we have to unchroot things\n                if request.type == Transaction.type:\n                    response = Transaction.unchroot(client, response)\n\n                async_object.set(response)\n\n            # Determine if watchers should be registered\n            watcher = getattr(request, \"watcher\", None)\n            if not client._stopped.is_set() and watcher:\n                if isinstance(request, (GetChildren, GetChildren2)):\n                    client._child_watchers[request.path].add(watcher)\n                else:\n                    client._data_watchers[request.path].add(watcher)\n\n        if isinstance(request, Close):\n            self.logger.log(BLATHER, \"Read close response\")\n            return CLOSE_RESPONSE"
            }
        ],
        "third_party": [
            "log",
            "clear",
            "log",
            "popleft",
            "async_object.set_exception",
            "AuthFailedError",
            "client._session_callback",
            "async_object.set",
            "log"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "python-zk/kazoo",
        "function_declaration": "def zk_loop(self)",
        "start_line": "534",
        "end_line": "554",
        "file_path": "kazoo/protocol/connection.py",
        "docstring": "The zk_loop function is the main handling loop for Zookeeper connections.\\nIt logs the start of the loop and clears the connection_stopped event.\\nThe function attempts to connect to Zookeeper using a retry policy until the client is stopped or the retry attempts fail.\\nIf connection attempts fail, it logs a warning message.\\nFinally, it sets the connection_stopped event, triggers the session callback with a closed state, and logs the stop of the connection.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b10e2c2c410b",
        "ground_truth": "def zk_loop(self):\n    \"\"\"Main Zookeeper handling loop\"\"\"\n    self.logger.log(BLATHER, \"ZK loop started\")\n    self.connection_stopped.clear()\n    retry = self.retry_sleeper.copy()\n    try:\n        while not self.client._stopped.is_set():\n            # If the connect_loop returns STOP_CONNECTING, stop retrying\n            if retry(self._connect_loop, retry) is STOP_CONNECTING:\n                break\n    except RetryFailedError:\n        self.logger.warning(\n            \"Failed connecting to Zookeeper \"\n            \"within the connection retry policy.\"\n        )\n    finally:\n        self.connection_stopped.set()\n        self.client._session_callback(KeeperState.CLOSED)\n        self.logger.log(BLATHER, \"Connection stopped\")",
        "import_statements": [
            "from binascii import hexlify",
            "from contextlib import contextmanager",
            "import copy",
            "import logging",
            "import random",
            "import select",
            "import socket",
            "import ssl",
            "import sys",
            "import time",
            "from kazoo.exceptions import (\n    AuthFailedError,\n    ConnectionDropped,\n    EXCEPTIONS,\n    SessionExpiredError,\n    NoNodeError,\n    SASLException,\n)",
            "from kazoo.loggingsupport import BLATHER",
            "from kazoo.protocol.serialization import (\n    Auth,\n    Close,\n    Connect,\n    Exists,\n    GetChildren,\n    GetChildren2,\n    Ping,\n    PingInstance,\n    ReplyHeader,\n    SASL,\n    Transaction,\n    Watch,\n    int_struct,\n)",
            "from kazoo.protocol.states import (\n    Callback,\n    KeeperState,\n    WatchedEvent,\n    EVENT_TYPE_MAP,\n)",
            "from kazoo.retry import (\n    ForceRetryError,\n    RetryFailedError,\n)"
        ],
        "reference_api": [
            "_session_callback",
            "retry",
            "log",
            "set",
            "warning",
            "clear",
            "is_set",
            "copy"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "log",
            "clear",
            "is_set",
            "retry",
            "warning",
            "_session_callback",
            "log"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "python-zk/kazoo",
        "function_declaration": "def _partitioner(self, identifier, members, partitions)",
        "start_line": "430",
        "end_line": "438",
        "file_path": "kazoo/recipe/partitioner.py",
        "docstring": "The _partitioner function assigns partitions to workers based on a given identifier.\\nIt sorts the list of partitions and the list of members.\\nThe function then finds the index of the identifier in the sorted members list and returns every nth partition starting from that index, where n is the number of workers.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "42b3d26a3540",
        "ground_truth": "def _partitioner(self, identifier, members, partitions):\n    # Ensure consistent order of partitions/members\n    all_partitions = sorted(partitions)\n    workers = sorted(members)\n    i = workers.index(identifier)\n    # Now return the partition list starting at our location and\n    # skipping the other workers\n    return all_partitions[i :: len(workers)]",
        "import_statements": [
            "from functools import partial",
            "import logging",
            "import os",
            "import socket",
            "from kazoo.exceptions import KazooException, LockTimeout",
            "from kazoo.protocol.states import KazooState",
            "from kazoo.recipe.watchers import PatientChildrenWatch"
        ],
        "reference_api": [
            "sorted",
            "len",
            "workers.index"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "workers.index"
        ]
    },
    {
        "subclass": "ZooKeeper",
        "owner/repo": "python-zk/kazoo",
        "function_declaration": "def leave(self)",
        "start_line": "162",
        "end_line": "169",
        "file_path": "kazoo/recipe/barrier.py",
        "docstring": "The leave function allows a node to exit a barrier, blocking until all nodes have left.\\nIt attempts to execute the leave operation with retries, and in case of an exception, it performs a best-effort cleanup.\\nAfter the operation, it sets the participating flag to False.",
        "language": "Python",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "738974d4e9c0",
        "ground_truth": "def leave(self):\n    \"\"\"Leave the barrier, blocks until all nodes have left\"\"\"\n    try:\n        self.client.retry(self._inner_leave)\n    except KazooException:  # pragma: nocover\n        # Failed to cleanly leave\n        self._best_effort_cleanup()\n    self.participating = False",
        "import_statements": [
            "import os",
            "import socket",
            "import uuid",
            "from kazoo.exceptions import KazooException, NoNodeError, NodeExistsError",
            "from kazoo.protocol.states import EventType"
        ],
        "reference_api": [
            "self._best_effort_cleanup",
            "retry"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "self._best_effort_cleanup",
                "code": "def _best_effort_cleanup(self):\n        try:\n            self.client.retry(self.client.delete, self.create_path)\n        except NoNodeError:\n            pass"
            }
        ],
        "third_party": [
            "retry"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public Result start()",
        "start_line": "44",
        "end_line": "55",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/controller/FtpManagerController.java",
        "docstring": "The start function attempts to start the HDFS over FTP server.\\nIt first checks the server status, and if it is not running, it starts the server.\\nThe function returns a success result if the server starts successfully, and an error result if an exception occurs during the process.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a321fef7d100",
        "ground_truth": "public Result start() {\n    try {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if(!status) {\n            hdfsOverFtpServer.startServer();\n        }\n        return new Result(true, \"FTP \u670d\u52a1\u542f\u52a8\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"FTP \u670d\u52a1\u542f\u52a8\u5931\u8d25\");\n    }\n}",
        "import_statements": [
            "import io.swagger.annotations.Api;",
            "import io.swagger.annotations.ApiOperation;",
            "import org.apache.hadoop.seaweed.ftp.service.HFtpService;",
            "import org.apache.hadoop.seaweed.ftp.controller.vo.Result;",
            "import org.apache.log4j.Logger;",
            "import org.springframework.beans.factory.annotation.Autowired;",
            "import org.springframework.web.bind.annotation.GetMapping;",
            "import org.springframework.web.bind.annotation.PutMapping;",
            "import org.springframework.web.bind.annotation.RequestMapping;",
            "import org.springframework.web.bind.annotation.RestController;",
            "import java.util.HashMap;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "startServer",
            "statusServer",
            "error"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "startServer",
            "statusServer",
            "error"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public Result stop()",
        "start_line": "59",
        "end_line": "69",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/controller/FtpManagerController.java",
        "docstring": "The stop function attempts to stop the FTP server.\\nIt first checks if the server is currently running.\\nIf the server is running, it stops the server and returns a successful result message.\\nIf an exception occurs, it logs the error and returns a failure result message.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "24b088689114",
        "ground_truth": "public Result stop() {\n    try {\n        boolean status = hdfsOverFtpServer.statusServer();\n        if(status) {\n            hdfsOverFtpServer.stopServer();\n        }\n        return new Result(true, \"FTP \u670d\u52a1\u505c\u6b62\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"FTP \u670d\u52a1\u505c\u6b62\u5931\u8d25\");\n    }",
        "import_statements": [
            "import io.swagger.annotations.Api;",
            "import io.swagger.annotations.ApiOperation;",
            "import org.apache.hadoop.seaweed.ftp.service.HFtpService;",
            "import org.apache.hadoop.seaweed.ftp.controller.vo.Result;",
            "import org.apache.log4j.Logger;",
            "import org.springframework.beans.factory.annotation.Autowired;",
            "import org.springframework.web.bind.annotation.GetMapping;",
            "import org.springframework.web.bind.annotation.PutMapping;",
            "import org.springframework.web.bind.annotation.RequestMapping;",
            "import org.springframework.web.bind.annotation.RestController;",
            "import java.util.HashMap;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "stopServer",
            "statusServer",
            "error"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "stopServer",
            "statusServer",
            "error"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public AuthorizationRequest authorize(AuthorizationRequest request)",
        "start_line": "195",
        "end_line": "223",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/users/HdfsUser.java",
        "docstring": "The authorize function processes an AuthorizationRequest by checking it against a list of authorities.\\nIf no authorities are available, it returns null.\\nIt iterates through each authority to see if they can authorize the request.\\nIf an authority can authorize the request, it updates the request.\\nIf authorization fails at any point, it returns null.\\nIf at least one authority successfully processes the request, it returns the updated request; otherwise, it returns null.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "af0884ffda66",
        "ground_truth": "public AuthorizationRequest authorize(AuthorizationRequest request) {\n List<Authority> authorities = getAuthorities();\n // check for no authorities at all\n if (authorities == null) {\n  return null;\n }\n boolean someoneCouldAuthorize = false;\n for (Authority authority : authorities) {\n  if (authority.canAuthorize(request)) {\n   someoneCouldAuthorize = true;\n   request = authority.authorize(request);\n   // authorization failed, return null\n   if (request == null) {\n    return null;\n   }\n  }\n }\n if (someoneCouldAuthorize) {\n  return request;\n } else {\n  return null;\n }\n}",
        "import_statements": [
            "import org.apache.ftpserver.ftplet.Authority;",
            "import org.apache.ftpserver.ftplet.AuthorizationRequest;",
            "import org.apache.ftpserver.ftplet.User;",
            "import org.apache.log4j.Logger;",
            "import java.io.Serializable;",
            "import java.util.ArrayList;",
            "import java.util.Collections;",
            "import java.util.List;"
        ],
        "reference_api": [
            "getAuthorities",
            "canAuthorize",
            "authorize"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getAuthorities",
                "code": "public List<Authority> getAuthorities() {\n\t\tif (authorities != null) {\n\t\t\treturn Collections.unmodifiableList(authorities);\n\t\t} else {\n\t\t\treturn null;\n\t\t}\n\t}"
            },
            {
                "name": "authorize",
                "code": "public AuthorizationRequest authorize(AuthorizationRequest request) {\n\t\tList<Authority> authorities = getAuthorities();\n\n\t\t// check for no authorities at all\n\t\tif (authorities == null) {\n\t\t\treturn null;\n\t\t}\n\n\t\tboolean someoneCouldAuthorize = false;\n\t\tfor (Authority authority : authorities) {\n\t\t\tif (authority.canAuthorize(request)) {\n\t\t\t\tsomeoneCouldAuthorize = true;\n\n\t\t\t\trequest = authority.authorize(request);\n\n\t\t\t\t// authorization failed, return null\n\t\t\t\tif (request == null) {\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\n\t\t}\n\n\t\tif (someoneCouldAuthorize) {\n\t\t\treturn request;\n\t\t} else {\n\t\t\treturn null;\n\t\t}\n\t}"
            }
        ],
        "third_party": [
            "canAuthorize"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public boolean mkdir()",
        "start_line": "228",
        "end_line": "238",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/service/filesystem/HdfsFileObject.java",
        "docstring": "The mkdir function attempts to create a directory in the HDFS file system at the specified path.\\nIt retrieves the file system instance and calls the mkdirs method on the fullPath.\\nIf successful, it returns true.\\nIf an IOException occurs, it prints the stack trace and returns false.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "920d28a0fe76",
        "ground_truth": "public boolean mkdir() {\n try {\n  FileSystem fs = HdfsOverFtpSystem.getDfs();\n  fs.mkdirs(fullPath);\n//\t\t\tfs.setOwner(path, user.getName(), user.getMainGroup());\n  return true;\n } catch (IOException e) {\n  e.printStackTrace();\n  return false;\n }\n}",
        "import_statements": [
            "import org.apache.ftpserver.ftplet.FtpFile;",
            "import org.apache.ftpserver.ftplet.User;",
            "import org.apache.hadoop.fs.*;",
            "import org.apache.hadoop.seaweed.ftp.users.HdfsUser;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.File;",
            "import java.io.IOException;",
            "import java.io.InputStream;",
            "import java.io.OutputStream;",
            "import java.util.Arrays;",
            "import java.util.Collections;",
            "import java.util.List;"
        ],
        "reference_api": [
            "getDfs",
            "mkdirs",
            "printStackTrace"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getDfs",
            "mkdirs",
            "printStackTrace"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public boolean delete()",
        "start_line": "245",
        "end_line": "254",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/service/filesystem/HdfsFileObject.java",
        "docstring": "The delete function attempts to delete a directory or file in the HDFS file system at the specified path.\\nIt retrieves the file system instance and calls the delete method on the fullPath with a recursive flag set to true.\\nIf successful, it returns true.\\nIf an IOException occurs, it prints the stack trace and returns false.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a2b04232d387",
        "ground_truth": "public boolean delete() {\n try {\n  FileSystem dfs = HdfsOverFtpSystem.getDfs();\n  dfs.delete(fullPath, true);\n  return true;\n } catch (IOException e) {\n  e.printStackTrace();\n  return false;\n }\n}",
        "import_statements": [
            "import org.apache.ftpserver.ftplet.FtpFile;",
            "import org.apache.ftpserver.ftplet.User;",
            "import org.apache.hadoop.fs.*;",
            "import org.apache.hadoop.seaweed.ftp.users.HdfsUser;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.File;",
            "import java.io.IOException;",
            "import java.io.InputStream;",
            "import java.io.OutputStream;",
            "import java.util.Arrays;",
            "import java.util.Collections;",
            "import java.util.List;"
        ],
        "reference_api": [
            "getDfs",
            "delete",
            "printStackTrace"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "delete",
                "code": "public boolean delete() {\n\t\ttry {\n\t\t\tFileSystem dfs = HdfsOverFtpSystem.getDfs();\n\t\t\tdfs.delete(fullPath, true);\n\t\t\treturn true;\n\t\t} catch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t\treturn false;\n\t\t}\n\t}"
            }
        ],
        "third_party": [
            "getDfs",
            "printStackTrace"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public boolean move(FtpFile ftpFile)",
        "start_line": "256",
        "end_line": "265",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/service/filesystem/HdfsFileObject.java",
        "docstring": "The move function attempts to rename and move an FTP file within the HDFS file system.\\nIt retrieves the file system instance and renames the file to the new path based on the parent directory and the FTP file's name.\\nIf successful, it returns true.\\nIf an IOException occurs, it prints the stack trace and returns false.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3407087c9af6",
        "ground_truth": "public boolean move(FtpFile ftpFile) {\n try {\n  FileSystem dfs = HdfsOverFtpSystem.getDfs();\n  dfs.rename(fullPath, new Path(fullPath.getParent() + File.separator + ftpFile.getName()));\n  return true;\n } catch (IOException e) {\n  e.printStackTrace();\n  return false;\n }\n}",
        "import_statements": [
            "import org.apache.ftpserver.ftplet.FtpFile;",
            "import org.apache.ftpserver.ftplet.User;",
            "import org.apache.hadoop.fs.*;",
            "import org.apache.hadoop.seaweed.ftp.users.HdfsUser;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.File;",
            "import java.io.IOException;",
            "import java.io.InputStream;",
            "import java.io.OutputStream;",
            "import java.util.Arrays;",
            "import java.util.Collections;",
            "import java.util.List;"
        ],
        "reference_api": [
            "getName",
            "getDfs",
            "rename",
            "getParent",
            "printStackTrace"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getName",
                "code": "public String getName() {\n\t\treturn path.getName();\n\t}"
            }
        ],
        "third_party": [
            "getDfs",
            "rename",
            "getParent",
            "printStackTrace"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public List<FtpFile> listFiles()",
        "start_line": "273",
        "end_line": "295",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/service/filesystem/HdfsFileObject.java",
        "docstring": "The listFiles function retrieves and lists files from an HDFS directory at the specified path.\\nIt gets the HDFS file system instance and lists the status of files in the directory.\\nIt constructs virtual file names based on the base directory's virtual path and creates an array of FtpFile objects.\\nIt returns an unmodifiable list of these FtpFile objects.\\nIf an IOException occurs, it logs the error and returns null.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "91227000a139",
        "ground_truth": "public List<FtpFile> listFiles() {\n try {\n  FileSystem dfs = HdfsOverFtpSystem.getDfs();\n  FileStatus fileStats[] = dfs.listStatus(fullPath);\n  // get the virtual name of the base directory\n  String virtualFileStr = getAbsolutePath();\n  if (virtualFileStr.charAt(virtualFileStr.length() - 1) != '/') {\n   virtualFileStr += '/';\n  }\n  FtpFile[] virtualFiles = new FtpFile[fileStats.length];\n  for (int i = 0; i < fileStats.length; i++) {\n   File fileObj = new File(fileStats[i].getPath().toString());\n   String fileName = virtualFileStr + fileObj.getName();\n   virtualFiles[i] = new HdfsFileObject(homePath.toString(), fileName, user);\n  }\n  return Collections.unmodifiableList(Arrays.asList(virtualFiles));\n } catch (IOException e) {\n  log.debug(\"\", e);\n  return null;\n }\n}",
        "import_statements": [
            "import org.apache.ftpserver.ftplet.FtpFile;",
            "import org.apache.ftpserver.ftplet.User;",
            "import org.apache.hadoop.fs.*;",
            "import org.apache.hadoop.seaweed.ftp.users.HdfsUser;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.File;",
            "import java.io.IOException;",
            "import java.io.InputStream;",
            "import java.io.OutputStream;",
            "import java.util.Arrays;",
            "import java.util.Collections;",
            "import java.util.List;"
        ],
        "reference_api": [
            "getPath",
            "getDfs",
            "getName",
            "getAbsolutePath",
            "toString",
            "unmodifiableList",
            "debug",
            "asList",
            "charAt",
            "listStatus",
            "length"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getName",
                "code": "public String getName() {\n\t\treturn path.getName();\n\t}"
            },
            {
                "name": "getAbsolutePath",
                "code": "public String getAbsolutePath() {\n\t\t// strip the last '/' if necessary\n\t\tString fullName = path.toString();\n\t\tint filelen = fullName.length();\n\t\tif ((filelen != 1) && (fullName.charAt(filelen - 1) == '/')) {\n\t\t\tfullName = fullName.substring(0, filelen - 1);\n\t\t}\n\n\t\treturn fullName;\n\t}"
            }
        ],
        "third_party": [
            "getPath",
            "getDfs",
            "unmodifiableList",
            "debug",
            "asList",
            "listStatus"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public void configure()",
        "start_line": "93",
        "end_line": "114",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/users/HdfsUserManager.java",
        "docstring": "The configure function sets the configuration state to true and initializes user data properties.\\nIt checks if the user data file exists and loads its properties.\\nIf an IOException occurs, it throws an FtpServerConfigurationException with the file path and error details.\\nFinally, it converts any deprecated property names.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "aec4623134b5",
        "ground_truth": "public void configure() {\n isConfigured = true;\n try {\n  userDataProp = new BaseProperties();\n  if (userDataFile != null && userDataFile.exists()) {\n   FileInputStream fis = null;\n   try {\n    fis = new FileInputStream(userDataFile);\n    userDataProp.load(fis);\n   } finally {\n    IoUtils.close(fis);\n   }\n  }\n } catch (IOException e) {\n  throw new FtpServerConfigurationException(\n    \"Error loading user data file : \"\n      + userDataFile.getAbsolutePath(), e);\n }\n convertDeprecatedPropertyNames();\n}",
        "import_statements": [
            "import org.apache.ftpserver.FtpServerConfigurationException;",
            "import org.apache.ftpserver.ftplet.*;",
            "import org.apache.ftpserver.usermanager.*;",
            "import org.apache.ftpserver.usermanager.impl.*;",
            "import org.apache.ftpserver.util.BaseProperties;",
            "import org.apache.ftpserver.util.IoUtils;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.File;",
            "import java.io.FileInputStream;",
            "import java.io.FileOutputStream;",
            "import java.io.IOException;",
            "import java.util.*;"
        ],
        "reference_api": [
            "exists",
            "getAbsolutePath",
            "convertDeprecatedPropertyNames",
            "load",
            "close"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "convertDeprecatedPropertyNames",
                "code": "private void convertDeprecatedPropertyNames() {\n\t\tEnumeration<?> keys = userDataProp.propertyNames();\n\n\t\tboolean doSave = false;\n\n\t\twhile (keys.hasMoreElements()) {\n\t\t\tString key = (String) keys.nextElement();\n\n\t\t\tif (key.startsWith(DEPRECATED_PREFIX)) {\n\t\t\t\tString newKey = PREFIX\n\t\t\t\t\t\t+ key.substring(DEPRECATED_PREFIX.length());\n\t\t\t\tuserDataProp.setProperty(newKey, userDataProp.getProperty(key));\n\t\t\t\tuserDataProp.remove(key);\n\n\t\t\t\tdoSave = true;\n\t\t\t}\n\t\t}\n\n\t\tif (doSave) {\n\t\t\ttry {\n\t\t\t\tsaveUserData();\n\t\t\t} catch (FtpException e) {\n\t\t\t\tthrow new FtpServerConfigurationException(\n\t\t\t\t\t\t\"Failed to save updated user data\", e);\n\t\t\t}\n\t\t}\n\t}"
            }
        ],
        "third_party": [
            "exists",
            "getAbsolutePath",
            "load",
            "close"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "private void saveUserData() throws FtpException",
        "start_line": "212",
        "end_line": "231",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/users/HdfsUserManager.java",
        "docstring": "The saveUserData function saves user data to a specified file.\\nIt checks if the parent directory of the user data file exists and creates it if necessary.\\nIf directory creation fails, it throws a configuration exception.\\nIt then attempts to write the user data to the file using a FileOutputStream.\\nIf an IOException occurs during the process, it logs the error and throws an FtpException.\\nFinally, it ensures the FileOutputStream is closed properly.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b5a2587f14d0",
        "ground_truth": "private void saveUserData() throws FtpException {\n File dir = userDataFile.getAbsoluteFile().getParentFile();\n if (dir != null && !dir.exists() && !dir.mkdirs()) {\n  String dirName = dir.getAbsolutePath();\n  throw new FtpServerConfigurationException(\n    \"Cannot create directory for user data file : \" + dirName);\n }\n // save user data\n FileOutputStream fos = null;\n try {\n  fos = new FileOutputStream(userDataFile);\n  userDataProp.store(fos, \"Generated file - don't edit (please)\");\n } catch (IOException ex) {\n  LOG.error(\"Failed saving user data\", ex);\n  throw new FtpException(\"Failed saving user data\", ex);\n } finally {\n  IoUtils.close(fos);\n }\n}",
        "import_statements": [
            "import org.apache.ftpserver.FtpServerConfigurationException;",
            "import org.apache.ftpserver.ftplet.*;",
            "import org.apache.ftpserver.usermanager.*;",
            "import org.apache.ftpserver.usermanager.impl.*;",
            "import org.apache.ftpserver.util.BaseProperties;",
            "import org.apache.ftpserver.util.IoUtils;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.File;",
            "import java.io.FileInputStream;",
            "import java.io.FileOutputStream;",
            "import java.io.IOException;",
            "import java.util.*;"
        ],
        "reference_api": [
            "exists",
            "mkdirs",
            "getAbsolutePath",
            "error",
            "getParentFile",
            "store",
            "close",
            "getAbsoluteFile"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "exists",
            "mkdirs",
            "getAbsolutePath",
            "error",
            "getParentFile",
            "store",
            "close",
            "getAbsoluteFile"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public Result delete(@PathVariable(value = \"user\") String user)",
        "start_line": "58",
        "end_line": "68",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/controller/UserController.java",
        "docstring": "The delete function deletes a user identified by the user parameter.\\nIt creates an instance of HdfsUserManager and sets its file to users.properties located in the current working directory.\\nIt attempts to delete the user and returns a success result message if successful.\\nIf an exception occurs, it logs the error and returns a failure result message.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5cf2989cf513",
        "ground_truth": "public Result delete(@PathVariable(value = \"user\") String user) {\n    try {\n        HdfsUserManager userManagerFactory = new HdfsUserManager();\n        userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n        userManagerFactory.delete(user);\n        return new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n    }catch (Exception e) {\n        log.error(e);\n        return new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n    }\n}",
        "import_statements": [
            "import io.swagger.annotations.Api;",
            "import io.swagger.annotations.ApiOperation;",
            "import org.apache.ftpserver.ftplet.User;",
            "import org.apache.ftpserver.usermanager.Md5PasswordEncryptor;",
            "import org.apache.ftpserver.usermanager.UserFactory;",
            "import org.apache.hadoop.seaweed.ftp.controller.vo.FtpUser;",
            "import org.apache.hadoop.seaweed.ftp.controller.vo.Result;",
            "import org.apache.hadoop.seaweed.ftp.users.HdfsUserManager;",
            "import org.apache.log4j.Logger;",
            "import org.springframework.web.bind.annotation.*;",
            "import java.io.File;"
        ],
        "reference_api": [
            "setFile",
            "delete",
            "getProperty",
            "error"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "delete",
                "code": "@DeleteMapping(\"/delete/{user}\")\n    @ApiOperation(\"\u5220\u9664\u7528\u6237\")\n    public Result delete(@PathVariable(value = \"user\") String user) {\n        try {\n            HdfsUserManager userManagerFactory = new HdfsUserManager();\n            userManagerFactory.setFile(new File(System.getProperty(\"user.dir\") + File.separator + \"users.properties\"));\n            userManagerFactory.delete(user);\n            return new Result(true, \"\u5220\u9664\u7528\u6237\u6210\u529f\");\n        }catch (Exception e) {\n            log.error(e);\n            return new Result(false, \"\u5220\u9664\u7528\u6237\u5931\u8d25\");\n        }\n    }"
            }
        ],
        "third_party": [
            "setFile",
            "getProperty",
            "error"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public FSDataInputStream open(Path path, int bufferSize) throws IOException",
        "start_line": "77",
        "end_line": "91",
        "file_path": "other/java/hdfs2/src/main/java/seaweed/hdfs/SeaweedFileSystem.java",
        "docstring": "The open function opens a file for reading from a specified path with a given buffer size.\\nIt logs the path and buffer size, qualifies the path, and attempts to open the file using the seaweed file system store.\\nIt creates and returns an FSDataInputStream with a buffered input stream.\\nIf an exception occurs, it logs a warning and returns null.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ce383a9a6497",
        "ground_truth": "public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n    LOG.debug(\"open path: {} bufferSize:{}\", path, bufferSize);\n    path = qualify(path);\n    try {\n        int seaweedBufferSize = this.getConf().getInt(FS_SEAWEED_BUFFER_SIZE, FS_SEAWEED_DEFAULT_BUFFER_SIZE);\n        FSInputStream inputStream = seaweedFileSystemStore.openFileForRead(path, statistics);\n        return new FSDataInputStream(new BufferedByteBufferReadableInputStream(inputStream, 4 * seaweedBufferSize));\n    } catch (Exception ex) {\n        LOG.warn(\"open path: {} bufferSize:{}\", path, bufferSize, ex);\n        return null;\n    }\n}",
        "import_statements": [
            "import org.apache.hadoop.conf.Configuration;",
            "import org.apache.hadoop.fs.*;",
            "import org.apache.hadoop.fs.permission.AclEntry;",
            "import org.apache.hadoop.fs.permission.AclStatus;",
            "import org.apache.hadoop.fs.permission.FsPermission;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.hadoop.util.Progressable;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import seaweedfs.client.FilerProto;",
            "import java.io.FileNotFoundException;",
            "import java.io.IOException;",
            "import java.io.OutputStream;",
            "import java.net.URI;",
            "import java.util.EnumSet;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "qualify",
            "getConf",
            "getInt",
            "debug",
            "warn",
            "openFileForRead"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "qualify",
                "code": "Path qualify(Path path) {\n        return path.makeQualified(uri, workingDirectory);\n    }"
            }
        ],
        "third_party": [
            "getConf",
            "getInt",
            "debug",
            "warn",
            "openFileForRead"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public boolean rename(Path src, Path dst) throws IOException",
        "start_line": "156",
        "end_line": "185",
        "file_path": "other/java/hdfs2/src/main/java/seaweed/hdfs/SeaweedFileSystem.java",
        "docstring": "The rename function attempts to rename a source path to a destination path in the file system.\\nIt first logs the rename operation and checks if the source is the root or if the source and destination are the same, returning false or true respectively in these cases.\\nIt looks up the destination entry and adjusts the destination path if the entry exists and is a directory.\\nThe function then qualifies both the source and adjusted destination paths and performs the rename operation using the file system store.\\nFinally, it returns true if the rename is successful.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f0fc12072c02",
        "ground_truth": "public boolean rename(Path src, Path dst) throws IOException {\n    LOG.debug(\"rename path: {} => {}\", src, dst);\n    if (src.isRoot()) {\n        return false;\n    }\n    if (src.equals(dst)) {\n        return true;\n    }\n    FilerProto.Entry entry = seaweedFileSystemStore.lookupEntry(dst);\n    Path adjustedDst = dst;\n    if (entry != null) {\n        FileStatus dstFileStatus = getFileStatus(dst);\n        String sourceFileName = src.getName();\n        if (!dstFileStatus.isDirectory()) {\n            return false;\n        }\n        adjustedDst = new Path(dst, sourceFileName);\n    }\n    Path qualifiedSrcPath = qualify(src);\n    Path qualifiedDstPath = qualify(adjustedDst);\n    seaweedFileSystemStore.rename(qualifiedSrcPath, qualifiedDstPath);\n    return true;\n}",
        "import_statements": [
            "import org.apache.hadoop.conf.Configuration;",
            "import org.apache.hadoop.fs.*;",
            "import org.apache.hadoop.fs.permission.AclEntry;",
            "import org.apache.hadoop.fs.permission.AclStatus;",
            "import org.apache.hadoop.fs.permission.FsPermission;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.hadoop.util.Progressable;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import seaweedfs.client.FilerProto;",
            "import java.io.FileNotFoundException;",
            "import java.io.IOException;",
            "import java.io.OutputStream;",
            "import java.net.URI;",
            "import java.util.EnumSet;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "getName",
            "isRoot",
            "getFileStatus",
            "equals",
            "qualify",
            "rename",
            "debug",
            "isDirectory",
            "lookupEntry"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getFileStatus",
                "code": "@Override\n    public FileStatus getFileStatus(Path path) throws IOException {\n\n        LOG.debug(\"getFileStatus path: {}\", path);\n\n        path = qualify(path);\n\n        return seaweedFileSystemStore.getFileStatus(path);\n    }"
            },
            {
                "name": "qualify",
                "code": "Path qualify(Path path) {\n        return path.makeQualified(uri, workingDirectory);\n    }"
            },
            {
                "name": "rename",
                "code": "@Override\n    public boolean rename(Path src, Path dst) throws IOException {\n\n        LOG.debug(\"rename path: {} => {}\", src, dst);\n\n        if (src.isRoot()) {\n            return false;\n        }\n\n        if (src.equals(dst)) {\n            return true;\n        }\n        FilerProto.Entry entry = seaweedFileSystemStore.lookupEntry(dst);\n\n        Path adjustedDst = dst;\n\n        if (entry != null) {\n            FileStatus dstFileStatus = getFileStatus(dst);\n            String sourceFileName = src.getName();\n            if (!dstFileStatus.isDirectory()) {\n                return false;\n            }\n            adjustedDst = new Path(dst, sourceFileName);\n        }\n\n        Path qualifiedSrcPath = qualify(src);\n        Path qualifiedDstPath = qualify(adjustedDst);\n\n        seaweedFileSystemStore.rename(qualifiedSrcPath, qualifiedDstPath);\n        return true;\n    }"
            }
        ],
        "third_party": [
            "getName",
            "isRoot",
            "debug",
            "isDirectory",
            "lookupEntry"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public boolean hasCapability(String capability)",
        "start_line": "54",
        "end_line": "62",
        "file_path": "other/java/hdfs3/src/main/java/seaweed/hdfs/SeaweedHadoopOutputStream.java",
        "docstring": "The hasCapability function checks if a specific capability is supported.\\nIt converts the capability string to lowercase and checks if it matches HSYNC or HFLUSH.\\nIf it matches, it returns the value of supportFlush.\\nFor any other capability, it returns false.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5104c071af46",
        "ground_truth": "public boolean hasCapability(String capability) {\n    switch (capability.toLowerCase(Locale.ENGLISH)) {\n        case StreamCapabilities.HSYNC:\n        case StreamCapabilities.HFLUSH:\n            return supportFlush;\n        default:\n            return false;\n    }\n}",
        "import_statements": [
            "import org.apache.hadoop.fs.StreamCapabilities;",
            "import org.apache.hadoop.fs.Syncable;",
            "import seaweedfs.client.FilerClient;",
            "import seaweedfs.client.FilerProto;",
            "import seaweedfs.client.SeaweedOutputStream;",
            "import java.io.IOException;",
            "import java.util.Locale;"
        ],
        "reference_api": [
            "toLowerCase"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "toLowerCase"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException",
        "start_line": "117",
        "end_line": "124",
        "file_path": "other/java/client/src/main/java/seaweedfs/client/SeaweedOutputStream.java",
        "docstring": "The flushWrittenBytesToServiceInternal function flushes written bytes to the service starting from a specified offset.\\nIt synchronizes the operation, writes metadata to the filer client, and updates the last flush offset.\\nIf an exception occurs during the metadata write, it throws an IOException.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6989c860c79b",
        "ground_truth": "private synchronized void flushWrittenBytesToServiceInternal(final long offset) throws IOException {\n    try {\n        SeaweedWrite.writeMeta(filerClient, getParentDirectory(path), entry);\n    } catch (Exception ex) {\n        throw new IOException(ex);\n    }\n    this.lastFlushOffset = offset;\n}",
        "import_statements": [
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.io.InterruptedIOException;",
            "import java.io.OutputStream;",
            "import java.nio.Buffer;",
            "import java.nio.ByteBuffer;",
            "import java.util.concurrent.*;"
        ],
        "reference_api": [
            "writeMeta",
            "getParentDirectory"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getParentDirectory",
                "code": "public static String getParentDirectory(String path) {\n        int protoIndex = path.indexOf(\"://\");\n        if (protoIndex >= 0) {\n            int pathStart = path.indexOf(\"/\", protoIndex+3);\n            path = path.substring(pathStart);\n        }\n        if (path.equals(\"/\")) {\n            return path;\n        }\n        int lastSlashIndex = path.lastIndexOf(\"/\");\n        if (lastSlashIndex == 0) {\n            return \"/\";\n        }\n        return path.substring(0, lastSlashIndex);\n    }"
            }
        ],
        "third_party": [
            "writeMeta"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException ",
        "start_line": "222",
        "end_line": "245",
        "file_path": "other/java/client/src/main/java/seaweedfs/client/SeaweedOutputStream.java",
        "docstring": "The submitWriteBufferToService function writes a ByteBuffer to a service at a specified position.\\nIt flips the buffer to prepare for reading, calculates the byte length, and ensures the task queue is not overloaded.\\nA write task is submitted to the completion service, which writes the data and releases the buffer.\\nThe write operation is recorded and the operation queue is managed.\\nThe function returns the number of bytes written.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "56e8fb9de5da",
        "ground_truth": "private synchronized int submitWriteBufferToService(final ByteBuffer bufferToWrite, final long writePosition) throws IOException {\n    ((Buffer)bufferToWrite).flip();\n    int bytesLength = bufferToWrite.limit() - bufferToWrite.position();\n    if (threadExecutor.getQueue().size() >= maxConcurrentRequestCount) {\n        waitForTaskToComplete();\n    }\n    final Future<Void> job = completionService.submit(() -> {\n        // System.out.println(path + \" is going to save [\" + (writePosition) + \",\" + ((writePosition) + bytesLength) + \")\");\n        SeaweedWrite.writeData(entry, replication, collection, filerClient, writePosition, bufferToWrite.array(), bufferToWrite.position(), bufferToWrite.limit(), path);\n        // System.out.println(path + \" saved [\" + (writePosition) + \",\" + ((writePosition) + bytesLength) + \")\");\n        ByteBufferPool.release(bufferToWrite);\n        return null;\n    });\n    writeOperations.add(new WriteOperation(job, writePosition, bytesLength));\n    // Try to shrink the queue\n    shrinkWriteOperationQueue();\n    return bytesLength;\n}",
        "import_statements": [
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.io.InterruptedIOException;",
            "import java.io.OutputStream;",
            "import java.nio.Buffer;",
            "import java.nio.ByteBuffer;",
            "import java.util.concurrent.*;"
        ],
        "reference_api": [
            "limit",
            "submit",
            "position",
            "shrinkWriteOperationQueue",
            "waitForTaskToComplete",
            "array",
            "writeData",
            "size",
            "add",
            "getQueue",
            "flip",
            "release"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "shrinkWriteOperationQueue",
                "code": "private synchronized void shrinkWriteOperationQueue() throws IOException {\n        try {\n            while (writeOperations.peek() != null && writeOperations.peek().task.isDone()) {\n                writeOperations.peek().task.get();\n                lastTotalAppendOffset += writeOperations.peek().length;\n                writeOperations.remove();\n            }\n        } catch (Exception e) {\n            lastError = new IOException(e);\n            throw lastError;\n        }\n    }"
            },
            {
                "name": "waitForTaskToComplete",
                "code": "private void waitForTaskToComplete() throws IOException {\n        boolean completed;\n        for (completed = false; completionService.poll() != null; completed = true) {\n            // keep polling until there is no data\n        }\n\n        if (!completed) {\n            try {\n                completionService.take();\n            } catch (InterruptedException e) {\n                lastError = (IOException) new InterruptedIOException(e.toString()).initCause(e);\n                throw lastError;\n            }\n        }\n    }"
            }
        ],
        "third_party": [
            "limit",
            "submit",
            "position",
            "array",
            "writeData",
            "size",
            "add",
            "getQueue",
            "flip",
            "release"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "private void waitForTaskToComplete() throws IOException",
        "start_line": "247",
        "end_line": "261",
        "file_path": "other/java/client/src/main/java/seaweedfs/client/SeaweedOutputStream.java",
        "docstring": "The waitForTaskToComplete function ensures that a task in the completion service has completed before proceeding.\\nIt continuously polls the completion service until no data is returned, indicating completion.\\nIf no task was completed, it waits for the next task to finish.\\nIf interrupted, it throws an IOException with the interruption details.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "bf27d955c38b",
        "ground_truth": "private void waitForTaskToComplete() throws IOException {\n    boolean completed;\n    for (completed = false; completionService.poll() != null; completed = true) {\n        // keep polling until there is no data\n    }\n    if (!completed) {\n        try {\n            completionService.take();\n        } catch (InterruptedException e) {\n            lastError = (IOException) new InterruptedIOException(e.toString()).initCause(e);\n            throw lastError;\n        }\n    }\n}",
        "import_statements": [
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.io.InterruptedIOException;",
            "import java.io.OutputStream;",
            "import java.nio.Buffer;",
            "import java.nio.ByteBuffer;",
            "import java.util.concurrent.*;"
        ],
        "reference_api": [
            "take",
            "initCause",
            "poll",
            "toString"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "take",
            "initCause",
            "poll"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "private synchronized void shrinkWriteOperationQueue() throws IOException",
        "start_line": "273",
        "end_line": "284",
        "file_path": "other/java/client/src/main/java/seaweedfs/client/SeaweedOutputStream.java",
        "docstring": "The shrinkWriteOperationQueue function processes and removes completed write operations from the queue.\\nIt iterates through the queue, checking if tasks are done, and updates the total append offset with the length of each completed operation.\\nIf an exception occurs, it captures the exception as an IOException and rethrows it.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5c7a311273ae",
        "ground_truth": "private synchronized void shrinkWriteOperationQueue() throws IOException {\n    try {\n        while (writeOperations.peek() != null && writeOperations.peek().task.isDone()) {\n            writeOperations.peek().task.get();\n            lastTotalAppendOffset += writeOperations.peek().length;\n            writeOperations.remove();\n        }\n    } catch (Exception e) {\n        lastError = new IOException(e);\n        throw lastError;\n    }\n}",
        "import_statements": [
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.io.InterruptedIOException;",
            "import java.io.OutputStream;",
            "import java.nio.Buffer;",
            "import java.nio.ByteBuffer;",
            "import java.util.concurrent.*;"
        ],
        "reference_api": [
            "isDone",
            "get",
            "remove",
            "peek"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "isDone",
            "get",
            "remove",
            "peek"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "protected HdfsFileSystemView(User user)",
        "start_line": "22",
        "end_line": "33",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/service/filesystem/HdfsFileSystemView.java",
        "docstring": "The HdfsFileSystemView constructor initializes an instance with a specified user.\\nIt checks if the user and the user's home directory are not null, throwing an IllegalArgumentException if either is null.\\nIt sets the homePath and user fields to the user's home directory and the user instance, respectively.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d8f07e66f690",
        "ground_truth": "protected HdfsFileSystemView(User user) {\n if (user == null) {\n  throw new IllegalArgumentException(\"user can not be null\");\n }\n if (user.getHomeDirectory() == null) {\n  throw new IllegalArgumentException(\n    \"User home directory can not be null\");\n }\n this.homePath = user.getHomeDirectory();\n this.user = user;\n}",
        "import_statements": [
            "import org.apache.ftpserver.ftplet.FileSystemView;",
            "import org.apache.ftpserver.ftplet.FtpFile;",
            "import org.apache.ftpserver.ftplet.User;",
            "import org.apache.hadoop.fs.Path;",
            "import java.io.File;"
        ],
        "reference_api": [
            "getHomeDirectory"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getHomeDirectory",
                "code": "public FtpFile getHomeDirectory() {\n\t\treturn new HdfsFileObject(homePath, File.separator, user);\n\t}"
            }
        ],
        "third_party": []
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public boolean changeWorkingDirectory(String dir)",
        "start_line": "50",
        "end_line": "77",
        "file_path": "other/java/hdfs-over-ftp/src/main/java/org/apache/hadoop/seaweed/ftp/service/filesystem/HdfsFileSystemView.java",
        "docstring": "The changeWorkingDirectory function changes the current working directory to the specified path.\\nIt determines the new path based on whether the input is absolute or relative, and constructs the appropriate Path object.\\nIt prevents changing to the parent directory of the root.\\nIf the new path is a directory, it updates the current path and returns true.\\nIf the new path is not a directory, it returns false.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "27807db0eb0f",
        "ground_truth": "public boolean changeWorkingDirectory(String dir) {\n Path path;\n if (dir.startsWith(File.separator) || new Path(currPath).equals(new Path(dir))) {\n  path = new Path(dir);\n } else if (currPath.length() > 1) {\n  path = new Path(currPath + File.separator + dir);\n } else {\n  if(dir.startsWith(\"/\")) {\n   path = new Path(dir);\n  }else {\n   path = new Path(File.separator + dir);\n  }\n }\n // \u9632\u6b62\u9000\u56de\u6839\u76ee\u5f55\n if (path.getName().equals(\"..\")) {\n  path = new Path(File.separator);\n }\n HdfsFileObject file = new HdfsFileObject(homePath, path.toString(), user);\n if (file.isDirectory()) {\n  currPath = path.toString();\n  return true;\n } else {\n  return false;\n }\n}",
        "import_statements": [
            "import org.apache.ftpserver.ftplet.FileSystemView;",
            "import org.apache.ftpserver.ftplet.FtpFile;",
            "import org.apache.ftpserver.ftplet.User;",
            "import org.apache.hadoop.fs.Path;",
            "import java.io.File;"
        ],
        "reference_api": [
            "getName",
            "equals",
            "toString",
            "isDirectory",
            "startsWith",
            "length"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getName",
            "isDirectory",
            "startsWith"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf)",
        "start_line": "30",
        "end_line": "40",
        "file_path": "other/java/hdfs3/src/main/java/seaweed/hdfs/SeaweedFileSystemStore.java",
        "docstring": "The SeaweedFileSystemStore constructor initializes a new instance with the specified host, port, gRPC port, and configuration.\\nIt creates a FilerClient with the provided parameters and sets the access mode for the volume server based on the configuration.\\nThe access mode can be \"direct\", \"publicUrl\", or \"filerProxy\".",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "be7aba23d184",
        "ground_truth": "public SeaweedFileSystemStore(String host, int port, int grpcPort, Configuration conf) {\n    filerClient = new FilerClient(host, port, grpcPort);\n    this.conf = conf;\n    String volumeServerAccessMode = this.conf.get(FS_SEAWEED_VOLUME_SERVER_ACCESS, \"direct\");\n    if (volumeServerAccessMode.equals(\"publicUrl\")) {\n        filerClient.setAccessVolumeServerByPublicUrl();\n    } else if (volumeServerAccessMode.equals(\"filerProxy\")) {\n        filerClient.setAccessVolumeServerByFilerProxy();\n    }\n}",
        "import_statements": [
            "import org.apache.hadoop.conf.Configuration;",
            "import org.apache.hadoop.fs.FSInputStream;",
            "import org.apache.hadoop.fs.FileStatus;",
            "import org.apache.hadoop.fs.FileSystem;",
            "import org.apache.hadoop.fs.Path;",
            "import org.apache.hadoop.fs.permission.FsPermission;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import seaweedfs.client.*;",
            "import java.io.FileNotFoundException;",
            "import java.io.IOException;",
            "import java.io.OutputStream;",
            "import java.util.ArrayList;",
            "import java.util.Arrays;",
            "import java.util.List;",
            "import static seaweed.hdfs.SeaweedFileSystem.*;"
        ],
        "reference_api": [
            "equals",
            "setAccessVolumeServerByFilerProxy",
            "get",
            "setAccessVolumeServerByPublicUrl"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "setAccessVolumeServerByFilerProxy",
            "get",
            "setAccessVolumeServerByPublicUrl"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "seaweedfs/seaweedfs",
        "function_declaration": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException",
        "start_line": "224",
        "end_line": "238",
        "file_path": "other/java/hdfs3/src/main/java/seaweed/hdfs/SeaweedFileSystemStore.java",
        "docstring": "The openFileForRead function opens a file for reading from a specified path.\\nIt logs the path being opened and looks up the file entry.\\nIf the entry is not found, it throws a FileNotFoundException.\\nIf the entry is found, it returns a new SeaweedHadoopInputStream for reading the file, using the filer client, statistics, and file path.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "7943a1723c23",
        "ground_truth": "public FSInputStream openFileForRead(final Path path, FileSystem.Statistics statistics) throws IOException {\n    LOG.debug(\"openFileForRead path:{}\", path);\n    FilerProto.Entry entry = lookupEntry(path);\n    if (entry == null) {\n        throw new FileNotFoundException(\"read non-exist file \" + path);\n    }\n    return new SeaweedHadoopInputStream(filerClient,\n        statistics,\n        path.toUri().getPath(),\n        entry);\n}",
        "import_statements": [
            "import org.apache.hadoop.conf.Configuration;",
            "import org.apache.hadoop.fs.FSInputStream;",
            "import org.apache.hadoop.fs.FileStatus;",
            "import org.apache.hadoop.fs.FileSystem;",
            "import org.apache.hadoop.fs.Path;",
            "import org.apache.hadoop.fs.permission.FsPermission;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import seaweedfs.client.*;",
            "import java.io.FileNotFoundException;",
            "import java.io.IOException;",
            "import java.io.OutputStream;",
            "import java.util.ArrayList;",
            "import java.util.Arrays;",
            "import java.util.List;",
            "import static seaweed.hdfs.SeaweedFileSystem.*;"
        ],
        "reference_api": [
            "getPath",
            "debug",
            "toUri",
            "lookupEntry"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "lookupEntry",
                "code": "public FilerProto.Entry lookupEntry(Path path) {\n\n        return filerClient.lookupEntry(getParentDirectory(path), path.getName());\n\n    }"
            }
        ],
        "third_party": [
            "getPath",
            "debug",
            "toUri"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "juicedata/juicefs",
        "function_declaration": "public void configure(JobConf conf)",
        "start_line": "55",
        "end_line": "70",
        "file_path": "sdk/java/src/main/java/io/juicefs/bench/IOMapperBase.java",
        "docstring": "The configure function sets up the configuration for a job.\\nIt assigns the configuration object and attempts to retrieve the local host name, defaulting to \"localhost\" if unsuccessful.\\nIt retrieves the number of threads per map and files per thread from the configuration settings.\\nFinally, it initializes a fixed thread pool with the specified number of threads, setting each thread as a daemon.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "36634cee9ed3",
        "ground_truth": "public void configure(JobConf conf) {\n  setConf(conf);\n  try {\n    hostName = InetAddress.getLocalHost().getHostName();\n  } catch (Exception e) {\n    hostName = \"localhost\";\n  }\n  threadsPerMap = conf.getInt(\"test.threadsPerMap\", 1);\n  filesPerThread = conf.getInt(\"test.filesPerThread\", 1);\n  pool = Executors.newFixedThreadPool(threadsPerMap, r -> {\n    Thread t = new Thread(r);\n    t.setDaemon(true);\n    return t;\n  });\n}",
        "import_statements": [
            "import org.apache.commons.logging.Log;",
            "import org.apache.commons.logging.LogFactory;",
            "import org.apache.hadoop.conf.Configured;",
            "import org.apache.hadoop.io.LongWritable;",
            "import org.apache.hadoop.io.Text;",
            "import org.apache.hadoop.mapred.JobConf;",
            "import org.apache.hadoop.mapred.Mapper;",
            "import org.apache.hadoop.mapred.OutputCollector;",
            "import org.apache.hadoop.mapred.Reporter;",
            "import java.io.Closeable;",
            "import java.io.IOException;",
            "import java.net.InetAddress;",
            "import java.util.ArrayList;",
            "import java.util.List;",
            "import java.util.concurrent.ExecutionException;",
            "import java.util.concurrent.ExecutorService;",
            "import java.util.concurrent.Executors;",
            "import java.util.concurrent.Future;",
            "import java.util.concurrent.atomic.AtomicLong;"
        ],
        "reference_api": [
            "getInt",
            "newFixedThreadPool",
            "getHostName",
            "setConf",
            "setDaemon",
            "getLocalHost"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getInt",
            "newFixedThreadPool",
            "getHostName",
            "setConf",
            "setDaemon",
            "getLocalHost"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "juicedata/juicefs",
        "function_declaration": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException",
        "start_line": "76",
        "end_line": "91",
        "file_path": "sdk/java/src/main/java/io/juicefs/JuiceFileSystem.java",
        "docstring": "The startTrashEmptier function initializes and starts a background trash emptier task for a given URI and configuration.\\nIt first checks if the trash emptier is already running for the specified host.\\nIf not, it creates a superuser and initializes a JuiceFileSystemImpl instance as the superuser.\\nIt then starts the trash emptier task using the initialized file system and the trash configuration, scheduling it to run every 10 minutes.\\nIf any exception occurs, it throws an IOException indicating the failure to start the trash emptier.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "53cd9065153d",
        "ground_truth": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException {\n  if (BgTaskUtil.isRunning(uri.getHost(), \"Trash emptier\")) {\n    return;\n  }\n  try {\n    UserGroupInformation superUser = UserGroupInformation.createRemoteUser(getConf(conf, \"superuser\", \"hdfs\"));\n    emptierFs = superUser.doAs((PrivilegedExceptionAction<FileSystem>) () -> {\n      JuiceFileSystemImpl fs = new JuiceFileSystemImpl();\n      fs.initialize(uri, conf);\n      return fs;\n    });\n    BgTaskUtil.startTrashEmptier(uri.getHost(), \"Trash emptier\", emptierFs, new Trash(emptierFs, conf).getEmptier(), TimeUnit.MINUTES.toMillis(10));\n  } catch (Exception e) {\n    throw new IOException(\"start trash failed!\",e);\n  }\n}",
        "import_statements": [
            "import io.juicefs.utils.BgTaskUtil;",
            "import io.juicefs.utils.PatchUtil;",
            "import org.apache.hadoop.classification.InterfaceAudience;",
            "import org.apache.hadoop.classification.InterfaceStability;",
            "import org.apache.hadoop.conf.Configuration;",
            "import org.apache.hadoop.fs.*;",
            "import org.apache.hadoop.fs.permission.FsPermission;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.hadoop.util.Progressable;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.net.URI;",
            "import java.security.PrivilegedExceptionAction;",
            "import java.util.concurrent.TimeUnit;"
        ],
        "reference_api": [
            "startTrashEmptier",
            "getHost",
            "getEmptier",
            "getConf",
            "toMillis",
            "initialize",
            "doAs",
            "createRemoteUser",
            "isRunning"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "startTrashEmptier",
                "code": "private void startTrashEmptier(URI uri, final Configuration conf) throws IOException {\n    if (BgTaskUtil.isRunning(uri.getHost(), \"Trash emptier\")) {\n      return;\n    }\n    try {\n      UserGroupInformation superUser = UserGroupInformation.createRemoteUser(getConf(conf, \"superuser\", \"hdfs\"));\n      emptierFs = superUser.doAs((PrivilegedExceptionAction<FileSystem>) () -> {\n        JuiceFileSystemImpl fs = new JuiceFileSystemImpl();\n        fs.initialize(uri, conf);\n        return fs;\n      });\n      BgTaskUtil.startTrashEmptier(uri.getHost(), \"Trash emptier\", emptierFs, new Trash(emptierFs, conf).getEmptier(), TimeUnit.MINUTES.toMillis(10));\n    } catch (Exception e) {\n      throw new IOException(\"start trash failed!\",e);\n    }\n  }"
            },
            {
                "name": "getConf",
                "code": "private String getConf(Configuration conf, String key, String value) {\n    String name = fs.getUri().getHost();\n    String v = conf.get(\"juicefs.\" + key, value);\n    if (name != null && !name.equals(\"\")) {\n      v = conf.get(\"juicefs.\" + name + \".\" + key, v);\n    }\n    if (v != null)\n      v = v.trim();\n    return v;\n  }"
            },
            {
                "name": "initialize",
                "code": "@Override\n  public void initialize(URI uri, Configuration conf) throws IOException {\n    super.initialize(uri, conf);\n    fileChecksumEnabled = Boolean.parseBoolean(getConf(conf, \"file.checksum\", \"false\"));\n    if (!Boolean.parseBoolean(getConf(conf, \"disable-trash-emptier\", \"false\"))) {\n      startTrashEmptier(uri, conf);\n    }\n  }"
            }
        ],
        "third_party": [
            "getHost",
            "getEmptier",
            "toMillis",
            "doAs",
            "createRemoteUser",
            "isRunning"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "juicedata/juicefs",
        "function_declaration": "public static void setContext(String context) throws Exception",
        "start_line": "24",
        "end_line": "34",
        "file_path": "sdk/java/src/main/java/io/juicefs/utils/CallerContextUtil.java",
        "docstring": "The setContext function sets the caller context to the specified context string.\\nIt retrieves the current caller context and checks its validity.\\nIf the current context is invalid or not set, it creates and sets a new caller context using the provided context.\\nIf the current context is valid but lacks a signature and does not already include the provided context, it appends the new context to the existing one and sets it.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ede797f6a8f2",
        "ground_truth": "public static void setContext(String context) throws Exception {\n  CallerContext current = CallerContext.getCurrent();\n  CallerContext.Builder builder;\n  if (current == null || !current.isContextValid()) {\n    builder = new CallerContext.Builder(context);\n    CallerContext.setCurrent(builder.build());\n  } else if (current.getSignature() == null && !current.getContext().endsWith(\"_\" + context)) {\n    builder = new CallerContext.Builder(current.getContext() + \"_\" + context);\n    CallerContext.setCurrent(builder.build());\n  }\n}",
        "import_statements": [
            "import org.apache.hadoop.ipc.CallerContext;"
        ],
        "reference_api": [
            "endsWith",
            "getSignature",
            "getCurrent",
            "getContext",
            "isContextValid",
            "setCurrent",
            "build"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "endsWith",
            "getSignature",
            "getCurrent",
            "getContext",
            "isContextValid",
            "setCurrent",
            "build"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "TileDB-Inc/TileDB",
        "function_declaration": "Status HDFS::remove_file(const URI& uri)",
        "start_line": "389",
        "end_line": "398",
        "file_path": "tiledb/sm/filesystem/hdfs_filesystem.cc",
        "docstring": "The remove_file function deletes a file from HDFS at the specified URI.\\nIt establishes a connection to the HDFS file system and attempts to delete the file using the hdfsDelete function.\\nIf the deletion fails, it logs and returns an error status.\\nIf successful, it returns an OK status.",
        "language": "CPP",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "56968a526f6a",
        "ground_truth": "Status HDFS::remove_file(const URI& uri) {\n  hdfsFS fs = nullptr;\n  RETURN_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsDelete(fs, uri.to_path().c_str(), 0);\n  if (ret < 0) {\n    return LOG_STATUS(\n        Status_HDFSError(std::string(\"Cannot delete file \") + uri.to_string()));\n  }\n  return Status::Ok();\n}",
        "import_statements": [],
        "reference_api": [
            "std::string",
            "libhdfs_->hdfsDelete",
            "LOG_STATUS",
            "uri.to_string",
            "uri.to_path().c_str",
            "Status_HDFSError",
            "RETURN_NOT_OK",
            "Status::Ok",
            "connect",
            "uri.to_path"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "connect",
                "code": "Status HDFS::connect(hdfsFS* fs) {\n  RETURN_NOT_OK(libhdfs_->status());\n  if (hdfs_ == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\"Not connected to HDFS namenode\"));\n  }\n  *fs = hdfs_;\n  return Status::Ok();\n}"
            }
        ],
        "third_party": [
            "libhdfs_->hdfsDelete",
            "LOG_STATUS",
            "uri.to_string",
            "uri.to_path().c_str",
            "Status_HDFSError",
            "RETURN_NOT_OK",
            "Status::Ok",
            "uri.to_path"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "TileDB-Inc/TileDB",
        "function_declaration": "Status HDFS::sync(const URI& uri)",
        "start_line": "493",
        "end_line": "522",
        "file_path": "tiledb/sm/filesystem/hdfs_filesystem.cc",
        "docstring": "The sync function synchronizes a file in HDFS specified by a URI.\\nIt connects to the HDFS file system and checks if the file exists.\\nIf the file does not exist, it returns an Ok status.\\nIf the file exists, it opens the file in write and append mode.\\nIf the file cannot be opened, it logs an error and returns a failure status.\\nIt attempts to flush the file's data to HDFS.\\nIf the flush operation fails, it logs an error and returns a failure status.\\nFinally, it closes the file and returns an Ok status if successful, otherwise logs an error and returns a failure status.",
        "language": "CPP",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f33adb3e8cb6",
        "ground_truth": "Status HDFS::sync(const URI& uri) {\n  hdfsFS fs = nullptr;\n  RETURN_NOT_OK(connect(&fs));\n   bool file_exists = false;\n  RETURN_NOT_OK(is_file(uri, &file_exists));\n  if (!file_exists)\n    return Status::Ok();\n   // Open file\n  hdfsFile file = libhdfs_->hdfsOpenFile(\n      fs, uri.to_path().c_str(), O_WRONLY | O_APPEND, 0, 0, 0);\n  if (!file) {\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\"Cannot sync file '\") + uri.to_string() +\n        \"'; File open error\"));\n  }\n  // Sync\n  if (libhdfs_->hdfsHFlush(fs, file)) {\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\"Failed syncing file '\") + uri.to_string() + \"'\"));\n  }\n  // Close file\n  if (libhdfs_->hdfsCloseFile(fs, file)) {\n    return LOG_STATUS(Status_HDFSError(\n        std::string(\"Cannot sync file \") + uri.to_string() +\n        \"; File closing error\"));\n  }\n  return Status::Ok();\n}",
        "import_statements": [],
        "reference_api": [
            "std::string",
            "libhdfs_->hdfsOpenFile",
            "is_file",
            "libhdfs_->hdfsCloseFile",
            "LOG_STATUS",
            "uri.to_string",
            "uri.to_path().c_str",
            "Status_HDFSError",
            "RETURN_NOT_OK",
            "Status::Ok",
            "libhdfs_->hdfsHFlush",
            "connect",
            "uri.to_path"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "is_file",
                "code": "Status HDFS::is_file(const URI& uri, bool* is_file) {\n  hdfsFS fs = nullptr;\n  RETURN_NOT_OK(connect(&fs));\n  int ret = libhdfs_->hdfsExists(fs, uri.to_path().c_str());\n  if (!ret) {\n    hdfsFileInfo* fileInfo =\n        libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n    if (fileInfo == NULL) {\n      *is_file = false;\n    } else if ((char)(fileInfo->mKind) == 'F') {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = true;\n    } else {\n      libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n      *is_file = false;\n    }\n  } else {\n    *is_file = false;\n  }\n  return Status::Ok();\n}"
            },
            {
                "name": "connect",
                "code": "Status HDFS::connect(hdfsFS* fs) {\n  RETURN_NOT_OK(libhdfs_->status());\n  if (hdfs_ == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\"Not connected to HDFS namenode\"));\n  }\n  *fs = hdfs_;\n  return Status::Ok();\n}"
            }
        ],
        "third_party": [
            "libhdfs_->hdfsOpenFile",
            "libhdfs_->hdfsCloseFile",
            "LOG_STATUS",
            "uri.to_string",
            "uri.to_path().c_str",
            "Status_HDFSError",
            "RETURN_NOT_OK",
            "Status::Ok",
            "libhdfs_->hdfsHFlush",
            "uri.to_path"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "TileDB-Inc/TileDB",
        "function_declaration": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes)",
        "start_line": "567",
        "end_line": "584",
        "file_path": "tiledb/sm/filesystem/hdfs_filesystem.cc",
        "docstring": "The file_size function retrieves the size of a file in HDFS specified by a URI.\\nIt connects to the HDFS file system and obtains the file information.\\nIf the file information cannot be retrieved or the path is not a file, it logs an error and returns a failure status.\\nIf the path is a file, it sets the size in bytes and frees the file information.\\nFinally, it returns an Ok status if successful.",
        "language": "CPP",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "893c46ab9675",
        "ground_truth": "Status HDFS::file_size(const URI& uri, uint64_t* nbytes) {\n  hdfsFS fs = nullptr;\n  RETURN_NOT_OK(connect(&fs));\n  hdfsFileInfo* fileInfo = libhdfs_->hdfsGetPathInfo(fs, uri.to_path().c_str());\n  if (fileInfo == nullptr) {\n    return LOG_STATUS(\n        Status_HDFSError(std::string(\"Not a file \") + uri.to_string()));\n  }\n  if ((char)(fileInfo->mKind) == 'F') {\n    *nbytes = static_cast<uint64_t>(fileInfo->mSize);\n  } else {\n    libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n    return LOG_STATUS(\n        Status_HDFSError(std::string(\"Not a file \") + uri.to_string()));\n  }\n  libhdfs_->hdfsFreeFileInfo(fileInfo, 1);\n  return Status::Ok();\n}",
        "import_statements": [],
        "reference_api": [
            "std::string",
            "libhdfs_->hdfsFreeFileInfo",
            "libhdfs_->hdfsGetPathInfo",
            "LOG_STATUS",
            "static_cast<uint64_t>",
            "uri.to_string",
            "uri.to_path().c_str",
            "Status_HDFSError",
            "RETURN_NOT_OK",
            "Status::Ok",
            "connect",
            "uri.to_path"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "connect",
                "code": "Status HDFS::connect(hdfsFS* fs) {\n  RETURN_NOT_OK(libhdfs_->status());\n  if (hdfs_ == nullptr) {\n    return LOG_STATUS(Status_HDFSError(\"Not connected to HDFS namenode\"));\n  }\n  *fs = hdfs_;\n  return Status::Ok();\n}"
            }
        ],
        "third_party": [
            "libhdfs_->hdfsFreeFileInfo",
            "libhdfs_->hdfsGetPathInfo",
            "LOG_STATUS",
            "static_cast<uint64_t>",
            "uri.to_string",
            "uri.to_path().c_str",
            "Status_HDFSError",
            "RETURN_NOT_OK",
            "Status::Ok",
            "uri.to_path"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString)",
        "start_line": "90",
        "end_line": "104",
        "file_path": "plugin/writer/hbase11xsqlwriter/src/main/java/com/wgzhao/addax/plugin/writer/hbase11xsqlwriter/HbaseSQLHelper.java",
        "docstring": "The getHbaseConfig function parses an HBase configuration string and extracts the Zookeeper quorum and znode parent values.\\nIt ensures the quorum includes the port number, defaulting to 2181 if not specified.\\nIf the znode parent is not provided, it defaults to a predefined value.\\nThe function returns a pair containing the quorum and znode parent.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3da0b4d8ae16",
        "ground_truth": "public static Pair<String, String> getHbaseConfig(String hbaseCfgString)\n{\n    assert hbaseCfgString != null;\n    Map<String, String> hbaseConfigMap = JSON.parseObject(hbaseCfgString, new TypeReference<Map<String, String>>() {});\n    String zkQuorum = hbaseConfigMap.get(HConstants.ZOOKEEPER_QUORUM);\n    // \u5982\u679c\u6ca1\u6709\u63d0\u4f9bZookeeper\u7aef\u53e3\uff0c\u5219\u4f7f\u7528\u9ed8\u8ba4\u7aef\u53e3\n    if (!zkQuorum.contains(\":\")) {\n        zkQuorum = zkQuorum + \":2181\";\n    }\n    String znode = hbaseConfigMap.get(HConstants.ZOOKEEPER_ZNODE_PARENT);\n    if (znode == null || znode.isEmpty()) {\n        znode = HBaseConstant.DEFAULT_ZNODE;\n    }\n    return new Pair<>(zkQuorum, znode);\n}",
        "import_statements": [
            "import com.wgzhao.addax.common.base.HBaseConstant;",
            "import com.wgzhao.addax.common.base.HBaseKey;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.TypeReference;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.hadoop.hbase.HConstants;",
            "import org.apache.hadoop.hbase.TableName;",
            "import org.apache.hadoop.hbase.client.Admin;",
            "import org.apache.hadoop.hbase.util.Pair;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.phoenix.jdbc.PhoenixConnection;",
            "import org.apache.phoenix.schema.ColumnNotFoundException;",
            "import org.apache.phoenix.schema.MetaDataClient;",
            "import org.apache.phoenix.schema.PTable;",
            "import org.apache.phoenix.schema.types.PDataType;",
            "import org.apache.phoenix.util.SchemaUtil;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.sql.Connection;",
            "import java.sql.DriverManager;",
            "import java.sql.ResultSet;",
            "import java.sql.ResultSetMetaData;",
            "import java.sql.SQLException;",
            "import java.sql.Statement;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "isEmpty",
            "parseObject",
            "get",
            "contains"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "isEmpty",
            "parseObject",
            "get"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "public static void validateConfig(HbaseSQLWriterConfig cfg)",
        "start_line": "117",
        "end_line": "151",
        "file_path": "plugin/writer/hbase11xsqlwriter/src/main/java/com/wgzhao/addax/plugin/writer/hbase11xsqlwriter/HbaseSQLHelper.java",
        "docstring": "The validateConfig function validates the configuration for HbaseSQLWriter.\\nIt establishes a JDBC connection and checks the specified table's existence in the given namespace.\\nIt retrieves the table schema and ensures that all configured columns exist in the target table.\\nIf any column does not exist or an error occurs during validation, it throws an appropriate AddaxException with a relevant error message.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "860cae2bb7f4",
        "ground_truth": "public static void validateConfig(HbaseSQLWriterConfig cfg)\n{\n    // \u6821\u9a8c\u96c6\u7fa4\u5730\u5740\uff1a\u5c1d\u8bd5\u8fde\u63a5\uff0c\u8fde\u4e0d\u4e0a\u5c31\u8bf4\u660e\u6709\u95ee\u9898\uff0c\u629b\u9519\u9000\u51fa\n    Connection conn = getJdbcConnection(cfg);\n    // \u68c0\u67e5\u8868:\u5b58\u5728\uff0c\u53ef\u7528\n    checkTable(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n    // \u6821\u9a8c\u5143\u6570\u636e\uff1a\u914d\u7f6e\u4e2d\u7ed9\u51fa\u7684\u5217\u5fc5\u987b\u662f\u76ee\u7684\u8868\u4e2d\u5df2\u7ecf\u5b58\u5728\u7684\u5217\n    PTable schema;\n    try {\n        schema = getTableSchema(conn, cfg.getNamespace(), cfg.getTableName(), cfg.isThinClient());\n    }\n    catch (SQLException e) {\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                \"Unable to get the metadata of table \" + cfg.getTableName(), e);\n    }\n    List<String> columnNames = cfg.getColumns();\n    try {\n        for (String colName : columnNames) {\n            schema.getColumnForColumnName(colName);\n        }\n    }\n    catch (ColumnNotFoundException e) {\n        // \u7528\u6237\u914d\u7f6e\u7684\u5217\u540d\u5728\u5143\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                \"The column '\" + e.getColumnName() + \"' your configured does not exists in the target table \"  + cfg.getTableName(), e);\n    }\n    catch (SQLException e) {\n        // \u5217\u540d\u6709\u4e8c\u4e49\u6027\u6216\u8005\u5176\u4ed6\u95ee\u9898\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.ILLEGAL_VALUE,\n                \"The column validation of target table \" + cfg.getTableName() + \"has got failure\", e);\n    }\n}",
        "import_statements": [
            "import com.wgzhao.addax.common.base.HBaseConstant;",
            "import com.wgzhao.addax.common.base.HBaseKey;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.TypeReference;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.hadoop.hbase.HConstants;",
            "import org.apache.hadoop.hbase.TableName;",
            "import org.apache.hadoop.hbase.client.Admin;",
            "import org.apache.hadoop.hbase.util.Pair;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.phoenix.jdbc.PhoenixConnection;",
            "import org.apache.phoenix.schema.ColumnNotFoundException;",
            "import org.apache.phoenix.schema.MetaDataClient;",
            "import org.apache.phoenix.schema.PTable;",
            "import org.apache.phoenix.schema.types.PDataType;",
            "import org.apache.phoenix.util.SchemaUtil;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.sql.Connection;",
            "import java.sql.DriverManager;",
            "import java.sql.ResultSet;",
            "import java.sql.ResultSetMetaData;",
            "import java.sql.SQLException;",
            "import java.sql.Statement;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "getColumnName",
            "getJdbcConnection",
            "getTableSchema",
            "getColumnForColumnName",
            "getTableName",
            "asAddaxException",
            "checkTable",
            "getColumns",
            "isThinClient",
            "getNamespace"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getJdbcConnection",
                "code": "public static Connection getJdbcConnection(HbaseSQLWriterConfig cfg)\n    {\n        String connStr = cfg.getConnectionString();\n        LOG.debug(\"Connecting to HBase cluster [{}] ...\", connStr);\n        Connection conn;\n        //\u662f\u5426\u6709Kerberos\u8ba4\u8bc1\n        haveKerberos = cfg.haveKerberos();\n        if (haveKerberos) {\n            kerberosKeytabFilePath = cfg.getKerberosKeytabFilePath();\n            kerberosPrincipal = cfg.getKerberosPrincipal();\n            hadoopConf.set(\"hadoop.security.authentication\", \"Kerberos\");\n        }\n        kerberosAuthentication(kerberosPrincipal, kerberosKeytabFilePath);\n        try {\n            Class.forName(\"org.apache.phoenix.jdbc.PhoenixDriver\");\n            if (cfg.isThinClient()) {\n                conn = getThinClientJdbcConnection(cfg);\n            }\n            else {\n                conn = DriverManager.getConnection(connStr);\n            }\n            conn.setAutoCommit(false);\n        }\n        catch (Throwable e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.GET_HBASE_CONNECTION_ERROR,\n                    \"Unable to connect to hbase cluster, please check the configuration and cluster status \", e);\n        }\n        LOG.debug(\"Connected to HBase cluster successfully.\");\n        return conn;\n    }"
            },
            {
                "name": "getTableSchema",
                "code": "public static PTable getTableSchema(Connection conn, String fullTableName)\n            throws SQLException\n    {\n        PhoenixConnection hconn = conn.unwrap(PhoenixConnection.class);\n        MetaDataClient mdc = new MetaDataClient(hconn);\n        String schemaName = SchemaUtil.getSchemaNameFromFullName(fullTableName);\n        String tableName = SchemaUtil.getTableNameFromFullName(fullTableName);\n        return mdc.updateCache(schemaName, tableName).getTable();\n    }"
            },
            {
                "name": "getTableName",
                "code": "private static TableName getTableName(String tableName)\n    {\n        if (tableName.contains(\".\")) {\n            tableName = tableName.replace(\".\", \":\");\n        }\n        return TableName.valueOf(tableName);\n    }"
            },
            {
                "name": "checkTable",
                "code": "public static void checkTable(Connection conn, String namespace, String tableName, boolean isThinClient)\n    {\n        //ignore check table when use thin client\n        if (!isThinClient) {\n            checkTable(conn, tableName);\n        }\n    }"
            }
        ],
        "third_party": [
            "getColumnName",
            "getColumnForColumnName",
            "asAddaxException",
            "getColumns",
            "isThinClient",
            "getNamespace"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)",
        "start_line": "190",
        "end_line": "204",
        "file_path": "plugin/writer/hbase11xsqlwriter/src/main/java/com/wgzhao/addax/plugin/writer/hbase11xsqlwriter/HbaseSQLHelper.java",
        "docstring": "The kerberosAuthentication function performs Kerberos authentication using a provided principal and keytab file path.\\nIf Kerberos is enabled and the principal and keytab file path are not blank, it sets the Hadoop configuration for UserGroupInformation.\\nIt attempts to log in using the principal and keytab file.\\nIf authentication fails, it logs an error message and throws an exception indicating a Kerberos login error.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "61721c327cf6",
        "ground_truth": "private static void kerberosAuthentication(String kerberosPrincipal, String kerberosKeytabFilePath)\n{\n    if (haveKerberos && StringUtils.isNotBlank(kerberosPrincipal) && StringUtils.isNotBlank(kerberosKeytabFilePath)) {\n        UserGroupInformation.setConfiguration(hadoopConf);\n        try {\n            UserGroupInformation.loginUserFromKeytab(kerberosPrincipal, kerberosKeytabFilePath);\n        }\n        catch (Exception e) {\n            String message = String.format(\"Kerberos authentication failed, please make sure that kerberosKeytabFilePath[%s] and kerberosPrincipal[%s] are correct\",\n                    kerberosKeytabFilePath, kerberosPrincipal);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.KERBEROS_LOGIN_ERROR, e);\n        }\n    }\n}",
        "import_statements": [
            "import com.wgzhao.addax.common.base.HBaseConstant;",
            "import com.wgzhao.addax.common.base.HBaseKey;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.TypeReference;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.hadoop.hbase.HConstants;",
            "import org.apache.hadoop.hbase.TableName;",
            "import org.apache.hadoop.hbase.client.Admin;",
            "import org.apache.hadoop.hbase.util.Pair;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.phoenix.jdbc.PhoenixConnection;",
            "import org.apache.phoenix.schema.ColumnNotFoundException;",
            "import org.apache.phoenix.schema.MetaDataClient;",
            "import org.apache.phoenix.schema.PTable;",
            "import org.apache.phoenix.schema.types.PDataType;",
            "import org.apache.phoenix.util.SchemaUtil;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.sql.Connection;",
            "import java.sql.DriverManager;",
            "import java.sql.ResultSet;",
            "import java.sql.ResultSetMetaData;",
            "import java.sql.SQLException;",
            "import java.sql.Statement;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "setConfiguration",
            "error",
            "isNotBlank",
            "loginUserFromKeytab",
            "asAddaxException",
            "format"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "setConfiguration",
            "error",
            "isNotBlank",
            "loginUserFromKeytab",
            "asAddaxException"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n            throws SQLException",
        "start_line": "285",
        "end_line": "309",
        "file_path": "plugin/writer/hbase11xsqlwriter/src/main/java/com/wgzhao/addax/plugin/writer/hbase11xsqlwriter/HbaseSQLHelper.java",
        "docstring": "The parseColType function extracts column names and their data types from a ResultSet.\\nIt iterates through the ResultSet, using metadata to identify column labels for type and name.\\nFor each row, it maps the column name to a ThinClientPColumn object with the corresponding data type.\\nIf either the column name or type is null, it throws an SQLException.\\nFinally, it returns a map of column names to their respective ThinClientPColumn objects.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9ceb7f0bb717",
        "ground_truth": "public static Map<String, ThinClientPTable.ThinClientPColumn> parseColType(ResultSet rs)\n        throws SQLException\n{\n    Map<String, ThinClientPTable.ThinClientPColumn> cols = new HashMap<>();\n    ResultSetMetaData md = rs.getMetaData();\n    int columnCount = md.getColumnCount();\n    while (rs.next()) {\n        String colName = null;\n        PDataType colType = null;\n        for (int i = 1; i <= columnCount; i++) {\n            if (\"TYPE_NAME\".equals(md.getColumnLabel(i))) {\n                colType = PDataType.fromSqlTypeName((String) rs.getObject(i));\n            }\n            else if (\"COLUMN_NAME\".equals(md.getColumnLabel(i))) {\n                colName = (String) rs.getObject(i);\n            }\n        }\n        if (colType == null || colName == null) {\n            throw new SQLException(\"ColType or colName is null, colType : \" + colType + \" , colName : \" + colName);\n        }\n        cols.put(colName, new ThinClientPTable.ThinClientPColumn(colName, colType));\n    }\n    return cols;\n}",
        "import_statements": [
            "import com.wgzhao.addax.common.base.HBaseConstant;",
            "import com.wgzhao.addax.common.base.HBaseKey;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.TypeReference;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.hadoop.hbase.HConstants;",
            "import org.apache.hadoop.hbase.TableName;",
            "import org.apache.hadoop.hbase.client.Admin;",
            "import org.apache.hadoop.hbase.util.Pair;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.phoenix.jdbc.PhoenixConnection;",
            "import org.apache.phoenix.schema.ColumnNotFoundException;",
            "import org.apache.phoenix.schema.MetaDataClient;",
            "import org.apache.phoenix.schema.PTable;",
            "import org.apache.phoenix.schema.types.PDataType;",
            "import org.apache.phoenix.util.SchemaUtil;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.sql.Connection;",
            "import java.sql.DriverManager;",
            "import java.sql.ResultSet;",
            "import java.sql.ResultSetMetaData;",
            "import java.sql.SQLException;",
            "import java.sql.Statement;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "fromSqlTypeName",
            "equals",
            "getObject",
            "next",
            "getColumnLabel",
            "getColumnCount",
            "put",
            "getMetaData"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "fromSqlTypeName",
            "getObject",
            "next",
            "getColumnLabel",
            "getColumnCount",
            "put",
            "getMetaData"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "public static void truncateTable(Connection conn, String tableName)",
        "start_line": "317",
        "end_line": "342",
        "file_path": "plugin/writer/hbase11xsqlwriter/src/main/java/com/wgzhao/addax/plugin/writer/hbase11xsqlwriter/HbaseSQLHelper.java",
        "docstring": "The truncateTable function truncates an HBase table using a Phoenix connection.\\nIt unwraps the connection to get the PhoenixConnection and retrieves the HBase Admin instance.\\nThe function checks if the table exists, disables it, and then truncates it with the option to preserve the splits.\\nIf successful, it logs a debug message indicating the table has been truncated.\\nIf an error occurs, it throws an exception with a relevant error code and message.\\nFinally, it ensures the Admin instance is closed properly.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "247fc8882f6f",
        "ground_truth": "public static void truncateTable(Connection conn, String tableName)\n{\n    PhoenixConnection sqlConn;\n    Admin admin = null;\n    try {\n        sqlConn = conn.unwrap(PhoenixConnection.class);\n        admin = sqlConn.getQueryServices().getAdmin();\n        TableName hTableName = getTableName(tableName);\n        // \u786e\u4fdd\u8868\u5b58\u5728\u3001\u53ef\u7528\n        checkTable(admin, hTableName);\n        // \u6e05\u7a7a\u8868\n        admin.disableTable(hTableName);\n        admin.truncateTable(hTableName, true);\n        LOG.debug(\"Table {} has been truncated.\", tableName);\n    }\n    catch (Throwable t) {\n        // \u6e05\u7a7a\u8868\u5931\u8d25\n        throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.TRUNCATE_HBASE_ERROR,\n                \"Failed to truncate \" + tableName + \".\", t);\n    }\n    finally {\n        if (admin != null) {\n            closeAdmin(admin);\n        }\n    }\n}",
        "import_statements": [
            "import com.wgzhao.addax.common.base.HBaseConstant;",
            "import com.wgzhao.addax.common.base.HBaseKey;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.TypeReference;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.hadoop.hbase.HConstants;",
            "import org.apache.hadoop.hbase.TableName;",
            "import org.apache.hadoop.hbase.client.Admin;",
            "import org.apache.hadoop.hbase.util.Pair;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.phoenix.jdbc.PhoenixConnection;",
            "import org.apache.phoenix.schema.ColumnNotFoundException;",
            "import org.apache.phoenix.schema.MetaDataClient;",
            "import org.apache.phoenix.schema.PTable;",
            "import org.apache.phoenix.schema.types.PDataType;",
            "import org.apache.phoenix.util.SchemaUtil;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.sql.Connection;",
            "import java.sql.DriverManager;",
            "import java.sql.ResultSet;",
            "import java.sql.ResultSetMetaData;",
            "import java.sql.SQLException;",
            "import java.sql.Statement;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "disableTable",
            "closeAdmin",
            "truncateTable",
            "getAdmin",
            "getQueryServices",
            "debug",
            "getTableName",
            "asAddaxException",
            "checkTable",
            "unwrap"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "closeAdmin",
                "code": "private static void closeAdmin(Admin admin)\n    {\n        try {\n            if (null != admin) {\n                admin.close();\n            }\n        }\n        catch (IOException e) {\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.CLOSE_HBASE_AMIN_ERROR, e);\n        }\n    }"
            },
            {
                "name": "truncateTable",
                "code": "public static void truncateTable(Connection conn, String tableName)\n    {\n        PhoenixConnection sqlConn;\n        Admin admin = null;\n        try {\n            sqlConn = conn.unwrap(PhoenixConnection.class);\n            admin = sqlConn.getQueryServices().getAdmin();\n            TableName hTableName = getTableName(tableName);\n            // \u786e\u4fdd\u8868\u5b58\u5728\u3001\u53ef\u7528\n            checkTable(admin, hTableName);\n            // \u6e05\u7a7a\u8868\n            admin.disableTable(hTableName);\n            admin.truncateTable(hTableName, true);\n            LOG.debug(\"Table {} has been truncated.\", tableName);\n        }\n        catch (Throwable t) {\n            // \u6e05\u7a7a\u8868\u5931\u8d25\n            throw AddaxException.asAddaxException(HbaseSQLWriterErrorCode.TRUNCATE_HBASE_ERROR,\n                    \"Failed to truncate \" + tableName + \".\", t);\n        }\n        finally {\n            if (admin != null) {\n                closeAdmin(admin);\n            }\n        }\n    }"
            },
            {
                "name": "getTableName",
                "code": "private static TableName getTableName(String tableName)\n    {\n        if (tableName.contains(\".\")) {\n            tableName = tableName.replace(\".\", \":\");\n        }\n        return TableName.valueOf(tableName);\n    }"
            },
            {
                "name": "checkTable",
                "code": "public static void checkTable(Connection conn, String namespace, String tableName, boolean isThinClient)\n    {\n        //ignore check table when use thin client\n        if (!isThinClient) {\n            checkTable(conn, tableName);\n        }\n    }"
            }
        ],
        "third_party": [
            "disableTable",
            "getAdmin",
            "getQueryServices",
            "debug",
            "asAddaxException",
            "unwrap"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "private void addSourceFileByType(String filePath)",
        "start_line": "261",
        "end_line": "277",
        "file_path": "plugin/reader/hdfsreader/src/main/java/com/wgzhao/addax/plugin/reader/hdfsreader/DFSUtil.java",
        "docstring": "The addSourceFileByType function adds a file to the source files list if it matches the specified file type.\\nIt checks the file type of the given file path against the configured file type.\\nIf the file type matches, it logs an info message and adds the file to the source files list.\\nIf the file type does not match, it logs an error message and throws an exception indicating the unsupported file type.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ac87e1b9a54e",
        "ground_truth": "private void addSourceFileByType(String filePath)\n{\n    // \u68c0\u67e5file\u7684\u7c7b\u578b\u548c\u7528\u6237\u914d\u7f6e\u7684fileType\u7c7b\u578b\u662f\u5426\u4e00\u81f4\n    boolean isMatchedFileType = checkHdfsFileType(filePath, this.specifiedFileType);\n    if (isMatchedFileType) {\n        LOG.info(\"The file [{}] format is [{}], add it to source files list.\", filePath, this.specifiedFileType);\n        sourceHDFSAllFilesList.add(filePath);\n    }\n    else {\n        String message = String.format(\"The file [%s] format is not the same of [%s] you configured.\"\n                , filePath, this.specifiedFileType);\n        LOG.error(message);\n        throw AddaxException.asAddaxException(\n                HdfsReaderErrorCode.FILE_TYPE_UNSUPPORTED, message);\n    }\n}",
        "import_statements": [
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.JSONObject;",
            "import com.google.common.base.Splitter;",
            "import com.google.common.collect.Iterables;",
            "import com.google.common.primitives.Ints;",
            "import com.google.common.primitives.Longs;",
            "import com.wgzhao.addax.common.base.Key;",
            "import com.wgzhao.addax.common.element.BoolColumn;",
            "import com.wgzhao.addax.common.element.BytesColumn;",
            "import com.wgzhao.addax.common.element.Column;",
            "import com.wgzhao.addax.common.element.ColumnEntry;",
            "import com.wgzhao.addax.common.element.DateColumn;",
            "import com.wgzhao.addax.common.element.DoubleColumn;",
            "import com.wgzhao.addax.common.element.LongColumn;",
            "import com.wgzhao.addax.common.element.Record;",
            "import com.wgzhao.addax.common.element.StringColumn;",
            "import com.wgzhao.addax.common.element.TimestampColumn;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.plugin.RecordSender;",
            "import com.wgzhao.addax.common.plugin.TaskPluginCollector;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.wgzhao.addax.storage.reader.StorageReaderErrorCode;",
            "import com.wgzhao.addax.storage.reader.StorageReaderUtil;",
            "import org.apache.avro.Conversions;",
            "import org.apache.avro.generic.GenericData;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.hadoop.fs.FSDataInputStream;",
            "import org.apache.hadoop.fs.FileStatus;",
            "import org.apache.hadoop.fs.FileSystem;",
            "import org.apache.hadoop.fs.Path;",
            "import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;",
            "import org.apache.hadoop.hive.ql.io.RCFile;",
            "import org.apache.hadoop.hive.ql.io.RCFileRecordReader;",
            "import org.apache.hadoop.hive.ql.io.orc.OrcFile;",
            "import org.apache.hadoop.hive.ql.io.orc.Reader;",
            "import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;",
            "import org.apache.hadoop.hive.serde2.columnar.BytesRefWritable;",
            "import org.apache.hadoop.io.LongWritable;",
            "import org.apache.hadoop.io.SequenceFile;",
            "import org.apache.hadoop.io.Text;",
            "import org.apache.hadoop.io.Writable;",
            "import org.apache.hadoop.mapred.FileSplit;",
            "import org.apache.hadoop.mapred.JobConf;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.hadoop.util.ReflectionUtils;",
            "import org.apache.orc.TypeDescription;",
            "import org.apache.parquet.example.data.Group;",
            "import org.apache.parquet.hadoop.ParquetFileReader;",
            "import org.apache.parquet.hadoop.ParquetReader;",
            "import org.apache.parquet.hadoop.example.GroupReadSupport;",
            "import org.apache.parquet.hadoop.util.HadoopInputFile;",
            "import org.apache.parquet.io.api.Binary;",
            "import org.apache.parquet.schema.MessageType;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.io.InputStream;",
            "import java.math.BigDecimal;",
            "import java.math.RoundingMode;",
            "import java.nio.ByteBuffer;",
            "import java.text.SimpleDateFormat;",
            "import java.util.ArrayList;",
            "import java.util.Arrays;",
            "import java.util.Date;",
            "import java.util.HashSet;",
            "import java.util.List;",
            "import java.util.Objects;",
            "import java.util.Set;",
            "import java.util.concurrent.TimeUnit;",
            "import static com.wgzhao.addax.common.base.Key.COLUMN;",
            "import static com.wgzhao.addax.common.base.Key.NULL_FORMAT;"
        ],
        "reference_api": [
            "error",
            "checkHdfsFileType",
            "info",
            "asAddaxException",
            "add",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "checkHdfsFileType",
                "code": "public boolean checkHdfsFileType(String filepath, String specifiedFileType)\n    {\n\n        Path file = new Path(filepath);\n\n        try (FileSystem fs = FileSystem.get(hadoopConf); FSDataInputStream in = fs.open(file)) {\n            if (StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.ORC)) {\n                return isORCFile(file, fs, in);\n            }\n            else if (StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.RC)) {\n                return isRCFile(filepath, in);\n            }\n            else if (StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.SEQ)) {\n\n                return isSequenceFile(file, in);\n            }\n            else if (StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.PARQUET)) {\n                return isParquetFile(file);\n            }\n            else if (StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.CSV)\n                    || StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.TEXT)) {\n                return true;\n            }\n        }\n        catch (Exception e) {\n            String message = String.format(\"Can not get the file format for [%s]\uff0cit only supports [%s].\",\n                    filepath, HdfsConstant.SUPPORT_FILE_TYPE);\n            LOG.error(message);\n            throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message, e);\n        }\n        return false;\n    }"
            }
        ],
        "third_party": [
            "error",
            "info",
            "asAddaxException",
            "add"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n            RecordSender recordSender, TaskPluginCollector taskPluginCollector)",
        "start_line": "296",
        "end_line": "319",
        "file_path": "plugin/reader/hdfsreader/src/main/java/com/wgzhao/addax/plugin/reader/hdfsreader/DFSUtil.java",
        "docstring": "The sequenceFileStartRead function reads data from a sequence file in HDFS and sends the records to a RecordSender.\\nIt logs the start of the reading process and opens the sequence file using the provided file path.\\nIt iterates through the file, reading key-value pairs.\\nFor each non-blank value, it transports the record using StorageReaderUtil.\\nIf an exception occurs, it logs an error message and throws an AddaxException with relevant details.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5e6a66d06162",
        "ground_truth": "public void sequenceFileStartRead(String sourceSequenceFilePath, Configuration readerSliceConfig,\n        RecordSender recordSender, TaskPluginCollector taskPluginCollector)\n{\n    LOG.info(\"Begin to read the sequence file [{}].\", sourceSequenceFilePath);\n    Path seqFilePath = new Path(sourceSequenceFilePath);\n    try (SequenceFile.Reader reader = new SequenceFile.Reader(this.hadoopConf,\n            SequenceFile.Reader.file(seqFilePath))) {\n        //\u83b7\u53d6SequenceFile.Reader\u5b9e\u4f8b\n        //\u83b7\u53d6key \u4e0e value\n        Writable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), this.hadoopConf);\n        Text value = new Text();\n        while (reader.next(key, value)) {\n            if (StringUtils.isNotBlank(value.toString())) {\n                StorageReaderUtil.transportOneRecord(recordSender, readerSliceConfig, taskPluginCollector, value.toString());\n            }\n        }\n    }\n    catch (Exception e) {\n        String message = String.format(\"Exception occurred while reading the file [%s].\", sourceSequenceFilePath);\n        LOG.error(message);\n        throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_SEQUENCE_FILE_ERROR, message, e);\n    }\n}",
        "import_statements": [
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.JSONObject;",
            "import com.google.common.base.Splitter;",
            "import com.google.common.collect.Iterables;",
            "import com.google.common.primitives.Ints;",
            "import com.google.common.primitives.Longs;",
            "import com.wgzhao.addax.common.base.Key;",
            "import com.wgzhao.addax.common.element.BoolColumn;",
            "import com.wgzhao.addax.common.element.BytesColumn;",
            "import com.wgzhao.addax.common.element.Column;",
            "import com.wgzhao.addax.common.element.ColumnEntry;",
            "import com.wgzhao.addax.common.element.DateColumn;",
            "import com.wgzhao.addax.common.element.DoubleColumn;",
            "import com.wgzhao.addax.common.element.LongColumn;",
            "import com.wgzhao.addax.common.element.Record;",
            "import com.wgzhao.addax.common.element.StringColumn;",
            "import com.wgzhao.addax.common.element.TimestampColumn;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.plugin.RecordSender;",
            "import com.wgzhao.addax.common.plugin.TaskPluginCollector;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.wgzhao.addax.storage.reader.StorageReaderErrorCode;",
            "import com.wgzhao.addax.storage.reader.StorageReaderUtil;",
            "import org.apache.avro.Conversions;",
            "import org.apache.avro.generic.GenericData;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.hadoop.fs.FSDataInputStream;",
            "import org.apache.hadoop.fs.FileStatus;",
            "import org.apache.hadoop.fs.FileSystem;",
            "import org.apache.hadoop.fs.Path;",
            "import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;",
            "import org.apache.hadoop.hive.ql.io.RCFile;",
            "import org.apache.hadoop.hive.ql.io.RCFileRecordReader;",
            "import org.apache.hadoop.hive.ql.io.orc.OrcFile;",
            "import org.apache.hadoop.hive.ql.io.orc.Reader;",
            "import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;",
            "import org.apache.hadoop.hive.serde2.columnar.BytesRefWritable;",
            "import org.apache.hadoop.io.LongWritable;",
            "import org.apache.hadoop.io.SequenceFile;",
            "import org.apache.hadoop.io.Text;",
            "import org.apache.hadoop.io.Writable;",
            "import org.apache.hadoop.mapred.FileSplit;",
            "import org.apache.hadoop.mapred.JobConf;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.hadoop.util.ReflectionUtils;",
            "import org.apache.orc.TypeDescription;",
            "import org.apache.parquet.example.data.Group;",
            "import org.apache.parquet.hadoop.ParquetFileReader;",
            "import org.apache.parquet.hadoop.ParquetReader;",
            "import org.apache.parquet.hadoop.example.GroupReadSupport;",
            "import org.apache.parquet.hadoop.util.HadoopInputFile;",
            "import org.apache.parquet.io.api.Binary;",
            "import org.apache.parquet.schema.MessageType;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.io.InputStream;",
            "import java.math.BigDecimal;",
            "import java.math.RoundingMode;",
            "import java.nio.ByteBuffer;",
            "import java.text.SimpleDateFormat;",
            "import java.util.ArrayList;",
            "import java.util.Arrays;",
            "import java.util.Date;",
            "import java.util.HashSet;",
            "import java.util.List;",
            "import java.util.Objects;",
            "import java.util.Set;",
            "import java.util.concurrent.TimeUnit;",
            "import static com.wgzhao.addax.common.base.Key.COLUMN;",
            "import static com.wgzhao.addax.common.base.Key.NULL_FORMAT;"
        ],
        "reference_api": [
            "newInstance",
            "format",
            "transportOneRecord",
            "error",
            "next",
            "file",
            "getKeyClass",
            "toString",
            "info",
            "asAddaxException",
            "isNotBlank"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "newInstance",
            "transportOneRecord",
            "error",
            "next",
            "file",
            "getKeyClass",
            "info",
            "asAddaxException",
            "isNotBlank"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "public boolean checkHdfsFileType(String filepath, String specifiedFileType)",
        "start_line": "663",
        "end_line": "694",
        "file_path": "plugin/reader/hdfsreader/src/main/java/com/wgzhao/addax/plugin/reader/hdfsreader/DFSUtil.java",
        "docstring": "The checkHdfsFileType function verifies the type of an HDFS file against a specified file type.\\nIt opens the file using the Hadoop FileSystem and checks its format based on the specified type.\\nThe function supports ORC, RC, Sequence, Parquet, CSV, and Text file types.\\nIf the file type matches, it returns true.\\nIf an error occurs during the process, it logs the error and throws an exception with a relevant error code and message.\\nIf the file type does not match any of the supported types, it returns false.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c450f959616c",
        "ground_truth": "public boolean checkHdfsFileType(String filepath, String specifiedFileType)\n{\n    Path file = new Path(filepath);\n    try (FileSystem fs = FileSystem.get(hadoopConf); FSDataInputStream in = fs.open(file)) {\n        if (StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.ORC)) {\n            return isORCFile(file, fs, in);\n        }\n        else if (StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.RC)) {\n            return isRCFile(filepath, in);\n        }\n        else if (StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.SEQ)) {\n            return isSequenceFile(file, in);\n        }\n        else if (StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.PARQUET)) {\n            return isParquetFile(file);\n        }\n        else if (StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.CSV)\n                || StringUtils.equalsIgnoreCase(specifiedFileType, HdfsConstant.TEXT)) {\n            return true;\n        }\n    }\n    catch (Exception e) {\n        String message = String.format(\"Can not get the file format for [%s]\uff0cit only supports [%s].\",\n                filepath, HdfsConstant.SUPPORT_FILE_TYPE);\n        LOG.error(message);\n        throw AddaxException.asAddaxException(HdfsReaderErrorCode.READ_FILE_ERROR, message, e);\n    }\n    return false;\n}",
        "import_statements": [
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.JSONObject;",
            "import com.google.common.base.Splitter;",
            "import com.google.common.collect.Iterables;",
            "import com.google.common.primitives.Ints;",
            "import com.google.common.primitives.Longs;",
            "import com.wgzhao.addax.common.base.Key;",
            "import com.wgzhao.addax.common.element.BoolColumn;",
            "import com.wgzhao.addax.common.element.BytesColumn;",
            "import com.wgzhao.addax.common.element.Column;",
            "import com.wgzhao.addax.common.element.ColumnEntry;",
            "import com.wgzhao.addax.common.element.DateColumn;",
            "import com.wgzhao.addax.common.element.DoubleColumn;",
            "import com.wgzhao.addax.common.element.LongColumn;",
            "import com.wgzhao.addax.common.element.Record;",
            "import com.wgzhao.addax.common.element.StringColumn;",
            "import com.wgzhao.addax.common.element.TimestampColumn;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.plugin.RecordSender;",
            "import com.wgzhao.addax.common.plugin.TaskPluginCollector;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.wgzhao.addax.storage.reader.StorageReaderErrorCode;",
            "import com.wgzhao.addax.storage.reader.StorageReaderUtil;",
            "import org.apache.avro.Conversions;",
            "import org.apache.avro.generic.GenericData;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.hadoop.fs.FSDataInputStream;",
            "import org.apache.hadoop.fs.FileStatus;",
            "import org.apache.hadoop.fs.FileSystem;",
            "import org.apache.hadoop.fs.Path;",
            "import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;",
            "import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;",
            "import org.apache.hadoop.hive.ql.io.RCFile;",
            "import org.apache.hadoop.hive.ql.io.RCFileRecordReader;",
            "import org.apache.hadoop.hive.ql.io.orc.OrcFile;",
            "import org.apache.hadoop.hive.ql.io.orc.Reader;",
            "import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;",
            "import org.apache.hadoop.hive.serde2.columnar.BytesRefWritable;",
            "import org.apache.hadoop.io.LongWritable;",
            "import org.apache.hadoop.io.SequenceFile;",
            "import org.apache.hadoop.io.Text;",
            "import org.apache.hadoop.io.Writable;",
            "import org.apache.hadoop.mapred.FileSplit;",
            "import org.apache.hadoop.mapred.JobConf;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.apache.hadoop.util.ReflectionUtils;",
            "import org.apache.orc.TypeDescription;",
            "import org.apache.parquet.example.data.Group;",
            "import org.apache.parquet.hadoop.ParquetFileReader;",
            "import org.apache.parquet.hadoop.ParquetReader;",
            "import org.apache.parquet.hadoop.example.GroupReadSupport;",
            "import org.apache.parquet.hadoop.util.HadoopInputFile;",
            "import org.apache.parquet.io.api.Binary;",
            "import org.apache.parquet.schema.MessageType;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.io.InputStream;",
            "import java.math.BigDecimal;",
            "import java.math.RoundingMode;",
            "import java.nio.ByteBuffer;",
            "import java.text.SimpleDateFormat;",
            "import java.util.ArrayList;",
            "import java.util.Arrays;",
            "import java.util.Date;",
            "import java.util.HashSet;",
            "import java.util.List;",
            "import java.util.Objects;",
            "import java.util.Set;",
            "import java.util.concurrent.TimeUnit;",
            "import static com.wgzhao.addax.common.base.Key.COLUMN;",
            "import static com.wgzhao.addax.common.base.Key.NULL_FORMAT;"
        ],
        "reference_api": [
            "isRCFile",
            "error",
            "isSequenceFile",
            "equalsIgnoreCase",
            "isParquetFile",
            "get",
            "asAddaxException",
            "isORCFile",
            "open",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "isRCFile",
                "code": "private boolean isRCFile(String filepath, FSDataInputStream in)\n    {\n\n        // The first version of RCFile used the sequence file header.\n        final byte[] originalMagic = {(byte) 'S', (byte) 'E', (byte) 'Q'};\n        // The 'magic' bytes at the beginning of the RCFile\n        final byte[] rcMagic = {(byte) 'R', (byte) 'C', (byte) 'F'};\n        // the version that was included with the original magic, which is mapped\n        // into ORIGINAL_VERSION\n        final byte ORIGINAL_MAGIC_VERSION_WITH_METADATA = 6;\n        // All the versions should be place in this list.\n        final int ORIGINAL_VERSION = 0;  // version with SEQ\n        // version with RCF\n        // final int NEW_MAGIC_VERSION = 1\n        // final int CURRENT_VERSION = NEW_MAGIC_VERSION\n        final int CURRENT_VERSION = 1;\n        byte version;\n\n        byte[] magic = new byte[rcMagic.length];\n        try {\n            in.seek(0);\n            in.readFully(magic);\n\n            if (Arrays.equals(magic, originalMagic)) {\n                if (in.readByte() != ORIGINAL_MAGIC_VERSION_WITH_METADATA) {\n                    return false;\n                }\n                version = ORIGINAL_VERSION;\n            }\n            else {\n                if (!Arrays.equals(magic, rcMagic)) {\n                    return false;\n                }\n\n                // Set 'version'\n                version = in.readByte();\n                if (version > CURRENT_VERSION) {\n                    return false;\n                }\n            }\n\n            if (version == ORIGINAL_VERSION) {\n                try {\n                    Class<?> keyCls = hadoopConf.getClassByName(Text.readString(in));\n                    Class<?> valCls = hadoopConf.getClassByName(Text.readString(in));\n                    if (!keyCls.equals(RCFile.KeyBuffer.class) || !valCls.equals(RCFile.ValueBuffer.class)) {\n                        return false;\n                    }\n                }\n                catch (ClassNotFoundException e) {\n                    return false;\n                }\n            }\n//            boolean decompress = in.readBoolean(); // is compressed?\n            if (version == ORIGINAL_VERSION) {\n                // is block-compressed? it should be always false.\n                boolean blkCompressed = in.readBoolean();\n                return !blkCompressed;\n            }\n            return true;\n        }\n        catch (IOException e) {\n            LOG.info(\"The file [{}] is not RC file.\", filepath);\n        }\n        return false;\n    }"
            },
            {
                "name": "isSequenceFile",
                "code": "private boolean isSequenceFile(Path filepath, FSDataInputStream in)\n    {\n        final byte[] seqMagic = {(byte) 'S', (byte) 'E', (byte) 'Q'};\n        byte[] magic = new byte[seqMagic.length];\n        try {\n            in.seek(0);\n            in.readFully(magic);\n            return Arrays.equals(magic, seqMagic);\n        }\n        catch (IOException e) {\n            LOG.info(\"The file [{}] is not Sequence file.\", filepath);\n        }\n        return false;\n    }"
            },
            {
                "name": "isParquetFile",
                "code": "private boolean isParquetFile(Path file)\n    {\n        try {\n            GroupReadSupport readSupport = new GroupReadSupport();\n            ParquetReader.Builder<Group> reader = ParquetReader.builder(readSupport, file);\n            ParquetReader<Group> build = reader.build();\n            if (build.read() != null) {\n                return true;\n            }\n        }\n        catch (IOException e) {\n            LOG.info(\"The file [{}] is not parquet file.\", file);\n        }\n        return false;\n    }"
            },
            {
                "name": "isORCFile",
                "code": "private boolean isORCFile(Path file, FileSystem fs, FSDataInputStream in)\n    {\n        try {\n            // figure out the size of the file using the option or filesystem\n            long size = fs.getFileStatus(file).getLen();\n\n            //read last bytes into buffer to get PostScript\n            int readSize = (int) Math.min(size, DIRECTORY_SIZE_GUESS);\n            in.seek(size - readSize);\n            ByteBuffer buffer = ByteBuffer.allocate(readSize);\n            in.readFully(buffer.array(), buffer.arrayOffset() + buffer.position(),\n                    buffer.remaining());\n\n            //read the PostScript\n            //get length of PostScript\n            int psLen = buffer.get(readSize - 1) & 0xff;\n            String orcMagic = org.apache.orc.OrcFile.MAGIC;\n            int len = orcMagic.length();\n            if (psLen < len + 1) {\n                return false;\n            }\n            int offset = buffer.arrayOffset() + buffer.position() + buffer.limit() - 1\n                    - len;\n            byte[] array = buffer.array();\n            // now look for the magic string at the end of the postscript.\n            if (Text.decode(array, offset, len).equals(orcMagic)) {\n                return true;\n            }\n            else {\n                // If it isn't there, this may be the 0.11.0 version of ORC.\n                // Read the first 3 bytes of the file to check for the header\n                in.seek(0);\n                byte[] header = new byte[len];\n                in.readFully(header, 0, len);\n                // if it isn't there, this isn't an ORC file\n                if (Text.decode(header, 0, len).equals(orcMagic)) {\n                    return true;\n                }\n            }\n        }\n        catch (IOException e) {\n            LOG.info(\"The file [{}] is not ORC file.\", file);\n        }\n        return false;\n    }"
            }
        ],
        "third_party": [
            "error",
            "equalsIgnoreCase",
            "get",
            "asAddaxException",
            "open"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "public static void closeConnection(Connection hConnection)",
        "start_line": "167",
        "end_line": "177",
        "file_path": "plugin/writer/hbase11xwriter/src/main/java/com/wgzhao/addax/plugin/writer/hbase11xwriter/Hbase11xHelper.java",
        "docstring": "The closeConnection function closes an HBase connection if it is not null.\\nIt attempts to close the provided connection and catches any IOException that occurs.\\nIf an exception is caught, it throws a custom exception with an appropriate error code.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "1bf487d2cc64",
        "ground_truth": "public static void closeConnection(Connection hConnection)\n{\n    try {\n        if (null != hConnection) {\n            hConnection.close();\n        }\n    }\n    catch (IOException e) {\n        throw AddaxException.asAddaxException(Hbase11xWriterErrorCode.CLOSE_HBASE_CONNECTION_ERROR, e);\n    }\n}",
        "import_statements": [
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.TypeReference;",
            "import com.wgzhao.addax.common.base.HBaseConstant;",
            "import com.wgzhao.addax.common.base.HBaseKey;",
            "import com.wgzhao.addax.common.base.Key;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.commons.lang3.Validate;",
            "import org.apache.hadoop.hbase.HBaseConfiguration;",
            "import org.apache.hadoop.hbase.TableName;",
            "import org.apache.hadoop.hbase.client.Admin;",
            "import org.apache.hadoop.hbase.client.BufferedMutator;",
            "import org.apache.hadoop.hbase.client.BufferedMutatorParams;",
            "import org.apache.hadoop.hbase.client.Connection;",
            "import org.apache.hadoop.hbase.client.ConnectionFactory;",
            "import org.apache.hadoop.hbase.client.HTable;",
            "import org.apache.hadoop.hbase.client.Table;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.nio.charset.Charset;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "close",
            "asAddaxException"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "close",
            "asAddaxException"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "public static void closeAdmin(Admin admin)",
        "start_line": "179",
        "end_line": "189",
        "file_path": "plugin/writer/hbase11xwriter/src/main/java/com/wgzhao/addax/plugin/writer/hbase11xwriter/Hbase11xHelper.java",
        "docstring": "The closeAdmin function safely closes an HBase Admin instance.\\nIt checks if the admin object is not null and attempts to close it.\\nIf an IOException occurs during the close operation, it throws an exception with a relevant error code and message.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "680728cbc3ec",
        "ground_truth": "public static void closeAdmin(Admin admin)\n{\n    try {\n        if (null != admin) {\n            admin.close();\n        }\n    }\n    catch (IOException e) {\n        throw AddaxException.asAddaxException(Hbase11xWriterErrorCode.CLOSE_HBASE_AMIN_ERROR, e);\n    }\n}",
        "import_statements": [
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.TypeReference;",
            "import com.wgzhao.addax.common.base.HBaseConstant;",
            "import com.wgzhao.addax.common.base.HBaseKey;",
            "import com.wgzhao.addax.common.base.Key;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.commons.lang3.Validate;",
            "import org.apache.hadoop.hbase.HBaseConfiguration;",
            "import org.apache.hadoop.hbase.TableName;",
            "import org.apache.hadoop.hbase.client.Admin;",
            "import org.apache.hadoop.hbase.client.BufferedMutator;",
            "import org.apache.hadoop.hbase.client.BufferedMutatorParams;",
            "import org.apache.hadoop.hbase.client.Connection;",
            "import org.apache.hadoop.hbase.client.ConnectionFactory;",
            "import org.apache.hadoop.hbase.client.HTable;",
            "import org.apache.hadoop.hbase.client.Table;",
            "import org.apache.hadoop.security.UserGroupInformation;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.nio.charset.Charset;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "close",
            "asAddaxException"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "close",
            "asAddaxException"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "private void convertCellToLine(Cell cell, Record record)\n            throws Exception",
        "start_line": "87",
        "end_line": "108",
        "file_path": "plugin/reader/hbase20xreader/src/main/java/com/wgzhao/addax/plugin/reader/hbase20xreader/MultiVersionTask.java",
        "docstring": "The convertCellToLine function converts an HBase Cell into a Record.\\nIt extracts the row key, timestamp, column family, qualifier, and value from the cell.\\nThe function determines the types and formats for the row key and column value based on predefined mappings.\\nIt converts the extracted data to the appropriate types and adds them as columns to the record.\\nThe columns added to the record include the row key, family and qualifier name, timestamp, and column value.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ce50bd53abef",
        "ground_truth": "private void convertCellToLine(Cell cell, Record record)\n        throws Exception\n{\n    byte[] rawRowkey = CellUtil.cloneRow(cell);\n    long timestamp = cell.getTimestamp();\n    byte[] cfAndQualifierName = Bytes.add(CellUtil.cloneFamily(cell), MultiVersionTask.colonByte, CellUtil.cloneQualifier(cell));\n    byte[] columnValue = CellUtil.cloneValue(cell);\n    ColumnType rawRowkeyType = ColumnType.getByTypeName(familyQualifierMap.get(HBaseConstant.ROWKEY_FLAG).get(HBaseKey.TYPE));\n    String familyQualifier = new String(cfAndQualifierName, StandardCharsets.UTF_8);\n    ColumnType columnValueType = ColumnType.getByTypeName(familyQualifierMap.get(familyQualifier).get(HBaseKey.TYPE));\n    String columnValueFormat = familyQualifierMap.get(familyQualifier).get(HBaseKey.FORMAT);\n    if (StringUtils.isBlank(columnValueFormat)) {\n        columnValueFormat = HBaseConstant.DEFAULT_DATE_FORMAT;\n    }\n    record.addColumn(convertBytesToAssignType(rawRowkeyType, rawRowkey, columnValueFormat));\n    record.addColumn(convertBytesToAssignType(ColumnType.STRING, cfAndQualifierName, columnValueFormat));\n    // \u76f4\u63a5\u5ffd\u7565\u4e86\u7528\u6237\u914d\u7f6e\u7684 timestamp \u7684\u7c7b\u578b\n    record.addColumn(new LongColumn(timestamp));\n    record.addColumn(convertBytesToAssignType(columnValueType, columnValue, columnValueFormat));\n}",
        "import_statements": [
            "import com.wgzhao.addax.common.base.HBaseConstant;",
            "import com.wgzhao.addax.common.base.HBaseKey;",
            "import com.wgzhao.addax.common.element.LongColumn;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.wgzhao.addax.common.element.Record;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.hadoop.hbase.Cell;",
            "import org.apache.hadoop.hbase.CellUtil;",
            "import org.apache.hadoop.hbase.client.Result;",
            "import org.apache.hadoop.hbase.client.Scan;",
            "import org.apache.hadoop.hbase.util.Bytes;",
            "import java.nio.charset.StandardCharsets;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "cloneValue",
            "getByTypeName",
            "addColumn",
            "convertBytesToAssignType",
            "cloneFamily",
            "getTimestamp",
            "isBlank",
            "add",
            "get",
            "cloneQualifier",
            "cloneRow"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "cloneValue",
            "getByTypeName",
            "addColumn",
            "convertBytesToAssignType",
            "cloneFamily",
            "getTimestamp",
            "isBlank",
            "add",
            "get",
            "cloneQualifier",
            "cloneRow"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "public static byte[] convertUserStartRowkey(Configuration configuration)",
        "start_line": "201",
        "end_line": "211",
        "file_path": "plugin/reader/hbase20xreader/src/main/java/com/wgzhao/addax/plugin/reader/hbase20xreader/Hbase20xHelper.java",
        "docstring": "The convertUserStartRowkey function converts a user-defined start row key from the configuration to a byte array.\\nIt retrieves the start row key as a string from the configuration.\\nIf the start row key is blank, it returns an empty byte array.\\nOtherwise, it checks if the row key is binary and converts the string to bytes accordingly.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "29e8f3a99f0c",
        "ground_truth": "public static byte[] convertUserStartRowkey(Configuration configuration)\n{\n    String startRowkey = configuration.getString(HBaseKey.START_ROW_KEY);\n    if (StringUtils.isBlank(startRowkey)) {\n        return HConstants.EMPTY_BYTE_ARRAY;\n    }\n    else {\n        boolean isBinaryRowkey = configuration.getBool(HBaseKey.IS_BINARY_ROW_KEY);\n        return Hbase20xHelper.stringToBytes(startRowkey, isBinaryRowkey);\n    }\n}",
        "import_statements": [
            "import com.wgzhao.addax.common.base.HBaseConstant;",
            "import com.wgzhao.addax.common.base.HBaseKey;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.TypeReference;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.commons.lang3.Validate;",
            "import org.apache.hadoop.hbase.HBaseConfiguration;",
            "import org.apache.hadoop.hbase.HConstants;",
            "import org.apache.hadoop.hbase.TableName;",
            "import org.apache.hadoop.hbase.client.Admin;",
            "import org.apache.hadoop.hbase.client.Connection;",
            "import org.apache.hadoop.hbase.client.ConnectionFactory;",
            "import org.apache.hadoop.hbase.client.RegionLocator;",
            "import org.apache.hadoop.hbase.client.ResultScanner;",
            "import org.apache.hadoop.hbase.client.Table;",
            "import org.apache.hadoop.hbase.util.Bytes;",
            "import org.apache.hadoop.hbase.util.Pair;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.nio.charset.Charset;",
            "import java.util.ArrayList;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "getString",
            "stringToBytes",
            "isBlank",
            "getBool"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "stringToBytes",
                "code": "private static byte[] stringToBytes(String rowkey, boolean isBinaryRowkey)\n    {\n        if (isBinaryRowkey) {\n            return Bytes.toBytesBinary(rowkey);\n        }\n        else {\n            return Bytes.toBytes(rowkey);\n        }\n    }"
            }
        ],
        "third_party": [
            "getString",
            "isBlank",
            "getBool"
        ]
    },
    {
        "subclass": "hdfs",
        "owner/repo": "wgzhao/Addax",
        "function_declaration": "public static byte[] convertInnerStartRowkey(Configuration configuration)",
        "start_line": "232",
        "end_line": "240",
        "file_path": "plugin/reader/hbase20xreader/src/main/java/com/wgzhao/addax/plugin/reader/hbase20xreader/Hbase20xHelper.java",
        "docstring": "The convertInnerStartRowkey function converts a start row key from a configuration into a byte array.\\nIt retrieves the start row key as a string from the configuration.\\nIf the start row key is blank, it returns an empty byte array.\\nOtherwise, it converts the start row key string to a binary byte array and returns it.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f5ad51545b10",
        "ground_truth": "public static byte[] convertInnerStartRowkey(Configuration configuration)\n{\n    String startRowkey = configuration.getString(HBaseKey.START_ROW_KEY);\n    if (StringUtils.isBlank(startRowkey)) {\n        return HConstants.EMPTY_BYTE_ARRAY;\n    }\n    return Bytes.toBytesBinary(startRowkey);\n}",
        "import_statements": [
            "import com.wgzhao.addax.common.base.HBaseConstant;",
            "import com.wgzhao.addax.common.base.HBaseKey;",
            "import com.wgzhao.addax.common.exception.AddaxException;",
            "import com.wgzhao.addax.common.util.Configuration;",
            "import com.alibaba.fastjson2.JSON;",
            "import com.alibaba.fastjson2.TypeReference;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.apache.commons.lang3.Validate;",
            "import org.apache.hadoop.hbase.HBaseConfiguration;",
            "import org.apache.hadoop.hbase.HConstants;",
            "import org.apache.hadoop.hbase.TableName;",
            "import org.apache.hadoop.hbase.client.Admin;",
            "import org.apache.hadoop.hbase.client.Connection;",
            "import org.apache.hadoop.hbase.client.ConnectionFactory;",
            "import org.apache.hadoop.hbase.client.RegionLocator;",
            "import org.apache.hadoop.hbase.client.ResultScanner;",
            "import org.apache.hadoop.hbase.client.Table;",
            "import org.apache.hadoop.hbase.util.Bytes;",
            "import org.apache.hadoop.hbase.util.Pair;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.IOException;",
            "import java.nio.charset.Charset;",
            "import java.util.ArrayList;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "getString",
            "isBlank",
            "toBytesBinary"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getString",
            "isBlank",
            "toBytesBinary"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record)",
        "start_line": "30",
        "end_line": "38",
        "file_path": "common/queue/src/main/java/org/thingsboard/server/queue/kafka/KafkaTbQueueMsg.java",
        "docstring": "The KafkaTbQueueMsg constructor initializes an instance using a ConsumerRecord.\\nIt extracts the key from the record and converts it to a UUID.\\nIt creates and populates a TbQueueMsgHeaders object with the headers from the record.\\nFinally, it sets the data field with the record's value.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8618eef8b0e9",
        "ground_truth": "public KafkaTbQueueMsg(ConsumerRecord<String, byte[]> record) {\n    this.key = UUID.fromString(record.key());\n    TbQueueMsgHeaders headers = new DefaultTbQueueMsgHeaders();\n    record.headers().forEach(header -> {\n        headers.put(header.key(), header.value());\n    });\n    this.headers = headers;\n    this.data = record.value();\n}",
        "import_statements": [
            "import org.apache.kafka.clients.consumer.ConsumerRecord;",
            "import org.thingsboard.server.queue.TbQueueMsg;",
            "import org.thingsboard.server.queue.TbQueueMsgHeaders;",
            "import org.thingsboard.server.queue.common.DefaultTbQueueMsgHeaders;",
            "import java.util.UUID;"
        ],
        "reference_api": [
            "headers",
            "value",
            "forEach",
            "key",
            "put",
            "fromString"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "headers",
            "value",
            "forEach",
            "key",
            "put",
            "fromString"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer()",
        "start_line": "126",
        "end_line": "133",
        "file_path": "common/queue/src/main/java/org/thingsboard/server/queue/provider/KafkaTbCoreQueueFactory.java",
        "docstring": "The createTransportNotificationsMsgProducer function creates a Kafka producer for transport notification messages.\\nIt builds a Kafka producer template using specified Kafka settings, a client ID based on the service ID, and a default topic name from the transport notification settings.\\nIt also sets an admin for the producer.\\nThe function returns the configured Kafka producer instance.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "46484df184dd",
        "ground_truth": "public TbQueueProducer<TbProtoQueueMsg<ToTransportMsg>> createTransportNotificationsMsgProducer() {\n    TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToTransportMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n    requestBuilder.settings(kafkaSettings);\n    requestBuilder.clientId(\"tb-core-transport-notifications-\" + serviceInfoProvider.getServiceId());\n    requestBuilder.defaultTopic(topicService.buildTopicName(transportNotificationSettings.getNotificationsTopic()));\n    requestBuilder.admin(notificationAdmin);\n    return requestBuilder.build();\n}",
        "import_statements": [
            "import com.google.protobuf.util.JsonFormat;",
            "import org.springframework.boot.autoconfigure.condition.ConditionalOnExpression;",
            "import org.springframework.context.annotation.Bean;",
            "import org.springframework.stereotype.Component;",
            "import org.thingsboard.server.common.msg.queue.ServiceType;",
            "import org.thingsboard.server.gen.js.JsInvokeProtos;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToCoreMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToCoreNotificationMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToHousekeeperServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToOtaPackageStateServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToRuleEngineMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToRuleEngineNotificationMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToTransportMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToUsageStatsServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToVersionControlServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.TransportApiRequestMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.TransportApiResponseMsg;",
            "import org.thingsboard.server.queue.TbQueueAdmin;",
            "import org.thingsboard.server.queue.TbQueueConsumer;",
            "import org.thingsboard.server.queue.TbQueueProducer;",
            "import org.thingsboard.server.queue.TbQueueRequestTemplate;",
            "import org.thingsboard.server.queue.common.DefaultTbQueueRequestTemplate;",
            "import org.thingsboard.server.queue.common.TbProtoJsQueueMsg;",
            "import org.thingsboard.server.queue.common.TbProtoQueueMsg;",
            "import org.thingsboard.server.queue.discovery.TbServiceInfoProvider;",
            "import org.thingsboard.server.queue.discovery.TopicService;",
            "import org.thingsboard.server.queue.kafka.TbKafkaAdmin;",
            "import org.thingsboard.server.queue.kafka.TbKafkaConsumerStatsService;",
            "import org.thingsboard.server.queue.kafka.TbKafkaConsumerTemplate;",
            "import org.thingsboard.server.queue.kafka.TbKafkaProducerTemplate;",
            "import org.thingsboard.server.queue.kafka.TbKafkaSettings;",
            "import org.thingsboard.server.queue.kafka.TbKafkaTopicConfigs;",
            "import org.thingsboard.server.queue.settings.TbQueueCoreSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueRemoteJsInvokeSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueRuleEngineSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueTransportApiSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueTransportNotificationSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueVersionControlSettings;",
            "import jakarta.annotation.PreDestroy;",
            "import java.nio.charset.StandardCharsets;",
            "import java.util.concurrent.atomic.AtomicLong;"
        ],
        "reference_api": [
            "buildTopicName",
            "admin",
            "clientId",
            "settings",
            "getNotificationsTopic",
            "getServiceId",
            "build",
            "builder",
            "defaultTopic"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "buildTopicName",
            "admin",
            "clientId",
            "settings",
            "getNotificationsTopic",
            "getServiceId",
            "build",
            "builder",
            "defaultTopic"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer()",
        "start_line": "189",
        "end_line": "199",
        "file_path": "common/queue/src/main/java/org/thingsboard/server/queue/provider/KafkaTbCoreQueueFactory.java",
        "docstring": "The createToCoreNotificationsMsgConsumer function creates a Kafka consumer for ToCoreNotificationMsg messages.\\nIt builds a TbKafkaConsumerTemplate with the specified settings, topic, client ID, and group ID.\\nIt sets a message decoder to parse the messages and configures the consumer with an admin and a statistics service.\\nFinally, it returns the constructed Kafka consumer.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4ac56ef9f89a",
        "ground_truth": "public TbQueueConsumer<TbProtoQueueMsg<ToCoreNotificationMsg>> createToCoreNotificationsMsgConsumer() {\n    TbKafkaConsumerTemplate.TbKafkaConsumerTemplateBuilder<TbProtoQueueMsg<ToCoreNotificationMsg>> consumerBuilder = TbKafkaConsumerTemplate.builder();\n    consumerBuilder.settings(kafkaSettings);\n    consumerBuilder.topic(topicService.getNotificationsTopic(ServiceType.TB_CORE, serviceInfoProvider.getServiceId()).getFullTopicName());\n    consumerBuilder.clientId(\"tb-core-notifications-consumer-\" + serviceInfoProvider.getServiceId());\n    consumerBuilder.groupId(topicService.buildTopicName(\"tb-core-notifications-node-\" + serviceInfoProvider.getServiceId()));\n    consumerBuilder.decoder(msg -> new TbProtoQueueMsg<>(msg.getKey(), ToCoreNotificationMsg.parseFrom(msg.getData()), msg.getHeaders()));\n    consumerBuilder.admin(notificationAdmin);\n    consumerBuilder.statsService(consumerStatsService);\n    return consumerBuilder.build();\n}",
        "import_statements": [
            "import com.google.protobuf.util.JsonFormat;",
            "import org.springframework.boot.autoconfigure.condition.ConditionalOnExpression;",
            "import org.springframework.context.annotation.Bean;",
            "import org.springframework.stereotype.Component;",
            "import org.thingsboard.server.common.msg.queue.ServiceType;",
            "import org.thingsboard.server.gen.js.JsInvokeProtos;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToCoreMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToCoreNotificationMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToHousekeeperServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToOtaPackageStateServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToRuleEngineMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToRuleEngineNotificationMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToTransportMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToUsageStatsServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToVersionControlServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.TransportApiRequestMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.TransportApiResponseMsg;",
            "import org.thingsboard.server.queue.TbQueueAdmin;",
            "import org.thingsboard.server.queue.TbQueueConsumer;",
            "import org.thingsboard.server.queue.TbQueueProducer;",
            "import org.thingsboard.server.queue.TbQueueRequestTemplate;",
            "import org.thingsboard.server.queue.common.DefaultTbQueueRequestTemplate;",
            "import org.thingsboard.server.queue.common.TbProtoJsQueueMsg;",
            "import org.thingsboard.server.queue.common.TbProtoQueueMsg;",
            "import org.thingsboard.server.queue.discovery.TbServiceInfoProvider;",
            "import org.thingsboard.server.queue.discovery.TopicService;",
            "import org.thingsboard.server.queue.kafka.TbKafkaAdmin;",
            "import org.thingsboard.server.queue.kafka.TbKafkaConsumerStatsService;",
            "import org.thingsboard.server.queue.kafka.TbKafkaConsumerTemplate;",
            "import org.thingsboard.server.queue.kafka.TbKafkaProducerTemplate;",
            "import org.thingsboard.server.queue.kafka.TbKafkaSettings;",
            "import org.thingsboard.server.queue.kafka.TbKafkaTopicConfigs;",
            "import org.thingsboard.server.queue.settings.TbQueueCoreSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueRemoteJsInvokeSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueRuleEngineSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueTransportApiSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueTransportNotificationSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueVersionControlSettings;",
            "import jakarta.annotation.PreDestroy;",
            "import java.nio.charset.StandardCharsets;",
            "import java.util.concurrent.atomic.AtomicLong;"
        ],
        "reference_api": [
            "buildTopicName",
            "statsService",
            "getData",
            "admin",
            "parseFrom",
            "getHeaders",
            "clientId",
            "groupId",
            "decoder",
            "getFullTopicName",
            "settings",
            "build",
            "getNotificationsTopic",
            "getKey",
            "topic",
            "builder",
            "getServiceId"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "buildTopicName",
            "statsService",
            "getData",
            "admin",
            "parseFrom",
            "getHeaders",
            "clientId",
            "groupId",
            "decoder",
            "getFullTopicName",
            "settings",
            "build",
            "getNotificationsTopic",
            "getKey",
            "topic",
            "builder",
            "getServiceId"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "protected void doSubscribe(List<String> topicNames)",
        "start_line": "73",
        "end_line": "81",
        "file_path": "common/queue/src/main/java/org/thingsboard/server/queue/kafka/TbKafkaConsumerTemplate.java",
        "docstring": "The doSubscribe function manages topic subscriptions for a consumer.\\nIf the provided list of topic names is not empty, it ensures each topic exists by creating it if necessary, then subscribes the consumer to these topics.\\nIf the list is empty, it logs an informational message and unsubscribes the consumer from all topics.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e13a82b26338",
        "ground_truth": "protected void doSubscribe(List<String> topicNames) {\n    if (!topicNames.isEmpty()) {\n        topicNames.forEach(admin::createTopicIfNotExists);\n        consumer.subscribe(topicNames);\n    } else {\n        log.info(\"unsubscribe due to empty topic list\");\n        consumer.unsubscribe();\n    }\n}",
        "import_statements": [
            "import lombok.Builder;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.kafka.clients.consumer.ConsumerConfig;",
            "import org.apache.kafka.clients.consumer.ConsumerRecord;",
            "import org.apache.kafka.clients.consumer.ConsumerRecords;",
            "import org.apache.kafka.clients.consumer.KafkaConsumer;",
            "import org.springframework.util.StopWatch;",
            "import org.thingsboard.server.queue.TbQueueAdmin;",
            "import org.thingsboard.server.queue.TbQueueMsg;",
            "import org.thingsboard.server.queue.common.AbstractTbQueueConsumerTemplate;",
            "import java.io.IOException;",
            "import java.time.Duration;",
            "import java.util.ArrayList;",
            "import java.util.Collections;",
            "import java.util.List;",
            "import java.util.Properties;"
        ],
        "reference_api": [
            "subscribe",
            "forEach",
            "unsubscribe",
            "info",
            "isEmpty"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "subscribe",
            "forEach",
            "unsubscribe",
            "info",
            "isEmpty"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis)",
        "start_line": "84",
        "end_line": "102",
        "file_path": "common/queue/src/main/java/org/thingsboard/server/queue/kafka/TbKafkaConsumerTemplate.java",
        "docstring": "The doPoll function retrieves records from a Kafka topic within a specified duration in milliseconds.\\nIt starts a stopwatch to measure the polling time and logs the topic and duration.\\nIt polls the consumer for records and stops the stopwatch, logging the total time taken.\\nIf no records are found, it returns an empty list.\\nOtherwise, it collects the records into a list and returns them.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8a20d0b48703",
        "ground_truth": "protected List<ConsumerRecord<String, byte[]>> doPoll(long durationInMillis) {\n    StopWatch stopWatch = new StopWatch();\n    stopWatch.start();\n    log.trace(\"poll topic {} maxDuration {}\", getTopic(), durationInMillis);\n    ConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(durationInMillis));\n    stopWatch.stop();\n    log.trace(\"poll topic {} took {}ms\", getTopic(), stopWatch.getTotalTimeMillis());\n    if (records.isEmpty()) {\n        return Collections.emptyList();\n    } else {\n        List<ConsumerRecord<String, byte[]>> recordList = new ArrayList<>(256);\n        records.forEach(recordList::add);\n        return recordList;\n    }\n}",
        "import_statements": [
            "import lombok.Builder;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.kafka.clients.consumer.ConsumerConfig;",
            "import org.apache.kafka.clients.consumer.ConsumerRecord;",
            "import org.apache.kafka.clients.consumer.ConsumerRecords;",
            "import org.apache.kafka.clients.consumer.KafkaConsumer;",
            "import org.springframework.util.StopWatch;",
            "import org.thingsboard.server.queue.TbQueueAdmin;",
            "import org.thingsboard.server.queue.TbQueueMsg;",
            "import org.thingsboard.server.queue.common.AbstractTbQueueConsumerTemplate;",
            "import java.io.IOException;",
            "import java.time.Duration;",
            "import java.util.ArrayList;",
            "import java.util.Collections;",
            "import java.util.List;",
            "import java.util.Properties;"
        ],
        "reference_api": [
            "stop",
            "emptyList",
            "poll",
            "getTotalTimeMillis",
            "isEmpty",
            "forEach",
            "ofMillis",
            "start",
            "getTopic",
            "trace"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "stop",
            "emptyList",
            "poll",
            "getTotalTimeMillis",
            "isEmpty",
            "forEach",
            "ofMillis",
            "start",
            "getTopic",
            "trace"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "public void onMsg(TbContext ctx, TbMsg msg)",
        "start_line": "121",
        "end_line": "144",
        "file_path": "rule-engine/rule-engine-components/src/main/java/org/thingsboard/rule/engine/kafka/TbKafkaNode.java",
        "docstring": "The onMsg function processes a message in a Kafka rule node context.\\nIt determines the topic and key patterns from the configuration and the message.\\nIf initialization encountered an error, it logs the failure and notifies the context.\\nOtherwise, it executes an asynchronous task to publish the message to Kafka with the specified topic and key.\\nIf an exception occurs, it logs the failure and notifies the context.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "328e4c39a8e1",
        "ground_truth": "public void onMsg(TbContext ctx, TbMsg msg) {\n    String topic = TbNodeUtils.processPattern(config.getTopicPattern(), msg);\n    String keyPattern = config.getKeyPattern();\n    var tbMsg = ackIfNeeded(ctx, msg);\n    try {\n        if (initError != null) {\n            ctx.tellFailure(tbMsg, new RuntimeException(\"Failed to initialize Kafka rule node producer: \" + initError.getMessage()));\n        } else {\n            ctx.getExternalCallExecutor().executeAsync(() -> {\n                publish(\n                        ctx,\n                        tbMsg,\n                        topic,\n                        keyPattern == null || keyPattern.isEmpty()\n                                ? null\n                                : TbNodeUtils.processPattern(config.getKeyPattern(), tbMsg)\n                );\n                return null;\n            });\n        }\n    } catch (Exception e) {\n        ctx.tellFailure(tbMsg, e);\n    }\n}",
        "import_statements": [
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.commons.lang3.BooleanUtils;",
            "import org.apache.kafka.clients.producer.KafkaProducer;",
            "import org.apache.kafka.clients.producer.Producer;",
            "import org.apache.kafka.clients.producer.ProducerConfig;",
            "import org.apache.kafka.clients.producer.ProducerRecord;",
            "import org.apache.kafka.clients.producer.RecordMetadata;",
            "import org.apache.kafka.common.config.SslConfigs;",
            "import org.apache.kafka.common.header.Headers;",
            "import org.apache.kafka.common.header.internals.RecordHeader;",
            "import org.apache.kafka.common.header.internals.RecordHeaders;",
            "import org.springframework.util.ReflectionUtils;",
            "import org.thingsboard.rule.engine.api.RuleNode;",
            "import org.thingsboard.rule.engine.api.TbContext;",
            "import org.thingsboard.rule.engine.api.TbNodeConfiguration;",
            "import org.thingsboard.rule.engine.api.TbNodeException;",
            "import org.thingsboard.rule.engine.api.util.TbNodeUtils;",
            "import org.thingsboard.rule.engine.external.TbAbstractExternalNode;",
            "import org.thingsboard.server.common.data.exception.ThingsboardKafkaClientError;",
            "import org.thingsboard.server.common.data.plugin.ComponentType;",
            "import org.thingsboard.server.common.msg.TbMsg;",
            "import org.thingsboard.server.common.msg.TbMsgMetaData;",
            "import java.lang.reflect.Field;",
            "import java.nio.charset.Charset;",
            "import java.nio.charset.StandardCharsets;",
            "import java.util.Properties;"
        ],
        "reference_api": [
            "processPattern",
            "executeAsync",
            "ackIfNeeded",
            "getMessage",
            "getTopicPattern",
            "publish",
            "getKeyPattern",
            "tellFailure",
            "getExternalCallExecutor",
            "isEmpty"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "publish",
                "code": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key) {\n        try {\n            if (!addMetadataKeyValuesAsKafkaHeaders) {\n                //TODO: external system executor\n                producer.send(new ProducerRecord<>(topic, key, msg.getData()),\n                        (metadata, e) -> processRecord(ctx, msg, metadata, e));\n            } else {\n                Headers headers = new RecordHeaders();\n                msg.getMetaData().values().forEach((k, v) -> headers.add(new RecordHeader(TB_MSG_MD_PREFIX + k, v.getBytes(toBytesCharset))));\n                producer.send(new ProducerRecord<>(topic, null, null, key, msg.getData(), headers),\n                        (metadata, e) -> processRecord(ctx, msg, metadata, e));\n            }\n        } catch (Exception e) {\n            log.debug(\"[{}] Failed to process message: {}\", ctx.getSelfId(), msg, e);\n        }\n    }"
            }
        ],
        "third_party": [
            "processPattern",
            "executeAsync",
            "ackIfNeeded",
            "getMessage",
            "getTopicPattern",
            "getKeyPattern",
            "tellFailure",
            "getExternalCallExecutor",
            "isEmpty"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key)",
        "start_line": "146",
        "end_line": "161",
        "file_path": "rule-engine/rule-engine-components/src/main/java/org/thingsboard/rule/engine/kafka/TbKafkaNode.java",
        "docstring": "The publish function sends a message to a Kafka topic with a specified key.\\nIt checks whether to add metadata key values as Kafka headers.\\nIf not, it sends the message directly to the topic.\\nIf metadata should be added, it creates headers from the message metadata and sends the message with these headers.\\nIn both cases, a callback function processes the record.\\nIf an exception occurs, it logs a debug message with the context ID and message.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "35f835e8c387",
        "ground_truth": "protected void publish(TbContext ctx, TbMsg msg, String topic, String key) {\n    try {\n        if (!addMetadataKeyValuesAsKafkaHeaders) {\n            //TODO: external system executor\n            producer.send(new ProducerRecord<>(topic, key, msg.getData()),\n                    (metadata, e) -> processRecord(ctx, msg, metadata, e));\n        } else {\n            Headers headers = new RecordHeaders();\n            msg.getMetaData().values().forEach((k, v) -> headers.add(new RecordHeader(TB_MSG_MD_PREFIX + k, v.getBytes(toBytesCharset))));\n            producer.send(new ProducerRecord<>(topic, null, null, key, msg.getData(), headers),\n                    (metadata, e) -> processRecord(ctx, msg, metadata, e));\n        }\n    } catch (Exception e) {\n        log.debug(\"[{}] Failed to process message: {}\", ctx.getSelfId(), msg, e);\n    }\n}",
        "import_statements": [
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.commons.lang3.BooleanUtils;",
            "import org.apache.kafka.clients.producer.KafkaProducer;",
            "import org.apache.kafka.clients.producer.Producer;",
            "import org.apache.kafka.clients.producer.ProducerConfig;",
            "import org.apache.kafka.clients.producer.ProducerRecord;",
            "import org.apache.kafka.clients.producer.RecordMetadata;",
            "import org.apache.kafka.common.config.SslConfigs;",
            "import org.apache.kafka.common.header.Headers;",
            "import org.apache.kafka.common.header.internals.RecordHeader;",
            "import org.apache.kafka.common.header.internals.RecordHeaders;",
            "import org.springframework.util.ReflectionUtils;",
            "import org.thingsboard.rule.engine.api.RuleNode;",
            "import org.thingsboard.rule.engine.api.TbContext;",
            "import org.thingsboard.rule.engine.api.TbNodeConfiguration;",
            "import org.thingsboard.rule.engine.api.TbNodeException;",
            "import org.thingsboard.rule.engine.api.util.TbNodeUtils;",
            "import org.thingsboard.rule.engine.external.TbAbstractExternalNode;",
            "import org.thingsboard.server.common.data.exception.ThingsboardKafkaClientError;",
            "import org.thingsboard.server.common.data.plugin.ComponentType;",
            "import org.thingsboard.server.common.msg.TbMsg;",
            "import org.thingsboard.server.common.msg.TbMsgMetaData;",
            "import java.lang.reflect.Field;",
            "import java.nio.charset.Charset;",
            "import java.nio.charset.StandardCharsets;",
            "import java.util.Properties;"
        ],
        "reference_api": [
            "getBytes",
            "getData",
            "processRecord",
            "values",
            "getSelfId",
            "forEach",
            "send",
            "debug",
            "add",
            "getMetaData"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "processRecord",
                "code": "private void processRecord(TbContext ctx, TbMsg msg, RecordMetadata metadata, Exception e) {\n        if (e == null) {\n            tellSuccess(ctx, processResponse(msg, metadata));\n        } else {\n            tellFailure(ctx, processException(msg, e), e);\n        }\n    }"
            }
        ],
        "third_party": [
            "getBytes",
            "getData",
            "values",
            "getSelfId",
            "forEach",
            "send",
            "debug",
            "add",
            "getMetaData"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "public void destroy()",
        "start_line": "164",
        "end_line": "172",
        "file_path": "rule-engine/rule-engine-components/src/main/java/org/thingsboard/rule/engine/kafka/TbKafkaNode.java",
        "docstring": "The destroy function closes the Kafka producer if it is not null.\\nIt attempts to close the producer and catches any exceptions that occur during the process.\\nIf an exception is caught, it logs an error message indicating the failure to close the producer.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "111a11189434",
        "ground_truth": "public void destroy() {\n    if (this.producer != null) {\n        try {\n            this.producer.close();\n        } catch (Exception e) {\n            log.error(\"Failed to close producer during destroy()\", e);\n        }\n    }\n}",
        "import_statements": [
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.commons.lang3.BooleanUtils;",
            "import org.apache.kafka.clients.producer.KafkaProducer;",
            "import org.apache.kafka.clients.producer.Producer;",
            "import org.apache.kafka.clients.producer.ProducerConfig;",
            "import org.apache.kafka.clients.producer.ProducerRecord;",
            "import org.apache.kafka.clients.producer.RecordMetadata;",
            "import org.apache.kafka.common.config.SslConfigs;",
            "import org.apache.kafka.common.header.Headers;",
            "import org.apache.kafka.common.header.internals.RecordHeader;",
            "import org.apache.kafka.common.header.internals.RecordHeaders;",
            "import org.springframework.util.ReflectionUtils;",
            "import org.thingsboard.rule.engine.api.RuleNode;",
            "import org.thingsboard.rule.engine.api.TbContext;",
            "import org.thingsboard.rule.engine.api.TbNodeConfiguration;",
            "import org.thingsboard.rule.engine.api.TbNodeException;",
            "import org.thingsboard.rule.engine.api.util.TbNodeUtils;",
            "import org.thingsboard.rule.engine.external.TbAbstractExternalNode;",
            "import org.thingsboard.server.common.data.exception.ThingsboardKafkaClientError;",
            "import org.thingsboard.server.common.data.plugin.ComponentType;",
            "import org.thingsboard.server.common.msg.TbMsg;",
            "import org.thingsboard.server.common.msg.TbMsgMetaData;",
            "import java.lang.reflect.Field;",
            "import java.nio.charset.Charset;",
            "import java.nio.charset.StandardCharsets;",
            "import java.util.Properties;"
        ],
        "reference_api": [
            "close",
            "error"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "close",
            "error"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer()",
        "start_line": "136",
        "end_line": "143",
        "file_path": "common/queue/src/main/java/org/thingsboard/server/queue/provider/KafkaMonolithQueueFactory.java",
        "docstring": "The createRuleEngineMsgProducer function creates and configures a Kafka producer for Rule Engine messages.\\nIt initializes a Kafka producer template builder with the required settings, including Kafka settings, client ID, default topic, and admin.\\nThe client ID is dynamically set using the service ID from the service info provider.\\nFinally, it builds and returns the configured Kafka producer.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f5d2487100a9",
        "ground_truth": "public TbQueueProducer<TbProtoQueueMsg<ToRuleEngineMsg>> createRuleEngineMsgProducer() {\n    TbKafkaProducerTemplate.TbKafkaProducerTemplateBuilder<TbProtoQueueMsg<ToRuleEngineMsg>> requestBuilder = TbKafkaProducerTemplate.builder();\n    requestBuilder.settings(kafkaSettings);\n    requestBuilder.clientId(\"monolith-rule-engine-\" + serviceInfoProvider.getServiceId());\n    requestBuilder.defaultTopic(topicService.buildTopicName(ruleEngineSettings.getTopic()));\n    requestBuilder.admin(ruleEngineAdmin);\n    return requestBuilder.build();\n}",
        "import_statements": [
            "import com.google.protobuf.util.JsonFormat;",
            "import org.springframework.boot.autoconfigure.condition.ConditionalOnExpression;",
            "import org.springframework.context.annotation.Bean;",
            "import org.springframework.stereotype.Component;",
            "import org.thingsboard.server.common.data.queue.Queue;",
            "import org.thingsboard.server.common.msg.queue.ServiceType;",
            "import org.thingsboard.server.gen.js.JsInvokeProtos;",
            "import org.thingsboard.server.gen.transport.TransportProtos;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToCoreMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToCoreNotificationMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToHousekeeperServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToOtaPackageStateServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToRuleEngineMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToRuleEngineNotificationMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToTransportMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.ToUsageStatsServiceMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.TransportApiRequestMsg;",
            "import org.thingsboard.server.gen.transport.TransportProtos.TransportApiResponseMsg;",
            "import org.thingsboard.server.queue.TbQueueAdmin;",
            "import org.thingsboard.server.queue.TbQueueConsumer;",
            "import org.thingsboard.server.queue.TbQueueProducer;",
            "import org.thingsboard.server.queue.TbQueueRequestTemplate;",
            "import org.thingsboard.server.queue.common.DefaultTbQueueRequestTemplate;",
            "import org.thingsboard.server.queue.common.TbProtoJsQueueMsg;",
            "import org.thingsboard.server.queue.common.TbProtoQueueMsg;",
            "import org.thingsboard.server.queue.discovery.TbServiceInfoProvider;",
            "import org.thingsboard.server.queue.discovery.TopicService;",
            "import org.thingsboard.server.queue.kafka.TbKafkaAdmin;",
            "import org.thingsboard.server.queue.kafka.TbKafkaConsumerStatsService;",
            "import org.thingsboard.server.queue.kafka.TbKafkaConsumerTemplate;",
            "import org.thingsboard.server.queue.kafka.TbKafkaProducerTemplate;",
            "import org.thingsboard.server.queue.kafka.TbKafkaSettings;",
            "import org.thingsboard.server.queue.kafka.TbKafkaTopicConfigs;",
            "import org.thingsboard.server.queue.settings.TbQueueCoreSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueRemoteJsInvokeSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueRuleEngineSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueTransportApiSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueTransportNotificationSettings;",
            "import org.thingsboard.server.queue.settings.TbQueueVersionControlSettings;",
            "import jakarta.annotation.PreDestroy;",
            "import java.nio.charset.StandardCharsets;",
            "import java.util.concurrent.atomic.AtomicLong;"
        ],
        "reference_api": [
            "buildTopicName",
            "admin",
            "clientId",
            "settings",
            "getServiceId",
            "build",
            "builder",
            "getTopic",
            "defaultTopic"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "buildTopicName",
            "admin",
            "clientId",
            "settings",
            "getServiceId",
            "build",
            "builder",
            "getTopic",
            "defaultTopic"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "private void startLogScheduling()",
        "start_line": "82",
        "end_line": "112",
        "file_path": "common/queue/src/main/java/org/thingsboard/server/queue/kafka/TbKafkaConsumerStatsService.java",
        "docstring": "The startLogScheduling function schedules periodic logging of Kafka consumer group statistics.\\nIt sets a timeout duration based on the Kafka response timeout configuration and schedules a task with a fixed delay.\\nThe task checks if statistics printing is required and, for each monitored consumer group, retrieves and compares group offsets and end offsets to identify topics with lag.\\nIf lagging topics are found, it logs their details.\\nIf an exception occurs during the process, it logs a warning with the group ID and error message, along with a detailed error trace.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "886264f5ca27",
        "ground_truth": "private void startLogScheduling() {\n    Duration timeoutDuration = Duration.ofMillis(statsConfig.getKafkaResponseTimeoutMs());\n    statsPrintScheduler.scheduleWithFixedDelay(() -> {\n        if (!isStatsPrintRequired()) {\n            return;\n        }\n        for (String groupId : monitoredGroups) {\n            try {\n                Map<TopicPartition, OffsetAndMetadata> groupOffsets = kafkaSettings.getAdminClient().listConsumerGroupOffsets(groupId).partitionsToOffsetAndMetadata()\n                        .get(statsConfig.getKafkaResponseTimeoutMs(), TimeUnit.MILLISECONDS);\n                Map<TopicPartition, Long> endOffsets = consumer.endOffsets(groupOffsets.keySet(), timeoutDuration);\n                List<GroupTopicStats> lagTopicsStats = getTopicsStatsWithLag(groupOffsets, endOffsets);\n                if (!lagTopicsStats.isEmpty()) {\n                    StringBuilder builder = new StringBuilder();\n                    for (int i = 0; i < lagTopicsStats.size(); i++) {\n                        builder.append(lagTopicsStats.get(i).toString());\n                        if (i != lagTopicsStats.size() - 1) {\n                            builder.append(\", \");\n                        }\n                    }\n                    log.info(\"[{}] Topic partitions with lag: [{}].\", groupId, builder.toString());\n                }\n            } catch (Exception e) {\n                log.warn(\"[{}] Failed to get consumer group stats. Reason - {}.\", groupId, e.getMessage());\n                log.trace(\"Detailed error: \", e);\n            }\n        }\n    }, statsConfig.getPrintIntervalMs(), statsConfig.getPrintIntervalMs(), TimeUnit.MILLISECONDS);\n}",
        "import_statements": [
            "import jakarta.annotation.PostConstruct;",
            "import jakarta.annotation.PreDestroy;",
            "import lombok.Builder;",
            "import lombok.Data;",
            "import lombok.RequiredArgsConstructor;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.kafka.clients.consumer.Consumer;",
            "import org.apache.kafka.clients.consumer.ConsumerConfig;",
            "import org.apache.kafka.clients.consumer.KafkaConsumer;",
            "import org.apache.kafka.clients.consumer.OffsetAndMetadata;",
            "import org.apache.kafka.common.TopicPartition;",
            "import org.springframework.beans.factory.annotation.Autowired;",
            "import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;",
            "import org.springframework.context.annotation.Lazy;",
            "import org.springframework.stereotype.Component;",
            "import org.thingsboard.common.util.ThingsBoardThreadFactory;",
            "import org.thingsboard.server.common.data.StringUtils;",
            "import org.thingsboard.server.common.data.id.TenantId;",
            "import org.thingsboard.server.common.msg.queue.ServiceType;",
            "import org.thingsboard.server.queue.discovery.PartitionService;",
            "import java.time.Duration;",
            "import java.util.ArrayList;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Properties;",
            "import java.util.Set;",
            "import java.util.concurrent.ConcurrentHashMap;",
            "import java.util.concurrent.Executors;",
            "import java.util.concurrent.ScheduledExecutorService;",
            "import java.util.concurrent.TimeUnit;"
        ],
        "reference_api": [
            "getMessage",
            "trace",
            "partitionsToOffsetAndMetadata",
            "endOffsets",
            "toString",
            "listConsumerGroupOffsets",
            "append",
            "info",
            "getAdminClient",
            "size",
            "get",
            "keySet",
            "getKafkaResponseTimeoutMs",
            "scheduleWithFixedDelay",
            "isStatsPrintRequired",
            "getPrintIntervalMs",
            "warn",
            "ofMillis",
            "getTopicsStatsWithLag",
            "isEmpty"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "isStatsPrintRequired",
                "code": "private boolean isStatsPrintRequired() {\n        boolean isMyRuleEnginePartition = partitionService.isMyPartition(ServiceType.TB_RULE_ENGINE, TenantId.SYS_TENANT_ID, TenantId.SYS_TENANT_ID);\n        boolean isMyCorePartition = partitionService.isMyPartition(ServiceType.TB_CORE, TenantId.SYS_TENANT_ID, TenantId.SYS_TENANT_ID);\n        return log.isInfoEnabled() && (isMyRuleEnginePartition || isMyCorePartition);\n    }"
            },
            {
                "name": "getTopicsStatsWithLag",
                "code": "private List<GroupTopicStats> getTopicsStatsWithLag(Map<TopicPartition, OffsetAndMetadata> groupOffsets, Map<TopicPartition, Long> endOffsets) {\n        List<GroupTopicStats> consumerGroupStats = new ArrayList<>();\n        for (TopicPartition topicPartition : groupOffsets.keySet()) {\n            long endOffset = endOffsets.get(topicPartition);\n            long committedOffset = groupOffsets.get(topicPartition).offset();\n            long lag = endOffset - committedOffset;\n            if (lag != 0) {\n                GroupTopicStats groupTopicStats = GroupTopicStats.builder()\n                        .topic(topicPartition.topic())\n                        .partition(topicPartition.partition())\n                        .committedOffset(committedOffset)\n                        .endOffset(endOffset)\n                        .lag(lag)\n                        .build();\n                consumerGroupStats.add(groupTopicStats);\n            }\n        }\n        return consumerGroupStats;\n    }"
            }
        ],
        "third_party": [
            "getMessage",
            "trace",
            "partitionsToOffsetAndMetadata",
            "endOffsets",
            "listConsumerGroupOffsets",
            "append",
            "info",
            "getAdminClient",
            "size",
            "get",
            "keySet",
            "getKafkaResponseTimeoutMs",
            "scheduleWithFixedDelay",
            "getPrintIntervalMs",
            "warn",
            "ofMillis",
            "isEmpty"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "thingsboard/thingsboard",
        "function_declaration": "protected Properties toAdminProps()",
        "start_line": "231",
        "end_line": "236",
        "file_path": "common/queue/src/main/java/org/thingsboard/server/queue/kafka/TbKafkaSettings.java",
        "docstring": "The toAdminProps function creates and returns a Properties object for configuring a Kafka Admin client.\\nIt first calls toProps to get the base properties, then adds the bootstrap servers and retries configurations to the Properties object.\\nFinally, it returns the configured Properties object.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "81553181cdc8",
        "ground_truth": "protected Properties toAdminProps() {\n    Properties props = toProps();\n    props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, servers);\n    props.put(AdminClientConfig.RETRIES_CONFIG, retries);\n    return props;\n}",
        "import_statements": [
            "import lombok.Getter;",
            "import lombok.Setter;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.kafka.clients.CommonClientConfigs;",
            "import org.apache.kafka.clients.admin.AdminClient;",
            "import org.apache.kafka.clients.admin.AdminClientConfig;",
            "import org.apache.kafka.clients.consumer.ConsumerConfig;",
            "import org.apache.kafka.clients.producer.ProducerConfig;",
            "import org.apache.kafka.common.config.SslConfigs;",
            "import org.apache.kafka.common.serialization.ByteArrayDeserializer;",
            "import org.apache.kafka.common.serialization.ByteArraySerializer;",
            "import org.apache.kafka.common.serialization.StringDeserializer;",
            "import org.apache.kafka.common.serialization.StringSerializer;",
            "import org.springframework.beans.factory.annotation.Value;",
            "import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;",
            "import org.springframework.boot.context.properties.ConfigurationProperties;",
            "import org.springframework.stereotype.Component;",
            "import org.thingsboard.server.common.data.TbProperty;",
            "import org.thingsboard.server.queue.util.PropertyUtils;",
            "import javax.annotation.PreDestroy;",
            "import java.util.Collections;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Properties;"
        ],
        "reference_api": [
            "toProps",
            "put"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "toProps",
                "code": "Properties toProps() {\n        Properties props = new Properties();\n\n        if (useConfluent) {\n            props.put(\"ssl.endpoint.identification.algorithm\", sslAlgorithm);\n            props.put(\"sasl.mechanism\", saslMechanism);\n            props.put(\"sasl.jaas.config\", saslConfig);\n            props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n        }\n\n        props.put(CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);\n        props.put(CommonClientConfigs.SESSION_TIMEOUT_MS_CONFIG, sessionTimeoutMs);\n\n        props.putAll(PropertyUtils.getProps(otherInline));\n\n        if (other != null) {\n            other.forEach(kv -> props.put(kv.getKey(), kv.getValue()));\n        }\n\n        configureSSL(props);\n\n        return props;\n    }"
            }
        ],
        "third_party": [
            "put"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "zhisheng17/flink-learning",
        "function_declaration": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group)",
        "start_line": "113",
        "end_line": "129",
        "file_path": "flink-learning-extends/flink-metrics/flink-metrics-kafka/src/main/java/org/apache/flink/metrics/kafka/KafkaReporter.java",
        "docstring": "The notifyOfAddedMetric function handles the addition of a new metric to a monitoring system.\\nIt creates a MetricEvent with a scoped name and tags derived from the metric name and group.\\nDepending on the type of the metric (Counter, Gauge, Histogram, or Meter), it adds the metric and its event to the corresponding collection.\\nIf the metric type is unknown, it logs a warning message indicating that the metric type is not supported.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3c8bf3ff31b7",
        "ground_truth": "public void notifyOfAddedMetric(Metric metric, String metricName, MetricGroup group) {\n MetricEvent metricEvent = new MetricEvent(getScopedName(metricName, group), getTags(group));\n synchronized (this) {\n  if (metric instanceof Counter) {\n   counters.put((Counter) metric, metricEvent);\n  } else if (metric instanceof Gauge) {\n   gauges.put((Gauge<?>) metric, metricEvent);\n  } else if (metric instanceof Histogram) {\n   histograms.put((Histogram) metric, metricEvent);\n  } else if (metric instanceof Meter) {\n   meters.put((Meter) metric, metricEvent);\n  } else {\n   LOG.warn(\"Cannot add unknown metric type {}. This indicates that the reporter \" +\n    \"does not support this metric type.\", metric.getClass().getName());\n  }\n }\n}",
        "import_statements": [
            "import org.apache.flink.annotation.PublicEvolving;",
            "import org.apache.flink.annotation.VisibleForTesting;",
            "import org.apache.flink.metrics.*;",
            "import org.apache.flink.metrics.kafka.util.JacksonUtil;",
            "import org.apache.flink.metrics.reporter.InstantiateViaFactory;",
            "import org.apache.flink.metrics.reporter.MetricReporter;",
            "import org.apache.flink.metrics.reporter.Scheduled;",
            "import org.apache.flink.runtime.metrics.groups.AbstractMetricGroup;",
            "import org.apache.flink.runtime.metrics.groups.FrontMetricGroup;",
            "import org.apache.kafka.clients.producer.KafkaProducer;",
            "import org.apache.kafka.clients.producer.Producer;",
            "import org.apache.kafka.clients.producer.ProducerRecord;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.File;",
            "import java.util.HashMap;",
            "import java.util.Map;",
            "import java.util.Properties;",
            "import java.util.regex.Pattern;",
            "import static org.apache.flink.metrics.kafka.KafkaReporterOptions.*;"
        ],
        "reference_api": [
            "getName",
            "getScopedName",
            "warn",
            "getTags",
            "getClass",
            "put"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getScopedName",
                "code": "private static String getScopedName(String metricName, MetricGroup group) {\n\t\treturn getLogicalScope(group) + SCOPE_SEPARATOR + metricName;\n\t}"
            },
            {
                "name": "getTags",
                "code": "private static Map<String, String> getTags(MetricGroup group) {\n\t\t// Keys are surrounded by brackets: remove them, transforming \"<name>\" to \"name\".\n\t\tMap<String, String> tags = new HashMap<>();\n\t\tfor (Map.Entry<String, String> variable : group.getAllVariables().entrySet()) {\n\t\t\tString name = variable.getKey();\n\t\t\t//remove TaskManager tm_id tag, because we will add container_id tag when report, the two tag value is same\n\t\t\tif (name.contains(\"tm_id\")) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttags.put(name.substring(1, name.length() - 1), variable.getValue());\n\t\t}\n\t\treturn tags;\n\t}"
            }
        ],
        "third_party": [
            "getName",
            "warn",
            "getClass",
            "put"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "zhisheng17/flink-learning",
        "function_declaration": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge)",
        "start_line": "338",
        "end_line": "357",
        "file_path": "flink-learning-extends/flink-metrics/flink-metrics-kafka/src/main/java/org/apache/flink/metrics/kafka/KafkaReporter.java",
        "docstring": "The addFields function adds a field to a MetricEvent based on the value of a Gauge.\\nIt retrieves the value from the Gauge and checks if the MetricEvent already has fields.\\nIf fields exist, it adds the field with the value, converting it to a Number or String as needed.\\nIf no fields exist, it creates a new map for the fields and adds the field with the appropriate value.\\nFinally, it returns the updated MetricEvent.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "77c6addc9b99",
        "ground_truth": "static MetricEvent addFields(MetricEvent metricEvent, String field, Gauge<?> gauge) {\n Object value = gauge.getValue();\n Map<String, Object> fields = metricEvent.getFields();\n if (fields != null) {\n  if (value instanceof Number) {\n   metricEvent.addField(field, (Number) value);\n  } else {\n   metricEvent.addField(field, String.valueOf(value));\n  }\n } else {\n  Map<String, Object> eventFields = new HashMap<>();\n  if (value instanceof Number) {\n   eventFields.put(field, (Number) value);\n  } else {\n   eventFields.put(field, String.valueOf(value));\n  }\n  metricEvent.setFields(eventFields);\n }\n return metricEvent;\n}",
        "import_statements": [
            "import org.apache.flink.annotation.PublicEvolving;",
            "import org.apache.flink.annotation.VisibleForTesting;",
            "import org.apache.flink.metrics.*;",
            "import org.apache.flink.metrics.kafka.util.JacksonUtil;",
            "import org.apache.flink.metrics.reporter.InstantiateViaFactory;",
            "import org.apache.flink.metrics.reporter.MetricReporter;",
            "import org.apache.flink.metrics.reporter.Scheduled;",
            "import org.apache.flink.runtime.metrics.groups.AbstractMetricGroup;",
            "import org.apache.flink.runtime.metrics.groups.FrontMetricGroup;",
            "import org.apache.kafka.clients.producer.KafkaProducer;",
            "import org.apache.kafka.clients.producer.Producer;",
            "import org.apache.kafka.clients.producer.ProducerRecord;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.File;",
            "import java.util.HashMap;",
            "import java.util.Map;",
            "import java.util.Properties;",
            "import java.util.regex.Pattern;",
            "import static org.apache.flink.metrics.kafka.KafkaReporterOptions.*;"
        ],
        "reference_api": [
            "addField",
            "getValue",
            "valueOf",
            "setFields",
            "put",
            "getFields"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "addField",
            "getValue",
            "setFields",
            "put",
            "getFields"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "zhisheng17/flink-learning",
        "function_declaration": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge)",
        "start_line": "359",
        "end_line": "379",
        "file_path": "flink-learning-extends/flink-metrics/flink-metrics-kafka/src/main/java/org/apache/flink/metrics/kafka/KafkaReporter.java",
        "docstring": "The addKafkaLagMetricFields function adds Kafka lag metrics to a MetricEvent object.\\nIt extracts values from a Gauge object, splits them into components, and updates the MetricEvent's fields with current offsets, data timestamp, and committed offsets.\\nThe function checks for changes in the Kafka lag metrics to avoid duplicates, updates the kafkaLagTimes map, and sets the event's timestamp before returning the updated MetricEvent.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a48971ffb3e9",
        "ground_truth": "static MetricEvent addKafkaLagMetricFields(MetricEvent metricEvent, Long timestamp, Gauge<?> gauge) {\n String gaugeValue = (String) gauge.getValue();\n String[] split = gaugeValue.split(\"_\");\n Map<String, String> tags = metricEvent.getTags();\n if (split.length == 3) {\n  Map<String, Object> fields = new HashMap<>(3);\n  fields.put(\"currentOffsets\", Long.valueOf(split[0]));\n  fields.put(\"currentDataTimestamp\", Long.valueOf(split[1]));\n  fields.put(\"committedOffsets\", Long.valueOf(split[2]));\n  String key = tags.get(\"kafka\") + tags.get(\"topic\") + tags.get(\"group\") + tags.get(\"partition\");\n  String value = split[0] + \"_\" +split[1];\n  if (kafkaLagTimes.get(key) != null && kafkaLagTimes.get(key).equals(value)) {\n   return null;\n  } else {\n   kafkaLagTimes.put(key, value);\n  }\n  metricEvent.setFields(fields);\n }\n metricEvent.setTimestamp(timestamp);\n return metricEvent;\n}",
        "import_statements": [
            "import org.apache.flink.annotation.PublicEvolving;",
            "import org.apache.flink.annotation.VisibleForTesting;",
            "import org.apache.flink.metrics.*;",
            "import org.apache.flink.metrics.kafka.util.JacksonUtil;",
            "import org.apache.flink.metrics.reporter.InstantiateViaFactory;",
            "import org.apache.flink.metrics.reporter.MetricReporter;",
            "import org.apache.flink.metrics.reporter.Scheduled;",
            "import org.apache.flink.runtime.metrics.groups.AbstractMetricGroup;",
            "import org.apache.flink.runtime.metrics.groups.FrontMetricGroup;",
            "import org.apache.kafka.clients.producer.KafkaProducer;",
            "import org.apache.kafka.clients.producer.Producer;",
            "import org.apache.kafka.clients.producer.ProducerRecord;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.io.File;",
            "import java.util.HashMap;",
            "import java.util.Map;",
            "import java.util.Properties;",
            "import java.util.regex.Pattern;",
            "import static org.apache.flink.metrics.kafka.KafkaReporterOptions.*;"
        ],
        "reference_api": [
            "equals",
            "getValue",
            "valueOf",
            "split",
            "setTimestamp",
            "get",
            "getTags",
            "setFields",
            "put"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getTags",
                "code": "private static Map<String, String> getTags(MetricGroup group) {\n\t\t// Keys are surrounded by brackets: remove them, transforming \"<name>\" to \"name\".\n\t\tMap<String, String> tags = new HashMap<>();\n\t\tfor (Map.Entry<String, String> variable : group.getAllVariables().entrySet()) {\n\t\t\tString name = variable.getKey();\n\t\t\t//remove TaskManager tm_id tag, because we will add container_id tag when report, the two tag value is same\n\t\t\tif (name.contains(\"tm_id\")) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttags.put(name.substring(1, name.length() - 1), variable.getValue());\n\t\t}\n\t\treturn tags;\n\t}"
            }
        ],
        "third_party": [
            "getValue",
            "setTimestamp",
            "get",
            "setFields",
            "put"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "zhisheng17/flink-learning",
        "function_declaration": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException",
        "start_line": "70",
        "end_line": "83",
        "file_path": "flink-learning-common/src/main/java/com/zhisheng/common/utils/KafkaConfigUtil.java",
        "docstring": "The buildSource function creates a DataStreamSource for MetricEvent using a Flink Kafka consumer.\\nIt retrieves global job parameters from the StreamExecutionEnvironment and builds Kafka properties.\\nA FlinkKafkaConsumer for MetricEvent is instantiated with the topic, schema, and properties.\\nIf a specific start time is provided, it sets the consumer to start from the specific offsets based on the given time.\\nFinally, it adds the consumer as a source to the StreamExecutionEnvironment and returns the DataStreamSource.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c2080c299bb0",
        "ground_truth": "public static DataStreamSource<MetricEvent> buildSource(StreamExecutionEnvironment env, String topic, Long time) throws IllegalAccessException {\n    ParameterTool parameterTool = (ParameterTool) env.getConfig().getGlobalJobParameters();\n    Properties props = buildKafkaProps(parameterTool);\n    FlinkKafkaConsumer<MetricEvent> consumer = new FlinkKafkaConsumer<>(\n            topic,\n            new MetricSchema(),\n            props);\n    //\u91cd\u7f6eoffset\u5230time\u65f6\u523b\n    if (time != 0L) {\n        Map<KafkaTopicPartition, Long> partitionOffset = buildOffsetByTime(props, parameterTool, time);\n        consumer.setStartFromSpecificOffsets(partitionOffset);\n    }\n    return env.addSource(consumer);\n}",
        "import_statements": [
            "import com.zhisheng.common.constant.PropertiesConstants;",
            "import com.zhisheng.common.model.MetricEvent;",
            "import com.zhisheng.common.schemas.MetricSchema;",
            "import org.apache.flink.api.java.utils.ParameterTool;",
            "import org.apache.flink.streaming.api.datastream.DataStreamSource;",
            "import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;",
            "import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;",
            "import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;",
            "import org.apache.kafka.clients.consumer.KafkaConsumer;",
            "import org.apache.kafka.clients.consumer.OffsetAndTimestamp;",
            "import org.apache.kafka.common.PartitionInfo;",
            "import org.apache.kafka.common.TopicPartition;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Properties;",
            "import static com.zhisheng.common.constant.PropertiesConstants.*;"
        ],
        "reference_api": [
            "getConfig",
            "getGlobalJobParameters",
            "setStartFromSpecificOffsets",
            "addSource",
            "buildOffsetByTime",
            "buildKafkaProps"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "buildOffsetByTime",
                "code": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time) {\n        props.setProperty(\"group.id\", \"query_time_\" + time);\n        KafkaConsumer consumer = new KafkaConsumer(props);\n        List<PartitionInfo> partitionsFor = consumer.partitionsFor(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC));\n        Map<TopicPartition, Long> partitionInfoLongMap = new HashMap<>();\n        for (PartitionInfo partitionInfo : partitionsFor) {\n            partitionInfoLongMap.put(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()), time);\n        }\n        Map<TopicPartition, OffsetAndTimestamp> offsetResult = consumer.offsetsForTimes(partitionInfoLongMap);\n        Map<KafkaTopicPartition, Long> partitionOffset = new HashMap<>();\n        offsetResult.forEach((key, value) -> partitionOffset.put(new KafkaTopicPartition(key.topic(), key.partition()), value.offset()));\n\n        consumer.close();\n        return partitionOffset;\n    }"
            },
            {
                "name": "buildKafkaProps",
                "code": "public static Properties buildKafkaProps() {\n        return buildKafkaProps(ParameterTool.fromSystemProperties());\n    }"
            }
        ],
        "third_party": [
            "getConfig",
            "getGlobalJobParameters",
            "setStartFromSpecificOffsets",
            "addSource"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "zhisheng17/flink-learning",
        "function_declaration": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time)",
        "start_line": "85",
        "end_line": "99",
        "file_path": "flink-learning-common/src/main/java/com/zhisheng/common/utils/KafkaConfigUtil.java",
        "docstring": "The buildOffsetByTime function calculates the offsets for each Kafka topic partition based on a specified timestamp.\\nIt sets the consumer group ID using the provided time and creates a KafkaConsumer with the properties.\\nIt retrieves partition information for the specified topic and creates a map associating each partition with the timestamp.\\nThe consumer fetches offsets for the given times, and the function builds a map of KafkaTopicPartition to offsets.\\nAfter fetching the offsets, the consumer is closed and the map of partition offsets is returned.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5e828afbe8e4",
        "ground_truth": "private static Map<KafkaTopicPartition, Long> buildOffsetByTime(Properties props, ParameterTool parameterTool, Long time) {\n    props.setProperty(\"group.id\", \"query_time_\" + time);\n    KafkaConsumer consumer = new KafkaConsumer(props);\n    List<PartitionInfo> partitionsFor = consumer.partitionsFor(parameterTool.getRequired(PropertiesConstants.METRICS_TOPIC));\n    Map<TopicPartition, Long> partitionInfoLongMap = new HashMap<>();\n    for (PartitionInfo partitionInfo : partitionsFor) {\n        partitionInfoLongMap.put(new TopicPartition(partitionInfo.topic(), partitionInfo.partition()), time);\n    }\n    Map<TopicPartition, OffsetAndTimestamp> offsetResult = consumer.offsetsForTimes(partitionInfoLongMap);\n    Map<KafkaTopicPartition, Long> partitionOffset = new HashMap<>();\n    offsetResult.forEach((key, value) -> partitionOffset.put(new KafkaTopicPartition(key.topic(), key.partition()), value.offset()));\n    consumer.close();\n    return partitionOffset;\n}",
        "import_statements": [
            "import com.zhisheng.common.constant.PropertiesConstants;",
            "import com.zhisheng.common.model.MetricEvent;",
            "import com.zhisheng.common.schemas.MetricSchema;",
            "import org.apache.flink.api.java.utils.ParameterTool;",
            "import org.apache.flink.streaming.api.datastream.DataStreamSource;",
            "import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;",
            "import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;",
            "import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition;",
            "import org.apache.kafka.clients.consumer.KafkaConsumer;",
            "import org.apache.kafka.clients.consumer.OffsetAndTimestamp;",
            "import org.apache.kafka.common.PartitionInfo;",
            "import org.apache.kafka.common.TopicPartition;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Properties;",
            "import static com.zhisheng.common.constant.PropertiesConstants.*;"
        ],
        "reference_api": [
            "partition",
            "setProperty",
            "offset",
            "forEach",
            "close",
            "getRequired",
            "partitionsFor",
            "offsetsForTimes",
            "put",
            "topic"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "partition",
            "setProperty",
            "offset",
            "forEach",
            "close",
            "getRequired",
            "partitionsFor",
            "offsetsForTimes",
            "put",
            "topic"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "zhisheng17/flink-learning",
        "function_declaration": "public static void writeToKafka() throws InterruptedException",
        "start_line": "21",
        "end_line": "50",
        "file_path": "flink-learning-basic/flink-learning-data-sources/src/main/java/com/zhisheng/data/sources/utils/KafkaUtil.java",
        "docstring": "The writeToKafka function sends a MetricEvent to a Kafka topic.\\nIt sets up Kafka producer properties, including the bootstrap servers and serializers for keys and values.\\nA KafkaProducer instance is created with these properties.\\nA MetricEvent is created and populated with the current timestamp, a name, tags, and fields.\\nA ProducerRecord is created with the serialized MetricEvent and sent to the specified topic.\\nThe function prints the sent data and flushes the producer to ensure all data is sent.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "28da89f56f67",
        "ground_truth": "public static void writeToKafka() throws InterruptedException {\n    Properties props = new Properties();\n    props.put(\"bootstrap.servers\", broker_list);\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); //key \u5e8f\u5217\u5316\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); //value \u5e8f\u5217\u5316\n    KafkaProducer producer = new KafkaProducer<String, String>(props);\n    MetricEvent metric = new MetricEvent();\n    metric.setTimestamp(System.currentTimeMillis());\n    metric.setName(\"mem\");\n    Map<String, String> tags = new HashMap<>();\n    Map<String, Object> fields = new HashMap<>();\n    tags.put(\"cluster\", \"zhisheng\");\n    tags.put(\"host_ip\", \"101.147.022.106\");\n    fields.put(\"used_percent\", 90d);\n    fields.put(\"max\", 27244873d);\n    fields.put(\"used\", 17244873d);\n    fields.put(\"init\", 27244873d);\n    metric.setTags(tags);\n    metric.setFields(fields);\n    ProducerRecord record = new ProducerRecord<String, String>(topic, null, null, GsonUtil.toJson(metric));\n    producer.send(record);\n    System.out.println(\"\u53d1\u9001\u6570\u636e: \" + GsonUtil.toJson(metric));\n    producer.flush();\n}",
        "import_statements": [
            "import com.zhisheng.common.model.MetricEvent;",
            "import com.zhisheng.common.utils.GsonUtil;",
            "import org.apache.kafka.clients.producer.KafkaProducer;",
            "import org.apache.kafka.clients.producer.ProducerRecord;",
            "import java.util.HashMap;",
            "import java.util.Map;",
            "import java.util.Properties;"
        ],
        "reference_api": [
            "setName",
            "setTags",
            "setTimestamp",
            "toJson",
            "send",
            "currentTimeMillis",
            "println",
            "flush",
            "setFields",
            "put"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "setName",
            "setTags",
            "setTimestamp",
            "toJson",
            "send",
            "currentTimeMillis",
            "println",
            "flush",
            "setFields",
            "put"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "zhisheng17/flink-learning",
        "function_declaration": "public void append(org.apache.logging.log4j.core.LogEvent event)",
        "start_line": "147",
        "end_line": "155",
        "file_path": "flink-learning-extends/FlinkLogKafkaAppender/Log4j2KafkaAppender/src/main/java/com/zhisheng/log/appender/KafkaLog4j2Appender.java",
        "docstring": "The append function processes and sends log events to a Kafka topic.\\nIt checks if the event's level is in a specified list and if the logger name does not contain \"xxx\".\\nIf these conditions are met, it sends the event to the Kafka topic using a Kafka producer.\\nIf an exception occurs during processing or sending, it logs a warning message.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "14bffe09c25a",
        "ground_truth": "public void append(org.apache.logging.log4j.core.LogEvent event) {\n    try {\n        if (level.contains(event.getLevel().toString().toUpperCase()) && !event.getLoggerName().contains(\"xxx\")) { //\u63a7\u5236\u54ea\u4e9b\u7c7b\u7684\u65e5\u5fd7\u4e0d\u6536\u96c6\n            producer.send(new ProducerRecord<>(topic, appId, subAppend(event)));\n        }\n    } catch (Exception e) {\n        log.warn(\"Parsing the log event or send log event to kafka has exception\", e);\n    }\n}",
        "import_statements": [
            "import com.zhisheng.flink.model.LogEvent;",
            "import com.zhisheng.flink.util.ExceptionUtil;",
            "import com.zhisheng.flink.util.JacksonUtil;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.core.JsonProcessingException;",
            "import org.apache.kafka.clients.producer.KafkaProducer;",
            "import org.apache.kafka.clients.producer.Producer;",
            "import org.apache.kafka.clients.producer.ProducerRecord;",
            "import org.apache.kafka.common.config.ConfigException;",
            "import org.apache.logging.log4j.core.Filter;",
            "import org.apache.logging.log4j.core.Layout;",
            "import org.apache.logging.log4j.core.appender.AbstractAppender;",
            "import org.apache.logging.log4j.core.config.Property;",
            "import org.apache.logging.log4j.core.config.plugins.Plugin;",
            "import org.apache.logging.log4j.core.config.plugins.PluginAttribute;",
            "import org.apache.logging.log4j.core.config.plugins.PluginElement;",
            "import org.apache.logging.log4j.core.config.plugins.PluginFactory;",
            "import org.apache.logging.log4j.core.config.plugins.validation.constraints.Required;",
            "import java.io.File;",
            "import java.io.Serializable;",
            "import java.net.InetAddress;",
            "import java.util.HashMap;",
            "import java.util.Map;",
            "import java.util.Properties;",
            "import java.util.UUID;"
        ],
        "reference_api": [
            "subAppend",
            "getLevel",
            "toString",
            "toUpperCase",
            "send",
            "getLoggerName",
            "warn",
            "contains"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "subAppend",
                "code": "private String subAppend(org.apache.logging.log4j.core.LogEvent event) throws JsonProcessingException {\n        LogEvent logEvent = new LogEvent();\n        Map<String, String> tags = new HashMap<>();\n        String logMessage = null;\n        try {\n            InetAddress inetAddress = InetAddress.getLocalHost();\n            tags.put(\"host_name\", inetAddress.getHostName());\n            tags.put(\"host_ip\", inetAddress.getHostAddress());\n        } catch (Exception e) {\n            log.error(\"Error getting the ip and host name of the node where the job({}) is running\", appId, e);\n        } finally {\n            try {\n                logMessage = ExceptionUtil.stacktraceToString(event.getThrown());\n                logEvent.setContent(logMessage);\n            } catch (Exception e) {\n                if (logMessage != null) {\n                    logMessage = logMessage + \"\\n\\t\" + e.getMessage();\n                }\n                logEvent.setContent(logMessage);\n            } finally {\n                logEvent.setId(UUID.randomUUID().toString());\n                logEvent.setTimestamp(event.getTimeMillis());\n                logEvent.setSource(source);\n                if (logMessage != null) {\n                    logMessage = event.getMessage().getFormattedMessage() + \"\\n\" + logMessage;\n                } else {\n                    logMessage = event.getMessage().getFormattedMessage();\n                }\n                logEvent.setContent(logMessage);\n\n                StackTraceElement eventSource = event.getSource();\n                tags.put(\"class_name\", eventSource.getClassName());\n                tags.put(\"method_name\", eventSource.getMethodName());\n                tags.put(\"file_name\", eventSource.getFileName());\n                tags.put(\"line_number\", String.valueOf(eventSource.getLineNumber()));\n\n                tags.put(\"logger_name\", event.getLoggerName());\n                tags.put(\"level\", event.getLevel().toString());\n                tags.put(\"thread_name\", event.getThreadName());\n                tags.put(\"app_id\", appId);\n                tags.put(\"container_id\", containerId);\n                tags.put(\"container_type\", containerType);\n                if (taskId != null) {\n                    tags.put(\"task_id\", taskId);\n                }\n                if (taskName != null) {\n                    tags.put(\"task_name\", taskName);\n                }\n                if (nodeIp != null) {\n                    tags.put(\"node_ip\", nodeIp);\n                }\n                logEvent.setTags(tags);\n            }\n        }\n        return JacksonUtil.toJson(logEvent);\n    }"
            }
        ],
        "third_party": [
            "getLevel",
            "toUpperCase",
            "send",
            "getLoggerName",
            "warn"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "influxdata/telegraf",
        "function_declaration": "func ValidateTopicSuffixMethod(method string) error",
        "start_line": "71",
        "end_line": "78",
        "file_path": "plugins/outputs/kafka/kafka.go",
        "docstring": "The ValidateTopicSuffixMethod function checks if a provided method is valid.\\nIt iterates through a list of valid topic suffix methods.\\nIf the method matches one of the valid methods, it returns nil (no error).\\nIf the method is not valid, it returns an error indicating the unknown topic suffix method.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a9e0786fb5c1",
        "ground_truth": "func ValidateTopicSuffixMethod(method string) error {\n for _, validMethod := range ValidTopicSuffixMethods {\n  if method == validMethod {\n   return nil\n  }\n }\n return fmt.Errorf(\"unknown topic suffix method provided: %s\", method)\n}",
        "import_statements": [
            "import (\n\t_ \"embed\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/gofrs/uuid/v5\"\n\n\t\"github.com/influxdata/telegraf\"\n\t\"github.com/influxdata/telegraf/internal\"\n\t\"github.com/influxdata/telegraf/plugins/common/kafka\"\n\t\"github.com/influxdata/telegraf/plugins/common/proxy\"\n\t\"github.com/influxdata/telegraf/plugins/outputs\"\n\t\"github.com/influxdata/telegraf/plugins/serializers\"\n)"
        ],
        "reference_api": [
            "fmt.Errorf"
        ],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "kafka",
        "owner/repo": "influxdata/telegraf",
        "function_declaration": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) ",
        "start_line": "174",
        "end_line": "191",
        "file_path": "plugins/outputs/kafka/kafka.go",
        "docstring": "The routingKey function generates a routing key for a Kafka message based on a metric.\\nIf a RoutingTag is specified and present in the metric's tags, it returns the value of that tag as the key.\\nIf the RoutingKey is set to \"random\", it generates and returns a new UUID.\\nOtherwise, it returns the RoutingKey as the key.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9b88d720d48a",
        "ground_truth": "func (k *Kafka) routingKey(metric telegraf.Metric) (string, error) {\n if k.RoutingTag != \"\" {\n  key, ok := metric.GetTag(k.RoutingTag)\n  if ok {\n   return key, nil\n  }\n }\n  if k.RoutingKey == \"random\" {\n  u, err := uuid.NewV4()\n  if err != nil {\n   return \"\", err\n  }\n  return u.String(), nil\n }\n  return k.RoutingKey, nil\n}",
        "import_statements": [
            "import (\n\t_ \"embed\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\t\"github.com/gofrs/uuid/v5\"\n\n\t\"github.com/influxdata/telegraf\"\n\t\"github.com/influxdata/telegraf/internal\"\n\t\"github.com/influxdata/telegraf/plugins/common/kafka\"\n\t\"github.com/influxdata/telegraf/plugins/common/proxy\"\n\t\"github.com/influxdata/telegraf/plugins/outputs\"\n\t\"github.com/influxdata/telegraf/plugins/serializers\"\n)"
        ],
        "reference_api": [
            "u.String",
            "metric.GetTag",
            "uuid.NewV4"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "u.String",
            "metric.GetTag",
            "uuid.NewV4"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "influxdata/telegraf",
        "function_declaration": "func (k *KafkaConsumer) compileTopicRegexps() error",
        "start_line": "189",
        "end_line": "203",
        "file_path": "plugins/inputs/kafka_consumer/kafka_consumer.go",
        "docstring": "The compileTopicRegexps function compiles a list of regular expressions for a KafkaConsumer.\\nIt initializes the regexps slice based on the length of the TopicRegexps list.\\nFor each regular expression string, it attempts to compile it.\\nIf compilation fails, it returns an error with details about the specific regular expression.\\nIf successful, it adds the compiled regular expression to the regexps slice and returns nil.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "dc35e80e7f8d",
        "ground_truth": "func (k *KafkaConsumer) compileTopicRegexps() error {\n // While we can add new topics matching extant regexps, we can't\n // update that list on the fly.  We compile them once at startup.\n // Changing them is a configuration change and requires a restart.\n  k.regexps = make([]regexp.Regexp, 0, len(k.TopicRegexps))\n for _, r := range k.TopicRegexps {\n  re, err := regexp.Compile(r)\n  if err != nil {\n   return fmt.Errorf(\"regular expression %q did not compile: '%w\", r, err)\n  }\n  k.regexps = append(k.regexps, *re)\n }\n return nil\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t_ \"embed\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\n\t\"github.com/influxdata/telegraf\"\n\t\"github.com/influxdata/telegraf/config\"\n\t\"github.com/influxdata/telegraf/internal\"\n\t\"github.com/influxdata/telegraf/plugins/common/kafka\"\n\t\"github.com/influxdata/telegraf/plugins/inputs\"\n)"
        ],
        "reference_api": [
            "fmt.Errorf",
            "len",
            "make",
            "regexp.Compile",
            "append"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "len",
            "make",
            "regexp.Compile",
            "append"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "influxdata/telegraf",
        "function_declaration": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo)",
        "start_line": "446",
        "end_line": "462",
        "file_path": "plugins/inputs/kafka_consumer/kafka_consumer.go",
        "docstring": "The onDelivery function handles message delivery tracking in a ConsumerGroupHandler.\\nIt locks the handler to ensure thread safety and retrieves the message associated with the given delivery ID from the undelivered map.\\nIf the message is found and marked as delivered, it marks the message as delivered in the session.\\nIt then removes the message from the undelivered map and releases a semaphore slot.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "daf09ba144b2",
        "ground_truth": "func (h *ConsumerGroupHandler) onDelivery(track telegraf.DeliveryInfo) {\n h.mu.Lock()\n defer h.mu.Unlock()\n  msg, ok := h.undelivered[track.ID()]\n if !ok {\n  h.log.Errorf(\"Could not mark message delivered: %d\", track.ID())\n  return\n }\n  if track.Delivered() {\n  msg.session.MarkMessage(msg.message, \"\")\n }\n  delete(h.undelivered, track.ID())\n <-h.sem\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t_ \"embed\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\n\t\"github.com/influxdata/telegraf\"\n\t\"github.com/influxdata/telegraf/config\"\n\t\"github.com/influxdata/telegraf/internal\"\n\t\"github.com/influxdata/telegraf/plugins/common/kafka\"\n\t\"github.com/influxdata/telegraf/plugins/inputs\"\n)"
        ],
        "reference_api": [
            "h.mu.Lock",
            "track.ID",
            "delete",
            "h.mu.Unlock",
            "msg.session.MarkMessage",
            "track.Delivered",
            "h.log.Errorf"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "h.mu.Lock",
            "track.ID",
            "delete",
            "h.mu.Unlock",
            "msg.session.MarkMessage",
            "track.Delivered",
            "h.log.Errorf"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "influxdata/telegraf",
        "function_declaration": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error",
        "start_line": "538",
        "end_line": "560",
        "file_path": "plugins/inputs/kafka_consumer/kafka_consumer.go",
        "docstring": "The ConsumeClaim function processes messages from a Kafka consumer group claim.\\nIt obtains the session context and enters a loop to reserve processing capacity.\\nIf reservation fails, it returns the error.\\nWithin the loop, it waits for the context to be done or for a message from the claim.\\nIf the context is done or the claim is closed, it exits the loop.\\nFor each message, it handles the message and logs any errors encountered during handling.",
        "language": "Go",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8ecf2a1efead",
        "ground_truth": "func (h *ConsumerGroupHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {\n ctx := session.Context()\n  for {\n  err := h.Reserve(ctx)\n  if err != nil {\n   return err\n  }\n   select {\n  case <-ctx.Done():\n   return nil\n  case msg, ok := <-claim.Messages():\n   if !ok {\n    return nil\n   }\n   err := h.Handle(session, msg)\n   if err != nil {\n    h.acc.AddError(err)\n   }\n  }\n }\n}",
        "import_statements": [
            "import (\n\t\"context\"\n\t_ \"embed\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/IBM/sarama\"\n\n\t\"github.com/influxdata/telegraf\"\n\t\"github.com/influxdata/telegraf/config\"\n\t\"github.com/influxdata/telegraf/internal\"\n\t\"github.com/influxdata/telegraf/plugins/common/kafka\"\n\t\"github.com/influxdata/telegraf/plugins/inputs\"\n)"
        ],
        "reference_api": [
            "session.Context",
            "h.Handle",
            "h.acc.AddError",
            "ctx.Done",
            "claim.Messages",
            "h.Reserve"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "session.Context",
            "h.Handle",
            "h.acc.AddError",
            "ctx.Done",
            "claim.Messages",
            "h.Reserve"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "yahoo/CMAK",
        "function_declaration": "def validateLogkafkaId(logkafka_id: String)",
        "start_line": "53",
        "end_line": "63",
        "file_path": "app/kafka/manager/utils/Logkafka.scala",
        "docstring": "The validateLogkafkaId function validates a given Logkafka ID based on specific conditions.\\nIt checks that the ID is not empty, not equal to \".\" or \"..\", and does not exceed the maximum allowed length.\\nIt uses a regular expression to ensure the ID contains only valid characters.\\nIf any condition fails, it triggers an appropriate error.",
        "language": "Scala",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "eba9dafa99ab",
        "ground_truth": "def validateLogkafkaId(logkafka_id: String) {\n  checkCondition(logkafka_id.length > 0, LogkafkaIdEmpty)\n  checkCondition(!(logkafka_id.equals(\".\") || logkafka_id.equals(\"..\")), InvalidLogkafkaId)\n  checkCondition(logkafka_id.length <= maxNameLength, InvalidLogkafkaIdLength)\n  rgx.findFirstIn(logkafka_id) match {\n    case Some(t) =>\n      checkCondition(t.equals(logkafka_id), IllegalCharacterInLogkafkaId(logkafka_id))\n    case None =>\n      checkCondition(false, IllegalCharacterInLogkafkaId(logkafka_id))\n  }\n}",
        "import_statements": [],
        "reference_api": [
            "rgx.findFirstIn",
            "t.equals",
            "IllegalCharacterInLogkafkaId",
            "logkafka_id.equals",
            "checkCondition"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "IllegalCharacterInLogkafkaId",
                "code": "IllegalCharacterInLogkafkaId"
            },
            {
                "name": "checkCondition",
                "code": "def checkCondition(cond: Boolean, error: UtilError) : Unit = {\n    if(!cond) {\n      throw new UtilException(error)\n    }\n  }"
            }
        ],
        "third_party": [
            "rgx.findFirstIn",
            "t.equals",
            "logkafka_id.equals"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "yahoo/CMAK",
        "function_declaration": "  private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =",
        "start_line": "200",
        "end_line": "222",
        "file_path": "app/kafka/manager/KafkaManager.scala",
        "docstring": "The tryWithKafkaManagerActor function sends a message to the Kafka manager actor and processes the response.\\nIt takes an input message and a function to transform the output, returning a Future containing either an ApiError or the transformed output.\\nIf the actor responds with an error, it logs the failure and returns an ApiError.\\nIf the response is successful, it applies the transformation function and handles any exceptions, returning either the transformed output or an ApiError.\\nThe function also includes error handling for any thrown exceptions during the process.",
        "language": "Scala",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "45d5aefd3f6c",
        "ground_truth": "private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n  (fn: Output => FOutput)\n  (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n{\n  implicit val ec = apiExecutionContext\n  system.actorSelection(kafkaManagerActor).ask(msg).map {\n    case err: ActorErrorResponse => \n      error(s\"Failed on input : $msg\")\n      -\\/(ApiError.from(err))\n    case o: Output =>\n      Try {\n        fn(o)\n      } match {\n        case Failure(t) => \n          error(s\"Failed on input : $msg\")\n          -\\/(ApiError.fromThrowable(t))\n        case Success(foutput) => \\/-(foutput)\n      }\n  }.recover { case t: Throwable =>\n    error(s\"Failed on input : $msg\", t)\n    -\\/(ApiError.fromThrowable(t))\n  }\n}",
        "import_statements": [],
        "reference_api": [
            "-\\/",
            "Try",
            "fn",
            "\\/-",
            "system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse => \n        error(s\"Failed on input : $msg\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) => \n            error(s\"Failed on input : $msg\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover",
            "error",
            "system.actorSelection(kafkaManagerActor).ask",
            "ApiError.from",
            "system.actorSelection(kafkaManagerActor).ask(msg).map",
            "system.actorSelection",
            "ApiError.fromThrowable"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "fn",
                "code": "fn"
            },
            {
                "name": "error",
                "code": "error"
            }
        ],
        "third_party": [
            "-\\/",
            "\\/-",
            "system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse => \n        error(s\"Failed on input : $msg\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) => \n            error(s\"Failed on input : $msg\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover",
            "system.actorSelection(kafkaManagerActor).ask",
            "ApiError.from",
            "system.actorSelection(kafkaManagerActor).ask(msg).map",
            "system.actorSelection",
            "ApiError.fromThrowable"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "yahoo/CMAK",
        "function_declaration": "  private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => Future[ApiError \\/ FOutput])\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =",
        "start_line": "224",
        "end_line": "236",
        "file_path": "app/kafka/manager/KafkaManager.scala",
        "docstring": "The withKafkaManagerActor function sends a message to the Kafka Manager actor and processes the response.\\nIt takes an input message and a function to handle the output.\\nIt sends the message to the Kafka Manager actor and applies the provided function to the response if it matches the expected output type.\\nIf an ActorErrorResponse is received, it returns an ApiError.\\nIf a throwable occurs, it recovers by returning an ApiError from the throwable.",
        "language": "Scala",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a535ce861222",
        "ground_truth": "private[this] def withKafkaManagerActor[Input, Output, FOutput](msg: Input)\n  (fn: Output => Future[ApiError \\/ FOutput])\n  (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n{\n  implicit val ec = apiExecutionContext\n  system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n    case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n    case o: Output =>\n      fn(o)\n  }.recover { case t: Throwable =>\n    -\\/(ApiError.fromThrowable(t))\n  }\n}",
        "import_statements": [],
        "reference_api": [
            "-\\/",
            "fn",
            "Future.successful",
            "system.actorSelection(kafkaManagerActor).ask",
            "system.actorSelection(kafkaManagerActor).ask(msg).flatMap",
            "ApiError.from",
            "system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover",
            "system.actorSelection",
            "ApiError.fromThrowable"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "fn",
                "code": "fn"
            }
        ],
        "third_party": [
            "-\\/",
            "system.actorSelection(kafkaManagerActor).ask",
            "system.actorSelection(kafkaManagerActor).ask(msg).flatMap",
            "ApiError.from",
            "system.actorSelection(kafkaManagerActor).ask(msg).flatMap {\n      case err: ActorErrorResponse => Future.successful(-\\/(ApiError.from(err)))\n      case o: Output =>\n        fn(o)\n    }.recover",
            "system.actorSelection",
            "ApiError.fromThrowable"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "yahoo/CMAK",
        "function_declaration": "  def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] =",
        "start_line": "351",
        "end_line": "361",
        "file_path": "app/kafka/manager/KafkaManager.scala",
        "docstring": "The runPreferredLeaderElection function initiates a preferred leader election for specified topics in a given cluster.\\nIt constructs a KMClusterCommandRequest with the cluster name and topics, then sends it to the Kafka manager actor.\\nThe function returns a Future that maps the command result to a disjunction, indicating success or an error.",
        "language": "Scala",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ad37d3914acc",
        "ground_truth": "def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n  implicit val ec = apiExecutionContext\n  withKafkaManagerActor(\n    KMClusterCommandRequest(\n      clusterName,\n      CMRunPreferredLeaderElection(topics)\n    )\n  ) { result: Future[CMCommandResult] =>\n    result.map(cmr => toDisjunction(cmr.result))\n  }\n}",
        "import_statements": [],
        "reference_api": [],
        "repo_defined_api_with_code": [],
        "third_party": []
    },
    {
        "subclass": "kafka",
        "owner/repo": "yahoo/CMAK",
        "function_declaration": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) =",
        "start_line": "363",
        "end_line": "373",
        "file_path": "app/kafka/manager/KafkaManager.scala",
        "docstring": "The runPreferredLeaderElectionWithAllTopics function initiates a preferred leader election for all topics in a specified cluster.\\nIt uses an implicit execution context for asynchronous operations.\\nThe function retrieves the list of topics for the given cluster and, if successful, runs the preferred leader election for the set of topics.\\nIf retrieving the topic list fails, it returns the error.",
        "language": "Scala",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3ecdb2c1534f",
        "ground_truth": "private def runPreferredLeaderElectionWithAllTopics(clusterName: String) = {\n  implicit val ec = apiExecutionContext\n  getTopicList(clusterName).flatMap { errorOrTopicList =>\n    errorOrTopicList.fold({ e =>\n      Future.successful(-\\/(e))\n    }, { topicList =>\n      runPreferredLeaderElection(clusterName, topicList.list.toSet)\n    })\n  }\n}",
        "import_statements": [],
        "reference_api": [
            "-\\/",
            "Future.successful",
            "errorOrTopicList.fold",
            "runPreferredLeaderElection",
            "getTopicList",
            "getTopicList(clusterName).flatMap"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "runPreferredLeaderElection",
                "code": "def runPreferredLeaderElection(clusterName: String, topics: Set[String]): Future[ApiError \\/ ClusterContext] = {\n    implicit val ec = apiExecutionContext\n    withKafkaManagerActor(\n      KMClusterCommandRequest(\n        clusterName,\n        CMRunPreferredLeaderElection(topics)\n      )\n    ) { result: Future[CMCommandResult] =>\n      result.map(cmr => toDisjunction(cmr.result))\n    }\n  }"
            },
            {
                "name": "getTopicList",
                "code": "def getTopicList(clusterName: String): Future[ApiError \\/ TopicList] = {\n    tryWithKafkaManagerActor(KMClusterQueryRequest(clusterName, KSGetTopics))(identity[TopicList])\n  }"

            }
        ],
        "third_party": [
            "-\\/",
            "errorOrTopicList.fold",
            "getTopicList(clusterName).flatMap"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "yahoo/CMAK",
        "function_declaration": "def manualPartitionAssignments(clusterName: String,\n                                 assignments: List[(String, List[(Int, List[Int])])]) = ",
        "start_line": "411",
        "end_line": "431",
        "file_path": "app/kafka/manager/KafkaManager.scala",
        "docstring": "The manualPartitionAssignments function manually assigns partitions to brokers for a specified cluster.\\nIt uses an implicit execution context for asynchronous operations and sends a KMClusterCommandRequest with the cluster name and partition assignments to the Kafka Manager actor.\\nThe function processes the command results, collecting any errors.\\nIf there are no errors, it returns a success indicator; otherwise, it returns the errors.",
        "language": "Scala",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5955ade118b9",
        "ground_truth": "def manualPartitionAssignments(clusterName: String,\n                               assignments: List[(String, List[(Int, List[Int])])]) = {\n  implicit val ec = apiExecutionContext\n  val results = tryWithKafkaManagerActor(\n    KMClusterCommandRequest (\n      clusterName,\n      CMManualPartitionAssignments(assignments)\n    )\n  ) { result: CMCommandResults =>\n    val errors = result.result.collect { case Failure(t) => ApiError(t.getMessage)}\n    if (errors.isEmpty)\n      \\/-({})\n    else\n      -\\/(errors)\n  }\n  results.map {\n    case -\\/(e) => -\\/(IndexedSeq(e))\n    case \\/-(lst) => lst\n  }\n}",
        "import_statements": [],
        "reference_api": [
            "-\\/",
            "\\/-",
            "result",
            "result.result.collect",
            "IndexedSeq",
            "KMClusterCommandRequest",
            "CMManualPartitionAssignments",
            "tryWithKafkaManagerActor",
            "tryWithKafkaManagerActor(\n      KMClusterCommandRequest (\n        clusterName,\n        CMManualPartitionAssignments(assignments)\n      )\n    )",
            "results.map",
            "ApiError"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "result",
                "code": "result"
            },
            {
                "name": "IndexedSeq",
                "code": "IndexedSeq"
            },
            {
                "name": "KMClusterCommandRequest",
                "code": "KMClusterCommandRequest"
            },
            {
                "name": "CMManualPartitionAssignments",
                "code": "CMManualPartitionAssignments"
            },
            {
                "name": "tryWithKafkaManagerActor",
                "code": "private[this] def tryWithKafkaManagerActor[Input, Output, FOutput](msg: Input)\n    (fn: Output => FOutput)\n    (implicit tag: ClassTag[Output]): Future[ApiError \\/ FOutput] =\n  {\n    implicit val ec = apiExecutionContext\n    system.actorSelection(kafkaManagerActor).ask(msg).map {\n      case err: ActorErrorResponse => \n        error(s\"Failed on input : $msg\")\n        -\\/(ApiError.from(err))\n      case o: Output =>\n        Try {\n          fn(o)\n        } match {\n          case Failure(t) => \n            error(s\"Failed on input : $msg\")\n            -\\/(ApiError.fromThrowable(t))\n          case Success(foutput) => \\/-(foutput)\n        }\n    }.recover { case t: Throwable =>\n      error(s\"Failed on input : $msg\", t)\n      -\\/(ApiError.fromThrowable(t))\n    }\n  }"
            },
            {
                "name": "ApiError",
                "code": "ApiError"
            }
        ],
        "third_party": [
            "-\\/",
            "\\/-",
            "result.result.collect",
            "tryWithKafkaManagerActor(\n      KMClusterCommandRequest (\n        clusterName,\n        CMManualPartitionAssignments(assignments)\n      )\n    )",
            "results.map"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "debezium/debezium",
        "function_declaration": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception",
        "start_line": "597",
        "end_line": "613",
        "file_path": "debezium-storage/debezium-storage-kafka/src/main/java/io/debezium/storage/kafka/history/KafkaSchemaHistory.java",
        "docstring": "The getKafkaBrokerConfig function retrieves the configuration of a Kafka broker using an AdminClient instance.\\nIt first describes the cluster to get the available nodes within a specified timeout.\\nIf no nodes are available, it throws a ConnectException.\\nIt selects the first node's ID and requests its configuration.\\nIf no configurations are received, it throws another ConnectException.\\nFinally, it returns the broker's configuration.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "26fbc2ba1d28",
        "ground_truth": "private Config getKafkaBrokerConfig(AdminClient admin) throws Exception {\n    final Collection<Node> nodes = admin.describeCluster().nodes().get(kafkaQueryTimeout.toMillis(), TimeUnit.MILLISECONDS);\n    if (nodes.isEmpty()) {\n        throw new ConnectException(\"No brokers available to obtain default settings\");\n    }\n    String nodeId = nodes.iterator().next().idString();\n    Set<ConfigResource> resources = Collections.singleton(new ConfigResource(ConfigResource.Type.BROKER, nodeId));\n    final Map<ConfigResource, Config> configs = admin.describeConfigs(resources).all().get(\n            kafkaQueryTimeout.toMillis(), TimeUnit.MILLISECONDS);\n    if (configs.isEmpty()) {\n        throw new ConnectException(\"No configs have been received\");\n    }\n    return configs.values().iterator().next();\n}",
        "import_statements": [
            "import java.io.IOException;",
            "import java.time.Duration;",
            "import java.time.temporal.ChronoUnit;",
            "import java.util.Collection;",
            "import java.util.Collections;",
            "import java.util.Map;",
            "import java.util.Optional;",
            "import java.util.Properties;",
            "import java.util.Set;",
            "import java.util.UUID;",
            "import java.util.concurrent.ExecutionException;",
            "import java.util.concurrent.ExecutorService;",
            "import java.util.concurrent.Future;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.function.Consumer;",
            "import org.apache.kafka.clients.admin.AdminClient;",
            "import org.apache.kafka.clients.admin.AdminClientConfig;",
            "import org.apache.kafka.clients.admin.Config;",
            "import org.apache.kafka.clients.admin.CreateTopicsResult;",
            "import org.apache.kafka.clients.admin.DescribeTopicsResult;",
            "import org.apache.kafka.clients.admin.NewTopic;",
            "import org.apache.kafka.clients.admin.TopicDescription;",
            "import org.apache.kafka.clients.consumer.ConsumerConfig;",
            "import org.apache.kafka.clients.consumer.ConsumerRecord;",
            "import org.apache.kafka.clients.consumer.ConsumerRecords;",
            "import org.apache.kafka.clients.consumer.KafkaConsumer;",
            "import org.apache.kafka.clients.consumer.OffsetResetStrategy;",
            "import org.apache.kafka.clients.producer.KafkaProducer;",
            "import org.apache.kafka.clients.producer.ProducerConfig;",
            "import org.apache.kafka.clients.producer.ProducerRecord;",
            "import org.apache.kafka.clients.producer.RecordMetadata;",
            "import org.apache.kafka.common.Node;",
            "import org.apache.kafka.common.TopicPartition;",
            "import org.apache.kafka.common.config.ConfigDef.Importance;",
            "import org.apache.kafka.common.config.ConfigDef.Type;",
            "import org.apache.kafka.common.config.ConfigDef.Width;",
            "import org.apache.kafka.common.config.ConfigResource;",
            "import org.apache.kafka.common.errors.TopicExistsException;",
            "import org.apache.kafka.common.errors.UnsupportedVersionException;",
            "import org.apache.kafka.common.serialization.StringDeserializer;",
            "import org.apache.kafka.common.serialization.StringSerializer;",
            "import org.apache.kafka.connect.errors.ConnectException;",
            "import org.apache.kafka.connect.source.SourceConnector;",
            "import org.apache.kafka.connect.source.SourceRecord;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import io.debezium.DebeziumException;",
            "import io.debezium.annotation.NotThreadSafe;",
            "import io.debezium.config.Configuration;",
            "import io.debezium.config.Field;",
            "import io.debezium.config.Field.Validator;",
            "import io.debezium.document.DocumentReader;",
            "import io.debezium.relational.HistorizedRelationalDatabaseConnectorConfig;",
            "import io.debezium.relational.history.AbstractSchemaHistory;",
            "import io.debezium.relational.history.HistoryRecord;",
            "import io.debezium.relational.history.HistoryRecordComparator;",
            "import io.debezium.relational.history.SchemaHistory;",
            "import io.debezium.relational.history.SchemaHistoryException;",
            "import io.debezium.relational.history.SchemaHistoryListener;",
            "import io.debezium.util.Collect;",
            "import io.debezium.util.Loggings;",
            "import io.debezium.util.Threads;"
        ],
        "reference_api": [
            "describeConfigs",
            "describeCluster",
            "next",
            "singleton",
            "iterator",
            "values",
            "idString",
            "nodes",
            "all",
            "get",
            "toMillis",
            "isEmpty"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "describeConfigs",
            "describeCluster",
            "next",
            "singleton",
            "iterator",
            "values",
            "idString",
            "nodes",
            "all",
            "get",
            "toMillis",
            "isEmpty"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "debezium/debezium",
        "function_declaration": "private static Validator forKafka(final Validator validator)",
        "start_line": "615",
        "end_line": "620",
        "file_path": "debezium-storage/debezium-storage-kafka/src/main/java/io/debezium/storage/kafka/history/KafkaSchemaHistory.java",
        "docstring": "The forKafka function returns a Validator that conditionally validates a configuration based on the schema history setting.\\nIf the schema history is set to KafkaSchemaHistory, it delegates the validation to the provided validator.\\nOtherwise, it returns 0, indicating no validation issues.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "adf40b68c93e",
        "ground_truth": "private static Validator forKafka(final Validator validator) {\n    return (config, field, problems) -> {\n        final String history = config.getString(HistorizedRelationalDatabaseConnectorConfig.SCHEMA_HISTORY);\n        return KafkaSchemaHistory.class.getName().equals(history) ? validator.validate(config, field, problems) : 0;\n    };\n}",
        "import_statements": [
            "import java.io.IOException;",
            "import java.time.Duration;",
            "import java.time.temporal.ChronoUnit;",
            "import java.util.Collection;",
            "import java.util.Collections;",
            "import java.util.Map;",
            "import java.util.Optional;",
            "import java.util.Properties;",
            "import java.util.Set;",
            "import java.util.UUID;",
            "import java.util.concurrent.ExecutionException;",
            "import java.util.concurrent.ExecutorService;",
            "import java.util.concurrent.Future;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.function.Consumer;",
            "import org.apache.kafka.clients.admin.AdminClient;",
            "import org.apache.kafka.clients.admin.AdminClientConfig;",
            "import org.apache.kafka.clients.admin.Config;",
            "import org.apache.kafka.clients.admin.CreateTopicsResult;",
            "import org.apache.kafka.clients.admin.DescribeTopicsResult;",
            "import org.apache.kafka.clients.admin.NewTopic;",
            "import org.apache.kafka.clients.admin.TopicDescription;",
            "import org.apache.kafka.clients.consumer.ConsumerConfig;",
            "import org.apache.kafka.clients.consumer.ConsumerRecord;",
            "import org.apache.kafka.clients.consumer.ConsumerRecords;",
            "import org.apache.kafka.clients.consumer.KafkaConsumer;",
            "import org.apache.kafka.clients.consumer.OffsetResetStrategy;",
            "import org.apache.kafka.clients.producer.KafkaProducer;",
            "import org.apache.kafka.clients.producer.ProducerConfig;",
            "import org.apache.kafka.clients.producer.ProducerRecord;",
            "import org.apache.kafka.clients.producer.RecordMetadata;",
            "import org.apache.kafka.common.Node;",
            "import org.apache.kafka.common.TopicPartition;",
            "import org.apache.kafka.common.config.ConfigDef.Importance;",
            "import org.apache.kafka.common.config.ConfigDef.Type;",
            "import org.apache.kafka.common.config.ConfigDef.Width;",
            "import org.apache.kafka.common.config.ConfigResource;",
            "import org.apache.kafka.common.errors.TopicExistsException;",
            "import org.apache.kafka.common.errors.UnsupportedVersionException;",
            "import org.apache.kafka.common.serialization.StringDeserializer;",
            "import org.apache.kafka.common.serialization.StringSerializer;",
            "import org.apache.kafka.connect.errors.ConnectException;",
            "import org.apache.kafka.connect.source.SourceConnector;",
            "import org.apache.kafka.connect.source.SourceRecord;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import io.debezium.DebeziumException;",
            "import io.debezium.annotation.NotThreadSafe;",
            "import io.debezium.config.Configuration;",
            "import io.debezium.config.Field;",
            "import io.debezium.config.Field.Validator;",
            "import io.debezium.document.DocumentReader;",
            "import io.debezium.relational.HistorizedRelationalDatabaseConnectorConfig;",
            "import io.debezium.relational.history.AbstractSchemaHistory;",
            "import io.debezium.relational.history.HistoryRecord;",
            "import io.debezium.relational.history.HistoryRecordComparator;",
            "import io.debezium.relational.history.SchemaHistory;",
            "import io.debezium.relational.history.SchemaHistoryException;",
            "import io.debezium.relational.history.SchemaHistoryListener;",
            "import io.debezium.util.Collect;",
            "import io.debezium.util.Loggings;",
            "import io.debezium.util.Threads;"
        ],
        "reference_api": [
            "getName",
            "getString",
            "equals",
            "validate"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getName",
            "getString",
            "validate"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "debezium/debezium",
        "function_declaration": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config)",
        "start_line": "135",
        "end_line": "143",
        "file_path": "debezium-connect-rest-extension/src/main/java/io/debezium/kcrestextension/DebeziumResource.java",
        "docstring": "The isTopicCreationEnabled function checks if topic creation is enabled based on the provided configuration and Kafka Connect version.\\nIt parses the current Kafka Connect version and retrieves the \"topic.creation.enable\" property from the configuration, defaulting to true if not set.\\nThe function returns true if the Kafka Connect version is compatible with topic creation and the property is set to true.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "07ff1ca41c93",
        "ground_truth": "private synchronized Boolean isTopicCreationEnabled(Map<String, ?> config) {\n    Version kafkaConnectVersion = parseVersion(AppInfoParser.getVersion());\n    String topicCreationProperty = (String) config.get(\"topic.creation.enable\");\n    if (null == topicCreationProperty) { // when config is not set, default to true\n        topicCreationProperty = \"true\";\n    }\n    return TOPIC_CREATION_KAFKA_VERSION.compareTo(kafkaConnectVersion) <= 0\n            && Boolean.parseBoolean(topicCreationProperty);\n}",
        "import_statements": [
            "import java.lang.Runtime.Version;",
            "import java.lang.reflect.Field;",
            "import java.util.ArrayList;",
            "import java.util.Arrays;",
            "import java.util.Collection;",
            "import java.util.Collections;",
            "import java.util.HashSet;",
            "import java.util.LinkedHashSet;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import java.util.regex.Matcher;",
            "import java.util.regex.Pattern;",
            "import javax.servlet.ServletContext;",
            "import javax.ws.rs.Consumes;",
            "import javax.ws.rs.GET;",
            "import javax.ws.rs.Path;",
            "import javax.ws.rs.Produces;",
            "import javax.ws.rs.core.MediaType;",
            "import org.apache.kafka.common.utils.AppInfoParser;",
            "import org.apache.kafka.connect.health.ConnectClusterState;",
            "import org.apache.kafka.connect.runtime.Herder;",
            "import org.apache.kafka.connect.runtime.isolation.PluginDesc;",
            "import org.apache.kafka.connect.transforms.Transformation;",
            "import org.apache.kafka.connect.transforms.predicates.Predicate;",
            "import io.debezium.DebeziumException;",
            "import io.debezium.kcrestextension.entities.PredicateDefinition;",
            "import io.debezium.kcrestextension.entities.TransformDefinition;",
            "import io.debezium.metadata.ConnectorDescriptor;"
        ],
        "reference_api": [
            "parseVersion",
            "getVersion",
            "parseBoolean",
            "get",
            "compareTo"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "parseVersion",
                "code": "public static Version parseVersion(String version) {\n        Matcher m = VERSION_PATTERN.matcher(version);\n        if (m.matches()) {\n            return Version.parse(version);\n        }\n        else if (m.lookingAt()) {\n            return Version.parse(m.group());\n        }\n        throw new IllegalArgumentException(\"Invalid version string: \\\"\" + version + \"\\\"\");\n    }"
            }
        ],
        "third_party": [
            "getVersion",
            "parseBoolean",
            "get"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "debezium/debezium",
        "function_declaration": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config)",
        "start_line": "57",
        "end_line": "76",
        "file_path": "debezium-embedded/src/main/java/io/debezium/embedded/KafkaConnectUtil.java",
        "docstring": "The kafkaOffsetBackingStore function initializes a KafkaOffsetBackingStore using the provided configuration map.\\nIt sets a client ID and prepares the properties for Kafka admin operations.\\nThe function checks for mandatory configuration options and throws an exception if any are missing.\\nIt creates a SharedTopicAdmin with the admin properties and returns a new KafkaOffsetBackingStore instance with the shared admin, client ID, and an offset store converter.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e2cc420e1e63",
        "ground_truth": "public static final KafkaOffsetBackingStore kafkaOffsetBackingStore(Map<String, String> config) {\n    final String clientId = \"debezium-server\";\n    final Map<String, Object> adminProps = new HashMap<>(config);\n    adminProps.put(CLIENT_ID_CONFIG, clientId + \"shared-admin\");\n    Stream.of(\n            DistributedConfig.BOOTSTRAP_SERVERS_CONFIG,\n            DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG,\n            DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG,\n            DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG).forEach(prop -> {\n                if (!adminProps.containsKey(prop)) {\n                    throw new DebeziumException(String.format(\n                            \"Cannot initialize Kafka offset storage, mandatory configuration option '%s' is missing\",\n                            prop));\n                }\n            });\n    SharedTopicAdmin sharedAdmin = new SharedTopicAdmin(adminProps);\n    return new KafkaOffsetBackingStore(sharedAdmin, () -> clientId, converterForOffsetStore());\n}",
        "import_statements": [
            "import static org.apache.kafka.clients.CommonClientConfigs.CLIENT_ID_CONFIG;",
            "import java.util.Collections;",
            "import java.util.HashMap;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import java.util.stream.Stream;",
            "import org.apache.kafka.connect.json.JsonConverter;",
            "import org.apache.kafka.connect.json.JsonConverterConfig;",
            "import org.apache.kafka.connect.runtime.distributed.DistributedConfig;",
            "import org.apache.kafka.connect.storage.Converter;",
            "import org.apache.kafka.connect.storage.FileOffsetBackingStore;",
            "import org.apache.kafka.connect.storage.KafkaOffsetBackingStore;",
            "import org.apache.kafka.connect.storage.MemoryOffsetBackingStore;",
            "import org.apache.kafka.connect.util.SharedTopicAdmin;",
            "import io.debezium.DebeziumException;"
        ],
        "reference_api": [
            "containsKey",
            "of",
            "converterForOffsetStore",
            "forEach",
            "put",
            "format"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "converterForOffsetStore",
                "code": "public static final Converter converterForOffsetStore() {\n        final JsonConverter converter = new JsonConverter();\n        converter.configure(Collections.singletonMap(JsonConverterConfig.SCHEMAS_ENABLE_CONFIG, \"false\"), true);\n        return converter;\n    }"
            }
        ],
        "third_party": [
            "containsKey",
            "of",
            "forEach",
            "put"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "debezium/debezium",
        "function_declaration": "private T readKey(JsonNode node) throws IOException",
        "start_line": "123",
        "end_line": "140",
        "file_path": "debezium-core/src/main/java/io/debezium/serde/json/JsonSerde.java",
        "docstring": "The readKey function deserializes a JsonNode into a key of type T.\\nIt checks if the node is an object and extracts the payload field if present.\\nIt iterates through the field names to determine if the key is simple or composite.\\nIf it's a simple key, it reads the value of the single field.\\nIf it's a composite key, it reads the entire node as the value.\\nFinally, it returns the deserialized key.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4ecb2279231e",
        "ground_truth": "private T readKey(JsonNode node) throws IOException {\n    if (!node.isObject()) {\n        return reader.readValue(node);\n    }\n    final JsonNode keys = node.has(PAYLOAD_FIELD) ? node.get(PAYLOAD_FIELD) : node;\n    final Iterator<String> keyFields = keys.fieldNames();\n    if (keyFields.hasNext()) {\n        final String id = keyFields.next();\n        if (!keyFields.hasNext()) {\n            // Simple key\n            return reader.readValue(keys.get(id));\n        }\n        // Composite key\n        return reader.readValue(keys);\n    }\n    return reader.readValue(keys);\n}",
        "import_statements": [
            "import java.io.IOException;",
            "import java.util.Iterator;",
            "import java.util.Map;",
            "import org.apache.kafka.common.serialization.Deserializer;",
            "import org.apache.kafka.common.serialization.Serde;",
            "import org.apache.kafka.common.serialization.Serializer;",
            "import com.fasterxml.jackson.core.JsonProcessingException;",
            "import com.fasterxml.jackson.databind.DeserializationFeature;",
            "import com.fasterxml.jackson.databind.JsonNode;",
            "import com.fasterxml.jackson.databind.ObjectMapper;",
            "import com.fasterxml.jackson.databind.ObjectReader;",
            "import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;",
            "import io.debezium.common.annotation.Incubating;",
            "import io.debezium.data.Envelope;"
        ],
        "reference_api": [
            "fieldNames",
            "has",
            "next",
            "readValue",
            "isObject",
            "get",
            "hasNext"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "readValue",
                "code": "private T readValue(JsonNode node) throws IOException {\n            JsonNode payload = node.get(PAYLOAD_FIELD);\n\n            // Schema + payload format\n            if (payload != null) {\n                node = payload;\n            }\n            // Debezium envelope\n            if (config.asEnvelope()) {\n                return reader.readValue(node);\n            }\n            else if (node.has(Envelope.FieldName.SOURCE) && node.has(config.sourceField())) {\n                return reader.readValue(node.get(config.sourceField()));\n            }\n            // Extracted format\n            else {\n                return reader.readValue(node);\n            }\n        }"
            }
        ],
        "third_party": [
            "fieldNames",
            "has",
            "next",
            "isObject",
            "get",
            "hasNext"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "debezium/debezium",
        "function_declaration": "public String topicNameFor(I id, String prefix, String delimiter)",
        "start_line": "114",
        "end_line": "140",
        "file_path": "debezium-core/src/main/java/io/debezium/schema/TopicSelector.java",
        "docstring": "The topicNameFor function generates a topic name based on an ID, prefix, and delimiter, then sanitizes it.\\nIt constructs the initial topic name using a delegate method.\\nIt iterates through the characters of the topic name, replacing invalid characters with a replacement character.\\nIf any replacements are made, it logs a warning and returns the sanitized name.\\nIf no replacements are necessary, it returns the original topic name.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b8ba1bfc7b5e",
        "ground_truth": "public String topicNameFor(I id, String prefix, String delimiter) {\n    String topicName = delegate.topicNameFor(id, prefix, delimiter);\n    StringBuilder sanitizedNameBuilder = new StringBuilder(topicName.length());\n    boolean changed = false;\n    for (int i = 0; i < topicName.length(); i++) {\n        char c = topicName.charAt(i);\n        if (isValidTopicNameCharacter(c)) {\n            sanitizedNameBuilder.append(c);\n        }\n        else {\n            sanitizedNameBuilder.append(REPLACEMENT_CHAR);\n            changed = true;\n        }\n    }\n    if (changed) {\n        String sanitizedName = sanitizedNameBuilder.toString();\n        LOGGER.warn(\"Topic '{}' name isn't a valid topic name, replacing it with '{}'.\", topicName, sanitizedName);\n        return sanitizedName;\n    }\n    else {\n        return topicName;\n    }\n}",
        "import_statements": [
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import io.debezium.annotation.ThreadSafe;",
            "import io.debezium.config.CommonConnectorConfig;",
            "import io.debezium.spi.schema.DataCollectionId;",
            "import io.debezium.util.BoundedConcurrentHashMap;",
            "import io.debezium.util.BoundedConcurrentHashMap.Eviction;"
        ],
        "reference_api": [
            "topicNameFor",
            "toString",
            "append",
            "warn",
            "isValidTopicNameCharacter",
            "charAt",
            "length"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "topicNameFor",
                "code": "public String topicNameFor(I id) {\n        return dataCollectionTopicNamer.topicNameFor(id, prefix, delimiter);\n    }"
            },
            {
                "name": "isValidTopicNameCharacter",
                "code": "private boolean isValidTopicNameCharacter(char c) {\n            return c == '.' || c == '_' || c == '-' || (c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z') || (c >= '0' && c <= '9');\n        }"
            }
        ],
        "third_party": [
            "append",
            "warn"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "debezium/debezium",
        "function_declaration": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                        ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) ",
        "start_line": "35",
        "end_line": "49",
        "file_path": "debezium-core/src/main/java/io/debezium/pipeline/ErrorHandler.java",
        "docstring": "The ErrorHandler constructor initializes an error handler for a source connector.\\nIt takes the connector type, connector configuration, change event queue, and an optional replaced error handler as parameters.\\nThe constructor sets the connector configuration and queue, initializes the producer throwable reference, and determines the maximum number of retries based on the connector configuration or defaults to unlimited retries.\\nIf a replaced error handler is provided, it carries over the retry count from the replaced handler.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9cf0d74c098d",
        "ground_truth": "public ErrorHandler(Class<? extends SourceConnector> connectorType, CommonConnectorConfig connectorConfig,\n                    ChangeEventQueue<?> queue, ErrorHandler replacedErrorHandler) {\n    this.connectorConfig = connectorConfig;\n    this.queue = queue;\n    this.producerThrowable = new AtomicReference<>();\n    if (connectorConfig != null) {\n        this.maxRetries = connectorConfig.getMaxRetriesOnError();\n    }\n    else {\n        this.maxRetries = RETRIES_UNLIMITED;\n    }\n    if (replacedErrorHandler != null) {\n        this.retries = replacedErrorHandler.getRetries();\n    }\n}",
        "import_statements": [
            "import java.io.IOException;",
            "import java.util.Collections;",
            "import java.util.Set;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "import org.apache.kafka.connect.errors.ConnectException;",
            "import org.apache.kafka.connect.errors.RetriableException;",
            "import org.apache.kafka.connect.source.SourceConnector;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import io.debezium.config.CommonConnectorConfig;",
            "import io.debezium.connector.base.ChangeEventQueue;"
        ],
        "reference_api": [
            "getRetries",
            "getMaxRetriesOnError"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getRetries",
                "code": "public int getRetries() {\n        return retries;\n    }"
            }
        ],
        "third_party": [
            "getMaxRetriesOnError"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "debezium/debezium",
        "function_declaration": "public void validate(Configuration configuration, Field.Set fields)",
        "start_line": "84",
        "end_line": "92",
        "file_path": "debezium-core/src/main/java/io/debezium/transforms/SmtManager.java",
        "docstring": "The validate function checks the configuration against a set of fields.\\nIt validates the configuration and iterates through the validation results.\\nIf any configuration value has error messages, it throws a ConfigException with the first error message for that value.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8ab4bf3a4dd8",
        "ground_truth": "public void validate(Configuration configuration, Field.Set fields) {\n    Map<String, ConfigValue> validations = configuration.validate(fields);\n    for (Map.Entry<String, ConfigValue> entry : validations.entrySet()) {\n        if (!entry.getValue().errorMessages().isEmpty()) {\n            final ConfigValue value = entry.getValue();\n            throw new ConfigException(value.name(), configuration.getString(value.name()), value.errorMessages().get(0));\n        }\n    }\n}",
        "import_statements": [
            "import static io.debezium.data.Envelope.Operation.MESSAGE;",
            "import static io.debezium.data.Envelope.Operation.TRUNCATE;",
            "import java.util.Map;",
            "import org.apache.kafka.common.config.ConfigException;",
            "import org.apache.kafka.common.config.ConfigValue;",
            "import org.apache.kafka.connect.connector.ConnectRecord;",
            "import org.apache.kafka.connect.source.SourceRecord;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import io.debezium.config.Configuration;",
            "import io.debezium.config.Field;",
            "import io.debezium.data.Envelope;",
            "import io.debezium.schema.SchemaFactory;"
        ],
        "reference_api": [
            "getString",
            "name",
            "getValue",
            "validate",
            "entrySet",
            "get",
            "errorMessages",
            "isEmpty"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "validate",
                "code": "public void validate(Configuration configuration, Field.Set fields) {\n        Map<String, ConfigValue> validations = configuration.validate(fields);\n        for (Map.Entry<String, ConfigValue> entry : validations.entrySet()) {\n            if (!entry.getValue().errorMessages().isEmpty()) {\n                final ConfigValue value = entry.getValue();\n                throw new ConfigException(value.name(), configuration.getString(value.name()), value.errorMessages().get(0));\n            }\n        }\n    }"
            }
        ],
        "third_party": [
            "getString",
            "name",
            "getValue",
            "entrySet",
            "get",
            "errorMessages",
            "isEmpty"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "debezium/debezium",
        "function_declaration": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level)",
        "start_line": "250",
        "end_line": "280",
        "file_path": "debezium-core/src/main/java/io/debezium/transforms/HeaderToValue.java",
        "docstring": "The buildNewSchema function creates a new schema based on an existing schema and additional headers.\\nIf the old schema is primitive, it returns the old schema.\\nOtherwise, it copies fields from the old schema to a new SchemaBuilder, recursively processing nested fields as needed.\\nIt also adds fields from the specified headers to the new schema if applicable.\\nThe function logs debug information about the fields copied and added, and returns the newly built schema.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "860405dc9abb",
        "ground_truth": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level) {\n    if (oldSchema.type().isPrimitive()) {\n        return oldSchema;\n    }\n    // Get fields from original schema\n    SchemaBuilder newSchemabuilder = SchemaUtil.copySchemaBasics(oldSchema, SchemaBuilder.struct());\n    for (org.apache.kafka.connect.data.Field field : oldSchema.fields()) {\n        if (isContainedIn(field.name(), nestedFields)) {\n            newSchemabuilder.field(field.name(), buildNewSchema(field.name(), field.schema(), headerToProcess, nestedFields, ++level));\n        }\n        else {\n            newSchemabuilder.field(field.name(), field.schema());\n        }\n    }\n    LOGGER.debug(\"Fields copied from the old schema {}\", newSchemabuilder.fields());\n    for (int i = 0; i < headers.size(); i++) {\n        Header currentHeader = headerToProcess.get(headers.get(i));\n        Optional<String> currentFieldName = getFieldName(fields.get(i), fieldName, level);\n        LOGGER.trace(\"CurrentHeader {} - currentFieldName {}\", headers.get(i), currentFieldName);\n        if (currentFieldName.isPresent() && currentHeader != null) {\n            newSchemabuilder = newSchemabuilder.field(currentFieldName.get(), currentHeader.schema());\n        }\n    }\n    LOGGER.debug(\"Fields added from headers {}\", newSchemabuilder.fields());\n    return newSchemabuilder.build();\n}",
        "import_statements": [
            "import static io.debezium.transforms.HeaderToValue.Operation.MOVE;",
            "import static java.lang.String.format;",
            "import static org.apache.kafka.connect.transforms.util.Requirements.requireStruct;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Optional;",
            "import java.util.function.Function;",
            "import java.util.stream.Collectors;",
            "import java.util.stream.StreamSupport;",
            "import org.apache.kafka.common.config.ConfigDef;",
            "import org.apache.kafka.common.config.ConfigException;",
            "import org.apache.kafka.connect.components.Versioned;",
            "import org.apache.kafka.connect.connector.ConnectRecord;",
            "import org.apache.kafka.connect.data.Schema;",
            "import org.apache.kafka.connect.data.SchemaBuilder;",
            "import org.apache.kafka.connect.data.Struct;",
            "import org.apache.kafka.connect.header.Header;",
            "import org.apache.kafka.connect.header.Headers;",
            "import org.apache.kafka.connect.transforms.Transformation;",
            "import org.apache.kafka.connect.transforms.util.SchemaUtil;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import io.debezium.Module;",
            "import io.debezium.config.Configuration;",
            "import io.debezium.config.Field;",
            "import io.debezium.util.BoundedConcurrentHashMap;"
        ],
        "reference_api": [
            "isContainedIn",
            "name",
            "field",
            "isPresent",
            "getFieldName",
            "type",
            "size",
            "struct",
            "schema",
            "fields",
            "debug",
            "get",
            "buildNewSchema",
            "trace",
            "copySchemaBasics",
            "build",
            "isPrimitive"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "isContainedIn",
                "code": "private boolean isContainedIn(String fieldName, List<String> nestedFields) {\n\n        return nestedFields.stream().anyMatch(s -> s.contains(fieldName));\n    }"
            },
            {
                "name": "getFieldName",
                "code": "private Optional<String> getFieldName(String destinationFieldName, String fieldName, int level) {\n\n        String[] nestedNames = destinationFieldName.split(\"\\\\.\");\n        if (isRootField(fieldName, nestedNames)) {\n            return Optional.of(nestedNames[0]);\n        }\n\n        if (isChildrenOf(fieldName, level, nestedNames)) {\n            return Optional.of(nestedNames[level]);\n        }\n\n        return Optional.empty();\n    }"
            },
            {
                "name": "buildNewSchema",
                "code": "private Schema buildNewSchema(String fieldName, Schema oldSchema, Map<String, Header> headerToProcess, List<String> nestedFields, int level) {\n\n        if (oldSchema.type().isPrimitive()) {\n            return oldSchema;\n        }\n\n        // Get fields from original schema\n        SchemaBuilder newSchemabuilder = SchemaUtil.copySchemaBasics(oldSchema, SchemaBuilder.struct());\n        for (org.apache.kafka.connect.data.Field field : oldSchema.fields()) {\n            if (isContainedIn(field.name(), nestedFields)) {\n\n                newSchemabuilder.field(field.name(), buildNewSchema(field.name(), field.schema(), headerToProcess, nestedFields, ++level));\n            }\n            else {\n                newSchemabuilder.field(field.name(), field.schema());\n            }\n        }\n\n        LOGGER.debug(\"Fields copied from the old schema {}\", newSchemabuilder.fields());\n        for (int i = 0; i < headers.size(); i++) {\n\n            Header currentHeader = headerToProcess.get(headers.get(i));\n            Optional<String> currentFieldName = getFieldName(fields.get(i), fieldName, level);\n            LOGGER.trace(\"CurrentHeader {} - currentFieldName {}\", headers.get(i), currentFieldName);\n            if (currentFieldName.isPresent() && currentHeader != null) {\n                newSchemabuilder = newSchemabuilder.field(currentFieldName.get(), currentHeader.schema());\n            }\n        }\n        LOGGER.debug(\"Fields added from headers {}\", newSchemabuilder.fields());\n        return newSchemabuilder.build();\n    }"
            }
        ],
        "third_party": [
            "name",
            "field",
            "isPresent",
            "type",
            "size",
            "struct",
            "schema",
            "fields",
            "debug",
            "get",
            "trace",
            "copySchemaBasics",
            "build",
            "isPrimitive"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "debezium/debezium",
        "function_declaration": "public static SnapshotRecord fromSource(Struct source)",
        "start_line": "46",
        "end_line": "55",
        "file_path": "debezium-core/src/main/java/io/debezium/connector/SnapshotRecord.java",
        "docstring": "The fromSource function creates a SnapshotRecord from a given Struct source.\\nIt checks if the source schema contains the SNAPSHOT_KEY field with the expected enum logical name.\\nIf the snapshotString is present, it converts it to uppercase and returns the corresponding SnapshotRecord value.\\nIf the conditions are not met, it returns null.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "681a0415df83",
        "ground_truth": "public static SnapshotRecord fromSource(Struct source) {\n    if (source.schema().field(AbstractSourceInfo.SNAPSHOT_KEY) != null\n            && io.debezium.data.Enum.LOGICAL_NAME.equals(source.schema().field(AbstractSourceInfo.SNAPSHOT_KEY).schema().name())) {\n        final String snapshotString = source.getString(AbstractSourceInfo.SNAPSHOT_KEY);\n        if (snapshotString != null) {\n            return SnapshotRecord.valueOf(snapshotString.toUpperCase());\n        }\n    }\n    return null;\n}",
        "import_statements": [
            "import org.apache.kafka.connect.data.Struct;"
        ],
        "reference_api": [
            "getString",
            "name",
            "equals",
            "field",
            "valueOf",
            "toUpperCase",
            "schema"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getString",
            "name",
            "field",
            "toUpperCase",
            "schema"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": "public static Optional<Float> parse(String version) throws NumberFormatException",
        "start_line": "10",
        "end_line": "20",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/util/KafkaVersion.java",
        "docstring": "The parse function attempts to parse a version string into an Optional Float.\\nIt splits the version string by periods and considers only the first two parts if there are more than two.\\nIt then splits the version by hyphens and parses the first part as a Float.\\nIf successful, it returns the Float wrapped in an Optional.\\nIf any exception occurs during parsing, it returns an empty Optional.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "85ef1162e38a",
        "ground_truth": "public static Optional<Float> parse(String version) throws NumberFormatException {\n  try {\n    final String[] parts = version.split(\"\\\\.\");\n    if (parts.length > 2) {\n      version = parts[0] + \".\" + parts[1];\n    }\n    return Optional.of(Float.parseFloat(version.split(\"-\")[0]));\n  } catch (Exception e) {\n    return Optional.empty();\n  }\n}",
        "import_statements": [
            "import java.util.Optional;"
        ],
        "reference_api": [
            "empty",
            "parseFloat",
            "split",
            "of"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "empty",
            "parseFloat",
            "of"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange)",
        "start_line": "70",
        "end_line": "85",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/KafkaConnectController.java",
        "docstring": "The createConnector function creates a new Kafka connector for a specified cluster and connect instance.\\nIt builds an AccessContext with relevant details and actions, then validates access using this context.\\nIf access is validated, it calls kafkaConnectService to create the connector and returns the result wrapped in a ResponseEntity.\\nAdditionally, it audits the operation using the context and the signal.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "64346d69eba0",
        "ground_truth": "public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                          @Valid Mono<NewConnectorDTO> connector,\n                                                          ServerWebExchange exchange) {\n  var context = AccessContext.builder()\n      .cluster(clusterName)\n      .connect(connectName)\n      .connectActions(ConnectAction.VIEW, ConnectAction.CREATE)\n      .operationName(\"createConnector\")\n      .build();\n  return validateAccess(context).then(\n      kafkaConnectService.createConnector(getCluster(clusterName), connectName, connector)\n          .map(ResponseEntity::ok)\n  ).doOnEach(sig -> audit(context, sig));\n}",
        "import_statements": [
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART;",
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART_ALL_TASKS;",
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART_FAILED_TASKS;",
            "import com.provectus.kafka.ui.api.KafkaConnectApi;",
            "import com.provectus.kafka.ui.model.ConnectDTO;",
            "import com.provectus.kafka.ui.model.ConnectorActionDTO;",
            "import com.provectus.kafka.ui.model.ConnectorColumnsToSortDTO;",
            "import com.provectus.kafka.ui.model.ConnectorDTO;",
            "import com.provectus.kafka.ui.model.ConnectorPluginConfigValidationResponseDTO;",
            "import com.provectus.kafka.ui.model.ConnectorPluginDTO;",
            "import com.provectus.kafka.ui.model.FullConnectorInfoDTO;",
            "import com.provectus.kafka.ui.model.NewConnectorDTO;",
            "import com.provectus.kafka.ui.model.SortOrderDTO;",
            "import com.provectus.kafka.ui.model.TaskDTO;",
            "import com.provectus.kafka.ui.model.rbac.AccessContext;",
            "import com.provectus.kafka.ui.model.rbac.permission.ConnectAction;",
            "import com.provectus.kafka.ui.service.KafkaConnectService;",
            "import java.util.Comparator;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import javax.validation.Valid;",
            "import lombok.RequiredArgsConstructor;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.springframework.http.ResponseEntity;",
            "import org.springframework.web.bind.annotation.RestController;",
            "import org.springframework.web.server.ServerWebExchange;",
            "import reactor.core.publisher.Flux;",
            "import reactor.core.publisher.Mono;"
        ],
        "reference_api": [
            "operationName",
            "audit",
            "createConnector",
            "cluster",
            "map",
            "validateAccess",
            "connectActions",
            "doOnEach",
            "then",
            "getCluster",
            "connect",
            "build",
            "builder"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "createConnector",
                "code": "@Override\n  public Mono<ResponseEntity<ConnectorDTO>> createConnector(String clusterName, String connectName,\n                                                            @Valid Mono<NewConnectorDTO> connector,\n                                                            ServerWebExchange exchange) {\n\n    var context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.VIEW, ConnectAction.CREATE)\n        .operationName(\"createConnector\")\n        .build();\n\n    return validateAccess(context).then(\n        kafkaConnectService.createConnector(getCluster(clusterName), connectName, connector)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }"
            }
        ],
        "third_party": [
            "operationName",
            "audit",
            "cluster",
            "map",
            "validateAccess",
            "connectActions",
            "doOnEach",
            "then",
            "getCluster",
            "connect",
            "build",
            "builder"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange)",
        "start_line": "107",
        "end_line": "123",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/KafkaConnectController.java",
        "docstring": "The deleteConnector function deletes a Kafka connector within a specified cluster and connect instance.\\nIt builds an AccessContext with necessary details, including cluster name, connect name, and required actions.\\nIt validates access using this context and, upon successful validation, calls the kafkaConnectService to delete the connector.\\nThe function returns a Mono<ResponseEntity<Void>> indicating the result and audits the operation.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "4c446fd64db6",
        "ground_truth": "public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                  String connectorName,\n                                                  ServerWebExchange exchange) {\n  var context = AccessContext.builder()\n      .cluster(clusterName)\n      .connect(connectName)\n      .connectActions(ConnectAction.VIEW, ConnectAction.EDIT)\n      .operationName(\"deleteConnector\")\n      .operationParams(Map.of(CONNECTOR_NAME, connectName))\n      .build();\n  return validateAccess(context).then(\n      kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n          .map(ResponseEntity::ok)\n  ).doOnEach(sig -> audit(context, sig));\n}",
        "import_statements": [
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART;",
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART_ALL_TASKS;",
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART_FAILED_TASKS;",
            "import com.provectus.kafka.ui.api.KafkaConnectApi;",
            "import com.provectus.kafka.ui.model.ConnectDTO;",
            "import com.provectus.kafka.ui.model.ConnectorActionDTO;",
            "import com.provectus.kafka.ui.model.ConnectorColumnsToSortDTO;",
            "import com.provectus.kafka.ui.model.ConnectorDTO;",
            "import com.provectus.kafka.ui.model.ConnectorPluginConfigValidationResponseDTO;",
            "import com.provectus.kafka.ui.model.ConnectorPluginDTO;",
            "import com.provectus.kafka.ui.model.FullConnectorInfoDTO;",
            "import com.provectus.kafka.ui.model.NewConnectorDTO;",
            "import com.provectus.kafka.ui.model.SortOrderDTO;",
            "import com.provectus.kafka.ui.model.TaskDTO;",
            "import com.provectus.kafka.ui.model.rbac.AccessContext;",
            "import com.provectus.kafka.ui.model.rbac.permission.ConnectAction;",
            "import com.provectus.kafka.ui.service.KafkaConnectService;",
            "import java.util.Comparator;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import javax.validation.Valid;",
            "import lombok.RequiredArgsConstructor;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.springframework.http.ResponseEntity;",
            "import org.springframework.web.bind.annotation.RestController;",
            "import org.springframework.web.server.ServerWebExchange;",
            "import reactor.core.publisher.Flux;",
            "import reactor.core.publisher.Mono;"
        ],
        "reference_api": [
            "operationName",
            "audit",
            "connect",
            "of",
            "cluster",
            "map",
            "validateAccess",
            "connectActions",
            "deleteConnector",
            "doOnEach",
            "then",
            "getCluster",
            "operationParams",
            "build",
            "builder"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "deleteConnector",
                "code": "@Override\n  public Mono<ResponseEntity<Void>> deleteConnector(String clusterName, String connectName,\n                                                    String connectorName,\n                                                    ServerWebExchange exchange) {\n\n    var context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(ConnectAction.VIEW, ConnectAction.EDIT)\n        .operationName(\"deleteConnector\")\n        .operationParams(Map.of(CONNECTOR_NAME, connectName))\n        .build();\n\n    return validateAccess(context).then(\n        kafkaConnectService.deleteConnector(getCluster(clusterName), connectName, connectorName)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }"
            }
        ],
        "third_party": [
            "operationName",
            "audit",
            "connect",
            "of",
            "cluster",
            "map",
            "validateAccess",
            "connectActions",
            "doOnEach",
            "then",
            "getCluster",
            "operationParams",
            "build",
            "builder"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) ",
        "start_line": "195",
        "end_line": "219",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/KafkaConnectController.java",
        "docstring": "The updateConnectorState function updates the state of a Kafka connector in a specified cluster.\\nIt determines the required connect actions based on the provided action.\\nAn access context is built with cluster, connect details, actions, operation name, and parameters.\\nThe function validates access using the context and then updates the connector state using the kafkaConnectService.\\nIt returns a Mono of ResponseEntity and performs auditing for each signal.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d34a49f3ca1b",
        "ground_truth": "public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                       String connectorName,\n                                                       ConnectorActionDTO action,\n                                                       ServerWebExchange exchange) {\n  ConnectAction[] connectActions;\n  if (RESTART_ACTIONS.contains(action)) {\n    connectActions = new ConnectAction[] {ConnectAction.VIEW, ConnectAction.RESTART};\n  } else {\n    connectActions = new ConnectAction[] {ConnectAction.VIEW, ConnectAction.EDIT};\n  }\n  var context = AccessContext.builder()\n      .cluster(clusterName)\n      .connect(connectName)\n      .connectActions(connectActions)\n      .operationName(\"updateConnectorState\")\n      .operationParams(Map.of(CONNECTOR_NAME, connectorName))\n      .build();\n  return validateAccess(context).then(\n      kafkaConnectService\n          .updateConnectorState(getCluster(clusterName), connectName, connectorName, action)\n          .map(ResponseEntity::ok)\n  ).doOnEach(sig -> audit(context, sig));\n}",
        "import_statements": [
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART;",
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART_ALL_TASKS;",
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART_FAILED_TASKS;",
            "import com.provectus.kafka.ui.api.KafkaConnectApi;",
            "import com.provectus.kafka.ui.model.ConnectDTO;",
            "import com.provectus.kafka.ui.model.ConnectorActionDTO;",
            "import com.provectus.kafka.ui.model.ConnectorColumnsToSortDTO;",
            "import com.provectus.kafka.ui.model.ConnectorDTO;",
            "import com.provectus.kafka.ui.model.ConnectorPluginConfigValidationResponseDTO;",
            "import com.provectus.kafka.ui.model.ConnectorPluginDTO;",
            "import com.provectus.kafka.ui.model.FullConnectorInfoDTO;",
            "import com.provectus.kafka.ui.model.NewConnectorDTO;",
            "import com.provectus.kafka.ui.model.SortOrderDTO;",
            "import com.provectus.kafka.ui.model.TaskDTO;",
            "import com.provectus.kafka.ui.model.rbac.AccessContext;",
            "import com.provectus.kafka.ui.model.rbac.permission.ConnectAction;",
            "import com.provectus.kafka.ui.service.KafkaConnectService;",
            "import java.util.Comparator;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import javax.validation.Valid;",
            "import lombok.RequiredArgsConstructor;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.springframework.http.ResponseEntity;",
            "import org.springframework.web.bind.annotation.RestController;",
            "import org.springframework.web.server.ServerWebExchange;",
            "import reactor.core.publisher.Flux;",
            "import reactor.core.publisher.Mono;"
        ],
        "reference_api": [
            "operationName",
            "updateConnectorState",
            "connect",
            "of",
            "cluster",
            "audit",
            "map",
            "validateAccess",
            "connectActions",
            "doOnEach",
            "contains",
            "then",
            "getCluster",
            "operationParams",
            "build",
            "builder"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "updateConnectorState",
                "code": "@Override\n  public Mono<ResponseEntity<Void>> updateConnectorState(String clusterName, String connectName,\n                                                         String connectorName,\n                                                         ConnectorActionDTO action,\n                                                         ServerWebExchange exchange) {\n    ConnectAction[] connectActions;\n    if (RESTART_ACTIONS.contains(action)) {\n      connectActions = new ConnectAction[] {ConnectAction.VIEW, ConnectAction.RESTART};\n    } else {\n      connectActions = new ConnectAction[] {ConnectAction.VIEW, ConnectAction.EDIT};\n    }\n\n    var context = AccessContext.builder()\n        .cluster(clusterName)\n        .connect(connectName)\n        .connectActions(connectActions)\n        .operationName(\"updateConnectorState\")\n        .operationParams(Map.of(CONNECTOR_NAME, connectorName))\n        .build();\n\n    return validateAccess(context).then(\n        kafkaConnectService\n            .updateConnectorState(getCluster(clusterName), connectName, connectorName, action)\n            .map(ResponseEntity::ok)\n    ).doOnEach(sig -> audit(context, sig));\n  }"
            }
        ],
        "third_party": [
            "operationName",
            "connect",
            "of",
            "cluster",
            "audit",
            "map",
            "validateAccess",
            "connectActions",
            "doOnEach",
            "then",
            "getCluster",
            "operationParams",
            "build",
            "builder"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig",
        "start_line": "280",
        "end_line": "287",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/controller/KafkaConnectController.java",
        "docstring": "The validateConnectorPluginConfig function validates the configuration of a connector plugin in a specified Kafka Connect cluster.\\nIt calls the kafkaConnectService to perform the validation using the cluster name, connect name, plugin name, and request body.\\nThe function then maps the validation result to an HTTP response entity with a status of OK.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "199cfe41b374",
        "ground_truth": "public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig(\n    String clusterName, String connectName, String pluginName, @Valid Mono<Map<String, Object>> requestBody,\n    ServerWebExchange exchange) {\n  return kafkaConnectService\n      .validateConnectorPluginConfig(\n          getCluster(clusterName), connectName, pluginName, requestBody)\n      .map(ResponseEntity::ok);\n}",
        "import_statements": [
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART;",
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART_ALL_TASKS;",
            "import static com.provectus.kafka.ui.model.ConnectorActionDTO.RESTART_FAILED_TASKS;",
            "import com.provectus.kafka.ui.api.KafkaConnectApi;",
            "import com.provectus.kafka.ui.model.ConnectDTO;",
            "import com.provectus.kafka.ui.model.ConnectorActionDTO;",
            "import com.provectus.kafka.ui.model.ConnectorColumnsToSortDTO;",
            "import com.provectus.kafka.ui.model.ConnectorDTO;",
            "import com.provectus.kafka.ui.model.ConnectorPluginConfigValidationResponseDTO;",
            "import com.provectus.kafka.ui.model.ConnectorPluginDTO;",
            "import com.provectus.kafka.ui.model.FullConnectorInfoDTO;",
            "import com.provectus.kafka.ui.model.NewConnectorDTO;",
            "import com.provectus.kafka.ui.model.SortOrderDTO;",
            "import com.provectus.kafka.ui.model.TaskDTO;",
            "import com.provectus.kafka.ui.model.rbac.AccessContext;",
            "import com.provectus.kafka.ui.model.rbac.permission.ConnectAction;",
            "import com.provectus.kafka.ui.service.KafkaConnectService;",
            "import java.util.Comparator;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import javax.validation.Valid;",
            "import lombok.RequiredArgsConstructor;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.springframework.http.ResponseEntity;",
            "import org.springframework.web.bind.annotation.RestController;",
            "import org.springframework.web.server.ServerWebExchange;",
            "import reactor.core.publisher.Flux;",
            "import reactor.core.publisher.Mono;"
        ],
        "reference_api": [
            "getCluster",
            "map",
            "validateConnectorPluginConfig"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "validateConnectorPluginConfig",
                "code": "@Override\n  public Mono<ResponseEntity<ConnectorPluginConfigValidationResponseDTO>> validateConnectorPluginConfig(\n      String clusterName, String connectName, String pluginName, @Valid Mono<Map<String, Object>> requestBody,\n      ServerWebExchange exchange) {\n    return kafkaConnectService\n        .validateConnectorPluginConfig(\n            getCluster(clusterName), connectName, pluginName, requestBody)\n        .map(ResponseEntity::ok);\n  }"
            }
        ],
        "third_party": [
            "getCluster",
            "map"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": "public static ConfigurableApplicationContext startApplication(String[] args)",
        "start_line": "20",
        "end_line": "25",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/KafkaUiApplication.java",
        "docstring": "The startApplication function initializes and runs a Spring application.\\nIt uses SpringApplicationBuilder to configure the application with KafkaUiApplication as the primary source.\\nIt adds an initializer for dynamic configuration properties and runs the application with the provided arguments.\\nThe function returns the resulting ConfigurableApplicationContext.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "88380ae300c4",
        "ground_truth": "public static ConfigurableApplicationContext startApplication(String[] args) {\n  return new SpringApplicationBuilder(KafkaUiApplication.class)\n      .initializers(DynamicConfigOperations.dynamicConfigPropertiesInitializer())\n      .build()\n      .run(args);\n}",
        "import_statements": [
            "import com.provectus.kafka.ui.util.DynamicConfigOperations;",
            "import org.springframework.boot.autoconfigure.SpringBootApplication;",
            "import org.springframework.boot.autoconfigure.ldap.LdapAutoConfiguration;",
            "import org.springframework.boot.builder.SpringApplicationBuilder;",
            "import org.springframework.context.ConfigurableApplicationContext;",
            "import org.springframework.scheduling.annotation.EnableAsync;",
            "import org.springframework.scheduling.annotation.EnableScheduling;"
        ],
        "reference_api": [
            "run",
            "build",
            "dynamicConfigPropertiesInitializer",
            "initializers"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "run",
            "build",
            "dynamicConfigPropertiesInitializer",
            "initializers"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search)",
        "start_line": "81",
        "end_line": "87",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaConnectService.java",
        "docstring": "The matchesSearchTerm function returns a Predicate for filtering FullConnectorInfoDTO objects based on a search term.\\nIf the search term is null, it returns a Predicate that always evaluates to true.\\nOtherwise, it returns a Predicate that checks if any of the strings obtained from the connector contain the search term, ignoring case.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3f022876f737",
        "ground_truth": "private Predicate<FullConnectorInfoDTO> matchesSearchTerm(@Nullable final String search) {\n  if (search == null) {\n    return c -> true;\n  }\n  return connector -> getStringsForSearch(connector)\n      .anyMatch(string -> StringUtils.containsIgnoreCase(string, search));\n}",
        "import_statements": [
            "import com.fasterxml.jackson.core.type.TypeReference;",
            "import com.fasterxml.jackson.databind.ObjectMapper;",
            "import com.provectus.kafka.ui.connect.api.KafkaConnectClientApi;",
            "import com.provectus.kafka.ui.connect.model.ConnectorStatus;",
            "import com.provectus.kafka.ui.connect.model.ConnectorStatusConnector;",
            "import com.provectus.kafka.ui.connect.model.ConnectorTopics;",
            "import com.provectus.kafka.ui.connect.model.TaskStatus;",
            "import com.provectus.kafka.ui.exception.NotFoundException;",
            "import com.provectus.kafka.ui.exception.ValidationException;",
            "import com.provectus.kafka.ui.mapper.ClusterMapper;",
            "import com.provectus.kafka.ui.mapper.KafkaConnectMapper;",
            "import com.provectus.kafka.ui.model.ConnectDTO;",
            "import com.provectus.kafka.ui.model.ConnectorActionDTO;",
            "import com.provectus.kafka.ui.model.ConnectorDTO;",
            "import com.provectus.kafka.ui.model.ConnectorPluginConfigValidationResponseDTO;",
            "import com.provectus.kafka.ui.model.ConnectorPluginDTO;",
            "import com.provectus.kafka.ui.model.ConnectorStateDTO;",
            "import com.provectus.kafka.ui.model.ConnectorTaskStatusDTO;",
            "import com.provectus.kafka.ui.model.FullConnectorInfoDTO;",
            "import com.provectus.kafka.ui.model.KafkaCluster;",
            "import com.provectus.kafka.ui.model.NewConnectorDTO;",
            "import com.provectus.kafka.ui.model.TaskDTO;",
            "import com.provectus.kafka.ui.model.connect.InternalConnectInfo;",
            "import com.provectus.kafka.ui.util.ReactiveFailover;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Optional;",
            "import java.util.function.Predicate;",
            "import java.util.stream.Stream;",
            "import javax.annotation.Nullable;",
            "import lombok.RequiredArgsConstructor;",
            "import lombok.SneakyThrows;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.springframework.stereotype.Service;",
            "import org.springframework.web.reactive.function.client.WebClientResponseException;",
            "import reactor.core.publisher.Flux;",
            "import reactor.core.publisher.Mono;"
        ],
        "reference_api": [
            "anyMatch",
            "getStringsForSearch",
            "containsIgnoreCase"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getStringsForSearch",
                "code": "private Stream<String> getStringsForSearch(FullConnectorInfoDTO fullConnectorInfo) {\n    return Stream.of(\n        fullConnectorInfo.getName(),\n        fullConnectorInfo.getConnect(),\n        fullConnectorInfo.getStatus().getState().getValue(),\n        fullConnectorInfo.getType().getValue());\n  }"
            }
        ],
        "third_party": [
            "anyMatch",
            "containsIgnoreCase"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": "  private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                       @Nullable Map<String, Object> propertiesMap)",
        "start_line": "185",
        "end_line": "199",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/config/ClustersProperties.java",
        "docstring": "The flattenClusterProperties function recursively flattens a nested map of cluster properties.\\nIt takes an optional prefix and a properties map, initializing an empty map for the flattened result.\\nFor each entry in the properties map, it constructs a new key by combining the prefix and the current key.\\nIf the value is another map, it recursively flattens it with the new key as the prefix.\\nOtherwise, it adds the key-value pair to the flattened map.\\nFinally, it returns the flattened map.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "9754aa1ec96c",
        "ground_truth": "private Map<String, Object> flattenClusterProperties(@Nullable String prefix,\n                                                     @Nullable Map<String, Object> propertiesMap) {\n  Map<String, Object> flattened = new HashMap<>();\n  if (propertiesMap != null) {\n    propertiesMap.forEach((k, v) -> {\n      String key = prefix == null ? k : prefix + \".\" + k;\n      if (v instanceof Map<?, ?>) {\n        flattened.putAll(flattenClusterProperties(key, (Map<String, Object>) v));\n      } else {\n        flattened.put(key, v);\n      }\n    });\n  }\n  return flattened;\n}",
        "import_statements": [
            "import com.provectus.kafka.ui.model.MetricsConfig;",
            "import jakarta.annotation.PostConstruct;",
            "import java.util.ArrayList;",
            "import java.util.HashMap;",
            "import java.util.HashSet;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import javax.annotation.Nullable;",
            "import lombok.AllArgsConstructor;",
            "import lombok.Builder;",
            "import lombok.Data;",
            "import lombok.NoArgsConstructor;",
            "import lombok.ToString;",
            "import org.springframework.boot.context.properties.ConfigurationProperties;",
            "import org.springframework.context.annotation.Configuration;",
            "import org.springframework.util.StringUtils;"
        ],
        "reference_api": [
            "forEach",
            "putAll",
            "put",
            "flattenClusterProperties"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "flattenClusterProperties",
                "code": "private void flattenClusterProperties() {\n    for (Cluster cluster : clusters) {\n      cluster.setProperties(flattenClusterProperties(null, cluster.getProperties()));\n    }\n  }"
            }
        ],
        "third_party": [
            "forEach",
            "putAll",
            "put"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": " private static Set<String> kafkaConfigKeysToSanitize()",
        "start_line": "60",
        "end_line": "68",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/KafkaConfigSanitizer.java",
        "docstring": "The kafkaConfigKeysToSanitize function identifies Kafka configuration keys that need to be sanitized.\\nIt creates a ConfigDef instance and adds SSL and SASL support configurations.\\nThe function filters the configuration keys to find those of type PASSWORD and collects them into a set, which it then returns.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "99266677c846",
        "ground_truth": "private static Set<String> kafkaConfigKeysToSanitize() {\n  final ConfigDef configDef = new ConfigDef();\n  SslConfigs.addClientSslSupport(configDef);\n  SaslConfigs.addClientSaslSupport(configDef);\n  return configDef.configKeys().entrySet().stream()\n      .filter(entry -> entry.getValue().type().equals(ConfigDef.Type.PASSWORD))\n      .map(Map.Entry::getKey)\n      .collect(Collectors.toSet());\n}",
        "import_statements": [
            "import static java.util.regex.Pattern.CASE_INSENSITIVE;",
            "import com.google.common.collect.ImmutableList;",
            "import java.util.Arrays;",
            "import java.util.Collection;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import java.util.regex.Pattern;",
            "import java.util.stream.Collectors;",
            "import javax.annotation.Nullable;",
            "import org.apache.kafka.common.config.ConfigDef;",
            "import org.apache.kafka.common.config.SaslConfigs;",
            "import org.apache.kafka.common.config.SslConfigs;",
            "import org.springframework.beans.factory.annotation.Value;",
            "import org.springframework.stereotype.Component;"
        ],
        "reference_api": [
            "filter",
            "configKeys",
            "addClientSslSupport",
            "equals",
            "getValue",
            "map",
            "addClientSaslSupport",
            "type",
            "toSet",
            "entrySet",
            "collect",
            "stream"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "filter",
            "configKeys",
            "addClientSslSupport",
            "getValue",
            "map",
            "addClientSaslSupport",
            "type",
            "toSet",
            "entrySet",
            "collect",
            "stream"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription)",
        "start_line": "54",
        "end_line": "61",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/service/FeatureService.java",
        "docstring": "The aclEdit function checks if ACL editing is allowed for a given cluster.\\nIt retrieves the authorized operations from the cluster description and determines if ACL view is enabled and if the user has permissions for all or alter operations.\\nIf editing is allowed, it returns a Mono containing the KAFKA_ACL_EDIT feature; otherwise, it returns an empty Mono.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "deebb6fcb0a8",
        "ground_truth": "private Mono<ClusterFeature> aclEdit(ReactiveAdminClient adminClient, ClusterDescription clusterDescription) {\n  var authorizedOps = Optional.ofNullable(clusterDescription.getAuthorizedOperations()).orElse(Set.of());\n  boolean canEdit = aclViewEnabled(adminClient)\n      && (authorizedOps.contains(AclOperation.ALL) || authorizedOps.contains(AclOperation.ALTER));\n  return canEdit\n      ? Mono.just(ClusterFeature.KAFKA_ACL_EDIT)\n      : Mono.empty();\n}",
        "import_statements": [
            "import com.provectus.kafka.ui.model.ClusterFeature;",
            "import com.provectus.kafka.ui.model.KafkaCluster;",
            "import com.provectus.kafka.ui.service.ReactiveAdminClient.ClusterDescription;",
            "import java.util.ArrayList;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Optional;",
            "import java.util.Set;",
            "import java.util.function.Predicate;",
            "import lombok.extern.slf4j.Slf4j;",
            "import org.apache.kafka.common.acl.AclOperation;",
            "import org.springframework.stereotype.Service;",
            "import reactor.core.publisher.Flux;",
            "import reactor.core.publisher.Mono;"
        ],
        "reference_api": [
            "orElse",
            "of",
            "just",
            "ofNullable",
            "getAuthorizedOperations",
            "contains",
            "empty",
            "aclViewEnabled"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "aclViewEnabled",
                "code": "private boolean aclViewEnabled(ReactiveAdminClient adminClient) {\n    return adminClient.getClusterFeatures().contains(ReactiveAdminClient.SupportedFeature.AUTHORIZED_SECURITY_ENABLED);\n  }"
            }
        ],
        "third_party": [
            "orElse",
            "of",
            "just",
            "ofNullable",
            "getAuthorizedOperations",
            "empty"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "provectus/kafka-ui",
        "function_declaration": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                   SeekOperations seekOperations)",
        "start_line": "37",
        "end_line": "59",
        "file_path": "kafka-ui-api/src/main/java/com/provectus/kafka/ui/emitter/BackwardEmitter.java",
        "docstring": "The nextPollingRange function calculates the next range of offsets to poll for each topic partition.\\nIt initializes a map of offsets to read from based on the previous range or the offsets from seek operations if the previous range is empty.\\nIt calculates the number of messages to poll per partition.\\nFor each topic partition, it determines the start offset and adjusts it based on the number of messages to poll, ensuring it does not go below the beginning offset.\\nThe function returns a map of topic partitions to their respective polling ranges.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a5d8cc77955f",
        "ground_truth": "protected TreeMap<TopicPartition, FromToOffset> nextPollingRange(TreeMap<TopicPartition, FromToOffset> prevRange,\n                                                                 SeekOperations seekOperations) {\n  TreeMap<TopicPartition, Long> readToOffsets = new TreeMap<>(Comparator.comparingInt(TopicPartition::partition));\n  if (prevRange.isEmpty()) {\n    readToOffsets.putAll(seekOperations.getOffsetsForSeek());\n  } else {\n    readToOffsets.putAll(\n        prevRange.entrySet()\n            .stream()\n            .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().from()))\n    );\n  }\n  int msgsToPollPerPartition = (int) Math.ceil((double) messagesPerPage / readToOffsets.size());\n  TreeMap<TopicPartition, FromToOffset> result = new TreeMap<>(Comparator.comparingInt(TopicPartition::partition));\n  readToOffsets.forEach((tp, toOffset) -> {\n    long tpStartOffset = seekOperations.getBeginOffsets().get(tp);\n    if (toOffset > tpStartOffset) {\n      result.put(tp, new FromToOffset(Math.max(tpStartOffset, toOffset - msgsToPollPerPartition), toOffset));\n    }\n  });\n  return result;\n}",
        "import_statements": [
            "import com.provectus.kafka.ui.model.ConsumerPosition;",
            "import com.provectus.kafka.ui.model.TopicMessageDTO;",
            "import com.provectus.kafka.ui.serdes.ConsumerRecordDeserializer;",
            "import java.util.Comparator;",
            "import java.util.Map;",
            "import java.util.TreeMap;",
            "import java.util.function.Predicate;",
            "import java.util.function.Supplier;",
            "import java.util.stream.Collectors;",
            "import org.apache.kafka.common.TopicPartition;"
        ],
        "reference_api": [
            "comparingInt",
            "putAll",
            "from",
            "toMap",
            "getValue",
            "getOffsetsForSeek",
            "size",
            "entrySet",
            "collect",
            "forEach",
            "get",
            "getBeginOffsets",
            "ceil",
            "max",
            "put",
            "stream",
            "isEmpty"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "comparingInt",
            "putAll",
            "from",
            "toMap",
            "getValue",
            "getOffsetsForSeek",
            "size",
            "entrySet",
            "collect",
            "forEach",
            "get",
            "getBeginOffsets",
            "ceil",
            "max",
            "put",
            "stream",
            "isEmpty"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "Graylog2/graylog2-server",
        "function_declaration": "public void doLaunch(final MessageInput input)",
        "start_line": "198",
        "end_line": "222",
        "file_path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java",
        "docstring": "The doLaunch function initializes and launches a message input based on the configuration mode.\\nIt checks if legacy mode is enabled and validates the corresponding configuration settings for ZooKeeper or bootstrap servers.\\nThe function waits for the server to be running and registers for server lifecycle events.\\nDepending on the mode, it either launches the legacy input or a consumer input.\\nIt also schedules a task to update the last second bytes read every second.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "c4c657caeba5",
        "ground_truth": "public void doLaunch(final MessageInput input) {\n    final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n    if (legacyMode) {\n        final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n        if (Strings.isNullOrEmpty(zooKeper)) {\n            throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n        }\n    } else {\n        final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n        if (Strings.isNullOrEmpty(bootStrap)) {\n            throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n        }\n    }\n    serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n    // listen for lifecycle changes\n    serverEventBus.register(this);\n    if (legacyMode) {\n        doLaunchLegacy(input);\n    } else {\n        doLaunchConsumer(input);\n    }\n    scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n}",
        "import_statements": [
            "import com.codahale.metrics.Gauge;",
            "import com.codahale.metrics.InstrumentedExecutorService;",
            "import com.codahale.metrics.MetricRegistry;",
            "import com.codahale.metrics.MetricSet;",
            "import com.google.common.base.Strings;",
            "import com.google.common.collect.ImmutableMap;",
            "import com.google.common.eventbus.EventBus;",
            "import com.google.common.eventbus.Subscribe;",
            "import com.google.common.util.concurrent.ThreadFactoryBuilder;",
            "import com.google.common.util.concurrent.Uninterruptibles;",
            "import com.google.inject.assistedinject.Assisted;",
            "import com.google.inject.assistedinject.AssistedInject;",
            "import org.apache.kafka.clients.consumer.ConsumerRecord;",
            "import org.apache.kafka.clients.consumer.ConsumerRecords;",
            "import org.apache.kafka.clients.consumer.InvalidOffsetException;",
            "import org.apache.kafka.clients.consumer.KafkaConsumer;",
            "import org.apache.kafka.common.KafkaException;",
            "import org.apache.kafka.common.errors.AuthorizationException;",
            "import org.apache.kafka.common.errors.WakeupException;",
            "import org.apache.kafka.common.serialization.ByteArrayDeserializer;",
            "import org.graylog.shaded.kafka09.consumer.Consumer;",
            "import org.graylog.shaded.kafka09.consumer.ConsumerConfig;",
            "import org.graylog.shaded.kafka09.consumer.ConsumerIterator;",
            "import org.graylog.shaded.kafka09.consumer.ConsumerTimeoutException;",
            "import org.graylog.shaded.kafka09.consumer.KafkaStream;",
            "import org.graylog.shaded.kafka09.consumer.TopicFilter;",
            "import org.graylog.shaded.kafka09.consumer.Whitelist;",
            "import org.graylog.shaded.kafka09.javaapi.consumer.ConsumerConnector;",
            "import org.graylog.shaded.kafka09.message.MessageAndMetadata;",
            "import org.graylog2.plugin.LocalMetricRegistry;",
            "import org.graylog2.plugin.ServerStatus;",
            "import org.graylog2.plugin.configuration.Configuration;",
            "import org.graylog2.plugin.configuration.ConfigurationRequest;",
            "import org.graylog2.plugin.configuration.fields.BooleanField;",
            "import org.graylog2.plugin.configuration.fields.ConfigurationField;",
            "import org.graylog2.plugin.configuration.fields.DropdownField;",
            "import org.graylog2.plugin.configuration.fields.NumberField;",
            "import org.graylog2.plugin.configuration.fields.TextField;",
            "import org.graylog2.plugin.inputs.MessageInput;",
            "import org.graylog2.plugin.inputs.annotations.ConfigClass;",
            "import org.graylog2.plugin.inputs.annotations.FactoryClass;",
            "import org.graylog2.plugin.inputs.codecs.CodecAggregator;",
            "import org.graylog2.plugin.inputs.transports.ThrottleableTransport;",
            "import org.graylog2.plugin.inputs.transports.Transport;",
            "import org.graylog2.plugin.journal.RawMessage;",
            "import org.graylog2.plugin.lifecycles.Lifecycle;",
            "import org.graylog2.plugin.system.NodeId;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import jakarta.inject.Named;",
            "import java.io.ByteArrayInputStream;",
            "import java.io.IOException;",
            "import java.nio.charset.StandardCharsets;",
            "import java.time.Duration;",
            "import java.util.List;",
            "import java.util.Optional;",
            "import java.util.Properties;",
            "import java.util.concurrent.CountDownLatch;",
            "import java.util.concurrent.ExecutorService;",
            "import java.util.concurrent.Executors;",
            "import java.util.concurrent.ScheduledExecutorService;",
            "import java.util.concurrent.ThreadFactory;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.concurrent.atomic.AtomicLong;",
            "import java.util.regex.Pattern;",
            "import java.util.stream.IntStream;",
            "import static com.codahale.metrics.MetricRegistry.name;"
        ],
        "reference_api": [
            "getString",
            "isNullOrEmpty",
            "set",
            "lifecycleStateChange",
            "awaitRunning",
            "scheduleAtFixedRate",
            "getBoolean",
            "doLaunchConsumer",
            "getAndSet",
            "register",
            "doLaunchLegacy"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "lifecycleStateChange",
                "code": "@Subscribe\n    public void lifecycleStateChange(Lifecycle lifecycle) {\n        LOG.debug(\"Lifecycle changed to {}\", lifecycle);\n        switch (lifecycle) {\n            case PAUSED:\n            case FAILED:\n            case HALTING:\n                pausedLatch = new CountDownLatch(1);\n                paused = true;\n                break;\n            default:\n                paused = false;\n                pausedLatch.countDown();\n                break;\n        }\n    }"
            },
            {
                "name": "doLaunchConsumer",
                "code": "private void doLaunchConsumer(final MessageInput input) {\n        final Properties props = new Properties();\n\n        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n        //noinspection ConstantConditions\n        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n        // Map largest -> latest, smallest -> earliest\n        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n        // if something breaks.\n        props.put(\"auto.commit.interval.ms\", \"1000\");\n        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        insertCustomProperties(props);\n\n        final int numThreads = configuration.getInt(CK_THREADS);\n        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n        // and then shutting down the connection.\n        // this is to avoid yanking away the connection from the consumer runnables\n        stopLatch = new CountDownLatch(numThreads);\n\n        IntStream.range(0, numThreads).forEach(i -> executor.submit(new ConsumerRunnable(props, input, i)));\n    }"
            },
            {
                "name": "doLaunchLegacy",
                "code": "private void doLaunchLegacy(final MessageInput input) {\n        final Properties props = new Properties();\n\n        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n        props.put(\"client.id\", \"gl2-\" + nodeId.getShortNodeId() + \"-\" + input.getId());\n\n        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n        props.put(\"fetch.wait.max.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n        props.put(\"zookeeper.connect\", configuration.getString(CK_ZOOKEEPER));\n        props.put(\"auto.offset.reset\", configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET));\n        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n        // if something breaks.\n        props.put(\"auto.commit.interval.ms\", \"1000\");\n        // Set a consumer timeout to avoid blocking on the consumer iterator.\n        props.put(\"consumer.timeout.ms\", \"1000\");\n\n        insertCustomProperties(props);\n\n        final int numThreads = configuration.getInt(CK_THREADS);\n        final ConsumerConfig consumerConfig = new ConsumerConfig(props);\n        cc = Consumer.createJavaConsumerConnector(consumerConfig);\n\n        final TopicFilter filter = new Whitelist(configuration.getString(CK_TOPIC_FILTER));\n\n        final List<KafkaStream<byte[], byte[]>> streams = cc.createMessageStreamsByFilter(filter, numThreads);\n\n        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n        // and then shutting down the connection.\n        // this is to avoid yanking away the connection from the consumer runnables\n        stopLatch = new CountDownLatch(streams.size());\n\n        for (final KafkaStream<byte[], byte[]> stream : streams) {\n            executor.submit(new Runnable() {\n                @Override\n                public void run() {\n                    final ConsumerIterator<byte[], byte[]> consumerIterator = stream.iterator();\n                    boolean retry;\n\n                    do {\n                        retry = false;\n\n                        try {\n                            // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n                            // noinspection WhileLoopReplaceableByForEach\n                            while (consumerIterator.hasNext()) {\n                                if (paused) {\n                                    // we try not to spin here, so we wait until the lifecycle goes back to running.\n                                    LOG.debug(\n                                            \"Message processing is paused, blocking until message processing is turned back on.\");\n                                    Uninterruptibles.awaitUninterruptibly(pausedLatch);\n                                }\n                                // check for being stopped before actually getting the message, otherwise we could end up losing that message\n                                if (stopped) {\n                                    break;\n                                }\n                                if (isThrottled()) {\n                                    blockUntilUnthrottled();\n                                }\n\n                                // process the message, this will immediately mark the message as having been processed. this gets tricky\n                                // if we get an exception about processing it down below.\n                                final MessageAndMetadata<byte[], byte[]> message = consumerIterator.next();\n\n                                final byte[] bytes = message.message();\n\n                                // it is possible that the message is null\n                                if (bytes == null) {\n                                    continue;\n                                }\n\n                                totalBytesRead.addAndGet(bytes.length);\n                                lastSecBytesReadTmp.addAndGet(bytes.length);\n\n                                final RawMessage rawMessage = new RawMessage(bytes);\n\n                                input.processRawMessage(rawMessage);\n                            }\n                        } catch (ConsumerTimeoutException e) {\n                            // Happens when there is nothing to consume, retry to check again.\n                            retry = true;\n                        } catch (Exception e) {\n                            LOG.error(\"Kafka consumer error, stopping consumer thread.\", e);\n                        }\n                    } while (retry && !stopped);\n                    // explicitly commit our offsets when stopping.\n                    // this might trigger a couple of times, but it won't hurt\n                    cc.commitOffsets();\n                    stopLatch.countDown();\n                }\n            });\n        }\n    }"
            }
        ],
        "third_party": [
            "getString",
            "isNullOrEmpty",
            "set",
            "awaitRunning",
            "scheduleAtFixedRate",
            "getBoolean",
            "getAndSet",
            "register"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "Graylog2/graylog2-server",
        "function_declaration": "private void insertCustomProperties(Properties props)",
        "start_line": "434",
        "end_line": "442",
        "file_path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java",
        "docstring": "The insertCustomProperties function adds custom properties to a given Properties object.\\nIt loads custom properties from a configuration string, converts it into a Properties object, and merges it with the existing properties.\\nIf an IOException occurs during this process, it logs an error message indicating the failure.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "dffe85dbb47b",
        "ground_truth": "private void insertCustomProperties(Properties props) {\n    try {\n        final Properties customProperties = new Properties();\n        customProperties.load(new ByteArrayInputStream(configuration.getString(CK_CUSTOM_PROPERTIES, \"\").getBytes(StandardCharsets.UTF_8)));\n        props.putAll(customProperties);\n    } catch (IOException e) {\n        LOG.error(\"Failed to read custom properties\", e);\n    }\n}",
        "import_statements": [
            "import com.codahale.metrics.Gauge;",
            "import com.codahale.metrics.InstrumentedExecutorService;",
            "import com.codahale.metrics.MetricRegistry;",
            "import com.codahale.metrics.MetricSet;",
            "import com.google.common.base.Strings;",
            "import com.google.common.collect.ImmutableMap;",
            "import com.google.common.eventbus.EventBus;",
            "import com.google.common.eventbus.Subscribe;",
            "import com.google.common.util.concurrent.ThreadFactoryBuilder;",
            "import com.google.common.util.concurrent.Uninterruptibles;",
            "import com.google.inject.assistedinject.Assisted;",
            "import com.google.inject.assistedinject.AssistedInject;",
            "import org.apache.kafka.clients.consumer.ConsumerRecord;",
            "import org.apache.kafka.clients.consumer.ConsumerRecords;",
            "import org.apache.kafka.clients.consumer.InvalidOffsetException;",
            "import org.apache.kafka.clients.consumer.KafkaConsumer;",
            "import org.apache.kafka.common.KafkaException;",
            "import org.apache.kafka.common.errors.AuthorizationException;",
            "import org.apache.kafka.common.errors.WakeupException;",
            "import org.apache.kafka.common.serialization.ByteArrayDeserializer;",
            "import org.graylog.shaded.kafka09.consumer.Consumer;",
            "import org.graylog.shaded.kafka09.consumer.ConsumerConfig;",
            "import org.graylog.shaded.kafka09.consumer.ConsumerIterator;",
            "import org.graylog.shaded.kafka09.consumer.ConsumerTimeoutException;",
            "import org.graylog.shaded.kafka09.consumer.KafkaStream;",
            "import org.graylog.shaded.kafka09.consumer.TopicFilter;",
            "import org.graylog.shaded.kafka09.consumer.Whitelist;",
            "import org.graylog.shaded.kafka09.javaapi.consumer.ConsumerConnector;",
            "import org.graylog.shaded.kafka09.message.MessageAndMetadata;",
            "import org.graylog2.plugin.LocalMetricRegistry;",
            "import org.graylog2.plugin.ServerStatus;",
            "import org.graylog2.plugin.configuration.Configuration;",
            "import org.graylog2.plugin.configuration.ConfigurationRequest;",
            "import org.graylog2.plugin.configuration.fields.BooleanField;",
            "import org.graylog2.plugin.configuration.fields.ConfigurationField;",
            "import org.graylog2.plugin.configuration.fields.DropdownField;",
            "import org.graylog2.plugin.configuration.fields.NumberField;",
            "import org.graylog2.plugin.configuration.fields.TextField;",
            "import org.graylog2.plugin.inputs.MessageInput;",
            "import org.graylog2.plugin.inputs.annotations.ConfigClass;",
            "import org.graylog2.plugin.inputs.annotations.FactoryClass;",
            "import org.graylog2.plugin.inputs.codecs.CodecAggregator;",
            "import org.graylog2.plugin.inputs.transports.ThrottleableTransport;",
            "import org.graylog2.plugin.inputs.transports.Transport;",
            "import org.graylog2.plugin.journal.RawMessage;",
            "import org.graylog2.plugin.lifecycles.Lifecycle;",
            "import org.graylog2.plugin.system.NodeId;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import jakarta.inject.Named;",
            "import java.io.ByteArrayInputStream;",
            "import java.io.IOException;",
            "import java.nio.charset.StandardCharsets;",
            "import java.time.Duration;",
            "import java.util.List;",
            "import java.util.Optional;",
            "import java.util.Properties;",
            "import java.util.concurrent.CountDownLatch;",
            "import java.util.concurrent.ExecutorService;",
            "import java.util.concurrent.Executors;",
            "import java.util.concurrent.ScheduledExecutorService;",
            "import java.util.concurrent.ThreadFactory;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.concurrent.atomic.AtomicLong;",
            "import java.util.regex.Pattern;",
            "import java.util.stream.IntStream;",
            "import static com.codahale.metrics.MetricRegistry.name;"
        ],
        "reference_api": [
            "getBytes",
            "getString",
            "putAll",
            "error",
            "load"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getBytes",
            "getString",
            "putAll",
            "error",
            "load"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "Graylog2/graylog2-server",
        "function_declaration": "public void write(List<RawMessageEvent> entries) throws MessageQueueException",
        "start_line": "74",
        "end_line": "102",
        "file_path": "graylog2-server/src/main/java/org/graylog2/shared/messageq/localkafka/LocalKafkaMessageQueueWriter.java",
        "docstring": "The write function writes a list of RawMessageEvent entries to a journal and updates metrics.\\nIt filters out null entries and maps each event to a journal entry while calculating the total message bytes.\\nIt attempts to write the entries to the journal, and if it fails, retries using an exponential back-off strategy.\\nAfter successfully writing, it updates the written messages and bytes metrics.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ea146a7bc30c",
        "ground_truth": "public void write(List<RawMessageEvent> entries) throws MessageQueueException {\n    final AtomicLong msgBytes = new AtomicLong(0);\n    final List<Journal.Entry> journalEntries = entries.stream()\n            .filter(Objects::nonNull)\n            .map(e -> new Journal.Entry(e.getMessageIdBytes(), e.getEncodedRawMessage()))\n            .peek(e -> msgBytes.addAndGet(e.getMessageBytes().length))\n            .collect(Collectors.toList());\n    try {\n        writeToJournal(journalEntries);\n    } catch (Exception e) {\n        LOG.error(\"Unable to write to journal - retrying\", e);\n        // Use retryer with exponential back-off to avoid spamming the logs.\n        try {\n            writeRetryer.call(() -> {\n                writeToJournal(journalEntries);\n                return null;\n            });\n        } catch (ExecutionException | RetryException ex) {\n            throw new MessageQueueException(\"Retryer exception\", ex);\n        }\n    }\n    metrics.writtenMessages().mark(journalEntries.size());\n    metrics.writtenBytes().mark(msgBytes.get());\n}",
        "import_statements": [
            "import com.github.rholder.retry.RetryException;",
            "import com.github.rholder.retry.Retryer;",
            "import com.github.rholder.retry.RetryerBuilder;",
            "import com.github.rholder.retry.StopStrategies;",
            "import com.github.rholder.retry.WaitStrategies;",
            "import com.google.common.util.concurrent.AbstractIdleService;",
            "import org.graylog2.shared.buffers.RawMessageEvent;",
            "import org.graylog2.shared.journal.Journal;",
            "import org.graylog2.shared.journal.LocalKafkaJournal;",
            "import org.graylog2.shared.messageq.MessageQueueException;",
            "import org.graylog2.shared.messageq.MessageQueueWriter;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import jakarta.inject.Inject;",
            "import jakarta.inject.Named;",
            "import jakarta.inject.Singleton;",
            "import java.util.List;",
            "import java.util.Objects;",
            "import java.util.concurrent.ExecutionException;",
            "import java.util.concurrent.Semaphore;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.concurrent.atomic.AtomicLong;",
            "import java.util.stream.Collectors;"
        ],
        "reference_api": [
            "filter",
            "call",
            "toList",
            "getEncodedRawMessage",
            "addAndGet",
            "error",
            "writtenBytes",
            "map",
            "peek",
            "size",
            "collect",
            "getMessageIdBytes",
            "writtenMessages",
            "get",
            "writeToJournal",
            "stream",
            "mark",
            "getMessageBytes"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "writeToJournal",
                "code": "private void writeToJournal(List<Journal.Entry> entries) {\n        final long lastOffset = kafkaJournal.write(entries);\n\n        LOG.debug(\"Processed batch, last journal offset: {}, signalling reader.\",\n                lastOffset);\n        journalFilled.release();\n    }"
            }
        ],
        "third_party": [
            "filter",
            "call",
            "toList",
            "getEncodedRawMessage",
            "addAndGet",
            "error",
            "writtenBytes",
            "map",
            "peek",
            "size",
            "collect",
            "getMessageIdBytes",
            "writtenMessages",
            "get",
            "stream",
            "mark",
            "getMessageBytes"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "Graylog2/graylog2-server",
        "function_declaration": "public void acknowledge(List<Message> messages)",
        "start_line": "42",
        "end_line": "52",
        "file_path": "graylog2-server/src/main/java/org/graylog2/shared/messageq/localkafka/LocalKafkaMessageQueueAcknowledger.java",
        "docstring": "The acknowledge function processes a list of messages to acknowledge them.\\nIt identifies the maximum valid message queue ID from the list of messages and acknowledges it if present.\\nFinally, it updates the metrics by marking the number of acknowledged messages.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "944e77614042",
        "ground_truth": "public void acknowledge(List<Message> messages) {\n    @SuppressWarnings(\"ConstantConditions\")\n    final Optional<Long> max =\n            messages.stream()\n                    .map(Message::getMessageQueueId)\n                    .filter(this::isValidMessageQueueId)\n                    .map(Long.class::cast)\n                    .max(Long::compare);\n    max.ifPresent(this::doAcknowledge);\n    metrics.acknowledgedMessages().mark(messages.size());\n}",
        "import_statements": [
            "import org.graylog2.plugin.Message;",
            "import org.graylog2.shared.journal.LocalKafkaJournal;",
            "import org.graylog2.shared.messageq.AbstractMessageQueueAcknowledger;",
            "import org.graylog2.shared.messageq.MessageQueueAcknowledger;",
            "import jakarta.inject.Inject;",
            "import jakarta.inject.Singleton;",
            "import java.util.List;",
            "import java.util.Optional;"
        ],
        "reference_api": [
            "filter",
            "map",
            "acknowledgedMessages",
            "size",
            "ifPresent",
            "max",
            "stream",
            "mark"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "filter",
            "map",
            "acknowledgedMessages",
            "size",
            "ifPresent",
            "max",
            "stream",
            "mark"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "Graylog2/graylog2-server",
        "function_declaration": "private void registerLegacyMetrics()",
        "start_line": "389",
        "end_line": "401",
        "file_path": "graylog2-server/src/main/java/org/graylog2/shared/journal/LocalKafkaJournal.java",
        "docstring": "The registerLegacyMetrics function registers legacy metrics in the metric registry.\\nIt filters the metrics to include only those starting with the LocalKafkaJournal class name.\\nFor each filtered metric, it constructs a legacy metric name and attempts to register it under this name in the metric registry.\\nIf registration fails, it logs a warning message indicating the original and legacy metric names.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "981280879490",
        "ground_truth": "private void registerLegacyMetrics() {\n    this.metricRegistry.getMetrics().entrySet().stream()\n            .filter(entry -> entry.getKey().startsWith(LocalKafkaJournal.class.getName()))\n            .forEach(entry -> {\n                String legacyName = LEGACY_CLASS_NAME +\n                        StringUtils.removeStart(entry.getKey(), LocalKafkaJournal.class.getName());\n                try {\n                    this.metricRegistry.register(legacyName, entry.getValue());\n                } catch (Exception e) {\n                    LOG.warn(\"Unable to register metric <{}> under legacy name <{}>.\", entry.getKey(), legacyName);\n                }\n            });\n}",
        "import_statements": [
            "import com.codahale.metrics.Gauge;",
            "import com.codahale.metrics.Meter;",
            "import com.codahale.metrics.MetricFilter;",
            "import com.codahale.metrics.MetricRegistry;",
            "import com.codahale.metrics.Timer;",
            "import com.github.joschi.jadconfig.util.Size;",
            "import com.google.common.collect.ImmutableMap;",
            "import com.google.common.collect.Iterables;",
            "import com.google.common.collect.Sets;",
            "import com.google.common.io.Files;",
            "import com.google.common.primitives.Ints;",
            "import com.google.common.util.concurrent.AbstractIdleService;",
            "import com.google.common.util.concurrent.Uninterruptibles;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.graylog.shaded.kafka09.common.KafkaException;",
            "import org.graylog.shaded.kafka09.common.OffsetOutOfRangeException;",
            "import org.graylog.shaded.kafka09.common.TopicAndPartition;",
            "import org.graylog.shaded.kafka09.log.CleanerConfig;",
            "import org.graylog.shaded.kafka09.log.Log;",
            "import org.graylog.shaded.kafka09.log.LogAppendInfo;",
            "import org.graylog.shaded.kafka09.log.LogConfig;",
            "import org.graylog.shaded.kafka09.log.LogManager;",
            "import org.graylog.shaded.kafka09.log.LogSegment;",
            "import org.graylog.shaded.kafka09.message.ByteBufferMessageSet;",
            "import org.graylog.shaded.kafka09.message.Message;",
            "import org.graylog.shaded.kafka09.message.MessageAndOffset;",
            "import org.graylog.shaded.kafka09.message.MessageSet;",
            "import org.graylog.shaded.kafka09.scala.Option;",
            "import org.graylog.shaded.kafka09.scala.collection.Iterator;",
            "import org.graylog.shaded.kafka09.scala.collection.JavaConversions;",
            "import org.graylog.shaded.kafka09.scala.collection.Map$;",
            "import org.graylog.shaded.kafka09.scala.runtime.AbstractFunction1;",
            "import org.graylog.shaded.kafka09.server.BrokerState;",
            "import org.graylog.shaded.kafka09.server.RunningAsBroker;",
            "import org.graylog.shaded.kafka09.utils.KafkaScheduler;",
            "import org.graylog.shaded.kafka09.utils.Time;",
            "import org.graylog2.plugin.GlobalMetricNames;",
            "import org.graylog2.plugin.ServerStatus;",
            "import org.graylog2.plugin.ThrottleState;",
            "import org.graylog2.plugin.lifecycles.LoadBalancerStatus;",
            "import org.graylog2.shared.metrics.HdrTimer;",
            "import org.graylog2.shared.utilities.ByteBufferUtils;",
            "import org.joda.time.DateTimeUtils;",
            "import org.joda.time.Duration;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import jakarta.inject.Inject;",
            "import jakarta.inject.Named;",
            "import jakarta.inject.Singleton;",
            "import java.io.File;",
            "import java.io.FileOutputStream;",
            "import java.io.IOException;",
            "import java.io.SyncFailedException;",
            "import java.io.UncheckedIOException;",
            "import java.nio.channels.ClosedByInterruptException;",
            "import java.nio.charset.StandardCharsets;",
            "import java.nio.file.AccessDeniedException;",
            "import java.nio.file.Path;",
            "import java.util.ArrayList;",
            "import java.util.Collections;",
            "import java.util.Date;",
            "import java.util.HashSet;",
            "import java.util.List;",
            "import java.util.Locale;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import java.util.SortedMap;",
            "import java.util.concurrent.Callable;",
            "import java.util.concurrent.ScheduledExecutorService;",
            "import java.util.concurrent.ScheduledFuture;",
            "import java.util.concurrent.atomic.AtomicInteger;",
            "import java.util.concurrent.atomic.AtomicLong;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "import static com.codahale.metrics.MetricRegistry.name;",
            "import static java.util.concurrent.TimeUnit.DAYS;",
            "import static java.util.concurrent.TimeUnit.MILLISECONDS;",
            "import static java.util.concurrent.TimeUnit.MINUTES;",
            "import static java.util.concurrent.TimeUnit.NANOSECONDS;",
            "import static java.util.concurrent.TimeUnit.SECONDS;",
            "import static org.graylog2.plugin.Tools.bytesToHex;"
        ],
        "reference_api": [
            "filter",
            "getName",
            "getValue",
            "forEach",
            "entrySet",
            "removeStart",
            "getMetrics",
            "getKey",
            "warn",
            "register",
            "stream",
            "startsWith"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getValue",
                "code": "@Override\n                public Date getValue() {\n                    long oldestSegment = Long.MAX_VALUE;\n                    for (final LogSegment segment : LocalKafkaJournal.this.getSegments()) {\n                        oldestSegment = Math.min(oldestSegment, segment.created());\n                    }\n\n                    return new Date(oldestSegment);\n                }"
            }
        ],
        "third_party": [
            "filter",
            "getName",
            "forEach",
            "entrySet",
            "removeStart",
            "getMetrics",
            "getKey",
            "warn",
            "register",
            "stream",
            "startsWith"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "Graylog2/graylog2-server",
        "function_declaration": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName)",
        "start_line": "417",
        "end_line": "426",
        "file_path": "graylog2-server/src/main/java/org/graylog2/shared/journal/LocalKafkaJournal.java",
        "docstring": "The registerHdrTimer function registers an HdrTimer with a given metric name in a MetricRegistry.\\nIt attempts to register the timer and, if an IllegalArgumentException is thrown (indicating the metric name is already registered), retrieves the existing timer with that name.\\nFinally, it returns the registered or retrieved timer.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3ca34bfd2d64",
        "ground_truth": "private Timer registerHdrTimer(MetricRegistry metricRegistry, final String metricName) {\n    Timer timer;\n    try {\n        timer = metricRegistry.register(metricName, new HdrTimer(1, MINUTES, 1));\n    } catch (IllegalArgumentException e) {\n        final SortedMap<String, Timer> timers = metricRegistry.getTimers((name, metric) -> metricName.equals(name));\n        timer = Iterables.getOnlyElement(timers.values());\n    }\n    return timer;\n}",
        "import_statements": [
            "import com.codahale.metrics.Gauge;",
            "import com.codahale.metrics.Meter;",
            "import com.codahale.metrics.MetricFilter;",
            "import com.codahale.metrics.MetricRegistry;",
            "import com.codahale.metrics.Timer;",
            "import com.github.joschi.jadconfig.util.Size;",
            "import com.google.common.collect.ImmutableMap;",
            "import com.google.common.collect.Iterables;",
            "import com.google.common.collect.Sets;",
            "import com.google.common.io.Files;",
            "import com.google.common.primitives.Ints;",
            "import com.google.common.util.concurrent.AbstractIdleService;",
            "import com.google.common.util.concurrent.Uninterruptibles;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.graylog.shaded.kafka09.common.KafkaException;",
            "import org.graylog.shaded.kafka09.common.OffsetOutOfRangeException;",
            "import org.graylog.shaded.kafka09.common.TopicAndPartition;",
            "import org.graylog.shaded.kafka09.log.CleanerConfig;",
            "import org.graylog.shaded.kafka09.log.Log;",
            "import org.graylog.shaded.kafka09.log.LogAppendInfo;",
            "import org.graylog.shaded.kafka09.log.LogConfig;",
            "import org.graylog.shaded.kafka09.log.LogManager;",
            "import org.graylog.shaded.kafka09.log.LogSegment;",
            "import org.graylog.shaded.kafka09.message.ByteBufferMessageSet;",
            "import org.graylog.shaded.kafka09.message.Message;",
            "import org.graylog.shaded.kafka09.message.MessageAndOffset;",
            "import org.graylog.shaded.kafka09.message.MessageSet;",
            "import org.graylog.shaded.kafka09.scala.Option;",
            "import org.graylog.shaded.kafka09.scala.collection.Iterator;",
            "import org.graylog.shaded.kafka09.scala.collection.JavaConversions;",
            "import org.graylog.shaded.kafka09.scala.collection.Map$;",
            "import org.graylog.shaded.kafka09.scala.runtime.AbstractFunction1;",
            "import org.graylog.shaded.kafka09.server.BrokerState;",
            "import org.graylog.shaded.kafka09.server.RunningAsBroker;",
            "import org.graylog.shaded.kafka09.utils.KafkaScheduler;",
            "import org.graylog.shaded.kafka09.utils.Time;",
            "import org.graylog2.plugin.GlobalMetricNames;",
            "import org.graylog2.plugin.ServerStatus;",
            "import org.graylog2.plugin.ThrottleState;",
            "import org.graylog2.plugin.lifecycles.LoadBalancerStatus;",
            "import org.graylog2.shared.metrics.HdrTimer;",
            "import org.graylog2.shared.utilities.ByteBufferUtils;",
            "import org.joda.time.DateTimeUtils;",
            "import org.joda.time.Duration;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import jakarta.inject.Inject;",
            "import jakarta.inject.Named;",
            "import jakarta.inject.Singleton;",
            "import java.io.File;",
            "import java.io.FileOutputStream;",
            "import java.io.IOException;",
            "import java.io.SyncFailedException;",
            "import java.io.UncheckedIOException;",
            "import java.nio.channels.ClosedByInterruptException;",
            "import java.nio.charset.StandardCharsets;",
            "import java.nio.file.AccessDeniedException;",
            "import java.nio.file.Path;",
            "import java.util.ArrayList;",
            "import java.util.Collections;",
            "import java.util.Date;",
            "import java.util.HashSet;",
            "import java.util.List;",
            "import java.util.Locale;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import java.util.SortedMap;",
            "import java.util.concurrent.Callable;",
            "import java.util.concurrent.ScheduledExecutorService;",
            "import java.util.concurrent.ScheduledFuture;",
            "import java.util.concurrent.atomic.AtomicInteger;",
            "import java.util.concurrent.atomic.AtomicLong;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "import static com.codahale.metrics.MetricRegistry.name;",
            "import static java.util.concurrent.TimeUnit.DAYS;",
            "import static java.util.concurrent.TimeUnit.MILLISECONDS;",
            "import static java.util.concurrent.TimeUnit.MINUTES;",
            "import static java.util.concurrent.TimeUnit.NANOSECONDS;",
            "import static java.util.concurrent.TimeUnit.SECONDS;",
            "import static org.graylog2.plugin.Tools.bytesToHex;"
        ],
        "reference_api": [
            "equals",
            "values",
            "getOnlyElement",
            "register",
            "getTimers"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "values",
            "getOnlyElement",
            "register",
            "getTimers"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "Graylog2/graylog2-server",
        "function_declaration": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name)",
        "start_line": "428",
        "end_line": "443",
        "file_path": "graylog2-server/src/main/java/org/graylog2/shared/journal/LocalKafkaJournal.java",
        "docstring": "The registerUncommittedGauge function registers a gauge metric in a MetricRegistry.\\nThe gauge returns the number of uncommitted messages.\\nIf the size is zero, it returns 0.\\nIf the committed offset is at the default value, it calculates the difference between the log end and start offsets.\\nOtherwise, it calculates the uncommitted messages as the difference between the log end offset and the committed offset.\\nIf the gauge registration throws an IllegalArgumentException, the exception is ignored.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e1d07a1fa986",
        "ground_truth": "private void registerUncommittedGauge(MetricRegistry metricRegistry, String name) {\n    try {\n        metricRegistry.register(name,\n                (Gauge<Long>) () -> {\n                    if (size() == 0) {\n                        return 0L;\n                    }\n                    if (committedOffset.get() == DEFAULT_COMMITTED_OFFSET) {\n                        return getLogEndOffset() - getLogStartOffset();\n                    }\n                    return Math.max(0, getLogEndOffset() - 1 - committedOffset.get());\n                });\n    } catch (IllegalArgumentException ignored) {\n        // already registered, we'll ignore that.\n    }\n}",
        "import_statements": [
            "import com.codahale.metrics.Gauge;",
            "import com.codahale.metrics.Meter;",
            "import com.codahale.metrics.MetricFilter;",
            "import com.codahale.metrics.MetricRegistry;",
            "import com.codahale.metrics.Timer;",
            "import com.github.joschi.jadconfig.util.Size;",
            "import com.google.common.collect.ImmutableMap;",
            "import com.google.common.collect.Iterables;",
            "import com.google.common.collect.Sets;",
            "import com.google.common.io.Files;",
            "import com.google.common.primitives.Ints;",
            "import com.google.common.util.concurrent.AbstractIdleService;",
            "import com.google.common.util.concurrent.Uninterruptibles;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.graylog.shaded.kafka09.common.KafkaException;",
            "import org.graylog.shaded.kafka09.common.OffsetOutOfRangeException;",
            "import org.graylog.shaded.kafka09.common.TopicAndPartition;",
            "import org.graylog.shaded.kafka09.log.CleanerConfig;",
            "import org.graylog.shaded.kafka09.log.Log;",
            "import org.graylog.shaded.kafka09.log.LogAppendInfo;",
            "import org.graylog.shaded.kafka09.log.LogConfig;",
            "import org.graylog.shaded.kafka09.log.LogManager;",
            "import org.graylog.shaded.kafka09.log.LogSegment;",
            "import org.graylog.shaded.kafka09.message.ByteBufferMessageSet;",
            "import org.graylog.shaded.kafka09.message.Message;",
            "import org.graylog.shaded.kafka09.message.MessageAndOffset;",
            "import org.graylog.shaded.kafka09.message.MessageSet;",
            "import org.graylog.shaded.kafka09.scala.Option;",
            "import org.graylog.shaded.kafka09.scala.collection.Iterator;",
            "import org.graylog.shaded.kafka09.scala.collection.JavaConversions;",
            "import org.graylog.shaded.kafka09.scala.collection.Map$;",
            "import org.graylog.shaded.kafka09.scala.runtime.AbstractFunction1;",
            "import org.graylog.shaded.kafka09.server.BrokerState;",
            "import org.graylog.shaded.kafka09.server.RunningAsBroker;",
            "import org.graylog.shaded.kafka09.utils.KafkaScheduler;",
            "import org.graylog.shaded.kafka09.utils.Time;",
            "import org.graylog2.plugin.GlobalMetricNames;",
            "import org.graylog2.plugin.ServerStatus;",
            "import org.graylog2.plugin.ThrottleState;",
            "import org.graylog2.plugin.lifecycles.LoadBalancerStatus;",
            "import org.graylog2.shared.metrics.HdrTimer;",
            "import org.graylog2.shared.utilities.ByteBufferUtils;",
            "import org.joda.time.DateTimeUtils;",
            "import org.joda.time.Duration;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import jakarta.inject.Inject;",
            "import jakarta.inject.Named;",
            "import jakarta.inject.Singleton;",
            "import java.io.File;",
            "import java.io.FileOutputStream;",
            "import java.io.IOException;",
            "import java.io.SyncFailedException;",
            "import java.io.UncheckedIOException;",
            "import java.nio.channels.ClosedByInterruptException;",
            "import java.nio.charset.StandardCharsets;",
            "import java.nio.file.AccessDeniedException;",
            "import java.nio.file.Path;",
            "import java.util.ArrayList;",
            "import java.util.Collections;",
            "import java.util.Date;",
            "import java.util.HashSet;",
            "import java.util.List;",
            "import java.util.Locale;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import java.util.SortedMap;",
            "import java.util.concurrent.Callable;",
            "import java.util.concurrent.ScheduledExecutorService;",
            "import java.util.concurrent.ScheduledFuture;",
            "import java.util.concurrent.atomic.AtomicInteger;",
            "import java.util.concurrent.atomic.AtomicLong;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "import static com.codahale.metrics.MetricRegistry.name;",
            "import static java.util.concurrent.TimeUnit.DAYS;",
            "import static java.util.concurrent.TimeUnit.MILLISECONDS;",
            "import static java.util.concurrent.TimeUnit.MINUTES;",
            "import static java.util.concurrent.TimeUnit.NANOSECONDS;",
            "import static java.util.concurrent.TimeUnit.SECONDS;",
            "import static org.graylog2.plugin.Tools.bytesToHex;"
        ],
        "reference_api": [
            "getLogStartOffset",
            "getLogEndOffset",
            "size",
            "get",
            "max",
            "register"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getLogStartOffset",
                "code": "public long getLogStartOffset() {\n        final Iterable<LogSegment> logSegments = JavaConversions.asJavaIterable(kafkaLog.logSegments());\n        final LogSegment segment = Iterables.getFirst(logSegments, null);\n        if (segment == null) {\n            return 0;\n        }\n        return segment.baseOffset();\n    }"
            },
            {
                "name": "getLogEndOffset",
                "code": "public long getLogEndOffset() {\n        return kafkaLog.logEndOffset();\n    }"
            },
            {
                "name": "size",
                "code": "public long size() {\n        return kafkaLog.size();\n    }"
            }
        ],
        "third_party": [
            "get",
            "max",
            "register"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "Graylog2/graylog2-server",
        "function_declaration": "private long flushMessages(List<Message> messages, long payloadSize)",
        "start_line": "563",
        "end_line": "585",
        "file_path": "graylog2-server/src/main/java/org/graylog2/shared/journal/LocalKafkaJournal.java",
        "docstring": "The flushMessages function writes a list of messages to a Kafka log and returns the last write offset.\\nIt first checks if the message list is empty and logs a debug message if so.\\nIf there are messages, it creates a ByteBufferMessageSet from the list and logs its size.\\nIt then appends the message set to the Kafka log and retrieves the last offset from the append operation.\\nIt logs the details of the write operation, marks the number of written messages, and returns the last write offset.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "8b30a387069f",
        "ground_truth": "private long flushMessages(List<Message> messages, long payloadSize) {\n    if (messages.isEmpty()) {\n        LOG.debug(\"No messages to flush, not trying to write an empty message set.\");\n        return -1L;\n    }\n    final ByteBufferMessageSet messageSet = new ByteBufferMessageSet(JavaConversions.asScalaBuffer(messages).toSeq());\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Trying to write ByteBufferMessageSet with size of {} bytes to journal\", messageSet.sizeInBytes());\n    }\n    final LogAppendInfo appendInfo = kafkaLog.append(messageSet, true);\n    long lastWriteOffset = appendInfo.lastOffset();\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Wrote {} messages to journal: {} bytes (payload {} bytes), log position {} to {}\",\n                messages.size(), messageSet.sizeInBytes(), payloadSize, appendInfo.firstOffset(), lastWriteOffset);\n    }\n    writtenMessages.mark(messages.size());\n    return lastWriteOffset;\n}",
        "import_statements": [
            "import com.codahale.metrics.Gauge;",
            "import com.codahale.metrics.Meter;",
            "import com.codahale.metrics.MetricFilter;",
            "import com.codahale.metrics.MetricRegistry;",
            "import com.codahale.metrics.Timer;",
            "import com.github.joschi.jadconfig.util.Size;",
            "import com.google.common.collect.ImmutableMap;",
            "import com.google.common.collect.Iterables;",
            "import com.google.common.collect.Sets;",
            "import com.google.common.io.Files;",
            "import com.google.common.primitives.Ints;",
            "import com.google.common.util.concurrent.AbstractIdleService;",
            "import com.google.common.util.concurrent.Uninterruptibles;",
            "import org.apache.commons.lang3.StringUtils;",
            "import org.graylog.shaded.kafka09.common.KafkaException;",
            "import org.graylog.shaded.kafka09.common.OffsetOutOfRangeException;",
            "import org.graylog.shaded.kafka09.common.TopicAndPartition;",
            "import org.graylog.shaded.kafka09.log.CleanerConfig;",
            "import org.graylog.shaded.kafka09.log.Log;",
            "import org.graylog.shaded.kafka09.log.LogAppendInfo;",
            "import org.graylog.shaded.kafka09.log.LogConfig;",
            "import org.graylog.shaded.kafka09.log.LogManager;",
            "import org.graylog.shaded.kafka09.log.LogSegment;",
            "import org.graylog.shaded.kafka09.message.ByteBufferMessageSet;",
            "import org.graylog.shaded.kafka09.message.Message;",
            "import org.graylog.shaded.kafka09.message.MessageAndOffset;",
            "import org.graylog.shaded.kafka09.message.MessageSet;",
            "import org.graylog.shaded.kafka09.scala.Option;",
            "import org.graylog.shaded.kafka09.scala.collection.Iterator;",
            "import org.graylog.shaded.kafka09.scala.collection.JavaConversions;",
            "import org.graylog.shaded.kafka09.scala.collection.Map$;",
            "import org.graylog.shaded.kafka09.scala.runtime.AbstractFunction1;",
            "import org.graylog.shaded.kafka09.server.BrokerState;",
            "import org.graylog.shaded.kafka09.server.RunningAsBroker;",
            "import org.graylog.shaded.kafka09.utils.KafkaScheduler;",
            "import org.graylog.shaded.kafka09.utils.Time;",
            "import org.graylog2.plugin.GlobalMetricNames;",
            "import org.graylog2.plugin.ServerStatus;",
            "import org.graylog2.plugin.ThrottleState;",
            "import org.graylog2.plugin.lifecycles.LoadBalancerStatus;",
            "import org.graylog2.shared.metrics.HdrTimer;",
            "import org.graylog2.shared.utilities.ByteBufferUtils;",
            "import org.joda.time.DateTimeUtils;",
            "import org.joda.time.Duration;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import jakarta.inject.Inject;",
            "import jakarta.inject.Named;",
            "import jakarta.inject.Singleton;",
            "import java.io.File;",
            "import java.io.FileOutputStream;",
            "import java.io.IOException;",
            "import java.io.SyncFailedException;",
            "import java.io.UncheckedIOException;",
            "import java.nio.channels.ClosedByInterruptException;",
            "import java.nio.charset.StandardCharsets;",
            "import java.nio.file.AccessDeniedException;",
            "import java.nio.file.Path;",
            "import java.util.ArrayList;",
            "import java.util.Collections;",
            "import java.util.Date;",
            "import java.util.HashSet;",
            "import java.util.List;",
            "import java.util.Locale;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import java.util.SortedMap;",
            "import java.util.concurrent.Callable;",
            "import java.util.concurrent.ScheduledExecutorService;",
            "import java.util.concurrent.ScheduledFuture;",
            "import java.util.concurrent.atomic.AtomicInteger;",
            "import java.util.concurrent.atomic.AtomicLong;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "import static com.codahale.metrics.MetricRegistry.name;",
            "import static java.util.concurrent.TimeUnit.DAYS;",
            "import static java.util.concurrent.TimeUnit.MILLISECONDS;",
            "import static java.util.concurrent.TimeUnit.MINUTES;",
            "import static java.util.concurrent.TimeUnit.NANOSECONDS;",
            "import static java.util.concurrent.TimeUnit.SECONDS;",
            "import static org.graylog2.plugin.Tools.bytesToHex;"
        ],
        "reference_api": [
            "asScalaBuffer",
            "mark",
            "toSeq",
            "size",
            "append",
            "firstOffset",
            "debug",
            "isDebugEnabled",
            "lastOffset",
            "sizeInBytes",
            "isEmpty"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "size",
                "code": "public long size() {\n        return kafkaLog.size();\n    }"
            }
        ],
        "third_party": [
            "asScalaBuffer",
            "mark",
            "toSeq",
            "append",
            "firstOffset",
            "debug",
            "isDebugEnabled",
            "lastOffset",
            "sizeInBytes",
            "isEmpty"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "didi/KnowStreaming",
        "function_declaration": "private void closeKafkaAdminClient(Long clusterPhyId) ",
        "start_line": "69",
        "end_line": "92",
        "file_path": "km-persistence/src/main/java/com/xiaojukeji/know/streaming/km/persistence/kafka/KafkaAdminClient.java",
        "docstring": "The closeKafkaAdminClient function safely closes Kafka AdminClient instances associated with a given cluster ID.\\nIt locks the client map to ensure thread safety and retrieves the list of AdminClient instances for the specified cluster ID.\\nIf no clients are found, the function returns.\\nIt logs the start of the closing process and attempts to close all AdminClient instances in the list.\\nIt logs a success message if all clients are closed successfully, or an error message if any closure fails.\\nAny exceptions during the process are caught and logged.\\nFinally, the lock is released.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ee338b61e75d",
        "ground_truth": "private void closeKafkaAdminClient(Long clusterPhyId) {\n    try {\n        modifyClientMapLock.lock();\n        List<AdminClient> adminClientList = KAFKA_ADMIN_CLIENT_MAP.remove(clusterPhyId);\n        if (adminClientList == null) {\n            return;\n        }\n        LOGGER.info(\"close kafka AdminClient starting, clusterPhyId:{}\", clusterPhyId);\n        boolean allSuccess = this.closeAdminClientList(clusterPhyId, adminClientList);\n        if (allSuccess) {\n            LOGGER.info(\"close kafka AdminClient success, clusterPhyId:{}\", clusterPhyId);\n        } else {\n            LOGGER.error(\"close kafka AdminClient exist failed and can ignore this error, clusterPhyId:{}\", clusterPhyId);\n        }\n    } catch (Exception e) {\n        LOGGER.error(\"close kafka AdminClient failed, clusterPhyId:{}\", clusterPhyId, e);\n    } finally {\n        modifyClientMapLock.unlock();\n    }\n}",
        "import_statements": [
            "import com.didiglobal.logi.log.ILog;",
            "import com.didiglobal.logi.log.LogFactory;",
            "import com.xiaojukeji.know.streaming.km.common.bean.entity.cluster.ClusterPhy;",
            "import com.xiaojukeji.know.streaming.km.common.exception.NotExistException;",
            "import com.xiaojukeji.know.streaming.km.common.utils.ConvertUtil;",
            "import com.xiaojukeji.know.streaming.km.persistence.AbstractClusterLoadedChangedHandler;",
            "import com.xiaojukeji.know.streaming.km.persistence.cache.LoadedClusterPhyCache;",
            "import org.apache.kafka.clients.admin.AdminClient;",
            "import org.apache.kafka.clients.admin.AdminClientConfig;",
            "import org.springframework.beans.factory.annotation.Value;",
            "import org.springframework.stereotype.Component;",
            "import java.time.Duration;",
            "import java.util.ArrayList;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Properties;",
            "import java.util.concurrent.ConcurrentHashMap;"
        ],
        "reference_api": [
            "error",
            "unlock",
            "info",
            "lock",
            "closeAdminClientList",
            "remove"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "closeAdminClientList",
                "code": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList) {\n        if (adminClientList == null) {\n            return true;\n        }\n\n        boolean allSuccess = true;\n        for (AdminClient adminClient: adminClientList) {\n            try {\n                // \u5173\u95ed\u5ba2\u6237\u7aef\uff0c\u8d85\u65f6\u65f6\u95f4\u4e3a30\u79d2\n                adminClient.close(Duration.ofSeconds(30));\n            } catch (Exception e) {\n                // ignore\n                LOGGER.error(\"close kafka AdminClient exist failed, clusterPhyId:{}\", clusterPhyId, e);\n                allSuccess = false;\n            }\n        }\n\n        return allSuccess;\n    }"
            },
            {
                "name": "remove",
                "code": "@Override\n    protected void remove(ClusterPhy clusterPhy) {\n        this.closeKafkaAdminClient(clusterPhy.getId());\n    }"
            }
        ],
        "third_party": [
            "error",
            "unlock",
            "info",
            "lock"
        ]
    },
    {
        "subclass": "kafka",
        "owner/repo": "didi/KnowStreaming",
        "function_declaration": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList)",
        "start_line": "140",
        "end_line": "158",
        "file_path": "km-persistence/src/main/java/com/xiaojukeji/know/streaming/km/persistence/kafka/KafkaAdminClient.java",
        "docstring": "The closeAdminClientList function attempts to close a list of Kafka AdminClient instances for a given cluster ID.\\nIf the list is null, it returns true.\\nFor each AdminClient in the list, it tries to close it with a timeout of 30 seconds.\\nIf an exception occurs during the closing process, it logs an error and sets the success flag to false.\\nThe function returns a boolean indicating whether all AdminClients were closed successfully.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6f0b935b971c",
        "ground_truth": "private boolean closeAdminClientList(Long clusterPhyId, List<AdminClient> adminClientList) {\n    if (adminClientList == null) {\n        return true;\n    }\n    boolean allSuccess = true;\n    for (AdminClient adminClient: adminClientList) {\n        try {\n            // \u5173\u95ed\u5ba2\u6237\u7aef\uff0c\u8d85\u65f6\u65f6\u95f4\u4e3a30\u79d2\n            adminClient.close(Duration.ofSeconds(30));\n        } catch (Exception e) {\n            // ignore\n            LOGGER.error(\"close kafka AdminClient exist failed, clusterPhyId:{}\", clusterPhyId, e);\n            allSuccess = false;\n        }\n    }\n    return allSuccess;\n}",
        "import_statements": [
            "import com.didiglobal.logi.log.ILog;",
            "import com.didiglobal.logi.log.LogFactory;",
            "import com.xiaojukeji.know.streaming.km.common.bean.entity.cluster.ClusterPhy;",
            "import com.xiaojukeji.know.streaming.km.common.exception.NotExistException;",
            "import com.xiaojukeji.know.streaming.km.common.utils.ConvertUtil;",
            "import com.xiaojukeji.know.streaming.km.persistence.AbstractClusterLoadedChangedHandler;",
            "import com.xiaojukeji.know.streaming.km.persistence.cache.LoadedClusterPhyCache;",
            "import org.apache.kafka.clients.admin.AdminClient;",
            "import org.apache.kafka.clients.admin.AdminClientConfig;",
            "import org.springframework.beans.factory.annotation.Value;",
            "import org.springframework.stereotype.Component;",
            "import java.time.Duration;",
            "import java.util.ArrayList;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.Properties;",
            "import java.util.concurrent.ConcurrentHashMap;"
        ],
        "reference_api": [
            "ofSeconds",
            "close",
            "error"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "ofSeconds",
            "close",
            "error"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "spring-cloud/spring-cloud-netflix",
        "function_declaration": "public void stop() ",
        "start_line": "73",
        "end_line": "88",
        "file_path": "spring-cloud-netflix-eureka-client-tls-tests/src/test/java/org/springframework/cloud/netflix/eureka/AppRunner.java",
        "docstring": "The stop function stops the application if it is running.\\nIt calls the stop method on the app and then waits for the app to stop, with a maximum of 5 attempts, pausing for 1 second between each attempt.\\nIf the thread is interrupted during sleep, it throws an IllegalStateException.\\nOnce the app has stopped, it sets the app to null.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "b5475aa94c1a",
        "ground_truth": "public void stop() {\n if (app != null) {\n  app.stop();\n  int attempts = 5;\n  while (app.isRunning() && attempts > 0) {\n   attempts = attempts - 1;\n   try {\n    Thread.sleep(1000);\n   }\n   catch (InterruptedException e) {\n    throw new IllegalStateException(e);\n   }\n  }\n  app = null;\n }\n}",
        "import_statements": [
            "import java.util.ArrayList;",
            "import java.util.LinkedHashMap;",
            "import java.util.List;",
            "import java.util.Map;",
            "import org.springframework.boot.builder.SpringApplicationBuilder;",
            "import org.springframework.cloud.test.TestSocketUtils;",
            "import org.springframework.context.ApplicationContext;",
            "import org.springframework.context.ConfigurableApplicationContext;"
        ],
        "reference_api": [
            "stop",
            "isRunning",
            "sleep"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "stop",
                "code": "public void stop() {\n\t\tif (app != null) {\n\t\t\tapp.stop();\n\t\t\tint attempts = 5;\n\t\t\twhile (app.isRunning() && attempts > 0) {\n\t\t\t\tattempts = attempts - 1;\n\t\t\t\ttry {\n\t\t\t\t\tThread.sleep(1000);\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\tthrow new IllegalStateException(e);\n\t\t\t\t}\n\t\t\t}\n\t\t\tapp = null;\n\t\t}\n\t}"
            }
        ],
        "third_party": [
            "isRunning",
            "sleep"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "spring-cloud/spring-cloud-netflix",
        "function_declaration": "static EurekaServerRunner startEurekaServer(Class config)",
        "start_line": "60",
        "end_line": "68",
        "file_path": "spring-cloud-netflix-eureka-client-tls-tests/src/test/java/org/springframework/cloud/netflix/eureka/BaseCertTests.java",
        "docstring": "The startEurekaServer function initializes and starts a Eureka server with the specified configuration.\\nIt creates a EurekaServerRunner instance, enables TLS, sets the keystore and truststore with the provided certificates and passwords, and then starts the server.\\nThe function returns the initialized and started Eureka server instance.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5da405de9e3d",
        "ground_truth": "static EurekaServerRunner startEurekaServer(Class config) {\n EurekaServerRunner server = new EurekaServerRunner(config);\n server.enableTls();\n server.setKeyStore(serverCert, KEY_STORE_PASSWORD, \"server\", KEY_PASSWORD);\n server.setTrustStore(caCert, KEY_STORE_PASSWORD);\n server.start();\n return server;\n}",
        "import_statements": [
            "import java.io.File;",
            "import java.io.FileOutputStream;",
            "import java.io.OutputStream;",
            "import java.security.KeyStore;",
            "import java.util.function.Supplier;",
            "import org.apache.commons.logging.Log;",
            "import org.apache.commons.logging.LogFactory;",
            "import org.junit.jupiter.api.AfterAll;",
            "import org.junit.jupiter.api.Assertions;",
            "import org.junit.jupiter.api.BeforeAll;",
            "import org.junit.jupiter.api.Test;",
            "import org.springframework.beans.factory.BeanCreationException;",
            "import static org.assertj.core.api.Assertions.assertThat;"
        ],
        "reference_api": [
            "setTrustStore",
            "setKeyStore",
            "start",
            "enableTls"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "setTrustStore",
            "setKeyStore",
            "start",
            "enableTls"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "spring-cloud/spring-cloud-netflix",
        "function_declaration": "static EurekaClientRunner startService(EurekaServerRunner server, Class config)",
        "start_line": "75",
        "end_line": "80",
        "file_path": "spring-cloud-netflix-eureka-client-tls-tests/src/test/java/org/springframework/cloud/netflix/eureka/BaseCertTests.java",
        "docstring": "The startService function initializes and starts a EurekaClientRunner instance.\\nIt creates a new EurekaClientRunner with the provided configuration class, Eureka server runner, and a service name \"testservice\".\\nIt enables TLS for the client and starts the service.\\nFinally, it returns the initialized and started EurekaClientRunner instance.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "19cbf70751b2",
        "ground_truth": "static EurekaClientRunner startService(EurekaServerRunner server, Class config) {\n EurekaClientRunner service = new EurekaClientRunner(config, server, \"testservice\");\n enableTlsClient(service);\n service.start();\n return service;\n}",
        "import_statements": [
            "import java.io.File;",
            "import java.io.FileOutputStream;",
            "import java.io.OutputStream;",
            "import java.security.KeyStore;",
            "import java.util.function.Supplier;",
            "import org.apache.commons.logging.Log;",
            "import org.apache.commons.logging.LogFactory;",
            "import org.junit.jupiter.api.AfterAll;",
            "import org.junit.jupiter.api.Assertions;",
            "import org.junit.jupiter.api.BeforeAll;",
            "import org.junit.jupiter.api.Test;",
            "import org.springframework.beans.factory.BeanCreationException;",
            "import static org.assertj.core.api.Assertions.assertThat;"
        ],
        "reference_api": [
            "start",
            "enableTlsClient"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "enableTlsClient",
                "code": "static void enableTlsClient(EurekaClientRunner runner) {\n\t\trunner.enableTls();\n\t\trunner.setKeyStore(clientCert, KEY_STORE_PASSWORD, KEY_PASSWORD);\n\t\trunner.setTrustStore(caCert, KEY_STORE_PASSWORD);\n\t}"
            }
        ],
        "third_party": [
            "start"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "spring-cloud/spring-cloud-netflix",
        "function_declaration": "static void createCertificates() throws Exception",
        "start_line": "101",
        "end_line": "117",
        "file_path": "spring-cloud-netflix-eureka-client-tls-tests/src/test/java/org/springframework/cloud/netflix/eureka/BaseCertTests.java",
        "docstring": "The createCertificates function generates and saves certificates for a CA, server, and client.\\nIt uses a KeyTool instance to create a CA certificate and signs server and client certificates with it.\\nThese certificates and keys are saved using saveCert and saveKeyAndCert functions.\\nAdditionally, it creates a second CA and client certificate pair, which are also saved.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d6ccff5c0536",
        "ground_truth": "static void createCertificates() throws Exception {\n KeyTool tool = new KeyTool();\n KeyAndCert ca = tool.createCA(\"MyCA\");\n KeyAndCert server = ca.sign(\"server\");\n KeyAndCert client = ca.sign(\"client\");\n caCert = saveCert(ca);\n serverCert = saveKeyAndCert(server);\n clientCert = saveKeyAndCert(client);\n KeyAndCert wrongCa = tool.createCA(\"WrongCA\");\n KeyAndCert wrongClient = wrongCa.sign(\"client\");\n wrongCaCert = saveCert(wrongCa);\n wrongClientCert = saveKeyAndCert(wrongClient);\n}",
        "import_statements": [
            "import java.io.File;",
            "import java.io.FileOutputStream;",
            "import java.io.OutputStream;",
            "import java.security.KeyStore;",
            "import java.util.function.Supplier;",
            "import org.apache.commons.logging.Log;",
            "import org.apache.commons.logging.LogFactory;",
            "import org.junit.jupiter.api.AfterAll;",
            "import org.junit.jupiter.api.Assertions;",
            "import org.junit.jupiter.api.BeforeAll;",
            "import org.junit.jupiter.api.Test;",
            "import org.springframework.beans.factory.BeanCreationException;",
            "import static org.assertj.core.api.Assertions.assertThat;"
        ],
        "reference_api": [
            "createCA",
            "saveCert",
            "sign",
            "saveKeyAndCert"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "saveCert",
                "code": "private static File saveCert(KeyAndCert keyCert) throws Exception {\n\t\treturn saveKeyStore(keyCert.subject(), keyCert::storeCert);\n\t}"
            },
            {
                "name": "saveKeyAndCert",
                "code": "private static File saveKeyAndCert(KeyAndCert keyCert) throws Exception {\n\t\treturn saveKeyStore(keyCert.subject(), () -> keyCert.storeKeyAndCert(KEY_PASSWORD));\n\t}"
            }
        ],
        "third_party": [
            "createCA",
            "sign"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "spring-cloud/spring-cloud-netflix",
        "function_declaration": "private void assertInSeconds(BooleanSupplier assertion, int seconds)",
        "start_line": "78",
        "end_line": "94",
        "file_path": "spring-cloud-netflix-eureka-client-tls-tests/src/test/java/org/springframework/cloud/netflix/eureka/EurekaClientRunner.java",
        "docstring": "The assertInSeconds function repeatedly checks a Boolean condition for a specified number of seconds.\\nIt starts by recording the current time and calculates the time limit in milliseconds.\\nIt then continuously evaluates the assertion until the condition is met or the time limit is reached.\\nIf the condition is not met within the time limit, it throws a RuntimeException.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d2eeaa6a6bbe",
        "ground_truth": "private void assertInSeconds(BooleanSupplier assertion, int seconds) {\n long start = System.currentTimeMillis();\n long limit = 1000L * seconds;\n long duration;\n do {\n  if (assertion.getAsBoolean()) {\n   return;\n  }\n  duration = System.currentTimeMillis() - start;\n  Thread.yield();\n }\n while (duration < limit);\n throw new RuntimeException();\n}",
        "import_statements": [
            "import java.io.File;",
            "import java.util.function.BooleanSupplier;",
            "import com.netflix.discovery.AbstractDiscoveryClientOptionalArgs;",
            "import org.springframework.cloud.client.discovery.DiscoveryClient;"
        ],
        "reference_api": [
            "getAsBoolean",
            "currentTimeMillis",
            "yield"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getAsBoolean",
            "currentTimeMillis",
            "yield"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "spring-cloud/spring-cloud-netflix",
        "function_declaration": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception",
        "start_line": "72",
        "end_line": "78",
        "file_path": "spring-cloud-netflix-eureka-client-tls-tests/src/test/java/org/springframework/cloud/netflix/eureka/KeyTool.java",
        "docstring": "The createCert function generates an X509 certificate using a provided KeyPair and CA string.\\nIt constructs a certificate builder with the public key and CA information.\\nThe function adds key usage and basic constraints extensions to the certificate.\\nFinally, it signs the certificate with the private key and returns the signed X509 certificate.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6bb261b0e4e1",
        "ground_truth": "public X509Certificate createCert(KeyPair keyPair, String ca) throws Exception {\n JcaX509v3CertificateBuilder builder = certBuilder(keyPair.getPublic(), ca, ca);\n builder.addExtension(Extension.keyUsage, true, new KeyUsage(KeyUsage.keyCertSign));\n builder.addExtension(Extension.basicConstraints, false, new BasicConstraints(true));\n return signCert(builder, keyPair.getPrivate());\n}",
        "import_statements": [
            "import java.math.BigInteger;",
            "import java.security.KeyPair;",
            "import java.security.KeyPairGenerator;",
            "import java.security.PrivateKey;",
            "import java.security.PublicKey;",
            "import java.security.SecureRandom;",
            "import java.security.cert.X509Certificate;",
            "import java.util.Date;",
            "import org.bouncycastle.asn1.DERSequence;",
            "import org.bouncycastle.asn1.x500.X500Name;",
            "import org.bouncycastle.asn1.x509.BasicConstraints;",
            "import org.bouncycastle.asn1.x509.Extension;",
            "import org.bouncycastle.asn1.x509.GeneralName;",
            "import org.bouncycastle.asn1.x509.GeneralNames;",
            "import org.bouncycastle.asn1.x509.KeyUsage;",
            "import org.bouncycastle.cert.X509CertificateHolder;",
            "import org.bouncycastle.cert.jcajce.JcaX509CertificateConverter;",
            "import org.bouncycastle.cert.jcajce.JcaX509v3CertificateBuilder;",
            "import org.bouncycastle.operator.ContentSigner;",
            "import org.bouncycastle.operator.jcajce.JcaContentSignerBuilder;"
        ],
        "reference_api": [
            "signCert",
            "getPrivate",
            "getPublic",
            "addExtension",
            "certBuilder"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "signCert",
                "code": "private X509Certificate signCert(JcaX509v3CertificateBuilder builder, PrivateKey privateKey) throws Exception {\n\t\tContentSigner signer = new JcaContentSignerBuilder(\"SHA256WithRSA\").build(privateKey);\n\t\tX509CertificateHolder holder = builder.build(signer);\n\n\t\treturn new JcaX509CertificateConverter().getCertificate(holder);\n\t}"
            },
            {
                "name": "certBuilder",
                "code": "private JcaX509v3CertificateBuilder certBuilder(PublicKey publicKey, String issuer, String subject) {\n\t\tX500Name issuerName = new X500Name(String.format(\"dc=%s\", issuer));\n\t\tX500Name subjectName = new X500Name(String.format(\"dc=%s\", subject));\n\n\t\tlong now = System.currentTimeMillis();\n\t\tBigInteger serialNum = BigInteger.valueOf(now);\n\t\tDate notBefore = new Date(now - ONE_DAY);\n\t\tDate notAfter = new Date(now + TEN_YEARS);\n\n\t\treturn new JcaX509v3CertificateBuilder(issuerName, serialNum, notBefore, notAfter, subjectName, publicKey);\n\t}"
            }
        ],
        "third_party": [
            "getPrivate",
            "getPublic",
            "addExtension"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "spring-cloud/spring-cloud-netflix",
        "function_declaration": "EurekaHttpClient getEurekaHttpClient()",
        "start_line": "92",
        "end_line": "107",
        "file_path": "spring-cloud-netflix-eureka-client/src/main/java/org/springframework/cloud/netflix/eureka/CloudEurekaClient.java",
        "docstring": "The getEurekaHttpClient function retrieves the EurekaHttpClient instance.\\nIf the EurekaHttpClient is not already set, it uses reflection to access the registrationClient field from the eurekaTransport object and assigns it to the EurekaHttpClient.\\nIf an IllegalAccessException occurs during this process, it logs an error message.\\nFinally, it returns the EurekaHttpClient instance.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "e4f9c32a7c9e",
        "ground_truth": "EurekaHttpClient getEurekaHttpClient() {\n if (this.eurekaHttpClient.get() == null) {\n  try {\n   Object eurekaTransport = this.eurekaTransportField.get(this);\n   Field registrationClientField = ReflectionUtils.findField(eurekaTransport.getClass(),\n     \"registrationClient\");\n   ReflectionUtils.makeAccessible(registrationClientField);\n   this.eurekaHttpClient.compareAndSet(null,\n     (EurekaHttpClient) registrationClientField.get(eurekaTransport));\n  }\n  catch (IllegalAccessException e) {\n   log.error(\"error getting EurekaHttpClient\", e);\n  }\n }\n return this.eurekaHttpClient.get();\n}",
        "import_statements": [
            "import java.lang.reflect.Field;",
            "import java.util.concurrent.atomic.AtomicLong;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "import com.netflix.appinfo.ApplicationInfoManager;",
            "import com.netflix.appinfo.InstanceInfo;",
            "import com.netflix.appinfo.InstanceInfo.InstanceStatus;",
            "import com.netflix.discovery.AbstractDiscoveryClientOptionalArgs;",
            "import com.netflix.discovery.DiscoveryClient;",
            "import com.netflix.discovery.EurekaClientConfig;",
            "import com.netflix.discovery.shared.transport.EurekaHttpClient;",
            "import com.netflix.discovery.shared.transport.EurekaHttpResponse;",
            "import com.netflix.discovery.shared.transport.jersey.TransportClientFactories;",
            "import org.apache.commons.logging.Log;",
            "import org.apache.commons.logging.LogFactory;",
            "import org.springframework.cloud.client.discovery.event.HeartbeatEvent;",
            "import org.springframework.context.ApplicationEventPublisher;",
            "import org.springframework.http.HttpStatus;",
            "import org.springframework.util.ReflectionUtils;"
        ],
        "reference_api": [
            "error",
            "findField",
            "makeAccessible",
            "get",
            "getClass",
            "compareAndSet"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "error",
            "findField",
            "makeAccessible",
            "get",
            "getClass",
            "compareAndSet"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "spring-cloud/spring-cloud-netflix",
        "function_declaration": "public List<String> getServices()",
        "start_line": "71",
        "end_line": "86",
        "file_path": "spring-cloud-netflix-eureka-client/src/main/java/org/springframework/cloud/netflix/eureka/EurekaDiscoveryClient.java",
        "docstring": "The getServices function retrieves a list of service names registered with the Eureka client.\\nIt first obtains the applications from the Eureka client.\\nIf no applications are found, it returns an empty list.\\nOtherwise, it iterates through the registered applications, adding the names of those with instances to a list.\\nThe names are converted to lowercase before being added to the list, which is then returned.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "5c404f04e34c",
        "ground_truth": "public List<String> getServices() {\n Applications applications = this.eurekaClient.getApplications();\n if (applications == null) {\n  return Collections.emptyList();\n }\n List<Application> registered = applications.getRegisteredApplications();\n List<String> names = new ArrayList<>();\n for (Application app : registered) {\n  if (app.getInstances().isEmpty()) {\n   continue;\n  }\n  names.add(app.getName().toLowerCase());\n }\n return names;\n}",
        "import_statements": [
            "import java.util.ArrayList;",
            "import java.util.Collections;",
            "import java.util.List;",
            "import com.netflix.appinfo.InstanceInfo;",
            "import com.netflix.discovery.EurekaClient;",
            "import com.netflix.discovery.EurekaClientConfig;",
            "import com.netflix.discovery.shared.Application;",
            "import com.netflix.discovery.shared.Applications;",
            "import org.springframework.cloud.client.ServiceInstance;",
            "import org.springframework.cloud.client.discovery.DiscoveryClient;",
            "import org.springframework.core.Ordered;"
        ],
        "reference_api": [
            "getName",
            "getInstances",
            "getRegisteredApplications",
            "getApplications",
            "toLowerCase",
            "add",
            "isEmpty",
            "emptyList"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "getInstances",
                "code": "@Override\n\tpublic List<ServiceInstance> getInstances(String serviceId) {\n\t\tList<InstanceInfo> infos = this.eurekaClient.getInstancesByVipAddress(serviceId, false);\n\t\tList<ServiceInstance> instances = new ArrayList<>();\n\t\tfor (InstanceInfo info : infos) {\n\t\t\tinstances.add(new EurekaServiceInstance(info));\n\t\t}\n\t\treturn instances;\n\t}"
            }
        ],
        "third_party": [
            "getName",
            "getRegisteredApplications",
            "getApplications",
            "toLowerCase",
            "add",
            "isEmpty",
            "emptyList"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "spring-cloud/spring-cloud-netflix",
        "function_declaration": "void populateHealthContributors(Map<String, HealthContributor> healthContributors)",
        "start_line": "109",
        "end_line": "124",
        "file_path": "spring-cloud-netflix-eureka-client/src/main/java/org/springframework/cloud/netflix/eureka/EurekaHealthCheckHandler.java",
        "docstring": "The populateHealthContributors function processes a map of health contributors.\\nFor each entry, it checks if the value is an instance of DiscoveryCompositeHealthContributor.\\nIf so, it iterates through its indicators, adding those that are not EurekaHealthIndicator to the healthContributors map as HealthIndicator instances.\\nIf the value is not a DiscoveryCompositeHealthContributor, it directly adds the entry to the healthContributors map.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "a21251337ade",
        "ground_truth": "void populateHealthContributors(Map<String, HealthContributor> healthContributors) {\n for (Map.Entry<String, HealthContributor> entry : healthContributors.entrySet()) {\n  // ignore EurekaHealthIndicator and flatten the rest of the composite\n  // otherwise there is a never ending cycle of down. See gh-643\n  if (entry.getValue() instanceof DiscoveryCompositeHealthContributor indicator) {\n   indicator.getIndicators().forEach((name, discoveryHealthIndicator) -> {\n    if (!(discoveryHealthIndicator instanceof EurekaHealthIndicator)) {\n     this.healthContributors.put(name, (HealthIndicator) discoveryHealthIndicator::health);\n    }\n   });\n  }\n  else {\n   this.healthContributors.put(entry.getKey(), entry.getValue());\n  }\n }\n}",
        "import_statements": [
            "import java.util.HashMap;",
            "import java.util.HashSet;",
            "import java.util.Map;",
            "import java.util.Set;",
            "import com.netflix.appinfo.HealthCheckHandler;",
            "import com.netflix.appinfo.InstanceInfo;",
            "import com.netflix.appinfo.InstanceInfo.InstanceStatus;",
            "import org.springframework.beans.BeansException;",
            "import org.springframework.beans.factory.InitializingBean;",
            "import org.springframework.boot.actuate.health.CompositeHealthContributor;",
            "import org.springframework.boot.actuate.health.CompositeReactiveHealthContributor;",
            "import org.springframework.boot.actuate.health.Health;",
            "import org.springframework.boot.actuate.health.HealthContributor;",
            "import org.springframework.boot.actuate.health.HealthIndicator;",
            "import org.springframework.boot.actuate.health.NamedContributor;",
            "import org.springframework.boot.actuate.health.ReactiveHealthContributor;",
            "import org.springframework.boot.actuate.health.ReactiveHealthIndicator;",
            "import org.springframework.boot.actuate.health.Status;",
            "import org.springframework.boot.actuate.health.StatusAggregator;",
            "import org.springframework.cloud.client.discovery.health.DiscoveryCompositeHealthContributor;",
            "import org.springframework.context.ApplicationContext;",
            "import org.springframework.context.ApplicationContextAware;",
            "import org.springframework.context.Lifecycle;",
            "import org.springframework.core.Ordered;",
            "import org.springframework.util.Assert;"
        ],
        "reference_api": [
            "getIndicators",
            "getValue",
            "entrySet",
            "forEach",
            "getKey",
            "put"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getIndicators",
            "getValue",
            "entrySet",
            "forEach",
            "getKey",
            "put"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "public String hello()",
        "start_line": "65",
        "end_line": "75",
        "file_path": "mini-spring-cloud-examples/mini-spring-cloud-consumer-examples/src/main/java/com/github/cloud/examples/ConsumerApplication.java",
        "docstring": "The hello() function attempts to discover an instance of \"provider-application\" using the Discovery Client. If found, it retrieves the URI of the first instance, sends a POST request to its \"/echo\" endpoint using RestTemplate, and returns the response. If no instances are found, it throws a RuntimeException indicating the absence of any service instance for \"provider-application\".",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "998fee829929",
        "ground_truth": "public String hello() {\n    List<ServiceInstance> serviceInstances = discoveryClient.getInstances(\"provider-application\");\n    if (serviceInstances.size() > 0) {\n        ServiceInstance serviceInstance = serviceInstances.get(0);\n        URI uri = serviceInstance.getUri();\n        String response = restTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}",
        "import_statements": [
            "import com.github.cloud.openfeign.EnableFeignClients;",
            "import com.github.cloud.tutu.discovery.TutuDiscoveryClient;",
            "import org.springframework.beans.factory.annotation.Autowired;",
            "import org.springframework.boot.SpringApplication;",
            "import org.springframework.boot.autoconfigure.SpringBootApplication;",
            "import org.springframework.cloud.client.ServiceInstance;",
            "import org.springframework.cloud.client.loadbalancer.LoadBalanced;",
            "import org.springframework.cloud.client.loadbalancer.LoadBalancerClient;",
            "import org.springframework.context.annotation.Bean;",
            "import org.springframework.context.annotation.Configuration;",
            "import org.springframework.web.bind.annotation.GetMapping;",
            "import org.springframework.web.bind.annotation.RestController;",
            "import org.springframework.web.client.RestTemplate;",
            "import java.net.URI;",
            "import java.util.List;"
        ],
        "reference_api": [
            "getInstances",
            "toString",
            "size",
            "postForObject",
            "get",
            "getUri"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getInstances",
            "size",
            "postForObject",
            "get",
            "getUri"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "public String world()",
        "start_line": "78",
        "end_line": "87",
        "file_path": "mini-spring-cloud-examples/mini-spring-cloud-consumer-examples/src/main/java/com/github/cloud/examples/ConsumerApplication.java",
        "docstring": "This function attempts to retrieve a service instance named \"provider-application\" using a load balancer client.\\nIf successful, it constructs a URI and sends a POST request to \"/echo\" endpoint of the chosen instance using a RestTemplate.\\nIt returns the response as a String.\\nIf no service instance is found, it throws a RuntimeException indicating the absence of the required service.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "31166ae7d3dc",
        "ground_truth": "public String world() {\n    ServiceInstance serviceInstance = loadBalancerClient.choose(\"provider-application\");\n    if (serviceInstance != null) {\n        URI uri = serviceInstance.getUri();\n        String response = restTemplate.postForObject(uri.toString() + \"/echo\", null, String.class);\n        return response;\n    }\n    throw new RuntimeException(\"No service instance for provider-application found\");\n}",
        "import_statements": [
            "import com.github.cloud.openfeign.EnableFeignClients;",
            "import com.github.cloud.tutu.discovery.TutuDiscoveryClient;",
            "import org.springframework.beans.factory.annotation.Autowired;",
            "import org.springframework.boot.SpringApplication;",
            "import org.springframework.boot.autoconfigure.SpringBootApplication;",
            "import org.springframework.cloud.client.ServiceInstance;",
            "import org.springframework.cloud.client.loadbalancer.LoadBalanced;",
            "import org.springframework.cloud.client.loadbalancer.LoadBalancerClient;",
            "import org.springframework.context.annotation.Bean;",
            "import org.springframework.context.annotation.Configuration;",
            "import org.springframework.web.bind.annotation.GetMapping;",
            "import org.springframework.web.bind.annotation.RestController;",
            "import org.springframework.web.client.RestTemplate;",
            "import java.net.URI;",
            "import java.util.List;"
        ],
        "reference_api": [
            "toString",
            "postForObject",
            "getUri",
            "choose"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "postForObject",
            "getUri",
            "choose"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port)",
        "start_line": "44",
        "end_line": "50",
        "file_path": "mini-spring-cloud-examples/tutu-server/src/main/java/com/github/cloud/examples/TutuServerApplication.java",
        "docstring": "This function registers a service identified by serviceName with the provided IP address and port number.\\nIt logs the registration details using a logger.\\nIf the serviceName is not already present in the serverMap, it creates a synchronized HashSet for it.\\nThen, it adds a new Server instance representing the provided IP and port to the corresponding serviceName entry in the serverMap.\\nThe function returns true upon successful registration.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "d7714f9cf0e9",
        "ground_truth": "public boolean register(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port) {\n    logger.info(\"register service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    serverMap.putIfAbsent(serviceName.toLowerCase(), Collections.synchronizedSet(new HashSet<>()));\n    Server server = new Server(ip, port);\n    serverMap.get(serviceName).add(server);\n    return true;\n}",
        "import_statements": [
            "import com.alibaba.fastjson.JSON;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import org.springframework.boot.SpringApplication;",
            "import org.springframework.boot.autoconfigure.SpringBootApplication;",
            "import org.springframework.web.bind.annotation.GetMapping;",
            "import org.springframework.web.bind.annotation.PostMapping;",
            "import org.springframework.web.bind.annotation.RequestParam;",
            "import org.springframework.web.bind.annotation.RestController;",
            "import java.util.Collections;",
            "import java.util.Enumeration;",
            "import java.util.HashSet;",
            "import java.util.Set;",
            "import java.util.concurrent.ConcurrentHashMap;"
        ],
        "reference_api": [
            "toLowerCase",
            "synchronizedSet",
            "putIfAbsent",
            "get",
            "info",
            "add"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "toLowerCase",
            "synchronizedSet",
            "putIfAbsent",
            "get",
            "info",
            "add"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port)",
        "start_line": "61",
        "end_line": "69",
        "file_path": "mini-spring-cloud-examples/tutu-server/src/main/java/com/github/cloud/examples/TutuServerApplication.java",
        "docstring": "This function deregisters a server identified by the serviceName, ip, and port parameters from a serverMap. It logs the deregistration action and removes the corresponding Server object from the set associated with the serviceName in the serverMap. It returns true upon successful deregistration.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "45f91ed1c560",
        "ground_truth": "public boolean deregister(@RequestParam(\"serviceName\") String serviceName, @RequestParam(\"ip\") String ip, @RequestParam(\"port\") Integer port) {\n    logger.info(\"deregister service, serviceName: {}, ip: {}, port: {}\", serviceName, ip, port);\n    Set<Server> serverSet = serverMap.get(serviceName.toLowerCase());\n    if (serverSet != null) {\n        Server server = new Server(ip, port);\n        serverSet.remove(server);\n    }\n    return true;\n}",
        "import_statements": [
            "import com.alibaba.fastjson.JSON;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import org.springframework.boot.SpringApplication;",
            "import org.springframework.boot.autoconfigure.SpringBootApplication;",
            "import org.springframework.web.bind.annotation.GetMapping;",
            "import org.springframework.web.bind.annotation.PostMapping;",
            "import org.springframework.web.bind.annotation.RequestParam;",
            "import org.springframework.web.bind.annotation.RestController;",
            "import java.util.Collections;",
            "import java.util.Enumeration;",
            "import java.util.HashSet;",
            "import java.util.Set;",
            "import java.util.concurrent.ConcurrentHashMap;"
        ],
        "reference_api": [
            "toLowerCase",
            "get",
            "info",
            "remove"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "toLowerCase",
            "get",
            "info",
            "remove"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry)",
        "start_line": "19",
        "end_line": "26",
        "file_path": "mini-spring-cloud-load-balancer/src/main/java/com/github/cloud/loadbalancer/ribbon/RibbonClientConfigurationRegistrar.java",
        "docstring": "This function registers bean definitions based on metadata retrieved from annotations. It first obtains attributes from the RibbonClients annotation using the provided metadata. If the attributes contain a defaultConfiguration key, it generates a name and registers a client configuration with the BeanDefinitionRegistry using the retrieved configuration.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "f671d9564e40",
        "ground_truth": "public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) {\n    Map<String, Object> attrs = metadata.getAnnotationAttributes(RibbonClients.class.getName(), true);\n    if (attrs != null && attrs.containsKey(\"defaultConfiguration\")) {\n        String name = \"default.\" + metadata.getClassName();\n        registerClientConfiguration(registry, name, attrs.get(\"defaultConfiguration\"));\n    }\n}",
        "import_statements": [
            "import org.springframework.beans.factory.support.BeanDefinitionBuilder;",
            "import org.springframework.beans.factory.support.BeanDefinitionRegistry;",
            "import org.springframework.context.annotation.ImportBeanDefinitionRegistrar;",
            "import org.springframework.core.type.AnnotationMetadata;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "getName",
            "containsKey",
            "getAnnotationAttributes",
            "registerClientConfiguration",
            "get",
            "getClassName"
        ],
        "repo_defined_api_with_code": [
            {
                "name": "registerClientConfiguration",
                "code": "private void registerClientConfiguration(BeanDefinitionRegistry registry, Object name,\n                                             Object configuration) {\n        BeanDefinitionBuilder builder = BeanDefinitionBuilder\n                .genericBeanDefinition(RibbonClientSpecification.class);\n        builder.addConstructorArgValue(name);\n        builder.addConstructorArgValue(configuration);\n        registry.registerBeanDefinition(name + \".RibbonClientSpecification\",\n                builder.getBeanDefinition());\n    }"
            }
        ],
        "third_party": [
            "getName",
            "containsKey",
            "getAnnotationAttributes",
            "get",
            "getClassName"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "public <T> ServiceInstance choose(String serviceId, Request<T> request) ",
        "start_line": "50",
        "end_line": "58",
        "file_path": "mini-spring-cloud-load-balancer/src/main/java/com/github/cloud/loadbalancer/ribbon/RibbonLoadBalancerClient.java",
        "docstring": "This function selects a server instance from a load balancer based on the service ID and a default context. If a server is chosen, it creates and returns a TutuServiceInstance representing the chosen server's details, including service ID, host, and port. If no server is available, it returns null.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "dbc92d8f146f",
        "ground_truth": "public <T> ServiceInstance choose(String serviceId, Request<T> request) {\n    ILoadBalancer loadBalancer = clientFactory.getInstance(serviceId, ILoadBalancer.class);\n    Server server = loadBalancer.chooseServer(\"default\");\n    if (server != null) {\n        return new TutuServiceInstance(serviceId, server.getHost(), server.getPort());\n    }\n    return null;\n}",
        "import_statements": [
            "import cn.hutool.core.util.StrUtil;",
            "import com.github.cloud.tutu.TutuServiceInstance;",
            "import com.netflix.loadbalancer.ILoadBalancer;",
            "import com.netflix.loadbalancer.Server;",
            "import org.springframework.cloud.client.ServiceInstance;",
            "import org.springframework.cloud.client.loadbalancer.LoadBalancerClient;",
            "import org.springframework.cloud.client.loadbalancer.LoadBalancerRequest;",
            "import org.springframework.cloud.client.loadbalancer.Request;",
            "import java.io.IOException;",
            "import java.net.URI;",
            "import java.net.URISyntaxException;"
        ],
        "reference_api": [
            "chooseServer",
            "getHost",
            "getPort",
            "getInstance"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "chooseServer",
            "getHost",
            "getPort",
            "getInstance"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "public URI reconstructURI(ServiceInstance server, URI original) ",
        "start_line": "68",
        "end_line": "84",
        "file_path": "mini-spring-cloud-load-balancer/src/main/java/com/github/cloud/loadbalancer/ribbon/RibbonLoadBalancerClient.java",
        "docstring": "This function reconstructs a URI based on a given ServiceInstance and an original URI. It replaces the service name in the original URI with the IP address and port of the service instance. If the original URI includes a query string, it appends it to the reconstructed URI. If any errors occur during URI construction, a RuntimeException is thrown.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "362eadbd3c5e",
        "ground_truth": "public URI reconstructURI(ServiceInstance server, URI original) {\n    try {\n        //\u5c06\u670d\u52a1\u540d\u79f0\u66ff\u6362\u4e3a\u670d\u52a1\u5b9e\u4f8b\u7684IP:\u7aef\u53e3\uff0c\u4f8b\u5982http://provider-application/echo\u88ab\u91cd\u5efa\u4e3ahttp://192.168.100.1:8888/echo\n        StringBuilder sb = new StringBuilder();\n        sb.append(original.getScheme()).append(\"://\");\n        sb.append(server.getHost());\n        sb.append(\":\").append(server.getPort());\n        sb.append(original.getRawPath());\n        if (StrUtil.isNotEmpty(original.getRawQuery())) {\n            sb.append(\"?\").append(original.getRawQuery());\n        }\n        URI newURI = new URI(sb.toString());\n        return newURI;\n    } catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n    }\n}",
        "import_statements": [
            "import cn.hutool.core.util.StrUtil;",
            "import com.github.cloud.tutu.TutuServiceInstance;",
            "import com.netflix.loadbalancer.ILoadBalancer;",
            "import com.netflix.loadbalancer.Server;",
            "import org.springframework.cloud.client.ServiceInstance;",
            "import org.springframework.cloud.client.loadbalancer.LoadBalancerClient;",
            "import org.springframework.cloud.client.loadbalancer.LoadBalancerRequest;",
            "import org.springframework.cloud.client.loadbalancer.Request;",
            "import java.io.IOException;",
            "import java.net.URI;",
            "import java.net.URISyntaxException;"
        ],
        "reference_api": [
            "getHost",
            "getRawPath",
            "getPort",
            "toString",
            "append",
            "getRawQuery",
            "isNotEmpty",
            "getScheme"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getHost",
            "getRawPath",
            "getPort",
            "append",
            "getRawQuery",
            "isNotEmpty",
            "getScheme"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "private List<TutuServer> getServer()",
        "start_line": "54",
        "end_line": "65",
        "file_path": "mini-spring-cloud-load-balancer/src/main/java/com/github/cloud/loadbalancer/ribbon/TutuServerList.java",
        "docstring": "This function retrieves a list of TutuServer instances by querying a discovery service using HTTP GET. It constructs parameters including the serviceName and sends a request to the specified server address concatenated with \"/list\". Upon receiving the response, it parses the JSON array into a list of TutuServer objects containing IP addresses and ports extracted from the JSON data.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "6801d2b2e4c8",
        "ground_truth": "private List<TutuServer> getServer() {\n    Map<String, Object> param = new HashMap<>();\n    param.put(\"serviceName\", serviceId);\n    String response = HttpUtil.get(discoveryProperties.getServerAddr() + \"/list\", param);\n    logger.info(\"query service instance, serviceId: {}, response: {}\", serviceId, response);\n    return JSON.parseArray(response).stream().map(hostInfo -> {\n        String ip = ((JSONObject) hostInfo).getString(\"ip\");\n        Integer port = ((JSONObject) hostInfo).getInteger(\"port\");\n        return new TutuServer(ip, port);\n    }).collect(Collectors.toList());\n}",
        "import_statements": [
            "import cn.hutool.http.HttpUtil;",
            "import com.alibaba.fastjson.JSON;",
            "import com.alibaba.fastjson.JSONObject;",
            "import com.github.cloud.tutu.TutuDiscoveryProperties;",
            "import com.netflix.client.config.IClientConfig;",
            "import com.netflix.loadbalancer.AbstractServerList;",
            "import org.slf4j.Logger;",
            "import org.slf4j.LoggerFactory;",
            "import java.util.HashMap;",
            "import java.util.List;",
            "import java.util.Map;",
            "import java.util.stream.Collectors;"
        ],
        "reference_api": [
            "getString",
            "toList",
            "getServerAddr",
            "map",
            "getInteger",
            "collect",
            "get",
            "info",
            "put",
            "stream",
            "parseArray"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getString",
            "toList",
            "getServerAddr",
            "map",
            "getInteger",
            "collect",
            "get",
            "info",
            "put",
            "stream",
            "parseArray"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "public Route getMatchingRoute(String path)",
        "start_line": "25",
        "end_line": "36",
        "file_path": "mini-spring-cloud-netflix-zuul/src/main/java/com/github/cloud/netflix/zuul/filters/SimpleRouteLocator.java",
        "docstring": "Summary: This function iterates through the configured Zuul routes to find a matching route based on the given path. It uses the Ant-style path matching provided by pathMatcher to compare the incoming path with each route's pattern. If a match is found, it extracts the target path from the original path and creates a new Route object with the extracted path and the corresponding service ID. If no match is found among the configured routes, it returns null.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "ab2fb77484fd",
        "ground_truth": "public Route getMatchingRoute(String path) {\n for (Map.Entry<String, ZuulProperties.ZuulRoute> entry : zuulProperties.getRoutes().entrySet()) {\n  ZuulProperties.ZuulRoute zuulRoute = entry.getValue();\n  String pattern = zuulRoute.getPath();\n  if (pathMatcher.match(pattern, path)) {\n   String targetPath = path.substring(pattern.indexOf(\"*\") - 1);\n   return new Route(targetPath, zuulRoute.getServiceId());\n  }\n }\n return null;\n}",
        "import_statements": [
            "import org.springframework.util.AntPathMatcher;",
            "import org.springframework.util.PathMatcher;",
            "import java.util.Map;"
        ],
        "reference_api": [
            "getPath",
            "getValue",
            "indexOf",
            "entrySet",
            "match",
            "getRoutes",
            "substring",
            "getServiceId"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getPath",
            "getValue",
            "entrySet",
            "match",
            "getRoutes",
            "getServiceId"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry)",
        "start_line": "27",
        "end_line": "44",
        "file_path": "mini-spring-cloud-openfeign/src/main/java/com/github/cloud/openfeign/FeignClientsRegistrar.java",
        "docstring": "This function registers bean definitions dynamically based on classes annotated with @FeignClient within the package of the importing class. It uses reflection to scan for classes with the FeignClient annotation, creates GenericBeanDefinition instances for each annotated class, configures them with FeignClientFactoryBean settings including contextId and type, and registers them with the BeanDefinitionRegistry using their class names as bean names.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "822071925513",
        "ground_truth": "public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) {\n    //\u4e3aFeignClient\u6ce8\u89e3\u4fee\u9970\u7684\u63a5\u53e3\u751f\u6210\u4ee3\u7406bean\u5373Feign\u5ba2\u6237\u7aef\uff0c\u5e76\u6ce8\u518c\u5230bean\u5bb9\u5668\n    String packageName = ClassUtils.getPackageName(importingClassMetadata.getClassName());\n    //\u626b\u63cf\u6240\u6709\u88abFeignClient\u6ce8\u89e3\u4fee\u9970\u7684\u63a5\u53e3\n    Set<Class<?>> classes = ClassUtil.scanPackageByAnnotation(packageName, FeignClient.class);\n    for (Class<?> clazz : classes) {\n        GenericBeanDefinition beanDefinition = new GenericBeanDefinition();\n        //\u4f7f\u7528FeignClientFactoryBean\u751f\u6210Feign\u5ba2\u6237\u7aef\n        beanDefinition.setBeanClass(FeignClientFactoryBean.class);\n        String clientName = clazz.getAnnotation(FeignClient.class).value();\n        beanDefinition.getPropertyValues().addPropertyValue(\"contextId\", clientName);\n        beanDefinition.getPropertyValues().addPropertyValue(\"type\", clazz);\n        //\u5c06Feign\u5ba2\u6237\u7aef\u6ce8\u518c\u8fdbbean\u5bb9\u5668\n        String beanName = clazz.getName();\n        registry.registerBeanDefinition(beanName, beanDefinition);\n    }\n}",
        "import_statements": [
            "import cn.hutool.core.util.ClassUtil;",
            "import org.springframework.beans.factory.support.BeanDefinitionRegistry;",
            "import org.springframework.beans.factory.support.GenericBeanDefinition;",
            "import org.springframework.context.annotation.ImportBeanDefinitionRegistrar;",
            "import org.springframework.core.type.AnnotationMetadata;",
            "import org.springframework.util.ClassUtils;",
            "import java.util.Set;"
        ],
        "reference_api": [
            "getName",
            "addPropertyValue",
            "getAnnotation",
            "registerBeanDefinition",
            "scanPackageByAnnotation",
            "value",
            "setBeanClass",
            "getPackageName",
            "getPropertyValues",
            "getClassName"
        ],
        "repo_defined_api_with_code": [],
        "third_party": [
            "getName",
            "addPropertyValue",
            "getAnnotation",
            "registerBeanDefinition",
            "scanPackageByAnnotation",
            "value",
            "setBeanClass",
            "getPackageName",
            "getPropertyValues",
            "getClassName"
        ]
    },
    {
        "subclass": "Netflix_oos",
        "owner/repo": "DerekYRC/mini-spring-cloud",
        "function_declaration": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties)",
        "start_line": "22",
        "end_line": "24",
        "file_path": "mini-spring-cloud-tutu-discovery/src/main/java/com/github/cloud/tutu/discovery/TutuDiscoveryAutoConfiguration.java",
        "docstring": " This function creates a new instance of TutuDiscoveryClient using the provided TutuDiscoveryProperties object and returns it.",
        "language": "Java",
        "created_time": "",
        "commit_sha": "",
        "instance_id": "3d678e3bf3c8",
        "ground_truth": "public TutuDiscoveryClient tutuDiscoveryClient(TutuDiscoveryProperties tutuDiscoveryProperties) {\n    return new TutuDiscoveryClient(tutuDiscoveryProperties);\n}",
        "import_statements": [
            "import com.github.cloud.tutu.TutuDiscoveryProperties;",
            "import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;",
            "import org.springframework.context.annotation.Bean;",
            "import org.springframework.context.annotation.Configuration;"
        ],
        "reference_api": [],
        "repo_defined_api_with_code": [],
        "third_party": []
    }
]